[
  {
    "title": "Content-based multimedia information retrieval",
    "doi": "https://doi.org/10.1145/1126004.1126005",
    "publication_date": "2006-02-01",
    "publication_year": 2006,
    "authors": "Michael S. Lew; Nicu Sebe; Chabane Djeraba; Ramesh Jain",
    "corresponding_authors": "",
    "abstract": "Extending beyond the boundaries of science, art, and culture, content-based multimedia information retrieval provides new paradigms and methods for searching through the myriad variety of media all over the world. This survey reviews 100+ recent articles on content-based multimedia information retrieval and discusses their role in current research directions which include browsing and search paradigms, user studies, affective computing, learning, semantic queries, new features and media types, high performance indexing, and evaluation techniques. Based on the current state of the art, we discuss the major challenges for the future.",
    "cited_by_count": 1537,
    "openalex_id": "https://openalex.org/W2147069236",
    "type": "article"
  },
  {
    "title": "A Discriminatively Learned CNN Embedding for Person Reidentification",
    "doi": "https://doi.org/10.1145/3159171",
    "publication_date": "2017-12-13",
    "publication_year": 2017,
    "authors": "Zhedong Zheng; Liang Zheng; Yi Yang",
    "corresponding_authors": "",
    "abstract": "We revisit two popular convolutional neural networks (CNN) in person re-identification (re-ID), i.e, verification and classification models. The two models have their respective advantages and limitations due to different loss functions. In this paper, we shed light on how to combine the two models to learn more discriminative pedestrian descriptors. Specifically, we propose a new siamese network that simultaneously computes identification loss and verification loss. Given a pair of training images, the network predicts the identities of the two images and whether they belong to the same identity. Our network learns a discriminative embedding and a similarity measurement at the same time, thus making full usage of the annotations. Albeit simple, the learned embedding improves the state-of-the-art performance on two public person re-ID benchmarks. Further, we show our architecture can also be applied in image retrieval.",
    "cited_by_count": 873,
    "openalex_id": "https://openalex.org/W3098711604",
    "type": "article"
  },
  {
    "title": "Video abstraction",
    "doi": "https://doi.org/10.1145/1198302.1198305",
    "publication_date": "2007-02-01",
    "publication_year": 2007,
    "authors": "Ba Tu Truong; Svetha Venkatesh",
    "corresponding_authors": "",
    "abstract": "The demand for various multimedia applications is rapidly increasing due to the recent advance in the computing and network infrastructure, together with the widespread use of digital video technology. Among the key elements for the success of these applications is how to effectively and efficiently manage and store a huge amount of audio visual information, while at the same time providing user-friendly access to the stored data. This has fueled a quickly evolving research area known as video abstraction . As the name implies, video abstraction is a mechanism for generating a short summary of a video, which can either be a sequence of stationary images ( keyframes ) or moving images ( video skims ). In terms of browsing and navigation, a good video abstract will enable the user to gain maximum information about the target video sequence in a specified time constraint or sufficient information in the minimum time. Over past years, various ideas and techniques have been proposed towards the effective abstraction of video contents. The purpose of this article is to provide a systematic classification of these works. We identify and detail, for each approach, the underlying components and how they are addressed in specific works.",
    "cited_by_count": 821,
    "openalex_id": "https://openalex.org/W2094998392",
    "type": "article"
  },
  {
    "title": "Unsupervised Person Re-identification",
    "doi": "https://doi.org/10.1145/3243316",
    "publication_date": "2018-10-10",
    "publication_year": 2018,
    "authors": "Hehe Fan; Liang Zheng; Chenggang Yan; Yi Yang",
    "corresponding_authors": "",
    "abstract": "The superiority of deeply learned pedestrian representations has been reported in very recent literature of person re-identification (re-ID). In this article, we consider the more pragmatic issue of learning a deep feature with no or only a few labels. We propose a progressive unsupervised learning (PUL) method to transfer pretrained deep representations to unseen domains. Our method is easy to implement and can be viewed as an effective baseline for unsupervised re-ID feature learning. Specifically, PUL iterates between (1) pedestrian clustering and (2) fine-tuning of the convolutional neural network (CNN) to improve the initialization model trained on the irrelevant labeled dataset. Since the clustering results can be very noisy, we add a selection operation between the clustering and fine-tuning. At the beginning, when the model is weak, CNN is fine-tuned on a small amount of reliable examples that locate near to cluster centroids in the feature space. As the model becomes stronger, in subsequent iterations, more images are being adaptively selected as CNN training samples. Progressively, pedestrian clustering and the CNN model are improved simultaneously until algorithm convergence. This process is naturally formulated as self-paced learning. We then point out promising directions that may lead to further improvement. Extensive experiments on three large-scale re-ID datasets demonstrate that PUL outputs discriminative features that improve the re-ID accuracy. Our code has been released at https://github.com/hehefan/Unsupervised-Person-Re-identification-Clustering-and-Fine-tuning.",
    "cited_by_count": 633,
    "openalex_id": "https://openalex.org/W2963975998",
    "type": "article"
  },
  {
    "title": "Procedural content generation for games",
    "doi": "https://doi.org/10.1145/2422956.2422957",
    "publication_date": "2013-02-01",
    "publication_year": 2013,
    "authors": "Mark Hendrikx; Sebastiaan Meijer; Joeri Van Der Velden; Alexandru Iosup",
    "corresponding_authors": "",
    "abstract": "Hundreds of millions of people play computer games every day. For them, game content—from 3D objects to abstract puzzles—plays a major entertainment role. Manual labor has so far ensured that the quality and quantity of game content matched the demands of the playing community, but is facing new scalability challenges due to the exponential growth over the last decade of both the gamer population and the production costs. Procedural Content Generation for Games (PCG-G) may address these challenges by automating, or aiding in, game content generation. PCG-G is difficult, since the generator has to create the content, satisfy constraints imposed by the artist, and return interesting instances for gamers. Despite a large body of research focusing on PCG-G, particularly over the past decade, ours is the first comprehensive survey of the field of PCG-G. We first introduce a comprehensive, six-layered taxonomy of game content: bits, space, systems, scenarios, design, and derived. Second, we survey the methods used across the whole field of PCG-G from a large research body. Third, we map PCG-G methods to game content layers; it turns out that many of the methods used to generate game content from one layer can be used to generate content from another. We also survey the use of methods in practice, that is, in commercial or prototype games. Fourth and last, we discuss several directions for future research in PCG-G, which we believe deserve close attention in the near future.",
    "cited_by_count": 504,
    "openalex_id": "https://openalex.org/W2035033474",
    "type": "article"
  },
  {
    "title": "Understanding and Creating Art with AI: Review and Outlook",
    "doi": "https://doi.org/10.1145/3475799",
    "publication_date": "2022-02-16",
    "publication_year": 2022,
    "authors": "Eva Cetinić; James She",
    "corresponding_authors": "",
    "abstract": "Technologies related to artificial intelligence (AI) have a strong impact on the changes of research and creative practices in visual arts. The growing number of research initiatives and creative applications that emerge in the intersection of AI and art motivates us to examine and discuss the creative and explorative potentials of AI technologies in the context of art. This article provides an integrated review of two facets of AI and art: (1) AI is used for art analysis and employed on digitized artwork collections, or (2) AI is used for creative purposes and generating novel artworks. In the context of AI-related research for art understanding, we present a comprehensive overview of artwork datasets and recent works that address a variety of tasks such as classification, object detection, similarity retrieval, multimodal representations, and computational aesthetics, among others. In relation to the role of AI in creating art, we address various practical and theoretical aspects of AI Art and consolidate related works that deal with those topics in detail. Finally, we provide a concise outlook on the future progression and potential impact of AI technologies on our understanding and creation of art.",
    "cited_by_count": 363,
    "openalex_id": "https://openalex.org/W3130295820",
    "type": "article"
  },
  {
    "title": "CM-GANs",
    "doi": "https://doi.org/10.1145/3284750",
    "publication_date": "2019-02-07",
    "publication_year": 2019,
    "authors": "Yuxin Peng; Jinwei Qi",
    "corresponding_authors": "",
    "abstract": "It is known that the inconsistent distributions and representations of different modalities, such as image and text, cause the heterogeneity gap, which makes it very challenging to correlate heterogeneous data and measure their similarities. Recently, generative adversarial networks (GANs) have been proposed and have shown their strong ability to model data distribution and learn discriminative representation. It has also been shown that adversarial learning can be fully exploited to learn discriminative common representations for bridging the heterogeneity gap. Inspired by this, we aim to effectively correlate large-scale heterogeneous data of different modalities with the power of GANs to model cross-modal joint distribution. In this article, we propose Cross-modal Generative Adversarial Networks (CM-GANs) with the following contributions. First, a cross-modal GAN architecture is proposed to model joint distribution over the data of different modalities. The inter-modality and intra-modality correlation can be explored simultaneously in generative and discriminative models. Both compete with each other to promote cross-modal correlation learning. Second, the cross-modal convolutional autoencoders with weight-sharing constraint are proposed to form the generative model. They not only exploit the cross-modal correlation for learning the common representations but also preserve reconstruction information for capturing the semantic consistency within each modality. Third, a cross-modal adversarial training mechanism is proposed, which uses two kinds of discriminative models to simultaneously conduct intra-modality and inter-modality discrimination. They can mutually boost to make the generated common representations more discriminative by the adversarial training process. In summary, our proposed CM-GAN approach can use GANs to perform cross-modal common representation learning by which the heterogeneous data can be effectively correlated. Extensive experiments are conducted to verify the performance of CM-GANs on cross-modal retrieval compared with 13 state-of-the-art methods on 4 cross-modal datasets.",
    "cited_by_count": 275,
    "openalex_id": "https://openalex.org/W2964216321",
    "type": "article"
  },
  {
    "title": "Machine Learning Techniques for the Diagnosis of Alzheimer’s Disease",
    "doi": "https://doi.org/10.1145/3344998",
    "publication_date": "2020-01-31",
    "publication_year": 2020,
    "authors": "M. Tanveer; Bharat Richhariya; Rooh Ullah Khan; Ashraf Haroon Rashid; Pritee Khanna; Mukesh Prasad; Chin‐Teng Lin",
    "corresponding_authors": "",
    "abstract": "Alzheimer’s disease is an incurable neurodegenerative disease primarily affecting the elderly population. Efficient automated techniques are needed for early diagnosis of Alzheimer’s. Many novel approaches are proposed by researchers for classification of Alzheimer’s disease. However, to develop more efficient learning techniques, better understanding of the work done on Alzheimer’s is needed. Here, we provide a review on 165 papers from 2005 to 2019, using various feature extraction and machine learning techniques. The machine learning techniques are surveyed under three main categories: support vector machine (SVM), artificial neural network (ANN), and deep learning (DL) and ensemble methods. We present a detailed review on these three approaches for Alzheimer’s with possible future directions.",
    "cited_by_count": 271,
    "openalex_id": "https://openalex.org/W3026072590",
    "type": "article"
  },
  {
    "title": "Applying Deep Learning for Epilepsy Seizure Detection and Brain Mapping Visualization",
    "doi": "https://doi.org/10.1145/3241056",
    "publication_date": "2019-01-31",
    "publication_year": 2019,
    "authors": "M. Shamim Hossain; Syed Umar Amin; Mansour Alsulaiman; Ghulam Muhammad",
    "corresponding_authors": "",
    "abstract": "Deep Convolutional Neural Network (CNN) has achieved remarkable results in computer vision tasks for end-to-end learning. We evaluate here the power of a deep CNN to learn robust features from raw Electroencephalogram (EEG) data to detect seizures. Seizures are hard to detect, as they vary both inter- and intra-patient. In this article, we use a deep CNN model for seizure detection task on an open-access EEG epilepsy dataset collected at the Boston Children's Hospital. Our deep learning model is able to extract spectral, temporal features from EEG epilepsy data and use them to learn the general structure of a seizure that is less sensitive to variations. For cross-patient EEG data, our method produced an overall sensitivity of 90.00%, specificity of 91.65%, and overall accuracy of 98.05% for the whole dataset of 23 patients. The system can detect seizures with an accuracy of 99.46%. Thus, it can be used as an excellent cross-patient seizure classifier. The results show that our model performs better than the previous state-of-the-art models for patient-specific and cross-patient seizure detection task. The method gave an overall accuracy of 99.65% for patient-specific data. The system can also visualize the special orientation of band power features. We use correlation maps to relate spectral amplitude features to the output in the form of images. By using the results from our deep learning model, this visualization method can be used as an effective multimedia tool for producing quick and relevant brain mapping images that can be used by medical experts for further investigation.",
    "cited_by_count": 266,
    "openalex_id": "https://openalex.org/W2915149867",
    "type": "article"
  },
  {
    "title": "Multimodal Hand and Foot Gesture Interaction for Handheld Devices",
    "doi": "https://doi.org/10.1145/2645860",
    "publication_date": "2014-10-01",
    "publication_year": 2014,
    "authors": "Zhihan Lv; Alaa Halawani; Shengzhong Feng; Haibo Li; Shafiq ur Réhman",
    "corresponding_authors": "",
    "abstract": "We present a hand-and-foot-based multimodal interaction approach for handheld devices. Our method combines input modalities (i.e., hand and foot) and provides a coordinated output to both modalities along with audio and video. Human foot gesture is detected and tracked using contour-based template detection (CTD) and Tracking-Learning-Detection (TLD) algorithm. 3D foot pose is estimated from passive homography matrix of the camera. 3D stereoscopic and vibrotactile are used to enhance the immersive feeling. We developed a multimodal football game based on the multimodal approach as a proof-of-concept. We confirm our systems user satisfaction through a user study.",
    "cited_by_count": 250,
    "openalex_id": "https://openalex.org/W1989571016",
    "type": "article"
  },
  {
    "title": "Depth Image Denoising Using Nuclear Norm and Learning Graph Model",
    "doi": "https://doi.org/10.1145/3404374",
    "publication_date": "2020-11-30",
    "publication_year": 2020,
    "authors": "Chenggang Yan; Zhisheng Li; Yongbing Zhang; Yutao Liu; Xiangyang Ji; Yongdong Zhang",
    "corresponding_authors": "",
    "abstract": "Depth image denoising is increasingly becoming the hot research topic nowadays, because it reflects the three-dimensional scene and can be applied in various fields of computer vision. But the depth images obtained from depth camera usually contain stains such as noise, which greatly impairs the performance of depth-related applications. In this article, considering that group-based image restoration methods are more effective in gathering the similarity among patches, a group-based nuclear norm and learning graph (GNNLG) model was proposed. For each patch, we find and group the most similar patches within a searching window. The intrinsic low-rank property of the grouped patches is exploited in our model. In addition, we studied the manifold learning method and devised an effective optimized learning strategy to obtain the graph Laplacian matrix, which reflects the topological structure of image, to further impose the smoothing priors to the denoised depth image. To achieve fast speed and high convergence, the alternating direction method of multipliers is proposed to solve our GNNLG. The experimental results show that the proposed method is superior to other current state-of-the-art denoising methods in both subjective and objective criterion.",
    "cited_by_count": 242,
    "openalex_id": "https://openalex.org/W3111278072",
    "type": "article"
  },
  {
    "title": "DenseNet-201-Based Deep Neural Network with Composite Learning Factor and Precomputation for Multiple Sclerosis Classification",
    "doi": "https://doi.org/10.1145/3341095",
    "publication_date": "2020-04-30",
    "publication_year": 2020,
    "authors": "Shuihua Wang‎; Yudong Zhang",
    "corresponding_authors": "",
    "abstract": "( Aim ) Multiple sclerosis is a neurological condition that may cause neurologic disability. Convolutional neural network can achieve good results, but tuning hyperparameters of CNN needs expert knowledge and are difficult and time-consuming. To identify multiple sclerosis more accurately, this article proposed a new transfer-learning-based approach. ( Method ) DenseNet-121, DenseNet-169, and DenseNet-201 neural networks were compared. In addition, we proposed the use of a composite learning factor (CLF) that assigns different learning factor to three types of layers: early frozen layers, middle layers, and late replaced layers. How to allocate layers into those three layers remains a problem. Hence, four transfer learning settings (viz., Settings A, B, C, and D) were tested and compared. A precomputation method was utilized to reduce the storage burden and accelerate the program. ( Results ) We observed that DenseNet-201-D (the layers from CP to T3 are frozen, the layers of D4 are updated with learning factor of 1, and the final new layers of FCL are randomly initialized with learning factor of 10) can achieve the best performance. The sensitivity, specificity, and accuracy of DenseNet-201-D was 98.27± 0.58, 98.35± 0.69, and 98.31± 0.53, respectively. ( Conclusion ) Our method gives better performances than state-of-the-art approaches. Furthermore, this composite learning rate gives superior results to traditional simple learning factor (SLF) strategy.",
    "cited_by_count": 235,
    "openalex_id": "https://openalex.org/W3090140268",
    "type": "article"
  },
  {
    "title": "Age-Invariant Face Recognition by Multi-Feature Fusionand Decomposition with Self-attention",
    "doi": "https://doi.org/10.1145/3472810",
    "publication_date": "2022-01-25",
    "publication_year": 2022,
    "authors": "Chenggang Yan; Lixuan Meng; Liang Li; Jiehua Zhang; Zhan Wang; Jian Yin; Jiyong Zhang; Yaoqi Sun; Bolun Zheng",
    "corresponding_authors": "",
    "abstract": "Different from general face recognition, age-invariant face recognition (AIFR) aims at matching faces with a big age gap. Previous discriminative methods usually focus on decomposing facial feature into age-related and age-invariant components, which suffer from the loss of facial identity information. In this article, we propose a novel Multi-feature Fusion and Decomposition (MFD) framework for age-invariant face recognition, which learns more discriminative and robust features and reduces the intra-class variants. Specifically, we first sample multiple face images of different ages with the same identity as a face time sequence. Then, the multi-head attention is employed to capture contextual information from facial feature series, extracted by the backbone network. Next, we combine feature decomposition with fusion based on the face time sequence to ensure that the final age-independent features effectively represent the identity information of the face and have stronger robustness against the aging process. Besides, we also mitigate imbalanced age distribution in the training data by a re-weighted age loss. We experimented with the proposed MFD over the popular CACD and CACD-VS datasets, where we show that our approach improves the AIFR performance than previous state-of-the-art methods. We simultaneously show the performance of MFD on LFW dataset.",
    "cited_by_count": 185,
    "openalex_id": "https://openalex.org/W4206982465",
    "type": "article"
  },
  {
    "title": "Survey on Deep Multi-modal Data Analytics: Collaboration, Rivalry, and Fusion",
    "doi": "https://doi.org/10.1145/3408317",
    "publication_date": "2021-01-31",
    "publication_year": 2021,
    "authors": "Yang Wang",
    "corresponding_authors": "Yang Wang",
    "abstract": "With the development of web technology, multi-modal or multi-view data has surged as a major stream for big data, where each modal/view encodes individual property of data objects. Often, different modalities are complementary to each other. This fact motivated a lot of research attention on fusing the multi-modal feature spaces to comprehensively characterize the data objects. Most of the existing state-of-the-arts focused on how to fuse the energy or information from multi-modal spaces to deliver a superior performance over their counterparts with single modal. Recently, deep neural networks have been exhibited as a powerful architecture to well capture the nonlinear distribution of high-dimensional multimedia data, so naturally does for multi-modal data. Substantial empirical studies are carried out to demonstrate its advantages that are benefited from deep multi-modal methods, which can essentially deepen the fusion from multi-modal deep feature spaces. In this article, we provide a substantial overview of the existing state-of-the-arts in the field of multi-modal data analytics from shallow to deep spaces. Throughout this survey, we further indicate that the critical components for this field go to collaboration, adversarial competition, and fusion over multi-modal spaces. Finally, we share our viewpoints regarding some future directions in this field.",
    "cited_by_count": 158,
    "openalex_id": "https://openalex.org/W3140110584",
    "type": "article"
  },
  {
    "title": "Precise No-Reference Image Quality Evaluation Based on Distortion Identification",
    "doi": "https://doi.org/10.1145/3468872",
    "publication_date": "2021-10-31",
    "publication_year": 2021,
    "authors": "Chenggang Yan; Tong Teng; Yutao Liu; Yongbing Zhang; Haoqian Wang; Xiangyang Ji",
    "corresponding_authors": "",
    "abstract": "The difficulty of no-reference image quality assessment (NR IQA) often lies in the lack of knowledge about the distortion in the image, which makes quality assessment blind and thus inefficient. To tackle such issue, in this article, we propose a novel scheme for precise NR IQA, which includes two successive steps, i.e., distortion identification and targeted quality evaluation. In the first step, we employ the well-known Inception-ResNet-v2 neural network to train a classifier that classifies the possible distortion in the image into the four most common distortion types, i.e., Gaussian white noise (WN), Gaussian blur (GB), jpeg compression (JPEG), and jpeg2000 compression (JP2K). Specifically, the deep neural network is trained on the large-scale Waterloo Exploration database, which ensures the robustness and high performance of distortion classification. In the second step, after determining the distortion type of the image, we then design a specific approach to quantify the image distortion level, which can estimate the image quality specially and more precisely. Extensive experiments performed on LIVE, TID2013, CSIQ, and Waterloo Exploration databases demonstrate that (1) the accuracy of our distortion classification is higher than that of the state-of-the-art distortion classification methods, and (2) the proposed NR IQA method outperforms the state-of-the-art NR IQA methods in quantifying the image quality.",
    "cited_by_count": 145,
    "openalex_id": "https://openalex.org/W3214007456",
    "type": "article"
  },
  {
    "title": "Learning Adaptive Spatial-Temporal Context-Aware Correlation Filters for UAV Tracking",
    "doi": "https://doi.org/10.1145/3486678",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Di Yuan; Xiaojun Chang; Zhihui Li; Zhenyu He",
    "corresponding_authors": "",
    "abstract": "Tracking in the unmanned aerial vehicle (UAV) scenarios is one of the main components of target-tracking tasks. Different from the target-tracking task in the general scenarios, the target-tracking task in the UAV scenarios is very challenging because of factors such as small scale and aerial view. Although the discriminative correlation filter (DCF)-based tracker has achieved good results in tracking tasks in general scenarios, the boundary effect caused by the dense sampling method will reduce the tracking accuracy, especially in UAV-tracking scenarios. In this work, we propose learning an adaptive spatial-temporal context-aware (ASTCA) model in the DCF-based tracking framework to improve the tracking accuracy and reduce the influence of boundary effect, thereby enabling our tracker to more appropriately handle UAV-tracking tasks. Specifically, our ASTCA model can learn a spatial-temporal context weight, which can precisely distinguish the target and background in the UAV-tracking scenarios. Besides, considering the small target scale and the aerial view in UAV-tracking scenarios, our ASTCA model incorporates spatial context information within the DCF-based tracker, which could effectively alleviate background interference. Extensive experiments demonstrate that our ASTCA method performs favorably against state-of-the-art tracking methods on some standard UAV datasets.",
    "cited_by_count": 126,
    "openalex_id": "https://openalex.org/W4214872590",
    "type": "article"
  },
  {
    "title": "Fine-Grained Visual Textual Alignment for Cross-Modal Retrieval Using Transformer Encoders",
    "doi": "https://doi.org/10.1145/3451390",
    "publication_date": "2021-11-12",
    "publication_year": 2021,
    "authors": "Nicola Messina; Giuseppe Amato; Andrea Esuli; Fabrizio Falchi; Claudio Gennaro; Stéphane Marchand‐Maillet",
    "corresponding_authors": "",
    "abstract": "Despite the evolution of deep-learning-based visual-textual processing systems, precise multi-modal matching remains a challenging task. In this work, we tackle the task of cross-modal retrieval through image-sentence matching based on word-region alignments, using supervision only at the global image-sentence level. Specifically, we present a novel approach called Transformer Encoder Reasoning and Alignment Network (TERAN). TERAN enforces a fine-grained match between the underlying components of images and sentences (i.e., image regions and words, respectively) to preserve the informative richness of both modalities. TERAN obtains state-of-the-art results on the image retrieval task on both MS-COCO and Flickr30k datasets. Moreover, on MS-COCO, it also outperforms current approaches on the sentence retrieval task. Focusing on scalable cross-modal information retrieval, TERAN is designed to keep the visual and textual data pipelines well separated. Cross-attention links invalidate any chance to separately extract visual and textual features needed for the online search and the offline indexing steps in large-scale retrieval systems. In this respect, TERAN merges the information from the two domains only during the final alignment phase, immediately before the loss computation. We argue that the fine-grained alignments produced by TERAN pave the way toward the research for effective and efficient methods for large-scale cross-modal information retrieval. We compare the effectiveness of our approach against relevant state-of-the-art methods. On the MS-COCO 1K test set, we obtain an improvement of 5.7% and 3.5% respectively on the image and the sentence retrieval tasks on the Recall@1 metric. The code used for the experiments is publicly available on GitHub at https://github.com/mesnico/TERAN .",
    "cited_by_count": 121,
    "openalex_id": "https://openalex.org/W3213100861",
    "type": "article"
  },
  {
    "title": "Wavelength-based Attributed Deep Neural Network for Underwater Image Restoration",
    "doi": "https://doi.org/10.1145/3511021",
    "publication_date": "2022-07-14",
    "publication_year": 2022,
    "authors": "Prasen Kumar Sharma; Ira Bisht; Arijit Sur",
    "corresponding_authors": "",
    "abstract": "Background: Underwater images, in general, suffer from low contrast and high color distortions due to the non-uniform attenuation of the light as it propagates through the water. In addition, the degree of attenuation varies with the wavelength, resulting in the asymmetric traversing of colors. Despite the prolific works for underwater image restoration (UIR) using deep learning, the above asymmetricity has not been addressed in the respective network engineering. Contributions: As the first novelty, this article shows that attributing the right receptive field size ( context ) based on the traversing range of the color channel may lead to a substantial performance gain for the task of UIR. Further, it is important to suppress the irrelevant multi-contextual features and increase the representational power of the model. Therefore, as a second novelty, we have incorporated an attentive skip mechanism to adaptively refine the learned multi-contextual features. The proposed framework, called Deep WaveNet , is optimized using the traditional pixel-wise and feature-based cost functions. An extensive set of experiments have been carried out to show the efficacy of the proposed scheme over existing best-published literature on benchmark datasets. More importantly, we have demonstrated a comprehensive validation of enhanced images across various high-level vision tasks, e.g., underwater image semantic segmentation and diver’s 2D pose estimation. A sample video to exhibit our real-world performance is available at https://tinyurl.com/yzcrup9n . Also, we have open-sourced our framework at https://github.com/pksvision/Deep-WaveNet-Underwater-Image-Restoration .",
    "cited_by_count": 108,
    "openalex_id": "https://openalex.org/W3171662571",
    "type": "article"
  },
  {
    "title": "A Review on Methods and Applications in Multimodal Deep Learning",
    "doi": "https://doi.org/10.1145/3545572",
    "publication_date": "2022-10-27",
    "publication_year": 2022,
    "authors": "Summaira Jabeen; Xi Li; Muhammad Shoib Amin; Omar El Farouk Bourahla; Songyuan Li; Abdul Jabbar",
    "corresponding_authors": "",
    "abstract": "Deep Learning has implemented a wide range of applications and has become increasingly popular in recent years. The goal of multimodal deep learning (MMDL) is to create models that can process and link information using various modalities. Despite the extensive development made for unimodal learning, it still cannot cover all the aspects of human learning. Multimodal learning helps to understand and analyze better when various senses are engaged in the processing of information. This article focuses on multiple types of modalities, i.e., image, video, text, audio, body gestures, facial expressions, physiological signals, flow, RGB, pose, depth, mesh, and point cloud. Detailed analysis of the baseline approaches and an in-depth study of recent advancements during the past five years (2017 to 2021) in multimodal deep learning applications has been provided. A fine-grained taxonomy of various multimodal deep learning methods is proposed, elaborating on different applications in more depth. Last, main issues are highlighted separately for each domain, along with their possible future research directions.",
    "cited_by_count": 103,
    "openalex_id": "https://openalex.org/W4307640249",
    "type": "review"
  },
  {
    "title": "Smart City Construction and Management by Digital Twins and BIM Big Data in COVID-19 Scenario",
    "doi": "https://doi.org/10.1145/3529395",
    "publication_date": "2022-04-04",
    "publication_year": 2022,
    "authors": "Zhihan Lv; Dongliang Chen; Haibin Lv",
    "corresponding_authors": "",
    "abstract": "With the rapid development of information technology and the spread of Corona Virus Disease 2019 (COVID-19), the government and urban managers are looking for ways to use technology to make the city smarter and safer. Intelligent transportation can play a very important role in the joint prevention. This work expects to explore the building information modeling (BIM) big data (BD) processing method of digital twins (DTs) of Smart City, thus speeding up the construction of Smart City and improve the accuracy of data processing. During construction, DTs build the same digital copy of the smart city. On this basis, BIM designs the building's keel and structure, optimizing various resources and configurations of the building. Regarding the fast data growth in smart cities, a complex data fusion and efficient learning algorithm, namely Multi- Graphics Processing Unit (GPU) , is proposed to process the multi-dimensional and complex BD based on the compositive rough set model. The Bayesian network solves the multi-label classification. Each label is regarded as a Bayesian network node. Then, the structural learning approach is adopted to learn the label Bayesian network's structure from data. On the P53-old and the P53-new datasets, the running time of Multi-GPU decreases as the number of GPUs increases, approaching the ideal linear speedup ratio. With the continuous increase of K value, the deterministic information input into the tag BN will be reduced, thus reducing the classification accuracy. When K = 3, MLBN can provide the best data analysis performance. On genbase dataset, the accuracy of MLBN is 0.982 ± 0.013. Through experiments, the BIM BD processing algorithm based on Bayesian Network Structural Learning (BNSL) helps decision-makers use complex data in smart cities efficiently.",
    "cited_by_count": 98,
    "openalex_id": "https://openalex.org/W4226242968",
    "type": "article"
  },
  {
    "title": "Cross-modal Graph Matching Network for Image-text Retrieval",
    "doi": "https://doi.org/10.1145/3499027",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Yuhao Cheng; Xiaoguang Zhu; Jiuchao Qian; Fei Wen; Peilin Liu",
    "corresponding_authors": "",
    "abstract": "Image-text retrieval is a fundamental cross-modal task whose main idea is to learn image-text matching. Generally, according to whether there exist interactions during the retrieval process, existing image-text retrieval methods can be classified into independent representation matching methods and cross-interaction matching methods. The independent representation matching methods generate the embeddings of images and sentences independently and thus are convenient for retrieval with hand-crafted matching measures (e.g., cosine or Euclidean distance). As to the cross-interaction matching methods, they achieve improvement by introducing the interaction-based networks for inter-relation reasoning, yet suffer the low retrieval efficiency. This article aims to develop a method that takes the advantages of cross-modal inter-relation reasoning of cross-interaction methods while being as efficient as the independent methods. To this end, we propose a graph-based Cross-modal Graph Matching Network (CGMN) , which explores both intra- and inter-relations without introducing network interaction. In CGMN, graphs are used for both visual and textual representation to achieve intra-relation reasoning across regions and words, respectively. Furthermore, we propose a novel graph node matching loss to learn fine-grained cross-modal correspondence and to achieve inter-relation reasoning. Experiments on benchmark datasets MS-COCO, Flickr8K, and Flickr30K show that CGMN outperforms state-of-the-art methods in image retrieval. Moreover, CGMM is much more efficient than state-of-the-art methods using interactive matching. The code is available at https://github.com/cyh-sj/CGMN .",
    "cited_by_count": 88,
    "openalex_id": "https://openalex.org/W4214819138",
    "type": "article"
  },
  {
    "title": "Music2Dance: DanceNet for Music-Driven Dance Generation",
    "doi": "https://doi.org/10.1145/3485664",
    "publication_date": "2022-02-16",
    "publication_year": 2022,
    "authors": "Wenlin Zhuang; Congyi Wang; Jinxiang Chai; Yangang Wang; Ming Shao; Siyu Xia",
    "corresponding_authors": "",
    "abstract": "Synthesize human motions from music (i.e., music to dance) is appealing and has attracted lots of research interests in recent years. It is challenging because of the requirement for realistic and complex human motions for dance, but more importantly, the synthesized motions should be consistent with the style, rhythm, and melody of the music. In this article, we propose a novel autoregressive generative model, DanceNet, to take the style, rhythm, and melody of music as the control signals to generate 3D dance motions with high realism and diversity. Due to the high long-term spatio-temporal complexity of dance, we propose the dilated convolution to improve the receptive field, and adopt the gated activation unit as well as separable convolution to enhance the fusion of motion features and control signals. To boost the performance of our proposed model, we capture several synchronized music-dance pairs by professional dancers and build a high-quality music-dance pair dataset. Experiments have demonstrated that the proposed method can achieve state-of-the-art results.",
    "cited_by_count": 87,
    "openalex_id": "https://openalex.org/W3010876998",
    "type": "article"
  },
  {
    "title": "Deep Learning-based Smart Predictive Evaluation for Interactive Multimedia-enabled Smart Healthcare",
    "doi": "https://doi.org/10.1145/3468506",
    "publication_date": "2022-01-25",
    "publication_year": 2022,
    "authors": "Zhihan Lv; Zengchen Yu; Shuxuan Xie; Atif Alamri",
    "corresponding_authors": "",
    "abstract": "Two-dimensional 1 arrays of bi-component structures made of cobalt and permalloy elliptical dots with thickness of 25 nm, length 1 mm and width of 225 nm, have been prepared by a self-aligned shadow deposition technique. Brillouin light scattering has been exploited to study the frequency dependence of thermally excited magnetic eigenmodes on the intensity of the external magnetic field, applied along the easy axis of the elements. This study aims to enhance the security for people's health, improve the medical level further, and increase the confidentiality of people's privacy information. Under the trend of wide application of deep learning algorithms, the convolutional neural network (CNN) is modified to build an interactive smart healthcare prediction and evaluation model (SHPE model) based on the deep learning model. The model is optimized and standardized for data processing. Then, the constructed model is simulated to analyze its performance. The results show that accuracy of the constructed system reaches 82.4%, which is at least 2.4% higher than other advanced CNN algorithms and 3.3% higher than other classical machine algorithms. It is proved based on comparison that the accuracy, precision, recall, and F1 of the constructed model are the highest. Further analysis on error shows that the constructed model shows the smallest error of 23.34 pixels. Therefore, it is proved that the built SHPE model shows higher prediction accuracy and smaller error while ensuring the safety performance, which provides an experimental reference for the prediction and evaluation of smart healthcare treatment in the later stage.",
    "cited_by_count": 86,
    "openalex_id": "https://openalex.org/W4207047994",
    "type": "article"
  },
  {
    "title": "UID2021: An Underwater Image Dataset for Evaluation of No-Reference Quality Assessment Metrics",
    "doi": "https://doi.org/10.1145/3578584",
    "publication_date": "2023-01-06",
    "publication_year": 2023,
    "authors": "Guojia Hou; Yuxuan Li; Huan Yang; Kunqian Li; Zhenkuan Pan",
    "corresponding_authors": "",
    "abstract": "Achieving subjective and objective quality assessment of underwater images is of high significance in underwater visual perception and image/video processing. However, the development of underwater image quality assessment (UIQA) is limited for the lack of publicly available underwater image datasets with human subjective scores and reliable objective UIQA metrics. To address this issue, we establish a large-scale underwater image dataset, dubbed UID2021, for evaluating no-reference (NR) UIQA metrics. The constructed dataset contains 60 multiply degraded underwater images collected from various sources, covering six common underwater scenes (i.e., bluish scene, blue-green scene, greenish scene, hazy scene, low-light scene, and turbid scene), and their corresponding 900 quality improved versions are generated by employing 15 state-of-the-art underwater image enhancement and restoration algorithms. Mean opinion scores with 52 observers for each image of UID2021 are also obtained by using the pairwise comparison sorting method. Both in-air and underwater-specific NR IQA algorithms are tested on our constructed dataset to fairly compare their performance and analyze their strengths and weaknesses. Our proposed UID2021 dataset enables ones to evaluate NR UIQA algorithms comprehensively and paves the way for further research on UIQA. The dataset is available at https://github.com/Hou-Guojia/UID2021 .",
    "cited_by_count": 71,
    "openalex_id": "https://openalex.org/W4313641268",
    "type": "article"
  },
  {
    "title": "Deep Convolutional Pooling Transformer for Deepfake Detection",
    "doi": "https://doi.org/10.1145/3588574",
    "publication_date": "2023-03-27",
    "publication_year": 2023,
    "authors": "Tianyi Wang; Harry H. Cheng; K. P. Chow; Liqiang Nie",
    "corresponding_authors": "",
    "abstract": "Recently, Deepfake has drawn considerable public attention due to security and privacy concerns in social media digital forensics. As the wildly spreading Deepfake videos on the Internet become more realistic, traditional detection techniques have failed in distinguishing between real and fake. Most existing deep learning methods mainly focus on local features and relations within the face image using convolutional neural networks as a backbone. However, local features and relations are insufficient for model training to learn enough general information for Deepfake detection. Therefore, the existing Deepfake detection methods have reached a bottleneck to further improve the detection performance. To address this issue, we propose a deep convolutional Transformer to incorporate the decisive image features both locally and globally. Specifically, we apply convolutional pooling and re-attention to enrich the extracted features and enhance efficacy. Moreover, we employ the barely discussed image keyframes in model training for performance improvement and visualize the feature quantity gap between the key and normal image frames caused by video compression. We finally illustrate the transferability with extensive experiments on several Deepfake benchmark datasets. The proposed solution consistently outperforms several state-of-the-art baselines on both within- and cross-dataset experiments.",
    "cited_by_count": 59,
    "openalex_id": "https://openalex.org/W4361003656",
    "type": "article"
  },
  {
    "title": "VM-UNet: Vision Mamba UNet for Medical Image Segmentation",
    "doi": "https://doi.org/10.1145/3767748",
    "publication_date": "2025-09-16",
    "publication_year": 2025,
    "authors": "Jiacheng Ruan; Jincheng Li; Suncheng Xiang",
    "corresponding_authors": "",
    "abstract": "In the realm of medical image segmentation, both CNN-based and Transformer-based models have been extensively explored. However, CNNs exhibit limitations in long-range modeling capabilities, whereas Transformers are hampered by their quadratic computational complexity. Recently, State Space Models (SSMs), exemplified by Mamba, have emerged as a promising approach. They not only excel in modeling long-range interactions but also maintain a linear computational complexity. In this paper, leveraging state space models, we propose a U-shape architecture model for medical image segmentation, named Vision Mamba UNet (VM-UNet). Specifically, the Visual State Space (VSS) block is introduced as the foundation block to capture extensive contextual information, and an asymmetrical encoder-decoder structure is constructed with fewer convolution layers to save calculation cost. We conduct comprehensive experiments on the ISIC17, ISIC18, and Synapse datasets, and the results indicate that VM-UNet performs competitively in medical image segmentation tasks, e.g. obtaining 89.03, 89.71 and 81.08 in terms of DSC score on three datasets respectively. To our best knowledge, this is the first medical image segmentation model constructed based on the pure SSM-based model. We aim to establish a baseline and provide valuable insights for the future development of more efficient and effective SSM-based segmentation systems. Our code is available at https://github.com/JCruan519/VM-UNet .",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W4414243662",
    "type": "article"
  },
  {
    "title": "Mastering Deepfake Detection: A Cutting-Edge Approach to Distinguish GAN and Diffusion-Model Images",
    "doi": "https://doi.org/10.1145/3652027",
    "publication_date": "2024-03-09",
    "publication_year": 2024,
    "authors": "Luca Guarnera; Oliver Giudice; Sebastiano Battiato",
    "corresponding_authors": "",
    "abstract": "Detecting and recognizing deepfakes is a pressing issue in the digital age. In this study, we first collected a dataset of pristine images and fake ones properly generated by nine different Generative Adversarial Network (GAN) architectures and four Diffusion Models (DM). The dataset contained a total of 83,000 images, with equal distribution between the real and deepfake data. Then, to address different deepfake detection and recognition tasks, we proposed a hierarchical multi-level approach. At the first level, we classified real images from AI-generated ones. At the second level, we distinguished between images generated by GANs and DMs. At the third level (composed of two additional sub-levels), we recognized the specific GAN and DM architectures used to generate the synthetic data. Experimental results demonstrated that our approach achieved more than 97% classification accuracy, outperforming existing state-of-the-art methods. The models obtained in the different levels turn out to be robust to various attacks such as JPEG compression (with different quality factor values) and resize (and others), demonstrating that the framework can be used and applied in real-world contexts (such as the analysis of multimedia data shared in the various social platforms) for support even in forensic investigations to counter the illicit use of these powerful and modern generative models. We are able to identify the specific GAN and DM architecture used to generate the image, which is critical in tracking down the source of the deepfake. Our hierarchical multi-level approach to deepfake detection and recognition shows promising results in identifying deepfakes allowing focus on underlying task by improving (about 2% on the average) standard multiclass flat detection systems. The proposed method has the potential to enhance the performance of deepfake detection systems, aid in the fight against the spread of fake images, and safeguard the authenticity of digital media.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W4392623241",
    "type": "article"
  },
  {
    "title": "Panoptes: scalable low-power video sensor networking technologies",
    "doi": "https://doi.org/10.1145/1062253.1062256",
    "publication_date": "2005-05-01",
    "publication_year": 2005,
    "authors": "Wu‐chi Feng; Ed Kaiser; Wu Feng; Mikael Le Baillif",
    "corresponding_authors": "",
    "abstract": "Video-based sensor networks can provide important visual information in a number of applications including: environmental monitoring, health care, emergency response, and video security. This article describes the Panoptes video-based sensor networking architecture, including its design, implementation, and performance. We describe two video sensor platforms that can deliver high-quality video over 802.11 networks with a power requirement less than 5 watts. In addition, we describe the streaming and prioritization mechanisms that we have designed to allow it to survive long-periods of disconnected operation. Finally, we describe a sample application and bitmapping algorithm that we have implemented to show the usefulness of our platform. Our experiments include an in-depth analysis of the bottlenecks within the system as well as power measurements for the various components of the system.",
    "cited_by_count": 235,
    "openalex_id": "https://openalex.org/W2069689564",
    "type": "article"
  },
  {
    "title": "Temporal event clustering for digital photo collections",
    "doi": "https://doi.org/10.1145/1083314.1083317",
    "publication_date": "2005-08-01",
    "publication_year": 2005,
    "authors": "Matthew Cooper; Jonathan Foote; Andreas Girgensohn; Lynn Wilcox",
    "corresponding_authors": "",
    "abstract": "Organizing digital photograph collections according to events such as holiday gatherings or vacations is a common practice among photographers. To support photographers in this task, we present similarity-based methods to cluster digital photos by time and image content. The approach is general and unsupervised, and makes minimal assumptions regarding the structure or statistics of the photo collection. We present several variants of an automatic unsupervised algorithm to partition a collection of digital photographs based either on temporal similarity alone, or on temporal and content-based similarity. First, interphoto similarity is quantified at multiple temporal scales to identify likely event clusters. Second, the final clusters are determined according to one of three clustering goodness criteria. The clustering criteria trade off computational complexity and performance. We also describe a supervised clustering method based on learning vector quantization. Finally, we review the results of an experimental evaluation of the proposed algorithms and existing approaches on two test collections.",
    "cited_by_count": 195,
    "openalex_id": "https://openalex.org/W2133268862",
    "type": "article"
  },
  {
    "title": "Multimedia streaming via TCP",
    "doi": "https://doi.org/10.1145/1352012.1352020",
    "publication_date": "2008-05-01",
    "publication_year": 2008,
    "authors": "Bing Wang; Jim Kurose; Prashant Shenoy; Don Towsley",
    "corresponding_authors": "",
    "abstract": "TCP is widely used in commercial multimedia streaming systems, with recent measurement studies indicating that a significant fraction of Internet streaming media is currently delivered over HTTP/TCP. These observations motivate us to develop analytic performance models to systematically investigate the performance of TCP for both live and stored-media streaming. We validate our models via ns simulations and experiments conducted over the Internet. Our models provide guidelines indicating the circumstances under which TCP streaming leads to satisfactory performance, showing, for example, that TCP generally provides good streaming performance when the achievable TCP throughput is roughly twice the media bitrate, with only a few seconds of startup delay.",
    "cited_by_count": 190,
    "openalex_id": "https://openalex.org/W2132454267",
    "type": "article"
  },
  {
    "title": "Semi-supervised distance metric learning for collaborative image retrieval and clustering",
    "doi": "https://doi.org/10.1145/1823746.1823752",
    "publication_date": "2010-08-01",
    "publication_year": 2010,
    "authors": "Steven C. H. Hoi; Wei Liu; Shih‐Fu Chang",
    "corresponding_authors": "",
    "abstract": "Learning a good distance metric plays a vital role in many multimedia retrieval and data mining tasks. For example, a typical content-based image retrieval (CBIR) system often relies on an effective distance metric to measure similarity between any two images. Conventional CBIR systems simply adopting Euclidean distance metric often fail to return satisfactory results mainly due to the well-known semantic gap challenge. In this article, we present a novel framework of Semi-Supervised Distance Metric Learning for learning effective distance metrics by exploring the historical relevance feedback log data of a CBIR system and utilizing unlabeled data when log data are limited and noisy. We formally formulate the learning problem into a convex optimization task and then present a new technique, named as “Laplacian Regularized Metric Learning” (LRML). Two efficient algorithms are then proposed to solve the LRML task. Further, we apply the proposed technique to two applications. One direct application is for Collaborative Image Retrieval (CIR), which aims to explore the CBIR log data for improving the retrieval performance of CBIR systems. The other application is for Collaborative Image Clustering (CIC), which aims to explore the CBIR log data for enhancing the clustering performance of image pattern clustering tasks. We conduct extensive evaluation to compare the proposed LRML method with a number of competing methods, including 2 standard metrics, 3 unsupervised metrics, and 4 supervised metrics with side information. Encouraging results validate the effectiveness of the proposed technique.",
    "cited_by_count": 172,
    "openalex_id": "https://openalex.org/W1977193486",
    "type": "article"
  },
  {
    "title": "Mulsemedia",
    "doi": "https://doi.org/10.1145/2617994",
    "publication_date": "2014-10-01",
    "publication_year": 2014,
    "authors": "Gheorghiță Ghinea; Christian Timmerer; Weisi Lin; Stephen R. Gulliver",
    "corresponding_authors": "",
    "abstract": "Mulsemedia—multiple sensorial media—captures a wide variety of research efforts and applications. This article presents a historic perspective on mulsemedia work and reviews current developments in the area. These take place across the traditional multimedia spectrum—from virtual reality applications to computer games—as well as efforts in the arts, gastronomy, and therapy, to mention a few. We also describe standardization efforts, via the MPEG-V standard, and identify future developments and exciting challenges the community needs to overcome.",
    "cited_by_count": 160,
    "openalex_id": "https://openalex.org/W1966403068",
    "type": "article"
  },
  {
    "title": "Content-based retrieval of 3D models",
    "doi": "https://doi.org/10.1145/1126004.1126006",
    "publication_date": "2006-02-01",
    "publication_year": 2006,
    "authors": "Alberto Del Bimbo; Pietro Pala",
    "corresponding_authors": "",
    "abstract": "In the past few years, there has been an increasing availability of technologies for the acquisition of digital 3D models of real objects and the consequent use of these models in a variety of applications, in medicine, engineering, and cultural heritage. In this framework, content-based retrieval of 3D objects is becoming an important subject of research, and finding adequate descriptors to capture global or local characteristics of the shape has become one of the main investigation goals. In this article, we present a comparative analysis of a few different solutions for description and retrieval by similarity of 3D models that are representative of the principal classes of approaches proposed. We have developed an experimental analysis by comparing these methods according to their robustness to deformations, the ability to capture an object's structural complexity, and the resolution at which models are considered.",
    "cited_by_count": 160,
    "openalex_id": "https://openalex.org/W2034785770",
    "type": "article"
  },
  {
    "title": "Generalized Deep Transfer Networks for Knowledge Propagation in Heterogeneous Domains",
    "doi": "https://doi.org/10.1145/2998574",
    "publication_date": "2016-11-18",
    "publication_year": 2016,
    "authors": "Jinhui Tang; Xiangbo Shu; Zechao Li; Guo-Jun Qi; Jingdong Wang",
    "corresponding_authors": "",
    "abstract": "In recent years, deep neural networks have been successfully applied to model visual concepts and have achieved competitive performance on many tasks. Despite their impressive performance, traditional deep networks are subjected to the decayed performance under the condition of lacking sufficient training data. This problem becomes extremely severe for deep networks trained on a very small dataset, making them overfitting by capturing nonessential or noisy information in the training set. Toward this end, we propose a novel generalized deep transfer networks (DTNs), capable of transferring label information across heterogeneous domains, textual domain to visual domain. The proposed framework has the ability to adequately mitigate the problem of insufficient training images by bringing in rich labels from the textual domain. Specifically, to share the labels between two domains, we build parameter- and representation-shared layers. They are able to generate domain-specific and shared interdomain features, making this architecture flexible and powerful in capturing complex information from different domains jointly. To evaluate the proposed method, we release a new dataset extended from NUS-WIDE at http://imag.njust.edu.cn/NUS-WIDE-128.html. Experimental results on this dataset show the superior performance of the proposed DTNs compared to existing state-of-the-art methods.",
    "cited_by_count": 150,
    "openalex_id": "https://openalex.org/W2552639984",
    "type": "article"
  },
  {
    "title": "Artificial Intelligence, Artists, and Art",
    "doi": "https://doi.org/10.1145/3326337",
    "publication_date": "2019-04-30",
    "publication_year": 2019,
    "authors": "Joo-Wha Hong; Nathaniel Ming Curran",
    "corresponding_authors": "",
    "abstract": "This study examines how people perceive artwork created by artificial intelligence (AI) and how presumed knowledge of an artist's identity (Human vs. AI) affects individuals’ evaluation of art. Drawing on Schema theory and theory of Computers Are Social Actors (CASA), this study used a survey-experiment that controlled for the identity of the artist (AI vs. Human) and presented participants with two types of artworks (AI-created vs. Human-created). After seeing images of six artworks created by either AI or human artists, participants ( n = 288) were asked to evaluate the artistic value using a validated scale commonly employed among art professionals. The study found that human-created artworks and AI-created artworks were not judged to be equivalent in their artistic value. Additionally, knowing that a piece of art was created by AI did not, in general, influence participants’ evaluation of art pieces’ artistic value. However, having a schema that AI cannot make art significantly influenced evaluation. Implications of the findings for application and theory are discussed.",
    "cited_by_count": 148,
    "openalex_id": "https://openalex.org/W2964268190",
    "type": "article"
  },
  {
    "title": "GamingAnywhere",
    "doi": "https://doi.org/10.1145/2537855",
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "Chun‐Ying Huang; Kuan-Ta Chen; Deyu Chen; Hwai-Jung Hsu; Cheng-Hsin Hsu",
    "corresponding_authors": "",
    "abstract": "We present the first open source cloud gaming system, called GamingAnywhere. In addition to its openness, we have designed, GamingAnywhere for high extensibility, portability, and reconfigurability. We implemented it on Windows, Linux, OS X, and Android. We conducted extensive experiments to evaluate its performance. Our experimental results indicate that GamingAnywhere is efficient, scalable, adaptable to network conditions, and achieves high responsiveness and streaming quality. GamingAnywhere can be employed by researchers, game developers, service providers, and end users for setting up cloud gaming testbeds, which we believe, will stimulate more research into innovations for cloud gaming systems and applications.",
    "cited_by_count": 140,
    "openalex_id": "https://openalex.org/W2171119981",
    "type": "article"
  },
  {
    "title": "Image Captioning with Deep Bidirectional LSTMs and Multi-Task Learning",
    "doi": "https://doi.org/10.1145/3115432",
    "publication_date": "2018-04-25",
    "publication_year": 2018,
    "authors": "Cheng Wang; Haojin Yang; Christoph Meinel",
    "corresponding_authors": "",
    "abstract": "Generating a novel and descriptive caption of an image is drawing increasing interests in computer vision, natural language processing, and multimedia communities. In this work, we propose an end-to-end trainable deep bidirectional LSTM (Bi-LSTM (Long Short-Term Memory)) model to address the problem. By combining a deep convolutional neural network (CNN) and two separate LSTM networks, our model is capable of learning long-term visual-language interactions by making use of history and future context information at high-level semantic space. We also explore deep multimodal bidirectional models, in which we increase the depth of nonlinearity transition in different ways to learn hierarchical visual-language embeddings. Data augmentation techniques such as multi-crop, multi-scale, and vertical mirror are proposed to prevent overfitting in training deep models. To understand how our models “translate” image to sentence, we visualize and qualitatively analyze the evolution of Bi-LSTM internal states over time. The effectiveness and generality of proposed models are evaluated on four benchmark datasets: Flickr8K, Flickr30K, MSCOCO, and Pascal1K datasets. We demonstrate that Bi-LSTM models achieve highly competitive performance on both caption generation and image-sentence retrieval even without integrating an additional mechanism (e.g., object detection, attention model). Our experiments also prove that multi-task learning is beneficial to increase model generality and gain performance. We also demonstrate the performance of transfer learning of the Bi-LSTM model significantly outperforms previous methods on the Pascal1K dataset.",
    "cited_by_count": 138,
    "openalex_id": "https://openalex.org/W2801271919",
    "type": "article"
  },
  {
    "title": "Video streaming using a location-based bandwidth-lookup service for bitrate planning",
    "doi": "https://doi.org/10.1145/2240136.2240137",
    "publication_date": "2012-07-01",
    "publication_year": 2012,
    "authors": "Haakon Riiser; Tore Endestad; Paul Vigmostad; Carsten Griwodz; Pål Halvorsen",
    "corresponding_authors": "",
    "abstract": "A lot of people around the world commute using public transportation and would like to spend this time viewing streamed video content such as news or sports updates. However, mobile wireless networks typically suffer from severe bandwidth fluctuations, and the networks are often completely unresponsive for several seconds, sometimes minutes. Today, there are several ways of adapting the video bitrate and thus the video quality to such fluctuations, for example, using scalable video codecs or segmented adaptive HTTP streaming that switches between nonscalable video streams encoded in different bitrates. Still, for a better long-term video playout experience that avoids disruptions and frequent quality changes while using existing video adaptation technology, it is desirable to perform bandwidth prediction and planned quality adaptation. This article describes a video streaming system for receivers equipped with a GPS. A receiver's download rate is constantly monitored, and periodically reported back to a central database along with associated GPS positional data. Thus, based on the current location, a streaming device can use a GPS-based bandwidth-lookup service in order to better predict the near-future bandwidth availability and create a schedule for the video playout that takes likely future availability into account. To create a prototype and perform initial tests, we conducted several field trials while commuting using public transportation. We show how our database has been used to predict bandwidth fluctuations and network outages, and how this information helps maintain uninterrupted playback with less compromise on video quality than possible without prediction.",
    "cited_by_count": 137,
    "openalex_id": "https://openalex.org/W2091326421",
    "type": "article"
  },
  {
    "title": "A survey of music similarity and recommendation from music context data",
    "doi": "https://doi.org/10.1145/2542205.2542206",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Peter Knees; Markus Schedl",
    "corresponding_authors": "",
    "abstract": "In this survey article, we give an overview of methods for music similarity estimation and music recommendation based on music context data. Unlike approaches that rely on music content and have been researched for almost two decades, music-context -based (or contextual ) approaches to music retrieval are a quite recent field of research within music information retrieval (MIR). Contextual data refers to all music-relevant information that is not included in the audio signal itself. In this article, we focus on contextual aspects of music primarily accessible through web technology. We discuss different sources of context-based data for individual music pieces and for music artists. We summarize various approaches for constructing similarity measures based on the collaborative or cultural knowledge incorporated into these data sources. In particular, we identify and review three main types of context-based similarity approaches: text-retrieval-based approaches (relying on web-texts, tags, or lyrics), co-occurrence-based approaches (relying on playlists, page counts, microblogs, or peer-to-peer-networks), and approaches based on user ratings or listening habits. This article elaborates the characteristics of the presented context-based measures and discusses their strengths as well as their weaknesses.",
    "cited_by_count": 134,
    "openalex_id": "https://openalex.org/W2020901563",
    "type": "article"
  },
  {
    "title": "Fixation Prediction through Multimodal Analysis",
    "doi": "https://doi.org/10.1145/2996463",
    "publication_date": "2016-10-25",
    "publication_year": 2016,
    "authors": "Xiongkuo Min; Guangtao Zhai; Ke Gu; Xiaokang Yang",
    "corresponding_authors": "",
    "abstract": "In this article, we propose to predict human eye fixation through incorporating both audio and visual cues. Traditional visual attention models generally make the utmost of stimuli’s visual features, yet they bypass all audio information. In the real world, however, we not only direct our gaze according to visual saliency, but also are attracted by salient audio cues. Psychological experiments show that audio has an influence on visual attention, and subjects tend to be attracted by the sound sources. Therefore, we propose fusing both audio and visual information to predict eye fixation. In our proposed framework, we first localize the moving--sound-generating objects through multimodal analysis and generate an audio attention map. Then, we calculate the spatial and temporal attention maps using the visual modality. Finally, the audio, spatial, and temporal attention maps are fused to generate the final audiovisual saliency map. The proposed method is applicable to scenes containing moving--sound-generating objects. We gather a set of video sequences and collect eye-tracking data under an audiovisual test condition. Experiment results show that we can achieve better eye fixation prediction performance when taking both audio and visual cues into consideration, especially in some typical scenes in which object motion and audio are highly correlated.",
    "cited_by_count": 134,
    "openalex_id": "https://openalex.org/W2533370895",
    "type": "article"
  },
  {
    "title": "Deep Learning for Mobile Multimedia",
    "doi": "https://doi.org/10.1145/3092831",
    "publication_date": "2017-06-28",
    "publication_year": 2017,
    "authors": "Kaoru Ota; Minh Son Dao; Vasileios Mezaris; Francesco G. B. De Natale",
    "corresponding_authors": "",
    "abstract": "Deep Learning (DL) has become a crucial technology for multimedia computing. It offers a powerful instrument to automatically produce high-level abstractions of complex multimedia data, which can be exploited in a number of applications, including object detection and recognition, speech-to- text, media retrieval, multimodal data analysis, and so on. The availability of affordable large-scale parallel processing architectures, and the sharing of effective open-source codes implementing the basic learning algorithms, caused a rapid diffusion of DL methodologies, bringing a number of new technologies and applications that outperform, in most cases, traditional machine learning technologies. In recent years, the possibility of implementing DL technologies on mobile devices has attracted significant attention. Thanks to this technology, portable devices may become smart objects capable of learning and acting. The path toward these exciting future scenarios, however, entangles a number of important research challenges. DL architectures and algorithms are hardly adapted to the storage and computation resources of a mobile device. Therefore, there is a need for new generations of mobile processors and chipsets, small footprint learning and inference algorithms, new models of collaborative and distributed processing, and a number of other fundamental building blocks. This survey reports the state of the art in this exciting research area, looking back to the evolution of neural networks, and arriving to the most recent results in terms of methodologies, technologies, and applications for mobile environments.",
    "cited_by_count": 132,
    "openalex_id": "https://openalex.org/W2724616073",
    "type": "article"
  },
  {
    "title": "Visual query suggestion",
    "doi": "https://doi.org/10.1145/1823746.1823747",
    "publication_date": "2010-08-01",
    "publication_year": 2010,
    "authors": "Zheng-Jun Zha; Linjun Yang; Tao Mei; Meng Wang; Zengfu Wang; Tat‐Seng Chua; Xian‐Sheng Hua",
    "corresponding_authors": "",
    "abstract": "Query suggestion is an effective approach to bridge the Intention Gap between the users' search intents and queries. Most existing search engines are able to automatically suggest a list of textual query terms based on users' current query input, which can be called Textual Query Suggestion. This article proposes a new query suggestion scheme named Visual Query Suggestion (VQS) which is dedicated to image search. VQS provides a more effective query interface to help users to precisely express their search intents by joint text and image suggestions. When a user submits a textual query, VQS first provides a list of suggestions, each containing a keyword and a collection of representative images in a dropdown menu. Once the user selects one of the suggestions, the corresponding keyword will be added to complement the initial query as the new textual query, while the image collection will be used as the visual query to further represent the search intent. VQS then performs image search based on the new textual query using text search techniques, as well as content-based visual retrieval to refine the search results by using the corresponding images as query examples. We compare VQS against three popular image search engines, and show that VQS outperforms these engines in terms of both the quality of query suggestion and the search performance.",
    "cited_by_count": 129,
    "openalex_id": "https://openalex.org/W1966043174",
    "type": "article"
  },
  {
    "title": "Dual-path Convolutional Image-Text Embeddings with Instance Loss",
    "doi": "https://doi.org/10.1145/3383184",
    "publication_date": "2020-05-22",
    "publication_year": 2020,
    "authors": "Zhedong Zheng; Liang Zheng; Michael Garrett; Yi Yang; Mingliang Xu; Yi-Dong Shen",
    "corresponding_authors": "",
    "abstract": "Matching images and sentences demands a fine understanding of both modalities. In this paper, we propose a new system to discriminatively embed the image and text to a shared visual-textual space. In this field, most existing works apply the ranking loss to pull the positive image / text pairs close and push the negative pairs apart from each other. However, directly deploying the ranking loss is hard for network learning, since it starts from the two heterogeneous features to build inter-modal relationship. To address this problem, we propose the instance loss which explicitly considers the intra-modal data distribution. It is based on an unsupervised assumption that each image / text group can be viewed as a class. So the network can learn the fine granularity from every image/text group. The experiment shows that the instance loss offers better weight initialization for the ranking loss, so that more discriminative embeddings can be learned. Besides, existing works usually apply the off-the-shelf features, i.e., word2vec and fixed visual feature. So in a minor contribution, this paper constructs an end-to-end dual-path convolutional network to learn the image and text representations. End-to-end learning allows the system to directly learn from the data and fully utilize the supervision. On two generic retrieval datasets (Flickr30k and MSCOCO), experiments demonstrate that our method yields competitive accuracy compared to state-of-the-art methods. Moreover, in language based person retrieval, we improve the state of the art by a large margin. The code has been made publicly available.",
    "cited_by_count": 128,
    "openalex_id": "https://openalex.org/W3029678209",
    "type": "article"
  },
  {
    "title": "Intercrossed Access Controls for Secure Financial Services on Multimedia Big Data in Cloud Systems",
    "doi": "https://doi.org/10.1145/2978575",
    "publication_date": "2016-09-15",
    "publication_year": 2016,
    "authors": "Yibin Li; Keke Gai; Zhong Ming; Hui Zhao; Meikang Qiu",
    "corresponding_authors": "",
    "abstract": "The dramatically growing demand of Cyber Physical and Social Computing (CPSC) has enabled a variety of novel channels to reach services in the financial industry. Combining cloud systems with multimedia big data is a novel approach for Financial Service Institutions (FSIs) to diversify service offerings in an efficient manner. However, the security issue is still a great issue in which the service availability often conflicts with the security constraints when the service media channels are varied. This paper focuses on this problem and proposes a novel approach using the Semantic-Based Access Control (SBAC) techniques for acquiring secure financial services on multimedia big data in cloud computing. The proposed approach is entitled IntercroSsed Secure Big Multimedia Model (2SBM), which is designed to secure accesses between various media through the multiple cloud platforms. The main algorithms supporting the proposed model include the Ontology-Based Access Recognition (OBAR) Algorithm and the Semantic Information Matching (SIM) Algorithm . We implement an experimental evaluation to prove the correctness and adoptability of our proposed scheme.",
    "cited_by_count": 123,
    "openalex_id": "https://openalex.org/W2520472180",
    "type": "article"
  },
  {
    "title": "Sparse transfer learning for interactive video search reranking",
    "doi": "https://doi.org/10.1145/2240136.2240139",
    "publication_date": "2012-07-01",
    "publication_year": 2012,
    "authors": "Xinmei Tian; Dacheng Tao; Yong Rui",
    "corresponding_authors": "",
    "abstract": "Visual reranking is effective to improve the performance of the text-based video search. However, existing reranking algorithms can only achieve limited improvement because of the well-known semantic gap between low-level visual features and high-level semantic concepts. In this article, we adopt interactive video search reranking to bridge the semantic gap by introducing user's labeling effort. We propose a novel dimension reduction tool, termed sparse transfer learning (STL), to effectively and efficiently encode user's labeling information. STL is particularly designed for interactive video search reranking. Technically, it (a) considers the pair-wise discriminative information to maximally separate labeled query relevant samples from labeled query irrelevant ones, (b) achieves a sparse representation for the subspace to encodes user's intention by applying the elastic net penalty, and (c) propagates user's labeling information from labeled samples to unlabeled samples by using the data distribution knowledge. We conducted extensive experiments on the TRECVID 2005, 2006 and 2007 benchmark datasets and compared STL with popular dimension reduction algorithms. We report superior performance by using the proposed STL-based interactive video search reranking.",
    "cited_by_count": 121,
    "openalex_id": "https://openalex.org/W1984907405",
    "type": "article"
  },
  {
    "title": "Inception U-Net Architecture for Semantic Segmentation to Identify Nuclei in Microscopy Cell Images",
    "doi": "https://doi.org/10.1145/3376922",
    "publication_date": "2020-02-17",
    "publication_year": 2020,
    "authors": "Narinder Singh Punn; Sonali Agarwal",
    "corresponding_authors": "",
    "abstract": "With the increasing applications of deep learning in biomedical image analysis, in this article we introduce an inception U-Net architecture for automating nuclei detection in microscopy cell images of varying size and modality to help unlock faster cures, inspired from Kaggle Data Science Bowl Challenge 2018 (KDSB18). This study follows from the fact that most of the analysis requires nuclei detection as the starting phase for getting an insight into the underlying biological process and further diagnosis. The proposed architecture consists of a switch normalization layer, convolution layers, and inception layers (concatenated 1x1, 3x3, and 5x5 convolution and the hybrid of a max and Hartley spectral pooling layer) connected in the U-Net fashion for generating the image masks. This article also illustrates the model perception of image masks using activation maximization and filter map visualization techniques. A novel objective function segmentation loss is proposed based on the binary cross entropy, dice coefficient, and intersection over union loss functions. The intersection over union score, loss value, and pixel accuracy metrics evaluate the model over the KDSB18 dataset. The proposed inception U-Net architecture exhibits quite significant results as compared to the original U-Net and recent U-Net++ architecture.",
    "cited_by_count": 119,
    "openalex_id": "https://openalex.org/W3014304846",
    "type": "article"
  },
  {
    "title": "QoE-Driven Rate Adaptation Heuristic for Fair Adaptive Video Streaming",
    "doi": "https://doi.org/10.1145/2818361",
    "publication_date": "2015-10-20",
    "publication_year": 2015,
    "authors": "Stefano Petrangeli; Jeroen Famaey; Maxim Claeys; Steven Latré; Filip De Turck",
    "corresponding_authors": "",
    "abstract": "HTTP Adaptive Streaming (HAS) is quickly becoming the de facto standard for video streaming services. In HAS, each video is temporally segmented and stored in different quality levels. Rate adaptation heuristics, deployed at the video player, allow the most appropriate level to be dynamically requested, based on the current network conditions. It has been shown that today’s heuristics underperform when multiple clients consume video at the same time, due to fairness issues among clients. Concretely, this means that different clients negatively influence each other as they compete for shared network resources. In this article, we propose a novel rate adaptation algorithm called FINEAS (Fair In-Network Enhanced Adaptive Streaming), capable of increasing clients’ Quality of Experience (QoE) and achieving fairness in a multiclient setting. A key element of this approach is an in-network system of coordination proxies in charge of facilitating fair resource sharing among clients. The strength of this approach is threefold. First, fairness is achieved without explicit communication among clients and thus no significant overhead is introduced into the network. Second, the system of coordination proxies is transparent to the clients, that is, the clients do not need to be aware of its presence. Third, the HAS principle is maintained, as the in-network components only provide the clients with new information and suggestions, while the rate adaptation decision remains the sole responsibility of the clients themselves. We evaluate this novel approach through simulations, under highly variable bandwidth conditions and in several multiclient scenarios. We show how the proposed approach can improve fairness up to 80% compared to state-of-the-art HAS heuristics in a scenario with three networks, each containing 30 clients streaming video at the same time.",
    "cited_by_count": 118,
    "openalex_id": "https://openalex.org/W2047770478",
    "type": "article"
  },
  {
    "title": "Two decades of internet video streaming",
    "doi": "https://doi.org/10.1145/2505805",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Baochun Li; Zhi Wang; Jiangchuan Liu; Wenwu Zhu",
    "corresponding_authors": "",
    "abstract": "For over two decades, video streaming over the Internet has received a substantial amount of attention from both academia and industry. Starting from the design of transport protocols for streaming video, research interests have later shifted to the peer-to-peer paradigm of designing streaming protocols at the application layer. More recent research has focused on building more practical and scalable systems, using Dynamic Adaptive Streaming over HTTP. In this article, we provide a retrospective view of the research results over the past two decades, with a focus on peer-to-peer streaming protocols and the effects of cloud computing and social media.",
    "cited_by_count": 117,
    "openalex_id": "https://openalex.org/W2004759491",
    "type": "article"
  },
  {
    "title": "Learning from Collective Intelligence",
    "doi": "https://doi.org/10.1145/2978656",
    "publication_date": "2016-11-02",
    "publication_year": 2016,
    "authors": "Hanwang Zhang; Xindi Shang; Huanbo Luan; Meng Wang; Tat‐Seng Chua",
    "corresponding_authors": "",
    "abstract": "Feature representation for visual content is the key to the progress of many fundamental applications such as annotation and cross-modal retrieval. Although recent advances in deep feature learning offer a promising route towards these tasks, they are limited in application domains where high-quality and large-scale training data are expensive to obtain. In this article, we propose a novel deep feature learning paradigm based on social collective intelligence, which can be acquired from the inexhaustible social multimedia content on the Web, in particular, largely social images and tags. Differing from existing feature learning approaches that rely on high-quality image-label supervision, our weak supervision is acquired by mining the visual-semantic embeddings from noisy, sparse, and diverse social image collections. The resultant image-word embedding space can be used to (1) fine-tune deep visual models for low-level feature extractions and (2) seek sparse representations as high-level cross-modal features for both image and text. We offer an easy-to-use implementation for the proposed paradigm, which is fast and compatible with any state-of-the-art deep architectures. Extensive experiments on several benchmarks demonstrate that the cross-modal features learned by our paradigm significantly outperforms others in various applications such as content-based retrieval, classification, and image captioning.",
    "cited_by_count": 117,
    "openalex_id": "https://openalex.org/W2546529582",
    "type": "article"
  },
  {
    "title": "Chinese Image Captioning via Fuzzy Attention-based DenseNet-BiLSTM",
    "doi": "https://doi.org/10.1145/3422668",
    "publication_date": "2021-01-31",
    "publication_year": 2021,
    "authors": "Huimin Lu; Rui Yang; Zhenrong Deng; Yonglin Zhang; Guangwei Gao; Rushi Lan",
    "corresponding_authors": "",
    "abstract": "Chinese image description generation tasks usually have some challenges, such as single-feature extraction, lack of global information, and lack of detailed description of the image content. To address these limitations, we propose a fuzzy attention-based DenseNet-BiLSTM Chinese image captioning method in this article. In the proposed method, we first improve the densely connected network to extract features of the image at different scales and to enhance the model’s ability to capture the weak features. At the same time, a bidirectional LSTM is used as the decoder to enhance the use of context information. The introduction of an improved fuzzy attention mechanism effectively improves the problem of correspondence between image features and contextual information. We conduct experiments on the AI Challenger dataset to evaluate the performance of the model. The results show that compared with other models, our proposed model achieves higher scores in objective quantitative evaluation indicators, including BLEU <?TeX $@1$?> , BLEU <?TeX $@4$?> , METEOR, ROUGEl, and CIDEr. The generated description sentence can accurately express the image content.",
    "cited_by_count": 115,
    "openalex_id": "https://openalex.org/W3148223059",
    "type": "article"
  },
  {
    "title": "A Fast Defogging Image Recognition Algorithm Based on Bilateral Hybrid Filtering",
    "doi": "https://doi.org/10.1145/3391297",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "Wei Liang; Jing Long; Kuan‐Ching Li; Jianbo Xu; Nanjun Ma; Xia Lei",
    "corresponding_authors": "",
    "abstract": "With the rapid advancement of video and image processing technologies in the Internet of Things, it is urgent to address the issues in real-time performance, clarity, and reliability of image recognition technology for a monitoring system in foggy weather conditions. In this work, a fast defogging image recognition algorithm is proposed based on bilateral hybrid filtering. First, the mathematical model based on bilateral hybrid filtering is established. The dark channel is used for filtering and denoising the defogging image. Next, a bilateral hybrid filtering method is proposed by using a combination of guided filtering and median filtering, as it can effectively improve the robustness and transmittance of defogging images. On this basis, the proposed algorithm dramatically decreases the computation complexity of defogging image recognition and reduces the image execution time. Experimental results show that the defogging effect and speed are promising, with the image recognition rate reaching to 98.8% after defogging.",
    "cited_by_count": 112,
    "openalex_id": "https://openalex.org/W3041454767",
    "type": "article"
  },
  {
    "title": "Mobile Device-to-Device Video Distribution",
    "doi": "https://doi.org/10.1145/2886776",
    "publication_date": "2016-03-08",
    "publication_year": 2016,
    "authors": "Liang Zhou",
    "corresponding_authors": "Liang Zhou",
    "abstract": "As video traffic has dominated the data flow of smartphones, traditional cellular communications face substantial transmission challenges. In this work, we study mobile device-to-device (D2D) video distribution that leverages the storage and communication capacities of smartphones. In such a mobile distributed framework, D2D communication represents an opportunistic process to selectively store and transmit local videos to meet the future demand of others. The performance is measured by the service time, which denotes the elapsed period for fulfilling the demand, and the corresponding implementation of each device depends on the video’s demand, availability, and size. The main contributions of this work lie in (1) considering the impact of video size in a practical mobile D2D video distribution scenario and proposing a general global estimation of the video distribution based on limited and local observations; (2) designing a purely distributed D2D video distribution scheme without the monitoring of any central controller; and (3) providing a practical implementation of the scheme, which does not need to know the video availability, user demand, and device mobility. Numerical results have demonstrated the efficiency and robustness of the proposed scheme.",
    "cited_by_count": 108,
    "openalex_id": "https://openalex.org/W2307856135",
    "type": "article"
  },
  {
    "title": "Deep Cross-Modal Correlation Learning for Audio and Lyrics in Music Retrieval",
    "doi": "https://doi.org/10.1145/3281746",
    "publication_date": "2019-02-13",
    "publication_year": 2019,
    "authors": "Yi Yu; Suhua Tang; Francisco Raposo; Lei Chen",
    "corresponding_authors": "",
    "abstract": "Deep cross-modal learning has successfully demonstrated excellent performance in cross-modal multimedia retrieval, with the aim of learning joint representations between different data modalities. Unfortunately, little research focuses on cross-modal correlation learning where temporal structures of different data modalities, such as audio and lyrics, should be taken into account. Stemming from the characteristic of temporal structures of music in nature, we are motivated to learn the deep sequential correlation between audio and lyrics. In this work, we propose a deep cross-modal correlation learning architecture involving two-branch deep neural networks for audio modality and text modality (lyrics). Data in different modalities are converted to the same canonical space where intermodal canonical correlation analysis is utilized as an objective function to calculate the similarity of temporal structures. This is the first study that uses deep architectures for learning the temporal correlation between audio and lyrics. A pretrained Doc2Vec model followed by fully connected layers is used to represent lyrics. Two significant contributions are made in the audio branch, as follows: (i) We propose an end-to-end network to learn cross-modal correlation between audio and lyrics, where feature extraction and correlation learning are simultaneously performed and joint representation is learned by considering temporal structures. (ii) And, as for feature extraction, we further represent an audio signal by a short sequence of local summaries (VGG16 features) and apply a recurrent neural network to compute a compact feature that better learns the temporal structures of music audio. Experimental results, using audio to retrieve lyrics or using lyrics to retrieve audio, verify the effectiveness of the proposed deep correlation learning architectures in cross-modal music retrieval.",
    "cited_by_count": 106,
    "openalex_id": "https://openalex.org/W2963435138",
    "type": "article"
  },
  {
    "title": "A Survey of Emerging Concepts and Challenges for QoE Management of Multimedia Services",
    "doi": "https://doi.org/10.1145/3176648",
    "publication_date": "2018-04-30",
    "publication_year": 2018,
    "authors": "Lea Skorin‐Kapov; Martı́n Varela; Tobias Hoßfeld; Kuan-Ta Chen",
    "corresponding_authors": "",
    "abstract": "Quality of Experience (QoE) has received much attention over the past years and has become a prominent issue for delivering services and applications. A significant amount of research has been devoted to understanding, measuring, and modelling QoE for a variety of media services. The next logical step is to actively exploit that accumulated knowledge to improve and manage the quality of multimedia services, while at the same time ensuring efficient and cost-effective network operations. Moreover, with many different players involved in the end-to-end service delivery chain, identifying the root causes of QoE impairments and finding effective solutions for meeting the end users’ requirements and expectations in terms of service quality is a challenging and complex problem. In this article, we survey state-of-the-art findings and present emerging concepts and challenges related to managing QoE for networked multimedia services. Going beyond a number of previously published survey articles addressing the topic of QoE management, we address QoE management in the context of ongoing developments, such as the move to softwarized networks, the exploitation of big data analytics and machine learning, and the steady rise of new and immersive services (e.g., augmented and virtual reality). We address the implications of such paradigm shifts in terms of new approaches in QoE modeling and the need for novel QoE monitoring and management infrastructures.",
    "cited_by_count": 105,
    "openalex_id": "https://openalex.org/W2804559716",
    "type": "article"
  },
  {
    "title": "Exploring Deep Learning for View-Based 3D Model Retrieval",
    "doi": "https://doi.org/10.1145/3377876",
    "publication_date": "2020-02-17",
    "publication_year": 2020,
    "authors": "Zan Gao; Yin‐Ming Li; Shaohua Wan",
    "corresponding_authors": "",
    "abstract": "In recent years, view-based 3D model retrieval has become one of the research focuses in the field of computer vision and machine learning. In fact, the 3D model retrieval algorithm consists of feature extraction and similarity measurement, and the robust features play a decisive role in the similarity measurement. Although deep learning has achieved comprehensive success in the field of computer vision, deep learning features are used for 3D model retrieval only in a small number of works. To the best of our knowledge, there is no benchmark to evaluate these deep learning features. To tackle this problem, in this work we systematically evaluate the performance of deep learning features in view-based 3D model retrieval on four popular datasets (ETH, NTU60, PSB, and MVRED) by different kinds of similarity measure methods. In detail, the performance of hand-crafted features and deep learning features are compared, and then the robustness of deep learning features is assessed. Finally, the difference between single-view deep learning features and multi-view deep learning features is also evaluated. By quantitatively analyzing the performances on different datasets, it is clear that these deep learning features can consistently outperform all of the hand-crafted features, and they are also more robust than the hand-crafted features when different degrees of noise are added into the image. The exploration of latent relationships among different views in multi-view deep learning network architectures shows that the performance of multi-view deep learning outperforms that of single-view deep learning features with low computational complexity.",
    "cited_by_count": 103,
    "openalex_id": "https://openalex.org/W3009238340",
    "type": "article"
  },
  {
    "title": "Paying More Attention to Saliency",
    "doi": "https://doi.org/10.1145/3177745",
    "publication_date": "2018-04-25",
    "publication_year": 2018,
    "authors": "Marcella Cornia; Lorenzo Baraldi; Giuseppe Serra; Rita Cucchiara",
    "corresponding_authors": "",
    "abstract": "Image captioning has been recently gaining a lot of attention thanks to the impressive achievements shown by deep captioning architectures, which combine Convolutional Neural Networks to extract image representations and Recurrent Neural Networks to generate the corresponding captions. At the same time, a significant research effort has been dedicated to the development of saliency prediction models, which can predict human eye fixations. Even though saliency information could be useful to condition an image captioning architecture, by providing an indication of what is salient and what is not, research is still struggling to incorporate these two techniques. In this work, we propose an image captioning approach in which a generative recurrent neural network can focus on different parts of the input image during the generation of the caption, by exploiting the conditioning given by a saliency prediction model on which parts of the image are salient and which are contextual. We show, through extensive quantitative and qualitative experiments on large-scale datasets, that our model achieves superior performance with respect to captioning baselines with and without saliency and to different state-of-the-art approaches combining saliency and captioning.",
    "cited_by_count": 100,
    "openalex_id": "https://openalex.org/W2962982762",
    "type": "article"
  },
  {
    "title": "From Theory to Practice",
    "doi": "https://doi.org/10.1145/3336497",
    "publication_date": "2019-04-30",
    "publication_year": 2019,
    "authors": "Kevin Spiteri; Ramesh K. Sitaraman; Daniel Sparacio",
    "corresponding_authors": "",
    "abstract": "Modern video streaming uses adaptive bitrate (ABR) algorithms that run inside video players and continually adjust the quality (i.e., bitrate) of the video segments that are downloaded and rendered to the user. To maximize the quality-of-experience (QoE) of the user, ABR algorithms must stream at a high bitrate with low rebuffering and low bitrate oscillations. Further, a good ABR algorithm is responsive to user and network events and can be used in demanding scenarios such as low-latency live streaming. Recent research papers provide an abundance of ABR algorithms but fall short on many of the above real-world requirements. We develop Sabre, an open-source publicly available simulation tool that enables fast and accurate simulation of adaptive streaming environments. We empirically validated Sabre to show that it accurately simulates real-world environments. We used Sabre to design and evaluate BOLA-E and DYNAMIC, two novel ABR algorithms. We also developed a FAST SWITCHING algorithm that can replace segments that have already been downloaded with higher-bitrate (thus, higher-quality) segments. The new algorithms provide higher QoE to the user in terms of higher bitrate, fewer rebuffers, and lesser bitrate oscillations. In addition, these algorithms react faster to user events such as startup and seek, and they respond more quickly to network events such as improvements in throughput. Further, they perform very well for live streams that require low latency, a challenging scenario for ABR algorithms. Overall, our algorithms offer superior video QoE and responsiveness for real-life adaptive video streaming, in comparison to the state-of-the-art. Importantly, all three algorithms presented in this article are now part of the official DASH reference player dash.js and are being used by video providers in production environments. While our evaluation and implementation are focused on the DASH environment, our algorithms are equally applicable to other adaptive streaming formats such as Apple HLS.",
    "cited_by_count": 96,
    "openalex_id": "https://openalex.org/W2963699234",
    "type": "article"
  },
  {
    "title": "Securing Multimedia by Using DNA-Based Encryption in the Cloud Computing Environment",
    "doi": "https://doi.org/10.1145/3392665",
    "publication_date": "2020-10-31",
    "publication_year": 2020,
    "authors": "Suyel Namasudra; Rupak Chakraborty; Abhishek Majumder; Nageswara Rao Moparthi",
    "corresponding_authors": "",
    "abstract": "Today, the size of a multimedia file is increasing day by day from gigabytes to terabytes or even petabytes, mainly because of the evolution of a large amount of real-time data. As most of the multimedia files are transmitted through the internet, hackers and attackers try to access the users’ personal and confidential data without any authorization. Thus, maintaining a strong security technique has become a significant concerned to protect the personal information. Deoxyribonucleic Acid (DNA) computing is an advanced field for improving security, which is based on the biological concept of DNA. A novel DNA-based encryption scheme is proposed in this article for protecting multimedia files in the cloud computing environment. Here, a 1024-bit secret key is generated based on DNA computing and the user's attributes and password to encrypt any multimedia file. To generate the secret key, the decimal encoding rule, American Standard Code for Information Interchange value, DNA reference key, and complementary rule are used, which enable the system to protect the multimedia file against many security attacks. Experimental results, as well as theoretical analyses, show the efficiency of the proposed scheme over some well-known existing schemes.",
    "cited_by_count": 96,
    "openalex_id": "https://openalex.org/W3113312121",
    "type": "article"
  },
  {
    "title": "Enhancing Person Re-identification in a Self-Trained Subspace",
    "doi": "https://doi.org/10.1145/3089249",
    "publication_date": "2017-06-28",
    "publication_year": 2017,
    "authors": "Xun Yang; Meng Wang; Richang Hong; Qi Tian; Yong Rui",
    "corresponding_authors": "",
    "abstract": "Despite the promising progress made in recent years, person re-identification (re-ID) remains a challenging task due to the complex variations in human appearances from different camera views. For this challenging problem, a large variety of algorithms have been developed in the fully supervised setting, requiring access to a large amount of labeled training data. However, the main bottleneck for fully supervised re-ID is the limited availability of labeled training samples. To address this problem, we propose a self-trained subspace learning paradigm for person re-ID that effectively utilizes both labeled and unlabeled data to learn a discriminative subspace where person images across disjoint camera views can be easily matched. The proposed approach first constructs pseudo-pairwise relationships among unlabeled persons using the k-nearest neighbors algorithm. Then, with the pseudo-pairwise relationships, the unlabeled samples can be easily combined with the labeled samples to learn a discriminative projection by solving an eigenvalue problem. In addition, we refine the pseudo-pairwise relationships iteratively, which further improves learning performance. A multi-kernel embedding strategy is also incorporated into the proposed approach to cope with the non-linearity in a person’s appearance and explore the complementation of multiple kernels. In this way, the performance of person re-ID can be greatly enhanced when training data are insufficient. Experimental results on six widely used datasets demonstrate the effectiveness of our approach, and its performance can be comparable to the reported results of most state-of-the-art fully supervised methods while using much fewer labeled data.",
    "cited_by_count": 95,
    "openalex_id": "https://openalex.org/W2608045553",
    "type": "article"
  },
  {
    "title": "Performance Modelling and Analysis of Software-Defined Networking under Bursty Multimedia Traffic",
    "doi": "https://doi.org/10.1145/2983637",
    "publication_date": "2016-09-21",
    "publication_year": 2016,
    "authors": "Wang Miao; Geyong Min; Yulei Wu; Haozhe Wang; Jia Hu",
    "corresponding_authors": "",
    "abstract": "Software-Defined Networking (SDN) is an emerging architecture for the next-generation Internet, providing unprecedented network programmability to handle the explosive growth of big data driven by the popularisation of smart mobile devices and the pervasiveness of content-rich multimedia applications. In order to quantitatively investigate the performance characteristics of SDN networks, several research efforts from both simulation experiments and analytical modelling have been reported in the current literature. Among those studies, analytical modelling has demonstrated its superiority in terms of cost-effectiveness in the evaluation of large-scale networks. However, for analytical tractability and simplification, existing analytical models are derived based on the unrealistic assumptions that the network traffic follows the Poisson process, which is suitable to model nonbursty text data, and the data plane of SDN is modelled by one simplified Single-Server Single-Queue (SSSQ) system. Recent measurement studies have shown that, due to the features of heavy volume and high velocity, the multimedia big data generated by real-world multimedia applications reveals the bursty and correlated nature in the network transmission. With the aim of capturing such features of realistic traffic patterns and obtaining a comprehensive and deeper understanding of the performance behaviour of SDN networks, this article presents a new analytical model to investigate the performance of SDN in the presence of the bursty and correlated arrivals modelled by the Markov Modulated Poisson Process (MMPP). The Quality-of-Service performance metrics in terms of the average latency and average network throughput of the SDN networks are derived based on the developed analytical model. To consider a realistic multiqueue system of forwarding elements, a Priority-Queue (PQ) system is adopted to model the SDN data plane. To address the challenging problem of obtaining the key performance metrics, for example, queue-length distribution of a PQ system with a given service capacity, a versatile methodology extending the Empty Buffer Approximation (EBA) method is proposed to facilitate the decomposition of such a PQ system to two SSSQ systems. The validity of the proposed model is demonstrated through extensive simulation experiments. To illustrate its application, the developed model is then utilised to study the strategy of the network configuration and resource allocation in SDN networks.",
    "cited_by_count": 92,
    "openalex_id": "https://openalex.org/W2522453496",
    "type": "article"
  },
  {
    "title": "Saliency Detection on Light Field",
    "doi": "https://doi.org/10.1145/3107956",
    "publication_date": "2017-07-27",
    "publication_year": 2017,
    "authors": "Jun Zhang; Meng Wang; Liang Lin; Xun Yang; Jun Gao; Yong Rui",
    "corresponding_authors": "",
    "abstract": "Saliency detection has recently received increasing research interest on using high-dimensional datasets beyond two-dimensional images. Despite the many available capturing devices and algorithms, there still exists a wide spectrum of challenges that need to be addressed to achieve accurate saliency detection. Inspired by the success of the light-field technique, in this article, we propose a new computational scheme to detect salient regions by integrating multiple visual cues from light-field images. First, saliency prior maps are generated from several light-field features based on superpixel-level intra-cue distinctiveness, such as color, depth, and flow inherited from different focal planes and multiple viewpoints. Then, we introduce the location prior to enhance the saliency maps. These maps will finally be merged into a single map using a random-search-based weighting strategy. Besides, we refine the object details by employing a two-stage saliency refinement to obtain the final saliency map. In addition, we present a more challenging benchmark dataset for light-field saliency analysis, named HFUT-Lytro , which consists of 255 light fields with a range from 53 to 64 images generated from each light-field image, therein spanning multiple occurrences of saliency detection challenges such as occlusions, cluttered background, and appearance changes. Experimental results show that our approach can achieve 0.6--6.7% relative improvements over state-of-the-art methods in terms of the F-measure and Precision metrics, which demonstrates the effectiveness of the proposed approach.",
    "cited_by_count": 92,
    "openalex_id": "https://openalex.org/W2741745116",
    "type": "article"
  },
  {
    "title": "Automatic Generation of Visual-Textual Presentation Layout",
    "doi": "https://doi.org/10.1145/2818709",
    "publication_date": "2016-02-09",
    "publication_year": 2016,
    "authors": "Xuyong Yang; Tao Mei; Yingqing Xu; Yong Rui; Shipeng Li",
    "corresponding_authors": "",
    "abstract": "Visual-textual presentation layout (e.g., digital magazine cover, poster, Power Point slides, and any other rich media), which combines beautiful image and overlaid readable texts, can result in an eye candy touch to attract users’ attention. The designing of visual-textual presentation layout is therefore becoming ubiquitous in both commercially printed publications and online digital magazines. However, handcrafting aesthetically compelling layouts still remains challenging for many small businesses and amateur users. This article presents a system to automatically generate visual-textual presentation layouts by investigating a set of aesthetic design principles, through which an average user can easily create visually appealing layouts. The system is attributed with a set of topic-dependent layout templates and a computational framework integrating high-level aesthetic principles (in a top-down manner) and low-level image features (in a bottom-up manner). The layout templates, designed with prior knowledge from domain experts, define spatial layouts, semantic colors, harmonic color models, and font emotion and size constraints. We formulate the typography as an energy optimization problem by minimizing the cost of text intrusion, the utility of visual space, and the mismatch of information importance in perception and semantics, constrained by the automatically selected template and further preserving color harmonization. We demonstrate that our designs achieve the best reading experience compared with the reimplementation of parts of existing state-of-the-art designs through a series of user studies.",
    "cited_by_count": 91,
    "openalex_id": "https://openalex.org/W2271551547",
    "type": "article"
  },
  {
    "title": "Attention-Based Modality-Gated Networks for Image-Text Sentiment Analysis",
    "doi": "https://doi.org/10.1145/3388861",
    "publication_date": "2020-07-05",
    "publication_year": 2020,
    "authors": "Feiran Huang; Kaimin Wei; Jian Weng; Zhoujun Li",
    "corresponding_authors": "",
    "abstract": "Sentiment analysis of social multimedia data has attracted extensive research interest and has been applied to many tasks, such as election prediction and products evaluation. Sentiment analysis of one modality (e.g., text or image) has been broadly studied. However, not much attention has been paid to the sentiment analysis of multimodal data. Different modalities usually have information that is complementary. Thus, it is necessary to learn the overall sentiment by combining the visual content with text description. In this article, we propose a novel method—Attention-Based Modality-Gated Networks (AMGN)—to exploit the correlation between the modalities of images and texts and extract the discriminative features for multimodal sentiment analysis. Specifically, a visual-semantic attention model is proposed to learn attended visual features for each word. To effectively combine the sentiment information on the two modalities of image and text, a modality-gated LSTM is proposed to learn the multimodal features by adaptively selecting the modality that presents stronger sentiment information. Then a semantic self-attention model is proposed to automatically focus on the discriminative features for sentiment classification. Extensive experiments have been conducted on both manually annotated and machine weakly labeled datasets. The results demonstrate the superiority of our approach through comparison with state-of-the-art models.",
    "cited_by_count": 89,
    "openalex_id": "https://openalex.org/W3041730883",
    "type": "article"
  },
  {
    "title": "Convolutional Attention Networks for Scene Text Recognition",
    "doi": "https://doi.org/10.1145/3231737",
    "publication_date": "2019-01-24",
    "publication_year": 2019,
    "authors": "Hongtao Xie; Shancheng Fang; Zheng-Jun Zha; Yating Yang; Yan Li; Yongdong Zhang",
    "corresponding_authors": "",
    "abstract": "In this article, we present Convoluitional Attention Networks (CAN) for unconstrained scene text recognition. Recent dominant approaches for scene text recognition are mainly based on Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN), where the CNN encodes images and the RNN generates character sequences. Our CAN is different from these methods; our CAN is completely built on CNN and includes an attention mechanism. The distinctive characteristics of our method include (i) CAN follows encoder-decoder architecture, in which the encoder is a deep two-dimensional CNN and the decoder is a one-dimensional CNN; (ii) the attention mechanism is applied in every convolutional layer of the decoder, and we propose a novel spatial attention method using average pooling; and (iii) position embeddings are equipped in both a spatial encoder and a sequence decoder to give our networks a sense of location. We conduct experiments on standard datasets for scene text recognition, including Street View Text , IIIT5K, and ICDAR datasets. The experimental results validate the effectiveness of different components and show that our convolutional-based method achieves state-of-the-art or competitive performance over prior works, even without the use of RNN.",
    "cited_by_count": 84,
    "openalex_id": "https://openalex.org/W2997351497",
    "type": "article"
  },
  {
    "title": "TripRes",
    "doi": "https://doi.org/10.1145/3401979",
    "publication_date": "2021-04-21",
    "publication_year": 2021,
    "authors": "Xiaolong Xu; Zijie Fang; Lianyong Qi; Xuyun Zhang; Qiang He; Xiaokang Zhou",
    "corresponding_authors": "",
    "abstract": "The Internet of Vehicles (IoV) connects vehicles, roadside units (RSUs) and other intelligent objects, enabling data sharing among them, thereby improving the efficiency of urban traffic and safety. Currently, collections of multimedia content, generated by multimedia surveillance equipment, vehicles, and so on, are transmitted to edge servers for implementation, because edge computing is a formidable paradigm for accommodating multimedia services with low-latency resource provisioning. However, the uneven or discrete distribution of the traffic flow covered by edge servers negatively affects the service performance (e.g., overload and underload) of edge servers in multimedia IoV systems. Therefore, how to accurately schedule and dynamically reserve proper numbers of resources for multimedia services in edge servers is still challenging. To address this challenge, a traffic flow prediction driven resource reservation method, called TripRes, is developed in this article. Specifically, the city map is divided into different regions, and the edge servers in a region are treated as a “big edge server” to simplify the complex distribution of edge servers. Then, future traffic flows are predicted using the deep spatiotemporal residual network (ST-ResNet), and future traffic flows are used to estimate the amount of multimedia services each region needs to offload to the edge servers. With the number of services to be offloaded in each region, their offloading destinations are determined through latency-sensitive transmission path selection. Finally, the performance of TripRes is evaluated using real-world big data with over 100M multimedia surveillance records from RSUs in Nanjing China.",
    "cited_by_count": 80,
    "openalex_id": "https://openalex.org/W3157990341",
    "type": "article"
  },
  {
    "title": "A Survey on Healthcare Data: A Security Perspective",
    "doi": "https://doi.org/10.1145/3422816",
    "publication_date": "2021-05-18",
    "publication_year": 2021,
    "authors": "Amit Kumar Singh; Ashima Anand; Zhihan Lv; Hoon Ko; Anand Mohan",
    "corresponding_authors": "",
    "abstract": "With the remarkable development of internet technologies, the popularity of smart healthcare has regularly come to the fore. Smart healthcare uses advanced technologies to transform the traditional medical system in an all-round way, making healthcare more efficient, more convenient, and more personalized. Unfortunately, medical data security is a serious issue in the smart healthcare systems. It becomes a fundamental challenge that requires the development of efficient innovative strategies towards fulfilling the healthcare needs and supporting secure healthcare transfer and delivery. This article provides a comprehensive survey on state-of-the-art techniques for health data security and their new trends for solving challenges in real-world applications. We survey the various notable cryptography, biometrics, watermarking, and blockchain-based security techniques for healthcare applications. A comparative analysis is also performed to identify the contribution of reviewed techniques in terms of their objective, methodology, type of medical data, important features, and limitations. At the end, we discuss the open issues and research directions to explore the promising areas for future research.",
    "cited_by_count": 80,
    "openalex_id": "https://openalex.org/W3162298008",
    "type": "article"
  },
  {
    "title": "Spherical Convolution Empowered Viewport Prediction in 360 Video Multicast with Limited FoV Feedback",
    "doi": "https://doi.org/10.1145/3511603",
    "publication_date": "2022-03-12",
    "publication_year": 2022,
    "authors": "Jie Li; Ling Han; Chong Zhang; Qiyue Li; Zhi Liu",
    "corresponding_authors": "",
    "abstract": "Field of view (FoV) prediction is critical in 360-degree video multicast, which is a key component of the emerging virtual reality and augmented reality applications. Most of the current prediction methods combining saliency detection and FoV information neither take into account that the distortion of projected 360-degree videos can invalidate the weight sharing of traditional convolutional networks nor do they adequately consider the difficulty of obtaining complete multi-user FoV information, which degrades the prediction performance. This article proposes a spherical convolution-empowered FoV prediction method, which is a multi-source prediction framework combining salient features extracted from 360-degree video with limited FoV feedback information. A spherical convolutional neural network is used instead of a traditional two-dimensional convolutional neural network to eliminate the problem of weight sharing failure caused by video projection distortion. Specifically, salient spatial-temporal features are extracted through a spherical convolution-based saliency detection model, after which the limited feedback FoV information is represented as a time-series model based on a spherical convolution-empowered gated recurrent unit network. Finally, the extracted salient video features are combined to predict future user FoVs. The experimental results show that the performance of the proposed method is better than other prediction methods.",
    "cited_by_count": 73,
    "openalex_id": "https://openalex.org/W4221086736",
    "type": "article"
  },
  {
    "title": "Towards Integrating Image Encryption with Compression: A Survey",
    "doi": "https://doi.org/10.1145/3498342",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Kedar Nath Singh; Amit Kumar Singh",
    "corresponding_authors": "",
    "abstract": "As digital images are consistently generated and transmitted online, the unauthorized utilization of these images is an increasing concern that has a significant impact on both security and privacy issues; additionally, the representation of digital images requires a large amount of data. In recent years, an image compression scheme has been widely considered; such a scheme saves on hardware storage space and lowers both the transmission time and bandwidth demand for various potential applications. In this article, we review the various approaches taken to consider joint encryption and compression, assessing both their merits and their limitations. In addition to the survey, we also briefly introduce the most interesting and most often utilized applications of image encryption and evaluation metrics, providing an overview of the various kinds of image encryption schemes available. The contribution made by these approaches is then summarized and compared, offering a consideration of the different technical perspectives. Lastly, we highlight the recent challenges and some potential research directions that could fill the gaps in these domains for both researchers and developers.",
    "cited_by_count": 71,
    "openalex_id": "https://openalex.org/W4214814702",
    "type": "article"
  },
  {
    "title": "Point Cloud Quality Assessment: Dataset Construction and Learning-based No-reference Metric",
    "doi": "https://doi.org/10.1145/3550274",
    "publication_date": "2022-07-28",
    "publication_year": 2022,
    "authors": "Yipeng Liu; Qi Yang; Yiling Xu; Le Yang",
    "corresponding_authors": "",
    "abstract": "Full-reference (FR) point cloud quality assessment (PCQA) has achieved impressive progress in recent years. However, in many cases, obtaining the reference point clouds is difficult, so no-reference (NR) metrics have become a research hotspot. Few researches about NR-PCQA are carried out due to the lack of a large-scale PCQA dataset. In this article, we first build a large-scale PCQA dataset named LS-PCQA, which includes 104 reference point clouds and more than 22,000 distorted samples. In the dataset, each reference point cloud is augmented with 31 types of impairments (e.g., Gaussian noise, contrast distortion, local missing, and compression loss) at 7 distortion levels. Besides, each distorted point cloud is assigned with a pseudo-quality score as its substitute of Mean Opinion Score. Inspired by the hierarchical perception system and considering the intrinsic attributes of point clouds, we propose a NR metric ResSCNN based on sparse convolutional neural network (CNN) to accurately estimate the subjective quality of point clouds. We conduct several experiments to evaluate the performance of the proposed NR metric. The results demonstrate that ResSCNN exhibits the state-of-the-art performance among all the existing NR-PCQA metrics and even outperforms some FR metrics. The dataset presented in this work will be made publicly accessible at https://smt.sjtu.edu.cn . The source code for the proposed ResSCNN can be found at https://github.com/lyp22/ResSCNN .",
    "cited_by_count": 71,
    "openalex_id": "https://openalex.org/W4288084829",
    "type": "article"
  },
  {
    "title": "Compatibility-Aware Web API Recommendation for Mashup Creation via Textual Description Mining",
    "doi": "https://doi.org/10.1145/3417293",
    "publication_date": "2021-01-31",
    "publication_year": 2021,
    "authors": "Lianyong Qi; Houbing Song; Xuyun Zhang; Gautam Srivastava; Xiaolong Xu; Shui Yu",
    "corresponding_authors": "",
    "abstract": "With the ever-increasing prosperity of web Application Programming Interface (API) sharing platforms, it is becoming an economic and efficient way for software developers to design their interested mashups through web API re-use. Generally, a software developer can browse, evaluate, and select his or her preferred web APIs from the API's sharing platforms to create various mashups with rich functionality. The big volume of candidate APIs places a heavy burden on software developers’ API selection decisions. This, in turn, calls for the support of intelligent API recommender systems. However, existing API recommender systems often face two challenges. First, they focus more on the functional accuracy of APIs while neglecting the APIs’ actual compatibility. This then creates incompatible mashups. Second, they often require software developers to input a set of keywords that can accurately describe the expected functions of the mashup to be developed. This second challenge tests partial developers who have little background knowledge in the fields. To tackle the above-mentioned challenges, in this article we propose a compatibility-aware and text description-driven web API recommendation approach (named WAR text ). WAR text guarantees the compatibility among the recommended APIs by utilizing the APIs’ composition records produced by historical mashup creations. Besides, WAR text entitles a software developer to type a simple text document that describes the expected mashup functions as input. Then through textual description mining, WAR text can precisely capture the developers’ functional requirements and then return a set of APIs with the highest compatibility. Finally, through a real-world mashup dataset ProgrammableWeb, we validate the feasibility of our novel approach.",
    "cited_by_count": 68,
    "openalex_id": "https://openalex.org/W3150083517",
    "type": "article"
  },
  {
    "title": "Perceptual Quality Assessment of Low-light Image Enhancement",
    "doi": "https://doi.org/10.1145/3457905",
    "publication_date": "2021-11-12",
    "publication_year": 2021,
    "authors": "Guangtao Zhai; Wei Sun; Xiongkuo Min; Jiantao Zhou",
    "corresponding_authors": "",
    "abstract": "Low-light image enhancement algorithms (LIEA) can light up images captured in dark or back-lighting conditions. However, LIEA may introduce various distortions such as structure damage, color shift, and noise into the enhanced images. Despite various LIEAs proposed in the literature, few efforts have been made to study the quality evaluation of low-light enhancement. In this article, we make one of the first attempts to investigate the quality assessment problem of low-light image enhancement. To facilitate the study of objective image quality assessment (IQA), we first build a large-scale low-light image enhancement quality (LIEQ) database. The LIEQ database includes 1,000 light-enhanced images, which are generated from 100 low-light images using 10 LIEAs. Rather than evaluating the quality of light-enhanced images directly, which is more difficult, we propose to use the multi-exposure fused (MEF) image and stack-based high dynamic range (HDR) image as a reference and evaluate the quality of low-light enhancement following a full-reference (FR) quality assessment routine. We observe that distortions introduced in low-light enhancement are significantly different from distortions considered in traditional image IQA databases that are well-studied, and the current state-of-the-art FR IQA models are also not suitable for evaluating their quality. Therefore, we propose a new FR low-light image enhancement quality assessment (LIEQA) index by evaluating the image quality from four aspects: luminance enhancement, color rendition, noise evaluation, and structure preserving, which have captured the most key aspects of low-light enhancement. Experimental results on the LIEQ database show that the proposed LIEQA index outperforms the state-of-the-art FR IQA models. LIEQA can act as an evaluator for various low-light enhancement algorithms and systems. To the best of our knowledge, this article is the first of its kind comprehensive low-light image enhancement quality assessment study.",
    "cited_by_count": 68,
    "openalex_id": "https://openalex.org/W3212254418",
    "type": "article"
  },
  {
    "title": "Knowledge-aware Multi-modal Adaptive Graph Convolutional Networks for Fake News Detection",
    "doi": "https://doi.org/10.1145/3451215",
    "publication_date": "2021-07-22",
    "publication_year": 2021,
    "authors": "Shengsheng Qian; Jun Hu; Quan Fang; Changsheng Xu",
    "corresponding_authors": "",
    "abstract": "In this article, we focus on fake news detection task and aim to automatically identify the fake news from vast amount of social media posts. To date, many approaches have been proposed to detect fake news, which includes traditional learning methods and deep learning-based models. However, there are three existing challenges: (i) How to represent social media posts effectively, since the post content is various and highly complicated; (ii) how to propose a data-driven method to increase the flexibility of the model to deal with the samples in different contexts and news backgrounds; and (iii) how to fully utilize the additional auxiliary information (the background knowledge and multi-modal information) of posts for better representation learning. To tackle the above challenges, we propose a novel Knowledge-aware Multi-modal Adaptive Graph Convolutional Networks (KMAGCN) to capture the semantic representations by jointly modeling the textual information, knowledge concepts, and visual information into a unified framework for fake news detection. We model posts as graphs and use a knowledge-aware multi-modal adaptive graph learning principal for the effective feature learning. Compared with existing methods, the proposed KMAGCN addresses challenges from three aspects: (1) It models posts as graphs to capture the non-consecutive and long-range semantic relations; (2) it proposes a novel adaptive graph convolutional network to handle the variability of graph data; and (3) it leverages textual information, knowledge concepts and visual information jointly for model learning. We have conducted extensive experiments on three public real-world datasets and superior results demonstrate the effectiveness of KMAGCN compared with other state-of-the-art algorithms.",
    "cited_by_count": 65,
    "openalex_id": "https://openalex.org/W3185106195",
    "type": "article"
  },
  {
    "title": "Local Correlation Ensemble with GCN Based on Attention Features for Cross-domain Person Re-ID",
    "doi": "https://doi.org/10.1145/3542820",
    "publication_date": "2022-06-09",
    "publication_year": 2022,
    "authors": "Yue Zhang; Fanghui Zhang; Yi Jin; Yigang Cen; Viacheslav Voronin; Shaohua Wan",
    "corresponding_authors": "",
    "abstract": "Person re-identification (Re-ID) has achieved great success in single-domain. However, it remains a challenging task to adapt a Re-ID model trained on one dataset to another one. Unsupervised domain adaption (UDA) was proposed to migrate a model from a labeled source domain to an unlabeled target domain. The main difference in the cross-domain is different background styles. Although the style transfer approach effectively reduces inter-domain gaps, it ignores the reduction of intra-class differences. Clustering-based pipelines maintain state-of-the-art performance for UDA by learning domain-independent features; however, most existing models do not sufficiently exploit the rich unlabeled samples in target domains due to unsatisfactory clustering. Thus, we propose a novel local correlation ensemble model that focuses on the diversity of intra-class information and the reliability of class centers. Specifically, a pedestrian attention module is proposed to enable the encoder to pay more attention to the person’s features to relieve interference caused by the shared background style. Furthermore, we propose a priority-distance graph convolutional network (PDGCN) module that employs a graph convolutional network network to predict the priority of a node as a class center and then calculates the distance between nodes with high priority values to screen out the class center nodes. Finally, the encoder features (local) and PDGCN features (context-aware) are combined to perform person Re-ID. The results of experiments on the large-scale public Re-ID datasets verified the effectiveness of the proposed method.",
    "cited_by_count": 64,
    "openalex_id": "https://openalex.org/W4281737392",
    "type": "article"
  },
  {
    "title": "An Effective Forest Fire Detection Framework Using Heterogeneous Wireless Multimedia Sensor Networks",
    "doi": "https://doi.org/10.1145/3473037",
    "publication_date": "2022-02-16",
    "publication_year": 2022,
    "authors": "Burak Kizilkaya; Enver Ever; Hakan Yekta Yatbaz; Adnan Yazıcı",
    "corresponding_authors": "",
    "abstract": "With improvements in the area of Internet of Things (IoT), surveillance systems have recently become more accessible. At the same time, optimizing the energy requirements of smart sensors, especially for data transmission, has always been very important and the energy efficiency of IoT systems has been the subject of numerous studies. For environmental monitoring scenarios, it is possible to extract more accurate information using smart multimedia sensors. However, multimedia data transmission is an expensive operation. In this study, a novel hierarchical approach is presented for the detection of forest fires. The proposed framework introduces a new approach in which multimedia and scalar sensors are used hierarchically to minimize the transmission of visual data. A lightweight deep learning model is also developed for devices at the edge of the network to improve detection accuracy and reduce the traffic between the edge devices and the sink. The framework is evaluated using a real testbed, network simulations, and 10-fold cross-validation in terms of energy efficiency and detection accuracy. Based on the results of our experiments, the validation accuracy of the proposed system is 98.28%, and the energy saving is 29.94%. The proposed deep learning model’s validation accuracy is very close to the accuracy of the best performing architectures when the existing studies and lightweight architectures are considered. In terms of suitability for edge computing, the proposed approach is superior to the existing ones with reduced computational requirements and model size.",
    "cited_by_count": 63,
    "openalex_id": "https://openalex.org/W4283380979",
    "type": "article"
  },
  {
    "title": "A Deep Multi-level Attentive Network for Multimodal Sentiment Analysis",
    "doi": "https://doi.org/10.1145/3517139",
    "publication_date": "2022-03-16",
    "publication_year": 2022,
    "authors": "Ashima Yadav; Dinesh Kumar Vishwakarma",
    "corresponding_authors": "",
    "abstract": "Multimodal sentiment analysis has attracted increasing attention with broad application prospects. The existing methods focuses on single modality, which fails to capture the social media content for multiple modalities. Moreover, in multi-modal learning, most of the works have focused on simply combining the two modalities, without exploring the complicated correlations between them. This resulted in dissatisfying performance for multimodal sentiment classification. Motivated by the status quo, we propose a Deep Multi-Level Attentive network, which exploits the correlation between image and text modalities to improve multimodal learning. Specifically, we generate the bi-attentive visual map along the spatial and channel dimensions to magnify CNNs representation power. Then we model the correlation between the image regions and semantics of the word by extracting the textual features related to the bi-attentive visual features by applying semantic attention. Finally, self-attention is employed to automatically fetch the sentiment-rich multimodal features for the classification. We conduct extensive evaluations on four real-world datasets, namely, MVSA-Single, MVSA-Multiple, Flickr, and Getty Images, which verifies the superiority of our method.",
    "cited_by_count": 61,
    "openalex_id": "https://openalex.org/W3111911126",
    "type": "article"
  },
  {
    "title": "Hierarchical Multi-Attention Transfer for Knowledge Distillation",
    "doi": "https://doi.org/10.1145/3568679",
    "publication_date": "2022-10-20",
    "publication_year": 2022,
    "authors": "Jianping Gou; Liyuan Sun; Baosheng Yu; Shaohua Wan; Dacheng Tao",
    "corresponding_authors": "",
    "abstract": "Knowledge distillation (KD) is a powerful and widely applicable technique for the compression of deep learning models. The main idea of knowledge distillation is to transfer knowledge from a large teacher model to a small student model, where the attention mechanism has been intensively explored in regard to its great flexibility for managing different teacher-student architectures. However, existing attention-based methods usually transfer similar attention knowledge from the intermediate layers of deep neural networks, leaving the hierarchical structure of deep representation learning poorly investigated for knowledge distillation. In this paper, we propose a hierarchical multi-attention transfer framework (HMAT) , where different types of attention are utilized to transfer the knowledge at different levels of deep representation learning for knowledge distillation. Specifically, position-based and channel-based attention knowledge characterize the knowledge from low-level and high-level feature representations, respectively, and activation-based attention knowledge characterize the knowledge from both mid-level and high-level feature representations. Extensive experiments on three popular visual recognition tasks, image classification, image retrieval, and object detection, demonstrate that the proposed hierarchical multi-attention transfer or HMAT significantly outperforms recent state-of-the-art KD methods.",
    "cited_by_count": 59,
    "openalex_id": "https://openalex.org/W4306874801",
    "type": "article"
  },
  {
    "title": "Voice-Face Homogeneity Tells Deepfake",
    "doi": "https://doi.org/10.1145/3625231",
    "publication_date": "2023-09-21",
    "publication_year": 2023,
    "authors": "Harry H. Cheng; Yangyang Guo; Tianyi Wang; Qi Li; Xiaojun Chang; Liqiang Nie",
    "corresponding_authors": "",
    "abstract": "Detecting forgery videos is highly desirable due to the abuse of deepfake. Existing detection approaches contribute to exploring the specific artifacts in deepfake videos and fit well on certain data. However, the growing technique on these artifacts keeps challenging the robustness of traditional deepfake detectors. As a result, the development of these approaches has reached a blockage. In this article, we propose to perform deepfake detection from an unexplored voice-face matching view. Our approach is founded on two supporting points: first, there is a high degree of homogeneity between the voice and face of an individual (i.e., they are highly correlated), and second, deepfake videos often involve mismatched identities between the voice and face due to face-swapping techniques. To this end, we develop a voice-face matching method that measures the matching degree between these two modalities to identify deepfake videos. Nevertheless, training on specific deepfake datasets makes the model overfit certain traits of deepfake algorithms. We instead advocate a method that quickly adapts to untapped forgery, with a pre-training then fine-tuning paradigm. Specifically, we first pre-train the model on a generic audio-visual dataset, followed by the fine-tuning on downstream deepfake data. We conduct extensive experiments over three widely exploited deepfake datasets: DFDC, FakeAVCeleb, and DeepfakeTIMIT. Our method obtains significant performance gains as compared to other state-of-the-art competitors. For instance, our method outperforms the baselines by nearly 2%, achieving an AUC of 86.11% on FakeAVCeleb. It is also worth noting that our method already achieves competitive results when fine-tuned on limited deepfake data.",
    "cited_by_count": 49,
    "openalex_id": "https://openalex.org/W4386928847",
    "type": "article"
  },
  {
    "title": "HCNCT: A Cross-chain Interaction Scheme for the Blockchain-based Metaverse",
    "doi": "https://doi.org/10.1145/3594542",
    "publication_date": "2023-04-25",
    "publication_year": 2023,
    "authors": "Yongjun Ren; Zhiying Lv; Naixue Xiong; Jin Wang",
    "corresponding_authors": "",
    "abstract": "As a new type of digital living space that blends virtual and reality, Metaverse combines many emerging technologies. It provides an immersive experience based on VR technology and stores and protects users’ digital content and digital assets through blockchain technology. However, different virtual environments are often highly heterogeneous in terms of underlying architecture and software implementation technology, which leads to many challenges in scalability and interoperability for blockchains serving the Metaverse. Cross-chain technology is an essential technology to realize the scalability and interoperability of blockchain. However, the current cross-chain technologies all have their own merits and demerits, and there is no cross-chain solution that can be fully applied to any scenario. To this end, in the blockchain-based Metaverse, this article proposes a cross-chain transaction scheme based on improved hash timelock, HCNCT. By combining the notary mechanism, this scheme uses a group of notaries to supervise and participate in cross-chain transactions, effectively solving the problem that malicious users create a large number of time-out transactions to block the transaction channel, which exists in the traditional hash timelock method. Besides, this article uses the verifiable secret sharing method in the notary group, which can effectively prevent the centralization problem of the notary mechanism. Moreover, this article discusses the process of key processing, cross-chain transaction and transaction verification of the scheme, and designs the user credibility evaluation mechanism, which can effectively reduce the occurrence of malicious default of users. Compared with existing solutions, our solution has the advantage of effectively addressing time-out transaction attacks and centralization issues while guaranteeing security. The experiments also verify the effectiveness of the proposed scheme.",
    "cited_by_count": 48,
    "openalex_id": "https://openalex.org/W4366985305",
    "type": "article"
  },
  {
    "title": "LogoDet-3K: A Large-scale Image Dataset for Logo Detection",
    "doi": "https://doi.org/10.1145/3466780",
    "publication_date": "2022-01-27",
    "publication_year": 2022,
    "authors": "Jing Wang; Weiqing Min; Sujuan Hou; Shengnan Ma; Yuanjie Zheng; Shuqiang Jiang",
    "corresponding_authors": "",
    "abstract": "Logo detection has been gaining considerable attention because of its wide range of applications in the multimedia field, such as copyright infringement detection, brand visibility monitoring, and product brand management on social media. In this article, we introduce LogoDet-3K, the largest logo detection dataset with full annotation, which has 3,000 logo categories, about 200,000 manually annotated logo objects, and 158,652 images. LogoDet-3K creates a more challenging benchmark for logo detection, for its higher comprehensive coverage and wider variety in both logo categories and annotated objects compared with existing datasets. We describe the collection and annotation process of our dataset and analyze its scale and diversity in comparison to other datasets for logo detection. We further propose a strong baseline method Logo-Yolo, which incorporates Focal loss and CIoU loss into the basic YOLOv3 framework for large-scale logo detection. It obtains about 4% improvement on the average performance compared with YOLOv3, and greater improvements compared with reported several deep detection models on LogoDet-3K. We perform extensive evaluation on three other existing datasets to further verify on both logo detection and retrieval tasks, and we demonstrate better generalization ability of LogoDet-3K on logo detection and retrieval tasks. The LogoDet-3K dataset is used to promote large-scale logo-related research. The code and LogoDet-3K can be found at https://github.com/Wangjing1551/LogoDet-3K-Dataset.",
    "cited_by_count": 46,
    "openalex_id": "https://openalex.org/W4210271974",
    "type": "article"
  },
  {
    "title": "Automatic Assessment of Depression and Anxiety through Encoding Pupil-wave from HCI in VR Scenes",
    "doi": "https://doi.org/10.1145/3513263",
    "publication_date": "2022-04-30",
    "publication_year": 2022,
    "authors": "Mi Li; Wei Zhang; Bin Hu; Jiaming Kang; Yuqi Wang; Shengfu Lu",
    "corresponding_authors": "",
    "abstract": "At present, there have been many studies on the methods of using the deep learning regression model to assess depression level based on behavioral signals (facial expression, speech, and language); however, the research on the assessment method of anxiety level using deep learning is absent. In this article, pupil-wave, a physiological signal collected by Human Computer Interaction (HCI) that can directly represent the emotional state, is developed to assess the level of depression and anxiety for the first time. In order to distinguish between different depression and anxiety levels, we use the HCI method to induce the participants’ emotional experience through three virtual reality (VR) emotional scenes of joyful, sad, and calm, and construct two differential pupil-waves of joyful and sad with the calm pupil-wave as the baseline. Correspondingly, a dual-channel fusion depression and anxiety level assessment model is constructed using the improved multi-scale convolution module and our proposed width-channel attention module for one-dimensional signal processing. The test results show that the MAE/RMSE of the depression and anxiety level assessment method proposed in this article is 3.05/4.11 and 2.49/1.85, respectively, which has better assessment performance than other related research methods. This study provides an automatic assessment technique based on human computer interaction and virtual reality for mental health physical examination.",
    "cited_by_count": 46,
    "openalex_id": "https://openalex.org/W4225117593",
    "type": "article"
  },
  {
    "title": "Transformer-Based Visual Grounding with Cross-Modality Interaction",
    "doi": "https://doi.org/10.1145/3587251",
    "publication_date": "2023-03-09",
    "publication_year": 2023,
    "authors": "Kun Li; Jiaxiu Li; Dan Guo; Xun Yang; Meng Wang",
    "corresponding_authors": "",
    "abstract": "This article tackles the challenging yet important task of Visual Grounding (VG), which aims to localize a visual region in the given image referred by a natural language query. Existing efforts on the VG task are twofold: (1) two-stage methods first extract region proposals and then rank them according to their similarities with the referring expression, which usually leads to suboptimal results due to the quality of region proposals; (2) one-stage methods usually predict all the possible coordinates of the target region online by leveraging modern object detection architectures, which pay little attention to cross-modality correlations and have limited generalization ability. To better address the task, we present an effective transformer-based end-to-end visual grounding approach, which focuses on capturing the cross-modality correlations between the referring expression and visual regions for accurately reasoning the location of the target region. Specifically, our model consists of a feature encoder, a cross-modality interactor, and a modality-agnostic decoder. The feature encoder is employed to capture the intra-modality correlation, which models the linguistic context in query and the spatial dependency in image respectively. The cross-modality interactor endows the model with the capability of highlighting the localization-relevant visual and textual cues by mutual verification of vision and language, which plays a key role in our model. The decoder learns a consolidated token representation enriched by multi-modal contexts and further directly predicts the box coordinates. Extensive experiments on five public benchmark datasets with quantitative and qualitative analysis clearly demonstrate the effectiveness and rationale of our proposed method.",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W4323663038",
    "type": "article"
  },
  {
    "title": "Robust RGB-T Tracking via Adaptive Modality Weight Correlation Filters and Cross-modality Learning",
    "doi": "https://doi.org/10.1145/3630100",
    "publication_date": "2023-10-25",
    "publication_year": 2023,
    "authors": "Mingliang Zhou; Xinwen Zhao; Futing Luo; Jun Luo; Huayan Pu; Tao Xiang",
    "corresponding_authors": "",
    "abstract": "RGBT tracking is gaining popularity due to its ability to provide effective tracking results in a variety of weather conditions. However, feature specificity and complementarity have not been fully used in existing models that directly fuse the correlation filtering response, which leads to poor tracker performance. In this article, we propose correlation filters with adaptive modality weight and cross-modality learning (AWCM) ability to solve multimodality tracking tasks. First, we use weighted activation to fuse thermal infrared and visible modalities, and the fusion modality is used as an auxiliary modality to suppress noise and increase the learning ability of shared modal features. Second, we design modal weights through average peak-to-correlation energy coefficients to improve model reliability. Third, we propose consistency in using the fusion modality as an intermediate variable for joint learning consistency, thereby increasing tracker robustness via interactive cross-modal learning. Finally, we use the alternating direction method of multipliers algorithm to produce a closed solution and conduct extensive experiments on the RGBT234, VOT-TIR2019, and GTOT tracking benchmark datasets to demonstrate the superior performance of the proposed AWCM against compared to existing tracking algorithms. The code developed in this study is available at the following website. 1",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W4388271799",
    "type": "article"
  },
  {
    "title": "Video Frame Interpolation: A Comprehensive Survey",
    "doi": "https://doi.org/10.1145/3556544",
    "publication_date": "2023-01-31",
    "publication_year": 2023,
    "authors": "Jiong Dong; Kaoru Ota; Mianxiong Dong",
    "corresponding_authors": "",
    "abstract": "Video Frame Interpolation (VFI) is a fascinating and challenging problem in the computer vision (CV) field, aiming to generate non-existing frames between two consecutive video frames. In recent years, many algorithms based on optical flow, kernel, or phase information have been proposed. In this article, we provide a comprehensive review of recent developments in the VFI technique. We first introduce the history of VFI algorithms’ development, the evaluation metrics, and publicly available datasets. We then compare each algorithm in detail, point out their advantages and disadvantages, and compare their interpolation performance and speed on different remarkable datasets. VFI technology has drawn continuous attention in the CV community, some video processing applications based on VFI are also mentioned in this survey, such as slow-motion generation, video compression, video restoration. Finally, we outline the bottleneck faced by the current video frame interpolation technology and discuss future research work.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W4318617422",
    "type": "article"
  },
  {
    "title": "Revolutionizing Visuals: The Role of Generative AI in Modern Image Generation",
    "doi": "https://doi.org/10.1145/3689641",
    "publication_date": "2024-08-22",
    "publication_year": 2024,
    "authors": "Gaurang Bansal; Aditya Nawal; Vinay Chamola; Norbert Herencsár",
    "corresponding_authors": "",
    "abstract": "Traditional multimedia experiences are undergoing a transformation as generative AI integration fosters enhanced creative workflows, streamlines content creation processes, and unlocks the potential for entirely new forms of multimedia storytelling. It has potential to generate captivating visuals to accompany a documentary based solely on historical text descriptions, or creating personalized and interactive multimedia experiences tailored to individual user preferences. From the high-resolution cameras in our smartphones to the immersive experiences offered by the latest technologies, the impact of generative imaging undeniable. This study delves into the burgeoning field of generative AI, with a focus on its revolutionary impact on image generation. It explores the background of traditional imaging in consumer electronics and the motivations for integrating AI, leading to enhanced capabilities in various applications. The research critically examines current advancements in state-of-the-art technologies like DALL-E 2, Craiyon, Stable Diffusion, Imagen, Jasper, NightCafe, and Deep AI, assessing their performance on parameters such as image quality, diversity, and efficiency. It also addresses the limitations and ethical challenges posed by this integration, balancing creative autonomy with AI automation. The novelty of this work lies in its comprehensive analysis and comparison of these AI systems, providing insightful results that highlight both their strengths and areas for improvement. The conclusion underscores the transformative potential of generative AI in image generation, paving the way for future research and development to further enhance and refine these technologies. This article serves as a critical guide for understanding the current landscape and future prospects of AI-driven image creation, offering a glimpse into the evolving synergy between human creativity and artificial intelligence.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W4401730946",
    "type": "article"
  },
  {
    "title": "Triplet Contrastive Representation Learning for Unsupervised Vehicle Re-identification",
    "doi": "https://doi.org/10.1145/3695255",
    "publication_date": "2024-09-06",
    "publication_year": 2024,
    "authors": "Fei Shen; Xiaoyu Du; Liyan Zhang; Xiangbo Shu; Jinhui Tang",
    "corresponding_authors": "",
    "abstract": "Part feature learning plays a crucial role in achieving fine-grained semantic understanding in unsupervised vehicle re-identification. However, existing approaches directly model part and global features, which can easily lead to severe gradient vanishing issues due to their unequal feature information and unreliable pseudo-labels. To address this problem, in this paper, we propose a triplet contrastive representation learning (TCRL) framework, which leverages cluster features to bridge the part features and global features for unsupervised vehicle re-identification. Specifically, TCRL devises three memory banks to store the instance/cluster features and proposes a proxy contrastive loss (PCL) to make contrastive learning between adjacent memory banks, thus presenting the associations between the part and global features as a transition of the part-cluster and cluster-global associations. Since the cluster memory bank copes with all the vehicle features, it can summarize them into a discriminative feature representation. To deeply exploit the instance/cluster information, TCRL proposes two additional loss functions. For the instance-level feature, a hybrid contrastive loss (HCL) re-defines the sample correlations by approaching the positive instance features and pushing all negative instance features away. For the cluster-level feature, a weighted regularization cluster contrastive loss (WRCCL) refines the pseudo labels by penalizing the mislabeled images according to the instance similarity. Extensive experiments show that TCRL outperforms many state-of-the-art unsupervised vehicle re-identification approaches.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W4402305045",
    "type": "article"
  },
  {
    "title": "Rethinking Feature Mining for Light Field Salient Object Detection",
    "doi": "https://doi.org/10.1145/3676967",
    "publication_date": "2024-07-08",
    "publication_year": 2024,
    "authors": "Guibiao Liao; Wei Gao",
    "corresponding_authors": "",
    "abstract": "Light field salient object detection (LF SOD) has recently received increasing attention. However, most current works typically rely on an individual focal stack backbone for feature extraction. This manner ignores the characteristic of blurred saliency-related regions and contour within focal slices, resulting in insufficient or even inaccurate saliency responses. Aiming at addressing this issue, we rethink the feature mining (i.e., exploration) within focal slices and focus on exploiting informative focal slice features and fully leveraging contour information for accurate LF SOD. First, we observe that the geometric relation between different regions within the focal slices is conducive to useful saliency feature mining if utilized properly. In light of this, we propose an implicit graph learning (IGL) approach. The IGL constructs graph structures to propagate informative geometric relations within the focal slices and all-focus features, and promotes crucial and discriminative focal stack feature mining via graph feature distillation. Second, unlike previous works that rarely utilize contour information, we propose a reciprocal refinement fusion (RRF) strategy. This strategy encourages saliency features and object contour cues to effectively complement each other. Furthermore, a contour hint injection mechanism is introduced to refine the feature expressions. Extensive experiments showcase the superiority of our approach over previous state-of-the-art models with an efficient real-time inference speed. Codes are available at https://github.com/gbliao/IRNet and https://openi.pcl.ac.cn/OpenVision/IRNet .",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W4400418313",
    "type": "article"
  },
  {
    "title": "Multi-grained Point Cloud Geometry Compression via Dual-model Prediction with Extended Octree",
    "doi": "https://doi.org/10.1145/3671001",
    "publication_date": "2024-06-12",
    "publication_year": 2024,
    "authors": "T.Y. Qin; Ge Li; Wei Gao; Shan Liu",
    "corresponding_authors": "",
    "abstract": "The state-of-the-art G-PCC (geometry-based point cloud compression) (Octree) is the fine-grained approach, which uses the octree to partition point clouds into voxels and predicts them based on neighbor occupancy in narrower spaces. However, G-PCC (Octree) is less effective at compressing dense point clouds than multi-grained approaches (such as G-PCC (Trisoup)), which exploit the continuous point distribution in nodes partitioned by the pruned octree over larger spaces. Therefore, we propose a lossy multi-grained compression with extended octree and dual-model prediction. The extended octree, where each partitioned node contains intra-block and extra-block points, is applied to address poor prediction (such as overfitting) at the node edges of the octree partition. For the points of each multi-grained node, dual-model prediction fits surfaces and projects residuals onto the surfaces, reducing projection residuals for efficient 2D compression and fitting complexity. In addition, a hybrid DWT-DCT transform for 2D projection residuals mitigates the resolution degradation of DWT and the blocking effect of DCT during high compression. Experimental results demonstrate the superior performance of our method over advanced G-PCC (Octree), achieving BD-rate gains of 55.9% and 45.3% for point-to-point ( D1 ) and point-to-plane ( D2 ) distortions, respectively. Our approach also outperforms G-PCC (Octree) and G-PCC (Trisoup) in subjective evaluation.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W4399570151",
    "type": "article"
  },
  {
    "title": "Parents and Children: Distinguishing Multimodal DeepFakes from Natural Images",
    "doi": "https://doi.org/10.1145/3665497",
    "publication_date": "2024-05-21",
    "publication_year": 2024,
    "authors": "Roberto Amoroso; Davide Morelli; Marcella Cornia; Lorenzo Baraldi; Alberto Del Bimbo; Rita Cucchiara",
    "corresponding_authors": "",
    "abstract": "Recent advancements in diffusion models have enabled the generation of realistic deepfakes from textual prompts in natural language. While these models have numerous benefits across various sectors, they have also raised concerns about the potential misuse of fake images and cast new pressures on fake image detection. In this work, we pioneer a systematic study on deepfake detection generated by state-of-the-art diffusion models. Firstly, we conduct a comprehensive analysis of the performance of contrastive and classification-based visual features, respectively extracted from CLIP-based models and ResNet or ViT-based architectures trained on image classification datasets. Our results demonstrate that fake images share common low-level cues, which render them easily recognizable. Further, we devise a multimodal setting wherein fake images are synthesized by different textual captions, which are used as seeds for a generator. Under this setting, we quantify the performance of fake detection strategies and introduce a contrastive-based disentangling method that lets us analyze the role of the semantics of textual descriptions and low-level perceptual cues. Finally, we release a new dataset, called COCOFake, containing about 1.2M images generated from the original COCO image-caption pairs using two recent text-to-image diffusion models, namely Stable Diffusion v1.4 and v2.0.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W4398184526",
    "type": "article"
  },
  {
    "title": "Deep Neighborhood-aware Proxy Hashing with Uniform Distribution Constraint for Cross-modal Retrieval",
    "doi": "https://doi.org/10.1145/3643639",
    "publication_date": "2024-01-27",
    "publication_year": 2024,
    "authors": "Yadong Huo; Qibing Qin; Jiangyan Dai; Wenfeng Zhang; Lei Huang; Chengduan Wang",
    "corresponding_authors": "",
    "abstract": "Cross-modal retrieval methods based on hashing have gained significant attention in both academic and industrial research. Deep learning techniques have played a crucial role in advancing supervised cross-modal hashing methods, leading to significant practical improvements. Despite these achievements, current deep cross-modal hashing still encounters some underexplored limitations. Specifically, most of the available deep hashing usually utilizes pair-wise or triplet-wise strategies to promote the separation of the inter-classes by calculating the relative similarities between samples, weakening the compactness of intra-class data from different modalities, which could generate ambiguous neighborhoods. In this article, the Deep Neighborhood-aware Proxy Hashing (DNPH) framework is proposed to learn a discriminative embedding space with the original neighborhood relation preserved. By introducing learnable shared category proxies, the neighborhood-aware proxy loss is proposed to project the heterogeneous data into a unified common embedding, in which the sample is pulled closer to the corresponding category proxy and is pushed away from other proxies, capturing small within-class scatter and big between-class scatter. To enhance the quality of the obtained binary codes, the uniform distribution constraint is developed to make each hash bit independently obey the discrete uniform distribution. In addition, the discrimination loss is designed to preserve modality-specific semantic information of samples. Extensive experiments are performed on three benchmark datasets to prove that our proposed DNPH framework achieves comparable or even better performance compared with the state-of-the-art cross-modal retrieval applications. The corresponding code implementation of our DNPH framework is as follows: https://github.com/QinLab-WFU/OUR-DNPH .",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W4391282658",
    "type": "article"
  },
  {
    "title": "GMS-3DQA: Projection-Based Grid Mini-patch Sampling for 3D Model Quality Assessment",
    "doi": "https://doi.org/10.1145/3643817",
    "publication_date": "2024-02-01",
    "publication_year": 2024,
    "authors": "Zicheng Zhang; Wei Sun; Haoning Wu; Yingjie Zhou; Chunyi Li; Zijian Chen; Xiongkuo Min; Guangtao Zhai; Weisi Lin",
    "corresponding_authors": "",
    "abstract": "Nowadays, most three-dimensional model quality assessment (3DQA) methods have been aimed at improving accuracy. However, little attention has been paid to the computational cost and inference time required for practical applications. Model-based 3DQA methods extract features directly from the 3D models, which are characterized by their high degree of complexity. As a result, many researchers are inclined towards utilizing projection-based 3DQA methods. Nevertheless, previous projection-based 3DQA methods directly extract features from multi-projections to ensure quality prediction accuracy, which calls for more resource consumption and inevitably leads to inefficiency. Thus, in this article, we address this challenge by proposing a no-reference (NR) projection-based G rid M ini-patch S ampling 3D Model Q uality A ssessment (GMS-3DQA) method. The projection images are rendered from six perpendicular viewpoints of the 3D model to cover sufficient quality information. To reduce redundancy and inference resources, we propose a multi-projection grid mini-patch sampling strategy (MP-GMS), which samples grid mini-patches from the multi-projections and forms the sampled grid mini-patches into one quality mini-patch map (QMM). The Swin-Transformer tiny backbone is then used to extract quality-aware features from the QMMs. The experimental results show that the proposed GMS-3DQA outperforms existing state-of-the-art NR-3DQA methods on the point cloud quality assessment databases for both accuracy and efficiency. The efficiency analysis reveals that the proposed GMS-3DQA requires far less computational resources and inference time than other 3DQA competitors. The code is available at https://github.com/zzc-1998/GMS-3DQA .",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W4391448717",
    "type": "article"
  },
  {
    "title": "<tt>TWIN-GPT</tt> : Digital Twins for Clinical Trials via Large Language Model",
    "doi": "https://doi.org/10.1145/3674838",
    "publication_date": "2024-07-10",
    "publication_year": 2024,
    "authors": "Yue Wang; Tianfan Fu; Yinlong Xu; Zihan Ma; Hongxia Xu; Bang Du; Yingzhou Lu; Honghao Gao; Jian Wu; Jintai Chen",
    "corresponding_authors": "",
    "abstract": "Clinical trials are indispensable for medical research and the development of new treatments. However, clinical trials often involve thousands of participants and can span several years to complete, with a high probability of failure during the process. Recently, there has been a burgeoning interest in virtual clinical trials, which simulate real-world scenarios and hold the potential to significantly enhance patient safety, expedite development, reduce costs, and contribute to the broader scientific knowledge in healthcare. Existing research often focuses on leveraging electronic health records (EHRs) to support clinical trial outcome prediction. Yet, trained with limited clinical trial outcome data, existing approaches frequently struggle to perform accurate predictions. Some research has attempted to generate EHRs to augment model development but has fallen short in personalizing the generation for individual patient profiles. Recently, the emergence of large language models has illuminated new possibilities, as their embedded comprehensive clinical knowledge has proven beneficial in addressing medical issues. In this paper, we propose a large language model-based digital twin creation approach, called TWIN-GPT . TWIN-GPT can establish cross-dataset associations of medical information given limited data, generating unique personalized digital twins for different patients, thereby preserving individual patient characteristics. Comprehensive experiments show that using digital twins created by TWIN-GPT can boost the clinical trial outcome prediction, exceeding various previous prediction approaches. Besides, we also demonstrate that TWIN-GPT can generate high-fidelity trial data that closely approximates specific patients, aiding in more accurate result predictions in data-scarce situations. Moreover, our study provides practical evidence for the application of digital twins in healthcare, highlighting its potential significance.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W4400493343",
    "type": "article"
  },
  {
    "title": "Deepfake Detection Using Spatiotemporal Transformer",
    "doi": "https://doi.org/10.1145/3643030",
    "publication_date": "2024-01-23",
    "publication_year": 2024,
    "authors": "Bachir Kaddar; Sid Ahmed Fezza; Zahid Akhtar; Wassim Hamidouche; Abdenour Hadid; Joan Serra-Sagristà",
    "corresponding_authors": "",
    "abstract": "Recent advances in generative models and the availability of large-scale benchmarks have made deepfake video generation and manipulation easier. Nowadays, the number of new hyper-realistic deepfake videos used for negative purposes is dramatically increasing, thus creating the need for effective deepfake detection methods. Although many existing deepfake detection approaches, particularly CNN-based methods, show promising results, they suffer from several drawbacks. In general, poor generalization results have been obtained under unseen/new deepfake generation methods. The crucial reason for the above defect is that CNN-based methods focus on the local spatial artifacts, which are unique for every manipulation method. Therefore, it is hard to learn the general forgery traces of different manipulation methods without considering the dependencies that extend beyond the local receptive field. To address this problem, this article proposes a framework that combines Convolutional Neural Network (CNN) with Vision Transformer (ViT) to improve detection accuracy and enhance generalizability. Our method, named HCiT , exploits the advantages of CNNs to extract meaningful local features, as well as the ViT’s self-attention mechanism to learn discriminative global contextual dependencies in a frame-level image explicitly. In this hybrid architecture, the high-level feature maps extracted from the CNN are fed into the ViT model that determines whether a specific video is fake or real. Experiments were performed on Faceforensics++, DeepFake Detection Challenge preview, Celeb datasets, and the results show that the proposed method significantly outperforms the state-of-the-art methods. In addition, the HCiT method shows a great capacity for generalization on datasets covering various techniques of deepfake generation. The source code is available at: https://github.com/KADDAR-Bachir/HCiT",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4391136344",
    "type": "article"
  },
  {
    "title": "Efficient Brain Tumor Segmentation with Lightweight Separable Spatial Convolutional Network",
    "doi": "https://doi.org/10.1145/3653715",
    "publication_date": "2024-03-23",
    "publication_year": 2024,
    "authors": "Hao Zhang; Meng Liu; Yuan Qi; Ning Yang; Shunbo Hu; Liqiang Nie; Wenyin Zhang",
    "corresponding_authors": "",
    "abstract": "Accurate and automated segmentation of lesions in brain MRI scans is crucial in diagnostics and treatment planning. Despite the significant achievements of existing approaches, they often require substantial computational resources and fail to fully exploit the synergy between low-level and high-level features. To address these challenges, we introduce the Separable Spatial Convolutional Network (SSCN), an innovative model that refines the U-Net architecture to achieve efficient brain tumor segmentation with minimal computational cost. SSCN integrates the PocketNet paradigm and replaces standard convolutions with depthwise separable convolutions, resulting in a significant reduction in parameters and computational load. Additionally, our feature complementary module enhances the interaction between features across the encoder-decoder structure, facilitating the integration of multi-scale features while maintaining low computational demands. The model also incorporates a separable spatial attention mechanism, enhancing its capability to discern spatial details. Empirical validations on standard datasets demonstrate the effectiveness of our proposed model, especially in segmenting small and medium-sized tumors, with only 0.27M parameters and 3.68 GFlops. Our code is available at https://github.com/zzpr/SSCN .",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4393115170",
    "type": "article"
  },
  {
    "title": "Potential Features Fusion Network for Multimodal Fake News Detection",
    "doi": "https://doi.org/10.1145/3711866",
    "publication_date": "2025-01-10",
    "publication_year": 2025,
    "authors": "Feifei Kou; Bingwei Wang; Haisheng Li; Chuangying Zhu; Lei Shi; Jiwei Zhang; Limei Qi",
    "corresponding_authors": "",
    "abstract": "With the popularization of social networks, fake news is also widely and rapidly spreading, which poses a great threat to the Internet. Therefore, how to detect fake news automatically and efficiently has become an urgent problem to be solved. However, the existing approaches mostly focus on the explicit features (images and text) and deep fusions, without considering potential features such as text emotion and image category. To find a solution to this issue, we propose a Potential Features Fusion Network (PFFN), which models the explicit and potential features at the same time. To exploit the potential image features, we introduce a mixture of experts structure to process the news image separately, which can best use the relationships between the news image category and fake news detection. Besides, we also extract emotion features as potential text features and fuse them with explicit text features. Finally, we establish an attention-based feature fusion network to fuse the potential features with the explicit features, which can obtain a multi-modal fusion feature of a piece of news and thus further improve the performance. We make experiments on four public datasets (Weibo16, Weibo19, Twitter, and PolitiFact), the results compared with the baseline approaches demonstrate that our PFFN has a better performance. Our code is available at https://github.com/Wang-bupt/PFFN",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4406233646",
    "type": "article"
  },
  {
    "title": "Mars: Paying More Attention to Visual Attributes for Text-based Person Search",
    "doi": "https://doi.org/10.1145/3721482",
    "publication_date": "2025-03-05",
    "publication_year": 2025,
    "authors": "Alex Ergasti; Tomaso Fontanini; Claudio Ferrari; Massimo Bertozzi; Andrea Prati",
    "corresponding_authors": "",
    "abstract": "Text-based person search (TBPS) is a problem that gained significant interest within the research community. The task is that of retrieving one or more images of a specific individual based on a textual description. The multi-modal nature of the task requires learning representations that bridge text and image data within a shared latent space. Existing TBPS systems face two major challenges. One is defined as inter-identity noise that is due to the inherent vagueness and imprecision of text descriptions and it indicates how descriptions of visual attributes can be generally associated to different people; the other is the intra-identity variations, which are all those nuisances e.g. , pose, illumination, that can alter the visual appearance of the same textual attributes for a given subject. To address these issues, this paper presents a novel TBPS architecture named MARS (Mae-Attribute-Relation-Sensitive), which enhances current state-of-the-art models by introducing two key components: a Visual Reconstruction Loss and an Attribute Loss. The former employs a Masked AutoEncoder trained to reconstruct randomly masked image patches with the aid of the textual description. In doing so the model is encouraged to learn more expressive representations and textual-visual relations in the latent space. The Attribute Loss, instead, balances the contribution of different types of attributes, defined as adjective-noun chunks of text. This loss ensures that every attribute is taken into consideration in the person retrieval process. Extensive experiments on three commonly used datasets, namely CUHK-PEDES, ICFG-PEDES, and RSTPReid, report performance improvements, with significant gains in the mean Average Precision (mAP) metric w.r.t. the current state of the art. Code will be available at https://github.com/ErgastiAlex/MARS .",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4408220440",
    "type": "article"
  },
  {
    "title": "Quality Assessment in the Era of Large Models: A Survey",
    "doi": "https://doi.org/10.1145/3722559",
    "publication_date": "2025-03-11",
    "publication_year": 2025,
    "authors": "Zicheng Zhang; Yingjie Zhou; Chunyi Li; Baixuan Zhao; Xiaohong Liu; Guangtao Zhai",
    "corresponding_authors": "",
    "abstract": "Quality assessment, which evaluates the visual quality level of multimedia experiences, has garnered significant attention from researchers and has evolved substantially through dedicated efforts. Before the advent of large models, quality assessment typically relied on small expert models tailored for specific tasks. While these smaller models are effective at handling their designated tasks and predicting quality levels, they often lack explainability and robustness. With the advancement of large models, which align more closely with human cognitive and perceptual processes, many researchers are now leveraging the prior knowledge embedded in these large models for quality assessment tasks. This emergence of quality assessment within the context of large models motivates us to provide a comprehensive review focusing on two key aspects: 1) the assessment of large models, and 2) the role of large models in assessment tasks. We begin by reflecting on the historical development of quality assessment. Subsequently, we move to detailed discussions of related works concerning quality assessment in the era of large models. Finally, we offer insights into the future progression and potential pathways for quality assessment in this new era. We hope this survey will enable a rapid understanding of the development of quality assessment in the era of large models and inspire further advancements in the field.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4408326517",
    "type": "article"
  },
  {
    "title": "Low-light Image Enhancement via CLIP-Fourier Guided Wavelet Diffusion",
    "doi": "https://doi.org/10.1145/3764933",
    "publication_date": "2025-09-01",
    "publication_year": 2025,
    "authors": "Minglong Xue; Jinhong He; Wenhai Wang; Mingliang Zhou",
    "corresponding_authors": "",
    "abstract": "Low-light image enhancement techniques have significantly progressed, but unstable image quality recovery and unsatisfactory visual perception are still significant challenges. To solve these problems, we propose a novel and robust low-light image enhancement method via CLIP-Fourier guided wavelet diffusion, abbreviated as CFWD. Specifically, the CFWD leverages multimodal visual-language information in the frequency domain space created by multiple wavelet transforms to guide the enhancement process. Multiscale supervision across different modalities facilitates the alignment of image features with semantic features during the wavelet diffusion process, effectively bridging the gap between the degraded and normal domains. Moreover, to further promote the effective recovery of the image details, we combine the Fourier transform based on the wavelet transform and construct a hybrid high-frequency perception module (HFPM) with a significant perception of the detailed features. This module avoids the diversity confusion of the wavelet diffusion process by guiding the fine-grained structure recovery of the enhancement results to achieve favourable metrics and perceptually oriented enhancement. Extensive quantitative and qualitative experiments on publicly available real-world benchmarks show that our approach outperforms existing state-of-the-art methods, achieving significant progress in image quality and noise suppression. The project code is available at https://github.com/hejh8/CFWD.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4413893930",
    "type": "article"
  },
  {
    "title": "Structured multimedia authoring",
    "doi": "https://doi.org/10.1145/1047936.1047943",
    "publication_date": "2005-02-01",
    "publication_year": 2005,
    "authors": "Dick C. A. Bulterman; Lynda Hardman",
    "corresponding_authors": "",
    "abstract": "Authoring context sensitive, interactive multimedia presentations is much more complex than authoring either purely audiovisual applications or text. Interactions among media objects need to be described as a set of spatio-temporal relationships that account for synchronous and asynchronous interactions, as well as on-demand linking behavior. This article considers the issues that need to be addressed by an authoring environment. We begin with a partitioning of concerns based on seven classes of authoring problems. We then describe a selection of multimedia authoring environments within four different authoring paradigms: structured, timeline, graph and scripting. We next provide observations and insights into the authoring process and argue that the structured paradigm provides the most useful framework for presentation authoring. We close with an example application of the structured multimedia authoring paradigm in the context of our own structure-based system GRiNS.",
    "cited_by_count": 130,
    "openalex_id": "https://openalex.org/W2018414919",
    "type": "article"
  },
  {
    "title": "Re-cinematography",
    "doi": "https://doi.org/10.1145/1404880.1404882",
    "publication_date": "2008-10-01",
    "publication_year": 2008,
    "authors": "Michael Gleicher; Feng Liu",
    "corresponding_authors": "",
    "abstract": "This article presents an approach to postprocessing casually captured videos to improve apparent camera movement. Re-cinematography transforms each frame of a video such that the video better follows cinematic conventions. The approach breaks a video into shorter segments. Segments of the source video where there is no intentional camera movement are made to appear as if the camera is completely static. For segments with camera motions, camera paths are keyframed automatically and interpolated with matrix logarithms to give velocity-profiled movements that appear intentional and directed. Closeups are inserted to provide compositional variety in otherwise uniform segments. The approach automatically balances the tradeoff between motion smoothness and distortion to the original imagery. Results from our prototype show improvements to poor quality home videos.",
    "cited_by_count": 116,
    "openalex_id": "https://openalex.org/W2052452567",
    "type": "article"
  },
  {
    "title": "The Story Picturing Engine---a system for automatic text illustration",
    "doi": "https://doi.org/10.1145/1126004.1126008",
    "publication_date": "2006-02-01",
    "publication_year": 2006,
    "authors": "Dhiraj Joshi; James Z. Wang; Jia Li",
    "corresponding_authors": "",
    "abstract": "We present an unsupervised approach to automated story picturing. Semantic keywords are extracted from the story, an annotated image database is searched. Thereafter, a novel image ranking scheme automatically determines the importance of each image. Both lexical annotations and visual content play a role in determining the ranks. Annotations are processed using the Wordnet. A mutual reinforcement-based rank is calculated for each image. We have implemented the methods in our Story Picturing Engine (SPE) system. Experiments on large-scale image databases are reported. A user study has been performed and statistical analysis of the results has been presented.",
    "cited_by_count": 113,
    "openalex_id": "https://openalex.org/W2020602420",
    "type": "article"
  },
  {
    "title": "The sweet smell of success",
    "doi": "https://doi.org/10.1145/2071396.2071398",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Gheorghiță Ghinea; Oluwakemi A. Ademoye",
    "corresponding_authors": "",
    "abstract": "Olfaction, or smell, is one of the last challenges which multimedia applications have to conquer. As far as computerized smell is concerned, there are several difficulties to overcome, particularly those associated with the ambient nature of smell. In this article, we present results from an empirical study exploring users' perception of olfaction-enhanced multimedia displays. Findings show that olfaction significantly adds to the user multimedia experience. Moreover, use of olfaction leads to an increased sense of reality and relevance. Our results also show that users are tolerant of the interference and distortion effects caused by olfactory effect in multimedia.",
    "cited_by_count": 101,
    "openalex_id": "https://openalex.org/W2036195562",
    "type": "article"
  },
  {
    "title": "Beyond search",
    "doi": "https://doi.org/10.1145/2043612.2043613",
    "publication_date": "2011-11-01",
    "publication_year": 2011,
    "authors": "Richang Hong; Jinhui Tang; Hung‐Khoon Tan; Chong‐Wah Ngo; Shuicheng Yan; Tat‐Seng Chua",
    "corresponding_authors": "",
    "abstract": "The explosive growth of Web videos brings out the challenge of how to efficiently browse hundreds or even thousands of videos at a glance. Given an event-driven query, social media Web sites usually return a large number of videos that are diverse and noisy in a ranking list. Exploring such results will be time-consuming and thus degrades user experience. This article presents a novel scheme that is able to summarize the content of video search results by mining and threading “key” shots, such that users can get an overview of main content of these videos at a glance. The proposed framework mainly comprises four stages. First, given an event query, a set of Web videos is collected associated with their ranking order and tags. Second, key-shots are established and ranked based on near-duplicate keyframe detection and they are threaded in a chronological order. Third, we analyze the tags associated with key-shots. Irrelevant tags are filtered out via a representativeness and descriptiveness analysis, whereas the remaining tags are propagated among key-shots by random walk. Finally, summarization is formulated as an optimization framework that compromises relevance of key-shots and user-defined skimming ratio. We provide two types of summarization: video skimming and visual-textual storyboard. We conduct user studies on twenty event queries for over hundred hours of videos crawled from YouTube. The evaluation demonstrates the feasibility and effectiveness of the proposed solution.",
    "cited_by_count": 97,
    "openalex_id": "https://openalex.org/W2042708146",
    "type": "article"
  },
  {
    "title": "Exploration in Interactive Personalized Music Recommendation",
    "doi": "https://doi.org/10.1145/2623372",
    "publication_date": "2014-08-01",
    "publication_year": 2014,
    "authors": "Wang Xin-xi; Yi Wang; David Hsu; Ye Wang",
    "corresponding_authors": "",
    "abstract": "Current music recommender systems typically act in a greedy manner by recommending songs with the highest user ratings. Greedy recommendation, however, is suboptimal over the long term: it does not actively gather information on user preferences and fails to recommend novel songs that are potentially interesting. A successful recommender system must balance the needs to explore user preferences and to exploit this information for recommendation. This article presents a new approach to music recommendation by formulating this exploration-exploitation trade-off as a reinforcement learning task. To learn user preferences, it uses a Bayesian model that accounts for both audio content and the novelty of recommendations. A piecewise-linear approximation to the model and a variational inference algorithm help to speed up Bayesian inference. One additional benefit of our approach is a single unified model for both music recommendation and playlist generation. We demonstrate the strong potential of the proposed approach with simulation results and a user study.",
    "cited_by_count": 97,
    "openalex_id": "https://openalex.org/W2071133755",
    "type": "article"
  },
  {
    "title": "Hybrid method based on topography for robust detection of iris center and eye corners",
    "doi": "https://doi.org/10.1145/2501643.2501647",
    "publication_date": "2013-08-01",
    "publication_year": 2013,
    "authors": "Arantxa Villanueva; Victoria Ponz; Laura Sesma-Sánchez; Mikel Ariz; Sonia Porta; Rafael Cabeza",
    "corresponding_authors": "",
    "abstract": "A multistage procedure to detect eye features is presented. Multiresolution and topographic classification are used to detect the iris center. The eye corner is calculated combining valley detection and eyelid curve extraction. The algorithm is tested in the BioID database and in a proprietary database containing more than 1200 images. The results show that the suggested algorithm is robust and accurate. Regarding the iris center our method obtains the best average behavior for the BioID database compared to other available algorithms. Additional contributions are that our algorithm functions in real time and does not require complex post processing stages.",
    "cited_by_count": 94,
    "openalex_id": "https://openalex.org/W2040336280",
    "type": "article"
  },
  {
    "title": "Supporting Healthy Grocery Shopping via Mobile Augmented Reality",
    "doi": "https://doi.org/10.1145/2808207",
    "publication_date": "2015-10-21",
    "publication_year": 2015,
    "authors": "Junho Ahn; James Williamson; Mike Gartrell; Richard Han; Qin Lv; Shivakant Mishra",
    "corresponding_authors": "",
    "abstract": "Augmented reality (AR) applications have recently become popular on modern smartphones. We explore the effectiveness of this mobile AR technology in the context of grocery shopping, in particular as a means to assist shoppers in making healthier decisions as they decide which grocery products to buy. We construct an AR-assisted mobile grocery-shopping application that makes real-time, customized recommendations of healthy products to users and also highlights products to avoid for various types of health concerns, such as allergies to milk or nut products, low-sodium or low-fat diets, and general caloric intake. We have implemented a prototype of this AR-assisted mobile grocery shopping application and evaluated its effectiveness in grocery store aisles. Our application's evaluation with typical grocery shoppers demonstrates that AR overlay tagging of products reduces the search time to find healthy food items, and that coloring the tags helps to improve the user's ability to quickly and easily identify recommended products, as well as products to avoid. We have evaluated our application's functionality by analyzing the data we collected from 15 in-person actual grocery-shopping subjects and 104 online application survey participants.",
    "cited_by_count": 91,
    "openalex_id": "https://openalex.org/W1965396258",
    "type": "article"
  },
  {
    "title": "Crowd Scene Understanding from Video",
    "doi": "https://doi.org/10.1145/3052930",
    "publication_date": "2017-03-27",
    "publication_year": 2017,
    "authors": "Jason M. Grant; Patrick J. Flynn",
    "corresponding_authors": "",
    "abstract": "Crowd video analysis has applications in crowd management, public space design, and visual surveillance. Example tasks potentially aided by automated analysis include anomaly detection (such as a person walking against the grain of traffic or rapid assembly/dispersion of groups of people), population and density measurements, and interactions between groups of people. This survey explores crowd analysis as it relates to two primary research areas: crowd statistics and behavior understanding. First, we survey methods for counting individuals and approximating the density of the crowd. Second, we showcase research efforts on behavior understanding as related to crowds. These works focus on identifying groups, interactions within small groups, and abnormal activity detection such as riots and bottlenecks in large crowds. Works presented in this section also focus on tracking groups of individuals, either as a single entity or a subset of individuals within the frame of reference. Finally, a summary of datasets available for crowd activity video research is provided.",
    "cited_by_count": 88,
    "openalex_id": "https://openalex.org/W2600200270",
    "type": "article"
  },
  {
    "title": "“Wow! You Are So Beautiful Today!”",
    "doi": "https://doi.org/10.1145/2659234",
    "publication_date": "2014-10-01",
    "publication_year": 2014,
    "authors": "Luoqi Liu; Junliang Xing; Si Liu; Hui Xu; Xi Zhou; Shuicheng Yan",
    "corresponding_authors": "",
    "abstract": "Beauty e-Experts, a fully automatic system for makeover recommendation and synthesis, is developed in this work. The makeover recommendation and synthesis system simultaneously considers many kinds of makeover items on hairstyle and makeup. Given a user-provided frontal face image with short/bound hair and no/light makeup, the Beauty e-Experts system not only recommends the most suitable hairdo and makeup, but also synthesizes the virtual hairdo and makeup effects. To acquire enough knowledge for beauty modeling, we built the Beauty e-Experts Database, which contains 1,505 female photos with a variety of attributes annotated with different discrete values. We organize these attributes into two different categories, beauty attributes and beauty-related attributes. Beauty attributes refer to those values that are changeable during the makeover process and thus need to be recommended by the system. Beauty-related attributes are those values that cannot be changed during the makeup process but can help the system to perform recommendation. Based on this Beauty e-Experts Dataset, two problems are addressed for the Beauty e-Experts system: what to recommend and how to wear it, which describes a similar process of selecting hairstyle and cosmetics in daily life. For the what-to-recommend problem, we propose a multiple tree-structured supergraph model to explore the complex relationships among high-level beauty attributes, mid-level beauty-related attributes, and low-level image features. Based on this model, the most compatible beauty attributes for a given facial image can be efficiently inferred. For the how-to-wear-it problem, an effective and efficient facial image synthesis module is designed to seamlessly synthesize the recommended makeovers into the user facial image. We have conducted extensive experiments on testing images of various conditions to evaluate and analyze the proposed system. The experimental results well demonstrate the effectiveness and efficiency of the proposed system.",
    "cited_by_count": 85,
    "openalex_id": "https://openalex.org/W1971375352",
    "type": "article"
  },
  {
    "title": "A Unified Framework for Multi-Modal Isolated Gesture Recognition",
    "doi": "https://doi.org/10.1145/3131343",
    "publication_date": "2018-02-21",
    "publication_year": 2018,
    "authors": "Jiali Duan; Jun Wan; Shuai Zhou; Xiaoyuan Guo; Stan Z. Li",
    "corresponding_authors": "",
    "abstract": "In this article, we focus on isolated gesture recognition and explore different modalities by involving RGB stream, depth stream, and saliency stream for inspection. Our goal is to push the boundary of this realm even further by proposing a unified framework that exploits the advantages of multi-modality fusion. Specifically, a spatial-temporal network architecture based on consensus-voting has been proposed to explicitly model the long-term structure of the video sequence and to reduce estimation variance when confronted with comprehensive inter-class variations. In addition, a three-dimensional depth-saliency convolutional network is aggregated in parallel to capture subtle motion characteristics. Extensive experiments are done to analyze the performance of each component and our proposed approach achieves the best results on two public benchmarks, ChaLearn IsoGD and RGBD-HuDaAct, outperforming the closest competitor by a margin of over 10% and 15%, respectively. Our project and codes will be released at https://davidsonic.github.io/index/acm_tomm_2017.html.",
    "cited_by_count": 83,
    "openalex_id": "https://openalex.org/W2791276322",
    "type": "article"
  },
  {
    "title": "Mobile Multi-Food Recognition Using Deep Learning",
    "doi": "https://doi.org/10.1145/3063592",
    "publication_date": "2017-08-10",
    "publication_year": 2017,
    "authors": "Parisa Pouladzadeh; Shervin Shirmohammadi",
    "corresponding_authors": "",
    "abstract": "In this article, we propose a mobile food recognition system that uses the picture of the food, taken by the user’s mobile device, to recognize multiple food items in the same meal, such as steak and potatoes on the same plate, to estimate the calorie and nutrition of the meal. To speed up and make the process more accurate, the user is asked to quickly identify the general area of the food by drawing a bounding circle on the food picture by touching the screen. The system then uses image processing and computational intelligence for food item recognition. The advantage of recognizing items, instead of the whole meal, is that the system can be trained with only single item food images. At the training stage, we first use region proposal algorithms to generate candidate regions and extract the convolutional neural network (CNN) features of all regions. Second, we perform region mining to select positive regions for each food category using maximum cover by our proposed submodular optimization method. At the testing stage, we first generate a set of candidate regions. For each region, a classification score is computed based on its extracted CNN features and predicted food names of the selected regions. Since fast response is one of the important parameters for the user who wants to eat the meal, certain heavy computational parts of the application are offloaded to the cloud. Hence, the processes of food recognition and calorie estimation are performed in cloud server. Our experiments, conducted with the FooDD dataset, show an average recall rate of 90.98%, precision rate of 93.05%, and accuracy of 94.11% compared to 50.8% to 88% accuracy of other existing food recognition systems.",
    "cited_by_count": 82,
    "openalex_id": "https://openalex.org/W2742792558",
    "type": "article"
  },
  {
    "title": "Dense 3D-Convolutional Neural Network for Person Re-Identification in Videos",
    "doi": "https://doi.org/10.1145/3231741",
    "publication_date": "2019-01-24",
    "publication_year": 2019,
    "authors": "Jiawei Liu; Zheng-Jun Zha; Xuejin Chen; Zilei Wang; Yongdong Zhang",
    "corresponding_authors": "",
    "abstract": "Person re-identification aims at identifying a certain pedestrian across non-overlapping multi-camera networks in different time and places. Existing person re-identification approaches mainly focus on matching pedestrians on images; however, little attention has been paid to re-identify pedestrians in videos. Compared to images, video clips contain motion patterns of pedestrians, which is crucial to person re-identification. Moreover, consecutive video frames present pedestrian appearance with different body poses and from different viewpoints, providing valuable information toward addressing the challenge of pose variation, occlusion, and viewpoint change, and so on. In this article, we propose a Dense 3D-Convolutional Network (D3DNet) to jointly learn spatio-temporal and appearance representation for person re-identification in videos. The D3DNet consists of multiple three-dimensional (3D) dense blocks and transition layers. The 3D dense blocks enlarge the receptive fields of visual neurons in both spatial and temporal dimensions, leading to discriminative appearance representation as well as short-term and long-term motion patterns of pedestrians without the requirement of an additional motion estimation module. Moreover, we formulate a loss function consisting of an identification loss and a center loss to minimize intra-class variance and maximize inter-class variance simultaneously, toward addressing the challenge of large intra-class variance and small inter-class variance. Extensive experiments on two real-world video datasets of person identification, i.e., MARS and iLIDS-VID, have shown the effectiveness of the proposed approach.",
    "cited_by_count": 80,
    "openalex_id": "https://openalex.org/W2912152775",
    "type": "article"
  },
  {
    "title": "INSTRE",
    "doi": "https://doi.org/10.1145/2700292",
    "publication_date": "2015-02-05",
    "publication_year": 2015,
    "authors": "Shuang Wang; Shuqiang Jiang",
    "corresponding_authors": "",
    "abstract": "Over the last several decades, researches on visual object retrieval and recognition have achieved fast and remarkable success. However, while the category-level tasks prevail in the community, the instance-level tasks (especially recognition) have not yet received adequate focuses. Applications such as content-based search engine and robot vision systems have alerted the awareness to bring instance-level tasks into a more realistic and challenging scenario. Motivated by the limited scope of existing instance-level datasets, in this article we propose a new benchmark for INSTance-level visual object REtrieval and REcognition (INSTRE). Compared with existing datasets, INSTRE has the following major properties: (1) balanced data scale, (2) more diverse intraclass instance variations, (3) cluttered and less contextual backgrounds, (4) object localization annotation for each image, (5) well-manipulated double-labelled images for measuring multiple object (within one image) case. We will quantify and visualize the merits of INSTRE data, and extensively compare them against existing datasets. Then on INSTRE, we comprehensively evaluate several popular algorithms to large-scale object retrieval problem with multiple evaluation metrics. Experimental results show that all the methods suffer a performance drop on INSTRE, proving that this field still remains a challenging problem. Finally we integrate these algorithms into a simple yet efficient scheme for recognition and compare it with classification-based methods. Importantly, we introduce the realistic multiobjects recognition problem. All experiments are conducted in both single object case and multiple objects case.",
    "cited_by_count": 78,
    "openalex_id": "https://openalex.org/W2040002737",
    "type": "article"
  },
  {
    "title": "OmniArt",
    "doi": "https://doi.org/10.1145/3273022",
    "publication_date": "2018-10-23",
    "publication_year": 2018,
    "authors": "Gjorgji Strezoski; Marcel Worring",
    "corresponding_authors": "",
    "abstract": "Baselines are the starting point of any quantitative multimedia research, and benchmarks are essential for pushing those baselines further. In this article, we present baselines for the artistic domain with a new benchmark dataset featuring over 2 million images with rich structured metadata dubbed OmniArt. OmniArt contains annotations for dozens of attribute types and features semantic context information through concepts, IconClass labels, color information, and (limited) object-level bounding boxes. For our dataset we establish and present baseline scores on multiple tasks such as artist attribution, creation period estimation, type, style, and school prediction. In addition to our metadata related experiments, we explore the color spaces of art through different types and evaluate a transfer learning object recognition pipeline.",
    "cited_by_count": 77,
    "openalex_id": "https://openalex.org/W2897923591",
    "type": "article"
  },
  {
    "title": "Social Event Classification via Boosted Multimodal Supervised Latent Dirichlet Allocation",
    "doi": "https://doi.org/10.1145/2659521",
    "publication_date": "2015-01-07",
    "publication_year": 2015,
    "authors": "Shengsheng Qian; Tianzhu Zhang; Changsheng Xu; M. Shamim Hossain",
    "corresponding_authors": "",
    "abstract": "With the rapidly increasing popularity of social media sites (e.g., Flickr, YouTube, and Facebook), it is convenient for users to share their own comments on many social events, which successfully facilitates social event generation, sharing and propagation and results in a large amount of user-contributed media data (e.g., images, videos, and text) for a wide variety of real-world events of different types and scales. As a consequence, it has become more and more difficult to exactly find the interesting events from massive social media data, which is useful to browse, search and monitor social events by users or governments. To deal with these issues, we propose a novel boosted multimodal supervised Latent Dirichlet Allocation (BMM-SLDA) for social event classification by integrating a supervised topic model, denoted as multi-modal supervised Latent Dirichlet Allocation (mm-SLDA), in the boosting framework. Our proposed BMM-SLDA has a number of advantages. (1) Our mm-SLDA can effectively exploit the multimodality and the multiclass property of social events jointly, and make use of the supervised category label information to classify multiclass social event directly. (2) It is suitable for large-scale data analysis by utilizing boosting weighted sampling strategy to iteratively select a small subset of data to efficiently train the corresponding topic models. (3) It effectively exploits social event structure by the document weight distribution with classification error and can iteratively learn new topic model to correct the previously misclassified event documents. We evaluate our BMM-SLDA on a real world dataset and show extensive experimental results, which demonstrate that our model outperforms state-of-the-art methods.",
    "cited_by_count": 76,
    "openalex_id": "https://openalex.org/W2050103272",
    "type": "article"
  },
  {
    "title": "SIFT match verification by geometric coding for large-scale partial-duplicate web image search",
    "doi": "https://doi.org/10.1145/2422956.2422960",
    "publication_date": "2013-02-01",
    "publication_year": 2013,
    "authors": "Wengang Zhou; Houqiang Li; Yijuan Lu; Qi Tian",
    "corresponding_authors": "",
    "abstract": "Most large-scale image retrieval systems are based on the bag-of-visual-words model. However, the traditional bag-of-visual-words model does not capture the geometric context among local features in images well, which plays an important role in image retrieval. In order to fully explore geometric context of all visual words in images, efficient global geometric verification methods have been attracting lots of attention. Unfortunately, current existing methods on global geometric verification are either computationally expensive to ensure real-time response, or cannot handle rotation well. To solve the preceding problems, in this article, we propose a novel geometric coding algorithm, to encode the spatial context among local features for large-scale partial-duplicate Web image retrieval. Our geometric coding consists of geometric square coding and geometric fan coding, which describe the spatial relationships of SIFT features into three geo-maps for global verification to remove geometrically inconsistent SIFT matches. Our approach is not only computationally efficient, but also effective in detecting partial-duplicate images with rotation, scale changes, partial-occlusion, and background clutter. Experiments in partial-duplicate Web image search, using two datasets with one million Web images as distractors, reveal that our approach outperforms the baseline bag-of-visual-words approach even following a RANSAC verification in mean average precision. Besides, our approach achieves comparable performance to other state-of-the-art global geometric verification methods, for example, spatial coding scheme, but is more computationally efficient.",
    "cited_by_count": 75,
    "openalex_id": "https://openalex.org/W2155980617",
    "type": "article"
  },
  {
    "title": "Multi-Camera Coordination and Control in Surveillance Systems",
    "doi": "https://doi.org/10.1145/2710128",
    "publication_date": "2015-06-02",
    "publication_year": 2015,
    "authors": "Prabhu Natarajan; Pradeep K. Atrey; Mohan Kankanhalli",
    "corresponding_authors": "",
    "abstract": "The use of multiple heterogeneous cameras is becoming more common in today's surveillance systems. In order to perform surveillance tasks, effective coordination and control in multi-camera systems is very important, and is catching significant research attention these days. This survey aims to provide researchers with a state-of-the-art overview of various techniques for multi-camera coordination and control ( MC 3 ) that have been adopted in surveillance systems. The existing literature on MC 3 is presented through several classifications based on the applicable architectures, frameworks and the associated surveillance tasks. Finally, a discussion on the open problems in surveillance area that can be solved effectively using MC 3 and the future directions in MC 3 research is presented",
    "cited_by_count": 71,
    "openalex_id": "https://openalex.org/W1177360271",
    "type": "article"
  },
  {
    "title": "Cache-Centric Video Recommendation",
    "doi": "https://doi.org/10.1145/2716310",
    "publication_date": "2015-06-02",
    "publication_year": 2015,
    "authors": "Dilip Kumar Krishnappa; Michael Zink; Carsten Griwodz; Pål Halvorsen",
    "corresponding_authors": "",
    "abstract": "In this article, we take advantage of the user behavior of requesting videos from the top of the related list provided by YouTube to improve the performance of YouTube caches. We recommend that local caches reorder the related lists associated with YouTube videos, presenting the cached content above noncached content. We argue that the likelihood that viewers select content from the top of the related list is higher than selection from the bottom, and pushing contents already in the cache to the top of the related list would increase the likelihood of choosing cached content. To verify that the position on the list really is the selection criterion more dominant than the content itself, we conduct a user study with 40 YouTube-using volunteers who were presented with random related lists in their everyday YouTube use. After confirming our assumption, we analyze the benefits of our approach by an investigation that is based on two traces collected from a university campus. Our analysis shows that the proposed reordering approach for related lists would lead to a 2 to 5 times increase in cache hit rate compared to an approach without reordering the related list. This increase in hit rate would lead to reduction in server load and backend bandwidth usage, which in turn reduces the latency in streaming the video requested by the viewer and has the potential to improve the overall performance of YouTube's content distribution system. An analysis of YouTube's recommendation system reveals that related lists are created from a small pool of videos, which increases the potential for caching content from related lists and reordering based on the content in the cache.",
    "cited_by_count": 71,
    "openalex_id": "https://openalex.org/W2261219935",
    "type": "article"
  },
  {
    "title": "User Quality of Experience of Mulsemedia Applications",
    "doi": "https://doi.org/10.1145/2661329",
    "publication_date": "2014-10-01",
    "publication_year": 2014,
    "authors": "Zhenhui Yuan; Shengyang Chen; Gheorghiță Ghinea; Gabriel‐Miro Muntean",
    "corresponding_authors": "",
    "abstract": "User Quality of Experience (QoE) is of fundamental importance in multimedia applications and has been extensively studied for decades. However, user QoE in the context of the emerging multiple-sensorial media (mulsemedia) services, which involve different media components than the traditional multimedia applications, have not been comprehensively studied. This article presents the results of subjective tests which have investigated user perception of mulsemedia content. In particular, the impact of intensity of certain mulsemedia components including haptic and airflow on user-perceived experience are studied. Results demonstrate that by making use of mulsemedia the overall user enjoyment levels increased by up to 77%.",
    "cited_by_count": 69,
    "openalex_id": "https://openalex.org/W1978514417",
    "type": "article"
  },
  {
    "title": "Image Enhancement in Encrypted Domain over Cloud",
    "doi": "https://doi.org/10.1145/2656205",
    "publication_date": "2015-02-05",
    "publication_year": 2015,
    "authors": "Ankita Lathey; Pradeep K. Atrey",
    "corresponding_authors": "",
    "abstract": "Cloud-based multimedia systems are becoming increasingly common. These systems offer not only storage facility, but also high-end computing infrastructure which can be used to process data for various analysis tasks ranging from low-level data quality enhancement to high-level activity and behavior identification operations. However, cloud data centers, being third party servers, are often prone to information leakage, raising security and privacy concerns. In this article, we present a Shamir's secret sharing based method to enhance the quality of encrypted image data over cloud. Using the proposed method we show that several image enhancement operations such as noise removal, antialiasing, edge and contrast enhancement, and dehazing can be performed in encrypted domain with near-zero loss in accuracy and minimal computation and data overhead. Moreover, the proposed method is proven to be information theoretically secure.",
    "cited_by_count": 69,
    "openalex_id": "https://openalex.org/W1982743464",
    "type": "article"
  },
  {
    "title": "Personalized Emotion Recognition by Personality-Aware High-Order Learning of Physiological Signals",
    "doi": "https://doi.org/10.1145/3233184",
    "publication_date": "2019-01-24",
    "publication_year": 2019,
    "authors": "Sicheng Zhao; Amir Gholaminejad; Guiguang Ding; Yue Gao; Jungong Han; Kurt Keutzer",
    "corresponding_authors": "",
    "abstract": "Due to the subjective responses of different subjects to physical stimuli, emotion recognition methodologies from physiological signals are increasingly becoming personalized. Existing works mainly focused on modeling the involved physiological corpus of each subject, without considering the psychological factors, such as interest and personality. The latent correlation among different subjects has also been rarely examined. In this article, we propose to investigate the influence of personality on emotional behavior in a hypergraph learning framework. Assuming that each vertex is a compound tuple (subject, stimuli), multi-modal hypergraphs can be constructed based on the personality correlation among different subjects and on the physiological correlation among corresponding stimuli. To reveal the different importance of vertices, hyperedges, and modalities, we learn the weights for each of them. As the hypergraphs connect different subjects on the compound vertices, the emotions of multiple subjects can be simultaneously recognized. In this way, the constructed hypergraphs are vertex-weighted multi-modal multi-task ones. The estimated factors, referred to as emotion relevance, are employed for emotion recognition. We carry out extensive experiments on the ASCERTAIN dataset and the results demonstrate the superiority of the proposed method, as compared to the state-of-the-art emotion recognition approaches.",
    "cited_by_count": 69,
    "openalex_id": "https://openalex.org/W2913218058",
    "type": "article"
  },
  {
    "title": "Interactive Search or Sequential Browsing? A Detailed Analysis of the Video Browser Showdown 2018",
    "doi": "https://doi.org/10.1145/3295663",
    "publication_date": "2019-02-13",
    "publication_year": 2019,
    "authors": "Jakub Lokoč; Gregor Kovalčík; Bernd Münzer; Klaus Schöffmann; Werner Bailer; Ralph Gasser; Stefanos Vrochidis; Phuong Anh Nguyen; Sitapa Rujikietgumjorn; Kai Uwe Barthel",
    "corresponding_authors": "",
    "abstract": "This work summarizes the findings of the 7th iteration of the Video Browser Showdown (VBS) competition organized as a workshop at the 24th International Conference on Multimedia Modeling in Bangkok. The competition focuses on video retrieval scenarios in which the searched scenes were either previously observed or described by another person (i.e., an example shot is not available). During the event, nine teams competed with their video retrieval tools in providing access to a shared video collection with 600 hours of video content. Evaluation objectives, rules, scoring, tasks, and all participating tools are described in the article. In addition, we provide some insights into how the different teams interacted with their video browsers, which was made possible by a novel interaction logging mechanism introduced for this iteration of the VBS. The results collected at the VBS evaluation server confirm that searching for one particular scene in the collection when given a limited time is still a challenging task for many of the approaches that were showcased during the event. Given only a short textual description, finding the correct scene is even harder. In ad hoc search with multiple relevant scenes, the tools were mostly able to find at least one scene, whereas recall was the issue for many teams. The logs also reveal that even though recent exciting advances in machine learning narrow the classical semantic gap problem, user-centric interfaces are still required to mediate access to specific content. Finally, open challenges and lessons learned are presented for future VBS events.",
    "cited_by_count": 65,
    "openalex_id": "https://openalex.org/W2911926260",
    "type": "article"
  },
  {
    "title": "A Benchmark Dataset and Comparison Study for Multi-modal Human Action Analytics",
    "doi": "https://doi.org/10.1145/3365212",
    "publication_date": "2020-05-22",
    "publication_year": 2020,
    "authors": "Jiaying Liu; Sijie Song; Chunhui Liu; Yanghao Li; Yueyu Hu",
    "corresponding_authors": "",
    "abstract": "Large-scale benchmarks provide a solid foundation for the development of action analytics. Most of the previous activity benchmarks focus on analyzing actions in RGB videos. There is a lack of large-scale and high-quality benchmarks for multi-modal action analytics. In this article, we introduce PKU Multi-Modal Dataset (PKU-MMD), a new large-scale benchmark for multi-modal human action analytics. It consists of about 28,000 action instances and 6.2 million frames in total and provides high-quality multi-modal data sources, including RGB, depth, infrared radiation (IR), and skeletons. To make PKU-MMD more practical, our dataset comprises two subsets under different settings for action understanding, namely Part I and Part II. Part I contains 1,076 untrimmed video sequences with 51 action classes performed by 66 subjects, while Part II contains 1,009 untrimmed video sequences with 41 action classes performed by 13 subjects. Compared to Part I, Part II is more challenging due to short action intervals, concurrent actions and heavy occlusion. PKU-MMD can be leveraged in two scenarios: action recognition with trimmed video clips and action detection with untrimmed video sequences. For each scenario, we provide benchmark performance on both subsets by conducting different methods with different modalities under two evaluation protocols, respectively. Experimental results show that PKU-MMD is a significant challenge to many state-of-the-art methods. We further illustrate that the features learned on PKU-MMD can be well transferred to other datasets. We believe this large-scale dataset will boost the research in the field of action analytics for the community.",
    "cited_by_count": 63,
    "openalex_id": "https://openalex.org/W3033793613",
    "type": "article"
  },
  {
    "title": "Data Hiding",
    "doi": "https://doi.org/10.1145/3382772",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "Amit Kumar Singh",
    "corresponding_authors": "Amit Kumar Singh",
    "abstract": "With the widespread growth of digital information and improved internet technologies, the demand for improved information security techniques has significantly increased due to privacy leakage, identity theft, illegal copying, and data distribution. Because of this, data hiding approaches have received much attention in several application areas. However, those approaches are unable to solve many issues that are necessary to measure in future investigations. This survey provides a comprehensive survey on data hiding techniques and their new trends for solving new challenges in real-world applications. The notable applications are telemedicine, 3D objects, mobile devices, cloud/distributed computing and data mining environments, chip and hardware protection, cyber physical systems, internet traffic, fusion of watermarking and encryption, joint compression and watermarking, biometric watermarking, watermarking at the physical layer, and many other perspectives. Further, the potential issues that existing approaches of data hiding face are identified. I believe that this survey will provide a valuable source of information for finding research directions for fledgling researchers and developers.",
    "cited_by_count": 63,
    "openalex_id": "https://openalex.org/W3034861701",
    "type": "article"
  },
  {
    "title": "A Weakly Supervised Semantic Segmentation Network by Aggregating Seed Cues: The Multi-Object Proposal Generation Perspective",
    "doi": "https://doi.org/10.1145/3419842",
    "publication_date": "2021-01-31",
    "publication_year": 2021,
    "authors": "Junsheng Xiao; Huahu Xu; Honghao Gao; Minjie Bian; Yang Li",
    "corresponding_authors": "",
    "abstract": "Weakly supervised semantic segmentation under image-level annotations is effectiveness for real-world applications. The small and sparse discriminative regions obtained from an image classification network that are typically used as the important initial location of semantic segmentation also form the bottleneck. Although deep convolutional neural networks (DCNNs) have exhibited promising performances for single-label image classification tasks, images of the real-world usually contain multiple categories, which is still an open problem. So, the problem of obtaining high-confidence discriminative regions from multi-label classification networks remains unsolved. To solve this problem, this article proposes an innovative three-step framework within the perspective of multi-object proposal generation. First, an image is divided into candidate boxes using the object proposal method. The candidate boxes are sent to a single-classification network to obtain the discriminative regions. Second, the discriminative regions are aggregated to obtain a high-confidence seed map. Third, the seed cues grow on the feature maps of high-level semantics produced by a backbone segmentation network. Experiments are carried out on the PASCAL VOC 2012 dataset to verify the effectiveness of our approach, which is shown to outperform other baseline image segmentation methods.",
    "cited_by_count": 62,
    "openalex_id": "https://openalex.org/W3144064738",
    "type": "article"
  },
  {
    "title": "Cloud Gaming with Foveated Video Encoding",
    "doi": "https://doi.org/10.1145/3369110",
    "publication_date": "2020-02-17",
    "publication_year": 2020,
    "authors": "Gazi Illahi; Thomas van Gemert; Matti Siekkinen; Enrico Masala; Antti Oulasvirta; Antti Ylä-Jääski",
    "corresponding_authors": "",
    "abstract": "Cloud gaming enables playing high-end games, originally designed for PC or game console setups, on low-end devices such as netbooks and smartphones, by offloading graphics rendering to GPU-powered cloud servers. However, transmitting the high-resolution video requires a large amount of network bandwidth, even though it is a compressed video stream. Foveated video encoding (FVE) reduces the bandwidth requirement by taking advantage of the non-uniform acuity of human visual system and by knowing where the user is looking. Based on a consumer-grade real-time eye tracker and an open source cloud gaming platform, we provide a cloud gaming FVE prototype that is game-agnostic and requires no modifications to the underlying game engine. In this article, we describe the prototype and its evaluation through measurements with representative games from different genres to understand the effect of parametrization of the FVE scheme on bandwidth requirements and to understand its feasibility from the latency perspective. We also present results from a user study on first-person shooter games. The results suggest that it is possible to find a “sweet spot” for the encoding parameters so the users hardly notice the presence of foveated encoding but at the same time the scheme yields most of the achievable bandwidth savings.",
    "cited_by_count": 61,
    "openalex_id": "https://openalex.org/W3009470780",
    "type": "article"
  },
  {
    "title": "Resilient Color Image Watermarking Using Accurate Quaternion Radial Substituted Chebyshev Moments",
    "doi": "https://doi.org/10.1145/3325193",
    "publication_date": "2019-05-31",
    "publication_year": 2019,
    "authors": "Khalid M. Hosny; Mohamed Darwish",
    "corresponding_authors": "",
    "abstract": "In this work, a new quaternion-based method for color image watermarking is proposed. In this method, a novel set of quaternion radial substituted Chebyshev moments (QRSCMs) is presented for robust geometrically invariant image watermarking. An efficient computational method is proposed for highly accurate, fast, and numerically stable QRSCMs in polar coordinates. The proposed watermarking method consists of three stages. In the first stage, the Arnold transform is used to improve the security of the watermarking scheme by scrambling the binary watermark. In the second stage, the proposed accurate and stable QRSCMs of the host color image are computed. In the third stage, the encrypted binary watermark is embedded into the host image by employing the quantization technique on selected-magnitude QRSCMs where the watermarked color image is obtained by adding the original host color image to the compensation image. Then, the binary watermark can be extracted directly without using the original image from the magnitudes of QRSCMs. Numerical experiments are performed where the performance of proposed method is compared with the existing quaternion moment-based watermarking methods. The comparison clearly shows that the proposed method is very efficient in terms of the visual imperceptibility capability and the robustness under different attacks compared to the existing quaternion moment-based watermarking algorithms.",
    "cited_by_count": 60,
    "openalex_id": "https://openalex.org/W2951380443",
    "type": "article"
  },
  {
    "title": "Trust Mechanism of Feedback Trust Weight in Multimedia Network",
    "doi": "https://doi.org/10.1145/3391296",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "Zhihan Lv; Houbing Song",
    "corresponding_authors": "",
    "abstract": "It is necessary to solve the inaccurate data arising from data reliability ignored by most data fusion algorithms drawing upon collaborative filtering and fuzzy network theory. Therefore, a model is constructed based on the collaborative filtering algorithm and fuzzy network theory to calculate the node trust value as the weight of weighted data fusion. First, a FTWDF (Feedback Trust Weighted for Data Fusion) is proposed. Second, EEFA (Efficiency unequal Fuzzy clustering Algorithm ) is introduced into FTWDF considering the defects of the clustering structure caused by ignoring the randomness of node energy consumption and cluster head selection in the practical application of the existing data fusion algorithm. Besides, the fuzzy logic is applied to cluster head selection and node clustering. Finally, an FTWDF-EEFA clustering algorithm is constructed for generating candidate cluster head nodes, which is verified by simulation experiments. The comparative analysis reveals that the accuracy of the FTWDF-EEFA clustering algorithm is 4.1% higher than that of the TMDF (Trust Multiple attributes Decision-making-based data Fusion) algorithm, and 8.3% higher than that of LDTS ( Larger Data fusion based on node Trust evaluation in wireless Sensor networks) algorithm. It performs better in accuracy and recommendation results during the processing of ML100M dataset and NF5M dataset. Besides, the new clustering algorithm increases the survival time of nodes when analyzing the number of death nodes to prolong networks’ lifespan. It improves the survival period of nodes, balances the network load, and prolongs networks’ lifespan. Furthermore, the FTWDF-EEFA clustering algorithm can balance nodes’ energy consumption and effectively save nodes’ overall energy through analysis. Therefore, the optimized algorithm can increase the lifespan of network and improve the trust mechanism effectively. The performance of the algorithm has reached the expected effect, providing a reference for the practical application of the trust mechanism in networks.",
    "cited_by_count": 60,
    "openalex_id": "https://openalex.org/W3041940313",
    "type": "article"
  },
  {
    "title": "Affective Computing for Large-scale Heterogeneous Multimedia Data",
    "doi": "https://doi.org/10.1145/3363560",
    "publication_date": "2019-11-30",
    "publication_year": 2019,
    "authors": "Sicheng Zhao; Shangfei Wang; Mohammad Soleymani; Dhiraj Joshi; Qiang Ji",
    "corresponding_authors": "",
    "abstract": "The wide popularity of digital photography and social networks has generated a rapidly growing volume of multimedia data (i.e., image, music, and video), resulting in a great demand for managing, retrieving, and understanding these data. Affective computing (AC) of these data can help to understand human behaviors and enable wide applications. In this article, we survey the state-of-the-art AC technologies comprehensively for large-scale heterogeneous multimedia data. We begin this survey by introducing the typical emotion representation models from psychology that are widely employed in AC. We briefly describe the available datasets for evaluating AC algorithms. We then summarize and compare the representative methods on AC of different multimedia types, i.e., images, music, videos, and multimodal data, with the focus on both handcrafted features-based methods and deep learning methods. Finally, we discuss some challenges and future directions for multimedia affective computing.",
    "cited_by_count": 60,
    "openalex_id": "https://openalex.org/W3125615400",
    "type": "article"
  },
  {
    "title": "Security and Privacy of Patient Information in Medical Systems Based on Blockchain Technology",
    "doi": "https://doi.org/10.1145/3408321",
    "publication_date": "2021-06-14",
    "publication_year": 2021,
    "authors": "Hongjiao Wu; Ashutosh Dhar Dwivedi; Gautam Srivastava",
    "corresponding_authors": "",
    "abstract": "The essence of “blockchain” is a shared database in which information stored is un-falsifiable, traceable, open, and transparent. Therefore, to improve the security of private information in medical systems, this article uses blockchain technology to design a method to protect private information in medical systems and effectively realize anti-theft control of private information. First, the Patient-oriented Privacy Preserving Access Control model is introduced into the access control process of private information in medical systems. Next, a private information storage platform is built by using blockchain technology, and information transmission is realized using standard cryptographic algorithms. In this process, file authorization contracts are also used to guarantee the security of private information and further prevent theft of medical private information. Our simulation results show that the storage response time of this method is kept below 1,000 ms, and the maximum information throughput rate reaches 550 kbit/s, which indicates that this method has strong performance in information storage and transmission efficiency. Moreover, the reliability and bandwidth utilization of data transmission across domains is higher, so the method has higher information security control performance and superior overall performance.",
    "cited_by_count": 60,
    "openalex_id": "https://openalex.org/W3171599904",
    "type": "article"
  },
  {
    "title": "eDiaPredict: An Ensemble-based Framework for Diabetes Prediction",
    "doi": "https://doi.org/10.1145/3415155",
    "publication_date": "2021-06-14",
    "publication_year": 2021,
    "authors": "Ashima Singh; A.P. Dhillon; Neeraj Kumar; M. Shamim Hossain; Ghulam Muhammad; Manoj Kumar",
    "corresponding_authors": "",
    "abstract": "Medical systems incorporate modern computational intelligence in healthcare. Machine learning techniques are applied to predict the onset and reoccurrence of the disease, identify biomarkers for survivability analysis depending upon certain health conditions of the patient. Early prediction of diseases like diabetes is essential as the number of diabetic patients of all age groups is increasing rapidly. To identify underlying reasons for the onset of diabetes in its early stage has become a challenging task for medical practitioners. Continuously increasing diabetic patient data has necessitated for the applications of efficient machine learning algorithms, which learns from the trends of the underlying data and recognizes the critical conditions in patients. In this article, an ensemble-based framework named e DiaPredict is proposed. It uses ensemble modeling, which includes an ensemble of different machine learning algorithms comprising XGBoost, Random Forest, Support Vector Machine, Neural Network, and Decision tree to predict diabetes status among patients. The performance of eDiaPredict has been evaluated using various performance parameters like accuracy, sensitivity, specificity, Gini Index, precision, area under curve, area under convex hull, minimum error rate, and minimum weighted coefficient. The effectiveness of the proposed approach is shown by its application on the PIMA Indian diabetes dataset wherein an accuracy of 95% is achieved.",
    "cited_by_count": 58,
    "openalex_id": "https://openalex.org/W3167375992",
    "type": "article"
  },
  {
    "title": "Gaussian Mixture Model Clustering with Incomplete Data",
    "doi": "https://doi.org/10.1145/3408318",
    "publication_date": "2021-01-31",
    "publication_year": 2021,
    "authors": "Yi Zhang; Miaomiao Li; Siwei Wang; Sisi Dai; Lei Luo; En Zhu; Huiying Xu; Xinzhong Zhu; Chaoyun Yao; Haoran Zhou",
    "corresponding_authors": "",
    "abstract": "Gaussian mixture model (GMM) clustering has been extensively studied due to its effectiveness and efficiency. Though demonstrating promising performance in various applications, it cannot effectively address the absent features among data, which is not uncommon in practical applications. In this article, different from existing approaches that first impute the absence and then perform GMM clustering tasks on the imputed data, we propose to integrate the imputation and GMM clustering into a unified learning procedure. Specifically, the missing data is filled by the result of GMM clustering, and the imputed data is then taken for GMM clustering. These two steps alternatively negotiate with each other to achieve optimum. By this way, the imputed data can best serve for GMM clustering. A two-step alternative algorithm with proved convergence is carefully designed to solve the resultant optimization problem. Extensive experiments have been conducted on eight UCI benchmark datasets, and the results have validated the effectiveness of the proposed algorithm.",
    "cited_by_count": 57,
    "openalex_id": "https://openalex.org/W3151630303",
    "type": "article"
  },
  {
    "title": "A Multimodal, Multimedia Point-of-Care Deep Learning Framework for COVID-19 Diagnosis",
    "doi": "https://doi.org/10.1145/3421725",
    "publication_date": "2021-01-31",
    "publication_year": 2021,
    "authors": "Md. Abdur Rahman; M. Shamim Hossain; Nabil Alrajeh; Brij B. Gupta",
    "corresponding_authors": "",
    "abstract": "In this article, we share our experiences in designing and developing a suite of deep neural network–(DNN) based COVID-19 case detection and recognition framework. Existing pathological tests such as RT-PCR-based pathogen RNA detection from nasal swabbing seem to display low detection rates during the early stages of virus contraction. Moreover, the reliance on a few overburdened laboratories based around an epicenter capable of supplying large numbers of RT-PCR tests makes this testing method non-scalable when the rate of infections is high. Similarly, finding an effective drug or vaccine with which to combat COVID-19 requires a long time and many clinical trials. The development of pathological COVID-19 tests is hindered by shortages in the supply chain of chemical reagents necessary for testing on a large scale. This diminishes the speed of diagnosis and the ability to filter out COVID-19 positive patients from uninfected patients on a national level. Existing research has shown that DNN has been successful in identifying COVID-19 from radiological media such as CT scans and X-ray images, audio media such as cough sounds, optical coherence tomography to identify conjunctivitis and pink eye symptoms on the ocular surface, body temperature measurement using smartphone fingerprint sensors or thermal cameras, the use of live facial detection to identify safe social distancing practices from camera images, and face mask detection from camera images. We also investigate the utility of federated learning in diagnosis cases where private data can be trained via edge learning. These point-of-care modalities can be integrated with DNN-based RT-PCR laboratory test results to assimilate multiple modalities of COVID-19 detection and thereby provide more dimensions of diagnosis. Finally, we will present our initial test results, which are encouraging.",
    "cited_by_count": 51,
    "openalex_id": "https://openalex.org/W3143375397",
    "type": "article"
  },
  {
    "title": "HCMSL: Hybrid Cross-modal Similarity Learning for Cross-modal Retrieval",
    "doi": "https://doi.org/10.1145/3412847",
    "publication_date": "2021-01-31",
    "publication_year": 2021,
    "authors": "Chengyuan Zhang; Jiayu Song; Xiaofeng Zhu; Lei Zhu; Shichao Zhang",
    "corresponding_authors": "",
    "abstract": "The purpose of cross-modal retrieval is to find the relationship between different modal samples and to retrieve other modal samples with similar semantics by using a certain modal sample. As the data of different modalities presents heterogeneous low-level feature and semantic-related high-level features, the main problem of cross-modal retrieval is how to measure the similarity between different modalities. In this article, we present a novel cross-modal retrieval method, named Hybrid Cross-Modal Similarity Learning model (HCMSL for short). It aims to capture sufficient semantic information from both labeled and unlabeled cross-modal pairs and intra-modal pairs with same classification label. Specifically, a coupled deep fully connected networks are used to map cross-modal feature representations into a common subspace. Weight-sharing strategy is utilized between two branches of networks to diminish cross-modal heterogeneity. Furthermore, two Siamese CNN models are employed to learn intra-modal similarity from samples of same modality. Comprehensive experiments on real datasets clearly demonstrate that our proposed technique achieves substantial improvements over the state-of-the-art cross-modal retrieval techniques.",
    "cited_by_count": 51,
    "openalex_id": "https://openalex.org/W3159088493",
    "type": "article"
  },
  {
    "title": "Is the Reign of Interactive Search Eternal? Findings from the Video Browser Showdown 2020",
    "doi": "https://doi.org/10.1145/3445031",
    "publication_date": "2021-07-22",
    "publication_year": 2021,
    "authors": "Jakub Lokoč; Patrik Veselý; František Mejzlík; Gregor Kovalčík; Tomáš Souček; Luca Rossetto; Klaus Schoeffmann; Werner Bailer; Cathal Gurrin; Loris Sauter; Jaeyub Song; Stefanos Vrochidis; Jiaxin Wu; Björn Þór Jónsson",
    "corresponding_authors": "",
    "abstract": "Comprehensive and fair performance evaluation of information retrieval systems represents an essential task for the current information age. Whereas Cranfield-based evaluations with benchmark datasets support development of retrieval models, significant evaluation efforts are required also for user-oriented systems that try to boost performance with an interactive search approach. This paper presents findings from the 9th Video Browser Showdown, a competition that focuses on a legitimate comparison of interactive search systems designed for challenging known-item search tasks over a large video collection. During previous installments of the competition, the interactive nature of participating systems was a key feature to satisfy known-item search needs and this paper continues to support this hypothesis. Despite the fact that top-performing systems integrate the most recent deep learning models into their retrieval process, interactive searching remains a necessary component of successful strategies for known-item search tasks. Alongside the description of competition settings, evaluated tasks, participating teams, and overall results, this paper presents a detailed analysis of query logs collected by the top three performing systems SOMHunter, VIRET, and vitrivr. The analysis provides a quantitative insight to the observed performance of the systems and constitutes a new baseline methodology for future events. The results reveal that the top two systems mostly relied on temporal queries before a correct frame was identified. An interaction log analysis complements the result log findings and points to the importance of result set and video browsing approaches. Finally, various outlooks are discussed in order to improve the Video Browser Showdown challenge in the future.",
    "cited_by_count": 51,
    "openalex_id": "https://openalex.org/W3183241901",
    "type": "article"
  },
  {
    "title": "xCos: An Explainable Cosine Metric for Face Verification Task",
    "doi": "https://doi.org/10.1145/3469288",
    "publication_date": "2021-10-31",
    "publication_year": 2021,
    "authors": "Yu‐Sheng Lin; Zhe-Yu Liu; Yuan Chen; Yu-Siang Wang; Ya-Liang Chang; Winston H. Hsu",
    "corresponding_authors": "",
    "abstract": "We study the XAI (explainable AI) on the face recognition task, particularly the face verification. Face verification has become a crucial task in recent days and it has been deployed to plenty of applications, such as access control, surveillance, and automatic personal log-on for mobile devices. With the increasing amount of data, deep convolutional neural networks can achieve very high accuracy for the face verification task. Beyond exceptional performances, deep face verification models need more interpretability so that we can trust the results they generate. In this article, we propose a novel similarity metric, called explainable cosine ( xCos ), that comes with a learnable module that can be plugged into most of the verification models to provide meaningful explanations. With the help of xCos , we can see which parts of the two input faces are similar, where the model pays its attention to, and how the local similarities are weighted to form the output xCos score. We demonstrate the effectiveness of our proposed method on LFW and various competitive benchmarks, not only resulting in providing novel and desirable model interpretability for face verification but also ensuring the accuracy as plugging into existing face recognition models.",
    "cited_by_count": 50,
    "openalex_id": "https://openalex.org/W3211809588",
    "type": "article"
  },
  {
    "title": "Integrating Scene Semantic Knowledge into Image Captioning",
    "doi": "https://doi.org/10.1145/3439734",
    "publication_date": "2021-05-11",
    "publication_year": 2021,
    "authors": "Haiyang Wei; Zhixin Li; Feicheng Huang; Canlong Zhang; Huifang Ma; Zhongzhi Shi",
    "corresponding_authors": "",
    "abstract": "Most existing image captioning methods use only the visual information of the image to guide the generation of captions, lack the guidance of effective scene semantic information, and the current visual attention mechanism cannot adjust the focus intensity on the image. In this article, we first propose an improved visual attention model. At each timestep, we calculated the focus intensity coefficient of the attention mechanism through the context information of the model, then automatically adjusted the focus intensity of the attention mechanism through the coefficient to extract more accurate visual information. In addition, we represented the scene semantic knowledge of the image through topic words related to the image scene, then added them to the language model. We used the attention mechanism to determine the visual information and scene semantic information that the model pays attention to at each timestep and combined them to enable the model to generate more accurate and scene-specific captions. Finally, we evaluated our model on Microsoft COCO (MSCOCO) and Flickr30k standard datasets. The experimental results show that our approach generates more accurate captions and outperforms many recent advanced models in various evaluation metrics.",
    "cited_by_count": 47,
    "openalex_id": "https://openalex.org/W3162954998",
    "type": "article"
  },
  {
    "title": "Uncertainty-Aware Semi-Supervised Method Using Large Unlabeled and Limited Labeled COVID-19 Data",
    "doi": "https://doi.org/10.1145/3462635",
    "publication_date": "2021-10-31",
    "publication_year": 2021,
    "authors": "Roohallah Alizadehsani; Danial Sharifrazi; Navid Hoseini Izadi; Javad Hassannataj Joloudari; Afshin Shoeibi; J. M. Górriz; Sadiq Hussain; Juan E. Arco; Zahra Alizadeh Sani; Fahime Khozeimeh; Abbas Khosravi; Saeid Nahavandi; Sheikh Mohammed Shariful Islam; U. Rajendra Acharya",
    "corresponding_authors": "",
    "abstract": "The new coronavirus has caused more than one million deaths and continues to spread rapidly. This virus targets the lungs, causing respiratory distress which can be mild or severe. The X-ray or computed tomography ( CT ) images of lungs can reveal whether the patient is infected with COVID-19 or not. Many researchers are trying to improve COVID-19 detection using artificial intelligence. Our motivation is to develop an automatic method that can cope with scenarios in which preparing labeled data is time consuming or expensive. In this article, we propose a Semi-supervised Classification using Limited Labeled Data ( SCLLD ) relying on Sobel edge detection and Generative Adversarial Networks ( GANs ) to automate the COVID-19 diagnosis. The GAN discriminator output is a probabilistic value which is used for classification in this work. The proposed system is trained using 10,000 CT scans collected from Omid Hospital, whereas a public dataset is also used for validating our system. The proposed method is compared with other state-of-the-art supervised methods such as Gaussian processes. To the best of our knowledge, this is the first time a semi-supervised method for COVID-19 detection is presented. Our system is capable of learning from a mixture of limited labeled and unlabeled data where supervised learners fail due to a lack of sufficient amount of labeled data. Thus, our semi-supervised training method significantly outperforms the supervised training of Convolutional Neural Network ( CNN ) when labeled training data is scarce. The 95% confidence intervals for our method in terms of accuracy, sensitivity, and specificity are 99.56 ± 0.20%, 99.88 ± 0.24%, and 99.40 ± 0.18%, respectively, whereas intervals for the CNN (trained supervised) are 68.34 ± 4.11%, 91.2 ± 6.15%, and 46.40 ± 5.21%.",
    "cited_by_count": 46,
    "openalex_id": "https://openalex.org/W3214479671",
    "type": "article"
  },
  {
    "title": "Explanation-Driven HCI Model to Examine the Mini-Mental State for Alzheimer’s Disease",
    "doi": "https://doi.org/10.1145/3527174",
    "publication_date": "2022-04-01",
    "publication_year": 2022,
    "authors": "Loveleen Gaur; Mohan Bhandari; Bhadwal Singh Shikhar; N. Z. Jhanjhi; Mohammad Shorfuzzaman; Mehedi Masud",
    "corresponding_authors": "",
    "abstract": "Directing research on Alzheimer’s disease toward only early prediction and accuracy cannot be considered a feasible approach toward tackling a ubiquitous degenerative disease today. Applying deep learning (DL), Explainable artificial intelligence, and advancing toward the human-computer interface (HCI) model can be a leap forward in medical research. This research aims to propose a robust explainable HCI model using SHAPley additive explanation, local interpretable model-agnostic explanations, and DL algorithms. The use of DL algorithms—logistic regression (80.87%), support vector machine (85.8%), k -nearest neighbor (87.24%), multilayer perceptron (91.94%), and decision tree (100%)—and explainability can help in exploring untapped avenues for research in medical sciences that can mold the future of HCI models. The presented model’s results show improved prediction accuracy by incorporating a user-friendly computer interface into decision-making, implying a high significance level in the context of biomedical and clinical research.",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W4220965327",
    "type": "article"
  },
  {
    "title": "Disentangling Features for Fashion Recommendation",
    "doi": "https://doi.org/10.1145/3531017",
    "publication_date": "2022-04-20",
    "publication_year": 2022,
    "authors": "Lavinia De Divitiis; Federico Becattini; Claudio Baecchi; Alberto Del Bimbo",
    "corresponding_authors": "",
    "abstract": "Online stores have become fundamental for the fashion industry, revolving around recommendation systems to suggest appropriate items to customers. Such recommendations often suffer from a lack of diversity and propose items that are similar to previous purchases of a user. Recently, a novel kind of approach based on Memory Augmented Neural Networks (MANNs) has been proposed, aimed at recommending a variety of garments to create an outfit by complementing a given fashion item. In this article we address the task of compatible garment recommendation developing a MANN architecture by taking into account the co-occurrence of clothing attributes, such as shape and color, to compose an outfit. To this end we obtain disentangled representations of fashion items and store them in external memory modules, used to guide recommendations at inference time. We show that our disentangled representations are able to achieve significantly better performance compared to the state of the art and also provide interpretable latent spaces, giving a qualitative explanation of the recommendations.",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W4224273707",
    "type": "article"
  },
  {
    "title": "Double Attention Based on Graph Attention Network for Image Multi-Label Classification",
    "doi": "https://doi.org/10.1145/3519030",
    "publication_date": "2022-03-12",
    "publication_year": 2022,
    "authors": "Wei Zhou; Zhiwu Xia; Peng Dou; Tao Su; Haifeng Hu",
    "corresponding_authors": "",
    "abstract": "The task of image multi-label classification is to accurately recognize multiple objects in an input image. Most of the recent works need to leverage the label co-occurrence matrix counted from training data to construct the graph structure, which are inflexible and may degrade model generalizability. In addition, these methods fail to capture the semantic correlation between the channel feature maps to further improve model performance. To address these issues, we propose DA-GAT (a D ouble A ttention framework based on the G raph A ttention ne T work) to effectively learn the correlation between labels from training data. First, we devise a new channel attention mechanism to enhance the semantic correlation between channel feature maps, so as to implicitly capture the correlation between labels. Second, we propose a new label attention mechanism to avoid the adverse impact of a manually constructed label co-occurrence matrix. It only needs to leverage the label embedding as the input of network, then automatically constructs the label relation matrix to explicitly establish the correlation between labels. Finally, we effectively fuse the output of these two attention mechanisms to further improve model performance. Extensive experiments are conducted on three public multi-label classification benchmarks. Our DA-GAT model achieves mean average precision of 87.1%, 96.6%, and 64.3% on MS-COCO 2014, PASCAL VOC 2007, and NUS-WIDE, respectively, and obviously outperforms other existing state-of-the-art methods. In addition, visual analysis experiments demonstrate that each attention mechanism can capture the correlation between labels well and significantly promote the model performance.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W4221079634",
    "type": "article"
  },
  {
    "title": "Hyper-node Relational Graph Attention Network for Multi-modal Knowledge Graph Completion",
    "doi": "https://doi.org/10.1145/3545573",
    "publication_date": "2022-06-30",
    "publication_year": 2022,
    "authors": "Shuang Liang; Anjie Zhu; Jiasheng Zhang; Jie Shao",
    "corresponding_authors": "",
    "abstract": "Knowledge graphs often suffer from incompleteness, and knowledge graph completion (KGC) aims at inferring the missing triplets through knowledge graph embedding from known factual triplets. However, most existing knowledge graph embedding methods only use the relational information of knowledge graph and treat the entities and relations as IDs with simple embedding layer, ignoring the multi-modal information among triplets, such as text descriptions, images, etc. In this work, we propose a novel network to incorporate different modal information with graph structure information for more precise representation of multi-modal knowledge graph, termed as hyper-node relational graph attention (HRGAT) network. In HRGAT, we use low-rank multi-modal fusion to model the intra-modality and inter-modality dynamics, which transforms the original knowledge graph to a hyper-node graph. Then, relational graph attention (RGAT) network is used, which contains relation-specific attention and entity-relation fusion operation to capture the graph structure information. Finally, we aggregate the updated multi-modal information and graph structure information to generate the final embeddings of knowledge graph to achieve KGC. By exploring multi-modal information and graph structure information, HRGAT embraces faster convergence speed and achieves the state-of-the-art for KGC on the standard datasets. Implementation code is available at https://github.com/broliang/HRGAT.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W4283730841",
    "type": "article"
  },
  {
    "title": "A DNA Based Colour Image Encryption Scheme Using A Convolutional Autoencoder",
    "doi": "https://doi.org/10.1145/3570165",
    "publication_date": "2022-11-03",
    "publication_year": 2022,
    "authors": "Fawad Ahmed; Muneeb Ur Rehman; Jawad Ahmad; Muhammad Shahbaz Khan; Wadii Boulila; Gautam Srivastava; Jerry Chun‐Wei Lin; William J. Buchanan",
    "corresponding_authors": "",
    "abstract": "With the advancement in technology, digital images can easily be transmitted and stored over the Internet. Encryption is used to avoid illegal interception of digital images. Encrypting large-sized colour images in their original dimension generally results in low encryption/decryption speed along with exerting a burden on the limited bandwidth of the transmission channel. To address the aforementioned issues, a new encryption scheme for colour images employing convolutional autoencoder, DNA and chaos is presented in this paper. The proposed scheme has two main modules, the dimensionality conversion module using the proposed convolutional autoencoder, and the encryption/decryption module using DNA and chaos. The dimension of the input colour image is first reduced from N × M × 3 to P × Q gray-scale image using the encoder. Encryption and decryption are then performed in the reduced dimension space. The decrypted gray-scale image is upsampled to obtain the original colour image having dimension N × M × 3 . The training and validation accuracy of the proposed autoencoder is 97% and 95%, respectively. Once the autoencoder is trained, it can be used to reduce and subsequently increase the dimension of any arbitrary input colour image. The efficacy of the designed autoencoder has been demonstrated by the successful reconstruction of the compressed image into the original colour image with negligible perceptual distortion. The second major contribution presented in this paper is an image encryption scheme using DNA along with multiple chaotic sequences and substitution boxes. The security of the proposed image encryption algorithm has been gauged using several evaluation parameters, such as histogram of the cipher image, entropy, NPCR, UACI, key sensitivity, contrast, and so on. The experimental results of the proposed scheme demonstrate its effectiveness to perform colour image encryption.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W4308437198",
    "type": "article"
  },
  {
    "title": "Deepfake Video Detection via Predictive Representation Learning",
    "doi": "https://doi.org/10.1145/3536426",
    "publication_date": "2022-05-16",
    "publication_year": 2022,
    "authors": "Shiming Ge; Fanzhao Lin; Chenyu Li; Daichi Zhang; Weiping Wang; Dan Zeng",
    "corresponding_authors": "",
    "abstract": "Increasingly advanced deepfake approaches have made the detection of deepfake videos very challenging. We observe that the general deepfake videos often exhibit appearance-level temporal inconsistencies in some facial components between frames, resulting in discriminative spatiotemporal latent patterns among semantic-level feature maps. Inspired by this finding, we propose a predictive representative learning approach termed Latent Pattern Sensing to capture these semantic change characteristics for deepfake video detection. The approach cascades a Convolution Neural Network-based encoder, a ConvGRU-based aggregator, and a single-layer binary classifier. The encoder and aggregator are pretrained in a self-supervised manner to form the representative spatiotemporal context features. Then, the classifier is trained to classify the context features, distinguishing fake videos from real ones. Finally, we propose a selective self-distillation fine-tuning method to further improve the robustness and performance of the detector. In this manner, the extracted features can simultaneously describe the latent patterns of videos across frames spatially and temporally in a unified way, leading to an effective and robust deepfake video detector. Extensive experiments and comprehensive analysis prove the effectiveness of our approach, e.g., achieving a very highest Area Under Curve (AUC) score of 99.94% on FaceForensics++ benchmark and surpassing 12 states of the art at least 7.90%@AUC and 8.69%@AUC on challenging DFDC and Celeb-DF(v2) benchmarks, respectively.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W4280579728",
    "type": "article"
  },
  {
    "title": "A Multimodal Framework for Large-Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals",
    "doi": "https://doi.org/10.1145/3490686",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Guanghao Yin; Shouqian Sun; Dian Yu; Dejian Li; Kejun Zhang",
    "corresponding_authors": "",
    "abstract": "Considerable attention has been paid to physiological signal-based emotion recognition in the field of affective computing. For reliability and user-friendly acquisition, electrodermal activity (EDA) has a great advantage in practical applications. However, EDA-based emotion recognition with large-scale subjects is still a tough problem. The traditional well-designed classifiers with hand-crafted features produce poorer results because of their limited representation abilities. And the deep learning models with auto feature extraction suffer the overfitting drop-off because of large-scale individual differences. Since music has a strong correlation with human emotion, static music can be involved as the external benchmark to constrain various dynamic EDA signals. In this article, we make an attempt by fusing the subject’s individual EDA features and the external evoked music features. And we propose an end-to-end multimodal framework, the one-dimensional residual temporal and channel attention network (RTCAN-1D). For EDA features, the channel-temporal attention mechanism for EDA-based emotion recognition is first involved in mine the temporal and channel-wise dynamic and steady features. The comparisons with single EDA-based SOTA models on DEAP and AMIGOS datasets prove the effectiveness of RTCAN-1D to mine EDA features. For music features, we simply process the music signal with the open-source toolkit openSMILE to obtain external feature vectors. We conducted systematic and extensive evaluations. The experiments on the current largest music emotion dataset PMEmo validate that the fusion of EDA and music is a reliable and efficient solution for large-scale emotion recognition.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W4214910852",
    "type": "article"
  },
  {
    "title": "Accelerating Transform Algorithm Implementation for Efficient Intra Coding of 8K UHD Videos",
    "doi": "https://doi.org/10.1145/3507970",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Yang Guo; Wei Gao; Siwei Ma; Ge Li",
    "corresponding_authors": "",
    "abstract": "Real-time ultra-high-definition (UHD) video applications have attracted much attention, where the encoder side urgently demands the high-throughput two-dimensional (2D) transform hardware implementation for the latest video coding standards. This article proposes an effective acceleration method for transform algorithm in UHD intra coding based on the third generation of audio video coding standard (AVS3). First, by conducting detailed statistical analysis, we devise an efficient hardware-friendly transform algorithm that can reduce running cycles and resource consumption remarkably. Second, to implement multiplierless computation for saving resources and power, a series of shift-and-add unit (SAU) hardwares are investigated to have much less adoptions of shifters and adders than the existing methods. Third, different types of hardware acceleration methods, including calculation pipelining, logical-loop unrolling, and module-level parallelism, are designed to efficaciously support the data-intensive high frame-rate 8K UHD video coding. Finally, due to the scarcity of 8K video sources, we also provide a new dataset for the performance verification. Experimental results demonstrate that our proposed method can effectively fulfill the real-time 8K intra encoding at beyond 60 fps, with very negligible loss on rate-distortion (R-D) performance, which is averagely 0.98% Bjontegaard-Delta Bit-Rate (BD-BR).",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W4214923300",
    "type": "article"
  },
  {
    "title": "Scenario-Aware Recurrent Transformer for Goal-Directed Video Captioning",
    "doi": "https://doi.org/10.1145/3503927",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Xin Man; Deqiang Ouyang; Xiangpeng Li; Jingkuan Song; Jie Shao",
    "corresponding_authors": "",
    "abstract": "Fully mining visual cues to aid in content understanding is crucial for video captioning. However, most state-of-the-art video captioning methods are limited to generating captions purely based on straightforward information while ignoring the scenario and context information. To fill the gap, we propose a novel, simple but effective scenario-aware recurrent transformer (SART) model to execute video captioning. Our model contains a “scenario understanding” module to obtain a global perspective across multiple frames, providing a specific scenario to guarantee a goal-directed description. Moreover, for the sake of achieving narrative continuity in the generated paragraph, a unified recurrent transformer is adopted. To demonstrate the effectiveness of our proposed SART, we have conducted comprehensive experiments on various large-scale video description datasets, including ActivityNet, YouCookII, and VideoStory. Additionally, we extend a story-oriented evaluation framework for assessing the quality of the generated caption more precisely. The superior performance has shown that SART has a strong ability to generate correct, deliberative, and narrative coherent video descriptions.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W4214931354",
    "type": "article"
  },
  {
    "title": "MKVSE: Multimodal Knowledge Enhanced Visual-semantic Embedding for Image-text Retrieval",
    "doi": "https://doi.org/10.1145/3580501",
    "publication_date": "2022-09-30",
    "publication_year": 2022,
    "authors": "Duoduo Feng; Xiangteng He; Yuxin Peng",
    "corresponding_authors": "",
    "abstract": "Image-text retrieval aims to take the text (image) query to retrieve the semantically relevant images (texts), which is fundamental and critical in the search system, online shopping, and social network. Existing works have shown the effectiveness of visual-semantic embedding and unimodal knowledge exploiting (e.g., textual knowledge) in connecting the image and text. However, they neglect the implicit multimodal knowledge relations between these two modalities when the image contains information that is not directly described in the text, hindering the ability to connect the image and text with the implicit semantic relations. For instance, an image shows a person next to the “tap” but the pairing text description may only include the word “wash,” missing the washing tool “tap.” The implicit semantic relation between image object “tap” and text word “wash” can help to connect the above image and text. To sufficiently utilize the implicit multimodal knowledge relations, we propose a M ultimodal K nowledge enhanced V isual- S emantic E mbedding (MKVSE) approach building a multimodal knowledge graph to explicitly represent the implicit multimodal knowledge relations and injecting it to visual-semantic embedding for image-text retrieval task. The contributions in this article can be summarized as follows: (1) M ultimodal K nowledge G raph (MKG) is proposed to explicitly represent the implicit multimodal knowledge relations between the image and text as intra-modal semantic relations and inter-modal co-occurrence relations . Intra-modal semantic relations provide synonymy information that is implicit in the unimodal data such as the text corpus. And inter-modal co-occurrence relations characterize the co-occurrence correlations (such as temporal, causal, and logical) that are implicit in image-text pairs. These two relations help establishing reliable image-text connections in the higher-level semantic space. (2) M ultimodal G raph C onvolution N etworks (MGCN) is proposed to reason on the MKG in two steps to sufficiently utilize the implicit multimodal knowledge relations. In the first step, MGCN focuses on the intra-modal relations to distinguish other entities in the semantic space. In the second step, MGCN focuses on the inter-modal relations to connect multimodal entities based on co-occurrence correlations. The two-step reasoning manner can sufficiently utilize the implicit semantic relations between two modal entities to enhance the embeddings of the image and text. Extensive experiments are conducted on two widely used datasets, namely, Flickr30k and MSCOCO, to demonstrate the superiority of the proposed MKVSE approach in achieving state-of-the-art performances. The codes are available at https://github.com/PKU-ICST-MIPL/MKVSE-TOMM2023 .",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W4317436342",
    "type": "article"
  },
  {
    "title": "Divide-and-Conquer-Based RDO-Free CU Partitioning for 8K Video Compression",
    "doi": "https://doi.org/10.1145/3634705",
    "publication_date": "2023-11-27",
    "publication_year": 2023,
    "authors": "Hang Yuan; Wei Gao; Siwei Ma; Yiqiang Yan",
    "corresponding_authors": "",
    "abstract": "8K (7689 × 4320) ultra high definition (UHD) videos are growing popular with the improvement of human visual experience demand. Therefore, the compression of 8K UHD videos has become a top priority in the third-generation audio video coding standard (AVS3). However, as an important part of the coding standard promotion, the real-time hardware implementation for AVS3-based 8K UHD video intra coding is severely hindered, especially in the coding unit (CU) partition stage. In order to break through the limitation, this paper proposes a divide-and-conquer-based rate-distortion-optimization-free (RDO-free) CU partitioning algorithm for efficient hardware implementation. Aimed at the complex CU partition in AVS3, we separately design a lightweight optimization for original partitioning rules to improve division efficiency and a decision tree-based RDO-free CU decision framework to eliminate the latency caused by the waiting for rate-distortion cost calculation in RDO strategy. Afterward, a divide-and-conquer-based hardware-friendly gradient difference calculating approach is devised to accelerate the learning feature extracting speed. To ensure that the proposed algorithm is sufficient to support the real-time CU partition for 8K videos, we also develop a hardware architecture based on FPGA. Experimental results illustrate that the software coding performance of our algorithm is significantly ahead of the efficient implementation uAVS3e for AVS3 and the reference software HM-16.20 for High Efficiency Video Coding (HEVC), even though there is 9.96% loss on BD-Rate Y. Considering its importance for the hardware implementation of AVS3-based 8K real-time encoder, the coding loss is acceptable. Moreover, the hardware simulation results on VU440 FPGA with Vivado 2019 show that our algorithm can support 61.12 frame per second (fps) CU partition for 8K UHD videos with only 0.00%, 0.00%, 1.01%, and 7.78% consumption of BRAM_18K, DSP48E, FF, and LUT, respectively. Additionally, with dual-path parallelism, 122.24 fps also can be implemented with controllable resource utilization, which achieves the state-of-the-art performance.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W4389057851",
    "type": "article"
  },
  {
    "title": "Data Augmentation-based Novel Deep Learning Method for Deepfaked Images Detection",
    "doi": "https://doi.org/10.1145/3592615",
    "publication_date": "2023-04-13",
    "publication_year": 2023,
    "authors": "Farkhund Iqbal; Ahmed Abbasi; Abdul Rehman Javed; Ahmad Almadhor; Zunera Jalil; Sajid Anwar; Imad Rida",
    "corresponding_authors": "",
    "abstract": "Recent advances in artificial intelligence have led to deepfake images, enabling users to replace a real face with a genuine one. deepfake images have recently been used to malign public figures, politicians, and even average citizens. deepfake but realistic images have been used to stir political dissatisfaction, blackmail, propagate false news, and even carry out bogus terrorist attacks. Thus, identifying real images from fakes has got more challenging. To avoid these issues, this study employs transfer learning and data augmentation technique to classify deepfake images. For experimentation, 190,335 RGB-resolution deepfake and real images and image augmentation methods are used to prepare the dataset. The experiments use the deep learning models: convolutional neural network (CNN), Inception V3, visual geometry group (VGG19), and VGG16 with a transfer learning approach. Essential evaluation metrics (accuracy, precision, recall, F1-score, confusion matrix, and AUC-ROC curve score) are used to test the efficacy of the proposed approach. Results revealed that the proposed approach achieves an accuracy, recall, F1-score and AUC-ROC score of 90% and 91% precision, with our fine-tuned VGG16 model outperforming other DL models in recognizing real and deepfakes.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W4365451524",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on Trustworthy Multimedia Computing and Applications in Urban Scenes",
    "doi": "https://doi.org/10.1145/3603534",
    "publication_date": "2023-07-12",
    "publication_year": 2023,
    "authors": "Wu Liu; Hailin Shi; Yunchao Wei; Dan Zeng; Nicu Sebe; Jiebo Luo",
    "corresponding_authors": "",
    "abstract": "Special Issue Part 1 (Issue 3) and Part 2 (Issue 4) of AIEDAM are based on a workshop on Learning and Creativity held at the 2002 conference on Artificial Intelligence in Design, AID '02 (www.cad.strath.ac.uk/AID02_workshop/Workshop_webpage.html; Gero, ...",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W4384069384",
    "type": "article"
  },
  {
    "title": "Web3 Metaverse: State-of-the-Art and Vision",
    "doi": "https://doi.org/10.1145/3630258",
    "publication_date": "2023-10-26",
    "publication_year": 2023,
    "authors": "Hongzhou Chen; Haihan Duan; Maha Abdallah; Yufeng Zhu; Yonggang Wen; Abdulmotaleb El Saddik; Wei Cai",
    "corresponding_authors": "",
    "abstract": "The metaverse, as a rapidly evolving socio-technical phenomenon, exhibits significant potential across diverse domains by leveraging Web3 (a.k.a. Web 3.0) technologies such as blockchain, smart contracts, and non-fungible tokens (NFTs). This survey aims to provide a comprehensive overview of the Web3 metaverse from a human-centered perspective. We (i) systematically review the development of the metaverse over the past 30 years, highlighting the balanced contributions from its core components: Web3, immersive convergence, and crowd intelligence communities, (ii) define the metaverse that integrates the Web3 community as the Web3 metaverse and propose an analysis framework from the community, society, and human layers to describe the features, missions, and relationships for each community and their overlapping sections, (iii) survey the state-of-the-art of the Web3 metaverse from a human-centered perspective, namely, the identity, field, and behavior aspects, and (iv) provide supplementary technical reviews. To the best of our knowledge, this work represents the first systematic, interdisciplinary survey on the Web3 metaverse. Specifically, we commence by discussing the potential for establishing decentralized identities (DID) utilizing mechanisms such as profile picture (PFP) NFTs, domain name NFTs, and soulbound tokens (SBTs). Subsequently, we examine land, utility, and equipment NFTs within the Web3 metaverse, highlighting interoperable and full on-chain solutions for existing centralization challenges. Lastly, we spotlight current research and practices about individual, intra-group, and inter-group behaviors within the Web3 metaverse, such as Creative Commons Zero license (CC0) NFTs, decentralized education, decentralized science (DeSci), and decentralized autonomous organizations (DAO). Furthermore, we share our insights into several promising directions, encompassing three key socio-technical facets of Web3 metaverse development.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W4388196927",
    "type": "article"
  },
  {
    "title": "Deep Learning Based Occluded Person Re-Identification: A Survey",
    "doi": "https://doi.org/10.1145/3610534",
    "publication_date": "2023-07-22",
    "publication_year": 2023,
    "authors": "Yunjie Peng; Jinlin Wu; Boqiang Xu; Chunshui Cao; Xu Liu; Zhenan Sun; Zhiqiang He",
    "corresponding_authors": "",
    "abstract": "Occluded person re-identification (Re-ID) focuses on addressing the occlusion problem when retrieving the person of interest across non-overlapping cameras. With the increasing demand for intelligent video surveillance and the application of person Re-ID technology, the real-world occlusion problem draws considerable interest from researchers. Although a large number of occluded person Re-ID methods have been proposed, there are few surveys that focus on occlusion. To fill this gap and help boost future research, this article provides a systematic survey of occluded person Re-ID. In this work, we review recent deep learning based occluded person Re-ID research. First, we summarize the main issues caused by occlusion as four groups: position misalignment, scale misalignment, noisy information, and missing information. Second, we categorize existing methods into six solution groups: matching, image transformation, multi-scale features, attention mechanism, auxiliary information, and contextual recovery. We also discuss the characteristics of each approach, as well as the issues they address. Furthermore, we present the performance comparison of recent occluded person Re-ID methods on four public datasets: Partial-ReID, Partial-iLIDS, Occluded-ReID, and Occluded-DukeMTMC. We conclude the study with thoughts on promising future research directions.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W4385144887",
    "type": "article"
  },
  {
    "title": "Attention-guided Multi-modality Interaction Network for RGB-D Salient Object Detection",
    "doi": "https://doi.org/10.1145/3624747",
    "publication_date": "2023-09-16",
    "publication_year": 2023,
    "authors": "Ruimin Wang; Fasheng Wang; Yiming Su; Jing Sun; Fuming Sun; Haojie Li",
    "corresponding_authors": "",
    "abstract": "The past decade has witnessed great progress in RGB-D salient object detection (SOD). However, there are two bottlenecks that limit its further development. The first one is low-quality depth maps. Most existing methods directly use raw depth maps to perform detection, but low-quality depth images can bring negative impacts to the detection performance. Hence, it is not desirable to utilize depth maps indiscriminately. The other one is how to effectively predict salient maps with clear boundary and complete salient region. To address these problems, an Attention-Guided Multi-Modality Interaction Network (AMINet) is proposed. First, we propose a new quality enhancement strategy for unreliable depth images, named D epth E nhancement M odule ( DEM ). With respect to the second issue, we propose C ross- M odality A ttention M odule ( CMAM ) to rapidly locate salient region. The B oundary- A ware M odule ( BAM ) is designed to utilize high-level feature to guide the low-level feature generation in a top-down way to make up for the dilution of the boundary. To further improve the accuracy, we propose A trous R efined B lock ( ARB ) to adaptively compensate for the shortcoming of atrous convolution. By integrating these interactive modules, features from depth and RGB streams can be refined efficiently, which consequently boosts the detection performance. Experimental results demonstrate the proposed AMINet exceeds state-of-the-art (SOTA) methods on several public RGB-D datasets.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W4386803179",
    "type": "article"
  },
  {
    "title": "ProActive DeepFake Detection using GAN-based Visible Watermarking",
    "doi": "https://doi.org/10.1145/3625547",
    "publication_date": "2023-09-24",
    "publication_year": 2023,
    "authors": "Aakash Varma Nadimpalli; Ajita Rattani",
    "corresponding_authors": "",
    "abstract": "With the advances in generative adversarial networks (GAN), facial manipulations called DeepFakes have caused major security risks and raised severe societal concerns. However, the popular DeepFake passive detection is an ex-post forensics countermeasure and fails in blocking the disinformation spread in advance. Alternatively, precautions such as adding perturbations to the real data for unnatural distorted DeepFake output easily spotted by the human eyes are introduced as proactive defenses. Recent studies suggest that these existing proactive defenses can be easily bypassed by employing simple image transformation and reconstruction techniques when applied to the perturbed real data and the distorted output, respectively. The aim of this article is to propose a novel proactive DeepFake detection technique using GAN-based visible watermarking. To this front, we propose a reconstructive regularization added to the GAN’s loss function that embeds a unique watermark to the assigned location of the generated fake image. Thorough experiments on multiple datasets confirm the viability of the proposed approach as a proactive defense mechanism against DeepFakes from the perspective of detection by human eyes. Thus, our proposed watermark-based GANs prevent the abuse of the pretrained GANs and smartphone apps, available via online repositories, for DeepFake creation for malicious purposes. Further, the watermarked DeepFakes can also be detected by the SOTA DeepFake detectors. This is critical for applications where automatic DeepFake detectors are used for mass audits due to the huge cost associated with human observers examining a large amount of data manually.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W4386998312",
    "type": "article"
  },
  {
    "title": "Less Is More: Learning from Synthetic Data with Fine-Grained Attributes for Person Re-Identification",
    "doi": "https://doi.org/10.1145/3588441",
    "publication_date": "2023-03-20",
    "publication_year": 2023,
    "authors": "Suncheng Xiang; Dahong Qian; Mengyuan Guan; Binjie Yan; Ting Liu; Yuzhuo Fu; Guanjie You",
    "corresponding_authors": "",
    "abstract": "Person re-identification (ReID) plays an important role in applications such as public security and video surveillance. Recently, learning from synthetic data [ 9 ], which benefits from the popularity of the synthetic data engine, has attracted great attention from the public. However, existing datasets are limited in quantity, diversity, and realisticity, and cannot be efficiently used for the ReID problem. To address this challenge, we manually construct a large-scale person dataset named FineGPR with fine-grained attribute annotations. Moreover, aiming to fully exploit the potential of FineGPR and promote the efficient training from millions of synthetic data, we propose an attribute analysis pipeline called AOST based on the traditional machine learning algorithm, which dynamically learns attribute distribution in a real domain, then eliminates the gap between synthetic and real-world data and thus is freely deployed to new scenarios. Experiments conducted on benchmarks demonstrate that FineGPR with AOST outperforms (or is on par with) existing real and synthetic datasets, which suggests its feasibility for the ReID task and proves the proverbial less-is-more principle. Our synthetic FineGPR dataset is publicly available at https://github.com/JeremyXSC/FineGPR .",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W4327950385",
    "type": "article"
  },
  {
    "title": "Multimodality Representation Learning: A Survey on Evolution, Pretraining and Its Applications",
    "doi": "https://doi.org/10.1145/3617833",
    "publication_date": "2023-08-29",
    "publication_year": 2023,
    "authors": "Muhammad Arslan Manzoor; Sarah Albarri; Ziting Xian; Zaiqiao Meng; Preslav Nakov; Shangsong Liang",
    "corresponding_authors": "",
    "abstract": "Multimodality Representation Learning, as a technique of learning to embed information from different modalities and their correlations, has achieved remarkable success on a variety of applications, such as Visual Question Answering (VQA), Natural Language for Visual Reasoning (NLVR), and Vision Language Retrieval (VLR). Among these applications, cross-modal interaction and complementary information from different modalities are crucial for advanced models to perform any multimodal task, e.g., understand, recognize, retrieve, or generate optimally. Researchers have proposed diverse methods to address these tasks. The different variants of transformer-based architectures performed extraordinarily on multiple modalities. This survey presents the comprehensive literature on the evolution and enhancement of deep learning multimodal architectures to deal with textual, visual and audio features for diverse cross-modal and modern multimodal tasks. This study summarizes the ( i ) recent task-specific deep learning methodologies, ( ii ) the pretraining types and multimodal pretraining objectives, ( iii ) from state-of-the-art pretrained multimodal approaches to unifying architectures, and ( iv ) multimodal task categories and possible future improvements that can be devised for better multimodal learning. Moreover, we prepare a dataset section for new researchers that covers most of the benchmarks for pretraining and finetuning. Finally, major challenges, gaps, and potential research topics are explored. A constantly-updated paperlist related to our survey is maintained at https://github.com/marslanm/multimodality-representation-learning .",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W4386254667",
    "type": "article"
  },
  {
    "title": "Jointly Harnessing Prior Structures and Temporal Consistency for Sign Language Video Generation",
    "doi": "https://doi.org/10.1145/3648368",
    "publication_date": "2024-02-16",
    "publication_year": 2024,
    "authors": "Yucheng Suo; Zhedong Zheng; Xiaohan Wang; Bang Zhang; Yi Yang",
    "corresponding_authors": "",
    "abstract": "Sign language provides a way for differently-abled individuals to express their feelings and emotions. However, learning sign language can be challenging and time consuming. An alternative approach is to animate user photos using sign language videos of specific words, which can be achieved using existing image animation methods. However, the finger motions in the generated videos are often not ideal. To address this issue, we propose the Structure-aware Temporal Consistency Network (STCNet), which jointly optimizes the prior structure of humans with temporal consistency to produce sign language videos. We use a fine-grained skeleton detector to acquire knowledge of body structure and introduce both short- and long-term cycle loss to ensure the continuity of the generated video. The two losses and keypoint detector network are optimized in an end-to-end manner. Quantitative and qualitative evaluations on three widely used datasets, namely LSA64, Phoenix-2014T, and WLASL-2000, demonstrate the effectiveness of the proposed method. It is our hope that this work can contribute to future studies on sign language production.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W4391884092",
    "type": "article"
  },
  {
    "title": "Heterogeneous Fusion and Integrity Learning Network for RGB-D Salient Object Detection",
    "doi": "https://doi.org/10.1145/3656476",
    "publication_date": "2024-04-05",
    "publication_year": 2024,
    "authors": "Haicheng Gao; Yiming Su; Fasheng Wang; Haojie Li",
    "corresponding_authors": "",
    "abstract": "While significant progress has been made in recent years in the field of salient object detection, there are still limitations in heterogeneous modality fusion and salient feature integrity learning. The former is primarily attributed to a paucity of attention from researchers to the fusion of cross-scale information between different modalities during processing multi-modal heterogeneous data, coupled with an absence of methods for adaptive control of their respective contributions. The latter constraint stems from the shortcomings in existing approaches concerning the prediction of salient region’s integrity. To address these problems, we propose a Heterogeneous Fusion and Integrity Learning Network for RGB-D Salient Object Detection (HFIL-Net). In response to the first challenge, we design an Advanced Semantic Guidance Aggregation (ASGA) module, which utilizes three fusion blocks to achieve the aggregation of three types of information: within-scale cross-modal, within-modal cross-scale, and cross-modal cross-scale. In addition, we embed the local fusion factor matrices in the ASGA module and utilize the global fusion factor matrices in the Multi-modal Information Adaptive Fusion module to control the contributions adaptively from different perspectives during the fusion process. For the second issue, we introduce the Feature Integrity Learning and Refinement Module. It leverages the idea of ”part-whole” relationships from capsule networks to learn feature integrity and further refine the learned features through attention mechanisms. Extensive experimental results demonstrate that our proposed HFIL-Net outperforms over 17 state-of-the-art detection methods in testing across seven challenging standard datasets. Codes and results are available on https://github.com/BojueGao/HFIL-Net .",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W4393979362",
    "type": "article"
  },
  {
    "title": "Blind Quality Assessment of Dense 3D Point Clouds with Structure Guided Resampling",
    "doi": "https://doi.org/10.1145/3664199",
    "publication_date": "2024-05-11",
    "publication_year": 2024,
    "authors": "Wei Zhou; Qi Yang; Wu Chen; Qiuping Jiang; Guangtao Zhai; Weisi Lin",
    "corresponding_authors": "",
    "abstract": "Objective quality assessment of three-dimensional (3D) point clouds is essential for the development of immersive multimedia systems in real-world applications. Despite the success of perceptual quality evaluation for 2D images and videos, blind/no-reference metrics are still scarce for 3D point clouds with large-scale irregularly distributed 3D points. Therefore, in this article, we propose an objective point cloud quality index with Structure Guided Resampling (SGR) to automatically evaluate the perceptually visual quality of dense 3D point clouds. The proposed SGR is a general-purpose blind quality assessment method without the assistance of any reference information. Specifically, considering that the human visual system is highly sensitive to structure information, we first exploit the unique normal vectors of point clouds to execute regional pre-processing that consists of keypoint resampling and local region construction. Then, we extract three groups of quality-related features, including (1) geometry density features, (2) color naturalness features, and (3) angular consistency features. Both the cognitive peculiarities of the human brain and naturalness regularity are involved in the designed quality-aware features that can capture the most vital aspects of distorted 3D point clouds. Extensive experiments on several publicly available subjective point cloud quality databases validate that our proposed SGR can compete with state-of-the-art full-reference, reduced-reference, and no-reference quality assessment algorithms.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4396834481",
    "type": "article"
  },
  {
    "title": "GANonymization: A GAN-based Face Anonymization Framework for Preserving Emotional Expressions",
    "doi": "https://doi.org/10.1145/3641107",
    "publication_date": "2024-01-17",
    "publication_year": 2024,
    "authors": "Fabio Hellmann; Silvan Mertes; Mohamed Benouis; Alexander Hustinx; Tzung‐Chien Hsieh; Cristina Conati; Peter Krawitz; Elisabeth André",
    "corresponding_authors": "",
    "abstract": "In recent years, the increasing availability of personal data has raised concerns regarding privacy and security. One of the critical processes to address these concerns is data anonymization, which aims to protect individual privacy and prevent the release of sensitive information. This research focuses on the importance of face anonymization. Therefore, we introduce GANonymization, a novel face anonymization framework with facial expression-preserving abilities. Our approach is based on a high-level representation of a face, which is synthesized into an anonymized version based on a generative adversarial network (GAN). The effectiveness of the approach was assessed by evaluating its performance in removing identifiable facial attributes to increase the anonymity of the given individual face. Additionally, the performance of preserving facial expressions was evaluated on several affect recognition datasets and outperformed the state-of-the-art methods in most categories. Finally, our approach was analyzed for its ability to remove various facial traits, such as jewelry, hair color, and multiple others. Here, it demonstrated reliable performance in removing these attributes. Our results suggest that GANonymization is a promising approach for anonymizing faces while preserving facial expressions.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4390946825",
    "type": "article"
  },
  {
    "title": "From CNNs to Transformers in Multimodal Human Action Recognition: A Survey",
    "doi": "https://doi.org/10.1145/3664815",
    "publication_date": "2024-05-13",
    "publication_year": 2024,
    "authors": "Muhammad Bilal Shaikh; Douglas Chai; Syed Mohammed Shamsul Islam; Naveed Akhtar",
    "corresponding_authors": "",
    "abstract": "Due to its widespread applications, human action recognition is one of the most widely studied research problems in Computer Vision. Recent studies have shown that addressing it using multimodal data leads to superior performance as compared to relying on a single data modality. During the adoption of deep learning for visual modelling in the last decade, action recognition approaches have mainly relied on Convolutional Neural Networks (CNNs). However, the recent rise of Transformers in visual modelling is now also causing a paradigm shift for the action recognition task. This survey captures this transition while focusing on Multimodal Human Action Recognition (MHAR). Unique to the induction of multimodal computational models is the process of \"fusing\" the features of the individual data modalities. Hence, we specifically focus on the fusion design aspects of the MHAR approaches. We analyze the classic and emerging techniques in this regard, while also highlighting the popular trends in the adaption of CNN and Transformer building blocks for the overall problem. In particular, we emphasize on recent design choices that have led to more efficient MHAR models. Unlike existing reviews, which discuss Human Action Recognition from a broad perspective, this survey is specifically aimed at pushing the boundaries of MHAR research by identifying promising architectural and fusion design choices to train practicable models. We also provide an outlook of the multimodal datasets from their scale and evaluation viewpoint. Finally, building on the reviewed literature, we discuss the challenges and future avenues for MHAR.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4396861023",
    "type": "article"
  },
  {
    "title": "Action-aware Linguistic Skeleton Optimization Network for Non-autoregressive Video Captioning",
    "doi": "https://doi.org/10.1145/3679203",
    "publication_date": "2024-07-20",
    "publication_year": 2024,
    "authors": "Shuailiang Chen; Xian Zhong; Yi Zhang; Lei Zhu; Ping Li; Xiaokang Yang; Bin Sheng",
    "corresponding_authors": "",
    "abstract": "Non-autoregressive video captioning methods generate visual words in parallel but often overlook semantic correlations among them, especially regarding verbs, leading to lower caption quality. To address this, we integrate action information of highlighted objects to enhance semantic connections among visual words. Our proposed Action-aware Language Skeleton Optimization Network (ALSO-Net) tackles the challenge of extracting action information across frames, improving understanding of complex context-dependent video actions and reducing sentence inconsistencies. ALSO-Net incorporates a linguistic skeleton tag generator to refine semantic correlations and a video action predictor to enhance verb prediction accuracy in video captions. We also address issues of unsatisfactory caption length and quality by jointly optimizing different levels of motion prediction loss. Experimental evaluation on prominent video captioning datasets demonstrates that ALSO-Net outperforms baseline methods by a significant margin and achieves competitive performance compared to state-of-the-art autoregressive methods with smaller model complexity and faster inference time.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4400850904",
    "type": "article"
  },
  {
    "title": "FaceSigns: Semi-Fragile Watermarks for Media Authentication",
    "doi": "https://doi.org/10.1145/3640466",
    "publication_date": "2024-01-13",
    "publication_year": 2024,
    "authors": "Paarth Neekhara; Shehzeen Hussain; Xinqiao Zhang; Ke Huang; Julian McAuley; Farinaz Koushanfar",
    "corresponding_authors": "",
    "abstract": "Manipulated media is becoming a prominent threat due to the recent advances in realistic image and video synthesis techniques. There have been several attempts at detecting synthetically tampered media using machine learning classifiers. However, such classifiers do not generalize well to black-box image synthesis techniques and have been shown to be vulnerable to adversarial examples. To address these challenges, we introduce FaceSigns —a deep learning-based semi-fragile watermarking technique that allows media authentication by verifying an invisible secret message embedded in the image pixels. Instead of identifying and detecting manipulated media using visual artifacts, we propose to proactively embed a semi-fragile watermark into a real image or video so that we can prove its authenticity when needed. FaceSigns is designed to be fragile to malicious manipulations or tampering while being robust to benign operations such as image/video compression, scaling, saturation, contrast adjustments, and so forth. This allows images and videos shared over the internet to retain the verifiable watermark as long as a malicious modification technique is not applied. We demonstrate that our framework can embed a 128-bit secret as an imperceptible image watermark that can be recovered with a high bit recovery accuracy at several compression levels, while being non-recoverable when unseen malicious manipulations are applied. For a set of unseen benign and malicious manipulations studied in our work, our framework can reliably detect manipulated content with an AUC score of 0.996, which is significantly higher than prior image watermarking and steganography techniques.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4390841259",
    "type": "article"
  },
  {
    "title": "MF <sup>2</sup> ShrT: Multimodal Feature Fusion Using Shared Layered Transformer for Face Anti-spoofing",
    "doi": "https://doi.org/10.1145/3640817",
    "publication_date": "2024-01-25",
    "publication_year": 2024,
    "authors": "Aashania Antil; Chhavi Dhiman",
    "corresponding_authors": "",
    "abstract": "In recent times, Face Anti-spoofing (FAS) has gained significant attention in both academic and industrial domains. Although various convolutional neural network (CNN)-based solutions have emerged, multimodal approaches incorporating RGB, depth, and information retrieval (IR) have exhibited better performance than unimodal classifiers. The increasing veracity of modern presentation attack instruments results in a persistent need to enhance the performance of such models. Recently, self-attention-based vision transformers (ViT) have become a popular choice in this field. Their fundamental aspects for multimodal FAS have not been thoroughly explored yet. Therefore, we propose a novel framework for FAS called MF 2 ShrT, which is based on a pretrained vision transformer. The proposed framework uses overlap patches and parameter sharing in the ViT network, allowing it to utilize multiple modalities in a computationally efficient manner. Furthermore, to effectively fuse intermediate features from different encoders of each ViT, we explore a T-encoder-based hybrid feature block enabling the system to identify correlations and dependencies across different modalities. MF 2 ShrT outperforms conventional vision transformers and achieves state-of-the-art performance on benchmarks CASIA-SURF and WMCA, demonstrating the efficiency of transformer-based models for presentation attack detection PAD).",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4391223066",
    "type": "article"
  },
  {
    "title": "4D Facial Expression Diffusion Model",
    "doi": "https://doi.org/10.1145/3653455",
    "publication_date": "2024-03-28",
    "publication_year": 2024,
    "authors": "Kaifeng Zou; Sylvain Faisan; Boyang Yu; Sébastien Valette; Hyewon Seo",
    "corresponding_authors": "",
    "abstract": "Facial expression generation is one of the most challenging and long-sought aspects of character animation, with many interesting applications. The challenging task, traditionally having relied heavily on digital craftspersons, remains yet to be explored. In this paper, we introduce a generative framework for generating 3D facial expression sequences (i.e. 4D faces) that can be conditioned on different inputs to animate an arbitrary 3D face mesh. It is composed of two tasks: (1) Learning the generative model that is trained over a set of 3D landmark sequences, and (2) Generating 3D mesh sequences of an input facial mesh driven by the generated landmark sequences. The generative model is based on a Denoising Diffusion Probabilistic Model (DDPM), which has achieved remarkable success in generative tasks of other domains. While it can be trained unconditionally, its reverse process can still be conditioned by various condition signals. This allows us to efficiently develop several downstream tasks involving various conditional generation, by using expression labels, text, partial sequences, or simply a facial geometry. To obtain the full mesh deformation, we then develop a landmark-guided encoder-decoder to apply the geometrical deformation embedded in landmarks on a given facial mesh. Experiments show that our model has learned to generate realistic, quality expressions solely from the dataset of relatively small size, improving over the state-of-the-art methods. Videos and qualitative comparisons with other methods can be found at https://github.com/ZOUKaifeng/4DFM. Code and models will be made available upon acceptance.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4393259614",
    "type": "article"
  },
  {
    "title": "StepNet: Spatial-temporal Part-aware Network for Isolated Sign Language Recognition",
    "doi": "https://doi.org/10.1145/3656046",
    "publication_date": "2024-04-03",
    "publication_year": 2024,
    "authors": "Shen Xiao-long; Zhedong Zheng; Yi Yang",
    "corresponding_authors": "",
    "abstract": "The goal of sign language recognition (SLR) is to help those who are hard of hearing or deaf overcome the communication barrier. Most existing approaches can be typically divided into two lines, i.e., Skeleton-based, and RGB-based methods, but both lines of methods have their limitations. Skeleton-based methods do not consider facial expressions, while RGB-based approaches usually ignore the fine-grained hand structure. To overcome both limitations, we propose a new framework called the Spatial-temporal Part-aware network (StepNet), based on RGB parts. As its name suggests, it is made up of two modules: Part-level Spatial Modeling and Part-level Temporal Modeling. Part-level Spatial Modeling, in particular, automatically captures the appearance-based properties, such as hands and faces, in the feature space without the use of any keypoint-level annotations. On the other hand, Part-level Temporal Modeling implicitly mines the long short-term context to capture the relevant attributes over time. Extensive experiments demonstrate that our StepNet, thanks to spatial-temporal modules, achieves competitive Top-1 Per-instance accuracy on three commonly used SLR benchmarks, i.e., 56.89% on WLASL, 77.2% on NMFs-CSL, and 77.1% on BOBSL. Additionally, the proposed method is compatible with the optical flow input and can produce superior performance if fused. For those who are hard of hearing, we hope that our work can act as a preliminary step.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4393855519",
    "type": "article"
  },
  {
    "title": "Wakeup-Darkness: When Multimodal Meets Unsupervised Low-light Image Enhancement",
    "doi": "https://doi.org/10.1145/3711929",
    "publication_date": "2025-01-15",
    "publication_year": 2025,
    "authors": "Xiaofeng Zhang; Zishan Xu; Hao Tang; Chaochen Gu; Wei Chen; Abdulmotaleb El Saddik",
    "corresponding_authors": "",
    "abstract": "Low-light image enhancement is a crucial visual task, and many unsupervised methods overlook the degradation of visible information in low-light scenes, adversely affecting the fusion of complementary information and hindering the generation of satisfactory results. To address this, we introduce Wakeup-Darkness, a multimodal enhancement framework that innovatively enriches user interaction through voice and textual commands. This approach signifies a technical leap and represents a paradigm shift in user engagement. We introduce a cross-modal feature fusion (CMFF) that synergizes semantic and depth context with low-light enhancement operations. Moreover, We propose a gated residual block (GRB) and a channel-aware look-up table (LUT) to adjust the intensity distribution of each channel. Crucially, the proposed Wakeup-Darkness scheme demonstrates remarkable generalization in unsupervised scenarios. The source code can be accessed from https://github.com/zhangbaijin/Wakeup-Dakness .",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4406391458",
    "type": "article"
  },
  {
    "title": "Mix-Modality Person Re-Identification: A New and Practical Paradigm",
    "doi": "https://doi.org/10.1145/3715142",
    "publication_date": "2025-01-28",
    "publication_year": 2025,
    "authors": "Wei Liu; Xin Xu; Hua Chang; Xin Yuan; Zheng Wang",
    "corresponding_authors": "",
    "abstract": "Current visible-infrared cross-modality person re-identification research has only focused on exploring the bi-modality mutual retrieval paradigm, and we propose a new and more practical mix-modality retrieval paradigm. Existing V isible- I nfrared person re-identification (VI-ReID) methods have achieved some results in the bi-modality mutual retrieval paradigm by learning the correspondence between visible and infrared modalities. However, significant performance degradation occurs due to the modality confusion problem when these methods are applied to the new mix-modality paradigm. Therefore, this paper proposes a M ix- M odality person re-identification (MM-ReID) task, explores the influence of modality mixing ratio on performance, and constructs mix-modality test sets for existing datasets according to the new mix-modality testing paradigm. To solve the modality confusion problem in MM-ReID, we propose a C ross- I dentity D iscrimination H armonization L oss (CIDHL) adjusting the distribution of samples in the hyperspherical feature space, pulling the centers of samples with the same identity closer, and pushing away the centers of samples with different identities while aggregating samples with the same modality and the same identity. Furthermore, we propose a M odality B ridge S imilarity O ptimization S trategy (MBSOS) to optimize the cross-modality similarity between the query and queried samples with the help of the similar bridge sample in the gallery. Extensive experiments demonstrate that compared to the original performance of existing cross-modality methods on MM-ReID, the addition of our CIDHL and MBSOS demonstrates a general improvement.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4406903488",
    "type": "article"
  },
  {
    "title": "Compression Approaches for LiDAR Point Clouds and Beyond: A Survey",
    "doi": "https://doi.org/10.1145/3715916",
    "publication_date": "2025-01-31",
    "publication_year": 2025,
    "authors": "Miaohui Wang; Runnan Huang; Wuyuan Xie; Zhan Ma; Siwei Ma",
    "corresponding_authors": "",
    "abstract": "With the widespread use of LiDAR sensors in autonomous driving, LiDAR point cloud compression (LPCC) plays an important role in effectively managing the storage, transmission, and perception of the growing volume of LiDAR data. Despite this need, there has been a noticeable absence of comprehensive investigations specifically dedicated to LPCC methods. To address this issue, this article presents a systematic survey of existing LPCCs, aiming to summarize recent progress and inspire future research in this field. We begin by providing a general introduction of LPCC fundamentals, covering the latest LiDAR point cloud (LPC) datasets, distinctive attributes, evaluation metrics, and data formats. We then conduct a careful review and comparison of LPCCs, examining image-based, octree-based, deep-learned, and other approaches, offering valuable insights into the strengths and weaknesses of cutting-edge models. Finally, we propose future research directions based on the limitations of recent LPCCs. We believe that the findings presented in this article will contribute to a deeper understanding of LPCCs and promote further development of LiDAR sensor-based systems.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4407031332",
    "type": "article"
  },
  {
    "title": "Unified Text-Image Space Alignment with Cross-Modal Prompting in CLIP for UDA",
    "doi": "https://doi.org/10.1145/3715699",
    "publication_date": "2025-02-05",
    "publication_year": 2025,
    "authors": "Yifan Jiao; Chenglong Cai; Bing‐Kun Bao",
    "corresponding_authors": "",
    "abstract": "Unsupervised Domain Adaptation (UDA) aims to transfer models trained on a labeled source domain to an unlabeled target domain. Due to the excellent generalization ability of Vision Language Models (VLMs) such as CLIP in downstream tasks, most recent methods apply CLIP to UDA tasks through learning domain-specific text prompts for source and target domains separately. However, these methods fail to dynamically adjust image features based on the characteristics of their respective domains, thereby limiting their alignment with domain-specific text prompts in CLIP’s joint space, which is a key factor in improving classification performance in the target domain. To bridge this gap, we propose a Unified Text-Image Space Alignment with Cross-Modal Prompting (UTISA) framework for UDA. Firstly, we introduce a cross-modal prompt learning (CMP) module to generate domain-specific image prompts and layer-specific image prompts for the visual branch to encode domain-specific knowledge globally and locally. Secondly, under the guidance of image prompts, we introduce a domain-aware multi-layer feature fusion (DMF) module to construct multi-layer domain features for each domain and enhance the image features with these multi-layer domain features, which enables the image features to better reflect the characteristics of their respective domains, thereby promoting their alignment with domain-specific text prompts. Moreover, we introduce a perturbation-driven regularization (PDR) mechanism for the target domain to enhance the robustness and generalization of the model. The experiments demonstrate that UTISA achieves the best performance on three mainstream UDA benchmarks, including 87.9% on OfficeHome, 90.9% on VisDA-2017, 62.4% on DomainNet.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4407156489",
    "type": "article"
  },
  {
    "title": "Exploring Interpretability in Deep Learning for Affective Computing: A Comprehensive Review",
    "doi": "https://doi.org/10.1145/3723005",
    "publication_date": "2025-03-12",
    "publication_year": 2025,
    "authors": "Xiaoke Zhang; Tenggan Zhang; Lei Sun; Jinming Zhao; Qin Jin",
    "corresponding_authors": "",
    "abstract": "Deep learning has shown impressive performance in affective computing, but its black-box characteristic limits the model’s interpretability, posing a challenge to further development and application. Compared with objective recognition tasks such as image recognition, emotion perception as a high-level cognition is more subjective, making it particularly important to enhance the interpretability of deep learning in affective computing. In recent years, some interpretability-related works have emerged, but there are few reviews on this topic yet. This article summarizes the explainable deep learning methods in affective computing from two aspects: first, the application of general explainable deep learning methods in affective computing from the perspectives of model-agnostic and model-specific is introduced; second, emotion-specific interpretability research that combines emotional psychology theories, physiological studies, and human cognition, covering task design, model design, and result analysis methods, is systematically reviewed. There are new explainable deep learning methods for multimodal and large language models in the context of emotion. Finally, we discuss five specific challenges and propose corresponding future directions to provide insights and references for subsequent research on affective computing interpretability.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4408361561",
    "type": "review"
  },
  {
    "title": "A Review of Player Engagement Estimation in Video Games: Challenges and Opportunities",
    "doi": "https://doi.org/10.1145/3722116",
    "publication_date": "2025-03-12",
    "publication_year": 2025,
    "authors": "Ammar Rashed; Shervin Shirmohammadi; Ihab Amer; Mohamed Hefeeda",
    "corresponding_authors": "",
    "abstract": "This article presents a review on the process of estimating player engagement in video gaming. To stay ahead of their competitors in entertainment, game developers need to understand, estimate, and maximize player engagement. We address the multidimensional nature of engagement, encompassing cognitive, emotional, and behavioral aspects across various gaming domains. We present a taxonomy of the diverse modalities for quantifying engagement, including physiological signals, observable behaviors, and gameplay data. We identify the challenges of conducting representative subjective studies in this domain and summarize various methods for establishing ground truth measurements. By synthesizing existing research, we provide insights into modeling techniques, highlight research gaps, and offer practical guidelines for implementing engagement measurement strategies. This review aims to aid researchers and industry professionals in navigating the complexities of player engagement estimation, ultimately contributing to enhanced game design, marketing, and user retention in the competitive gaming landscape.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4408361573",
    "type": "review"
  },
  {
    "title": "ACM SIGMM retreat report on future directions in multimedia research",
    "doi": "https://doi.org/10.1145/1047936.1047938",
    "publication_date": "2005-02-01",
    "publication_year": 2005,
    "authors": "Lawrence A. Rowe; Ramesh Jain",
    "corresponding_authors": "",
    "abstract": "The ACM Multimedia Special Interest Group was created ten years ago. Since that time, researchers have solved a number of important problems related to media processing, multimedia databases, and distributed multimedia applications. A strategic retreat was organized as part of ACM Multimedia 2003 to assess the current state of multimedia research and suggest directions for future research. This report presents the recommendations developed during the retreat. The major observation is that research in the past decade has significantly advanced hardware and software support for distributed multimedia applications and that future research should focus on identifying and delivering applications that impact users in the real-world.The retreat suggested that the community focus on solving three grand challenges: (1) make authoring complex multimedia titles as easy as using a word processor or drawing program, (2) make interactions with remote people and environments nearly the same as interactions with local people and environments, and (3) make capturing, storing, finding, and using digital media an everyday occurrence in our computing environment. The focus of multimedia researchers should be on applications that incorporate correlated media, fuse data from different sources, and use context to improve application performance.",
    "cited_by_count": 110,
    "openalex_id": "https://openalex.org/W2048101761",
    "type": "article"
  },
  {
    "title": "Foveated gaze-contingent displays for peripheral LOD management, 3D visualization, and stereo imaging",
    "doi": "https://doi.org/10.1145/1314303.1314309",
    "publication_date": "2007-12-01",
    "publication_year": 2007,
    "authors": "Andrew T. Duchowski; Arzu Çöltekin",
    "corresponding_authors": "",
    "abstract": "Advancements in graphics hardware have allowed development of hardware-accelerated imaging displays. This article reviews techniques for real-time simulation of arbitrary visual fields over still images and video. The goal is to provide the vision sciences and perceptual graphics communities techniques for the investigation of fundamental processes of visual perception. Classic gaze-contingent displays used for these purposes are reviewed and for the first time a pixel shader is introduced for display of a high-resolution window over peripherally degraded stimulus. The pixel shader advances current state-of-the-art by allowing real-time processing of still or streamed images, obviating the need for preprocessing or storage.",
    "cited_by_count": 92,
    "openalex_id": "https://openalex.org/W2059413159",
    "type": "article"
  },
  {
    "title": "Video interactions in online video social networks",
    "doi": "https://doi.org/10.1145/1596990.1596994",
    "publication_date": "2009-10-01",
    "publication_year": 2009,
    "authors": "Fabrí­cio Benevenuto; Tiago Rodrigues; Virgı́lio Almeida; Jussara M. Almeida; Keith W. Ross",
    "corresponding_authors": "",
    "abstract": "This article characterizes video-based interactions that emerge from YouTube's video response feature, which allows users to discuss themes and to provide reviews for products or places using much richer media than text. Based on crawled data covering a representative subset of videos and users, we present a characterization from two perspectives: the video response view and the interaction network view. In addition to providing valuable statistical models for various characteristics, our study uncovers typical user behavioral patterns in video-based environments and shows evidence of opportunistic behavior.",
    "cited_by_count": 89,
    "openalex_id": "https://openalex.org/W2084771439",
    "type": "article"
  },
  {
    "title": "Defining user perception of distributed multimedia quality",
    "doi": "https://doi.org/10.1145/1201730.1201731",
    "publication_date": "2006-11-01",
    "publication_year": 2006,
    "authors": "Stephen R. Gulliver; Gheorghiță Ghinea",
    "corresponding_authors": "",
    "abstract": "This article presents the results of a study that explored the human side of the multimedia experience. We propose a model that assesses quality variation from three distinct levels: the network, the media and the content levels ; and from two views: the technical and the user perspective. By facilitating parameter variation at each of the quality levels and from each of the perspectives, we were able to examine their impact on user quality perception. Results show that a significant reduction in frame rate does not proportionally reduce the user's understanding of the presentation independent of technical parameters, that multimedia content type significantly impacts user information assimilation, user level of enjoyment, and user perception of quality, and that the device display type impacts user information assimilation and user perception of quality. Finally, to ensure the transfer of information, low-level abstraction (network-level) parameters, such as delay and jitter, should be adapted; to maintain the user's level of enjoyment, high-level abstraction quality parameters (content-level), such as the appropriate use of display screens, should be adapted.",
    "cited_by_count": 87,
    "openalex_id": "https://openalex.org/W2088275954",
    "type": "article"
  },
  {
    "title": "Watch-and-comment as a paradigm toward ubiquitous interactive video editing",
    "doi": "https://doi.org/10.1145/1412196.1412201",
    "publication_date": "2008-10-01",
    "publication_year": 2008,
    "authors": "Renan G. Cattelan; César Teixeira; Rudinei Goularte; Maria da Graça Campos Pimentel",
    "corresponding_authors": "",
    "abstract": "The literature reports research efforts allowing the editing of interactive TV multimedia documents by end-users. In this article we propose complementary contributions relative to end-user generated interactive video, video tagging, and collaboration. In earlier work we proposed the watch-and-comment (WaC) paradigm as the seamless capture of an individual's comments so that corresponding annotated interactive videos be automatically generated. As a proof of concept, we implemented a prototype application, the WaCTool, that supports the capture of digital ink and voice comments over individual frames and segments of the video, producing a declarative document that specifies both: different media stream structure and synchronization. In this article, we extend the WaC paradigm in two ways. First, user-video interactions are associated with edit commands and digital ink operations. Second, focusing on collaboration and distribution issues, we employ annotations as simple containers for context information by using them as tags in order to organize, store and distribute information in a P2P-based multimedia capture platform. We highlight the design principles of the watch-and-comment paradigm, and demonstrate related results including the current version of the WaCTool and its architecture. We also illustrate how an interactive video produced by the WaCTool can be rendered in an interactive video environment, the Ginga-NCL player, and include results from a preliminary evaluation.",
    "cited_by_count": 85,
    "openalex_id": "https://openalex.org/W2018575282",
    "type": "article"
  },
  {
    "title": "Interactive TV narratives",
    "doi": "https://doi.org/10.1145/1412196.1412198",
    "publication_date": "2008-10-01",
    "publication_year": 2008,
    "authors": "Marian F. Ursu; Maureen Thomas; Ian Kegel; Doug Williams; Mika Lumi Tuomola; Inger Lindstedt; Terence Wright; Andra Leurdijk; Vilmos Zsombori; Julia Sussner; Ulf Myrestam; Nina Lansbury",
    "corresponding_authors": "",
    "abstract": "This article is motivated by the question whether television should do more than simply offer interactive services alongside (and separately from) traditional linear programs, in the context of its dominance being seriously challenged and threatened by interactive forms of screen media entertainment. It suggests: yes. Interactive narrativity , that is, the ability to interact with (and influence) stories whilst they are being told, represents one clear development path for interactive television. The capabilities of computing technology are ripe for exploring this new form of storytelling, from creation to commercial distribution. The article starts by looking at the relationship between narrativity and interactivity in the current context of screen media, and identifies clear signs of interest from certain European public broadcasters in interactive TV narratives. It then presents in detail four recent experimental interactive TV productions in the genres of drama, news, and documentary, developed in collaboration with public broadcasters, which illustrate the potential and richness of this new form of storytelling, but also highlight new technological capabilities necessary for such productions. A number of essential technological requirements are then discussed in more detail in the final part. The article suggests that the ShapeShifting Media Technology, employed in the implementation of the four productions, has made significant advances both at the technological and the creative ends in supporting the development of interactive TV narrativity, but, however, that further developments are required before being able to answer questions such as “Would end users want such a form of screen media entertainment?” and “Would it be effective for both end users and producers?”",
    "cited_by_count": 85,
    "openalex_id": "https://openalex.org/W2027734923",
    "type": "article"
  },
  {
    "title": "An automated end-to-end lecture capture and broadcasting system",
    "doi": "https://doi.org/10.1145/1324287.1324293",
    "publication_date": "2008-01-01",
    "publication_year": 2008,
    "authors": "Cha Zhang; Rui Yong; J. P. Wickersham Crawford; Li-wei He",
    "corresponding_authors": "",
    "abstract": "Remote viewing of lectures presented to a live audience is becoming increasingly popular. At the same time, the lectures can be recorded for subsequent on-demand viewing over the Internet. Providing such services, however, is often prohibitive due to the labor-intensive cost of capturing and pre/post-processing. This article presents a complete automated end-to-end system that supports capturing, broadcasting, viewing, archiving and searching of presentations. Specifically, we describe a system architecture that minimizes the pre- and post-production time, and a fully automated lecture capture system called iCam2 that synchronously captures all contents of the lecture, including audio, video, and presentation material. No staff is needed during lecture capture and broadcasting, so the operational cost of the system is negligible. The system has been used on a daily basis for more than 4 years, during which 522 lectures have been captured. These lectures have been viewed over 20,000 times.",
    "cited_by_count": 81,
    "openalex_id": "https://openalex.org/W1974846296",
    "type": "article"
  },
  {
    "title": "Multipath live streaming via TCP",
    "doi": "https://doi.org/10.1145/1556134.1556142",
    "publication_date": "2009-08-01",
    "publication_year": 2009,
    "authors": "Bing Wang; Wei Wei; Guoqiang Zheng; Don Towsley",
    "corresponding_authors": "",
    "abstract": "Motivated by the wide use of TCP for multimedia streaming in practice and the increasing availability of multipath between end hosts, we study multipath live streaming via TCP in this article. We first design a simple and practical TCP-based multipath streaming scheme, named Dynamic MPath-streaming (DMP-streaming) , which dynamically distributes packets over multiple paths by implicitly inferring the available bandwidths on these paths. To allow systematic performance study, we develop an analytical model for DMP-streaming and validate the model using extensive ns simulation and Internet experiments. We explore the parameter space of this model and find that DMP-streaming generally provides satisfactory performance when the aggregate achievable TCP throughput is 1.6 times the video bitrate, when allowing a few seconds of startup delay. Last, we comment on the benefits of using multipath versus single path for TCP-based streaming.",
    "cited_by_count": 80,
    "openalex_id": "https://openalex.org/W2088885617",
    "type": "article"
  },
  {
    "title": "Video quality for face detection, recognition, and tracking",
    "doi": "https://doi.org/10.1145/2000486.2000488",
    "publication_date": "2011-08-01",
    "publication_year": 2011,
    "authors": "Pavel Korshunov; Wei Tsang Ooi",
    "corresponding_authors": "",
    "abstract": "Many distributed multimedia applications rely on video analysis algorithms for automated video and image processing. Little is known, however, about the minimum video quality required to ensure an accurate performance of these algorithms. In an attempt to understand these requirements, we focus on a set of commonly used face analysis algorithms. Using standard datasets and live videos, we conducted experiments demonstrating that the algorithms show almost no decrease in accuracy until the input video is reduced to a certain critical quality, which amounts to significantly lower bitrate compared to the quality commonly acceptable for human vision. Since computer vision percepts video differently than human vision, existing video quality metrics, designed for human perception, cannot be used to reason about the effects of video quality reduction on accuracy of video analysis algorithms. We therefore investigate two alternate video quality metrics, blockiness and mutual information, and show how they can be used to estimate the critical video qualities for face analysis algorithms.",
    "cited_by_count": 62,
    "openalex_id": "https://openalex.org/W2030253454",
    "type": "article"
  },
  {
    "title": "Optimal Selection of Adaptive Streaming Representations",
    "doi": "https://doi.org/10.1145/2700294",
    "publication_date": "2015-02-24",
    "publication_year": 2015,
    "authors": "Laura Toni; Ramón Aparicio-Pardo; Karine Pires; Gwendal Simon; Alberto Blanc; Pascal Frossard",
    "corresponding_authors": "",
    "abstract": "Adaptive streaming addresses the increasing and heterogenous demand of multimedia content over the Internet by offering several encoded versions for each video sequence. Each version (or representation) has a different resolution and bit rate, aimed at a specific set of users, like TV or mobile phone clients. While most existing works on adaptive streaming deal with effective playout-control strategies at the client side, we take in this paper a providers' perspective and propose solutions to improve user satisfaction by optimizing the encoding rates of the video sequences. We formulate an integer linear program that maximizes users' average satisfaction, taking into account the network dynamics, the video content information, and the user population characteristics. The solution of the optimization is a set of encoding parameters that permit to create different streams to robustly satisfy users' requests over time. We simulate multiple adaptive streaming sessions characterized by realistic network connections models, where the proposed solution outperforms commonly used vendor recommendations, in terms of user satisfaction but also in terms of fairness and outage probability. The simulation results further show that video content information as well as network constraints and users' statistics play a crucial role in selecting proper encoding parameters to provide fairness a mong users and to reduce network resource usage. We finally propose a few practical guidelines that can be used to choose the encoding parameters based on the user base characteristics, the network capacity and the type of video content.",
    "cited_by_count": 61,
    "openalex_id": "https://openalex.org/W2021757921",
    "type": "article"
  },
  {
    "title": "QoE-Based Low-Delay Live Streaming Using Throughput Predictions",
    "doi": "https://doi.org/10.1145/2990505",
    "publication_date": "2016-10-25",
    "publication_year": 2016,
    "authors": "Konstantin Miller; Abdel-Karim Al-Tamimi; Adam Wolisz",
    "corresponding_authors": "",
    "abstract": "Recently, Hypertext Transfer Protocol (HTTP)-based adaptive streaming has become the de facto standard for video streaming over the Internet. It allows clients to dynamically adapt media characteristics to the varying network conditions to ensure a high quality of experience (QoE)—that is, minimize playback interruptions while maximizing video quality at a reasonable level of quality changes. In the case of live streaming, this task becomes particularly challenging due to the latency constraints. The challenge further increases if a client uses a wireless access network, where the throughput is subject to considerable fluctuations. Consequently, live streams often exhibit latencies of up to 20 to 30 seconds. In the present work, we introduce an adaptation algorithm for HTTP-based live streaming called LOLYPOP (short for low-latency prediction-based adaptation), which is designed to operate with a transport latency of a few seconds. To reach this goal, LOLYPOP leverages Transmission Control Protocol throughput predictions on multiple time scales, from 1 to 10 seconds, along with estimations of the relative prediction error distributions. In addition to satisfying the latency constraint, the algorithm heuristically maximizes the QoE by maximizing the average video quality as a function of the number of skipped segments and quality transitions. To select an efficient prediction method, we studied the performance of several time series prediction methods in IEEE 802.11 wireless access networks. We evaluated LOLYPOP under a large set of experimental conditions, limiting the transport latency to 3 seconds, against a state-of-the-art adaptation algorithm called FESTIVE . We observed that the average selected video representation index is by up to a factor of 3 higher than with the baseline approach. We also observed that LOLYPOP is able to reach points from a broader region in the QoE space, and thus it is better adjustable to the user profile or service provider requirements.",
    "cited_by_count": 60,
    "openalex_id": "https://openalex.org/W2292702228",
    "type": "article"
  },
  {
    "title": "Digital Lollipop",
    "doi": "https://doi.org/10.1145/2996462",
    "publication_date": "2016-10-25",
    "publication_year": 2016,
    "authors": "Nimesha Ranasinghe; Ellen Yi–Luen",
    "corresponding_authors": "",
    "abstract": "Among the five primary senses, the sense of taste is the least explored as a form of digital media applied in Human--Computer Interface. This article presents an experimental instrument, the Digital Lollipop, for digitally simulating the sensation of taste (gustation) by utilizing electrical stimulation on the human tongue. The system is capable of manipulating the properties of electric currents (magnitude, frequency, and polarity) to formulate different stimuli. To evaluate the effectiveness of this method, the system was experimentally tested in two studies. The first experiment was conducted using separate regions of the human tongue to record occurrences of basic taste sensations and their respective intensity levels. The results indicate occurrences of sour, salty, bitter, and sweet sensations from different regions of the tongue. One of the major discoveries of this experiment was that the sweet taste emerges via an inverse-current mechanism, which deserves further research in the future. The second study was conducted to compare natural and artificial (virtual) sour taste sensations and examine the possibility of effectively controlling the artificial sour taste at three intensity levels (mild, medium, and strong). The proposed method is attractive since it does not require any chemical solutions and facilitates further research opportunities in several directions including human--computer interaction, virtual reality, food and beverage, as well as medicine.",
    "cited_by_count": 60,
    "openalex_id": "https://openalex.org/W2536728667",
    "type": "article"
  },
  {
    "title": "Effective transfer tagging from image to video",
    "doi": "https://doi.org/10.1145/2457450.2457456",
    "publication_date": "2013-05-01",
    "publication_year": 2013,
    "authors": "Shuangming Yang; Yi Yang; Heng Tao Shen",
    "corresponding_authors": "",
    "abstract": "Recent years have witnessed a great explosion of user-generated videos on the Web. In order to achieve an effective and efficient video search, it is critical for modern video search engines to associate videos with semantic keywords automatically. Most of the existing video tagging methods can hardly achieve reliable performance due to deficiency of training data. It is noticed that abundant well-tagged data are available in other relevant types of media (e.g., images). In this article, we propose a novel video tagging framework, termed as Cross-Media Tag Transfer (CMTT), which utilizes the abundance of well-tagged images to facilitate video tagging. Specifically, we build a “cross-media tunnel” to transfer knowledge from images to videos. To this end, an optimal kernel space, in which distribution distance between images and video is minimized, is found to tackle the domain-shift problem. A novel cross-media video tagging model is proposed to infer tags by exploring the intrinsic local structures of both labeled and unlabeled data, and learn reliable video classifiers. An efficient algorithm is designed to optimize the proposed model in an iterative and alternative way. Extensive experiments illustrate the superiority of our proposal compared to the state-of-the-art algorithms.",
    "cited_by_count": 58,
    "openalex_id": "https://openalex.org/W1994179401",
    "type": "article"
  },
  {
    "title": "EEG Correlates of Pleasant and Unpleasant Odor Perception",
    "doi": "https://doi.org/10.1145/2637287",
    "publication_date": "2014-10-01",
    "publication_year": 2014,
    "authors": "Eleni Kroupi; Ashkan Yazdani; Jean-Marc Vésin; Touradj Ebrahimi",
    "corresponding_authors": "",
    "abstract": "Olfaction-enhanced multimedia experience is becoming vital for strengthening the sensation of reality and the quality of user experience. One approach to investigate olfactory perception is to analyze the alterations in brain activity during stimulation with different odors. In this article, the changes in the electroencephalogram (EEG) when perceiving hedonically-different odors are studied. Results of within and across-subject analysis are presented. We show that EEG-based odor classification using brain activity is possible and can be used to automatically recognize odor pleasantness when a subject-specific classifier is trained. However, it is a challenging problem to design a generic classifier.",
    "cited_by_count": 58,
    "openalex_id": "https://openalex.org/W2009767195",
    "type": "article"
  },
  {
    "title": "Multiple-Scent Enhanced Multimedia Synchronization",
    "doi": "https://doi.org/10.1145/2637293",
    "publication_date": "2014-10-01",
    "publication_year": 2014,
    "authors": "Niall Murray; Brian Lee; Yuansong Qiao; Gabriel‐Miro Muntean",
    "corresponding_authors": "",
    "abstract": "This study looked at users' perception of interstream synchronization between audiovisual media and two olfactory streams. The ability to detect skews and the perception and impact of skews on user Quality of Experience (QoE) is analyzed. The olfactory streams are presented with the same skews (i.e., delay) and with variable skews (i.e., jitter and mix of scents). This article reports the limits beyond which desynchronization reduces user-perceived quality levels. Also, a minimum gap between the presentations of consecutive scents is identified, necessary to ensuring enhanced user-perceived quality. There is no evidence (not considering scent type) that overlapping or mixing of scents increases user QoE levels for olfaction-enhanced multimedia.",
    "cited_by_count": 58,
    "openalex_id": "https://openalex.org/W2150738409",
    "type": "article"
  },
  {
    "title": "Adaptive Fractional-Pixel Motion Estimation Skipped Algorithm for Efficient HEVC Motion Estimation",
    "doi": "https://doi.org/10.1145/3159170",
    "publication_date": "2018-01-04",
    "publication_year": 2018,
    "authors": "Zhaoqing Pan; Jianjun Lei; Yajuan Zhang; Fu Lee Wang",
    "corresponding_authors": "",
    "abstract": "High-Efficiency Video Coding (HEVC) efficiently addresses the storage and transmit problems of high-definition videos, especially for 4K videos. The variable-size Prediction Units (PUs)--based Motion Estimation (ME) contributes a significant compression rate to the HEVC encoder and also generates a huge computation load. Meanwhile, high-level encoding complexity prevents widespread adoption of the HEVC encoder in multimedia systems. In this article, an adaptive fractional-pixel ME skipped scheme is proposed for low-complexity HEVC ME. First, based on the property of the variable-size PUs--based ME process and the video content partition relationship among variable-size PUs, all inter-PU modes during a coding unit encoding process are classified into root-type PU mode and children-type PU modes. Then, according to the ME result of the root-type PU mode, the fractional-pixel ME of its children-type PU modes is adaptively skipped. Simulation results show that, compared to the original ME in HEVC reference software, the proposed algorithm reduces ME encoding time by an average of 63.22% while encoding efficiency performance is maintained.",
    "cited_by_count": 58,
    "openalex_id": "https://openalex.org/W2781560105",
    "type": "article"
  },
  {
    "title": "Real-Time Load Reduction in Multimedia Big Data for Mobile Internet",
    "doi": "https://doi.org/10.1145/2990473",
    "publication_date": "2016-10-12",
    "publication_year": 2016,
    "authors": "Kun Wang; Jun Mi; Chenhan Xu; Qingquan Zhu; Lei Shu; Der‐Jiunn Deng",
    "corresponding_authors": "",
    "abstract": "In the age of multimedia big data, the popularity of mobile devices has been in an unprecedented growth, the speed of data increasing is faster than ever before, and Internet traffic is rapidly increasing, not only in volume but also in heterogeneity. Therefore, data processing and network overload have become two urgent problems. To address these problems, extensive papers have been published on image analysis using deep learning, but only a few works have exploited this approach for video analysis. In this article, a hybrid-stream model is proposed to solve these problems for video analysis. Functionality of this model covers Data Preprocessing, Data Classification, and Data-Load-Reduction Processing. Specifically, an improved Convolutional Neural Networks (CNN) classification algorithm is designed to evaluate the importance of each video frame and video clip to enhance classification precision. Then, a reliable keyframe extraction mechanism will recognize the importance of each frame or clip, and decide whether to abandon it automatically by a series of correlation operations. The model will reduce data load to a dynamic threshold changed by σ, control the input size of the video in mobile Internet, and thus reduce network overload. Through experimental simulations, we find that the size of processed video has been effectively reduced and the quality of experience (QoE) has not been lowered due to a suitably selected parameter η. The simulation also shows that the model has a steady performance and is powerful enough for continuously growing multimedia big data.",
    "cited_by_count": 57,
    "openalex_id": "https://openalex.org/W2529891837",
    "type": "article"
  },
  {
    "title": "Online Early-Late Fusion Based on Adaptive HMM for Sign Language Recognition",
    "doi": "https://doi.org/10.1145/3152121",
    "publication_date": "2017-12-20",
    "publication_year": 2017,
    "authors": "Dan Guo; Wengang Zhou; Houqiang Li; Meng Wang",
    "corresponding_authors": "",
    "abstract": "In sign language recognition (SLR) with multimodal data, a sign word can be represented by multiply features, for which there exist an intrinsic property and a mutually complementary relationship among them. To fully explore those relationships, we propose an online early-late fusion method based on the adaptive Hidden Markov Model (HMM). In terms of the intrinsic property, we discover that inherent latent change states of each sign are related not only to the number of key gestures and body poses but also to their translation relationships. We propose an adaptive HMM method to obtain the hidden state number of each sign by affinity propagation clustering. For the complementary relationship, we propose an online early-late fusion scheme. The early fusion (feature fusion) is dedicated to preserving useful information to achieve a better complementary score, while the late fusion (score fusion) uncovers the significance of those features and aggregates them in a weighting manner. Different from classical fusion methods, the fusion is query adaptive. For different queries, after feature selection (including the combined feature), the fusion weight is inversely proportional to the area under the curve of the normalized query score list for each selected feature. The whole fusion process is effective and efficient. Experiments verify the effectiveness on the signer-independent SLR with large vocabulary. Compared either on different dataset sizes or to different SLR models, our method demonstrates consistent and promising performance.",
    "cited_by_count": 56,
    "openalex_id": "https://openalex.org/W2776541261",
    "type": "article"
  },
  {
    "title": "Twitter is Faster",
    "doi": "https://doi.org/10.1145/2637285",
    "publication_date": "2015-01-07",
    "publication_year": 2015,
    "authors": "Zhengyu Deng; Ming Yan; Jitao Sang; Changsheng Xu",
    "corresponding_authors": "",
    "abstract": "Traditional personalized video recommendation methods focus on utilizing user profile or user history behaviors to model user interests, which follows a static strategy and fails to capture the swift shift of the short-term interests of users. According to our cross-platform data analysis, the information emergence and propagation is faster in social textual stream-based platforms than that in multimedia sharing platforms at micro user level. Inspired by this, we propose a dynamic user modeling strategy to tackle personalized video recommendation issues in the multimedia sharing platform YouTube, by transferring knowledge from the social textual stream-based platform Twitter. In particular, the cross-platform video recommendation strategy is divided into two steps. (1) Real-time hot topic detection: the hot topics that users are currently following are extracted from users' tweets, which are utilized to obtain the related videos in YouTube. (2) Time-aware video recommendation: for the target user in YouTube, the obtained videos are ranked by considering the user profile in YouTube, time factor, and quality factor to generate the final recommendation list. In this way, the short-term (hot topics) and long-term (user profile) interests of users are jointly considered. Carefully designed experiments have demonstrated the advantages of the proposed method.",
    "cited_by_count": 55,
    "openalex_id": "https://openalex.org/W2050544466",
    "type": "article"
  },
  {
    "title": "A Tucker Deep Computation Model for Mobile Multimedia Feature Learning",
    "doi": "https://doi.org/10.1145/3063593",
    "publication_date": "2017-08-10",
    "publication_year": 2017,
    "authors": "Qingchen Zhang; Laurence T. Yang; Xingang Liu; Zhikui Chen; Peng Li",
    "corresponding_authors": "",
    "abstract": "Recently, the deep computation model, as a tensor deep learning model, has achieved super performance for multimedia feature learning. However, the conventional deep computation model involves a large number of parameters. Typically, training a deep computation model with millions of parameters needs high-performance servers with large-scale memory and powerful computing units, limiting the growth of the model size for multimedia feature learning on common devices such as portable CPUs and conventional desktops. To tackle this problem, this article proposes a Tucker deep computation model by using the Tucker decomposition to compress the weight tensors in the full-connected layers for multimedia feature learning. Furthermore, a learning algorithm based on the back-propagation strategy is devised to train the parameters of the Tucker deep computation model. Finally, the performance of the Tucker deep computation model is evaluated by comparing with the conventional deep computation model on two representative multimedia datasets, that is, CUAVE and SNAE2, in terms of accuracy drop, parameter reduction, and speedup in the experiments. Results imply that the Tucker deep computation model can achieve a large-parameter reduction and speedup with a small accuracy drop for multimedia feature learning.",
    "cited_by_count": 55,
    "openalex_id": "https://openalex.org/W2744816398",
    "type": "article"
  },
  {
    "title": "Enabling Live Video Analytics with a Scalable and Privacy-Aware Framework",
    "doi": "https://doi.org/10.1145/3209659",
    "publication_date": "2018-06-15",
    "publication_year": 2018,
    "authors": "Junjue Wang; Brandon Amos; Anupam Das; Padmanabhan Pillai; Norman Sadeh; Mahadev Satyanarayanan",
    "corresponding_authors": "",
    "abstract": "We show how to build the components of a privacy-aware, live video analytics ecosystem from the bottom up, starting with OpenFace, our new open-source face recognition system that approaches state-of-the-art accuracy. Integrating OpenFace with interframe tracking, we build RTFace, a mechanism for denaturing video streams that selectively blurs faces according to specified policies at full frame rates. This enables privacy management for live video analytics while providing a secure approach for handling retrospective policy exceptions. Finally, we present a scalable, privacy-aware architecture for large camera networks using RTFace and show how it can be an enabler for a vibrant ecosystem and marketplace of privacy-aware video streams and analytics services.",
    "cited_by_count": 53,
    "openalex_id": "https://openalex.org/W2808982453",
    "type": "article"
  },
  {
    "title": "Deep Triplet Neural Networks with Cluster-CCA for Audio-Visual Cross-Modal Retrieval",
    "doi": "https://doi.org/10.1145/3387164",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "Donghuo Zeng; Yi Yu; Keizo Oyama",
    "corresponding_authors": "",
    "abstract": "Cross-modal retrieval aims to retrieve data in one modality by a query in another modality, which has been a very interesting research issue in the field of multimedia, information retrieval, and computer vision, and database. Most existing works focus on cross-modal retrieval between text-image, text-video, and lyrics-audio. Little research addresses cross-modal retrieval between audio and video due to limited audio-video paired datasets and semantic information. The main challenge of the audio-visual cross-modal retrieval task focuses on learning joint embeddings from a shared subspace for computing the similarity across different modalities, where generating new representations is to maximize the correlation between audio and visual modalities space. In this work, we propose TNN-C-CCA, a novel deep triplet neural network with cluster canonical correlation analysis, which is an end-to-end supervised learning architecture with an audio branch and a video branch. We not only consider the matching pairs in the common space but also compute the mismatching pairs when maximizing the correlation. In particular, two significant contributions are made. First, a better representation by constructing a deep triplet neural network with triplet loss for optimal projections can be generated to maximize correlation in the shared subspace. Second, positive examples and negative examples are used in the learning stage to improve the capability of embedding learning between audio and video. Our experiment is run over fivefold cross validation, where average performance is applied to demonstrate the performance of audio-video cross-modal retrieval. The experimental results achieved on two different audio-visual datasets show that the proposed learning architecture with two branches outperforms existing six canonical correlation analysis–based methods and four state-of-the-art-based cross-modal retrieval methods.",
    "cited_by_count": 53,
    "openalex_id": "https://openalex.org/W3041053424",
    "type": "article"
  },
  {
    "title": "Adaptive Exploration for Unsupervised Person Re-identification",
    "doi": "https://doi.org/10.1145/3369393",
    "publication_date": "2020-02-17",
    "publication_year": 2020,
    "authors": "Yuhang Ding; Hehe Fan; Mingliang Xu; Yi Yang",
    "corresponding_authors": "",
    "abstract": "Due to domain bias, directly deploying a deep person re-identification (re-ID) model trained on one dataset often achieves considerably poor accuracy on another dataset. In this article, we propose an Adaptive Exploration (AE) method to address the domain-shift problem for re-ID in an unsupervised manner. Specifically, in the target domain, the re-ID model is inducted to (1) maximize distances between all person images and (2) minimize distances between similar person images. In the first case, by treating each person image as an individual class, a non-parametric classifier with a feature memory is exploited to encourage person images to move far away from each other. In the second case, according to a similarity threshold, our method adaptively selects neighborhoods for each person image in the feature space. By treating these similar person images as the same class, the non-parametric classifier forces them to stay closer. However, a problem of the adaptive selection is that, when an image has too many neighborhoods, it is more likely to attract other images as its neighborhoods. As a result, a minority of images may select a large number of neighborhoods while a majority of images has only a few neighborhoods. To address this issue, we additionally integrate a balance strategy into the adaptive selection. We evaluate our methods with two protocols. The first one is called “target-only re-ID”, in which only the unlabeled target data is used for training. The second one is called “domain adaptive re-ID”, in which both the source data and the target data are used during training. Experimental results on large-scale re-ID datasets demonstrate the effectiveness of our method. Our code has been released at https://github.com/dyh127/Adaptive-Exploration-for-Unsupervised-Person-Re-Identification.",
    "cited_by_count": 52,
    "openalex_id": "https://openalex.org/W2961917444",
    "type": "article"
  },
  {
    "title": "Blind Image Quality Assessment by Natural Scene Statistics and Perceptual Characteristics",
    "doi": "https://doi.org/10.1145/3414837",
    "publication_date": "2020-08-25",
    "publication_year": 2020,
    "authors": "Yutao Liu; Ke Gu; Xiu Li; Yongbing Zhang",
    "corresponding_authors": "",
    "abstract": "Opinion-unaware blind image quality assessment (OU BIQA) refers to establishing a blind quality prediction model without using the expensive subjective quality scores, which is a highly promising direction in the BIQA research. In this article, we focus on OU BIQA and propose a novel OU BIQA method. Specifically, in our proposed method, we deeply investigate the natural scene statistics (NSS) and the perceptual characteristics of the human brain for visual perception. Accordingly, a set of quality-aware NSS and perceptual characteristics-related features are designed to characterize the image quality effectively. For inferring the image quality, we learn a pristine multivariate Gaussian (MVG) model on a collection of pristine images, which serves as the reference information for quality evaluation. At last, the quality of a new given image is defined by measuring the divergence between its MVG model and the learned pristine MVG model. Thorough experiments performed on seven popular image databases demonstrate that the proposed OU BIQA method delivers superior performance to the state-of-the-art OU BIQA methods. The Matlab source code of the proposed method will be made publicly available at https://github.com/YT2015?tab=;repositories.",
    "cited_by_count": 52,
    "openalex_id": "https://openalex.org/W3080766718",
    "type": "article"
  },
  {
    "title": "Ensemble of Deep Models for Event Recognition",
    "doi": "https://doi.org/10.1145/3199668",
    "publication_date": "2018-05-01",
    "publication_year": 2018,
    "authors": "Kashif Ahmad; Mohamed Lamine Mekhalfi; Nicola Conci; Farid Melgani; Francesco G. B. De Natale",
    "corresponding_authors": "",
    "abstract": "In this article, we address the problem of recognizing an event from a single related picture. Given the large number of event classes and the limited information contained in a single shot, the problem is known to be particularly hard. To achieve a reliable detection, we propose a combination of multiple classifiers, and we compare three alternative strategies to fuse the results of each classifier, namely: (i) induced order weighted averaging operators, (ii) genetic algorithms, and (iii) particle swarm optimization. Each method is aimed at determining the optimal weights to be assigned to the decision scores yielded by different deep models, according to the relevant optimization strategy. Experimental tests have been performed on three event recognition datasets, evaluating the performance of various deep models, both alone and selectively combined. Experimental results demonstrate that the proposed approach outperforms traditional multiple classifier solutions based on uniform weighting, and outperforms recent state-of-the-art approaches.",
    "cited_by_count": 51,
    "openalex_id": "https://openalex.org/W2800124572",
    "type": "article"
  },
  {
    "title": "Recurrent Attention Network with Reinforced Generator for Visual Dialog",
    "doi": "https://doi.org/10.1145/3390891",
    "publication_date": "2020-07-05",
    "publication_year": 2020,
    "authors": "Hehe Fan; Linchao Zhu; Yi Yang; Fei Wu",
    "corresponding_authors": "",
    "abstract": "In Visual Dialog, an agent has to parse temporal context in the dialog history and spatial context in the image to hold a meaningful dialog with humans. For example, to answer “what is the man on her left wearing?” the agent needs to (1) analyze the temporal context in the dialog history to infer who is being referred to as “her,” (2) parse the image to attend “her,” and (3) uncover the spatial context to shift the attention to “her left” and check the apparel of the man. In this article, we use a dialog network to memorize the temporal context and an attention processor to parse the spatial context. Since the question and the image are usually very complex, which makes it difficult for the question to be grounded with a single glimpse, the attention processor attends to the image multiple times to better collect visual information. In the Visual Dialog task, the generative decoder (G) is trained under the word-by-word paradigm, which suffers from the lack of sentence-level training. We propose to reinforce G at the sentence level using the discriminative model (D), which aims to select the right answer from a few candidates, to ameliorate the problem. Experimental results on the VisDial dataset demonstrate the effectiveness of our approach.",
    "cited_by_count": 50,
    "openalex_id": "https://openalex.org/W3038528491",
    "type": "article"
  },
  {
    "title": "When Game Becomes Life",
    "doi": "https://doi.org/10.1145/2957750",
    "publication_date": "2016-08-03",
    "publication_year": 2016,
    "authors": "Adele Lu Jia; Siqi Shen; Dick Epema; Alexandru Iosup",
    "corresponding_authors": "",
    "abstract": "Online gaming franchises such as World of Tanks, Defense of the Ancients, and StarCraft have attracted hundreds of millions of users who, apart from playing the game, also socialize with each other through gaming and viewing gamecasts. As a form of User Generated Content (UGC), gamecasts play an important role in user entertainment and gamer education. They deserve the attention of both industrial partners and the academic communities, corresponding to the large amount of revenue involved and the interesting research problems associated with UGC sites and social networks. Although previous work has put much effort into analyzing general UGC sites such as YouTube, relatively little is known about the gamecast sharing sites. In this work, we provide the first comprehensive study of gamecast sharing sites, including commercial streaming-based sites such as Amazon’s Twitch.tv and community-maintained replay-based sites such as WoTreplays. We collect and share a novel dataset on WoTreplays that includes more than 380,000 game replays, shared by more than 60,000 creators with more than 1.9 million gamers. Together with an earlier published dataset on Twitch.tv, we investigate basic characteristics of gamecast sharing sites, and we analyze the activities of their creators and spectators. Among our results, we find that (i) WoTreplays and Twitch.tv are both fast-consumed repositories, with millions of gamecasts being uploaded, viewed, and soon forgotten; (ii) both the gamecasts and the creators exhibit highly skewed popularity, with a significant heavy tail phenomenon; and (iii) the upload and download preferences of creators and spectators are different: while the creators emphasize their individual skills, the spectators appreciate team-wise tactics. Our findings provide important knowledge for infrastructure and service improvement, for example, in the design of proper resource allocation mechanisms that consider future gamecasting and in the tuning of incentive policies that further help player retention.",
    "cited_by_count": 49,
    "openalex_id": "https://openalex.org/W2505131069",
    "type": "article"
  },
  {
    "title": "How Deep Features Have Improved Event Recognition in Multimedia",
    "doi": "https://doi.org/10.1145/3306240",
    "publication_date": "2019-05-31",
    "publication_year": 2019,
    "authors": "Kashif Ahmad; Nicola Conci",
    "corresponding_authors": "",
    "abstract": "Event recognition is one of the areas in multimedia that is attracting great attention of researchers. Being applicable in a wide range of applications, from personal to collective events, a number of interesting solutions for event recognition using multimedia information sources have been proposed. On the other hand, following their immense success in classification, object recognition, and detection, deep learning has been shown to perform well in event recognition tasks also. Thus, a large portion of the literature on event analysis relies nowadays on deep learning architectures. In this article, we provide an extensive overview of the existing literature in this field, analyzing how deep features and deep learning architectures have changed the performance of event recognition frameworks. The literature on event-based analysis of multimedia contents can be categorized into four groups, namely (i) event recognition in single images; (ii) event recognition in personal photo collections; (iii) event recognition in videos; and (iv) event recognition in audio recordings. In this article, we extensively review different deep-learning-based frameworks for event recognition in these four domains. Furthermore, we also review some benchmark datasets made available to the scientific community to validate novel event recognition pipelines. In the final part of the manuscript, we also provide a detailed discussion on basic insights gathered from the literature review, and identify future trends and challenges.",
    "cited_by_count": 49,
    "openalex_id": "https://openalex.org/W2950746339",
    "type": "article"
  },
  {
    "title": "Visual Attention Analysis and Prediction on Human Faces for Children with Autism Spectrum Disorder",
    "doi": "https://doi.org/10.1145/3337066",
    "publication_date": "2019-10-15",
    "publication_year": 2019,
    "authors": "Huiyu Duan; Xiongkuo Min; Yi Fang; Lei Fan; Xiaokang Yang; Guangtao Zhai",
    "corresponding_authors": "",
    "abstract": "The focus of this article is to analyze and predict the visual attention of children with Autism Spectrum Disorder (ASD) when looking at human faces. Social difficulties are the hallmark features of ASD and will lead to atypical visual attention toward various stimuli more or less, especially on human faces. Learning the visual attention of children with ASD could contribute to related research in the field of medical science, psychology, and education. We first construct a Visual Attention on Faces for Autism Spectrum Disorder (VAFA) database, which consists of 300 natural scene images with human faces and corresponding eye movement data collected from 13 children with ASD. Compared with matched typically developing (TD) controls, we quantify atypical visual attention on human faces in ASD. Statistics show that some high-level factors such as face size, facial features, face pose, and facial emotions have different impacts on the visual attention of children with ASD. Combining the feature maps extracted from the state-of-the-art saliency models, we get the visual attention model on human faces for individuals with ASD. The proposed model shows the best performance among all competitors. With the help of our proposed model, researchers in related fields could design specialized education contents containing human faces for the children with ASD or produce the specific model for rapidly screening ASD using their eye movement data.",
    "cited_by_count": 47,
    "openalex_id": "https://openalex.org/W2980919678",
    "type": "article"
  },
  {
    "title": "Active Balancing Mechanism for Imbalanced Medical Data in Deep Learning–Based Classification Models",
    "doi": "https://doi.org/10.1145/3357253",
    "publication_date": "2020-01-31",
    "publication_year": 2020,
    "authors": "Hongyi Zhang; Haoke Zhang; Sandeep Pirbhulal; Wanqing Wu; Victor Hugo C. de Albuquerque",
    "corresponding_authors": "",
    "abstract": "Imbalanced data always has a serious impact on a predictive model, and most under-sampling techniques consume more time and suffer from loss of samples containing critical information during imbalanced data processing, especially in the biomedical field. To solve these problems, we developed an active balancing mechanism (ABM) based on valuable information contained in the biomedical data. ABM adopts the Gaussian naïve Bayes method to estimate the object samples and entropy as a query function to evaluate sample information and only retains valuable samples of the majority class to achieve under-sampling. The Physikalisch Technische Bundesanstalt diagnostic electrocardiogram (ECG) database, including 5,173 normal ECG samples and 26,654 myocardial infarction ECG samples, is applied to verify the validity of ABM. At imbalance rates of 13 and 5, experimental results reveal that ABM takes 7.7 seconds and 13.2 seconds, respectively. Both results are significantly faster than five conventional under-sampling methods. In addition, at the imbalance rate of 13, ABM-based data obtained the highest accuracy of 92.23% and 97.52% using support vector machines and modified convolutional neural networks (MCNNs) with eight layers, respectively. At the imbalance rate of 5, the processed data by ABM also achieved the best accuracy of 92.31% and 98.46% based on support vector machines and MCNNs, respectively. Furthermore, ABM has better performance than two compared methods in F 1-measure, G-means, and area under the curve. Consequently, ABM could be a useful and effective approach to deal with imbalanced data in general, particularly biomedical myocardial infarction ECG datasets, and the MCNN can also achieve higher performance compared to the state of the art.",
    "cited_by_count": 46,
    "openalex_id": "https://openalex.org/W3013629901",
    "type": "article"
  },
  {
    "title": "Human Activity Recognition from Multiple Sensors Data Using Multi-fusion Representations and CNNs",
    "doi": "https://doi.org/10.1145/3377882",
    "publication_date": "2020-05-22",
    "publication_year": 2020,
    "authors": "Farzan Majeed Noori; Michael A. Riegler; Md. Zia Uddin; Jim Tørresen",
    "corresponding_authors": "",
    "abstract": "With the emerging interest in the ubiquitous sensing field, it has become possible to build assistive technologies for persons during their daily life activities to provide personalized feedback and services. For instance, it is possible to detect an individual’s behavioral pattern (e.g., physical activity, location, and mood) by using sensors embedded in smart-watches and smartphones. The multi-sensor environments also come with some challenges, such as how to fuse and combine different sources of data. In this article, we explore several methods of fusion for multi-representations of data from sensors. Furthermore, multiple representations of sensor data were generated and then fused using data-level, feature-level , and decision-level fusions . The presented methods were evaluated using three publicly available human activity recognition (HAR) datasets. The presented approaches utilize Deep Convolutional Neural Networks (CNNs). A generic architecture for fusion of different sensors is proposed. The proposed method shows promising performance, with the best results reaching an overall accuracy of 98.4% for the Context-Awareness via Wrist-Worn Motion Sensors (HANDY) dataset and 98.7% for the Wireless Sensor Data Mining (WISDM version 1.1) dataset. Both results outperform previous approaches.",
    "cited_by_count": 46,
    "openalex_id": "https://openalex.org/W3030949666",
    "type": "article"
  },
  {
    "title": "Random Forest with Self-Paced Bootstrap Learning in Lung Cancer Prognosis",
    "doi": "https://doi.org/10.1145/3345314",
    "publication_date": "2020-01-31",
    "publication_year": 2020,
    "authors": "Qingyong Wang; Yun Zhou; Weiping Ding; Zhiguo Zhang; Khan Muhammad; Zehong Cao",
    "corresponding_authors": "",
    "abstract": "Training gene expression data with supervised learning approaches can provide an alarm sign for early treatment of lung cancer to decrease death rates. However, the samples of gene features involve lots of noises in a realistic environment. In this study, we present a random forest with self-paced learning bootstrap for improvement of lung cancer classification and prognosis based on gene expression data. To be specific, we propose an ensemble learning with random forest approach to improving the model classification performance by selecting multi-classifiers. Then, we investigate the sampling strategy by gradually embedding from high- to low-quality samples by self-paced learning. The experimental results based on five public lung cancer datasets show that our proposed method could select significant genes exactly, which improves classification performance compared to that of existing approaches. We believe that our proposed method has the potential to assist doctors in gene selections and lung cancer prognosis.",
    "cited_by_count": 45,
    "openalex_id": "https://openalex.org/W3021432747",
    "type": "article"
  },
  {
    "title": "Label Consistent Flexible Matrix Factorization Hashing for Efficient Cross-modal Retrieval",
    "doi": "https://doi.org/10.1145/3446774",
    "publication_date": "2021-07-22",
    "publication_year": 2021,
    "authors": "Donglin Zhang; Xiao‐Jun Wu; Jun Yu",
    "corresponding_authors": "",
    "abstract": "Hashing methods have sparked a great revolution on large-scale cross-media search due to its effectiveness and efficiency. Most existing approaches learn unified hash representation in a common Hamming space to represent all multimodal data. However, the unified hash codes may not characterize the cross-modal data discriminatively, because the data may vary greatly due to its different dimensionalities, physical properties, and statistical information. In addition, most existing supervised cross-modal algorithms preserve the similarity relationship by constructing an n × n pairwise similarity matrix, which requires a large amount of calculation and loses the category information. To mitigate these issues, a novel cross-media hashing approach is proposed in this article, dubbed label flexible matrix factorization hashing (LFMH). Specifically, LFMH jointly learns the modality-specific latent subspace with similar semantic by the flexible matrix factorization. In addition, LFMH guides the hash learning by utilizing the semantic labels directly instead of the large n × n pairwise similarity matrix. LFMH transforms the heterogeneous data into modality-specific latent semantic representation. Therefore, we can obtain the hash codes by quantifying the representations, and the learned hash codes are consistent with the supervised labels of multimodal data. Then, we can obtain the similar binary codes of the corresponding modality, and the binary codes can characterize such samples flexibly. Accordingly, the derived hash codes have more discriminative power for single-modal and cross-modal retrieval tasks. Extensive experiments on eight different databases demonstrate that our model outperforms some competitive approaches.",
    "cited_by_count": 45,
    "openalex_id": "https://openalex.org/W3186995366",
    "type": "article"
  },
  {
    "title": "Exploring Image Enhancement for Salient Object Detection in Low Light Images",
    "doi": "https://doi.org/10.1145/3414839",
    "publication_date": "2021-01-31",
    "publication_year": 2021,
    "authors": "Xin Xu; Shiqin Wang; Zheng Wang; Xiaolong Zhang; Ruimin Hu",
    "corresponding_authors": "",
    "abstract": "Low light images captured in a non-uniform illumination environment usually are degraded with the scene depth and the corresponding environment lights. This degradation results in severe object information loss in the degraded image modality, which makes the salient object detection more challenging due to low contrast property and artificial light influence. However, existing salient object detection models are developed based on the assumption that the images are captured under a sufficient brightness environment, which is impractical in real-world scenarios. In this work, we propose an image enhancement approach to facilitate the salient object detection in low light images. The proposed model directly embeds the physical lighting model into the deep neural network to describe the degradation of low light images, in which the environment light is treated as a point-wise variate and changes with local content. Moreover, a Non-Local-Block Layer is utilized to capture the difference of local content of an object against its local neighborhood favoring regions. To quantitative evaluation, we construct a low light Images dataset with pixel-level human-labeled ground-truth annotations and report promising results on four public datasets and our benchmark dataset.",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W3143419403",
    "type": "article"
  },
  {
    "title": "A Multi-agent Feature Selection and Hybrid Classification Model for Parkinson's Disease Diagnosis",
    "doi": "https://doi.org/10.1145/3433180",
    "publication_date": "2021-05-18",
    "publication_year": 2021,
    "authors": "Mazin Abed Mohammed; Mohamed Elhoseny; Karrar Hameed Abdulkareem; Salama A. Mostafa; Mashael Maashi",
    "corresponding_authors": "",
    "abstract": "Parkinson's disease (PD) diagnostics includes numerous analyses related to the neurological, physical, and psychical status of the patient. Medical teams analyze multiple symptoms and patient history considering verified genetic influences. The proposed method investigates the voice symptoms of this disease. The voice files are processed, and the feature extraction is conducted. Several machine learning techniques are used to recognize Parkinson's and healthy patients. This study focuses on examining PD diagnosis through voice data features. A new multi-agent feature filter (MAFT) algorithm is proposed to select the best features from the voice dataset. The MAFT algorithm is designed to select a set of features to improve the overall performance of prediction models and prevent over-fitting possibly due to extreme reduction to the features. Moreover, this algorithm aims to reduce the complexity of the prediction, accelerate the training phase, and build a robust training model. Ten different machine learning methods are then integrated with the MAFT algorithm to form a powerful voice-based PD diagnosis model. Recorded test results of the PD prediction model using the actual and filtered features yielded 86.38% and 86.67% accuracies on average, respectively. With the aid of the MAFT feature selection, the test results are improved by 3.2% considering the hybrid model (HM) and 3.1% considering the Naïve Bayesian and random forest. Subsequently, an HM, which comprises a binary convolutional neural network and three feature selection algorithms (namely, genetic algorithm, Adam optimizer, and mini-batch gradient descent), is proposed to improve the classification accuracy of the PD. The results reveal that PD achieves an overall accuracy of 93.7%. The HM is integrated with the MAFT, and the combination realizes an overall accuracy of 96.9%. These results demonstrate that the combination of the MAFT algorithm and the HM model significantly enhances the PD diagnosis outcomes.",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W3163092490",
    "type": "article"
  },
  {
    "title": "Global-Local Enhancement Network for NMF-Aware Sign Language Recognition",
    "doi": "https://doi.org/10.1145/3436754",
    "publication_date": "2021-07-22",
    "publication_year": 2021,
    "authors": "Hezhen Hu; Wengang Zhou; Junfu Pu; Houqiang Li",
    "corresponding_authors": "",
    "abstract": "Sign language recognition (SLR) is a challenging problem, involving complex manual features, i.e., hand gestures, and fine-grained non-manual features (NMFs), i.e., facial expression, mouth shapes, etc. Although manual features are dominant, non-manual features also play an important role in the expression of a sign word. Specifically, many sign words convey different meanings due to non-manual features, even though they share the same hand gestures. This ambiguity introduces great challenges in the recognition of sign words. To tackle the above issue, we propose a simple yet effective architecture called Global-local Enhancement Network (GLE-Net), including two mutually promoted streams towards different crucial aspects of SLR. Of the two streams, one captures the global contextual relationship, while the other stream captures the discriminative fine-grained cues. Moreover, due to the lack of datasets explicitly focusing on this kind of features, we introduce the first non-manual-features-aware isolated Chinese sign language dataset~(NMFs-CSL) with a total vocabulary size of 1,067 sign words in daily life. Extensive experiments on NMFs-CSL and SLR500 datasets demonstrate the effectiveness of our method.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W3185538031",
    "type": "article"
  },
  {
    "title": "Conditional LSTM-GAN for Melody Generation from Lyrics",
    "doi": "https://doi.org/10.1145/3424116",
    "publication_date": "2021-02-28",
    "publication_year": 2021,
    "authors": "Yi Yu; Abhishek Srivastava; Simon Canales",
    "corresponding_authors": "",
    "abstract": "Melody generation from lyrics has been a challenging research issue in the field of artificial intelligence and music, which enables us to learn and discover latent relationships between interesting lyrics and accompanying melodies. Unfortunately, the limited availability of a paired lyrics–melody dataset with alignment information has hindered the research progress. To address this problem, we create a large dataset consisting of 12,197 MIDI songs each with paired lyrics and melody alignment through leveraging different music sources where alignment relationship between syllables and music attributes is extracted. Most importantly, we propose a novel deep generative model, conditional Long Short-Term Memory (LSTM)–Generative Adversarial Network for melody generation from lyrics, which contains a deep LSTM generator and a deep LSTM discriminator both conditioned on lyrics. In particular, lyrics-conditioned melody and alignment relationship between syllables of given lyrics and notes of predicted melody are generated simultaneously. Extensive experimental results have proved the effectiveness of our proposed lyrics-to-melody generative model, where plausible and tuneful sequences can be inferred from lyrics.",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W3154236293",
    "type": "article"
  },
  {
    "title": "Part-wise Spatio-temporal Attention Driven CNN-based 3D Human Action Recognition",
    "doi": "https://doi.org/10.1145/3441628",
    "publication_date": "2021-07-22",
    "publication_year": 2021,
    "authors": "Chhavi Dhiman; Dinesh Kumar Vishwakarma; Paras Agarwal",
    "corresponding_authors": "",
    "abstract": "Recently, human activity recognition using skeleton data is increasing due to its ease of acquisition and finer shape details. Still, it suffers from a wide range of intra-class variation, inter-class similarity among the actions and view variation due to which extraction of discriminative spatial and temporal features is still a challenging problem. In this regard, we present a novel Residual Inception Attention Driven CNN (RIAC-Net) Network, which visualizes the dynamics of the action in a part-wise manner. The complete skeletonis partitioned into five key parts: Head to Spine, Left Leg, Right Leg, Left Hand, Right Hand. For each part, a Compact Action Skeleton Sequence (CASS) is defined. Part-wise skeleton-based motion dynamics highlights discriminative local features of the skeleton that helps to overcome the challenges of inter-class similarity and intra-class variation with improved recognition performance. The RIAC-Net architecture is inspired by the concept of inception-residual representation that unifies the Attention Driven Residues (ADR) with inception-based Spatio-Temporal Convolution Features (STCF) to learn efficient salient action features. An ablation study is also carried out to analyze the effect of ADR over simple residue-based action representation. The robustness of the proposed framework is evaluated by performing an extensive experiment on four challenging datasets: UT Kinect Action 3D, Florence 3D action, MSR Daily Action3D, and NTU RGB-D datasets, which consistently demonstrate the superiority of the proposed method over other state-of-the-art methods.",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W3186755040",
    "type": "article"
  },
  {
    "title": "Privacy-preserving Decentralized Learning Framework for Healthcare System",
    "doi": "https://doi.org/10.1145/3426474",
    "publication_date": "2021-06-14",
    "publication_year": 2021,
    "authors": "Harsh Kasyap; Somanath Tripathy",
    "corresponding_authors": "",
    "abstract": "Clinical trials and drug discovery would not be effective without the collaboration of institutions. Earlier, it has been at the cost of individual’s privacy. Several pacts and compliances have been enforced to avoid data breaches. The existing schemes collect the participant’s data to a central repository for learning predictions as the collaboration is indispensable for research advances. The current COVID pandemic has put a question mark on our existing setup where the existing data repository has proved to be obsolete. There is a need for contemporary data collection, processing, and learning. The smartphones and devices held by the last person of the society have also made them a potential contributor. It demands to design a distributed and decentralized Collaborative Learning system that would make the knowledge inference from every data point. Federated Learning [21], proposed by Google, brings the concept of in-place model training by keeping the data intact to the device. Though it is privacy-preserving in nature, however, it is susceptible to inference, poisoning, and Sybil attacks. Blockchain is a decentralized programming paradigm that provides a broader control of the system, making it attack resistant. It poses challenges of high computing power, storage, and latency. These emerging technologies can contribute to the desired learning system and motivate them to address their security and efficiency issues. This article systematizes the security issues in Federated Learning, its corresponding mitigation strategies, and Blockchain’s challenges. Further, a Blockchain-based Federated Learning architecture with two layers of participation is presented, which improves the global model accuracy and guarantees participant’s privacy. It leverages the channel mechanism of Blockchain for parallel model training and distribution. It facilitates establishing decentralized trust between the participants and the gateways using the Blockchain, which helps to have only honest participants.",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W3167050701",
    "type": "article"
  },
  {
    "title": "Cross-Modal Hybrid Feature Fusion for Image-Sentence Matching",
    "doi": "https://doi.org/10.1145/3458281",
    "publication_date": "2021-11-12",
    "publication_year": 2021,
    "authors": "Xing Xu; Yifan Wang; Yixuan He; Yang Yang; Alan Hanjalić; Heng Tao Shen",
    "corresponding_authors": "",
    "abstract": "Image-sentence matching is a challenging task in the field of language and vision, which aims at measuring the similarities between images and sentence descriptions. Most existing methods independently map the global features of images and sentences into a common space to calculate the image-sentence similarity. However, the image-sentence similarity obtained by these methods may be coarse as (1) an intermediate common space is introduced to implicitly match the heterogeneous features of images and sentences in a global level, and (2) only the inter-modality relations of images and sentences are captured while the intra-modality relations are ignored. To overcome the limitations, we propose a novel Cross-Modal Hybrid Feature Fusion (CMHF) framework for directly learning the image-sentence similarity by fusing multimodal features with inter- and intra-modality relations incorporated. It can robustly capture the high-level interactions between visual regions in images and words in sentences, where flexible attention mechanisms are utilized to generate effective attention flows within and across the modalities of images and sentences. A structured objective with ranking loss constraint is formed in CMHF to learn the image-sentence similarity based on the fused fine-grained features of different modalities bypassing the usage of intermediate common space. Extensive experiments and comprehensive analysis performed on two widely used datasets—Microsoft COCO and Flickr30K—show the effectiveness of the hybrid feature fusion framework in CMHF, in which the state-of-the-art matching performance is achieved by our proposed CMHF method.",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W3212847756",
    "type": "article"
  },
  {
    "title": "Fine-Grained Visual Computing Based on Deep Learning",
    "doi": "https://doi.org/10.1145/3418215",
    "publication_date": "2021-01-31",
    "publication_year": 2021,
    "authors": "Zhihan Lv; Liang Qiao; Amit Kumar Singh; Qingjun Wang",
    "corresponding_authors": "",
    "abstract": "With increasing amounts of information, the image information received by people also increases exponentially. To perform fine-grained categorization and recognition of images and visual calculations, this study combines the Visual Geometry Group Network 16 model of convolutional neural networks and the vision attention mechanism to build a multi-level fine-grained image feature categorization model. Finally, the TensorFlow platform is utilized to simulate the fine-grained image classification model based on the visual attention mechanism. The results show that in terms of accuracy and required training time, the fine-grained image categorization effect of the multi-level feature categorization model constructed by this study is optimal, with an accuracy rate of 85.3% and a minimum training time of 108 s. In the similarity effect analysis, it is found that the chi-square distance between Log Gabor features and the degree of image distortion show a strong positive correlation; in addition, the validity of this measure is verified. Therefore, through the research in this study, it is found that the constructed fine-grained image categorization model has higher accuracy in image recognition categorization, shorter training time, and significantly better performance in similar feature effects, which provides an experimental reference for the visual computing of fine-grained images in the future.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W3157821078",
    "type": "article"
  },
  {
    "title": "Multi-Tier CloudVR",
    "doi": "https://doi.org/10.1145/3429441",
    "publication_date": "2021-05-11",
    "publication_year": 2021,
    "authors": "Abbas Mehrabi; Matti Siekkinen; Teemu Kämäräinen; Antti yl ̈-J ̈ ̈ski",
    "corresponding_authors": "",
    "abstract": "The availability of high bandwidth with low-latency communication in 5G mobile networks enables remote rendered real-time virtual reality (VR) applications. Remote rendering of VR graphics in a cloud removes the need for local personal computer for graphics rendering and augments weak graphics processing unit capacity of stand-alone VR headsets. However, to prevent the added network latency of remote rendering from ruining user experience, rendering a locally navigable viewport that is larger than the field of view of the HMD is necessary. The size of the viewport required depends on latency: Longer latency requires rendering a larger viewport and streaming more content. In this article, we aim to utilize multi-access edge computing to assist the backend cloud in such remote rendered interactive VR. Given the dependency between latency and amount and quality of the content streamed, our objective is to jointly optimize the tradeoff between average video quality and delivery latency. Formulating the problem as mixed integer nonlinear programming, we leverage the interpolation between client’s field of view frame size and overall latency to convert the problem to integer nonlinear programming model and then design efficient online algorithms to solve it. The results of our simulations supplemented by real-world user data reveal that enabling a desired balance between video quality and latency, our algorithm particularly achieves the improvements of on average about 22% and 12% in term of video delivery latency and 8% in term of video quality compared to respectively order-of-arrival, threshold-based, and random-location strategies.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W3161249521",
    "type": "article"
  },
  {
    "title": "Bottom-up and Layerwise Domain Adaptation for Pedestrian Detection in Thermal Images",
    "doi": "https://doi.org/10.1145/3418213",
    "publication_date": "2021-02-28",
    "publication_year": 2021,
    "authors": "My Kieu; Andrew D. Bagdanov; Marco Bertini",
    "corresponding_authors": "",
    "abstract": "Pedestrian detection is a canonical problem for safety and security applications, and it remains a challenging problem due to the highly variable lighting conditions in which pedestrians must be detected. This article investigates several domain adaptation approaches to adapt RGB-trained detectors to the thermal domain. Building on our earlier work on domain adaptation for privacy-preserving pedestrian detection, we conducted an extensive experimental evaluation comparing top-down and bottom-up domain adaptation and also propose two new bottom-up domain adaptation strategies. For top-down domain adaptation, we leverage a detector pre-trained on RGB imagery and efficiently adapt it to perform pedestrian detection in the thermal domain. Our bottom-up domain adaptation approaches include two steps: first, training an adapter segment corresponding to initial layers of the RGB-trained detector adapts to the new input distribution; then, we reconnect the adapter segment to the original RGB-trained detector for final adaptation with a top-down loss. To the best of our knowledge, our bottom-up domain adaptation approaches outperform the best-performing single-modality pedestrian detection results on KAIST and outperform the state of the art on FLIR.",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W3154706820",
    "type": "article"
  },
  {
    "title": "Multitarget Tracking Using Siamese Neural Networks",
    "doi": "https://doi.org/10.1145/3441656",
    "publication_date": "2021-05-18",
    "publication_year": 2021,
    "authors": "Na An; Wei Yan",
    "corresponding_authors": "",
    "abstract": "In this article, we detect and track visual objects by using Siamese network or twin neural network. The Siamese network is constructed to classify moving objects based on the associations of object detection network and object tracking network, which are thought of as the two branches of the twin neural network. The proposed tracking method was designed for single-target tracking, which implements multitarget tracking by using deep neural networks and object detection. The contributions of this article are stated as follows. First, we implement the proposed method for visual object tracking based on multiclass classification using deep neural networks. Then, we attain multitarget tracking by combining the object detection network and the single-target tracking network. Next, we uplift the tracking performance by fusing the outcomes of the object detection network and object tracking network. Finally, we speculate on the object occlusion problem based on IoU and similarity score, which effectively diminish the influence of this issue in multitarget tracking.",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W3160908140",
    "type": "article"
  },
  {
    "title": "An Explainable Deep Learning Ensemble Model for Robust Diagnosis of Diabetic Retinopathy Grading",
    "doi": "https://doi.org/10.1145/3469841",
    "publication_date": "2021-10-26",
    "publication_year": 2021,
    "authors": "Mohammad Shorfuzzaman; M. Shamim Hossain; Abdulmotaleb El Saddik",
    "corresponding_authors": "",
    "abstract": "Diabetic retinopathy (DR) is one of the most common causes of vision loss in people who have diabetes for a prolonged period. Convolutional neural networks (CNNs) have become increasingly popular for computer-aided DR diagnosis using retinal fundus images. While these CNNs are highly reliable, their lack of sufficient explainability prevents them from being widely used in medical practice. In this article, we propose a novel explainable deep learning ensemble model where weights from different models are fused into a single model to extract salient features from various retinal lesions found on fundus images. The extracted features are then fed to a custom classifier for the final diagnosis of DR severity level. The model is trained on an APTOS dataset containing retinal fundus images of various DR grades using a cyclical learning rates strategy with an automatic learning rate finder for decaying the learning rate to improve model accuracy. We develop an explainability approach by leveraging gradient-weighted class activation mapping and shapely adaptive explanations to highlight the areas of fundus images that are most indicative of different DR stages. This allows ophthalmologists to view our model's decision in a way that they can understand. Evaluation results using three different datasets (APTOS, MESSIDOR, IDRiD) show the effectiveness of our model, achieving superior classification rates with a high degree of precision (0.970), sensitivity (0.980), and AUC (0.978). We believe that the proposed model, which jointly offers state-of-the-art diagnosis performance and explainability, will address the black-box nature of deep CNN models in robust detection of DR grading.",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W3208874525",
    "type": "article"
  },
  {
    "title": "Decoupled Low-Light Image Enhancement",
    "doi": "https://doi.org/10.1145/3498341",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Shijie Hao; Xu Han; Yanrong Guo; Meng Wang",
    "corresponding_authors": "",
    "abstract": "The visual quality of photographs taken under imperfect lightness conditions can be degenerated by multiple factors, e.g., low lightness, imaging noise, color distortion, and so on. Current low-light image enhancement models focus on the improvement of low lightness only, or simply deal with all the degeneration factors as a whole, therefore leading to sub-optimal results. In this article, we propose to decouple the enhancement model into two sequential stages. The first stage focuses on improving the scene visibility based on a pixel-wise non-linear mapping. The second stage focuses on improving the appearance fidelity by suppressing the rest degeneration factors. The decoupled model facilitates the enhancement in two aspects. On the one hand, the whole low-light enhancement can be divided into two easier subtasks. The first one only aims to enhance the visibility. It also helps to bridge the large intensity gap between the low-light and normal-light images. In this way, the second subtask can be described as the local appearance adjustment. On the other hand, since the parameter matrix learned from the first stage is aware of the lightness distribution and the scene structure, it can be incorporated into the second stage as the complementary information. In the experiments, our model demonstrates the state-of-the-art performance in both qualitative and quantitative comparisons, compared with other low-light image enhancement models. In addition, the ablation studies also validate the effectiveness of our model in multiple aspects, such as model structure and loss function.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W4214934757",
    "type": "article"
  },
  {
    "title": "Self-supervised Calorie-aware Heterogeneous Graph Networks for Food Recommendation",
    "doi": "https://doi.org/10.1145/3524618",
    "publication_date": "2022-03-17",
    "publication_year": 2022,
    "authors": "Yaguang Song; Xiaoshan Yang; Changsheng Xu",
    "corresponding_authors": "",
    "abstract": "With the rapid development of online recipe sharing platforms, food recommendation is emerging as an important application. Although recent studies have made great progress on food recommendation, they have two shortcomings that are likely to affect the recommendation performance. (1) The relations between ingredients are not considered, which may lead to sub-optimal representations of recipes and further result in the neglect of the user’s personalized ingredient combination preference. (2) Existing methods do not consider the impact of users’ preferences on calories in users’ food decision-making process. In this article, we propose a Self-supervised Calorie-aware Heterogeneous Graph Network (SCHGN) to model the relations between ingredients and incorporate calories of food simultaneously. Specifically, we first incorporate users, recipes, ingredients, and calories into a heterogeneous graph and explicitly present the complex relations among them with directed edges. Then, we explore the co-occurrence relation of ingredients in different recipes via self-supervised ingredient prediction. To capture users’ dynamic preferences on calories of food, we learn calorie-aware user representations by hierarchical message passing and compute a comprehensive user-guided recipe representation by attention mechanism. The final food recommendation is accomplished based on the similarity between a user’s calorie-aware representation and the user-guided representation of a recipe. Extensive experiment results on benchmark datasets demonstrate the effectiveness of the proposed method.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W4221100917",
    "type": "article"
  },
  {
    "title": "Answer Questions with Right Image Regions: A Visual Attention Regularization Approach",
    "doi": "https://doi.org/10.1145/3498340",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Yibing Liu; Yangyang Guo; Jianhua Yin; Xuemeng Song; Weifeng Liu; Liqiang Nie; Min Zhang",
    "corresponding_authors": "",
    "abstract": "Visual attention in Visual Question Answering (VQA) targets at locating the right image regions regarding the answer prediction, offering a powerful technique to promote multi-modal understanding. However, recent studies have pointed out that the highlighted image regions from the visual attention are often irrelevant to the given question and answer, leading to model confusion for correct visual reasoning. To tackle this problem, existing methods mostly resort to aligning the visual attention weights with human attentions. Nevertheless, gathering such human data is laborious and expensive, making it burdensome to adapt well-developed models across datasets. To address this issue, in this article, we devise a novel visual attention regularization approach, namely, AttReg, for better visual grounding in VQA. Specifically, AttReg first identifies the image regions that are essential for question answering yet unexpectedly ignored (i.e., assigned with low attention weights) by the backbone model. And then a mask-guided learning scheme is leveraged to regularize the visual attention to focus more on these ignored key regions. The proposed method is very flexible and model-agnostic, which can be integrated into most visual attention-based VQA models and require no human attention supervision. Extensive experiments over three benchmark datasets, i.e., VQA-CP v2, VQA-CP v1, and VQA v2, have been conducted to evaluate the effectiveness of AttReg. As a by-product, when incorporating AttReg into the strong baseline LMH, our approach can achieve a new state-of-the-art accuracy of 60.00% with an absolute performance gain of 7.01% on the VQA-CP v2 benchmark dataset. In addition to the effectiveness validation, we recognize that the faithfulness of the visual attention in VQA has not been well explored in literature. In the light of this, we propose to empirically validate such property of visual attention and compare it with the prevalent gradient-based approaches.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W3128560592",
    "type": "article"
  },
  {
    "title": "Frequency-aware Camouflaged Object Detection",
    "doi": "https://doi.org/10.1145/3545609",
    "publication_date": "2022-06-30",
    "publication_year": 2022,
    "authors": "Jiaying Lin; Xin Tan; Ke Xu; Lizhuang Ma; Rynson W. H. Lau",
    "corresponding_authors": "",
    "abstract": "Camouflaged object detection (COD) is important as it has various potential applications. Unlike salient object detection (SOD), which tries to identify visually salient objects, COD tries to detect objects that are visually very similar to the surrounding background. We observe that recent COD methods try to fuse features from different levels using some context aggregation strategies originally developed for SOD. Such an approach, however, may not be appropriate for COD as these existing context aggregation strategies are good at detecting distinctive objects while weakening the features from less discriminative objects. To address this problem, we propose in this article to exploit frequency learning to suppress the confusing high-frequency texture information, to help separate camouflaged objects from their surrounding background, and a frequency-based method, called FBNet, for camouflaged object detection. Specifically, we design a frequency-aware context aggregation (FACA) module to suppress high-frequency information and aggregate multi-scale features from a frequency perspective, an adaptive frequency attention (AFA) module to enhance the features of the learned important frequency components, and a gradient-weighted loss function to guide the proposed method to pay more attention to contour details. Experimental results show that our model outperforms relevant state-of-the-art methods.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W4283724275",
    "type": "article"
  },
  {
    "title": "EiMOL: A Secure Medical Image Encryption Algorithm based on Optimization and the Lorenz System",
    "doi": "https://doi.org/10.1145/3561513",
    "publication_date": "2022-09-09",
    "publication_year": 2022,
    "authors": "KN Singh; OP Singh; Amit Kumar Singh; Amrit Kumar Agrawal",
    "corresponding_authors": "",
    "abstract": "Nowadays, the demand for digital images from different intelligent devices and sensors has dramatically increased in smart healthcare. Due to advanced low-cost and easily available tools and software, manipulation of these images is an easy task. Thus, the security of digital images is a serious challenge for the content owners, healthcare communities, and researchers against illegal access and fraudulent usage. In this article, a secure medical image encryption algorithm, EiMOL , based on optimization and the Lorenz system, is proposed for smart healthcare applications. In the first stage, an optimized random sequence (ORS) is generated through directed weighted complex network particle swarm optimization using the genetic algorithm (GDWCN-PSO). This random number matrix and the Lorenz system are adopted to encrypt plain medical images, obtaining the cipher messages with a relationship to the plain images. According to our obtained results, the proposed EiMOL encryption algorithm is effective and resistant to the many attacks on benchmark Kaggle and Open-i datasets. Further, extensive experimental results demonstrate that the proposed algorithm outperforms the state-of-the-art approaches.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W4295135090",
    "type": "article"
  },
  {
    "title": "Deep Unsupervised Key Frame Extraction for Efficient Video Classification",
    "doi": "https://doi.org/10.1145/3571735",
    "publication_date": "2022-12-12",
    "publication_year": 2022,
    "authors": "Hao Tang; Lei Ding; Songsong Wu; Bin Ren; Nicu Sebe; Paolo Rota",
    "corresponding_authors": "",
    "abstract": "Video processing and analysis have become an urgent task, as a huge amount of videos (e.g., YouTube, Hulu) are uploaded online every day. The extraction of representative key frames from videos is important in video processing and analysis since it greatly reduces computing resources and time. Although great progress has been made recently, large-scale video classification remains an open problem, as the existing methods have not well balanced the performance and efficiency simultaneously. To tackle this problem, this work presents an unsupervised method to retrieve the key frames, which combines the convolutional neural network and temporal segment density peaks clustering. The proposed temporal segment density peaks clustering is a generic and powerful framework, and it has two advantages compared with previous works. One is that it can calculate the number of key frames automatically. The other is that it can preserve the temporal information of the video. Thus, it improves the efficiency of video classification. Furthermore, a long short-term memory network is added on the top of the convolutional neural network to further elevate the performance of classification. Moreover, a weight fusion strategy of different input networks is presented to boost performance. By optimizing both video classification and key frame extraction simultaneously, we achieve better classification performance and higher efficiency. We evaluate our method on two popular datasets (i.e., HMDB51 and UCF101), and the experimental results consistently demonstrate that our strategy achieves competitive performance and efficiency compared with the state-of-the-art approaches.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W4311412445",
    "type": "article"
  },
  {
    "title": "Skeleton Sequence and RGB Frame Based Multi-Modality Feature Fusion Network for Action Recognition",
    "doi": "https://doi.org/10.1145/3491228",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Xiaoguang Zhu; Ye Zhu; Haoyu Wang; Honglin Wen; Yan Yan; Peilin Liu",
    "corresponding_authors": "",
    "abstract": "Action recognition has been a heated topic in computer vision for its wide application in vision systems. Previous approaches achieve improvement by fusing the modalities of the skeleton sequence and RGB video. However, such methods pose a dilemma between the accuracy and efficiency for the high complexity of the RGB video network. To solve the problem, we propose a multi-modality feature fusion network to combine the modalities of the skeleton sequence and RGB frame instead of the RGB video, as the key information contained by the combination of the skeleton sequence and RGB frame is close to that of the skeleton sequence and RGB video. In this way, complementary information is retained while the complexity is reduced by a large margin. To better explore the correspondence of the two modalities, a two-stage fusion framework is introduced in the network. In the early fusion stage, we introduce a skeleton attention module that projects the skeleton sequence on the single RGB frame to help the RGB frame focus on the limb movement regions. In the late fusion stage, we propose a cross-attention module to fuse the skeleton feature and the RGB feature by exploiting the correlation. Experiments on two benchmarks, NTU RGB+D and SYSU, show that the proposed model achieves competitive performance compared with the state-of-the-art methods while reducing the complexity of the network.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W4214858596",
    "type": "article"
  },
  {
    "title": "Shuffle-invariant Network for Action Recognition in Videos",
    "doi": "https://doi.org/10.1145/3485665",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Qinghongya Shi; Hongbo Zhang; Zhe Li; Ji‐Xiang Du; Qing Lei; Jinghua Liu",
    "corresponding_authors": "",
    "abstract": "The local key features in video are important for improving the accuracy of human action recognition. However, most end-to-end methods focus on global feature learning from videos, while few works consider the enhancement of the local information in a feature. In this article, we discuss how to automatically enhance the ability to discriminate the local information in an action feature and improve the accuracy of action recognition. To address these problems, we assume that the critical level of each region for the action recognition task is different and will not change with the region location shuffle. We therefore propose a novel action recognition method called the shuffle-invariant network. In the proposed method, the shuffled video is generated by regular region cutting and random confusion to enhance the input data. The proposed network adopts the multitask framework, which includes one feature backbone network and three task branches: local critical feature shuffle-invariant learning, adversarial learning, and an action classification network. To enhance the local features, the feature response of each region is predicted by a local critical feature learning network. To train this network, an L 1-based critical feature shuffle-invariant loss is defined to ensure that the ordered feature response list of these regions remains unchanged after region location shuffle. Then, the adversarial learning is applied to eliminate the noise caused by the region shuffle. Finally, the action classification network combines these two tasks to jointly guide the training of the feature backbone network and obtain more effective action features. In the testing phase, only the action classification network is applied to identify the action category of the input video. We verify the proposed method on the HMDB51 and UCF101 action datasets. Several ablation experiments are constructed to verify the effectiveness of each module. The experimental results show that our approach achieves the state-of-the-art performance.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W4214923132",
    "type": "article"
  },
  {
    "title": "SADnet: Semi-supervised Single Image Dehazing Method Based on an Attention Mechanism",
    "doi": "https://doi.org/10.1145/3478457",
    "publication_date": "2022-02-16",
    "publication_year": 2022,
    "authors": "Ziyi Sun; Yunfeng Zhang; Fangxun Bao; Ping Wang; Xunxiang Yao; Caiming Zhang",
    "corresponding_authors": "",
    "abstract": "Many real-life tasks such as military reconnaissance and traffic monitoring require high-quality images. However, images acquired in foggy or hazy weather pose obstacles to the implementation of these real-life tasks; consequently, image dehazing is an important research problem. To meet the requirements of practical applications, a single image dehazing algorithm has to be able to effectively process real-world hazy images with high computational efficiency. In this article, we present a fast and robust semi-supervised dehazing algorithm named SADnet for practical applications. SADnet utilizes both synthetic datasets and natural hazy images for training, so it has good generalizability for real-world hazy images. Furthermore, considering the uneven distribution of haze in the atmospheric environment, a Channel-Spatial Self-Attention (CSSA) mechanism is presented to enhance the representational power of the proposed SADnet. Extensive experimental results demonstrate that the presented approach achieves good dehazing performances and competitive running times compared with other state-of-the-art image dehazing algorithms.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W4283369524",
    "type": "article"
  },
  {
    "title": "On Modality Bias Recognition and Reduction",
    "doi": "https://doi.org/10.1145/3565266",
    "publication_date": "2022-09-29",
    "publication_year": 2022,
    "authors": "Yangyang Guo; Liqiang Nie; Harry H. Cheng; Zhiyong Cheng; Mohan Kankanhalli; Alberto Del Bimbo",
    "corresponding_authors": "",
    "abstract": "Making each modality in multi-modal data contribute is of vital importance to learning a versatile multi-modal model. Existing methods, however, are often dominated by one or few of modalities during model training, resulting in sub-optimal performance. In this article, we refer to this problem as modality bias and attempt to study it in the context of multi-modal classification systematically and comprehensively. After stepping into several empirical analyses, we recognize that one modality affects the model prediction more just because this modality has a spurious correlation with instance labels. To primarily facilitate the evaluation on the modality bias problem, we construct two datasets, respectively, for the colored digit recognition and video action recognition tasks in line with the Out-of-Distribution (OoD) protocol. Collaborating with the benchmarks in the visual question answering task, we empirically justify the performance degradation of the existing methods on these OoD datasets, which serves as evidence to justify the modality bias learning. In addition, to overcome this problem, we propose a plug-and-play loss function method, whereby the feature space for each label is adaptively learned according to the training set statistics. Thereafter, we apply this method on 10 baselines in total to test its effectiveness. From the results on four datasets regarding the above three tasks, our method yields remarkable performance improvements compared with the baselines, demonstrating its superiority on reducing the modality bias problem.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W4297999738",
    "type": "article"
  },
  {
    "title": "Graph Attention Transformer Network for Multi-label Image Classification",
    "doi": "https://doi.org/10.1145/3578518",
    "publication_date": "2022-12-29",
    "publication_year": 2022,
    "authors": "Jin Yuan; Shikai Chen; Yao Zhang; Zhongchao Shi; Xin Geng; Jianping Fan; Yong Rui",
    "corresponding_authors": "",
    "abstract": "Multi-label classification aims to recognize multiple objects or attributes from images. The key to solving this issue relies on effectively characterizing the inter-label correlations or dependencies, which bring the prevailing graph neural network. However, current methods often use the co-occurrence probability of labels based on the training set as the adjacency matrix to model this correlation, which is greatly limited by the dataset and affects the model’s generalization ability. This article proposes a Graph Attention Transformer Network, a general framework for multi-label image classification by mining rich and effective label correlation. First, we use the cosine similarity value of the pre-trained label word embedding as the initial correlation matrix, which can represent richer semantic information than the co-occurrence one. Subsequently, we propose the graph attention transformer layer to transfer this adjacency matrix to adapt to the current domain. Our extensive experiments have demonstrated that our proposed methods can achieve highly competitive performance on three datasets.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W4313331808",
    "type": "article"
  },
  {
    "title": "Deep Q Network–Driven Task Offloading for Efficient Multimedia Data Analysis in Edge Computing–Assisted IoV",
    "doi": "https://doi.org/10.1145/3548687",
    "publication_date": "2022-06-30",
    "publication_year": 2022,
    "authors": "Chenyi Yang; Xiaolong Xu; Xiaokang Zhou; Lianyong Qi",
    "corresponding_authors": "",
    "abstract": "With the prosperity of Industry 4.0, numerous emerging industries continue to gain popularity and their market scales are expanding ceaselessly. The Internet of Vehicles (IoV), one of the thriving intelligent industries, enjoys bright development prospects. However, at the same time, the reliability and availability of IoV applications are confronted with two major bottlenecks of time delay and energy consumption. To make matters worse, massive heterogeneous and multi-dimensional multimedia data generated on the IoV present a huge obstacle to effective data analysis. Fortunately, the advent of edge computing technology enables tasks to be offloaded to edge servers, which significantly reduces total overhead of IoV systems. Deep reinforcement learning (DRL), equipped with its excellent perception and decision-making capability, is undoubtedly a dominant technology to solve task offloading problems. In this article, we first employ an optimized Fuzzy C-means algorithm to cluster vehicles and other edge devices according to their respective service quality requirements. Then, we employ an election algorithm to assist in maintaining the stability of the IoV. Last, we propose a task-offloading algorithm based on the Deep Q Network (DQN) to acquire an optimal task offloading scheme. Massive simulation experiments demonstrate the superiority of our method in minimizing time delay and energy consumption.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W4286490622",
    "type": "article"
  },
  {
    "title": "A Study of Human–AI Symbiosis for Creative Work: Recent Developments and Future Directions in Deep Learning",
    "doi": "https://doi.org/10.1145/3542698",
    "publication_date": "2022-07-28",
    "publication_year": 2022,
    "authors": "Bahar Uddin Mahmud; G.Y. Hong; Bernard Fong",
    "corresponding_authors": "",
    "abstract": "Recent advances in Artificial Intelligence (AI), particularly deep learning, are having an enormous impact on our society today. Record numbers of jobs previously held by people have been automated, from manufacturing to transportation to customer services. The concerns of AI replacing humans by taking over people's jobs need to be urgently addressed. This article investigates some promising different directions of AI development: Instead of using AI to replace people, we should use AI to team up with people so that both can work better and smarter. Human–AI symbiosis refers to people and AI working together to jointly solve problems and perform specific tasks. The recent developments in deep learning models and frameworks have significantly improved the efficiency and performance of human and AI collaborations. In this article, some research work on human–AI collaborative environments has been extensively studied and analyzed to reveal the progress in this field. Although the teaming of humans and machines includes many complex tasks, the development has been very promising. One of the main goals in this field is to develop additional capabilities in machines capable of being successful teammates with a human partner. The correctness of the outcomes is often determined by the underlying technology and how performance and human satisfaction are measured through the collaborative nature of the system. We conclude that the teaming of humans and AI, particularly deep learning, has the advantage of combining the power of AI with the human domain expertise to improve performance and create value. Human–AI symbiosis could be a promising future direction for AI's continuing integration into the world.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W4288084733",
    "type": "article"
  },
  {
    "title": "Toward Visual Behavior and Attention Understanding for Augmented 360 Degree Videos",
    "doi": "https://doi.org/10.1145/3565024",
    "publication_date": "2022-09-29",
    "publication_year": 2022,
    "authors": "Yucheng Zhu; Xiongkuo Min; Dandan Zhu; Guangtao Zhai; Xiaokang Yang; Wenjun Zhang; Ke Gu; Jiantao Zhou",
    "corresponding_authors": "",
    "abstract": "Augmented reality (AR) overlays digital content onto reality. In an AR system, correct and precise estimations of user visual fixations and head movements can enhance the quality of experience by allocating more computational resources for analyzing, rendering, and 3D registration on the areas of interest. However, there is inadequate research to help in understanding the visual explorations of the users when using an AR system or modeling AR visual attention. To bridge the gap between the saliency prediction on real-world scenes and on scenes augmented by virtual information, we construct the ARVR saliency dataset. The virtual reality (VR) technique is employed to simulate the real-world. Annotations of object recognition and tracking as augmented contents are blended into omnidirectional videos. The saliency annotations of head and eye movements for both original and augmented videos are collected and together constitute the ARVR dataset. We also design a model that is capable of solving the saliency prediction problem in AR. Local block images are extracted to simulate the viewport and offset the projection distortion. Conspicuous visual cues in the local block images are extracted to constitute the spatial features. The optical flow information is estimated as an important temporal feature. We also consider the interplay between virtual information and reality. The composition of the augmentation information is distinguished, and the joint effects of adversarial augmentation and complementary augmentation are estimated. The Markov chain is constructed with block images as graph nodes. In the determination of the edge weights, both the characteristics of the viewing behaviors and the visual saliency mechanisms are considered. The order of importance for block images is estimated through the state of equilibrium of the Markov chain. Extensive experiments are conducted to demonstrate the effectiveness of the proposed method.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W4297999749",
    "type": "article"
  },
  {
    "title": "Generation of Realistic Synthetic Financial Time-series",
    "doi": "https://doi.org/10.1145/3501305",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Mihai Dogariu; Liviu–Daniel Stefan; Bogdan Boteanu; Claudiu Lamba; Bomi Kim; Bogdan Ionescu",
    "corresponding_authors": "",
    "abstract": "Financial markets have always been a point of interest for automated systems. Due to their complex nature, financial algorithms and fintech frameworks require vast amounts of data to accurately respond to market fluctuations. This data availability is tied to the daily market evolution, so it is impossible to accelerate its acquisition. In this article, we discuss several solutions for augmenting financial datasets via synthesizing realistic time-series with the help of generative models. This problem is complex, since financial time series present very specific properties, e.g., fat-tail distribution, cross-correlation between different stocks, specific autocorrelation, cluster volatility and so on. In particular, we propose solutions for capturing cross-correlations between different stocks and for transitioning from fixed to variable length time-series without resorting to sequence modeling networks, and adapt various network architectures, e.g., fully connected and convolutional GANs, variational autoencoders, and generative moment matching networks. Finally, we tackle the problem of evaluating the quality of synthetic financial time-series. We introduce qualitative and quantitative metrics, along with a portfolio trend prediction framework that validates our generative models’ performance. We carry out experiments on real-world financial data extracted from the US stock market, proving the benefits of these techniques.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W4214813689",
    "type": "article"
  },
  {
    "title": "Causal Inference with Knowledge Distilling and Curriculum Learning for Unbiased VQA",
    "doi": "https://doi.org/10.1145/3487042",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Yonghua Pan; Zechao Li; Liyan Zhang; Jinhui Tang",
    "corresponding_authors": "",
    "abstract": "Recently, many Visual Question Answering (VQA) models rely on the correlations between questions and answers yet neglect those between the visual information and the textual information. They would perform badly if the handled data distribute differently from the training data (i.e., out-of-distribution (OOD) data). Towards this end, we propose a two-stage unbiased VQA approach that addresses the unbiased issue from a causal perspective. In the causal inference stage, we mark the spurious correlation on the causal graph, explore the counterfactual causality, and devise a causal target based on the inherent correlations between the conventional and counterfactual VQA models. In the distillation stage, we introduce the causal target into the training process and leverages distilling as well as curriculum learning to capture the unbiased model. Since Causal Inference with Knowledge Distilling and Curriculum Learning (CKCL) reinforces the contribution of the visual information and eliminates the impact of the spurious correlation by distilling the knowledge in causal inference to the VQA model, it contributes to the good performance on both the standard data and out-of-distribution data. The extensive experimental results on VQA-CP v2 dataset demonstrate the superior performance of the proposed method compared to the state-of-the-art (SotA) methods.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W4214825151",
    "type": "article"
  },
  {
    "title": "Generative Metric Learning for Adversarially Robust Open-world Person Re-Identification",
    "doi": "https://doi.org/10.1145/3522714",
    "publication_date": "2022-03-12",
    "publication_year": 2022,
    "authors": "Deyin Liu; Lin Wu; Richang Hong; Zongyuan Ge; Jialie Shen; Farid Boussaïd; Mohammed Bennamoun",
    "corresponding_authors": "",
    "abstract": "The vulnerability of re-identification (re-ID) models under adversarial attacks is of significant concern as criminals may use adversarial perturbations to evade surveillance systems. Unlike a closed-world re-ID setting (i.e., a fixed number of training categories), a reliable re-ID system in the open world raises the concern of training a robust yet discriminative classifier, which still shows robustness in the context of unknown examples of an identity. In this work, we improve the robustness of open-world re-ID models by proposing a generative metric learning approach to generate adversarial examples that are regularized to produce robust distance metric. The proposed approach leverages the expressive capability of generative adversarial networks to defend the re-ID models against feature disturbance attacks. By generating the target people variants and sampling the triplet units for metric learning, our learned distance metrics are regulated to produce accurate predictions in the feature metric space. Experimental results on the three re-ID datasets, i.e., Market-1501, DukeMTMC-reID, and MSMT17 demonstrate the robustness of our method.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W4220654809",
    "type": "article"
  },
  {
    "title": "Egocentric Early Action Prediction via Adversarial Knowledge Distillation",
    "doi": "https://doi.org/10.1145/3544493",
    "publication_date": "2022-06-16",
    "publication_year": 2022,
    "authors": "Na Zheng; Xuemeng Song; Tianyu Su; Weifeng Liu; Yan Yan; Liqiang Nie",
    "corresponding_authors": "",
    "abstract": "Egocentric early action prediction aims to recognize actions from the first-person view by only observing a partial video segment, which is challenging due to the limited context information of the partial video. In this article, to tackle the egocentric early action prediction problem, we propose a novel multi-modal adversarial knowledge distillation framework. In particular, our approach involves a teacher network to learn the enhanced representation of the partial video by considering the future unobserved video segment, and a student network to mimic the teacher network to produce the powerful representation of the partial video and based on that predicting the action label. To promote the knowledge distillation between the teacher and the student network, we seamlessly integrate adversarial learning with latent and discriminative knowledge regularizations encouraging the learned representations of the partial video to be more informative and discriminative toward the action prediction. Finally, we devise a multi-modal fusion module toward comprehensively predicting the action label. Extensive experiments on two public egocentric datasets validate the superiority of our method over the state-of-the-art methods. We have released the codes and involved parameters to benefit other researchers. 1",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W4283026051",
    "type": "article"
  },
  {
    "title": "A Survey on Temporal Sentence Grounding in Videos",
    "doi": "https://doi.org/10.1145/3532626",
    "publication_date": "2022-05-20",
    "publication_year": 2022,
    "authors": "Xiaohan Lan; Yitian Yuan; Xin Wang; Zhi Wang; Wenwu Zhu",
    "corresponding_authors": "",
    "abstract": "Temporal sentence grounding in videos (TSGV), which aims at localizing one target segment from an untrimmed video with respect to a given sentence query, has drawn increasing attentions in the research community over the past few years. Different from the task of temporal action localization, TSGV is more flexible since it can locate complicated activities via natural languages, without restrictions from predefined action categories. Meanwhile, TSGV is more challenging since it requires both textual and visual understanding for semantic alignment between two modalities (i.e., text and video). In this survey, we give a comprehensive overview for TSGV, which (i) summarizes the taxonomy of existing methods, (ii) provides a detailed description of the evaluation protocols (i.e., datasets and metrics) to be used in TSGV, and (iii) in-depth discusses potential problems of current benchmarking designs and research directions for further investigations. To the best of our knowledge, this is the first systematic survey on temporal sentence grounding. More specifically, we first discuss existing TSGV approaches by grouping them into four categories, i.e., two-stage methods, single-stage methods, reinforcement learning-based methods, and weakly supervised methods. Then we present the benchmark datasets and evaluation metrics to assess current research progress. Finally, we discuss some limitations in TSGV through pointing out potential problems improperly resolved in the current evaluation protocols, which may push forwards more cutting-edge research in TSGV. Besides, we also share our insights on several promising directions, including four typical tasks with new and practical settings based on TSGV.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W4298364821",
    "type": "article"
  },
  {
    "title": "Toward Intelligent Fashion Design: A Texture and Shape Disentangled Generative Adversarial Network",
    "doi": "https://doi.org/10.1145/3567596",
    "publication_date": "2022-10-13",
    "publication_year": 2022,
    "authors": "Han Yan; Haijun Zhang; Jianyang Shi; Jianghong Ma; Xiaofei Xu",
    "corresponding_authors": "",
    "abstract": "Texture and shape in fashion, constituting essential elements of garments, characterize the body and surface of the fabric and outline the silhouette of clothing, respectively. The selection of texture and shape plays a critical role in the design process, as they largely determine the success of a new design for fashion items. In this research, we propose a texture and shape disentangled generative adversarial network (TSD-GAN) to perform “intelligent” design with the transformation of texture and shape in fashion items. Our TSD-GAN aims to learn how to disentangle the features of texture and shape of different fashion items in an unsupervised manner. Specifically, a fashion attribute encoder is developed to decompose the input fashion items into independent representations of texture and shape. Then, to learn the coarse or fine styles hidden in the features of texture and shape, a texture mapping network and a shape mapping network are proposed to disentangle the features into different hierarchical representations. The different hierarchical representations of texture and shape are then fed into a multi-factor-based generator to generate mixed-style fashion items. In addition, a multi-discriminator framework is developed to distinguish the authenticity and texture similarity between the generated images and the real images. Experimental results on different fashion categories demonstrate that our proposed TSD-GAN may be useful for assisting designers to accomplish the design process by transforming the texture and shape of fashion items.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W4304892053",
    "type": "article"
  },
  {
    "title": "Bidirectional Transformer GAN for Long-term Human Motion Prediction",
    "doi": "https://doi.org/10.1145/3579359",
    "publication_date": "2023-01-10",
    "publication_year": 2023,
    "authors": "Mengyi Zhao; Hao Tang; Pan Xie; Shuling Dai; Nicu Sebe; Wei Wang",
    "corresponding_authors": "",
    "abstract": "The mainstream motion prediction methods usually focus on short-term prediction, and their predicted long-term motions often fall into an average pose, i.e., the freezing forecasting problem [ 27 ]. To mitigate this problem, we propose a novel Bidirectional Transformer-based Generative Adversarial Network (BiTGAN) for long-term human motion prediction. The bidirectional setup leads to consistent and smooth generation in both forward and backward directions. Besides, to make full use of the history motions, we split them into two parts. The first part is fed to the Transformer encoder in our BiTGAN while the second part is used as the decoder input. This strategy can alleviate the exposure problem [ 37 ]. Additionally, to better maintain both the local (i.e., frame-level pose) and global (i.e., video-level semantic) similarities between the predicted motion sequence and the real one, the soft dynamic time warping (Soft-DTW) loss is introduced into the generator. Finally, we utilize a dual-discriminator to distinguish the predicted sequence at both frame and sequence levels. Extensive experiments on the public Human3.6M dataset demonstrate that our proposed BiTGAN achieves state-of-the-art performance on long-term (4 s ) human motion prediction, and reduces the average error of all actions by 4%.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W4315491145",
    "type": "article"
  },
  {
    "title": "Multimodal Presentation of Interactive Audio-Tactile Graphics Supporting the Perception of Visual Information by Blind People",
    "doi": "https://doi.org/10.1145/3586076",
    "publication_date": "2023-03-02",
    "publication_year": 2023,
    "authors": "Michał Maćkowski; Piotr Brzoza; Mateusz Kawulok; Rafał Meisel; Dominik Spińczyk",
    "corresponding_authors": "",
    "abstract": "Due to the limitations in the perception of graphical information by blind people and the need to substitute the sense of sight with other senses, the correct use of multimedia in the presentation of graphics is particularly important. The aim of the authors was to correctly present visual information in the tactile and audio form and to provide contextually selected information to reduce the existing cognitive barriers. In this article, the authors decided to research the method of exploring a tactile picture by a blind person and providing contextual and semantic information about the touched image elements using the developed tool for multimodal presentation of interactive audio-tactile graphics supporting the perception of blind people. The use of multimedia should improve the perception of the conveyed content. Therefore, the effectiveness of interpreting the information contained in the tactile image is verified by analyzing the tactile, audio, and contextual perceptions during the experiments. The following issues were considered in detail from the point of view of the blind and visually impaired: the recognition of shapes of image elements depending on their size and properties, the optimal width of elements displayed on the tablet screen, the time intervals between taps on an image element, and the acceptable length of graphic element audio description. The results from this study suggest the need to select the image presentation parameters in various perception channels to the user's needs. The findings of our research on tactile shape perception of geometric figures indicate potential problems in recognizing the shapes of figures in the case of wrong preparation of tactile pictures. In the case of a small difference in the proportions of the length of the sides of the figure or a slight difference in the measures of the angles of the figure sides, we have noticed that most blind people fail to recognize its shape. Considerable progress in improving such perception can be achieved by increasing the proportions in the lengths of the sides and the measures of the angles between the sides of the figure. Further steps concern introducing an alternative audio description of the properties of the figure so as to improve the interpretability of the figure shape. For most users, the width of a virtual line in a digital image displayed on a tablet should be around 5 mm for a corresponding 1 mm tactile line. The configuration of the time intervals defining the user's gestures (two-taps, three-taps) should be about 340 ms. Applying audio descriptions to tactile picture elements improves the understanding and interpretation of the information presented on it. Most participants in the test group accepted 5 to 10 seconds of voice prompts. Longer messages were incomprehensible, and details were hard to remember. In the proposed solution, the audio description can be divided into two or three different descriptions available to the user after tapping two or three times on an image element. It allows the user to decide on the amount of contextual information needed about the touched tactile picture element. The research attempted to show the typical values of various image presentation parameters and their ranges of values and indicated effective methods of their selection for a specific system user.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W4322768551",
    "type": "article"
  },
  {
    "title": "MIS: A Multi-Identifier Management and Resolution System in the Metaverse",
    "doi": "https://doi.org/10.1145/3597641",
    "publication_date": "2023-05-26",
    "publication_year": 2023,
    "authors": "Han Wang; Hui Li; Abla Smahi; Feng Zhao; Yao Yao; Ching Chuen Chan; Shiyu Wang; Wenyuan Yang; Shuo‐Yen Robert Li",
    "corresponding_authors": "",
    "abstract": "The metaverse gradually evolves into a virtual world containing a series of interconnected sub-metaverses. Diverse digital resources, including identities, contents, services, and supporting data, are key components of the sub-metaverse. Therefore, a Domain Name System (DNS)-like system is necessary for efficient management and resolution. However, the legacy DNS was designed with security vulnerabilities and trust risks due to centralized issues. Blockchain is used to mitigate these concerns due to its decentralized features. Additionally, it supports identity management as a default feature, making it a natural fit for the metaverse. While there are several DNS alternatives based on the blockchain, they either manage only a single type of identifiers or isolate identities from other sorts of identifiers, making it difficult for sub-metaverses to coexist and connect with each other. This article proposes a M ulti- I dentifier management and resolution S ystem (MIS) in the metaverse, supporting the registration, resolution, and inter-translation functions. The basic MIS is portrayed as a four-tier architecture on a consortium blockchain due to its manageability, enhanced security, and efficiency properties. On-chain data is lightweight and compressed to save on storage while accelerating reading and writing operations. The resource data is encrypted based on the attributes of the sub-metaverse in the storage tier for privacy protection and access control. For users with decentralization priorities, a modification named EMIS is built on top of Ethereum. Finally, MIS is implemented on two testbeds and is available online as the open-source system. The first testbed consists of 4 physical servers located in the UK and Malaysia while the second is made up of 200 virtual machines (VMs) spread over 26 countries across all 5 continents on Google Cloud. Experiments indicate that MIS provides efficient reading and writing performance than the legacy DNS and other public blockchain-based workarounds including EMIS and Ethereum Name Service (ENS).",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W4378376502",
    "type": "article"
  },
  {
    "title": "Head Pose Estimation Patterns as Deepfake Detectors",
    "doi": "https://doi.org/10.1145/3612928",
    "publication_date": "2023-08-03",
    "publication_year": 2023,
    "authors": "Federico Becattini; Carmen Bisogni; Vincenzo Loia; Chiara Pero; Fei Hao",
    "corresponding_authors": "",
    "abstract": "The capacity to create “fake” videos has recently raised concerns about the reliability of multimedia content. Identifying between true and false information is a critical step toward resolving this problem. On this issue, several algorithms utilizing deep learning and facial landmarks have yielded intriguing results. Facial landmarks are traits that are solely tied to the subject’s head posture. Based on this observation, we study how Head Pose Estimation (HPE) patterns may be utilized to detect deepfakes in this work. The HPE patterns studied are based on FSA-Net, SynergyNet, and WSM, which are among the most performant approaches on the state-of-the-art. Finally, using a machine learning technique based on K-Nearest Neighbor and Dynamic Time Warping, their temporal patterns are categorized as authentic or false. We also offer a set of experiments for examining the feasibility of using deep learning techniques on such patterns. The findings reveal that the ability to recognize a deepfake video utilizing an HPE pattern is dependent on the HPE methodology. On the contrary, performance is less dependent on the performance of the utilized HPE technique. Experiments are carried out on the FaceForensics++ dataset that presents both identity swap and expression swap examples. The findings show that FSA-Net is an effective feature extraction method for determining whether a pattern belongs to a deepfake or not. The approach is also robust in comparison to deepfake videos created using various methods or for different goals. In the mean the method obtain 86% of accuracy on the identity swap task and 86.5% of accuracy on the expression swap. These findings offer up various possibilities and future directions for solving the deepfake detection problem using specialized HPE approaches, which are also known to be fast and reliable.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W4385501912",
    "type": "article"
  },
  {
    "title": "Composed Image Retrieval using Contrastive Learning and Task-oriented CLIP-based Features",
    "doi": "https://doi.org/10.1145/3617597",
    "publication_date": "2023-08-30",
    "publication_year": 2023,
    "authors": "Alberto Baldrati; Marco Bertini; Tiberio Uricchio; Alberto Del Bimbo",
    "corresponding_authors": "",
    "abstract": "Given a query composed of a reference image and a relative caption, the Composed Image Retrieval goal is to retrieve images visually similar to the reference one that integrates the modifications expressed by the caption. Given that recent research has demonstrated the efficacy of large-scale vision and language pre-trained (VLP) models in various tasks, we rely on features from the OpenAI CLIP model to tackle the considered task. We initially perform a task-oriented fine-tuning of both CLIP encoders using the element-wise sum of visual and textual features. Then, in the second stage, we train a Combiner network that learns to combine the image-text features integrating the bimodal information and providing combined features used to perform the retrieval. We use contrastive learning in both stages of training. Starting from the bare CLIP features as a baseline, experimental results show that the task-oriented fine-tuning and the carefully crafted Combiner network are highly effective and outperform more complex state-of-the-art approaches on FashionIQ and CIRR, two popular and challenging datasets for composed image retrieval. Code and pre-trained models are available at https://github.com/ABaldrati/CLIP4Cir .",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W4386290297",
    "type": "article"
  },
  {
    "title": "Incomplete Multiview Clustering via Semidiscrete Optimal Transport for Multimedia Data Mining in IoT",
    "doi": "https://doi.org/10.1145/3625548",
    "publication_date": "2023-09-26",
    "publication_year": 2023,
    "authors": "Jing Gao; Peng Li; Asif Ali Laghari; Gautam Srivastava; Thippa Reddy Gadekallu; Sidra Abbas; Jianing Zhang",
    "corresponding_authors": "",
    "abstract": "With the wide deployment of the Internet of Things (IoT), large volumes of incomplete multiview data that violates data integrity is generated by various applications, which inevitably produces negative impacts on the quality of service of IoT systems. Incomplete multiview clustering (IMC), as an essential technique of data processing, has the potential for mining patterns of incomplete IoT data. However, previous methods utilize notion-strong distances that can only measure differences between distributions at the overlap of data manifolds in fusing complementary information of data for pattern mining. They may suffer from biased estimation and information loss in capturing intrinsic structures of incomplete multiview data. To address these challenges, a semidiscrete multiview optimal transport (SD-MOT) is defined for IMC, which utilizes distances with weak notions to capture intrinsic structures of incomplete multiview data. Specifically, IMC is recast as an equivalent optimal transport between continuous incomplete multiview data and discrete clustering centroids, to avoid the strict assumption on overlap between manifolds in pattern mining. Then, SD-MOT is instantiated as a deep incomplete contrastive clustering network to remedy biased estimation and information loss on intrinsic structures of incomplete multiview data. Afterwards, a variational solution to SD-MOT is derived to effectively train the network parameters for pattern mining. Finally, extensive experiments on four representative incomplete multiview datasets verify the superiority of SD-MOT in comparison with nine baseline methods.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W4387055596",
    "type": "article"
  },
  {
    "title": "Low-light Image Enhancement via a Frequency-based Model with Structure and Texture Decomposition",
    "doi": "https://doi.org/10.1145/3590965",
    "publication_date": "2023-04-04",
    "publication_year": 2023,
    "authors": "Mingliang Zhou; Hongyue Leng; Bin Fang; Tao Xiang; Xuekai Wei; Weijia Jia",
    "corresponding_authors": "",
    "abstract": "This article proposes a frequency-based structure and texture decomposition model in a Retinex-based framework for low-light image enhancement and noise suppression. First, we utilize the total variation-based noise estimation to decompose the observed image into low-frequency and high-frequency components. Second, we use a Gaussian kernel for noise suppression in the high-frequency layer. Third, we propose a frequency-based structure and texture decomposition method to achieve low-light enhancement. We extract texture and structure priors by using the high-frequency layer and a low-frequency layer, respectively. We present an optimization problem and solve it with the augmented Lagrange multiplier to generate a balance between structure and texture in the reflectance map. Our experimental results reveal that the proposed method can achieve superior performance in naturalness preservation and detail retention compared with state-of-the-art algorithms for low-light image enhancement. Our code is available on the following website. 1",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W4362554287",
    "type": "article"
  },
  {
    "title": "Unifying Dual-Attention and Siamese Transformer Network for Full-Reference Image Quality Assessment",
    "doi": "https://doi.org/10.1145/3597434",
    "publication_date": "2023-05-18",
    "publication_year": 2023,
    "authors": "Zhenjun Tang; Zhiyuan Chen; Zhixin Li; Bineng Zhong; Xianquan Zhang; Xinpeng Zhang",
    "corresponding_authors": "",
    "abstract": "Image Quality Assessment (IQA) is a critical task of computer vision. Most Full-Reference (FR) IQA methods have limitation in the accurate prediction of perceptual qualities of the traditional distorted images and the Generative Adversarial Networks (GANs) based distorted images. To address this issue, we propose a novel method by Unifying Dual-Attention and Siamese Transformer Network (UniDASTN) for FR-IQA. An important contribution is the spatial attention module composed of a Siamese Transformer Network and a feature fusion block. It can focus on significant regions and effectively maps the perceptual differences between the reference and distorted images to a latent distance for distortion evaluation. Another contribution is the dual-attention strategy that exploits channel attention and spatial attention to aggregate features for enhancing distortion sensitivity. In addition, a novel loss function is designed by jointly exploiting Mean Square Error (MSE), bidirectional Kullback–Leibler divergence, and rank order of quality scores. The designed loss function can offer stable training and thus enables the proposed UniDASTN to effectively learn visual perceptual image quality. Extensive experiments on standard IQA databases are conducted to validate the effectiveness of the proposed UniDASTN. The IQA results demonstrate that the proposed UniDASTN outperforms some state-of-the-art FR-IQA methods on the LIVE, CSIQ, TID2013, and PIPAL databases.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W4377021900",
    "type": "article"
  },
  {
    "title": "Modeling Long-range Dependencies and Epipolar Geometry for Multi-view Stereo",
    "doi": "https://doi.org/10.1145/3596445",
    "publication_date": "2023-05-05",
    "publication_year": 2023,
    "authors": "Jie Zhu; Bo Peng; Wanqing Li; Haifeng Shen; Qingming Huang; Jianjun Lei",
    "corresponding_authors": "",
    "abstract": "This article proposes a network, referred to as Multi-View Stereo TRansformer (MVSTR) for depth estimation from multi-view images. By modeling long-range dependencies and epipolar geometry, the proposed MVSTR is capable of extracting dense features with global context and 3D consistency, which are crucial for reliable matching in multi-view stereo (MVS). Specifically, to tackle the problem of the limited receptive field of existing CNN-based MVS methods, a global-context Transformer module is designed to establish intra-view long-range dependencies so that global contextual features of each view are obtained. In addition, to further enable features of each view to be 3D consistent, a 3D-consistency Transformer module with an epipolar feature sampler is built, where epipolar geometry is modeled to effectively facilitate cross-view interaction. Experimental results show that the proposed MVSTR achieves the best overall performance on the DTU dataset and demonstrates strong generalization on the Tanks &amp; Temples benchmark dataset.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W4372348803",
    "type": "article"
  },
  {
    "title": "Deep Learning for Logo Detection: A Survey",
    "doi": "https://doi.org/10.1145/3611309",
    "publication_date": "2023-07-28",
    "publication_year": 2023,
    "authors": "Sujuan Hou; Jiacheng Li; Weiqing Min; Qiang Hou; Yanna Zhao; Yuanjie Zheng; Shuqiang Jiang",
    "corresponding_authors": "",
    "abstract": "Logo detection has gradually become a research hotspot in the field of computer vision and multimedia for its various applications, such as social media monitoring, intelligent transportation, and video advertising recommendation. Recent advances in this area are dominated by deep learning-based solutions, where many datasets, learning strategies, network architectures, and loss functions have been employed. This article reviews the advance in applying deep learning techniques to logo detection. First, we discuss a comprehensive account of public datasets designed to facilitate performance evaluation of logo detection algorithms, which tend to be more diverse, more challenging, and more reflective of real life. Next, we perform an in-depth analysis of the existing logo detection strategies and their strengths and weaknesses of each learning strategy. Subsequently, we summarize the applications of logo detection in various fields, from intelligent transportation and brand monitoring to copyright and trademark compliance. Finally, we analyze the potential challenges and present the future directions for the development of logo detection. This study aims better to inform readers about the current state of logo detection and encourage more researchers to get involved in logo detection.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W4385343171",
    "type": "article"
  },
  {
    "title": "Hypergraph Association Weakly Supervised Crowd Counting",
    "doi": "https://doi.org/10.1145/3594670",
    "publication_date": "2023-04-26",
    "publication_year": 2023,
    "authors": "Bo Li; Yong Zhang; Chengyang Zhang; Xinglin Piao; Baocai Yin",
    "corresponding_authors": "",
    "abstract": "Weakly supervised crowd counting involves the regression of the number of individuals present in an image, using only the total number as the label. However, this task is plagued by two primary challenges: the large variation of head size and uneven distribution of crowd density. To address these issues, we propose a novel Hypergraph Association Crowd Counting (HACC) framework. Our approach consists of a new multi-scale dilated pyramid module that can efficiently handle the large variation of head size. Further, we propose a novel hypergraph association module to solve the problem of uneven distribution of crowd density by encoding higher-order associations among features, which opens a new direction to solve this problem. Experimental results on multiple datasets demonstrate that our HACC model achieves new state-of-the-art results.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W4367050941",
    "type": "article"
  },
  {
    "title": "Perceptual Quality Assessment of Omnidirectional Images: A Benchmark and Computational Model",
    "doi": "https://doi.org/10.1145/3640344",
    "publication_date": "2024-01-26",
    "publication_year": 2024,
    "authors": "Xuelin Liu; Jiebin Yan; Liping Huang; Yuming Fang; Zheng Wan; Yang Liu",
    "corresponding_authors": "",
    "abstract": "Compared with traditional 2D images, omnidirectional images (also referred to as 360 ∘ images) have more complicated perceptual characteristics due to the particularities of imaging and display. How humans perceive omnidirectional images in an immersive environment and form the immersive quality of experience are important problems. Thus, it is crucial to measure the quality of omnidirectional images under different viewing conditions, which suffer from realistic distortions. In this article, we build a large-scale subjective assessment database for omnidirectional images and carry out a comprehensive psychophysical experiment to study the relationships between different factors (viewing conditions and viewing behaviors) and the perceptual quality of omnidirectional images. In addition, we collect both subjective ratings and head movement data. A thorough analysis of the collected subjective data is also provided, where we make several interesting findings. Moreover, with the proposed database, we propose a novel transformer-based omnidirectional image quality assessment model. To be consistent with the human viewing process, viewing conditions and behaviors are naturally incorporated into the proposed model. Specifically, the proposed model mainly consists of three parts: viewport sequence generation, multi-scale feature extraction, and perceptual quality prediction. Extensive experimental results conducted on the proposed database demonstrate the effectiveness of the proposed method over existing image quality assessment methods.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4391262288",
    "type": "article"
  },
  {
    "title": "MultiMatch: Multi-task Learning for Semi-supervised Domain Generalization",
    "doi": "https://doi.org/10.1145/3648680",
    "publication_date": "2024-02-19",
    "publication_year": 2024,
    "authors": "Lei Qi; Hongpeng Yang; Yinghuan Shi; Xin Geng",
    "corresponding_authors": "",
    "abstract": "Domain generalization (DG) aims at learning a model on source domains to well generalize on the unseen target domain. Although it has achieved great success, most of the existing methods require the label information for all training samples in source domains, which is time-consuming and expensive in the real-world application. In this article, we resort to solving the semi-supervised domain generalization (SSDG) task, where there are a few label information in each source domain. To address the task, we first analyze the theory of multi-domain learning, which highlights that (1) mitigating the impact of domain gap and (2) exploiting all samples to train the model can effectively reduce the generalization error in each source domain so as to improve the quality of pseudo-labels. According to the analysis, we propose MultiMatch, i.e., extending FixMatch to the multi-task learning framework, producing the high-quality pseudo-label for SSDG. To be specific, we consider each training domain as a single task (i.e., local task) and combine all training domains together (i.e., global task) to train an extra task for the unseen test domain. In the multi-task framework, we utilize the independent batch normalization and classifier for each task, which can effectively alleviate the interference from different domains during pseudo-labeling. Also, most of the parameters in the framework are shared, which can be trained by all training samples sufficiently. Moreover, to further boost the pseudo-label accuracy and the model’s generalization, we fuse the predictions from the global task and local task during training and testing, respectively. A series of experiments validate the effectiveness of the proposed method, and it outperforms the existing semi-supervised methods and the SSDG method on several benchmark DG datasets.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4391926652",
    "type": "article"
  },
  {
    "title": "Towards Retrieval-Augmented Architectures for Image Captioning",
    "doi": "https://doi.org/10.1145/3663667",
    "publication_date": "2024-05-03",
    "publication_year": 2024,
    "authors": "Sara Sarto; Marcella Cornia; Lorenzo Baraldi; Alessandro Nicolosi; Rita Cucchiara",
    "corresponding_authors": "",
    "abstract": "The objective of image captioning models is to bridge the gap between the visual and linguistic modalities by generating natural language descriptions that accurately reflect the content of input images. In recent years, researchers have leveraged deep learning-based models and made advances in the extraction of visual features and the design of multimodal connections to tackle this task. This work presents a novel approach toward developing image captioning models that utilize an external k NN memory to improve the generation process. Specifically, we propose two model variants that incorporate a knowledge retriever component that is based on visual similarities, a differentiable encoder to represent input images, and a k NN-augmented language model to predict tokens based on contextual cues and text retrieved from the external memory. We experimentally validate our approach on COCO and nocaps datasets and demonstrate that incorporating an explicit external memory can significantly enhance the quality of captions, especially with a larger retrieval corpus. This work provides valuable insights into retrieval-augmented captioning models and opens up new avenues for improving image captioning at a larger scale.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4396612944",
    "type": "article"
  },
  {
    "title": "Light-Aware Contrastive Learning for Low-Light Image Enhancement",
    "doi": "https://doi.org/10.1145/3665498",
    "publication_date": "2024-05-24",
    "publication_year": 2024,
    "authors": "Xu Wu; Zhihui Lai; Jie Zhou; Xianxu Hou; Witold Pedrycz; Linlin Shen",
    "corresponding_authors": "",
    "abstract": "Low-Light Image Enhancement (LLIE) presents challenges due to texture information loss and uneven illumination, which can distort feature distribution and reduce the quality of the enhanced images. However, current deep learning methods for LLIE only use supervised information from clear images to extract low-light image features, while disregarding the negative information in low-light images (i.e., low illumination and noise). To address these challenges, we propose a novel LLIE method, LACR-VAE, by leveraging the negative information and considering the uneven illumination. In particular, a Light-Aware Contrastive Regularization (LACR) based on contrastive learning is designed to exploit information from both clear and low-light images. The LACR aims to align latent variables of enhanced images with clear images, away from those of low-light images. This allows the method to prioritize essential elements for LLIE and minimize noise and lighting variations. Furthermore, considering the uneven illumination with diverse region sizes and shapes, a Region-CAlibrated Module (RCAM) is present to learn local and global illumination relations among image regions, and an Attention-guided Multi-Scale Module (AMSM) is designed to extract multi-scale features that improve the model’s representation capability. Extensive experiments show that our method achieves superior performance than previous works. Specifically, our method yields a significant enhancement in the National Aeronautics and Space Administration (NASA) testset, achieving an improvement of at least 0.99 in PSNR and 0.0409 in SSIM. Codes and datasets are available at https://github.com/csxuwu/LACR-VAE .",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4398781982",
    "type": "article"
  },
  {
    "title": "PRest-Net: Multi-domain Probability Estimation Network for Robust Image Forgery Detection",
    "doi": "https://doi.org/10.1145/3711930",
    "publication_date": "2025-01-17",
    "publication_year": 2025,
    "authors": "Jiaxin Chen; Xin Liao; Zhenxing Qian; Zheng Qin",
    "corresponding_authors": "",
    "abstract": "As an important carrier of information transmission in online social networks (OSNs), the authenticity protection of images is of great significance. However, the abuse of image processing technology makes its security questionable. Meantime, lossy operations adopted by OSNs will change the forgery artifacts, which brings challenges to robust image forgery detection. To address this issue, considering the suppression of lossy noise caused by transmission, a novel multi-domain probability estimation network (PRest-Net) is proposed. Firstly, we design a multi-domain probability estimation method to capture the most differentiated regional information from the spatial, residual, and wavelet domains. Since the wavelet coefficient of semantic information is larger than that of lossy noise, and the edge texture can be highlighted in the residual image, the negative effect of lossy noise would be reduced and semantic forgery traces can be exposed more easily. We further design a forgery detector composed of low-level feature extraction, high-level feature extraction, and regional edge difference learning module, which can adaptively learn rich forgery clues. Extensive experimental results are provided to validate the superiority of PRest-Net compared with existing state-of-the-art detectors in the scenarios of detecting forged images transmitted over various OSNs.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4406526554",
    "type": "article"
  },
  {
    "title": "Unleashing Creativity in the Metaverse: Generative AI and Multimodal Content",
    "doi": "https://doi.org/10.1145/3713075",
    "publication_date": "2025-01-21",
    "publication_year": 2025,
    "authors": "Abdulmotaleb El Saddik; Jamil Ahmad; Mustaqeem Khan; Saad Abouzahir; Wail Gueaieb",
    "corresponding_authors": "",
    "abstract": "The metaverse presents an emerging creative expression and collaboration frontier where generative artificial intelligence (GenAI) can play a pivotal role with its ability to generate multimodal content from simple prompts. These prompts allow the metaverse to interact with GenAI, where context information, instructions, input data, or even output indications constituting the prompt can come from within the metaverse. However, their integration poses challenges regarding interoperability, lack of standards, scalability, and maintaining a high-quality user experience. This article explores how GenAI can productively assist in enhancing creativity within the contexts of the metaverse and unlock new opportunities. We provide a technical, in-depth overview of the different generative models for image, video, audio, and 3D content within the metaverse environments. We also explore the bottlenecks, opportunities, and innovative applications of GenAI from the perspectives of end users, developers, service providers, and AI researchers. This survey commences by highlighting the potential of GenAI for enhancing the metaverse experience through dynamic content generation to populate massive virtual worlds. Subsequently, we shed light on the ongoing research practices and trends in multimodal content generation, enhancing realism and creativity and alleviating bottlenecks related to standardization, computational cost, privacy, and safety. Last, we share insights into promising research directions toward the integration of GenAI with the metaverse for creative enhancement, improved immersion, and innovative interactive applications.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4406676560",
    "type": "article"
  },
  {
    "title": "Multi-modal Sarcasm Detection on Social Media via Multi-Granularity Information Fusion",
    "doi": "https://doi.org/10.1145/3715139",
    "publication_date": "2025-01-24",
    "publication_year": 2025,
    "authors": "Lisong Ou; Zhixin Li",
    "corresponding_authors": "",
    "abstract": "The rising popularity of diverse social media platforms, commonly utilized by individuals to articulate their emotions in everyday interactions, has spurred a growing interest in the task of multi-modal sarcasm detection (MSD). Nonetheless, due to the unique nature of sarcasm, there remain two main challenges to achieving robust MSD. Firstly, prevailing methodologies often overlook the issue of weak multi-modal correlation, thereby neglecting the crucial sarcasm cues that are inherent in each uni-modal source. Secondly, there are inefficiencies in modeling cross-modal interactions in unaligned multi-modal data. To tackle these challenges, we introduce a multi-granularity information fusion network for multi-modal sarcasm detection. Specifically, we design a multi-task CLIP framework that can utilize multi-granularity cues from multiple tasks (i.e., text, image, and text-image interaction tasks) for multi-modal sarcasm detection. Furthermore, we devise a global-local cross-modal interaction learning method that uses discourse-level representations from each modality as the global multi-modal context to engage with local uni-modal features. This enables the global multi-modal context and local uni-modal features to mutually enhance and progressively improve through a multi-layer superposition. Following extensive experimental results and thorough analysis, our model achieves state-of-the-art performance in multi-modal sarcasm detection.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4406797526",
    "type": "article"
  },
  {
    "title": "Multi-scale Dynamic Fusion for Visible-Infrared Person Re-Identification",
    "doi": "https://doi.org/10.1145/3715330",
    "publication_date": "2025-01-28",
    "publication_year": 2025,
    "authors": "Shen Wang; Yu Wang; Renjie Qiao; Kejun Wu; Chia‐Wen Lin; Chengtao Cai",
    "corresponding_authors": "",
    "abstract": "Visible-infrared person re-identification (VI-ReID) aims to match persons across visible and infrared modalities; however, its performance is prone to complex dynamic scenes, such as occlusions, background shifts, and pose changes. In this paper, we propose a Multi-scale Dynamic Fusion Network (MDFN) to address these challenges in the VI-ReID task. Specifically, the proposed MDFN consists of the Dynamic Feature Fusion (DFF), Dynamic Perception Enhancement (DPE), and Feature Reweighting with Similarity (FRS) modules. The DFF module dynamically extracts local and long-range dependencies among features to obtain finer-grained discriminative features. The DPE module extracts multi-scale features from both visible and infrared modalities to generate diverse embeddings. The FRS module mitigates the impact of information imbalance between modalities, thereby further improving performance. Extensive experiments on the SYSU-MM01 and RegDB datasets show that our MDFN outperforms other state-of-the-art methods, especially in complex dynamic scenes with occlusions, background shifts, and pose changes.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4406903260",
    "type": "article"
  },
  {
    "title": "AdGPT: Explore Meaningful Advertising with ChatGPT",
    "doi": "https://doi.org/10.1145/3720546",
    "publication_date": "2025-02-27",
    "publication_year": 2025,
    "authors": "Jiannan Huang; Mengxue Qu; Longfei Li; Yunchao Wei",
    "corresponding_authors": "",
    "abstract": "Advertising is pervasive in everyday life. Some advertisements are not as readily comprehensible, as they convey a deeper message or purpose, which is referred to as “meaningful advertising”. These ads often aim to create an emotional connection with the audience or promote a social cause. Developing a method for automatically understanding meaningful advertising would be advantageous for the dissemination and creation of such ads. However, current models of ad understanding primarily focus on the superficial aspects of images. In this paper, we introduce AdGPT, a model that leverages visual expert analysis to guide Large Language Models (LLMs) in generating adaptive reasoning chains. Informed by these chains of thought, the model can intelligently comprehend meaningful ads regarding category, content, and sentiment. To assess the effectiveness of our approach, we extract a subset of meaningful ads from the widely used Pitt’s ad images for analysis. Beyond employing traditional ad understanding metrics to evaluate the LLMs’ comprehensive ad comprehension, we also develop a novel generative metric that aligns with user study evaluations for consistent performance assessment. Experiments show that our methods outperform existing state-of-the-art (SOTA) approaches directly linking visual expert models and LLMs and large-scale visual-language models. Code is available at https://github.com/Rbrq03/AdGPT",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4408022448",
    "type": "article"
  },
  {
    "title": "Video Fire Recognition Using Zero-shot Vision-language Models Guided by a Task-aware Object Detector",
    "doi": "https://doi.org/10.1145/3721291",
    "publication_date": "2025-03-03",
    "publication_year": 2025,
    "authors": "Diego Gragnaniello; Antonio Greco; Carlo Sansone; Bruno Vento",
    "corresponding_authors": "",
    "abstract": "Fire detection from images or videos has gained a growing interest in recent years due to the criticality of the application. Both reliable real-time detectors and efficient retrieval techniques, able to process large databases acquired by sensor networks, are needed. Even if the reliability of artificial vision methods improved in the last years, some issues are still open problems. In particular, literature methods often reveal a low generalization capability when employed in scenarios different from the training ones in terms of framing distance, surrounding environment, or weather conditions. This can be addressed by considering contextual information and, more specifically, using vision-language models capable of interpreting and describing the framed scene. In this work, we propose FIRE-TASTIC: FIre REcognition with Task-Aware Spatio-Temporal Image Captioning, a novel framework to use object detectors in conjunction with vision-language models for fire detection and information retrieval. The localization capability of the former makes it able to detect even tiny fire traces but expose the system to false alarms. These are strongly reduced by the impressive zero-shot generalization capability of the latter, which can recognize and describe fire-like objects without prior fine-tuning. We also present a variant of the FIRE-TASTIC framework based on Visual Question Answering instead of Image Captioning, which allows one to customize the retrieved information with personalized questions. To integrate the high-level information provided by both neural networks, we propose a novel method to query the vision-language models using the temporal and spatial localization information provided by the object detector. The proposal can improve the retrieval performance, as evidenced by the experiments conducted on two recent fire detection datasets, showing the effectiveness and the generalization capabilities of FIRE-TASTIC, which surpasses the state of the art. Moreover, the vision-language model, which is unsuitable for video processing due to its high computational load, is executed only on suspicious frames, allowing for real-time processing. This makes FIRE-TASTIC suitable for both real-time processing and information retrieval on large datasets.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4408112786",
    "type": "article"
  },
  {
    "title": "Immersive Multimedia Communication: State-of-the-Art on eXtended Reality Streaming",
    "doi": "https://doi.org/10.1145/3721292",
    "publication_date": "2025-03-03",
    "publication_year": 2025,
    "authors": "Haopeng Wang; Haiwei Dong; Abdulmotaleb El Saddik",
    "corresponding_authors": "",
    "abstract": "Extended reality (XR) is rapidly advancing and poised to revolutionize content creation and consumption. In XR, users integrate various sensory inputs to form a cohesive perception of the virtual environment. This survey reviews the state-of-the-art in XR streaming, focusing on multiple paradigms. To begin, we define XR and introduce various XR headsets along with their multimodal interaction methods to provide a foundational understanding. We then analyze XR traffic characteristics to highlight the unique data transmission requirements. We also explore factors that influence the quality of experience in XR systems, aiming to identify key elements for enhancing user satisfaction. Following this, we present visual attention-based optimization methods for XR streaming to improve efficiency and performance. Finally, we examine current applications and highlight challenges to provide insights into ongoing and future developments of XR.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4408112819",
    "type": "article"
  },
  {
    "title": "Local-Aware Residual Attention Vision Transformer for Visible-Infrared Person Re-Identification",
    "doi": "https://doi.org/10.1145/3723358",
    "publication_date": "2025-03-14",
    "publication_year": 2025,
    "authors": "Xuecheng Hua; Ke Cheng; Gege Zhu; Hu Lu; Yuanquan Wang; Shitong Wang",
    "corresponding_authors": "",
    "abstract": "Visible-infrared person re-identification (VI-ReID) task is to retrieve the same pedestrian across the visible and infrared modalities. The existing transformer-based works are constrained by the inherent structure of the ViT that feature collapse in deeper layers and the over-globalization of extracted features, resulting in incomplete learning of local and low-level features. However, these features are instrumental in representing and identifying elements within visible-infrared images more comprehensively, which increases the accuracy and robustness of cross-modal pedestrian matching. To solve the above problem, we propose the Local-Aware Residual Attention Vision Transformer (LAReViT) to enhance the learning of fine-grained local and shallow-level information to reinforce the feature discrimination and comprehensiveness in ViT. Specifically, the Local-Aware Residual Module (LAR), which uses a novel Local Residual Attention mechanism (LRA), is proposed to increase the fine-grained local information contained in feature extraction. In order to exploit fine-grained local information lost in lower-level visual features, the LRA in the LAR module adopts novel attention residual connections. Additionally, we propose a Positional Channel Reconstruction Module (PCR) that takes advantage of the local receptive field benefits of convolution. PCR reweights features within patches at the channel level, further facilitating the network emphasis on effective fine-grained local information. Finally, the novel Center Aggregation Loss (CAL) is designed to reduce modality discrepancies moderately and promote comprehensive feature extraction. Extensive experiments conducted on the SYSU-MM01, RegDB, and LLCM datasets demonstrate the state-of-the-art performance achieved by our proposed method. The code is available at https://github.com/Hua-XC/LAReViT.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4408449731",
    "type": "article"
  },
  {
    "title": "Multimodal Food Learning",
    "doi": "https://doi.org/10.1145/3715143",
    "publication_date": "2025-03-20",
    "publication_year": 2025,
    "authors": "Weiqing Min; Xiaobin Hong; Yuxin Liu; Mingyu Huang; Ying Jin; Pengfei Zhou; Leyi Xu; Yilin Wang; Shuqiang Jiang; Yong Rui",
    "corresponding_authors": "",
    "abstract": "Food-centered study has received more attention in the multimedia community for its profound impact on our survival, nutrition and health, pleasure, and enjoyment. Our experience of food is typically multi-sensory: We see food objects, smell its odors, taste its flavors, feel its texture, and hear sounds when chewing. Therefore, multimodal food learning is vital in food-centered study, which aims to relate information from multiple food modalities to support various multimedia tasks, ranging from recognition, retrieval, generation, recommendation, and interaction, enabling applications in different fields like healthcare and agriculture. However, there is no surveys on this topic to our knowledge. To fill this gap, this article formalizes multimodal food learning and comprehensively surveys its typical tasks, technical achievements, existing datasets, and applications to provide the blueprint with researchers and practitioners. Based on the current state of the art, we identify both open research issues and promising research directions, such as multimodal food learning benchmark construction, multimodal food foundation model construction, and multimodality diet estimation. We also point out that closer cooperation from researchers between multimedia and food science can handle some existing challenges and meanwhile open up more new opportunities to advance the fast development of multimodal food learning. This is the first comprehensive survey in this topic and we anticipate about 170 reviewed research articles can benefit academia and industry in this community and beyond.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4408655747",
    "type": "article"
  },
  {
    "title": "Actual Cause Guided Adaptive Gradient Scaling for Balanced Multimodal Sentiment Analysis",
    "doi": "https://doi.org/10.1145/3736415",
    "publication_date": "2025-05-20",
    "publication_year": 2025,
    "authors": "Jili Chen; Qionghao Huang; Changqin Huang; Xiaodi Huang",
    "corresponding_authors": "",
    "abstract": "Multimodal sentiment analysis leverages information from multiple sensors to achieve a comprehensive interpretation of emotions. However, different modalities do not always boost each other as expected. They compete with each other, leading to some modalities being under-optimized during the training process. To address this issue, we propose Adaptive Gradient Scaling with Sparse Mixture-of-Experts (AGS-SMoE) . We first discuss the issue of modal preemption in unified multimodal learning from the perspective of causal preemption. Driven by actual cause, we use the gradient norms from different encoders at two fusion stages as evidence, estimating the current modal preemption state using a parameter-free method. Then, based on the dynamic preemption factor, we design a gradient scaling method to balance optimization for different encoders. Furthermore, we use Mixture-of-Experts to sparsify and perceive multimodal tokens in different preemption states. As a result, our experiments on four multimodal sentiment analysis datasets have achieved state-of-the-art results. Moreover, our method improves modal representation learning at different stages. Extensive experiments confirm that our method can alleviate the modal preemption problem in a plug-and-play manner. Our code is available at https://github.com/TheShy-Dream/AGS-SMoE .",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4410515608",
    "type": "article"
  },
  {
    "title": "Innovations and Challenges of AI in Film: A Methodological Framework for Future Exploration",
    "doi": "https://doi.org/10.1145/3736724",
    "publication_date": "2025-05-22",
    "publication_year": 2025,
    "authors": "Shah Muhammad Imtiyaj Uddin; Rashadul Islam Sumon; Md Ariful Islam Mozumder; Md Kamran Hussin Chowdhury; Tagne Poupi Theodore Armand; Hee‐Cheol Kim",
    "corresponding_authors": "",
    "abstract": "With a focus on producing a new AI-based filmmaking platform, this study explores the revolutionary impact of AI on the film industry. Because of how movies are envisioned, made, and seen, it is drastically shifting due to AI. An in-depth analysis of the emergence and applications of AI in film is provided in this article, with particular attention to how these technologies are incorporated into the screenplay writing, pre-production, cinematography, editing, and visual effects phases of the filmmaking process. It focuses on how blockchain technology is revolutionizing the management of intellectual property and film distribution, how Natural Language Processing (NLP) is automating the drafting of scripts, and how generative AI is producing novel visual styles and narrative structures. AI transforms filmmaking in technical and artistic aspects by increasing production, creating new avenues for creativity, and providing distinctive visual forms. Aside from these important ethical issues, the article also discusses employment displacement, copyright infringement, authorship, creative purpose, and the possibility that AI will reinforce prejudices or preconceptions. A focus on new rules for the moral and responsible use of AI that balance the advantages of automation with the need for human creativity and oversight is placed on the legal advancements governing AI in the film industry. The research also examines how audiences respond to and understand these novel storytelling techniques by contrasting the aesthetics of traditionally made movies with those produced by AI. AI films may need help to retain emotional depth and narrative coherence, even though they frequently use cutting-edge visual effects and interactive storytelling. The study includes case studies and practical scenarios, such as the AI-generated film Sunspring , to interpret the real-world application of AI in filmmaking. Besides, the article extends AI’s role in audio and sound design, highlighting improvements in automated dubbing, sound effects synthesis, and music composition. Our research sheds light on how AI will affect filmmaking in the future, which adds to the larger conversation on the impact of emerging technologies on the creative industries. As the film business develops, AI presents important issues that need to be carefully handled, in addition to the tremendous prospects it gives for innovation and democratization.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4410609932",
    "type": "article"
  },
  {
    "title": "Benchmarking Multi-dimensional AIGC Video Quality Assessment: A Dataset and Unified Model",
    "doi": "https://doi.org/10.1145/3749844",
    "publication_date": "2025-07-22",
    "publication_year": 2025,
    "authors": "Zhichao Zhang; Wei Sun; Xinyue Li; Jun Jia; Xiongkuo Min; Zicheng Zhang; Chunyi Li; Zijian Chen; Puyi Wang; Fengyu Sun; Shangling Jui; Guangtao Zhai",
    "corresponding_authors": "",
    "abstract": "In recent years, artificial intelligence (AI)-driven video generation has gained significant attention due to great advancements in visual and language generative techniques. Consequently, there is a growing need for accurate video quality assessment (VQA) metrics to evaluate the perceptual quality of AI-generated content (AIGC) videos and optimize video generation models. However, assessing the quality of AIGC videos remains a significant challenge because these videos often exhibit highly complex distortions, such as unnatural actions and irrational objects. To address this challenge, we systematically investigate the AIGC-VQA problem in this paper, considering both subjective and objective quality assessment perspectives. For the subjective perspective, we construct the L arge-scale G enerated V ideo Q uality assessment (LGVQ) dataset, consisting of \\(2,808\\) AIGC videos generated by six video generation models using \\(468\\) carefully curated text prompts. Unlike previous subjective VQA experiments, we evaluate the perceptual quality of AIGC videos from three critical dimensions: spatial quality, temporal quality, and text-video alignment, which hold utmost importance for current video generation techniques. For the objective perspective, we establish a benchmark for evaluating existing quality assessment metrics on the LGVQ dataset. Our findings show that current metrics perform poorly on this dataset, highlighting a gap in effective evaluation tools. To bridge this gap, we propose the U nify G enerated V ideo Q uality assessment (UGVQ) model, designed to accurately evaluate the multi-dimensional quality of AIGC videos. The UGVQ model integrates the visual and motion features of videos with the textual features of their corresponding prompts, forming a unified quality-aware feature representation tailored to AIGC videos. Experimental results demonstrate that UGVQ achieves state-of-the-art performance on the LGVQ dataset across all three quality dimensions, validating its effectiveness as an accurate quality metric for AIGC videos. We hope that our benchmark can promote the development of AIGC-VQA studies. Both the LGVQ dataset and the UGVQ model are publicly available on https://github.com/zczhang-sjtu/UGVQ.git .",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4412565292",
    "type": "article"
  },
  {
    "title": "How late can you update gaze-contingent multiresolutional displays without detection?",
    "doi": "https://doi.org/10.1145/1314303.1314310",
    "publication_date": "2007-12-01",
    "publication_year": 2007,
    "authors": "Lester C. Loschky; Gary S. Wolverton",
    "corresponding_authors": "",
    "abstract": "This study investigated perceptual disruptions in gaze-contingent multiresolutional displays (GCMRDs) due to delays in updating the center of highest resolution after an eye movement. GCMRDs can be used to save processing resources and transmission bandwidth in many types of single-user display applications, such as virtual reality, video-telephony, simulators, and remote piloting. The current study found that image update delays as late as 60 ms after an eye movement did not significantly increase the detectability of image blur and/or motion transients due to the update. This is good news for designers of GCMRDs, since 60 ms is ample time to update many GCMRDs after an eye movement without disrupting perception. The study also found that longer eye movements led to greater blur and/or transient detection due to moving the eyes further into the low-resolution periphery, effectively reducing the image resolution at fixation prior to the update. In GCMRD applications where longer saccades are more likely (e.g., displays with relatively large distances between objects), this problem could be overcome by increasing the size of the region of highest resolution.",
    "cited_by_count": 74,
    "openalex_id": "https://openalex.org/W2025271966",
    "type": "article"
  },
  {
    "title": "Exploring large-scale peer-to-peer live streaming topologies",
    "doi": "https://doi.org/10.1145/1386109.1386112",
    "publication_date": "2008-08-01",
    "publication_year": 2008,
    "authors": "Chuan Wu; Baochun Li; Shuqiao Zhao",
    "corresponding_authors": "",
    "abstract": "Real-world live peer-to-peer (P2P) streaming applications have been successfully deployed in the Internet, delivering live multimedia content to millions of users at any given time. With relative simplicity in design with respect to peer selection and topology construction protocols and without much algorithmic sophistication, current-generation live P2P streaming applications are able to provide users with adequately satisfying viewing experiences. That said, little existing research has provided sufficient insights on the time-varying internal characteristics of peer-to-peer topologies in live streaming. This article presents Magellan , our collaborative work with UUSee Inc., Beijing, China, for exploring and charting graph theoretical properties of practical P2P streaming topologies, gaining important insights in their topological dynamics over a long period of time. With more than 120 GB worth of traces starting September 2006 from a commercially deployed P2P live streaming system that represents UUSee's core product, we have completed a thorough and in-depth investigation of the topological properties in large-scale live P2P streaming, as well as their evolutionary behavior over time, for example, at different times of the day and in flash crowd scenarios. We seek to explore real-world P2P streaming topologies with respect to their graph theoretical metrics, such as the degree, clustering coefficient, and reciprocity. In addition, we compare our findings with results from existing studies on topological properties of P2P file sharing applications, and present new and unique observations specific to streaming. We have observed that live P2P streaming sessions demonstrate excellent scalability, a high level of reciprocity, a clustering phenomenon in each ISP, and a degree distribution that does not follow the power-law distribution.",
    "cited_by_count": 74,
    "openalex_id": "https://openalex.org/W2052835965",
    "type": "article"
  },
  {
    "title": "Multimedia streaming using multiple TCP connections",
    "doi": "https://doi.org/10.1145/1352012.1352016",
    "publication_date": "2008-05-01",
    "publication_year": 2008,
    "authors": "Sunand Tullimas; Thinh Nguyen; Rich Edgecomb; Sen-ching S. Cheung",
    "corresponding_authors": "",
    "abstract": "In recent years, multimedia applications over the Internet become increasingly popular. However, packet loss, delay, and time-varying bandwidth of the Internet have remained the major problems for multimedia streaming applications. As such, a number of approaches, including network infrastructure and protocol, source and channel coding, have been proposed to either overcome or alleviate these drawbacks of the Internet. In this article, we propose the MultiTCP system, a receiver-driven, TCP-based system for multimedia streaming over the Internet. Our proposed algorithm aims at providing resilience against short term insufficient bandwidth by using multiple TCP connections for the same application. Our proposed system enables the application to achieve and control the desired sending rate during congested periods, which cannot be achieved using traditional TCP. Finally, our proposed system is implemented at the application layer, and hence, no kernel modification to TCP is necessary. We analyze the proposed system, and present simulation and experimental results to demonstrate its advantages over the traditional single-TCP-based approach.",
    "cited_by_count": 67,
    "openalex_id": "https://openalex.org/W2009750107",
    "type": "article"
  },
  {
    "title": "Audio keywords generation for sports video analysis",
    "doi": "https://doi.org/10.1145/1352012.1352015",
    "publication_date": "2008-05-01",
    "publication_year": 2008,
    "authors": "Min Xu; Changsheng Xu; Ling‐Yu Duan; Jesse S. Jin; Suhuai Luo",
    "corresponding_authors": "",
    "abstract": "Sports video has attracted a global viewership. Research effort in this area has been focused on semantic event detection in sports video to facilitate accessing and browsing. Most of the event detection methods in sports video are based on visual features. However, being a significant component of sports video, audio may also play an important role in semantic event detection. In this paper, we have borrowed the concept of the “keyword” from the text mining domain to define a set of specific audio sounds. These specific audio sounds refer to a set of game-specific sounds with strong relationships to the actions of players, referees, commentators, and audience, which are the reference points for interesting sports events. Unlike low-level features, audio keywords can be considered as a mid-level representation, able to facilitate high-level analysis from the semantic concept point of view. Audio keywords are created from low-level audio features with learning by support vector machines. With the help of video shots, the created audio keywords can be used to detect semantic events in sports video by Hidden Markov Model (HMM) learning. Experiments on creating audio keywords and, subsequently, event detection based on audio keywords have been very encouraging. Based on the experimental results, we believe that the audio keyword is an effective representation that is able to achieve satisfying results for event detection in sports video. Application in three sports types demonstrates the practicality of the proposed method.",
    "cited_by_count": 67,
    "openalex_id": "https://openalex.org/W2089368853",
    "type": "article"
  },
  {
    "title": "Invisible watermarking based on creation and robust insertion-extraction of image adaptive watermarks",
    "doi": "https://doi.org/10.1145/1413862.1413865",
    "publication_date": "2008-11-01",
    "publication_year": 2008,
    "authors": "Saraju P. Mohanty; Bharat Bhargava",
    "corresponding_authors": "",
    "abstract": "This article presents a novel invisible robust watermarking scheme for embedding and extracting a digital watermark in an image. The novelty lies in determining a perceptually important subimage in the host image. Invisible insertion of the watermark is performed in the most significant region of the host image such that tampering of that portion with an intention to remove or destroy will degrade the esthetic quality and value of the image. One feature of the algorithm is that this subimage is used as a region of interest for the watermarking process and eliminates the chance of watermark removal. Another feature of the algorithm is the creation of a compound watermark using the input user watermark (logo) and attributes of the host image. This facilitates the homogeneous fusion of a watermark with the cover image, preserves the quality of the host image, and allows robust insertion-extraction. Watermark creation consists of two distinct phases. During the first phase, a statistical image is synthesized from a perceptually important subimage of the image. A compound watermark is created by embedding a watermark (logo) into the statistical synthetic image by using a visible watermarking technique. This compound watermark is invisibly embedded into the important block of the host image. The authentication process involves extraction of the perceptive logo as well statistical testing for two-layer evidence. Results of the experimentation using standard benchmarks demonstrates the robustness and efficacy of the proposed watermarking approach. Ownership proof could be established under various hostile attacks.",
    "cited_by_count": 63,
    "openalex_id": "https://openalex.org/W2065800307",
    "type": "article"
  },
  {
    "title": "Understanding overlay characteristics of a large-scale peer-to-peer IPTV system",
    "doi": "https://doi.org/10.1145/1865106.1865115",
    "publication_date": "2010-11-01",
    "publication_year": 2010,
    "authors": "Long Vu; Indranil Gupta; Klara Nahrstedt; Liang Jin",
    "corresponding_authors": "",
    "abstract": "This article presents results from our measurement and modeling efforts on the large-scale peer-to-peer (p2p) overlay graphs spanned by the PPLive system, the most popular and largest p2p IPTV (Internet Protocol Television) system today. Unlike other previous studies on PPLive, which focused on either network-centric or user-centric measurements of the system, our study is unique in (a) focusing on PPLive overlay-specific characteristics, and (b) being the first to derive mathematical models for its distributions of node degree, session length, and peer participation in simultaneous overlays. Our studies reveal characteristics of multimedia streaming p2p overlays that are markedly different from existing file-sharing p2p overlays. Specifically, we find that: (1) PPLive overlays are similar to random graphs in structure and thus more robust and resilient to the massive failure of nodes, (2) Average degree of a peer in the overlay is independent of the channel population size and the node degree distribution can be fitted by a piecewise function, (3) The availability correlation between PPLive peer pairs is bimodal, that is, some pairs have highly correlated availability, while others have no correlation, (4) Unlike p2p file-sharing peers, PPLive peers are impatient and session lengths (discretized, per channel) are typically geometrically distributed, (5) Channel population size is time-sensitive, self-repeated, event-dependent, and varies more than in p2p file-sharing networks, (6) Peering relationships are slightly locality-aware, and (7) Peer participation in simultaneous overlays follows a Zipf distribution. We believe that our findings can be used to understand current large-scale p2p streaming systems for future planning of resource usage, and to provide useful and practical hints for future design of large-scale p2p streaming systems.",
    "cited_by_count": 61,
    "openalex_id": "https://openalex.org/W2162930318",
    "type": "article"
  },
  {
    "title": "Video accessibility enhancement for hearing-impaired users",
    "doi": "https://doi.org/10.1145/2037676.2037681",
    "publication_date": "2011-10-01",
    "publication_year": 2011,
    "authors": "Richang Hong; Meng Wang; Xiao‐Tong Yuan; Mengdi Xu; Jianguo Jiang; Shuicheng Yan; Tat‐Seng Chua",
    "corresponding_authors": "",
    "abstract": "There are more than 66 million people suffering from hearing impairment and this disability brings them difficulty in video content understanding due to the loss of audio information. If the scripts are available, captioning technology can help them in a certain degree by synchronously illustrating the scripts during the playing of videos. However, we show that the existing captioning techniques are far from satisfactory in assisting the hearing-impaired audience to enjoy videos. In this article, we introduce a scheme to enhance video accessibility using a Dynamic Captioning approach, which explores a rich set of technologies including face detection and recognition, visual saliency analysis, text-speech alignment, etc. Different from the existing methods that are categorized as static captioning, dynamic captioning puts scripts at suitable positions to help the hearing-impaired audience better recognize the speaking characters. In addition, it progressively highlights the scripts word-by-word via aligning them with the speech signal and illustrates the variation of voice volume. In this way, the special audience can better track the scripts and perceive the moods that are conveyed by the variation of volume. We implemented the technology on 20 video clips and conducted an in-depth study with 60 real hearing-impaired users. The results demonstrated the effectiveness and usefulness of the video accessibility enhancement scheme.",
    "cited_by_count": 60,
    "openalex_id": "https://openalex.org/W2041317995",
    "type": "article"
  },
  {
    "title": "LiveSky",
    "doi": "https://doi.org/10.1145/1823746.1823750",
    "publication_date": "2010-08-01",
    "publication_year": 2010,
    "authors": "Hao Yin; Xuening Liu; Tongyu Zhan; Vyas Sekar; Feng Qiu; Chuang Lin; Hui Zhang; Bo Li",
    "corresponding_authors": "",
    "abstract": "We present the design and deployment experiences with LiveSky , a commercial hybrid CDN-P2P live streaming system, which inherits the best of both CDN and P2P. We address several key challenges, including: 1) ease of integration with existing CDN infrastructure, 2) dynamic resource scaling while guaranteeing quality-of-service, 3) providing good user experience, ensuring network friendliness and upload fairness. LiveSky has been used for several large-scale live streaming events in China. Our evaluation results from real-world indicate that such a hybrid CDN-P2P system provides quality and performance comparable to a CDN and effectively scales the system capacity.",
    "cited_by_count": 60,
    "openalex_id": "https://openalex.org/W2050037384",
    "type": "article"
  },
  {
    "title": "Mining flickr landmarks by modeling reconstruction sparsity",
    "doi": "https://doi.org/10.1145/2037676.2037688",
    "publication_date": "2011-10-01",
    "publication_year": 2011,
    "authors": "Rongrong Ji; Yue Gao; Bineng Zhong; Hongxun Yao; Qi Tian",
    "corresponding_authors": "",
    "abstract": "In recent years, there have been ever-growing geographical tagged photos on the community Web sites such as Flickr. Discovering touristic landmarks from these photos can help us to make better sense of our visual world. In this article, we report our work on mining landmarks from geotagged Flickr photos for city scene summarization and touristic recommendations. We begin by exploring the geographical and visual statistics of the Web users' photographing manner, based on which we conduct landmark mining in two steps: First, we propose to partition each city into geographical regions based on spectral clustering over the geotags of Flickr photos. Second, in each landmark region, we present a representative photo mining scheme based on sparse representation. Our main idea is to regard the landmark mining problem as a process to find photos whose visual signatures can be reconstructed using other photos of this landmark region with a minimal coding length. This sparse reconstruction scheme offers a general perspective to mine the representative photos. Indeed, by simplifying the data correlation constraints in our scheme, several previous works in representative photo discovery and landmark mining can be derived. Finally, we introduce a Hyperlink-Induced Topic Search model to refine our landmark ranking, which incorporates the community knowledge to simulate the landmark ranking problem as a dynamic page ranking problem. We have deployed our proposed landmark mining framework on a city scene summarization and navigation system, which works on one million geotagged Flickr photos coming from twenty worldwide metropolises. We have also quantitatively compared our scheme with several state-of-the-art works.",
    "cited_by_count": 58,
    "openalex_id": "https://openalex.org/W1976745449",
    "type": "article"
  },
  {
    "title": "Improving the extraction of bilingual terminology from Wikipedia",
    "doi": "https://doi.org/10.1145/1596990.1596995",
    "publication_date": "2009-10-01",
    "publication_year": 2009,
    "authors": "Maike Erdmann; Kotaro Nakayama; Takahiro Hara; Shojiro Nishio",
    "corresponding_authors": "",
    "abstract": "Research on the automatic construction of bilingual dictionaries has achieved impressive results. Bilingual dictionaries are usually constructed from parallel corpora, but since these corpora are available only for selected text domains and language pairs, the potential of other resources is being explored as well. In this article, we want to further pursue the idea of using Wikipedia as a corpus for bilingual terminology extraction. We propose a method that extracts term-translation pairs from different types of Wikipedia link information. After that, an SVM classifier trained on the features of manually labeled training data determines the correctness of unseen term-translation pairs.",
    "cited_by_count": 57,
    "openalex_id": "https://openalex.org/W2003948532",
    "type": "article"
  },
  {
    "title": "A real-time remote rendering system for interactive mobile graphics",
    "doi": "https://doi.org/10.1145/2348816.2348825",
    "publication_date": "2012-09-01",
    "publication_year": 2012,
    "authors": "Shu Shi; Klara Nahrstedt; Roy H. Campbell",
    "corresponding_authors": "",
    "abstract": "Mobile devices are gradually changing people's computing behaviors. However, due to the limitations of physical size and power consumption, they are not capable of delivering a 3D graphics rendering experience comparable to desktops. Many applications with intensive graphics rendering workloads are unable to run on mobile platforms directly. This issue can be addressed with the idea of remote rendering: the heavy 3D graphics rendering computation runs on a powerful server and the rendering results are transmitted to the mobile client for display. However, the simple remote rendering solution inevitably suffers from the large interaction latency caused by wireless networks, and is not acceptable for many applications that have very strict latency requirements. In this article, we present an advanced low-latency remote rendering system that assists mobile devices to render interactive 3D graphics in real-time. Our design takes advantage of an image based rendering technique: 3D image warping, to synthesize the mobile display from the depth images generated on the server. The research indicates that the system can successfully reduce the interaction latency while maintaining the high rendering quality by generating multiple depth images at the carefully selected viewpoints. We study the problem of viewpoint selection, propose a real-time reference viewpoint prediction algorithm, and evaluate the algorithm performance with real-device experiments.",
    "cited_by_count": 57,
    "openalex_id": "https://openalex.org/W2061043836",
    "type": "article"
  },
  {
    "title": "Derivative-based audio steganalysis",
    "doi": "https://doi.org/10.1145/2000486.2000492",
    "publication_date": "2011-08-01",
    "publication_year": 2011,
    "authors": "Qingzhong Liu; Andrew H. Sung; Mengyu Qiao",
    "corresponding_authors": "",
    "abstract": "This article presents a second-order derivative-based audio steganalysis. First, Mel-cepstrum coefficients and Markov transition features from the second-order derivative of the audio signal are extracted; a support vector machine is then applied to the features for discovering the existence of hidden data in digital audio streams. Also, the relation between audio signal complexity and steganography detection accuracy, which is an issue relevant to audio steganalysis performance evaluation but so far has not been explored, is analyzed experimentally. Results demonstrate that, in comparison with a recently proposed signal stream-based Mel-cepstrum method, the second-order derivative-based audio steganalysis method gains a considerable advantage under all categories of signal complexity--especially for audio streams with high signal complexity, which are generally the most challenging for steganalysis-and thereby significantly improves the state of the art in audio steganalysis.",
    "cited_by_count": 56,
    "openalex_id": "https://openalex.org/W2039365749",
    "type": "article"
  },
  {
    "title": "Image search—from thousands to billions in 20 years",
    "doi": "https://doi.org/10.1145/2490823",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Lei Zhang; Yong Rui",
    "corresponding_authors": "",
    "abstract": "This article presents a comprehensive review and analysis on image search in the past 20 years, emphasizing the challenges and opportunities brought by the astonishing increase of dataset scales from thousands to billions in the same time period, which was witnessed first-hand by the authors as active participants in this research area. Starting with a retrospective review of three stages of image search in the history, the article highlights major breakthroughs around the year 2000 in image search features, indexing methods, and commercial systems, which marked the transition from stage two to stage three. Subsequent sections describe the image search research from four important aspects: system framework, feature extraction and image representation, indexing, and big data's potential. Based on the review, the concluding section discusses open research challenges and suggests future research directions in effective visual representation, image knowledge base construction, implicit user feedback and crowdsourcing, mobile image search, and creative multimedia interfaces.",
    "cited_by_count": 56,
    "openalex_id": "https://openalex.org/W2105853573",
    "type": "article"
  },
  {
    "title": "Enabling multi-party 3D tele-immersive environments with <i>ViewCast</i>",
    "doi": "https://doi.org/10.1145/1671962.1671963",
    "publication_date": "2010-03-01",
    "publication_year": 2010,
    "authors": "Zhenyu Yang; Wanmin Wu; Klara Nahrstedt; Gregorij Kurillo; Růžena Bajcsy",
    "corresponding_authors": "",
    "abstract": "Three-dimensional tele-immersive (3DTI) environments have great potential to promote collaborative work among geographically distributed users. However, most existing 3DTI systems only work with two sites due to the huge demand of resources and the lack of a simple yet powerful networking model to handle connectivity, scalability, and quality-of-service (QoS) guarantees. In this article, we explore the design space from the angle of multi-stream management to enable multi-party 3DTI communication. Multiple correlated 3D video streams are employed to provide a comprehensive representation of the physical scene in each 3DTI environment, and are rendered together to establish a common cyberspace among all participating 3DTI environments. The existence of multi-stream correlation provides the unique opportunity for new approaches in QoS provisioning. Previous work mostly concentrated on compression and adaptation techniques on the per-stream basis while ignoring the application layer semantics and the coordination required among streams. We propose an innovative and generalized ViewCast model to coordinate the multi-stream content dissemination over an overlay network. ViewCast leverages view semantics in 3D free-viewpoint video systems to fill the gap between high-level user interest and low-level stream management. In ViewCast, only the view information is specified by the user/application, while the underlying control dynamically performs stream differentiation, selection, coordination, and dissemination. We present the details of ViewCast and evaluate it through both simulation and 3DTI sessions among tele-immersive environments residing in different institutes across the Internet2. Our experimental results demonstrate the implementation feasibility and performance enhancement of ViewCast in supporting multi-party 3DTI collaboration.",
    "cited_by_count": 55,
    "openalex_id": "https://openalex.org/W2010107599",
    "type": "article"
  },
  {
    "title": "GPSView",
    "doi": "https://doi.org/10.1145/2422956.2422959",
    "publication_date": "2013-02-01",
    "publication_year": 2013,
    "authors": "Yan-Tao Zheng; Shuicheng Yan; Zheng-Jun Zha; Yiqun Li; Xiangdong Zhou; Tat‐Seng Chua; Ramesh Jain",
    "corresponding_authors": "",
    "abstract": "GPS devices have been widely used in automobiles to compute navigation routes to destinations. The generated driving route targets the minimal traveling distance, but neglects the sightseeing experience of the route. In this study, we propose an augmented GPS navigation system, GPSView , to incorporate a scenic factor into the routing. The goal of GPSView is to plan a driving route with scenery and sightseeing qualities, and therefore allow travelers to enjoy sightseeing on the drive. To do so, we first build a database of scenic roadways with vistas of landscapes and sights along the roadside. Specifically, we adapt an attention-based approach to exploit community-contributed GPS-tagged photos on the Internet to discover scenic roadways. The premise is: a multitude of photos taken along a roadway imply that this roadway is probably appealing and catches the public's attention. By analyzing the geospatial distribution of photos, the proposed approach discovers the roadside sight spots, or Points-Of-Interest (POIs), which have good scenic qualities and visibility to travelers on the roadway. Finally, we formulate scenic driving route planning as an optimization task towards the best trade-off between sightseeing experience and traveling distance. Testing in the northern California area shows that the proposed system can deliver promising results.",
    "cited_by_count": 55,
    "openalex_id": "https://openalex.org/W2030740234",
    "type": "article"
  },
  {
    "title": "Diagnosing network-wide P2P live streaming inefficiencies",
    "doi": "https://doi.org/10.1145/2089085.2089090",
    "publication_date": "2012-02-01",
    "publication_year": 2012,
    "authors": "Chuan Wu; Baochun Li; Shuqiao Zhao",
    "corresponding_authors": "",
    "abstract": "Large-scale live peer-to-peer (P2P) streaming applications have been successfully deployed in today's Internet. While they can accommodate hundreds of thousands of users simultaneously with hundreds of channels of programming, there still commonly exist channels and times where and when the streaming quality is unsatisfactory. In this paper, based on more than two terabytes and one year worth of live traces from UUSee, a large-scale commercial P2P live streaming system, we show an in-depth network-wide diagnosis of streaming inefficiencies, commonly present in typical mesh-based P2P live streaming systems. As the first highlight of our work, we identify an evolutionary pattern of low streaming quality in the system, and the distribution of streaming inefficiencies across various streaming channels and in different geographical regions. We then carry out an extensive investigation to explore the causes to such streaming inefficiencies over different times and across different channels/regions at specific times, by investigating the impact of factors such as the number of peers, peer upload bandwidth, inter-peer bandwidth availability, server bandwidth consumption, and many more. The original discoveries we have brought forward include the two-sided effects of peer population on the streaming quality in a streaming channel, the significant impact of inter-peer bandwidth bottlenecks at peak times, and the inefficient utilization of server capacities across concurrent channels. Based on these insights, we identify problems within the existing P2P live streaming design and discuss a number of suggestions to improve real-world streaming protocols operating at a large scale.",
    "cited_by_count": 55,
    "openalex_id": "https://openalex.org/W2109065271",
    "type": "article"
  },
  {
    "title": "Information recall task impact in olfaction-enhanced multimedia",
    "doi": "https://doi.org/10.1145/2487268.2487270",
    "publication_date": "2013-06-01",
    "publication_year": 2013,
    "authors": "Oluwakemi A. Ademoye; Gheorghiță Ghinea",
    "corresponding_authors": "",
    "abstract": "Enhancing multimedia applications with olfactory sensations is one of the last challenges in the area. While there is evidence, both scientific and anecdotal, that olfactory cues help users in information recall tasks, there is a lack of work when the targeted information is one contained in a multimedia presentation, which is precisely the focus of this article. Accordingly, we present the results of two experimental studies. The first study measured the impact of olfactory media variation on the user's ability to perceive, synthesize, and analyze the informational content of olfactory-enhanced multimedia videos; the second study measured the impact of information content, and an information recall task in respect of user perception of the relevance, sense of reality, and acceptability of the olfactory media content, as well as the overall enjoyment of the experience. Results show that the use of olfactory media content, both pleasant and unpleasant, in multimedia displays does not significantly impact on information assimilation in a negative way. Moreover, the addition of a performance task may enhance the user's understanding of the correlation between the characteristic odor(s) and the scenario under consideration, as well as enable users to consciously learn the odors.",
    "cited_by_count": 54,
    "openalex_id": "https://openalex.org/W2027499733",
    "type": "article"
  },
  {
    "title": "Robust Color Image Watermarking Using Geometric Invariant Quaternion Polar Harmonic Transform",
    "doi": "https://doi.org/10.1145/2700299",
    "publication_date": "2015-02-05",
    "publication_year": 2015,
    "authors": "Hongying Yang; Xiangyang Wang; Panpan Niu; Ai-long Wang",
    "corresponding_authors": "",
    "abstract": "It is a challenging work to design a robust color image watermarking scheme against geometric distortions. Moments and moment invariants have become a powerful tool in robust image watermarking owing to their image description capability and geometric invariance property. However, the existing moment-based watermarking schemes were mainly designed for gray images but not for color images, and detection quality and robustness will be lowered when watermark is directly embedded into the luminance component or three color channels of color images. Furthermore, the imperceptibility of the embedded watermark is not well guaranteed. Based on algebra of quaternions and polar harmonic transform (PHT), we introduced the quaternion polar harmonic transform (QPHT) for invariant color image watermarking in this article, which can be seen as the generalization of PHT for gray-level images. It is shown that the QPHT can be obtained from the PHT of each color channel. We derived and analyzed the rotation, scaling, and translation (RST) invariant property of QPHT. We also discussed the problem of color image watermarking using QPHT. Experimental results are provided to illustrate the efficiency of the proposed color image watermarking against geometric distortions and common image processing operations (including color attacks).",
    "cited_by_count": 50,
    "openalex_id": "https://openalex.org/W2072142347",
    "type": "article"
  },
  {
    "title": "A Multiplexing Scheme for Multimodal Teleoperation",
    "doi": "https://doi.org/10.1145/3063594",
    "publication_date": "2017-04-11",
    "publication_year": 2017,
    "authors": "Burak Çizmeci; Xiao Xu; Rahul Chaudhari; Christoph Bachhuber; Nicolas Alt; Eckehard Steinbach",
    "corresponding_authors": "",
    "abstract": "This article proposes an application-layer multiplexing scheme for teleoperation systems with multimodal feedback (video, audio, and haptics). The available transmission resources are carefully allocated to avoid delay-jitter for the haptic signal potentially caused by the size and arrival time of the video and audio data. The multiplexing scheme gives high priority to the haptic signal and applies a preemptive-resume scheduling strategy to stream the audio and video data. The proposed approach estimates the available transmission rate in real time and adapts the video bitrate, data throughput, and force buffer size accordingly. Furthermore, the proposed scheme detects sudden transmission rate drops and applies congestion control to avoid abrupt delay increases and converge promptly to the altered transmission rate. The performance of the proposed scheme is measured objectively in terms of end-to-end signal latencies, packet rates, and peak signal-to-noise ratio (PSNR) for visual quality. Moreover, peak-delay and convergence time measurements are carried out to investigate the performance of the congestion control mode of the system.",
    "cited_by_count": 48,
    "openalex_id": "https://openalex.org/W2606780237",
    "type": "article"
  },
  {
    "title": "A Top-Down Approach for Video Summarization",
    "doi": "https://doi.org/10.1145/2632267",
    "publication_date": "2014-08-01",
    "publication_year": 2014,
    "authors": "Genliang Guan; Zhiyong Wang; Shaohui Mei; Maximilian Ott; Mingyi He; Dagan Feng",
    "corresponding_authors": "",
    "abstract": "While most existing video summarization approaches aim to identify important frames of a video from either a global or local perspective, we propose a top-down approach consisting of scene identification and scene summarization. For scene identification, we represent each frame with global features and utilize a scalable clustering method. We then formulate scene summarization as choosing those frames that best cover a set of local descriptors with minimal redundancy. In addition, we develop a visual word-based approach to make our approach more computationally scalable. Experimental results on two benchmark datasets demonstrate that our proposed approach clearly outperforms the state-of-the-art.",
    "cited_by_count": 47,
    "openalex_id": "https://openalex.org/W2043784772",
    "type": "article"
  },
  {
    "title": "Deep Artwork Detection and Retrieval for Automatic Context-Aware Audio Guides",
    "doi": "https://doi.org/10.1145/3092832",
    "publication_date": "2017-06-28",
    "publication_year": 2017,
    "authors": "Lorenzo Seidenari; Claudio Baecchi; Tiberio Uricchio; Andrea Ferracani; Marco Bertini; Alberto Del Bimbo",
    "corresponding_authors": "",
    "abstract": "In this article, we address the problem of creating a smart audio guide that adapts to the actions and interests of museum visitors. As an autonomous agent, our guide perceives the context and is able to interact with users in an appropriate fashion. To do so, it understands what the visitor is looking at, if the visitor is moving inside the museum hall, or if he or she is talking with a friend. The guide performs automatic recognition of artworks, and it provides configurable interface features to improve the user experience and the fruition of multimedia materials through semi-automatic interaction. Our smart audio guide is backed by a computer vision system capable of working in real time on a mobile device, coupled with audio and motion sensors. We propose the use of a compact Convolutional Neural Network (CNN) that performs object classification and localization. Using the same CNN features computed for these tasks, we perform also robust artwork recognition. To improve the recognition accuracy, we perform additional video processing using shape-based filtering, artwork tracking, and temporal filtering. The system has been deployed on an NVIDIA Jetson TK1 and a NVIDIA Shield Tablet K1 and tested in a real-world environment (Bargello Museum of Florence).",
    "cited_by_count": 47,
    "openalex_id": "https://openalex.org/W2733092256",
    "type": "article"
  },
  {
    "title": "Multichannel-Kernel Canonical Correlation Analysis for Cross-View Person Reidentification",
    "doi": "https://doi.org/10.1145/3038916",
    "publication_date": "2017-03-06",
    "publication_year": 2017,
    "authors": "Giuseppe Lisanti; Svebor Karaman; Iacopo Masi",
    "corresponding_authors": "",
    "abstract": "In this paper we introduce a method to overcome one of the main challenges of person re-identification in multi-camera networks, namely cross-view appearance changes. The proposed solution addresses the extreme variability of person appearance in different camera views by exploiting multiple feature representations. For each feature, Kernel Canonical Correlation Analysis (KCCA) with different kernels is exploited to learn several projection spaces in which the appearance correlation between samples of the same person observed from different cameras is maximized. An iterative logistic regression is finally used to select and weigh the contributions of each feature projections and perform the matching between the two views. Experimental evaluation shows that the proposed solution obtains comparable performance on VIPeR and PRID 450s datasets and improves on PRID and CUHK01 datasets with respect to the state of the art.",
    "cited_by_count": 46,
    "openalex_id": "https://openalex.org/W2472876510",
    "type": "article"
  },
  {
    "title": "Spatiotemporal-Textual Co-Attention Network for Video Question Answering",
    "doi": "https://doi.org/10.1145/3320061",
    "publication_date": "2019-04-30",
    "publication_year": 2019,
    "authors": "Zheng-Jun Zha; Jiawei Liu; Tianhao Yang; Yongdong Zhang",
    "corresponding_authors": "",
    "abstract": "Visual Question Answering (VQA) is to provide a natural language answer for a pair of an image or video and a natural language question. Despite recent progress on VQA, existing works primarily focus on image question answering and are suboptimal for video question answering. This article presents a novel Spatiotemporal-Textual Co-Attention Network (STCA-Net) for video question answering. The STCA-Net jointly learns spatially and temporally visual attention on videos as well as textual attention on questions. It concentrates on the essential cues in both visual and textual spaces for answering question, leading to effective question-video representation. In particular, a question-guided attention network is designed to learn question-aware video representation with a spatial-temporal attention module. It concentrates the network on regions of interest within the frames of interest across the entire video. A video-guided attention network is proposed to learn video-aware question representation with a textual attention module, leading to fine-grained understanding of question. The learned video and question representations are used by an answer predictor to generate answers. Extensive experiments on two challenging datasets of video question answering, i.e., MSVD-QA and MSRVTT-QA, have shown the effectiveness of the proposed approach.",
    "cited_by_count": 45,
    "openalex_id": "https://openalex.org/W2964057271",
    "type": "article"
  },
  {
    "title": "SAfeDJ",
    "doi": "https://doi.org/10.1145/2808201",
    "publication_date": "2015-10-21",
    "publication_year": 2015,
    "authors": "Xiping Hu; Jun-qi Deng; Jidi Zhao; Wenyan Hu; Edith C.‐H. Ngai; Renfei Wang; Johnny Shen; Min Liang; Xitong Li; Victor C. M. Leung; Yu‐Kwong Kwok",
    "corresponding_authors": "",
    "abstract": "Driving is an integral part of our everyday lives, but it is also a time when people are uniquely vulnerable. Previous research has demonstrated that not only does listening to suitable music while driving not impair driving performance, but it could lead to an improved mood and a more relaxed body state, which could improve driving performance and promote safe driving significantly. In this article, we propose SAfeDJ, a smartphone-based situation-aware music recommendation system, which is designed to turn driving into a safe and enjoyable experience. SAfeDJ aims at helping drivers to diminish fatigue and negative emotion. Its design is based on novel interactive methods, which enable in-car smartphones to orchestrate multiple sources of sensing data and the drivers' social context, in collaboration with cloud computing to form a seamless crowdsensing solution. This solution enables different smartphones to collaboratively recommend preferable music to drivers according to each driver's specific situations in an automated and intelligent manner. Practical experiments of SAfeDJ have proved its effectiveness in music-mood analysis, and mood-fatigue detections of drivers with reasonable computation and communication overheads on smartphones. Also, our user studies have demonstrated that SAfeDJ helps to decrease fatigue degree and negative mood degree of drivers by 49.09% and 36.35%, respectively, compared to traditional smartphone-based music player under similar driving situations.",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W2071086297",
    "type": "article"
  },
  {
    "title": "Robust Hashing Based on Quaternion Zernike Moments for Image Authentication",
    "doi": "https://doi.org/10.1145/2978572",
    "publication_date": "2016-09-15",
    "publication_year": 2016,
    "authors": "Junlin Ouyang; Xingzi Wen; Jianxun Liu; Jinjun Chen",
    "corresponding_authors": "",
    "abstract": "The reliability and security of multimedia contents in transmission, communications, storage, and usage have attracted special attention. Robust image hashing, also referred to as perceptual image hashing, is widely applied in multimedia authentication and forensics, image retrieval, image indexing, and digital image watermarking. In this work, a novel robust image hashing method based on quaternion Zernike moments (QZMs) is proposed. QZMs offer a sound way to jointly deal with the three channels of color images without discarding chrominance information; the generated hash is thus shorter than the hash of three channels separately processing. The proposed approach's performance was evaluated on the color images database of UCID and compared with several recent and efficient methods. These experiments show that the proposed scheme provides a short hash in length that is robust to most common image content-preserving manipulations like JPEG compression, filtering, noise, scaling, and large angle rotation operations.",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W2519432244",
    "type": "article"
  },
  {
    "title": "A Deep Learning System for Recognizing Facial Expression in Real-Time",
    "doi": "https://doi.org/10.1145/3311747",
    "publication_date": "2019-05-31",
    "publication_year": 2019,
    "authors": "Miao Yu; Haiwei Dong; Jihad Mohamad Jaam; Abdulmotaleb El Saddik",
    "corresponding_authors": "",
    "abstract": "This article presents an image-based real-time facial expression recognition system that is able to recognize the facial expressions of several subjects on a webcam at the same time. Our proposed methodology combines a supervised transfer learning strategy and a joint supervision method with center loss, which is crucial for facial tasks. A newly proposed Convolutional Neural Network (CNN) model, MobileNet, which has both accuracy and speed, is deployed in both offline and in a real-time framework that enables fast and accurate real-time output. Evaluations towards two publicly available datasets, JAFFE and CK+, are carried out respectively. The JAFFE dataset reaches an accuracy of 95.24%, while an accuracy of 96.92% is achieved on the 6-class CK+ dataset, which contains only the last frames of image sequences. At last, the average run-time cost for the recognition of the real-time implementation is around 3.57ms/frame on a NVIDIA Quadro K4200 GPU.",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W2953391430",
    "type": "article"
  },
  {
    "title": "Texture and Geometry Scattering Representation-Based Facial Expression Recognition in 2D+3D Videos",
    "doi": "https://doi.org/10.1145/3131345",
    "publication_date": "2018-03-06",
    "publication_year": 2018,
    "authors": "Yongqiang Yao; Di Huang; Xudong Yang; Yunhong Wang; Liming Chen",
    "corresponding_authors": "",
    "abstract": "Facial Expression Recognition (FER) is one of the most important topics in the domain of computer vision and pattern recognition, and it has attracted increasing attention for its scientific challenges and application potentials. In this article, we propose a novel and effective approach to FER using multi-model two-dimensional (2D) and 3D videos, which encodes both static and dynamic clues by scattering convolution network. First, a shape-based detection method is introduced to locate the start and the end of an expression in videos; segment its onset, apex, and offset states; and sample the important frames for emotion analysis. Second, the frames in Apex of 2D videos are represented by scattering, conveying static texture details. Those of 3D videos are processed in a similar way, but to highlight static shape details, several geometric maps in terms of multiple order differential quantities, i.e., Normal Maps and Shape Index Maps, are generated as the input of scattering, instead of original smooth facial surfaces. Third, the average of neighboring samples centred at each key texture frame or shape map in Onset is computed, and the scattering features extracted from all the average samples of 2D and 3D videos are then concatenated to capture dynamic texture and shape cues, respectively. Finally, Multiple Kernel Learning is adopted to combine the features in the 2D and 3D modalities and compute similarities to predict the expression label. Thanks to the scattering descriptor, the proposed approach not only encodes distinct local texture and shape variations of different expressions as by several milestone operators, such as SIFT, HOG, and so on, but also captures subtle information hidden in high frequencies in both channels, which is quite crucial to better distinguish expressions that are easily confused. The validation is conducted on the BU-4DFE and BP-4D databa ses, and the accuracies reached are very competitive, indicating its competency for this issue.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W2794377003",
    "type": "article"
  },
  {
    "title": "Structure-Aware Deep Learning for Product Image Classification",
    "doi": "https://doi.org/10.1145/3231742",
    "publication_date": "2019-01-24",
    "publication_year": 2019,
    "authors": "Zhineng Chen; Shanshan Ai; Caiyan Jia",
    "corresponding_authors": "",
    "abstract": "Automatic product image classification is a task of crucial importance with respect to the management of online retailers. Motivated by recent advancements of deep Convolutional Neural Networks (CNN) on image classification, in this work we revisit the problem in the context of product images with the existence of a predefined categorical hierarchy and attributes, aiming to leverage the hierarchy and attributes to improve classification accuracy. With these structure-aware clues, we argue that more advanced deep models could be developed beyond the flat one-versus-all classification performed by conventional CNNs. To this end, novel efforts of this work include a salient-sensitive CNN that gazes into the product foreground by inserting a dedicated spatial attention module; a multiclass regression-based refinement that is expected to predict more accurately by merging prediction scores from multiple preceding CNNs, each corresponding to a distinct classifier in the hierarchy; and a multitask deep learning architecture that effectively explores correlations among categories and attributes for categorical label prediction. Experimental results on nearly 1 million real-world product images basically validate the effectiveness of the proposed efforts individually and jointly, from which performance gains are observed.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W2912849458",
    "type": "article"
  },
  {
    "title": "Tile-based Adaptive Streaming for Virtual Reality Video",
    "doi": "https://doi.org/10.1145/3362101",
    "publication_date": "2019-11-30",
    "publication_year": 2019,
    "authors": "Jeroen van der Hooft; Maria Torres Vega; Stefano Petrangeli; Tim Wauters; Filip De Turck",
    "corresponding_authors": "",
    "abstract": "The increasing popularity of head-mounted devices and 360° video cameras allows content providers to provide virtual reality (VR) video streaming over the Internet, using a two-dimensional representation of the immersive content combined with traditional HTTP adaptive streaming (HAS) techniques. However, since only a limited part of the video (i.e., the viewport) is watched by the user, the available bandwidth is not optimally used. Recent studies have shown the benefits of adaptive tile-based video streaming; rather than sending the whole 360° video at once, the video is cut into temporal segments and spatial tiles, each of which can be requested at a different quality level. This allows prioritization of viewable video content and thus results in an increased bandwidth utilization. Given the early stages of research, there are still a number of open challenges to unlock the full potential of adaptive tile-based VR streaming. The aim of this work is to provide an answer to several of these open research questions. Among others, we propose two tile-based rate adaptation heuristics for equirectangular VR video, which use the great-circle distance between the viewport center and the center of each of the tiles to decide upon the most appropriate quality representation. We also introduce a feedback loop in the quality decision process, which allows the client to revise prior decisions based on more recent information on the viewport location. Furthermore, we investigate the benefits of parallel TCP connections and the use of HTTP/2 as an application layer optimization. Through an extensive evaluation, we show that the proposed optimizations result in a significant improvement in terms of video quality (more than twice the time spent on the highest quality layer), compared to non-tiled HAS solutions.",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W2996368995",
    "type": "article"
  },
  {
    "title": "Toward Delay-Efficient Game-Aware Data Centers for Cloud Gaming",
    "doi": "https://doi.org/10.1145/2983639",
    "publication_date": "2016-09-21",
    "publication_year": 2016,
    "authors": "Maryam Amiri; Hussein Al Osman; Shervin Shirmohammadi; Maha Abdallah",
    "corresponding_authors": "",
    "abstract": "Gaming on demand is an emerging service that has recently started to garner prominence in the gaming industry. Cloud-based video games provide affordable, flexible, and high-performance solutions for end-users with constrained computing resources and enables them to play high-end graphic games on low-end thin clients. Despite its advantages, cloud gaming's Quality of Experience (QoE) suffers from high and varying end-to-end delay. Since the significant part of computational processing, including game rendering and video compression, is performed in data centers, controlling the transfer of information within the cloud has an important impact on the quality of cloud gaming services. In this article, a novel method for minimizing the end-to-end latency within a cloud gaming data center is proposed. We formulate an optimization problem for reducing delay, and propose a Lagrangian Relaxation (LR) time-efficient heuristic algorithm as a practical solution. Simulation results indicate that the heuristic method can provide close-to-optimal solutions. Also, the proposed model reduces end-to-end delay and delay variation by almost 11% and 13.5%, respectively, and outperforms the existing server-centric and network-centric models. As a byproduct, our proposed method also achieves better fairness among multiple competing players by almost 45%, on average, in comparison with existing methods.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W2521354882",
    "type": "article"
  },
  {
    "title": "Privacy Protection for Medical Data Sharing in Smart Healthcare",
    "doi": "https://doi.org/10.1145/3408322",
    "publication_date": "2020-10-31",
    "publication_year": 2020,
    "authors": "Liming Fang; Changchun Yin; Juncen Zhu; Chunpeng Ge; M. Tanveer; Alireza Jolfaei; Zehong Cao",
    "corresponding_authors": "",
    "abstract": "In virtue of advances in smart networks and the cloud computing paradigm, smart healthcare is transforming. However, there are still challenges, such as storing sensitive data in untrusted and controlled infrastructure and ensuring the secure transmission of medical data, among others. The rapid development of watermarking provides opportunities for smart healthcare. In this article, we propose a new data-sharing framework and a data access control mechanism. The applications are submitted by the doctors, and the data is processed in the medical data center of the hospital, stored in semi-trusted servers to support the selective sharing of electronic medical records from different medical institutions between different doctors. Our approach ensures that privacy concerns are taken into account when processing requests for access to patients’ medical information. For accountability, after data is modified or leaked, both patients and doctors must add digital watermarks associated with their identification when uploading data. Extensive analytical and experimental results are presented that show the security and efficiency of our proposed scheme.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W3110891612",
    "type": "article"
  },
  {
    "title": "Few-shot Food Recognition via Multi-view Representation Learning",
    "doi": "https://doi.org/10.1145/3391624",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "Shuqiang Jiang; Weiqing Min; Yongqiang Lyu; Linhu Liu",
    "corresponding_authors": "",
    "abstract": "This article considers the problem of few-shot learning for food recognition. Automatic food recognition can support various applications, e.g., dietary assessment and food journaling. Most existing works focus on food recognition with large numbers of labelled samples, and fail to recognize food categories with few samples. To address this problem, we propose a Multi-View Few-Shot Learning (MVFSL) framework to explore additional ingredient information for few-shot food recognition. Besides category-oriented deep visual features, we introduce ingredient-supervised deep network to extract ingredient-oriented features. As general and intermediate attributes of food, ingredient-oriented features are informative and complementary to category-oriented features, and thus they play an important role in improving food recognition. Particularly in few-shot food recognition, ingredient information can bridge the gap between disjoint training categories and test categories. To take advantage of ingredient information, we fuse these two kinds of features by first combining their feature maps from their respective deep networks and then convolving combined feature maps. Such convolution is further incorporated into a multi-view relation network, which is capable of comparing pairwise images to enable fine-grained feature learning. MVFSL is trained in an end-to-end fashion for joint optimization on two types of feature learning subnetworks and relation subnetworks. Extensive experiments on different food datasets have consistently demonstrated the advantage of MVFSL in multi-view feature fusion. Furthermore, we extend another two types of networks, namely, Siamese Network and Matching Network, by introducing ingredient information for few-shot food recognition. Experimental results have also shown that introducing ingredient information into these two networks can improve the performance of few-shot food recognition.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W3041350610",
    "type": "article"
  },
  {
    "title": "SDN-Assisted DDoS Defense Framework for the Internet of Multimedia Things",
    "doi": "https://doi.org/10.1145/3394956",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "Kshira Sagar Sahoo; Deepak Puthal",
    "corresponding_authors": "",
    "abstract": "The Internet of Things is visualized as a fundamental networking model that bridges the gap between the cyber and real-world entity. Uniting the real-world object with virtualization technology is opening further opportunities for innovation in nearly every individual’s life. Moreover, the usage of smart heterogeneous multimedia devices is growing extensively. These multimedia devices that communicate among each other through the Internet form a unique paradigm called the Internet of Multimedia Things (IoMT). As the volume of the collected data in multimedia application increases, the security, reliability of communications, and overall quality of service need to be maintained. Primarily, distributed denial of service attacks unveil the pervasiveness of vulnerabilities in IoMT systems. However, the Software Defined Network (SDN) is a new network architecture that has the central visibility of the entire network, which helps to detect any attack effectively. In this regard, the combination of SDN and IoMT, termed SD-IoMT , has the immense ability to improve the network management and security capabilities of the IoT system. This article proposes an SDN-assisted two-phase detection framework, namely SD-IoMT-Protector, in which the first phase utilizes the entropy technique as the detection metric to verify and alert about the malicious traffic. The second phase has trained with an optimized machine learning technique for classifying different attacks. The outcomes of the experimental results signify the usefulness and effectiveness of the proposed framework for addressing distributed denial of service issues of the SD-IoMT system.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W3041089020",
    "type": "article"
  },
  {
    "title": "Region-Level Visual Consistency Verification for Large-Scale Partial-Duplicate Image Search",
    "doi": "https://doi.org/10.1145/3383582",
    "publication_date": "2020-05-22",
    "publication_year": 2020,
    "authors": "Zhili Zhou; Q. M. Jonathan Wu; Yimin Yang; Xingming Sun",
    "corresponding_authors": "",
    "abstract": "Most recent large-scale image search approaches build on a bag-of-visual-words model, in which local features are quantized and then efficiently matched between images. However, the limited discriminability of local features and the BOW quantization errors cause a lot of mismatches between images, which limit search accuracy. To improve the accuracy, geometric verification is popularly adopted to identify geometrically consistent local matches for image search, but it is hard to directly use these matches to distinguish partial-duplicate images from non-partial-duplicate images. To address this issue, instead of simply identifying geometrically consistent matches, we propose a region-level visual consistency verification scheme to confirm whether there are visually consistent region (VCR) pairs between images for partial-duplicate search. Specifically, after the local feature matching, the potential VCRs are constructed via mapping the regions segmented from candidate images to a query image by utilizing the properties of the matched local features. Then, the compact gradient descriptor and convolutional neural network descriptor are extracted and matched between the potential VCRs to verify their visual consistency to determine whether they are VCRs. Moreover, two fast pruning algorithms are proposed to further improve efficiency. Extensive experiments demonstrate the proposed approach achieves higher accuracy than the state of the art and provide comparable efficiency for large-scale partial-duplicate search tasks.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W3031399851",
    "type": "article"
  },
  {
    "title": "Lightweight Multi-party Authentication and Key Agreement Protocol in IoT-based E-Healthcare Service",
    "doi": "https://doi.org/10.1145/3398039",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "Amiya Kumar Sahu; Suraj Sharma; Deepak Puthal",
    "corresponding_authors": "",
    "abstract": "Internet of Things (IoT) is playing a promising role in e-healthcare applications in the recent decades; nevertheless, security is one of the crucial challenges in the current field of study. Many healthcare devices (for instance, a sensor-augmented insulin pump and heart-rate sensor) collect a user’s real-time data (such as glucose level and heart rate) and send them to the cloud for proper analysis and diagnosis of the user. However, the real-time user’s data are vulnerable to various authentication attacks while sending through an insecure channel. Besides that, the attacks may further open scope for many other subsequent attacks. Existing security mechanisms concentrate on two-party mutual authentication. However, an IoT-enabled healthcare application involves multiple parties such as a patient, e-healthcare test-equipment, doctors, and cloud servers that requires multi-party authentication for secure communication. Moreover, the design and implementation of a lightweight security mechanism that fits into the resource constraint IoT-enabled healthcare devices are challenging. Therefore, this article proposes a lightweight, multi-party authentication and key-establishment protocol in IoT-based e-healthcare service access network to counter the attacks in resource constraint devices. The proposed multi-party protocol has used a lattice-based cryptographic construct such as Identity-Based Encryption (IBE) to acquire security, privacy, and efficiency. The study provided all-round analysis of the scheme, such as security, power consumption, and practical usage, in the following ways. The proposed scheme is tested by a formal security tool, Scyther, to testify the security properties of the protocol. In addition, security analysis for various attacks and comparison with other existing works are provided to show the robust security characteristics. Further, an experimental evaluation of the proposed scheme using IBE cryptographic construct is provided to validate the practical usage. The power consumption of the scheme is also computed and compared with existing works to evaluate its efficiency.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W3038950817",
    "type": "article"
  },
  {
    "title": "EGroupNet",
    "doi": "https://doi.org/10.1145/3379449",
    "publication_date": "2020-05-22",
    "publication_year": 2020,
    "authors": "Mingxing Duan; Kenli Li; Aijia Ouyang; Khin Nandar Win; Keqin Li; Qi Tian",
    "corresponding_authors": "",
    "abstract": "Although age estimation is easily affected by smiling, race, gender, and other age-related attributes, most of the researchers did not pay attention to the correlations among these attributes. Moreover, many researchers perform age estimation from a wide range of age; however, conducting an age prediction over a narrow age range may achieve better results. This article proposes a hierarchic approach referred to as EGroupNet for age prediction. The method includes two main stages, i.e., feature enhancement via excavating the correlations among age-related attributes and age estimation based on different age group schemes. First, we apply the multi-task learning model to learn multiple face attributes simultaneously to obtain discriminative features of different attributes. Second, we project the outputs of fully connected layers of several subnetworks into a highly correlated matrix space via the correlation learning process. Third, we classify these enhanced features into narrow age groups using two Extreme Learning Machine models. Finally, we make predictions based on the results of the age groups mergence. We conduct a large number of experiments on MORPH-II, LAP-2016 dataset, and Adience benchmark. The mean absolute errors of the two different settings on MORPH-II are 2.48 and 2.13 years, respectively; the normal score (ε) on the LAP-2016 dataset is 0.3578; and the accuracy of age prediction on Adience benchmark is 0.6978.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W3032589391",
    "type": "article"
  },
  {
    "title": "Zero-shot Cross-modal Retrieval by Assembling AutoEncoder and Generative Adversarial Network",
    "doi": "https://doi.org/10.1145/3424341",
    "publication_date": "2021-01-31",
    "publication_year": 2021,
    "authors": "Xing Xu; Jialin Tian; Kaiyi Lin; Huimin Lu; Jie Shao; Heng Tao Shen",
    "corresponding_authors": "",
    "abstract": "Conventional cross-modal retrieval models mainly assume the same scope of the classes for both the training set and the testing set. This assumption limits their extensibility on zero-shot cross-modal retrieval (ZS-CMR), where the testing set consists of unseen classes that are disjoint with seen classes in the training set. The ZS-CMR task is more challenging due to the heterogeneous distributions of different modalities and the semantic inconsistency between seen and unseen classes. A few of recently proposed approaches are inspired by zero-shot learning to estimate the distribution underlying multimodal data by generative models and make the knowledge transfer from seen classes to unseen classes by leveraging class embeddings. However, directly borrowing the idea from zero-shot learning (ZSL) is not fully adaptive to the retrieval task, since the core of the retrieval task is learning the common space. To address the above issues, we propose a novel approach named Assembling AutoEncoder and Generative Adversarial Network (AAEGAN), which combines the strength of AutoEncoder (AE) and Generative Adversarial Network (GAN), to jointly incorporate common latent space learning, knowledge transfer, and feature synthesis for ZS-CMR. Besides, instead of utilizing class embeddings as common space, the AAEGAN approach maps all multimodal data into a learned latent space with the distribution alignment via three coupled AEs. We empirically show the remarkable improvement for ZS-CMR task and establish the state-of-the-art or competitive performance on four image-text retrieval datasets.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W3142108776",
    "type": "article"
  },
  {
    "title": "Attribute-wise Explainable Fashion Compatibility Modeling",
    "doi": "https://doi.org/10.1145/3425636",
    "publication_date": "2021-02-28",
    "publication_year": 2021,
    "authors": "Xin Yang; Xuemeng Song; Fuli Feng; Haokun Wen; Ling‐Yu Duan; Liqiang Nie",
    "corresponding_authors": "",
    "abstract": "With the boom of the fashion market and people’s daily needs for beauty, clothing matching has gained increased research attention. In a sense, tackling this problem lies in modeling the human notions of the compatibility between fashion items, i.e., Fashion Compatibility Modeling (FCM), which plays an important role in a wide bunch of commercial applications, including clothing recommendation and dressing assistant. Recent advances in multimedia processing have shown remarkable effectiveness in accurate compatibility evaluation. However, these studies work like a black box and cannot provide appropriate explanations, which are indeed of importance for gaining users’ trust and improving their experience. In fact, fashion experts usually explain the compatibility evaluation through the matching patterns between fashion attributes (e.g., a silk tank top cannot go with a knit dress). Inspired by this, we devise an attribute-wise explainable FCM solution, named ExFCM , which can simultaneously generate the item-level compatibility evaluation for input fashion items and the attribute-level explanations for the evaluation result. In particular, ExFCM consists of two key components: attribute-wise representation learning and attribute interaction modeling. The former works on learning the region-aware attribute representation for each item with the threshold global average pooling. Besides, the latter is responsible for compiling the attribute-level matching signals into the overall compatibility evaluation adaptively with the attentive interaction mechanism. Note that ExFCM is trained without any attribute-level compatibility annotations, which facilitates its practical applications. Extensive experiments on two real-world datasets validate that ExFCM can generate more accurate compatibility evaluations than the existing methods, together with reasonable explanations.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W3154400137",
    "type": "article"
  },
  {
    "title": "A Densely Connected Network Based on U-Net for Medical Image Segmentation",
    "doi": "https://doi.org/10.1145/3446618",
    "publication_date": "2021-07-22",
    "publication_year": 2021,
    "authors": "Zhenzhen Yang; Pengfei Xu; Yongpeng Yang; Bing‐Kun Bao",
    "corresponding_authors": "",
    "abstract": "The U-Net has become the most popular structure in medical image segmentation in recent years. Although its performance for medical image segmentation is outstanding, a large number of experiments demonstrate that the classical U-Net network architecture seems to be insufficient when the size of segmentation targets changes and the imbalance happens between target and background in different forms of segmentation. To improve the U-Net network architecture, we develop a new architecture named densely connected U-Net (DenseUNet) network in this article. The proposed DenseUNet network adopts a dense block to improve the feature extraction capability and employs a multi-feature fuse block fusing feature maps of different levels to increase the accuracy of feature extraction. In addition, in view of the advantages of the cross entropy and the dice loss functions, a new loss function for the DenseUNet network is proposed to deal with the imbalance between target and background. Finally, we test the proposed DenseUNet network and compared it with the multi-resolutional U-Net (MultiResUNet) and the classic U-Net networks on three different datasets. The experimental results show that the DenseUNet network has significantly performances compared with the MultiResUNet and the classic U-Net networks.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W3185105934",
    "type": "article"
  },
  {
    "title": "Bi-Directional Co-Attention Network for Image Captioning",
    "doi": "https://doi.org/10.1145/3460474",
    "publication_date": "2021-11-12",
    "publication_year": 2021,
    "authors": "Weitao Jiang; Weixuan Wang; Haifeng Hu",
    "corresponding_authors": "",
    "abstract": "Image Captioning, which automatically describes an image with natural language, is regarded as a fundamental challenge in computer vision. In recent years, significant advance has been made in image captioning through improving attention mechanism. However, most existing methods construct attention mechanisms based on singular visual features, such as patch features or object features, which limits the accuracy of generated captions. In this article, we propose a Bidirectional Co-Attention Network (BCAN) that combines multiple visual features to provide information from different aspects. Different features are associated with predicting different words, and there are a priori relations between these multiple visual features. Based on this, we further propose a bottom-up and top-down bi-directional co-attention mechanism to extract discriminative attention information. Furthermore, most existing methods do not exploit an effective multimodal integration strategy, generally using addition or concatenation to combine features. To solve this problem, we adopt the Multivariate Residual Module (MRM) to integrate multimodal attention features. Meanwhile, we further propose a Vertical MRM to integrate features of the same category, and a Horizontal MRM to combine features of the different categories, which can balance the contribution of the bottom-up co-attention and the top-down co-attention. In contrast to the existing methods, the BCAN is able to obtain complementary information from multiple visual features via the bi-directional co-attention strategy, and integrate multimodal information via the improved multivariate residual strategy. We conduct a series of experiments on two benchmark datasets (MSCOCO and Flickr30k), and the results indicate that the proposed BCAN achieves the superior performance.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W3211865849",
    "type": "article"
  },
  {
    "title": "ART-UP: A Novel Method for Generating Scanning-Robust Aesthetic QR Codes",
    "doi": "https://doi.org/10.1145/3418214",
    "publication_date": "2021-02-28",
    "publication_year": 2021,
    "authors": "Mingliang Xu; Qingfeng Li; Jianwei Niu; Hao Su; Xiting Liu; Weiwei Xu; Pei Lv; Bing Zhou; Yi Yang",
    "corresponding_authors": "",
    "abstract": "Quick response (QR) codes are usually scanned in different environments, so they must be robust to variations in illumination, scale, coverage, and camera angles. Aesthetic QR codes improve the visual quality, but subtle changes in their appearance may cause scanning failure. In this article, a new method to generate scanning-robust aesthetic QR codes is proposed, which is based on a module-based scanning probability estimation model that can effectively balance the tradeoff between visual quality and scanning robustness. Our method locally adjusts the luminance of each module by estimating the probability of successful sampling. The approach adopts the hierarchical, coarse-to-fine strategy to enhance the visual quality of aesthetic QR codes, which sequentially generate the following three codes: a binary aesthetic QR code, a grayscale aesthetic QR code, and the final color aesthetic QR code. Our approach also can be used to create QR codes with different visual styles by adjusting some initialization parameters. User surveys and decoding experiments were adopted for evaluating our method compared with state-of-the-art algorithms, which indicates that the proposed approach has excellent performance in terms of both visual quality and scanning robustness.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W3156950846",
    "type": "article"
  },
  {
    "title": "Doctor's Dilemma: Evaluating an Explainable Subtractive Spatial Lightweight Convolutional Neural Network for Brain Tumor Diagnosis",
    "doi": "https://doi.org/10.1145/3457187",
    "publication_date": "2021-10-26",
    "publication_year": 2021,
    "authors": "Ambeshwar Kumar; R. Manikandan; Utku Köse; Deepak Gupta; Suresh Chandra Satapathy",
    "corresponding_authors": "",
    "abstract": "In Medicine Deep Learning has become an essential tool to achieve outstanding diagnosis on image data. However, one critical problem is that Deep Learning comes with complicated, black-box models so it is not possible to analyze their trust level directly. So, Explainable Artificial Intelligence (XAI) methods are used to build additional interfaces for explaining how the model has reached the outputs by moving from the input data. Of course, that's again another competitive problem to analyze if such methods are successful according to the human view. So, this paper comes with two important research efforts: (1) to build an explainable deep learning model targeting medical image analysis, and (2) to evaluate the trust level of this model via several evaluation works including human contribution. The target problem was selected as the brain tumor classification, which is a remarkable, competitive medical image-based problem for Deep Learning. In the study, MR-based pre-processed brain images were received by the Subtractive Spatial Lightweight Convolutional Neural Network (SSLW-CNN) model, which includes additional operators to reduce the complexity of classification. In order to ensure the explainable background, the model also included Class Activation Mapping (CAM). It is important to evaluate the trust level of a successful model. So, numerical success rates of the SSLW-CNN were evaluated based on the peak signal-to-noise ratio (PSNR), computational time, computational overhead, and brain tumor classification accuracy. The objective of the proposed SSLW-CNN model is to obtain faster and good tumor classification with lesser time. The results illustrate that the SSLW-CNN model provides better performance of PSNR which is enhanced by 8%, classification accuracy is improved by 33%, computation time is reduced by 19%, computation overhead is decreased by 23%, and classification time is minimized by 13%, as compared to state-of-the-art works. Because the model provided good numerical results, it was then evaluated in terms of XAI perspective by including doctor-model based evaluations such as feedback CAM visualizations, usability, expert surveys, comparisons of CAM with other XAI methods, and manual diagnosis comparison. The results show that the SSLW-CNN provides good performance on brain tumor diagnosis and ensures a trustworthy solution for the doctors.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W3211024110",
    "type": "article"
  },
  {
    "title": "Revisiting Local Descriptor for Improved Few-Shot Classification",
    "doi": "https://doi.org/10.1145/3511917",
    "publication_date": "2022-02-18",
    "publication_year": 2022,
    "authors": "Jun He; Richang Hong; Xueliang Liu; Mingliang Xu; Qianru Sun",
    "corresponding_authors": "",
    "abstract": "Few-shot classification studies the problem of quickly adapting a deep learner to understanding novel classes based on few support images. In this context, recent research efforts have been aimed at designing more and more complex classifiers that measure similarities between query and support images but left the importance of feature embeddings seldom explored. We show that the reliance on sophisticated classifiers is not necessary, and a simple classifier applied directly to improved feature embeddings can instead outperform most of the leading methods in the literature. To this end, we present a new method, named DCAP, for few-shot classification, in which we investigate how one can improve the quality of embeddings by leveraging Dense Classification and Attentive Pooling (DCAP) . Specifically, we propose to train a learner on base classes with abundant samples to solve dense classification problem first and then meta-train the learner on plenty of randomly sampled few-shot tasks to adapt it to few-shot scenario or the test time scenario. During meta-training, we suggest to pool feature maps by applying attentive pooling instead of the widely used global average pooling to prepare embeddings for few-shot classification. Attentive pooling learns to reweight local descriptors, explaining what the learner is looking for as evidence for decision making. Experiments on two benchmark datasets show the proposed method to be superior in multiple few-shot settings while being simpler and more explainable. Code is publicly available at https://github.com/Ukeyboard/dcap/ .",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W4212932569",
    "type": "article"
  },
  {
    "title": "Detection of AI-Manipulated Fake Faces via Mining Generalized Features",
    "doi": "https://doi.org/10.1145/3499026",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Yang Yu; Rongrong Ni; Wenjie Li; Yao Zhao",
    "corresponding_authors": "",
    "abstract": "Recently, AI-manipulated face techniques have developed rapidly and constantly, which has raised new security issues in society. Although existing detection methods consider different categories of fake faces, the performance on detecting the fake faces with “unseen” manipulation techniques is still poor due to the distribution bias among cross-manipulation techniques. To solve this problem, we propose a novel framework that focuses on mining intrinsic features and further eliminating the distribution bias to improve the generalization ability. First, we focus on mining the intrinsic clues in the channel difference image (CDI) and spectrum image (SI) view of two different aspects, including the camera imaging process and the indispensable step in AI manipulation process. Then, we introduce the Octave Convolution and an attention-based fusion module to effectively and adaptively mine intrinsic features from CDI and SI view of these two different but intrinsic aspects. Finally, we design an alignment module to eliminate the bias of manipulation techniques to obtain a more generalized detection framework. We evaluate the proposed framework on four categories of fake faces datasets with the most popular and state-of-the-art manipulation techniques and achieve very competitive performances. We further conduct experiments on cross-manipulation techniques, and the results of our method show the superior advantages on improving generalization ability.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W4214809420",
    "type": "article"
  },
  {
    "title": "3D Tooth Instance Segmentation Learning Objectness and Affinity in Point Cloud",
    "doi": "https://doi.org/10.1145/3504033",
    "publication_date": "2022-03-26",
    "publication_year": 2022,
    "authors": "Yan Tian; Yujie Zhang; Wei‐Gang Chen; Dongsheng Liu; Huiyan Wang; Huayi Xu; Jianfeng Han; Yiwen Ge",
    "corresponding_authors": "",
    "abstract": "Digital dentistry has received more attention in the past decade. However, current deep learning-based methods still encounter difficult challenges. The proposal-based methods are sensitive to the localization results due to the lack of local cues, while the proposal-free methods have poor clustering outputs because of the affinity measured by the low-level characteristics, especially in situations of tightly arranged teeth. In this article, we present a novel proposal-based approach to combine objectness and pointwise knowledge in an attention mechanism for point cloud-based tooth instance segmentation, using local information to improve 3D proposal generation and measuring the importance of local points by calculating the center distance. We evaluate the performance of our approach by constructing a Shining3D tooth instance segmentation dataset. The experimental results verify that our approach gives competitive results when compared with the other available approaches.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W4220953242",
    "type": "article"
  },
  {
    "title": "Fine-grained Image Classification via Multi-scale Selective Hierarchical Biquadratic Pooling",
    "doi": "https://doi.org/10.1145/3492221",
    "publication_date": "2022-01-25",
    "publication_year": 2022,
    "authors": "Min Tan; Yuan Fu; Jun Yu; Guijun Wang; Xiaoling Gu",
    "corresponding_authors": "",
    "abstract": "How to extract distinctive features greatly challenges the fine-grained image classification tasks. In previous models, bilinear pooling has been frequently adopted to address this problem. However, most bilinear pooling models neglect either intra or inter layer feature interaction. This insufficient interaction brings in the loss of discriminative information. In this article, we devise a novel fine-grained image classification approach named M ulti-scale S elective H ierarchical bi Q uadratic P ooling (MSHQP). The proposed biquadratic pooling simultaneously models intra and inter layer feature interactions and enhances part response by integrating multi-layer features. The subsequent coarse-to-fine multi-scale interaction structure captures the complementary information within features. Finally, the active interaction selection module adaptively learns the optimal interaction subset for a specific dataset. Consequently, we obtain a robust image representation with coarse-to-fine semantics. We conduct experiments on five benchmark datasets. The experimental results demonstrate that MSHQP achieves competitive or even match the state-of-the-art methods in terms of both accuracy and computational efficiency, with 89.0%, 94.9%, 93.4%, 90.4%, and 91.5% top-1 classification accuracy on CUB-200-2011, Stanford-Cars, FGVC-Aircraft, Stanford-Dog, and VegFru, respectively.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W4207054665",
    "type": "article"
  },
  {
    "title": "Exploring Relations in Untrimmed Videos for Self-Supervised Learning",
    "doi": "https://doi.org/10.1145/3473342",
    "publication_date": "2022-01-25",
    "publication_year": 2022,
    "authors": "Dezhao Luo; Yu Zhou; Bo Fang; Yucan Zhou; Dayan Wu; Weiping Wang",
    "corresponding_authors": "",
    "abstract": "Existing video self-supervised learning methods mainly rely on trimmed videos for model training. They apply their methods and verify the effectiveness on trimmed video datasets including UCF101 and Kinetics-400, among others. However, trimmed datasets are manually annotated from untrimmed videos. In this sense, these methods are not truly unsupervised. In this article, we propose a novel self-supervised method, referred to as Exploring Relations in Untrimmed Videos (ERUV), which can be straightforwardly applied to untrimmed videos (real unlabeled) to learn spatio-temporal features. ERUV first generates single-shot videos by shot change detection. After that, some designed sampling strategies are used to model relations for video clips. The strategies are saved as our self-supervision signals. Finally, the network learns representations by predicting the category of relations between the video clips. ERUV is able to compare the differences and similarities of video clips, which is also an essential procedure for video-related tasks. We validate our learned models with action recognition, video retrieval, and action similarity labeling tasks with four kinds of 3D convolutional neural networks. Experimental results show that ERUV is able to learn richer representations with untrimmed videos, and it outperforms state-of-the-art self-supervised methods with significant margins.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W3047573812",
    "type": "article"
  },
  {
    "title": "A Novel GAPG Approach to Automatic Property Generation for Formal Verification: The GAN Perspective",
    "doi": "https://doi.org/10.1145/3517154",
    "publication_date": "2022-02-18",
    "publication_year": 2022,
    "authors": "Honghao Gao; Baobin Dai; Huaikou Miao; Xiaoxian Yang; Ramón J. Durán Barroso; Walayat Hussain",
    "corresponding_authors": "",
    "abstract": "Formal methods have been widely used to support software testing to guarantee correctness and reliability. For example, model checking technology attempts to ensure that the verification property of a specific formal model is satisfactory for discovering bugs or abnormal behavior from the perspective of temporal logic. However, because automatic approaches are lacking, a software developer/tester must manually specify verification properties. A generative adversarial network (GAN) learns features from input training data and outputs new data with similar or coincident features. GANs have been successfully used in the image processing and text processing fields and achieved interesting and automatic results. Inspired by the power of GANs, in this article, we propose a GAN-based automatic property generation (GAPG) approach to generate verification properties supporting model checking. First, the verification properties in the form of computational tree logic (CTL) are encoded and used as input to the GAN. Second, we introduce regular expressions as grammar rules to check the correctness of the generated properties. These rules work to detect and filter meaningless properties that occur because the GAN learning process is uncontrollable and may generate unsuitable properties in real applications. Third, the learning network is further trained by using labeled information associated with the input properties. These are intended to guide the training process to generate additional new properties, particularly those that map to corresponding formal models. Finally, a series of comprehensive experiments demonstrate that the proposed GAPG method can obtain new verification properties from two aspects: (1) using only CTL formulas and (2) using CTL formulas combined with Kripke structures.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W4213195950",
    "type": "article"
  },
  {
    "title": "Image Quality Assessment–driven Reinforcement Learning for Mixed Distorted Image Restoration",
    "doi": "https://doi.org/10.1145/3532625",
    "publication_date": "2022-04-29",
    "publication_year": 2022,
    "authors": "Xiaoyu Zhang; Wei Gao; Ge Li; Qiuping Jiang; Runmin Cong",
    "corresponding_authors": "",
    "abstract": "Due to the diversity of the degradation process that is difficult to model, the recovery of mixed distorted images is still a challenging problem. The deep learning model trained under certain degradation declines significantly in other degradation situations. In this article, we explore ways to use a combination of tools to deal with the mixed distortion. First, we illustrate the limitations of a single deep network in dealing with multiple distortion types and then introduce a hierarchical toolkit with distinguished powerful tools. Second, we investigate how an efficient representation of images combined with a reinforcement learning (RL) paradigm helps to deal with tool noise in continuous restoration. The proposed method can accurately capture the distortion preferences for selecting the optimal recovery tools by RL agent. Finally, to fully utilize random tools for unknown distortion combinations, we adopt the exploration scheme with various quality evaluation methods to achieve more quality improvements. Experimental results demonstrate that the peak signal-to-noise ratio of the proposed method is 3.30 dB higher than other state-of-the-art RL-based methods on the CSIQ single distortion dataset and 0.95 dB higher on the DIV2K mixed distortion dataset.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W4225150336",
    "type": "article"
  },
  {
    "title": "Retrieval Augmented Convolutional Encoder-decoder Networks for Video Captioning",
    "doi": "https://doi.org/10.1145/3539225",
    "publication_date": "2022-05-26",
    "publication_year": 2022,
    "authors": "Jingwen Chen; Yingwei Pan; Yehao Li; Ting Yao; Hongyang Chao; Tao Mei",
    "corresponding_authors": "",
    "abstract": "Video captioning has been an emerging research topic in computer vision, which aims to generate a natural sentence to correctly reflect the visual content of a video. The well-established way of doing so is to rely on encoder-decoder paradigm by learning to encode the input video and decode the variable-length output sentence in a sequence-to-sequence manner. Nevertheless, these approaches often fail to produce complex and descriptive sentences as natural as those from human being, since the models are incapable of memorizing all visual contents and syntactic structures in the human-annotated video-sentence pairs. In this article, we uniquely introduce a Retrieval Augmentation Mechanism (RAM) that enables the explicit reference to existing video-sentence pairs within any encoder-decoder captioning model. Specifically, for each query video, a video-sentence retrieval model is first utilized to fetch semantically relevant sentences from the training sentence pool, coupled with the corresponding training videos. RAM then writes the relevant video-sentence pairs into memory and reads the memorized visual contents/syntactic structures in video-sentence pairs from memory to facilitate the word prediction at each timestep. Furthermore, we present Retrieval Augmented Convolutional Encoder-Decoder Network (R-ConvED), which novelly integrates RAM into convolutional encoder-decoder structure to boost video captioning. Extensive experiments on MSVD, MSR-VTT, Activity Net Captions, and VATEX datasets validate the superiority of our proposals and demonstrate quantitatively compelling results.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W4281560470",
    "type": "article"
  },
  {
    "title": "Learning the User’s Deeper Preferences for Multi-modal Recommendation Systems",
    "doi": "https://doi.org/10.1145/3573010",
    "publication_date": "2022-12-07",
    "publication_year": 2022,
    "authors": "Fei Lei; Zhongqi Cao; Yuning Yang; Yibo Ding; Cong Zhang",
    "corresponding_authors": "",
    "abstract": "Recommendation system plays an important role in the rapid development of micro-video sharing platform. Micro-video has rich modal features, such as visual, audio, and text. It is of great significance to carry out personalized recommendation by integrating multi-modal features. However, most of the current multi-modal recommendation systems can only enrich the feature representation on the item side, while it leads to poor learning of user preferences. To solve this problem, we propose a novel module named Learning the User’s Deeper Preferences (LUDP) , which constructs the item-item modal similarity graph and user preference graph in each modality to explore the learning of item and user representation. Specifically, we construct item-item similar modalities graph using multi-modal features, the item ID embedding is propagated and aggregated on the graph to learn the latent structural information of items; The user preference graph is constructed through the historical interaction between the user and item, on which the multi-modal features are aggregated as the user’s preference for the modal. Finally, combining the two parts as auxiliary information enhances the user and item representation learned from the collaborative signals to learn deeper user preferences. Through a large number of experiments on two public datasets (TikTok, Movielens), our model is proved to be superior to the most advanced multi-modal recommendation methods.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W4311808565",
    "type": "article"
  },
  {
    "title": "FasterPose: A Faster Simple Baseline for Human Pose Estimation",
    "doi": "https://doi.org/10.1145/3503464",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Hanbin Dai; Hailin Shi; Wu Liu; Linfang Wang; Yinglu Liu; Tao Mei",
    "corresponding_authors": "",
    "abstract": "The performance of human pose estimation depends on the spatial accuracy of keypoint localization. Most existing methods pursue the spatial accuracy through learning the high-resolution (HR) representation from input images. By the experimental analysis, we find that the HR representation leads to a sharp increase of computational cost, while the accuracy improvement remains marginal compared with the low-resolution (LR) representation. In this article, we propose a design paradigm for cost-effective network with LR representation for efficient pose estimation, named FasterPose. Whereas the LR design largely shrinks the model complexity, how to effectively train the network with respect to the spatial accuracy is a concomitant challenge. We study the training behavior of FasterPose and formulate a novel regressive cross-entropy (RCE) loss function for accelerating the convergence and promoting the accuracy. The RCE loss generalizes the ordinary cross-entropy loss from the binary supervision to a continuous range, thus the training of pose estimation network is able to benefit from the sigmoid function. By doing so, the output heatmap can be inferred from the LR features without loss of spatial accuracy, while the computational cost and model size has been significantly reduced. Compared with the previously dominant network of pose estimation, our method reduces 58% of the FLOPs and simultaneously gains 1.3% improvement of accuracy. Extensive experiments show that FasterPose yields promising results on the common benchmarks, i.e., COCO and MPII, consistently validating the effectiveness and efficiency for practical utilization, especially the low-latency and low-energy-budget applications in the non-GPU scenarios.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W3182545801",
    "type": "article"
  },
  {
    "title": "Exploiting Attention-Consistency Loss For Spatial-Temporal Stream Action Recognition",
    "doi": "https://doi.org/10.1145/3538749",
    "publication_date": "2022-05-28",
    "publication_year": 2022,
    "authors": "Haotian Xu; Xiaobo Jin; Qiufeng Wang; Amir Hussain; Kaizhu Huang",
    "corresponding_authors": "",
    "abstract": "Currently, many action recognition methods mostly consider the information from spatial streams. We propose a new perspective inspired by the human visual system to combine both spatial and temporal streams to measure their attention consistency. Specifically, a branch-independent convolutional neural network (CNN) based algorithm is developed with a novel attention-consistency loss metric, enabling the temporal stream to concentrate on consistent discriminative regions with the spatial stream in the same period. The consistency loss is further combined with the cross-entropy loss to enhance the visual attention consistency. We evaluate the proposed method for action recognition on two benchmark datasets: Kinetics400 and UCF101. Despite its apparent simplicity, our proposed framework with the attention consistency achieves better performance than most of the two-stream networks, i.e., 75.7% top-1 accuracy on Kinetics400 and 95.7% on UCF101, while reducing 7.1% computational cost compared with our baseline. Particularly, our proposed method can attain remarkable improvements on complex action classes, showing that our proposed network can act as a potential benchmark to handle complicated scenarios in industry 4.0 applications.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W4281749424",
    "type": "article"
  },
  {
    "title": "Deep Semantic and Attentive Network for Unsupervised Video Summarization",
    "doi": "https://doi.org/10.1145/3477538",
    "publication_date": "2022-02-16",
    "publication_year": 2022,
    "authors": "Sheng-hua Zhong; Jingxu Lin; Jianglin Lu; Ahmed Fares; Tongwei Ren",
    "corresponding_authors": "",
    "abstract": "With the rapid growth of video data, video summarization is a promising approach to shorten a lengthy video into a compact version. Although supervised summarization approaches have achieved state-of-the-art performance, they require frame-level annotated labels. Such an annotation process is time-consuming and tedious. In this article, we propose a novel deep summarization framework named Deep Semantic and Attentive Network for Video Summarization (DSAVS) that can select the most semantically representative summary by minimizing the distance between video representation and text representation without any frame-level labels. Another challenge associated with video summarization tasks mainly originates from the difficulty of considering temporal information over a long time. Long Short-Term Memory (LSTM) performs well for temporal dependencies modeling but does not work well with long video clips. Therefore, we introduce a self-attention mechanism into our summarization framework to capture the long-range temporal dependencies among the frames. Extensive experiments on two popular benchmark datasets, i.e., SumMe and TVSum, show that our proposed framework outperforms other state-of-the-art unsupervised approaches and even most supervised methods.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W4283399854",
    "type": "article"
  },
  {
    "title": "RD-IOD: Two-Level Residual-Distillation-Based Triple-Network for Incremental Object Detection",
    "doi": "https://doi.org/10.1145/3472393",
    "publication_date": "2022-01-27",
    "publication_year": 2022,
    "authors": "Dongbao Yang; Yu Zhou; Wei Shi; Dayan Wu; Weiping Wang",
    "corresponding_authors": "",
    "abstract": "As a basic component in multimedia applications, object detectors are generally trained on a fixed set of classes that are pre-defined. However, new object classes often emerge after the models are trained in practice. Modern object detectors based on Convolutional Neural Networks (CNN) suffer from catastrophic forgetting when fine-tuning on new classes without the original training data. Therefore, it is critical to improve the incremental learning capability on object detection. In this article, we propose a novel Residual-Distillation-based Incremental learning method on Object Detection (RD-IOD). Our approach rests on the creation of a triple-network based on Faster R-CNN. To enable continuous learning from new classes, we use the original model as well as a residual model to guide the learning of the incremental model on new classes while maintaining the previous learned knowledge. To better maintain the discrimination between the features of old and new classes, the residual model is jointly trained with the incremental model on new classes in the incremental learning procedure. In addition, a two-level distillation scheme is designed to guide the training process, which consists of (1) a general distillation for imitating the original model in feature space along with a residual distillation on the features in both image level and instance level, and (2) a joint classification distillation on the output layers. To well preserve the learned knowledge, we design a 2-threshold training strategy to guide the learning of a Region Proposal Network and a detection head. Extensive experiments conducted on VOC2007 and COCO demonstrate that the proposed method can effectively learn to incrementally detect objects of new classes, and the problem of catastrophic forgetting is mitigated. Our code is available at https://github.com/yangdb/RD-IOD.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W4210768747",
    "type": "article"
  },
  {
    "title": "A Novel Multi-Sample Generation Method for Adversarial Attacks",
    "doi": "https://doi.org/10.1145/3506852",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Mingxing Duan; Kenli Li; Jiayan Deng; Bin Xiao; Qi Tian",
    "corresponding_authors": "",
    "abstract": "Deep learning models are widely used in daily life, which bring great convenience to our lives, but they are vulnerable to attacks. How to build an attack system with strong generalization ability to test the robustness of deep learning systems is a hot issue in current research, among which the research on black-box attacks is extremely challenging. Most current research on black-box attacks assumes that the input dataset is known. However, in fact, it is difficult for us to obtain detailed information for those datasets. In order to solve the above challenges, we propose a multi-sample generation model for black-box model attacks, called MsGM. MsGM is mainly composed of three parts: multi-sample generation, substitute model training, and adversarial sample generation and attack. Firstly, we design a multi-task generation model to learn the distribution of the original dataset. The model first converts an arbitrary signal of a certain distribution into the shared features of the original dataset through deconvolution operations, and then according to different input conditions, multiple identical sub-networks generate the corresponding targeted samples. Secondly, the generated sample features achieve different outputs through querying the black-box model and training the substitute model, which are used to construct different loss functions to optimize and update the generator and substitute model. Finally, some common white-box attack methods are used to attack the substitute model to generate corresponding adversarial samples, which are utilized to attack the black-box model. We conducted a large number of experiments on the MNIST and CIFAR-10 datasets. The experimental results show that under the same settings and attack algorithms, MsGM achieves better performance than the based models.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W4214893179",
    "type": "article"
  },
  {
    "title": "Optimizing Performance of Federated Person Re-identification: Benchmarking and Analysis",
    "doi": "https://doi.org/10.1145/3531013",
    "publication_date": "2022-04-20",
    "publication_year": 2022,
    "authors": "Weiming Zhuang; Xin Gan; Yonggang Wen; Shuai Zhang",
    "corresponding_authors": "",
    "abstract": "The increasingly stringent data privacy regulations limit the development of person re-identification (ReID) because person ReID training requires centralizing an enormous amount of data that contains sensitive personal information. To address this problem, we introduce federated person re-identification (FedReID) -- implementing federated learning, an emerging distributed training method, to person ReID. FedReID preserves data privacy by aggregating model updates, instead of raw data, from clients to a central server. Furthermore, we optimize the performance of FedReID under statistical heterogeneity via benchmark analysis. We first construct a benchmark with an enhanced algorithm, two architectures, and nine person ReID datasets with large variances to simulate the real-world statistical heterogeneity. The benchmark results present insights and bottlenecks of FedReID under statistical heterogeneity, including challenges in convergence and poor performance on datasets with large volumes. Based on these insights, we propose three optimization approaches: (1) We adopt knowledge distillation to facilitate the convergence of FedReID by better transferring knowledge from clients to the server; (2) We introduce client clustering to improve the performance of large datasets by aggregating clients with similar data distributions; (3) We propose cosine distance weight to elevate performance by dynamically updating the weights for aggregation depending on how well models are trained in clients. Extensive experiments demonstrate that these approaches achieve satisfying convergence with much better performance on all datasets. We believe that FedReID will shed light on implementing and optimizing federated learning on more computer vision applications.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W4224224340",
    "type": "article"
  },
  {
    "title": "Dynamic Message Propagation Network for RGB-D and Video Salient Object Detection",
    "doi": "https://doi.org/10.1145/3597612",
    "publication_date": "2023-05-19",
    "publication_year": 2023,
    "authors": "Baian Chen; Zhilei Chen; Xiaowei Hu; Jun Xu; Haoran Xie; Jing Qin; Mingqiang Wei",
    "corresponding_authors": "",
    "abstract": "Exploiting long-range semantic contexts and geometric information is crucial to infer salient objects from RGB and depth features. However, existing methods mainly focus on excavating local features within fixed regions by continuously feeding forward networks. In this article, we introduce Dynamic Message Propagation (DMP) to dynamically learn context information within more flexible regions. We integrate DMP into a Siamese-based network to process the RGB image and depth map separately and design a multi-level feature fusion module to explore cross-level information between refined RGB and depth features. Extensive experiments show clear improvements of our method over 17 methods on six benchmark datasets for RGB-D salient object detection (SOD). Additionally, our method outperforms its competitors for the video SOD task. Code is available at https://github.com/chenbaian-cs/DMPNet .",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W4377096805",
    "type": "article"
  },
  {
    "title": "Toward Effective Semi-supervised Node Classification with Hybrid Curriculum Pseudo-labeling",
    "doi": "https://doi.org/10.1145/3626528",
    "publication_date": "2023-10-04",
    "publication_year": 2023,
    "authors": "Xiao Luo; Wei Ju; Yiyang Gu; Yifang Qin; Siyu Yi; Daqing Wu; Luchen Liu; Ming Zhang",
    "corresponding_authors": "",
    "abstract": "Semi-supervised node classification is a crucial challenge in relational data mining and has attracted increasing interest in research on graph neural networks (GNNs). However, previous approaches merely utilize labeled nodes to supervise the overall optimization, but fail to sufficiently explore the information of their underlying label distribution. Even worse, they often overlook the robustness of models, which may cause instability of network outputs to random perturbations. To address the aforementioned shortcomings, we develop a novel framework termed Hybrid Curriculum Pseudo-Labeling (HCPL) for efficient semi-supervised node classification. Technically, HCPL iteratively annotates unlabeled nodes by training a GNN model on the labeled samples and any previously pseudo-labeled samples, and repeatedly conducts this process. To improve the model robustness, we introduce a hybrid pseudo-labeling strategy that incorporates both prediction confidence and uncertainty under random perturbations, therefore mitigating the influence of erroneous pseudo-labels. Finally, we leverage the idea of curriculum learning to start from annotating easy samples, and gradually explore hard samples as the iteration grows. Extensive experiments on a number of benchmarks demonstrate that our HCPL beats various state-of-the-art baselines in diverse settings.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W4387343643",
    "type": "article"
  },
  {
    "title": "Subjective and Objective Quality Assessment for in-the-Wild Computer Graphics Images",
    "doi": "https://doi.org/10.1145/3631357",
    "publication_date": "2023-11-02",
    "publication_year": 2023,
    "authors": "Zicheng Zhang; Wei Sun; Yingjie Zhou; Jun Jia; Zhichao Zhang; Jing Liu; Xiongkuo Min; Guangtao Zhai",
    "corresponding_authors": "",
    "abstract": "Computer graphics images (CGIs) are artificially generated by means of computer programs and are widely perceived under various scenarios, such as games, streaming media, etc. In practice, the quality of CGIs consistently suffers from poor rendering during production, inevitable compression artifacts during the transmission of multimedia applications, and low aesthetic quality resulting from poor composition and design. However, few works have been dedicated to dealing with the challenge of computer graphics image quality assessment (CGIQA). Most image quality assessment (IQA) metrics are developed for natural scene images (NSIs) and validated on databases consisting of NSIs with synthetic distortions, which are not suitable for in-the-wild CGIs. To bridge the gap between evaluating the quality of NSIs and CGIs, we construct a large-scale in-the-wild CGIQA database consisting of 6,000 CGIs (CGIQA-6k) and carry out the subjective experiment in a well-controlled laboratory environment to obtain the accurate perceptual ratings of the CGIs. Then, we propose an effective deep learning–based no-reference (NR) IQA model by utilizing both distortion and aesthetic quality representation. Experimental results show that the proposed method outperforms all other state-of-the-art NR IQA methods on the constructed CGIQA-6k database and other CGIQA-related databases. The database is released at https://github.com/zzc-1998/CGIQA6K .",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W4388210343",
    "type": "article"
  },
  {
    "title": "Supervised Hierarchical Online Hashing for Cross-modal Retrieval",
    "doi": "https://doi.org/10.1145/3632527",
    "publication_date": "2023-11-13",
    "publication_year": 2023,
    "authors": "Kai Han; Yu Liu; Rukai Wei; Ke Zhou; Jinhui Xu; Kun Long",
    "corresponding_authors": "",
    "abstract": "Online cross-modal hashing has gained attention for its adaptability in processing streaming data. However, existing methods only define the hard similarity between data using labels. This results in poor retrieval performance, as they fail to exploit the semantic structure information of labels and miss the high-quality hash codes guided by the hierarchical relevance between labels. In addition, they ignore the bit-flipping problem, which leads to sub-optimal cross-modal retrieval performance. To address these issues, we propose Supervised Hierarchical Online Hashing (SHOH) for cross-modal retrieval. Our approach acquires hierarchical similarity via cross-layer affiliation of labels and explores its application to online hashing. We design a hierarchical similarity learning method in the online learning framework, which includes virtual center learning and hierarchical similarity embedding. Labels with soft similarity bridge the label hierarchy and cross-modal hash embedding. Furthermore, we propose a Weighted Retrieval Strategy (WRS) to mitigate the impact caused by bit-flipping errors. Extensive experiments and verification on hierarchical and non-hierarchical datasets demonstrate that SHOH preserves accurate inter-class distances and achieves performance improvements compared to state-of-the-art methods. The source code is available at https://github.com/HUST-IDSM-AI/SHOH .",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W4388632411",
    "type": "article"
  },
  {
    "title": "LFR-GAN: Local Feature Refinement based Generative Adversarial Network for Text-to-Image Generation",
    "doi": "https://doi.org/10.1145/3589002",
    "publication_date": "2023-03-30",
    "publication_year": 2023,
    "authors": "Zijun Deng; Xiangteng He; Yuxin Peng",
    "corresponding_authors": "",
    "abstract": "Text-to-image generation aims to generate images from text descriptions. Its main challenge lies in two aspects: (1) Semantic consistency, i.e., the generated images should be semantically consistent with the input text; and (2) Visual reality, i.e., the generated images should look like real images. To ensure text-image consistency, existing works mainly learn to establish the cross-modal representations via a text encoder and image encoder. However, due to the limited representation capability of the fixed-length embeddings and the flexibility of the free-form text descriptions, the learned text-to-image model is incapable of maintaining the semantic consistency between image local regions and fine-grained descriptions. As a result, the generated images sometimes miss some fine-grained attributes of the generated object, such as the color or shape of a part of the object. To address this issue, this paper proposes a Local Feature Refinement Based Generative Adversarial Network (LFR-GAN) , which first divides the text into some independent fine-grained attributes and generates an initial image, then refines the image details based on these attributes. The main contributions are three-fold: (1) An attribute modeling approach is proposed to model the fine-grained text descriptions by mapping them into representations of independent attributes, which provides more fine-grained details for image generation. (2) A local feature refinement approach is proposed to enable the generated image to form a complete reflection of the fine-grained attributes contained in the text description. (3) A multi-stage generation approach is proposed to realize the fine-grained manipulation of complex images progressively, which aims to improve the performance of the refinement and generate photo-realistic images. Extensive experiments on the CUB and Oxford102 datasets show the effectiveness of our LFR-GAN approach in both text-to-image generation and text-guided image manipulation tasks. Our LFR-GAN approach shows superior performance compared to the state-of-the-art methods. The codes will be released at https://github.com/PKU-ICST-MIPL/LFR-GAN_TOMM2023 .",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4361271773",
    "type": "article"
  },
  {
    "title": "Joint Audio-Visual Attention with Contrastive Learning for More General Deepfake Detection",
    "doi": "https://doi.org/10.1145/3625100",
    "publication_date": "2023-10-11",
    "publication_year": 2023,
    "authors": "Yibo Zhang; Weiguo Lin; Junfeng Xu",
    "corresponding_authors": "",
    "abstract": "With the continuous advancement of deepfake technology, there has been a surge in the creation of realistic fake videos. Unfortunately, the malicious utilization of deepfake poses a significant threat to societal morality and political security. Therefore, numerous researchers have proposed various deepfake detection methods. However, traditional deepfake approaches tend to focus on specific forgery features, such as artifacts or inconsistent actions, which can be vulnerable to specialized countermeasures. Recent studies show an intrinsic correlation between facial and audio cues, which can be exploited for deepfake detection. To address these challenges and enhance the robustness and generalization of deepfake detection algorithms, we propose a novel joint audio-visual deepfake detection model named AVA-CL, which is capable of detecting deepfakes in both audio and visual domains. Furthermore, exploiting the inherent correlation and consistency between audio and visual enhances the effectiveness of deepfake detection significantly. Through extensive experiments, we demonstrate that our proposed AVA-CL model outperforms many state-of-the-art (SOTA) methods with superior robustness and generalization capabilities. This research presents a promising approach for deepfake detection and reducing the harm caused by malicious use.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4387540674",
    "type": "article"
  },
  {
    "title": "Detection of Moving Object Using Superpixel Fusion Network",
    "doi": "https://doi.org/10.1145/3579998",
    "publication_date": "2023-01-12",
    "publication_year": 2023,
    "authors": "Yang Li",
    "corresponding_authors": "Yang Li",
    "abstract": "Moving object detection is still a challenging task in complex scenes. The existing methods based on deep learning mainly use U-Nets and have achieved amazing results. However, they ignore the local continuity between pixels. In order to solve this problem, a method based on a superpixel fusion network (SF-Net) is proposed in this article. First, the median filter is used to extract the candidate foreground (called pixel features ) and the image sequence is segmented by superpixel. Then, the histogram features (called superpixel features ) of the candidate foreground superpixels are extracted. Next, the pixel features and the superpixel features are the inputs of SF-Net, respectively. Experiments show the effectiveness of SF-Net on 34 image sequences and the average F-measure reaches 0.84. SF-Net can remove more background noise and has stronger expression ability than a network with the same depth.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W4315705044",
    "type": "article"
  },
  {
    "title": "Robust Copyright Protection Technique with High-embedding Capacity for Color Images",
    "doi": "https://doi.org/10.1145/3580502",
    "publication_date": "2023-01-20",
    "publication_year": 2023,
    "authors": "Dhanesh K Mahto; Amit Kumar Singh; Kedar Nath Singh; Om Prakash Singh; Amrit Kumar Agrawal",
    "corresponding_authors": "",
    "abstract": "Copyright violation issues have a growing impact on applications of the digital era, especially images. It is not easy to guarantee the copyright protection of essential information. This paper presents a robust copyright protection technique with high embedding capacity for color images. The technique first fuses the multi-focus images using non-subsampled contourlet transform (NSCT) to ensure that the fused images have more rich information than a single image. Further, hash value of the cover media is computed for authentication purposes. The fused image and hash value of the cover is then embedded into the cover media with the help of transformed-domain schemes. To achieve higher security, we apply an encryption scheme on a fused media watermark before embedding it into the cover. Lastly, a hybrid optimization algorithm is employed to compute an optimal factor, which makes a good imperceptibility and robustness at the same time. We demonstrate that the proposed technique is effective and resistant to common attacks on image datasets. Compared to existing works, this work increases the 9.5% robustness and 8.8% quality with high embedding capacity.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W4317536085",
    "type": "article"
  },
  {
    "title": "Multimodal Neurosymbolic Approach for Explainable Deepfake Detection",
    "doi": "https://doi.org/10.1145/3624748",
    "publication_date": "2023-09-20",
    "publication_year": 2023,
    "authors": "Ijaz Ul Haq; Khalid Mahmood Malik; Khan Muhammad",
    "corresponding_authors": "",
    "abstract": "Deepfake detection has become increasingly important in recent years owing to the widespread availability of deepfake generation technologies. Existing deepfake detection methods present two primary limitations; i.e., they are trained on a specific type of deepfake dataset, which renders them vulnerable to unseen deepfakes, and they regard deepfakes as a “black box” with limited explainability, making it difficult for non-AI experts to understand and trust the decisions. Hence, this article proposes a novel neurosymbolic deepfake detection framework that exploits the fact that human emotions cannot be imitated easily owing to their complex nature. We argue that deep fakes typically exhibit inter- or intra-modality inconsistencies in the emotional expressions of the person being manipulated. Thus, the proposed framework performs inter- and intra-modality reasoning on emotions extracted from audio and visual modalities using a psychological and arousal-valence model for deepfake detection. In addition to fake detection, the proposed framework provides textual explanations for its decisions. The results obtained using the Presidential Deepfakes Dataset and World Leaders Dataset of real and manipulated videos demonstrate the effectiveness of our approach in detecting deepfakes and highlight the potential of a neurosymbolic approach for expandability.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W4386889783",
    "type": "article"
  },
  {
    "title": "CrowdGraph: Weakly supervised Crowd Counting via Pure Graph Neural Network",
    "doi": "https://doi.org/10.1145/3638774",
    "publication_date": "2023-12-27",
    "publication_year": 2023,
    "authors": "Chengyang Zhang; Yong Zhang; Bo Li; Xinglin Piao; Baocai Yin",
    "corresponding_authors": "",
    "abstract": "Most existing weakly supervised crowd counting methods utilize Convolutional Neural Networks (CNN) or Transformer to estimate the total number of individuals in an image. However, both CNN-based (grid-to-count paradigm) and Transformer-based (sequence-to-count paradigm) methods take images as inputs in a regular form. This approach treats all pixels equally but cannot address the uneven distribution problem within human crowds. This challenge would lead to a decline in the counting performance of the model. Compared with grid and sequence, the graph structure could better explore the relationship among features. In this article, we propose a new graph-based crowd counting method named CrowdGraph, which reinterprets the weakly supervised crowd counting problem from a graph-to-count perspective. In the proposed CrowdGraph, each image is constructed as a graph, and a graph-based network is designed to extract features at the graph level. CrowdGraph comprises three main components: a dynamic graph convolutional backbone, a multi-scale dilated graph convolution module, and a regression head. To the best of our knowledge, CrowdGraph is the first method that is completely formulated based on the Graph Neural Network (GNN) for the crowd counting task. Extensive experiments demonstrate that the proposed CrowdGraph outperforms pure CNN-based and pure Transformer-based weakly supervised methods comprehensively and achieves highly competitive counting performance.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W4390263318",
    "type": "article"
  },
  {
    "title": "Improving Continuous Sign Language Recognition with Consistency Constraints and Signer Removal",
    "doi": "https://doi.org/10.1145/3640815",
    "publication_date": "2024-01-15",
    "publication_year": 2024,
    "authors": "Ronglai Zuo; Brian Mak",
    "corresponding_authors": "",
    "abstract": "Deep-learning-based continuous sign language recognition (CSLR) models typically consist of a visual module, a sequential module, and an alignment module. However, the effectiveness of training such CSLR backbones is hindered by limited training samples, rendering the use of a single connectionist temporal classification loss insufficient. To address this limitation, we propose three auxiliary tasks to enhance CSLR backbones. First, we enhance the visual module, which is particularly sensitive to the challenges posed by limited training samples, from the perspective of consistency. Specifically, since sign languages primarily rely on signers’ facial expressions and hand movements to convey information, we develop a keypoint-guided spatial attention module that directs the visual module to focus on informative regions, thereby ensuring spatial attention consistency. Furthermore, recognizing that the output features of both the visual and sequential modules represent the same sentence, we leverage this prior knowledge to better exploit the power of the backbone. We impose a sentence embedding consistency constraint between the visual and sequential modules, enhancing the representation power of both features. The resulting CSLR model, referred to as consistency-enhanced CSLR, demonstrates superior performance on signer-dependent datasets, where all signers appear during both training and testing. To enhance its robustness for the signer-independent setting, we propose a signer removal module based on feature disentanglement, effectively eliminating signer-specific information from the backbone. To validate the effectiveness of the proposed auxiliary tasks, we conduct extensive ablation studies. Notably, utilizing a transformer-based backbone, our model achieves state-of-the-art or competitive performance on five benchmarks, including PHOENIX-2014, PHOENIX-2014-T, PHOENIX-2014-SI, CSL, and CSL-Daily. Code and models are available at https://github.com/2000ZRL/LCSA_C2SLR_SRM.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4390878798",
    "type": "article"
  },
  {
    "title": "SPIRIT: Style-guided Patch Interaction for Fashion Image Retrieval with Text Feedback",
    "doi": "https://doi.org/10.1145/3640345",
    "publication_date": "2024-01-15",
    "publication_year": 2024,
    "authors": "Yanzhe Chen; Jiahuan Zhou; Yuxin Peng",
    "corresponding_authors": "",
    "abstract": "Fashion image retrieval with text feedback aims to find the target image according to the reference image and the modification from the user. This is a challenging task, as it requires not only the synergistic understanding of both visual and textual modalities but also the ability to model a wide variety of styles that fashion images contain. Hence, the crucial aspect of addressing this problem lies in exploiting the abundant semantic information inherent in fashion images and correlating it with the textual description of style. Recognizing that style is generally situated at the local level, we explicitly define style as the commonalities and differences between local areas of fashion images. Building upon this, we propose a Style-guided Patch InteRaction approach for fashion Image retrieval with Text feedback (SPIRIT), which focuses on the decisive influence of local details of fashion images on their style. Three corresponding networks are designed pertinently. The Patch-level Style Commonality network is introduced to fully leverage the semantic information among patches and compute their average as the style commonality. Subsequently, the Patch-level Style Difference network employs a graph reasoning network to model the patch-level difference and filter out insignificant patches. By considering the above two networks, mutual information about style is obtained from the interaction between patches. Finally, the Visual Textual Fusion network is utilized to integrate visual features with rich semantic information and textual features. Experimental results on four benchmark datasets demonstrate that our proposed SPIRIT achieves state-of-the-art performance. Source code is available at https://github.com/PKU-ICST-MIPL/SPIRIT_TOMM2024 .",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4390879096",
    "type": "article"
  },
  {
    "title": "Gloss-driven Conditional Diffusion Models for Sign Language Production",
    "doi": "https://doi.org/10.1145/3663572",
    "publication_date": "2024-05-03",
    "publication_year": 2024,
    "authors": "Shengeng Tang; Feng Xue; Jingjing Wu; Shuo Wang; Richang Hong",
    "corresponding_authors": "",
    "abstract": "Sign Language Production (SLP) aims to convert text or audio sentences into sign language videos corresponding to their semantics, which is challenging due to the diversity and complexity of sign languages, and cross-modal semantic mapping issues. In this work, we propose a Gloss-driven Conditional Diffusion Model (GCDM) for SLP. The core of the GCDM is a diffusion model architecture, in which the sign gloss sequence is encoded by a Transformer-based encoder and input into the diffusion model as a semantic prior condition. In the process of sign pose generation, the textual semantic priors carried in the encoded gloss features are integrated into the embedded Gaussian noise via cross-attention. Subsequently, the model converts the fused features into sign language pose sequences through T-round denoising steps. During the training process, the model uses the ground-truth labels of sign poses as the starting point, generates Gaussian noise through T rounds of noise, and then performs T rounds of denoising to approximate the real sign language gestures. The entire process is constrained by the MAE loss function to ensure that the generated sign language gestures are as close as possible to the real labels. In the inference phase, the model directly randomly samples a set of Gaussian noise, generates multiple sign language gesture sequence hypotheses under the guidance of the gloss sequence, and outputs a high-confidence sign language gesture video by averaging multiple hypotheses. Experimental results on the Phoenix2014T dataset show that the proposed GCDM method achieves competitiveness in both quantitative performance and qualitative visualization.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4396612946",
    "type": "article"
  },
  {
    "title": "CrossFormer: Cross-modal Representation Learning via Heterogeneous Graph Transformer",
    "doi": "https://doi.org/10.1145/3688801",
    "publication_date": "2024-09-20",
    "publication_year": 2024,
    "authors": "Xiao Liang; Erkun Yang; Cheng Deng; Yanhua Yang",
    "corresponding_authors": "",
    "abstract": "Transformers have been recognized as powerful tools for various cross-modal tasks due to their superior ability to perform representation learning through self-attention. Existing transformer-based cross-modal models can be categorized into single-stream and dual-stream ones. By performing fine-grained interaction with self-attention on the cross-modal concatenated features, the former can simultaneously learn intra- and inter-modal correlations. However, this simple concatenation treats the inputs of different modalities equally; as a result, the heterogeneous differences between modalities are ignored, leading to a modality gap. The latter process the inputs of different modalities separately, then perform cross-modal interaction on the subsequently fused networks, resulting in a failure to integrate the fine-grained correlations of both intra- and inter-modality in a uniform module. To this end, we propose an effective heterogeneous graph transformer for dual-stream cross-modal representation learning, named CrossFormer, which constructs a heterogeneous graph as a bridge to achieve fine-grained intra- and inter-modal interaction on a dual-stream network. Specifically, we first represent multi-modal data with a heterogeneous graph, then develop a dual-positional encoding strategy that enables the heterogeneous graph to obtain the relative positional information. Finally, a dual-stream self-attention is performed on the heterogeneous graph, bridging the gap between modalities and effectively capturing fine-grained intra- and inter-modal interactions simultaneously. Extensive experiments on various cross-modal tasks demonstrate the superiority of our method.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4402678995",
    "type": "article"
  },
  {
    "title": "Personalized Federated Mutual Learning for Unsupervised Camera-aware Person Re-identification",
    "doi": "https://doi.org/10.1145/3696453",
    "publication_date": "2024-09-21",
    "publication_year": 2024,
    "authors": "Jiabei Liu; Weiming Zhuang; Yuanyuan Liu; Yonggang Wen; Jun Huang; Wei Lin",
    "corresponding_authors": "",
    "abstract": "Person re-identification (ReID) is essential for enhancing security and tracking in multi-camera surveillance systems. To achieve effective person re-identification (ReID) performance across diverse datasets, the Federated Unsupervised Person Re-identification via Camera-aware Clustering (FedUCA) approach has made strides in utilizing distributed datasets while ensuring data privacy. Nevertheless, its uniform model may not adequately cater to the specific characteristics of each participant’s data, given the diversity in camera perspectives and client-specific data variances, thus obtaining degraded results. To address this issue, we propose an advanced framework, Personalized Federated Dual-Model Learning for Camera-Aware Person Re-Identification (PerFedDual), which introduces knowledge-sharing techniques inherent to mutual learning for FedUCA with the camera-centric clustering process. PerFedDual supports a dual-model training approach that creates a cooperative learning space that improves both the global model and client-specific models by exchanging knowledge both ways. The methodology adopted is precisely adjusted to the distinctive data environment of each client, ensuring the protection of privacy while simultaneously enhancing the accuracy and flexibility of ReID models in diverse camera configurations. The empirical evaluation reveals that PerFedDual outperforms FedUCA and alternative federated learning strategies, highlighting the benefits of our technique that leverage collective intelligence to enhance unsupervised person re-identification.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4402709859",
    "type": "article"
  },
  {
    "title": "Privacy and Integrity Protection for IoT Multimodal Data Using Machine Learning and Blockchain",
    "doi": "https://doi.org/10.1145/3638769",
    "publication_date": "2024-01-06",
    "publication_year": 2024,
    "authors": "Qingzhi Liu; Yuchen Huang; Chenglu Jin; Xiaohan Zhou; Ying Mao; Cagatay Catal; Long Cheng",
    "corresponding_authors": "",
    "abstract": "With the wide application of Internet of Things (IoT) technology, large volumes of multimodal data are collected and analyzed for various diagnoses, analyses, and predictions to help in decision-making and management. However, the research on protecting data integrity and privacy is quite limited, while the lack of proper protection for sensitive data may have significant impacts on the benefits and gains of data owners. In this research, we propose a protection solution for data integrity and privacy. Specifically, our system protects data integrity through distributed systems and blockchain technology. Meanwhile, our system guarantees data privacy using differential privacy and Machine Learning (ML) techniques. Our system aims to maintain the usability of the data for further data analytical tasks of data users, while encrypting the data according to the requirements of data owners. We implement our solution with smart contracts, distributed file systems, and ML models. The experimental results show that our proposed solution can effectively encrypt source IoT data according to the requirements of data users while data integrity can be protected under the blockchain.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4390640482",
    "type": "article"
  },
  {
    "title": "GANs in the Panorama of Synthetic Data Generation Methods",
    "doi": "https://doi.org/10.1145/3657294",
    "publication_date": "2024-04-10",
    "publication_year": 2024,
    "authors": "Bruno Vaz; Álvaro Figueira",
    "corresponding_authors": "",
    "abstract": "This paper focuses on the creation and evaluation of synthetic data to address the challenges of imbalanced datasets in machine learning applications (ML), using fake news detection as a case study. We conducted a thorough literature review on generative adversarial networks (GANs) for tabular data, synthetic data generation methods, and synthetic data quality assessment. By augmenting a public news dataset with synthetic data generated by different GAN architectures, we demonstrate the potential of synthetic data to improve ML models’ performance in fake news detection. Our results show a significant improvement in classification performance, especially in the underrepresented class. We also modify and extend a data usage approach to evaluate the quality of synthetic data and investigate the relationship between synthetic data quality and data augmentation performance in classification tasks. We found a positive correlation between synthetic data quality and performance in the underrepresented class, highlighting the importance of high-quality synthetic data for effective data augmentation.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4394686888",
    "type": "article"
  },
  {
    "title": "Federated Learning-Based Anomaly Detection with Isolation Forest in the IoT-Edge Continuum",
    "doi": "https://doi.org/10.1145/3702995",
    "publication_date": "2024-11-02",
    "publication_year": 2024,
    "authors": "Haolong Xiang; Xuyun Zhang; Xiaolong Xu; Amin Beheshti; Lianyong Qi; Yujie Hong; Wanchun Dou",
    "corresponding_authors": "",
    "abstract": "Traditional methods for ensuring security and privacy face challenges in safeguarding multimedia data within the IoT-edge continuum, as their significant computational demands render them unsuitable for IoT devices with limited resources. Next, we find that the federated learning techniques can naturally adapt to edge frameworks and provide effective data security and privacy protection. In this paper, we propose FLiForest, an innovative anomaly detection approach that integrates federated learning with the isolation forest algorithm, tailored for the IoT-edge continuum. Specifically, our method designs a three-stage process, including data collection and sampling, model training, and data testing, to train an isolation forest among clients and edge servers jointly. In the training of each layer, all clients upload parameters to the central server for aggregation. FLiForest facilitates decentralized model training across IoT devices, enhancing data privacy and reducing computational burden, without necessitating the exchange of multimedia data. Through extensive experiments on a variety of multimedia datasets, the efficacy of our method is benchmarked against the state-of-the-art anomaly detection methods, showcasing its superior detection accuracy and robustness in ensuring data privacy and security.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4403996974",
    "type": "article"
  },
  {
    "title": "An analytical study of peer-to-peer media streaming systems",
    "doi": "https://doi.org/10.1145/1111604.1111607",
    "publication_date": "2005-11-01",
    "publication_year": 2005,
    "authors": "Yi-Cheng Tu; Jianzhong Sun; Mohamed Hefeeda; Sunil Prabhakar",
    "corresponding_authors": "",
    "abstract": "Recent research efforts have demonstrated the great potential of building cost-effective media streaming systems on top of peer-to-peer (P2P) networks. A P2P media streaming architecture can reach a large streaming capacity that is difficult to achieve in conventional server-based streaming services. Hybrid streaming systems that combine the use of dedicated streaming servers and P2P networks were proposed to build on the advantages of both paradigms. However, the dynamics of such systems and the impact of various factors on system behavior are not totally clear. In this article, we present an analytical framework to quantitatively study the features of a hybrid media streaming model. Based on this framework, we derive an equation to describe the capacity growth of a single-file streaming system. We then extend the analysis to multi-file scenarios. We also show how the system achieves optimal allocation of server bandwidth among different media objects. The unpredictable departure/failure of peers is a critical factor that affects the performance of P2P systems. We utilize the concept of peer lifespan to model peer failures. The original capacity growth equation is enhanced with coefficients generated from peer lifespans that follow an exponential distribution. We also propose a failure model under arbitrarily distributed peer lifespan. Results from large-scale simulations support our analysis.",
    "cited_by_count": 65,
    "openalex_id": "https://openalex.org/W2117760870",
    "type": "article"
  },
  {
    "title": "Understanding performance in coliseum, an immersive videoconferencing system",
    "doi": "https://doi.org/10.1145/1062253.1062258",
    "publication_date": "2005-05-01",
    "publication_year": 2005,
    "authors": "H. Harlyn Baker; Nina Bhatti; Donald Tanguay; Irwin Sobel; Dan Gelb; Michael E. Goss; W. Bruce Culbertson; Thomas Malzbender",
    "corresponding_authors": "",
    "abstract": "Coliseum is a multiuser immersive remote teleconferencing system designed to provide collaborative workers the experience of face-to-face meetings from their desktops. Five cameras are attached to each PC display and directed at the participant. From these video streams, view synthesis methods produce arbitrary-perspective renderings of the participant and transmit them to others at interactive rates, currently about 15 frames per second. Combining these renderings in a shared synthetic environment gives the appearance of having all participants interacting in a common space. In this way, Coliseum enables users to share a virtual world, with acquired-image renderings of their appearance replacing the synthetic representations provided by more conventional avatar-populated virtual worlds. The system supports virtual mobility---participants may move around the shared space---and reciprocal gaze, and has been demonstrated in collaborative sessions of up to ten Coliseum workstations, and sessions spanning two continents.Coliseum is a complex software system which pushes commodity computing resources to the limit. We set out to measure the different aspects of resource, network, CPU, memory, and disk usage to uncover the bottlenecks and guide enhancement and control of system performance. Latency is a key component of Quality of Experience for video conferencing. We present how each aspect of the system---cameras, image processing, networking, and display---contributes to total latency. Performance measurement is as complex as the system to which it is applied. We describe several techniques to estimate performance through direct light-weight instrumentation as well as use of realistic end-to-end measures that mimic actual user experience. We describe the various techniques and how they can be used to improve system performance for Coliseum and other network applications. This article summarizes the Coliseum technology and reports on issues related to its performance---its measurement, enhancement, and control.",
    "cited_by_count": 63,
    "openalex_id": "https://openalex.org/W2004643852",
    "type": "article"
  },
  {
    "title": "Disk scheduling in a multimedia I/O system",
    "doi": "https://doi.org/10.1145/1047936.1047941",
    "publication_date": "2005-02-01",
    "publication_year": 2005,
    "authors": "A. L. Narasimha Reddy; Jim Wyllie; K. B. R. Wijayaratne",
    "corresponding_authors": "",
    "abstract": "This article provides a retrospective of our original paper by the same title in the Proceedings of the First ACM Conference on Multimedia, published in 1993. This article examines the problem of disk scheduling in a multimedia I/O system. In a multimedia server, the disk requests may have constant data rate requirements and need guaranteed service. We propose a new scheduling algorithm, SCAN-EDF, that combines the features of SCAN type of seek optimizing algorithm with an Earliest Deadline First (EDF) type of real-time scheduling algorithm. We compare SCAN-EDF with other scheduling strategies and show that SCAN-EDF combines the best features of both SCAN and EDF. We also investigate the impact of buffer space on the maximum number of video streams that can be supported.We show that by making the deadlines larger than the request periods, a larger number of streams can be supported.We also describe how we extended the SCAN-EDF algorithm in the PRISM multimedia architecture. PRISM is an integrated multimedia server, designed to satisfy the QOS requirements of multiple classes of requests. Our experience in implementing the extended SCAN-EDF algorithm in a generic operating system is discussed and performance metrics and results are presented to illustrate how the SCAN-EDF extensions and implementation strategies have succeeded in meeting the QOS requirements of different classes of requests.",
    "cited_by_count": 63,
    "openalex_id": "https://openalex.org/W2082993844",
    "type": "article"
  },
  {
    "title": "GridCast",
    "doi": "https://doi.org/10.1145/1412196.1412199",
    "publication_date": "2008-10-01",
    "publication_year": 2008,
    "authors": "Bin Cheng; Lex Stein; Hai Jin; Xiaofei Liao; Zheng Zhang",
    "corresponding_authors": "",
    "abstract": "Video-on-Demand (VoD) is a compelling application, but costly. VoD is costly due to the load it places on video source servers. Many have proposed using peer-to-peer (P2P) techniques to shift load from servers to peers. Yet, nobody has implemented and deployed a system to openly and systematically evaluate how these techniques work. This article describes the design, implementation and evaluation of GridCast, a real deployed P2P VoD system. GridCast has been live on CERNET since May of 2006. It provides seek, pause, and play operations, and employs peer sharing to improve system scalability. In peak months, GridCast has served videos to 23,000 unique users. From the first deployment, we have gathered information to understand the system and evaluate how to further improve peer sharing through caching and replication. We first show that GridCast with single video caching (SVC) can decrease load on source servers by an average of 22% from a client-server architecture. We analyze the net effect on system resources and determine that peer upload is largely idle. This leads us to changing the caching algorithm to cache multiple videos (MVC). MVC decreases source load by an average of 51% over the client-server. The improvement is greater as user load increases. This bodes well for peer-assistance at larger scales. A detailed analysis of MVC shows that departure misses become a major issue in a P2P VoD system with caching optimization. Motivated by this observation, we examine how to use replication to eliminate departure misses and further reduce server load. A framework for lazy replication is presented and evaluated in this article. In this framework, two predictors are plugged in to create the working replication algorithm. With these two simple predictors, lazy replication can decrease server load by 15% from MVC with only a minor increase in network traffic.",
    "cited_by_count": 60,
    "openalex_id": "https://openalex.org/W2035491066",
    "type": "article"
  },
  {
    "title": "Virtual videography",
    "doi": "https://doi.org/10.1145/1198302.1198306",
    "publication_date": "2007-02-01",
    "publication_year": 2007,
    "authors": "Rachel Heck; Michael N. Wallick; Michael Gleicher",
    "corresponding_authors": "",
    "abstract": "Well-produced videos provide a convenient and effective way to archive lectures. In this article, we offer a new way to create lecture videos that retains many of the advantages of well-composed recordings, without the cost and intrusion of a video production crew. We present an automated system called Virtual Videography that employs the art of videography to mimic videographer-produced videos, while unobtrusively recording lectures. The system uses the data recorded by unattended video cameras and microphones to produce a new edited video as an offline postprocess. By producing videos offline, our system can use future information when planning shot sequences and synthesizing new shots. Using simple syntactic cues gathered from the original video and a novel shot planning algorithm, the system makes cinematic decisions without any semantic understanding of the lecture.",
    "cited_by_count": 54,
    "openalex_id": "https://openalex.org/W2294706300",
    "type": "article"
  },
  {
    "title": "Fragment, tag, enrich, and send",
    "doi": "https://doi.org/10.1145/1556134.1556136",
    "publication_date": "2009-08-01",
    "publication_year": 2009,
    "authors": "Pablo César; Dick C. A. Bulterman; Jack Jansen; David Geerts; Hendrik Knoche; William Seager",
    "corresponding_authors": "",
    "abstract": "The migration of media consumption to personal computers retains distributed social viewing, but only via nonsocial, strictly personal interfaces. This article presents an architecture, and implementation for media sharing that allows for enhanced social interactions among users. Using a mixed-device model, our work allows targeted, personalized enrichment of content. All recipients see common content, while differentiated content is delivered to individuals via their personal secondary screens. We describe the goals, architecture, and implementation of our system in this article. In order to validate our results, we also present results from two user studies involving disjoint sets of test participants.",
    "cited_by_count": 51,
    "openalex_id": "https://openalex.org/W1979173150",
    "type": "article"
  },
  {
    "title": "Correlative multilabel video annotation with temporal kernels",
    "doi": "https://doi.org/10.1145/1404880.1404883",
    "publication_date": "2008-10-01",
    "publication_year": 2008,
    "authors": "Guo-Jun Qi; Xian‐Sheng Hua; Yong Rui; Jinhui Tang; Tao Mei; Meng Wang; Hao Zhang",
    "corresponding_authors": "",
    "abstract": "Automatic video annotation is an important ingredient for semantic-level video browsing, search and navigation. Much attention has been paid to this topic in recent years. These researches have evolved through two paradigms. In the first paradigm, each concept is individually annotated by a pre-trained binary classifier. However, this method ignores the rich information between the video concepts and only achieves limited success. Evolved from the first paradigm, the methods in the second paradigm add an extra step on the top of the first individual classifiers to fuse the multiple detections of the concepts. However, the performance of these methods can be degraded by the error propagation incurred in the first step to the second fusion one. In this article, another paradigm of the video annotation method is proposed to address these problems. It simultaneously annotates the concepts as well as model correlations between them in one step by the proposed Correlative Multilabel (CML) method, which benefits from the compensation of complementary information between different labels. Furthermore, since the video clips are composed by temporally ordered frame sequences, we extend the proposed method to exploit the rich temporal information in the videos. Specifically, a temporal-kernel is incorporated into the CML method based on the discriminative information between Hidden Markov Models (HMMs) that are learned from the videos. We compare the performance between the proposed approach and the state-of-the-art approaches in the first and second paradigms on the widely used TRECVID data set. As to be shown, superior performance of the proposed method is gained.",
    "cited_by_count": 50,
    "openalex_id": "https://openalex.org/W2094067871",
    "type": "article"
  },
  {
    "title": "Protecting the content integrity of digital imagery with fidelity preservation",
    "doi": "https://doi.org/10.1145/2000486.2000489",
    "publication_date": "2011-08-01",
    "publication_year": 2011,
    "authors": "Pei-Yu Lin; Jung-San Lee; Chin‐Chen Chang",
    "corresponding_authors": "",
    "abstract": "Fragile watermarking is applied to protect the content integrity of digital images. The main concerns related to watermarking include retaining the quality of the watermarked image and retaining the ability to detect whether any manipulation has occurred. Because recent watermarking techniques seriously distort the quality of the protected image after embedding the authentication code into the image content, attention has been drawn to how to satisfy both the need for image fidelity and detection ability. To account for the influence from both essentials, a novel algorithm is proposed in this article. The new scheme utilizes a weighted-sum function to embed ( n + 1) authentication bits into a block with 2 n pixels by modifying only one original pixel with (±1). With fewer authentication codes, the new process can protect the content of the image. The experimental results demonstrate that the approach can guarantee the fidelity of the watermarked image while retaining tamper-proof functionality.",
    "cited_by_count": 47,
    "openalex_id": "https://openalex.org/W2082417656",
    "type": "article"
  },
  {
    "title": "User perception of media content association in olfaction-enhanced multimedia",
    "doi": "https://doi.org/10.1145/2379790.2379794",
    "publication_date": "2012-11-01",
    "publication_year": 2012,
    "authors": "Gheorghiță Ghinea; Oluwakemi A. Ademoye",
    "corresponding_authors": "",
    "abstract": "Olfaction is an exciting challenge facing multimedia applications. In this article we have investigated user perception of the association between olfactory media content and video media content in olfactory-enhanced multimedia. Results show that the association between scent and content has a significant impact on the user-perceived experience of olfactory-enhanced multimedia.",
    "cited_by_count": 47,
    "openalex_id": "https://openalex.org/W2105341559",
    "type": "article"
  },
  {
    "title": "Interactive partner control in close interactions for real-time applications",
    "doi": "https://doi.org/10.1145/2487268.2487274",
    "publication_date": "2013-06-01",
    "publication_year": 2013,
    "authors": "Edmond S. L. Ho; Jacky C. P. Chan; Taku Komura; Howard Leung",
    "corresponding_authors": "",
    "abstract": "This article presents a new framework for synthesizing motion of a virtual character in response to the actions performed by a user-controlled character in real time. In particular, the proposed method can handle scenes in which the characters are closely interacting with each other such as those in partner dancing and fighting. In such interactions, coordinating the virtual characters with the human player automatically is extremely difficult because the system has to predict the intention of the player character. In addition, the style variations from different users affect the accuracy in recognizing the movements of the player character when determining the responses of the virtual character. To solve these problems, our framework makes use of the spatial relationship-based representation of the body parts called interaction mesh, which has been proven effective for motion adaptation. The method is computationally efficient, enabling real-time character control for interactive applications. We demonstrate its effectiveness and versatility in synthesizing a wide variety of motions with close interactions.",
    "cited_by_count": 45,
    "openalex_id": "https://openalex.org/W2100851637",
    "type": "article"
  },
  {
    "title": "Modeling and assessing quality of information in multisensor multimedia monitoring systems",
    "doi": "https://doi.org/10.1145/1870121.1870124",
    "publication_date": "2011-01-01",
    "publication_year": 2011,
    "authors": "M. Anwar Hossain; Pradeep K. Atrey; Abdulmotaleb El Saddik",
    "corresponding_authors": "",
    "abstract": "Current sensor-based monitoring systems use multiple sensors in order to identify high-level information based on the events that take place in the monitored environment. This information is obtained through low-level processing of sensory media streams, which are usually noisy and imprecise, leading to many undesired consequences such as false alarms, service interruptions, and often violation of privacy. Therefore, we need a mechanism to compute the quality of sensor-driven information that would help a user or a system in making an informed decision and improve the automated monitoring process. In this article, we propose a model to characterize such quality of information in a multisensor multimedia monitoring system in terms of certainty, accuracy/confidence and timeliness. Our model adopts a multimodal fusion approach to obtain the target information and dynamically compute these attributes based on the observations of the participating sensors. We consider the environment context, the agreement/disagreement among the sensors, and their prior confidence in the fusion process in determining the information of interest. The proposed method is demonstrated by developing and deploying a real-time monitoring system in a simulated smart environment. The effectiveness and suitability of the method has been demonstrated by dynamically assessing the value of the three quality attributes with respect to the detection and identification of human presence in the environment.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W2044816253",
    "type": "article"
  },
  {
    "title": "Exploiting online music tags for music emotion classification",
    "doi": "https://doi.org/10.1145/2037676.2037683",
    "publication_date": "2011-10-01",
    "publication_year": 2011,
    "authors": "Yu‐Ching Lin; Yi‐Hsuan Yang; Homer H. Chen",
    "corresponding_authors": "",
    "abstract": "The online repository of music tags provides a rich source of semantic descriptions useful for training emotion-based music classifier. However, the imbalance of the online tags affects the performance of emotion classification. In this paper, we present a novel data-sampling method that eliminates the imbalance but still takes the prior probability of each emotion class into account. In addition, a two-layer emotion classification structure is proposed to harness the genre information available in the online repository of music tags. We show that genre-based grouping as a precursor greatly improves the performance of emotion classification. On the average, the incorporation of online genre tags improves the performance of emotion classification by a factor of 55% over the conventional single-layer system. The performance of our algorithm for classifying 183 emotion classes reaches 0.36 in example-based f-score.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W2065261119",
    "type": "article"
  },
  {
    "title": "Near-lossless semantic video summarization and its applications to video analysis",
    "doi": "https://doi.org/10.1145/2487268.2487269",
    "publication_date": "2013-06-01",
    "publication_year": 2013,
    "authors": "Tao Mei; Lin-Xie Tang; Jinhui Tang; Xian‐Sheng Hua",
    "corresponding_authors": "",
    "abstract": "The ever increasing volume of video content on the Web has created profound challenges for developing efficient indexing and search techniques to manage video data. Conventional techniques such as video compression and summarization strive for the two commonly conflicting goals of low storage and high visual and semantic fidelity. With the goal of balancing both video compression and summarization, this article presents a novel approach, called Near-Lossless Semantic Summarization (NLSS), to summarize a video stream with the least high-level semantic information loss by using an extremely small piece of metadata. The summary consists of compressed image and audio streams, as well as the metadata for temporal structure and motion information. Although at a very low compression rate (around 1/40 of H.264 baseline, where traditional compression techniques can hardly preserve an acceptable visual fidelity), the proposed NLSS still can be applied to many video-oriented tasks, such as visualization, indexing and browsing, duplicate detection, concept detection, and so on. We evaluate the NLSS on TRECVID and other video collections, and demonstrate that it is a powerful tool for significantly reducing storage consumption, while keeping high-level semantic fidelity.",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W2041771741",
    "type": "article"
  },
  {
    "title": "Bagadus",
    "doi": "https://doi.org/10.1145/2541011",
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "Håkon Kvale Stensland; Vamsidhar Reddy Gaddam; Marius Tennøe; Espen Helgedagsrud; Mikkel Næss; Henrik Kjus Alstad; Asgeir Mortensen; Ragnar Langseth; Sigurd Ljødal; Østein Landsverk; Carsten Griwodz; Pål Halvorsen; Magnus Stenhaug; Dag Johansen",
    "corresponding_authors": "",
    "abstract": "The importance of winning has increased the role of performance analysis in the sports industry, and this underscores how statistics and technology keep changing the way sports are played. Thus, this is a growing area of interest, both from a computer system view in managing the technical challenges and from a sport performance view in aiding the development of athletes. In this respect, Bagadus is a real-time prototype of a sports analytics application using soccer as a case study. Bagadus integrates a sensor system, a soccer analytics annotations system, and a video processing system using a video camera array. A prototype is currently installed at Alfheim Stadium in Norway, and in this article, we describe how the system can be used in real-time to playback events. The system supports both stitched panorama video and camera switching modes and creates video summaries based on queries to the sensor system. Moreover, we evaluate the system from a systems point of view, benchmarking different approaches, algorithms, and trade-offs, and show how the system runs in real time.",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W1964725199",
    "type": "article"
  },
  {
    "title": "A holistic approach to aesthetic enhancement of photographs",
    "doi": "https://doi.org/10.1145/2037676.2037678",
    "publication_date": "2011-10-01",
    "publication_year": 2011,
    "authors": "Subhabrata Bhattacharya; Rahul Sukthankar; Mubarak Shah",
    "corresponding_authors": "",
    "abstract": "This article presents an interactive application that enables users to improve the visual aesthetics of their digital photographs using several novel spatial recompositing techniques. This work differs from earlier efforts in two important aspects: (1) it focuses on both photo quality assessment and improvement in an integrated fashion, (2) it enables the user to make informed decisions about improving the composition of a photograph. The tool facilitates interactive selection of one or more than one foreground objects present in a given composition, and the system presents recommendations for where it can be relocated in a manner that optimizes a learned aesthetic metric while obeying semantic constraints. For photographic compositions that lack a distinct foreground object, the tool provides the user with crop or expansion recommendations that improve the aesthetic appeal by equalizing the distribution of visual weights between semantically different regions. The recomposition techniques presented in the article emphasize learning support vector regression models that capture visual aesthetics from user data and seek to optimize this metric iteratively to increase the image appeal. The tool demonstrates promising aesthetic assessment and enhancement results on variety of images and provides insightful directions towards future research.",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W2018896047",
    "type": "article"
  },
  {
    "title": "Scalable Object Retrieval with Compact Image Representation from Generic Object Regions",
    "doi": "https://doi.org/10.1145/2818708",
    "publication_date": "2015-10-20",
    "publication_year": 2015,
    "authors": "Shaoyan Sun; Wengang Zhou; Qi Tian; Houqiang Li",
    "corresponding_authors": "",
    "abstract": "In content-based visual object retrieval, image representation is one of the fundamental issues in improving retrieval performance. Existing works adopt either local SIFT-like features or holistic features, and may suffer sensitivity to noise or poor discrimination power. In this article, we propose a compact representation for scalable object retrieval from few generic object regions. The regions are identified with a general object detector and are described with a fusion of learning-based features and aggregated SIFT features. Further, we compress feature representation in large-scale image retrieval scenarios. We evaluate the performance of the proposed method on two public ground-truth datasets, with promising results. Experimental results on a million-scale image database demonstrate superior retrieval accuracy with efficiency gain in both computation and memory usage.",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W2066142224",
    "type": "article"
  },
  {
    "title": "VlogSense",
    "doi": "https://doi.org/10.1145/2037676.2037690",
    "publication_date": "2011-10-01",
    "publication_year": 2011,
    "authors": "Joan-Isaac Biel; Daniel Gática-Pérez",
    "corresponding_authors": "",
    "abstract": "We introduce the automatic analysis of conversational vlogs (VlogSense, for short) as a new research domain in social media. Conversational vlogs are inherently multimodal, depict natural behavior, and are suitable for large-scale analysis. Given their diversity in terms of content, VlogSense requires the integration of robust methods for multimodal analysis and for social media understanding. We present an original study on the automatic characterization of vloggers' audiovisual nonverbal behavior, grounded in work from social psychology and behavioral computing. Our study on 2,269 vlogs from YouTube shows that several nonverbal cues are significantly correlated with the social attention received by videos.",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W2122217324",
    "type": "article"
  },
  {
    "title": "User-profile-based perceived olfactory and visual media synchronization",
    "doi": "https://doi.org/10.1145/2540994",
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "Niall Murray; Yuansong Qiao; Brian Lee; Gabriel‐Miro Muntean",
    "corresponding_authors": "",
    "abstract": "As a step towards enhancing users' perceived multimedia quality levels, this article presents the results of a study which looked at user's perception of inter-stream synchronization between scent and video. The ability to detect and the perception of and impact of skew on user's quality of experience is analyzed considering user's age, sex, and culture (user profile). The results indicate that skews beyond a certain level between olfaction and video have a negative impact on user-perceived experience. Olfaction before video is more noticeable to users than olfaction after video, and assessors are more tolerable of olfactory data presented after video.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W2003345537",
    "type": "article"
  },
  {
    "title": "Mesh Discriminative Features for 3D Steganalysis",
    "doi": "https://doi.org/10.1145/2535555",
    "publication_date": "2014-04-01",
    "publication_year": 2014,
    "authors": "Ying Yang; Ioannis Ivrissimtzis",
    "corresponding_authors": "",
    "abstract": "We propose a steganalytic algorithm for triangle meshes, based on the supervised training of a classifier by discriminative feature vectors. After a normalization step, the triangle mesh is calibrated by one step of Laplacian smoothing and then a feature vector is computed, encoding geometric information corresponding to vertices, edges and faces. For a given steganographic or watermarking algorithm, we create a training set containing unmarked meshes and meshes marked by that algorithm, and train a classifier using Quadratic Discriminant Analysis. The performance of the proposed method was evaluated on six well-known watermarking/steganographic schemes with satisfactory accuracy rates.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W2087953725",
    "type": "article"
  },
  {
    "title": "Contrast Enhancement Estimation for Digital Image Forensics",
    "doi": "https://doi.org/10.1145/3183518",
    "publication_date": "2018-05-22",
    "publication_year": 2018,
    "authors": "Longyin Wen; Honggang Qi; Siwei Lyu",
    "corresponding_authors": "",
    "abstract": "Inconsistency in contrast enhancement can be used to expose image forgeries. In this work, we describe a new method to estimate contrast enhancement operations from a single image. Our method takes advantage of the nature of contrast enhancement as a mapping between pixel values and the distinct characteristics it introduces to the image pixel histogram. Our method recovers the original pixel histogram and the contrast enhancement simultaneously from a single image with an iterative algorithm. Unlike previous works, our method is robust in the presence of additive noise perturbations that are used to hide the traces of contrast enhancement. Furthermore, we also develop an effective method to detect image regions undergone contrast enhancement transformations that are different from the rest of the image, and we use this method to detect composite images. We perform extensive experimental evaluations to demonstrate the efficacy and efficiency of our method.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W2625745508",
    "type": "article"
  },
  {
    "title": "Detection of Human, Legitimate Bot, and Malicious Bot in Online Social Networks Based on Wavelets",
    "doi": "https://doi.org/10.1145/3183506",
    "publication_date": "2018-03-26",
    "publication_year": 2018,
    "authors": "Sylvio Barbon; Gabriel Fillipe Centini Campos; Gabriel Marques Tavares; Rodrigo Augusto Igawa; Mário Lemes Proença; Rodrigo Capobianco Guido",
    "corresponding_authors": "",
    "abstract": "Social interactions take place in environments that influence people’s behaviours and perceptions. Nowadays, the users of Online Social Network (OSN) generate a massive amount of content based on social interactions. However, OSNs wide popularity and ease of access created a perfect scenario to practice malicious activities, compromising their reliability. To detect automatic information broadcast in OSN, we developed a wavelet-based model that classifies users as being human, legitimate robot, or malicious robot, as a result of spectral patterns obtained from users’ textual content. We create the feature vector from the Discrete Wavelet Transform along with a weighting scheme called Lexicon-based Coefficient Attenuation. In particular, we induce a classification model using the Random Forest algorithm over two real Twitter datasets. The corresponding results show the developed model achieved an average accuracy of 94.47% considering two different scenarios: single theme and miscellaneous one.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W2794977433",
    "type": "article"
  },
  {
    "title": "Aesthetic Highlight Detection in Movies Based on Synchronization of Spectators’ Reactions",
    "doi": "https://doi.org/10.1145/3175497",
    "publication_date": "2018-07-24",
    "publication_year": 2018,
    "authors": "Michał Muszyński; Θεόδωρος Κωστούλας; Patrizia Lombardo; Thierry Pun; Guillaume Chanel",
    "corresponding_authors": "",
    "abstract": "Detection of aesthetic highlights is a challenge for understanding the affective processes taking place during movie watching. In this article, we study spectators’ responses to movie aesthetic stimuli in a social context. Moreover, we look for uncovering the emotional component of aesthetic highlights in movies. Our assumption is that synchronized spectators’ physiological and behavioral reactions occur during these highlights because: ( i ) aesthetic choices of filmmakers are made to elicit specific emotional reactions (e.g., special effects, empathy, and compassion toward a character) and ( ii ) watching a movie together causes spectators’ affective reactions to be synchronized through emotional contagion. We compare different approaches to estimation of synchronization among multiple spectators’ signals, such as pairwise, group, and overall synchronization measures to detect aesthetic highlights in movies. The results show that the unsupervised architecture relying on synchronization measures is able to capture different properties of spectators’ synchronization and detect aesthetic highlights based on both spectators’ electrodermal and acceleration signals. We discover that pairwise synchronization measures perform the most accurately independently of the category of the highlights and movie genres. Moreover, we observe that electrodermal signals have more discriminative power than acceleration signals for highlight detection.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W2884988226",
    "type": "article"
  },
  {
    "title": "Gait Recognition from Motion Capture Data",
    "doi": "https://doi.org/10.1145/3152124",
    "publication_date": "2018-02-21",
    "publication_year": 2018,
    "authors": "Michal Balážia; Petr Sojka",
    "corresponding_authors": "",
    "abstract": "Gait recognition from motion capture data, as a pattern classification discipline, can be improved by the use of machine learning. This article contributes to the state of the art with a statistical approach for extracting robust gait features directly from raw data by a modification of Linear Discriminant Analysis with Maximum Margin Criterion. Experiments on the CMU MoCap database show that the suggested method outperforms 13 relevant methods based on geometric features and a method to learn the features by a combination of Principal Component Analysis and Linear Discriminant Analysis. The methods are evaluated in terms of the distribution of biometric templates in respective feature spaces expressed in a number of class separability coefficients and classification metrics. Results also indicate a high portability of learned features, what means that we can learn what aspects of walk people generally differ in and extract those as general gait features. Recognizing people without needing group-specific features is convenient, as particular people might not always provide annotated learning data. As a contribution to reproducible research, our evaluation framework and database have been made publicly available. This research makes motion capture technology directly applicable for human recognition.",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W2747644948",
    "type": "article"
  },
  {
    "title": "A Quality of Experience Model for Haptic Virtual Environments",
    "doi": "https://doi.org/10.1145/2540991",
    "publication_date": "2014-04-01",
    "publication_year": 2014,
    "authors": "Abdelwahab Hamam; Abdulmotaleb El Saddik; Jihad Mohamad Alja’am",
    "corresponding_authors": "",
    "abstract": "Haptic-based Virtual Reality (VR) applications have many merits. What is still obscure, from the designer's perspective of these applications, is the experience the users will undergo when they use the VR system. Quality of Experience (QoE) is an evaluation metric from the user's perspective that unfortunately has received limited attention from the research community. Assessing the QoE of VR applications reflects the amount of overall satisfaction and benefits gained from the application in addition to laying the foundation for ideal user-centric design in the future. In this article, we propose a taxonomy for the evaluation of QoE for multimedia applications and in particular VR applications. We model this taxonomy using a Fuzzy logic Inference System (FIS) to quantitatively measure the QoE of haptic virtual environments. We build and test our FIS by conducting a users' study analysis to evaluate the QoE of a haptic game application. Our results demonstrate that the proposed FIS model reflects the user's estimation of the application's quality significantly with low error and hence is suited for QoE evaluation.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W2036272305",
    "type": "article"
  },
  {
    "title": "Cross-Platform Emerging Topic Detection and Elaboration from Multimedia Streams",
    "doi": "https://doi.org/10.1145/2730889",
    "publication_date": "2015-06-02",
    "publication_year": 2015,
    "authors": "Bing‐Kun Bao; Changsheng Xu; Weiqing Min; M. Shamim Hossain",
    "corresponding_authors": "",
    "abstract": "With the explosive growth of online media platforms in recent years, it becomes more and more attractive to provide users a solution of emerging topic detection and elaboration. And this posts a real challenge to both industrial and academic researchers because of the overwhelming information available in multiple modalities and with large outlier noises. This article provides a method on emerging topic detection and elaboration using multimedia streams cross different online platforms. Specifically, Twitter, New York Times and Flickr are selected for the work to represent the microblog, news portal and imaging sharing platforms. The emerging keywords of Twitter are firstly extracted using aging theory. Then, to overcome the nature of short length message in microblog, Robust Cross-Platform Multimedia Co-Clustering (RCPMM-CC) is proposed to detect emerging topics with three novelties: 1) The data from different media platforms are in multimodalities; 2) The coclustering is processed based on a pairwise correlated structure, in which the involved three media platforms are pairwise dependent; 3) The noninformative samples are automatically pruned away at the same time of coclustering. In the last step of cross-platform elaboration, we enrich each emerging topic with the samples from New York Times and Flickr by computing the implicit links between social topics and samples from selected news and Flickr image clusters, which are obtained by RCPMM-CC. Qualitative and quantitative evaluation results demonstrate the effectiveness of our method.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W2240167975",
    "type": "article"
  },
  {
    "title": "An Efficient Framework for Compressed Domain Watermarking in P Frames of High-Efficiency Video Coding (HEVC)--Encoded Video",
    "doi": "https://doi.org/10.1145/3002178",
    "publication_date": "2017-01-17",
    "publication_year": 2017,
    "authors": "Tanima Dutta; Hari Prabhat Gupta",
    "corresponding_authors": "",
    "abstract": "Digital watermarking has received much attention in recent years as a promising solution to copyright protection. Video watermarking in compressed domain has gained importance since videos are stored and transmitted in a compressed format. This decreases the overhead to fully decode and re-encode the video for embedding and extraction of the watermark. High Efficiency Video Coding (HEVC/H.265) is the latest and most efficient video compression standard and a successor to H.264 Advanced Video Coding. In this article, we propose a robust watermarking framework for HEVC-encoded video using informed detector. A readable watermark is embedded invisibly in P frames for better perceptual quality. Our framework imposes security and robustness by selecting appropriate blocks using a random key and the spatio-temporal characteristics of the compressed video. A detail analysis of the strengths of different compressed domain features is performed for implementing the watermarking framework. We experimentally demonstrate the utility of the proposed work. The results show that the proposed work effectively limits the increase in video bitrate and degradation in perceptual quality. The proposed framework is robust against re-encoding and image processing attacks.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W2576671951",
    "type": "article"
  },
  {
    "title": "When Smart Devices Interact With Pervasive Screens",
    "doi": "https://doi.org/10.1145/3115933",
    "publication_date": "2017-08-12",
    "publication_year": 2017,
    "authors": "Pai Chet Ng; James She; Kang Eun Jeon; Matthias Baldauf",
    "corresponding_authors": "",
    "abstract": "The meeting of pervasive screens and smart devices has witnessed the birth of screen-smart device interaction (SSI), a key enabler to many novel interactive use cases. Most current surveys focus on direct human-screen interaction, and to the best of our knowledge, none have studied state-of-the-art SSI. This survey identifies three core elements of SSI and delivers a timely discussion on SSI oriented around the screen, the smart device, and the interaction modality. Two evaluation metrics (i.e., interaction latency and accuracy) have been adopted and refined to match the evaluation criterion of SSI. The bottlenecks that hinder the further advancement of the current SSI in connection with this metrics are studied. Last, future research challenges and opportunities are highlighted in the hope of inspiring continuous research efforts to realize the next generation of SSI.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W2747574631",
    "type": "article"
  },
  {
    "title": "Efficient Image Hashing with Geometric Invariant Vector Distance for Copy Detection",
    "doi": "https://doi.org/10.1145/3355394",
    "publication_date": "2019-11-30",
    "publication_year": 2019,
    "authors": "Shiguang Liu; Ziqing Huang",
    "corresponding_authors": "",
    "abstract": "Hashing method is an efficient technique of multimedia security for content protection. It maps an image into a content-based compact code for denoting the image itself. While most existing algorithms focus on improving the classification between robustness and discrimination, little attention has been paid to geometric invariance under normal digital operations, and therefore results in quite fragile to geometric distortion when applied in image copy detection. In this article, a novel effective image hashing method is proposed based on geometric invariant vector distance in both spatial domain and frequency domain. First, the image is preprocessed by some joint operations to extract robust features. Then, the preprocessed image is randomly divided into several overlapping blocks under a secret key, and two different feature matrices are separately obtained in the spatial domain and frequency domain through invariant moment and low frequency discrete cosine transform coefficients. Furthermore, the invariant distances between vectors in feature matrices are calculated and quantified to form a compact hash code. We conduct various experiments to demonstrate that the proposed hashing not only reaches good classification between robustness and discrimination, but also resists most geometric distortion in image copy detection. In addition, both receiver operating characteristics curve comparisons and mean average precision in copy detection clearly illustrate that the proposed hashing method outperforms state-of-the-art algorithms.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W3018599631",
    "type": "article"
  },
  {
    "title": "Semantic Photo Retargeting Under Noisy Image Labels",
    "doi": "https://doi.org/10.1145/2886775",
    "publication_date": "2016-05-20",
    "publication_year": 2016,
    "authors": "Luming Zhang; Xuelong Li; Liqiang Nie; Yan Yan; Roger Zimmermann",
    "corresponding_authors": "",
    "abstract": "With the popularity of mobile devices, photo retargeting has become a useful technique that adapts a high-resolution photo onto a low-resolution screen. Conventional approaches are limited in two aspects. The first factor is the de-emphasized role of semantic content that is many times more important than low-level features in photo aesthetics. Second is the importance of image spatial modeling: toward a semantically reasonable retargeted photo, the spatial distribution of objects within an image should be accurately learned. To solve these two problems, we propose a new semantically aware photo retargeting that shrinks a photo according to region semantics. The key technique is a mechanism transferring semantics of noisy image labels (inaccurate labels predicted by a learner like an SVM) into different image regions. In particular, we first project the local aesthetic features (graphlets in this work) onto a semantic space, wherein image labels are selectively encoded according to their noise level. Then, a category-sharing model is proposed to robustly discover the semantics of each image region. The model is motivated by the observation that the semantic distribution of graphlets from images tagged by a common label remains stable in the presence of noisy labels. Thereafter, a spatial pyramid is constructed to hierarchically encode the spatial layout of graphlet semantics. Based on this, a probabilistic model is proposed to enforce the spatial layout of a retargeted photo to be maximally similar to those from the training photos. Experimental results show that (1) noisy image labels predicted by different learners can improve the retargeting performance, according to both qualitative and quantitative analysis, and (2) the category-sharing model stays stable even when 32.36% of image labels are incorrectly predicted.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W2408139116",
    "type": "article"
  },
  {
    "title": "Measuring Individual Video QoE",
    "doi": "https://doi.org/10.1145/3183512",
    "publication_date": "2018-04-30",
    "publication_year": 2018,
    "authors": "Yi Zhu; Sharath Chandra Guntuku; Weisi Lin; Gheorghiţă Ghinea; Judith Redi",
    "corresponding_authors": "",
    "abstract": "The next generation of multimedia services have to be optimized in a personalized way, taking user factors into account for the evaluation of individual experience. Previous works have investigated the influence of user factors mostly in a controlled laboratory environment which often includes a limited number of users and fails to reflect real-life environment. Social media, especially Facebook, provide an interesting alternative for Internet-based subjective evaluation. In this article, we develop (and open-source) a Facebook application, named YouQ 1 , as an experimental platform for studying individual experience for videos. Our results show that subjective experiments based on YouQ can produce reliable results as compared to a controlled laboratory experiment. Additionally, YouQ has the ability to collect user information automatically from Facebook, which can be used for modeling individual experience.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W2802543148",
    "type": "article"
  },
  {
    "title": "Synthesizing Facial Photometries and Corresponding Geometries Using Generative Adversarial Networks",
    "doi": "https://doi.org/10.1145/3337067",
    "publication_date": "2019-10-15",
    "publication_year": 2019,
    "authors": "Gil Shamai; Ron Slossberg; Ron Kimmel",
    "corresponding_authors": "",
    "abstract": "Artificial data synthesis is currently a well-studied topic with useful applications in data science, computer vision, graphics, and many other fields. Generating realistic data is especially challenging, since human perception is highly sensitive to non-realistic appearance. In recent times, new levels of realism have been achieved by advances in GAN training procedures and architectures. These successful models, however, are tuned mostly for use with regularly sampled data such as images, audio, and video. Despite the successful application of the architecture on these types of media, applying the same tools to geometric data poses a far greater challenge. The study of geometric deep learning is still a debated issue within the academic community, as the lack of intrinsic parametrization inherent to geometric objects prohibits the direct use of convolutional filters, a main building block of today’s machine learning systems. In this article, we propose a new method for generating realistic human facial geometries coupled with overlayed textures. We circumvent the parametrization issue by utilizing a specialized non-rigid alignment procedure, and imposing a global mapping from our data to the unit rectangle. This mapping enables the representation of our geometric data as regularly sampled 2D images. We further discuss how to design such a mapping to control the distortion and conserve area within the target image. By representing geometric textures and geometries as images, we are able to use advanced GAN methodologies to generate new plausible textures and geometries. We address the often-neglected topic of relationship between texture and geometry and propose different methods for fitting generated geometries to generated textures. In addition, we widen the scope of our discussion and offer a new method for training GAN models on partially corrupted data. Finally, we provide empirical evidence demonstrating our generative model’s ability to produce examples of new facial identities, independent from the training data, while maintaining a high level of realism—two traits that are often at odds.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W3019406411",
    "type": "article"
  },
  {
    "title": "Device-to-Device Communication for Mobile Multimedia in Emerging 5G Networks",
    "doi": "https://doi.org/10.1145/2983641",
    "publication_date": "2016-09-21",
    "publication_year": 2016,
    "authors": "Jiajia Liu; Nei Kato; Hirotaka Ujikawa; Ken-Ichi Suzuki",
    "corresponding_authors": "",
    "abstract": "Device-to-device (D2D) communication, which utilizes mobile devices located within close proximity for direct connection and data exchange, holds great promise for improving energy and spectrum efficiency of mobile multimedia in 5G networks. It has been observed that most available D2D-based works—considered only the single-cell scenario with a single BS. Such scenario-based schemes, although tractable and able to illustrate the relationship between D2D links and cellular links, failed to take into account the distribution of surrounding base stations and user equipments (UEs), as well as the accumulated interference from ongoing transmissions in other cells. Furthermore, the single-tier network with one BS considered in available works is far from the real 5G scenario in which multi-tier BSs are heterogeneously distributed among the whole network area. In light of such observations, we present in this article a model for network coverage probability and average rate analysis in a D2D communication overlaying a two-tier downlink cellular network, where nineteen macro base stations (MBSs) with pico base stations (PBSs) placed at the end point of macro cell (hexagons) borders are employed according to the 3GPP specifications, and mobile users are spatially distributed according to the homogeneous Poisson Point Process model. Each mobile UE is able to establish a D2D link with adjacent UEs or connect to a nearby macro or pico base station. Stochastic geometric analysis is adopted to characterize the intratier interference distribution within the MBS-tier, PBS-tier, and D2D-tier based on which network coverage probability and per-user average rate are derived with a careful consideration of important issues such as threshold value, SINR value, user density, content hit rate, spectrum allocation, and cell coverage range. Our results show that, even for the overlaying case, D2D communication can significantly improve network coverage probability and per-user average downlink rate. Another finding is that the frequency allocation for D2D communications should be carefully tuned according to network settings, which may result in totally different varying behaviors for the per-user average rate.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W2521280776",
    "type": "article"
  },
  {
    "title": "Quality of Experience-Centric Management of Adaptive Video Streaming Services",
    "doi": "https://doi.org/10.1145/3165266",
    "publication_date": "2018-04-30",
    "publication_year": 2018,
    "authors": "Stefano Petrangeli; Jeroen van der Hooft; Tim Wauters; Filip De Turck",
    "corresponding_authors": "",
    "abstract": "Video streaming applications currently dominate Internet traffic. Particularly, HTTP Adaptive Streaming (HAS) has emerged as the dominant standard for streaming videos over the best-effort Internet, thanks to its capability of matching the video quality to the available network resources. In HAS, the video client is equipped with a heuristic that dynamically decides the most suitable quality to stream the content, based on information such as the perceived network bandwidth or the video player buffer status. The goal of this heuristic is to optimize the quality as perceived by the user, the so-called Quality of Experience (QoE). Despite the many advantages brought by the adaptive streaming principle, optimizing users’ QoE is far from trivial. Current heuristics are still suboptimal when sudden bandwidth drops occur, especially in wireless environments, thus leading to freezes in the video playout, the main factor influencing users’ QoE. This issue is aggravated in case of live events, where the player buffer has to be kept as small as possible in order to reduce the playout delay between the user and the live signal. In light of the above, in recent years, several works have been proposed with the aim of extending the classical purely client-based structure of adaptive video streaming, in order to fully optimize users’ QoE. In this article, a survey is presented of research works on this topic together with a classification based on where the optimization takes place. This classification goes beyond client-based heuristics to investigate the usage of server- and network-assisted architectures and of new application and transport layer protocols. In addition, we outline the major challenges currently arising in the field of multimedia delivery, which are going to be of extreme relevance in future years.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W2801125139",
    "type": "article"
  },
  {
    "title": "Designing and Evaluating a Mesh Simplification Algorithm for Virtual Reality",
    "doi": "https://doi.org/10.1145/3209661",
    "publication_date": "2018-06-27",
    "publication_year": 2018,
    "authors": "Kanchan Bahirat; Chengyuan Lai; Ryan P. McMahan; Balakrishnan Prabhakaran",
    "corresponding_authors": "",
    "abstract": "With the increasing accessibility of the mobile head-mounted displays (HMDs), mobile virtual reality (VR) systems are finding applications in various areas. However, mobile HMDs are highly constrained with limited graphics processing units (GPUs) and low processing power and onboard memory. Hence, VR developers must be cognizant of the number of polygons contained within their virtual environments to avoid rendering at low frame rates and inducing simulator sickness. The most robust and rapid approach to keeping the overall number of polygons low is to use mesh simplification algorithms to create low-poly versions of pre-existing, high-poly models. Unfortunately, most existing mesh simplification algorithms cannot adequately handle meshes with lots of boundaries or nonmanifold meshes, which are common attributes of many 3D models. In this article, we present QEM 4VR , a high-fidelity mesh simplification algorithm specifically designed for VR. This algorithm addresses the deficiencies of prior quadric error metric (QEM) approaches by leveraging the insight that the most relevant boundary edges lie along curvatures while linear boundary edges can be collapsed. Additionally, our algorithm preserves key surface properties, such as normals, texture coordinates, colors, and materials, as it preprocesses 3D models and generates their low-poly approximations offline. We evaluated the effectiveness of our QEM 4VR algorithm by comparing its simplified-mesh results to those of prior QEM variations in terms of geometric approximation error, texture error, progressive approximation errors, frame rate impact, and perceptual quality measures. We found that QEM 4VR consistently yielded simplified meshes with less geometric approximation error and texture error than the prior QEM variations. It afforded better frame rates than QEM variations with boundary preservation constraints that create unnecessary lower bounds on overall polygon count reduction. Our evaluation revealed that QEM 4VR did not fair well in terms of existing perceptual distance measurements, but human-based inspections demonstrate that these algorithmic measurements are not suitable substitutes for actual human perception. In turn, we present a user-based methodology for evaluating the perceptual qualities of mesh simplification algorithms.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W2810764940",
    "type": "article"
  },
  {
    "title": "Modality-Invariant Image-Text Embedding for Image-Sentence Matching",
    "doi": "https://doi.org/10.1145/3300939",
    "publication_date": "2019-02-07",
    "publication_year": 2019,
    "authors": "Ruoyu Liu; Yao Zhao; Shikui Wei; Liang Zheng; Yi Yang",
    "corresponding_authors": "",
    "abstract": "Performing direct matching among different modalities (like image and text) can benefit many tasks in computer vision, multimedia, information retrieval, and information fusion. Most of existing works focus on class-level image-text matching, called cross-modal retrieval , which attempts to propose a uniform model for matching images with all types of texts, for example, tags, sentences, and articles (long texts). Although cross-model retrieval alleviates the heterogeneous gap among visual and textual information, it can provide only a rough correspondence between two modalities. In this article, we propose a more precise image-text embedding method, image-sentence matching, which can provide heterogeneous matching in the instance level. The key issue for image-text embedding is how to make the distributions of the two modalities consistent in the embedding space. To address this problem, some previous works on the cross-model retrieval task have attempted to pull close their distributions by employing adversarial learning. However, the effectiveness of adversarial learning on image-sentence matching has not been proved and there is still not an effective method. Inspired by previous works, we propose to learn a modality-invariant image-text embedding for image-sentence matching by involving adversarial learning. On top of the triplet loss--based baseline, we design a modality classification network with an adversarial loss, which classifies an embedding into either the image or text modality. In addition, the multi-stage training procedure is carefully designed so that the proposed network not only imposes the image-text similarity constraints by ground-truth labels, but also enforces the image and text embedding distributions to be similar by adversarial learning. Experiments on two public datasets (Flickr30k and MSCOCO) demonstrate that our method yields stable accuracy improvement over the baseline model and that our results compare favorably to the state-of-the-art methods.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W2912738862",
    "type": "article"
  },
  {
    "title": "Audio Masking Effect on Inter-Component Skews in Olfaction-Enhanced Multimedia Presentations",
    "doi": "https://doi.org/10.1145/2957753",
    "publication_date": "2016-08-03",
    "publication_year": 2016,
    "authors": "Oluwakemi A. Ademoye; Niall Murray; Gabriel‐Miro Muntean; Gheorghiță Ghinea",
    "corresponding_authors": "",
    "abstract": "Media-rich content plays a vital role in consumer applications today, as these applications try to find new and interesting ways to engage their users. Video, audio, and the more traditional forms of media content continue to dominate with respect to the use of media content to enhance the user experience. Tactile interactivity has also now become widely popular in modern computing applications, while our olfactory and gustatory senses continue to have a limited role. However, in recent times, there have been significant advancements regarding the use of olfactory media content (i.e., smell), and there are a variety of devices now available to enable its computer-controlled emission. This paper explores the impact of the audio stream on user perception of olfactory-enhanced video content in the presence of skews between the olfactory and video media. This research uses the results from two experimental studies of user-perceived quality of olfactory-enhanced multimedia, where audio was present and absent, respectively. Specifically, the paper shows that the user Quality of Experience (QoE) is generally higher in the absence of audio for nearly perfect synchronized olfactory-enhanced multimedia presentations (i.e., an olfactory media skew of between {−10,+10s}); however, for greater olfactory media skews (ranging between {−30s;−10s} and {+10s, +30s}) user QoE is higher when the audio stream is present. It can be concluded that the presence of the audio has the ability to mask larger synchronization skews between the other media components in olfaction-enhanced multimedia presentations.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2497779771",
    "type": "article"
  },
  {
    "title": "HTTP/2-based Frame Discarding for Low-Latency Adaptive Video Streaming",
    "doi": "https://doi.org/10.1145/3280854",
    "publication_date": "2019-02-07",
    "publication_year": 2019,
    "authors": "Mariem Ben Yahia; Yannick Le Louédec; Gwendal Simon; Loutfi Nuaymi; Xavier Corbillon",
    "corresponding_authors": "",
    "abstract": "In this article, we propose video delivery schemes insuring around 1s delivery latency with Dynamic Adaptive Streaming over HTTP (DASH), which is a standard version of HTTP Live Streaming (HLS), so as to benefit from the video representation switching between successive video segments. We also propose HTTP/2-based algorithms to apply video frame discarding policies inside a video segment when a selected DASH representation does not match with the available network resources. The current solutions with small buffer suffer from rebuffering events. Rebuffering not only impacts the Quality of Experience (QoE) but also increases the delivery delay between the displayed and the original video streams. In this work, we completely eliminate rebuffering events by developing optimal and practical video frame discarding algorithms to meet the 1s latency constraint. In all our algorithms, we request the video frames individually through HTTP/2 multiple streams, and we selectively drop the least meaningful video frames thanks to HTTP/2 stream resetting feature. Our simulations show that the proposed algorithms eliminate rebuffering while insuring an acceptable video quality with at least a Peak Signal to Noise Ratio (PSNR) of 35dB compared to 25dB of the basic First In First Out (FIFO) algorithm. We also quantify and qualify the resulting temporal distortion of the video segments per algorithm. An important number of missing video frames results in a temporal fluidity break known as video jitter . The displayed video looks like a series of snapshots. We show that both the optimal Integer Linear Program (ILP) and practical algorithms decrease the frequency and duration of the jitters. For example, practical algorithms reduce the number of crashed displayed videos (presenting one jitter longer than 1,350ms) with 22% compared to the basic FIFO algorithm. We also show that requesting video frames separately with HTTP/2 slightly increases the overhead from 4.34% to 5.76%.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2914208805",
    "type": "article"
  },
  {
    "title": "Spatial Structure Preserving Feature Pyramid Network for Semantic Image Segmentation",
    "doi": "https://doi.org/10.1145/3321512",
    "publication_date": "2019-08-31",
    "publication_year": 2019,
    "authors": "Yuan Yuan; Jie Fang; Xiaoqiang Lu; Yachuang Feng",
    "corresponding_authors": "",
    "abstract": "Recently, progress on semantic image segmentation is substantial, benefiting from the rapid development of Convolutional Neural Networks. Semantic image segmentation approaches proposed lately have been mostly based on Fully convolutional Networks (FCNs). However, these FCN-based methods use large receptive fields and too many pooling layers to depict the discriminative semantic information of the images. Specifically, on one hand, convolutional kernel with large receptive field smooth the detailed edges, since too much contexture information is used to depict the “center pixel.” However, the pooling layer increases the receptive field through zooming out the latest feature maps, which loses many detailed information of the image, especially in the deeper layers of the network. These operations often cause low spatial resolution inside deep layers, which leads to spatially fragmented prediction. To address this problem, we exploit the inherent multi-scale and pyramidal hierarchy of deep convolutional networks to extract the feature maps with different resolutions and take full advantages of these feature maps via a gradually stacked fusing way. Specifically, for two adjacent convolutional layers, we upsample the features from deeper layer with stride of 2 and then stack them on the features from shallower layer. Then, a convolutional layer with kernels of 1× 1 is followed to fuse these stacked features. The fused feature preserves the spatial structure information of the image; meanwhile, it owns strong discriminative capability for pixel classification. Additionally, to further preserve the spatial structure information and regional connectivity of the predicted category label map, we propose a novel loss term for the network. In detail, two graph model-based spatial affinity matrixes are proposed, which are used to depict the pixel-level relationships in the input image and predicted category label map respectively, and then their cosine distance is backward propagated to the network. The proposed architecture, called spatial structure preserving feature pyramid network, significantly improves the spatial resolution of the predicted category label map for semantic image segmentation. The proposed method achieves state-of-the-art results on three public and challenging datasets for semantic image segmentation.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2996825526",
    "type": "article"
  },
  {
    "title": "Analysis of the Security of Internet of Multimedia Things",
    "doi": "https://doi.org/10.1145/3398201",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "Zhihan Lv; Liang Qiao; Houbing Song",
    "corresponding_authors": "",
    "abstract": "To study the security performance of the Internet of multimedia things on the privacy protection of user identity, behavior trajectory, and preference under the new information technology industry wave, in this study, aiming at the problems of the sharing of Internet of things perception data and the exposure of users’ privacy information, the Anonymous Batch Authentication Scheme (ABAH) for privacy protection is designed. Hash-based Message Authentication Code is used to cancel the list-checking process and analyze its security performance. Compared with the methods of elliptic curve digital signature algorithm, Bayes least-square method, identity-based bulk verification, anonymous batch authentication and key protocol, conditional privacy authentication scheme, and expert message authentication protocol, the transmission delay, packet loss rate, and computation cost are studied without considering the undo list and during the undo check. The results show that with the increase of information size, the transmission delay and packet loss rate also increase, and the transmission delay of ABAH increases by about 15%, while the correlation between speed and transmission delay is small. In the case of the same amount of validation information, ABAH has the highest validation efficiency, and it still has an efficient validation effect in the case of invalid information. The message packet loss rate for ABAH is always 0 when the undo check validation overhead is considered. It can be found that ABAH can avoid the communication overhead and privacy leakage caused by the revocation list, ensure the integrity of batch verification information, meet the security performance of the vehicular ad hoc network under the Internet of Things, and protect the privacy of users from being disclosed.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W3041038663",
    "type": "article"
  },
  {
    "title": "An LSH-based Offloading Method for IoMT Services in Integrated Cloud-Edge Environment",
    "doi": "https://doi.org/10.1145/3408319",
    "publication_date": "2020-10-31",
    "publication_year": 2020,
    "authors": "Xiaolong Xu; Qihe Huang; Yiwen Zhang; Shancang Li; Lianyong Qi; Wanchun Dou",
    "corresponding_authors": "",
    "abstract": "Benefiting from the massive available data provided by Internet of multimedia things (IoMT), enormous intelligent services requiring information of various types to make decisions are emerging. Generally, the IoMT devices are equipped with limited computing power, interfering with the process of computation-intensive services. Currently, to satisfy a wide range of service requirements, the novel computing paradigms, i.e., cloud computing and edge computing, can potentially be integrated for service accommodation. Nevertheless, the private information (i.e., location, service type, etc.) in the services is prone to spilling out during service offloading in the cloud-edge computing. To avoid privacy leakage while improving service utility, including the service response time and energy consumption for service executions, a &lt;underline&gt;L&lt;/underline&gt;ocality-sensitive-hash (LSH)-based &lt;underline&gt;o&lt;/underline&gt;ffloading &lt;underline&gt;m&lt;/underline&gt;ethod, named LOM, is devised. Specifically, LSH is leveraged to encrypt the feature information for the services offloaded to the edge servers with the intention of privacy preservation. Eventually, comparative experiments are conducted to verify the effectiveness of LOM with respect to promoting service utility.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W3032185305",
    "type": "article"
  },
  {
    "title": "Constrained LSTM and Residual Attention for Image Captioning",
    "doi": "https://doi.org/10.1145/3386725",
    "publication_date": "2020-07-05",
    "publication_year": 2020,
    "authors": "Liang Yang; Haifeng Hu; Songlong Xing; Xinlong Lu",
    "corresponding_authors": "",
    "abstract": "Visual structure and syntactic structure are essential in images and texts, respectively. Visual structure depicts both entities in an image and their interactions, whereas syntactic structure in texts can reflect the part-of-speech constraints between adjacent words. Most existing methods either use visual global representation to guide the language model or generate captions without considering the relationships of different entities or adjacent words. Thus, their language models lack relevance in both visual and syntactic structure. To solve this problem, we propose a model that aligns the language model to certain visual structure and also constrains it with a specific part-of-speech template. In addition, most methods exploit the latent relationship between words in a sentence and pre-extracted visual regions in an image yet ignore the effects of unextracted regions on predicted words. We develop a residual attention mechanism to simultaneously focus on the pre-extracted visual objects and unextracted regions in an image. Residual attention is capable of capturing precise regions of an image corresponding to the predicted words considering both the effects of visual objects and unextracted regions. The effectiveness of our entire framework and each proposed module are verified on two classical datasets: MSCOCO and Flickr30k. Our framework is on par with or even better than the state-of-the-art methods and achieves superior performance on COCO captioning Leaderboard.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W3082436432",
    "type": "article"
  },
  {
    "title": "A Fast View Synthesis Implementation Method for Light Field Applications",
    "doi": "https://doi.org/10.1145/3459098",
    "publication_date": "2021-11-12",
    "publication_year": 2021,
    "authors": "Wei Gao; Linjie Zhou; Lvfang Tao",
    "corresponding_authors": "",
    "abstract": "View synthesis (VS) for light field images is a very time-consuming task due to the great quantity of involved pixels and intensive computations, which may prevent it from the practical three-dimensional real-time systems. In this article, we propose an acceleration approach for deep learning-based light field view synthesis, which can significantly reduce calculations by using compact-resolution (CR) representation and super-resolution (SR) techniques, as well as light-weight neural networks. The proposed architecture has three cascaded neural networks, including a CR network to generate the compact representation for original input views, a VS network to synthesize new views from down-scaled compact views, and a SR network to reconstruct high-quality views with full resolution. All these networks are jointly trained with the integrated losses of CR, VS, and SR networks. Moreover, due to the redundancy of deep neural networks, we use the efficient light-weight strategy to prune filters for simplification and inference acceleration. Experimental results demonstrate that the proposed method can greatly reduce the processing time and become much more computationally efficient with competitive image quality.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W3213461820",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on Recent Trends in Medical Data Security for e-Health Applications",
    "doi": "https://doi.org/10.1145/3459601",
    "publication_date": "2021-05-18",
    "publication_year": 2021,
    "authors": "Amit Kumar Singh; Zhihan Lv; Hoon Ko",
    "corresponding_authors": "",
    "abstract": "introduction Share on Introduction to the Special Issue on Recent Trends in Medical Data Security for e-Health Applications Editors: Amit Kumar Singh Search about this author , Zhihan Lv Search about this author , Hoon Ko Search about this author Authors Info & Claims ACM Transactions on Multimedia Computing, Communications, and ApplicationsVolume 17Issue 2sJune 2021 Article No.: 58pp 1–3https://doi.org/10.1145/3459601Published:18 May 2021Publication History 6citation187DownloadsMetricsTotal Citations6Total Downloads187Last 12 Months106Last 6 weeks24 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W3161667159",
    "type": "article"
  },
  {
    "title": "Moment is Important: Language-Based Video Moment Retrieval via Adversarial Learning",
    "doi": "https://doi.org/10.1145/3478025",
    "publication_date": "2022-02-16",
    "publication_year": 2022,
    "authors": "Yawen Zeng; Da Cao; Shaofei Lu; Hanling Zhang; Xu Jiao; Zheng Qin",
    "corresponding_authors": "",
    "abstract": "The newly emerging language-based video moment retrieval task aims at retrieving a target video moment from an untrimmed video given a natural language as the query. It is more applicable in reality since it is able to accurately localize a specific video moment, as compared to traditional whole video retrieval. In this work, we propose a novel solution to thoroughly investigate the language-based video moment retrieval issue under the adversarial learning. The key of our solution is to formulate the language-based video moment retrieval task as an adversarial learning problem with two tightly connected components. Specifically, a reinforcement learning is employed as a generator to produce a set of possible video moments. Meanwhile, a multi-task learning is utilized as a discriminator, which integrates inter-modal and intra-modal in a unified framework by employing a sequential update strategy. Finally, the generator and the discriminator are mutually reinforced in the adversarial learning, which is able to jointly optimize the performance of both video moment ranking and video moment localization. Extensive experimental results on two challenging benchmarks, i.e., Charades-STA and TACoS datasets, have well demonstrated the effectiveness and rationality of our proposed solution. Meanwhile, on the larger and unbiased datasets, i.e., ActivityNet Captions and ActivityNet-CD, our proposed framework exhibits excellent robustness.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W4212848270",
    "type": "article"
  },
  {
    "title": "Towards Accurate Oriented Object Detection in Aerial Images with Adaptive Multi-level Feature Fusion",
    "doi": "https://doi.org/10.1145/3513133",
    "publication_date": "2022-02-18",
    "publication_year": 2022,
    "authors": "Peining Zhen; Shuqi Wang; Suming Zhang; Xiaotao Yan; Wei Wang; Zhigang Ji; Hai‐Bao Chen",
    "corresponding_authors": "",
    "abstract": "Detecting objects in aerial images is a long-standing and challenging problem since the objects in aerial images vary dramatically in size and orientation. Most existing neural network based methods are not robust enough to provide accurate oriented object detection results in aerial images since they do not consider the correlations between different levels and scales of features. In this paper, we propose a novel two-stage network-based detector with a daptive f eature f usion towards highly accurate oriented object det ection in aerial images, named AFF-Det . First, a multi-scale feature fusion module (MSFF) is built on the top layer of the extracted feature pyramids to mitigate the semantic information loss in the small-scale features. We also propose a cascaded oriented bounding box regression method to transform the horizontal proposals into oriented ones. Then the transformed proposals are assigned to all feature pyramid network (FPN) levels and aggregated by the weighted RoI feature aggregation (WRFA) module. The above modules can adaptively enhance the feature representations in different stages of the network based on the attention mechanism. Finally, a rotated decoupled-RCNN head is introduced to obtain the classification and localization results. Extensive experiments are conducted on the DOTA and HRSC2016 datasets to demonstrate the advantages of our proposed AFF-Det. The best detection results can achieve 80.73% mAP and 90.48% mAP, respectively, on these two datasets, outperforming recent state-of-the-art methods.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W4213251678",
    "type": "article"
  },
  {
    "title": "Deep Illumination-Enhanced Face Super-Resolution Network for Low-Light Images",
    "doi": "https://doi.org/10.1145/3495258",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Kehua Guo; Min Hu; Sheng Ren; Fangfang Li; Jian Zhang; Haifu Guo; Xiaoyan Kui",
    "corresponding_authors": "",
    "abstract": "Face images are typically a key component in the fields of security and criminal investigation. However, due to lighting and shooting angles, faces taken under low-light conditions are often difficult to recognize. Face super-resolution (FSR) technology can restore high-resolution faces based on low-resolution inputs. However, existing face super-resolution methods typically rely on prior knowledge of inaccurate faces estimated from low-resolution images. Faces restored by low-light inputs may suffer from problems such as low brightness and many missing details. In this article, we proposed an Illumination-Enhanced Face Super-Resolution (IEFSR) model that can progressively super-resolve low-light faces of 32 × 32 pixels by an upscaling factor of 8. While reconstructing the low-light low-resolution face into a clear and high-quality face, we introduce a coarse low-resolution (LR) restoration network to recover the LR face details hidden in the dark. In the generator, we use a series of style blocks with noise to make the generated faces appear to have a more realistic visual aesthetic. Additionally, we introduce spectrum normalization in the discriminator to improve training stability. Extensive experimental evaluations show that the proposed IEFSR yields visually and metrically more attractive results than existing state-of-the-art FSR methods.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W4214849322",
    "type": "article"
  },
  {
    "title": "RDH-DES: Reversible Data Hiding over Distributed Encrypted-Image Servers Based on Secret Sharing",
    "doi": "https://doi.org/10.1145/3512797",
    "publication_date": "2022-03-09",
    "publication_year": 2022,
    "authors": "Lizhi Xiong; Xiao Han; Ching‐Nung Yang; Zhihua Xia",
    "corresponding_authors": "",
    "abstract": "Reversible Data Hiding in Encrypted Image (RDHEI) schemes may redistribute the data hiding procedure to other parties and can preserve privacy of the cover image. Recently, cloud computing technology has led to the rapid growth of networked media, and many multimedia rights are owned by multiple parties, such as a film's producer and multiple distributors. Thus, the data hiding task could be distributed to multiple distributed servers. Multi-party data hiding has become an important demand for networked media. In addition, it is essential to preserve multi-server and multi-message privacy and data integrity. However, most of the RDHEI schemes involve only one data hider. That inspired us to design the secure multi-party embedding over distributed encrypted-image servers as a solution for multi-party RDHEI applications. In this article, we propose a novel Reversible Data Hiding over Distributed Encrypted-Image Servers (RDH-DES) based on secret sharing. The Chinese remainder theorem, secret sharing, and block-level scrambling are developed as a lightweight cryptography to generate the encrypted image shares. These shares are distributed to different image servers and are used to embed secret data in the proposed framework. The marked encrypted image can be constructed through the marked encrypted shares from different parties, and the decryption and extraction can be completed by the receiver. The experimental results and theoretical analysis have demonstrated that the proposed scheme is secure and effective.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W4220714597",
    "type": "article"
  },
  {
    "title": "Boosting Scene Graph Generation with Visual Relation Saliency",
    "doi": "https://doi.org/10.1145/3514041",
    "publication_date": "2022-03-17",
    "publication_year": 2022,
    "authors": "Yong Zhang; Yingwei Pan; Ting Yao; Rui Huang; Tao Mei; Chang‐Wen Chen",
    "corresponding_authors": "",
    "abstract": "The scene graph is a symbolic data structure that comprehensively describes the objects and visual relations in a visual scene, while ignoring the inherent perceptual saliency of each visual relation (i.e., relation saliency). However, humans often quickly allocate attention to important/salient visual relations in a scene. To align with such human perception of a scene, we explicitly model the perceptual saliency of visual relation in scene graph by upgrading each graph edge (i.e., visual relation) with an attribute of relation saliency. We present a new design, named as Saliency-guided Message Passing (SMP), that boosts the generation of such scene graph structure with the guidance from the visual relation saliency. Technically, an object interaction encoder is first utilized to strengthen object relation representations by jointly exploiting the appearance, semantic, and spatial relations in between. A branch is further leveraged to estimate the relation saliency of each visual relation by ordinal regression. Next, conditioned on the object and relation features (coupled with the estimated relation saliency), our SMP enhances scene graph generation by performing message passing over the objects and the most salient relations. Extensive experiments on VG-KR and VG150 datasets demonstrate the superiority of SMP for the scene graph generation. Moreover, we empirically validate the compelling generalizability of the learned scene graphs via SMP on downstream tasks like cross-model retrieval and image captioning.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W4220974559",
    "type": "article"
  },
  {
    "title": "Progressive Localization Networks for Language-Based Moment Localization",
    "doi": "https://doi.org/10.1145/3543857",
    "publication_date": "2022-06-11",
    "publication_year": 2022,
    "authors": "Qi Zheng; Jianfeng Dong; Xiaoye Qu; Xun Yang; Yabing Wang; Pan Zhou; Baolong Liu; Xun Wang",
    "corresponding_authors": "",
    "abstract": "This article targets the task of language-based video moment localization. The language-based setting of this task allows for an open set of target activities, resulting in a large variation of the temporal lengths of video moments. Most existing methods prefer to first sample sufficient candidate moments with various temporal lengths, then match them with the given query to determine the target moment. However, candidate moments generated with a fixed temporal granularity may be suboptimal to handle the large variation in moment lengths. To this end, we propose a novel multi-stage Progressive Localization Network (PLN) that progressively localizes the target moment in a coarse-to-fine manner. Specifically, each stage of PLN has a localization branch and focuses on candidate moments that are generated with a specific temporal granularity. The temporal granularities of candidate moments are different across the stages. Moreover, we devise a conditional feature manipulation module and an upsampling connection to bridge the multiple localization branches. In this fashion, the later stages are able to absorb the previously learned information, thus facilitating the more fine-grained localization. Extensive experiments on three public datasets demonstrate the effectiveness of our proposed PLN for language-based moment localization, especially for localizing short moments in long videos.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W4293083879",
    "type": "article"
  },
  {
    "title": "CUR Transformer: A Convolutional Unbiased Regional Transformer for Image Denoising",
    "doi": "https://doi.org/10.1145/3566125",
    "publication_date": "2022-10-11",
    "publication_year": 2022,
    "authors": "Kang Xu; Weixin Li; Xia Wang; Xiaoyan Hu; Ke Yan; Xiaojie Wang; Dong Xuan",
    "corresponding_authors": "",
    "abstract": "Image denoising is a fundamental problem in computer vision and multimedia computation. Non-local filters are effective for image denoising. But existing deep learning methods that use non-local computation structures are mostly designed for high-level tasks, and global self-attention is usually adopted. For the task of image denoising, they have high computational complexity and have a lot of redundant computation of uncorrelated pixels. To solve this problem and combine the marvelous advantages of non-local filter and deep learning, we propose a Convolutional Unbiased Regional (CUR) transformer. Based on the prior that, for each pixel, its similar pixels are usually spatially close, our insights are that (1) we partition the image into non-overlapped windows and perform regional self-attention to reduce the search range of each pixel, and (2) we encourage pixels across different windows to communicate with each other. Based on our insights, the CUR transformer is cascaded by a series of convolutional regional self-attention (CRSA) blocks with U-style short connections. In each CRSA block, we use convolutional layers to extract the query, key, and value features, namely Q , K , and V , of the input feature. Then, we partition the Q , K , and V features into local non-overlapped windows and perform regional self-attention within each window to obtain the output feature of this CRSA block. Among different CRSA blocks, we perform the unbiased window partition by changing the partition positions of the windows. Experimental results show that the CUR transformer outperforms the state-of-the-art methods significantly on four low-level vision tasks, including real and synthetic image denoising, JPEG compression artifact reduction, and low-light image enhancement.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W4304190490",
    "type": "article"
  },
  {
    "title": "A Optimized BERT for Multimodal Sentiment Analysis",
    "doi": "https://doi.org/10.1145/3566126",
    "publication_date": "2022-10-17",
    "publication_year": 2022,
    "authors": "Jun Wu; Tianliang Zhu; Jiahui Zhu; Tianyi Li; Chunzhi Wang",
    "corresponding_authors": "",
    "abstract": "Sentiment analysis of one modality (e.g., text or image) has been broadly studied. However, not much attention has been paid to the sentiment analysis of multi-modal data. As the research on and applications of multi-modal data analysis are becoming more and more broad, it is necessary to optimize BERT internal structure. This article proposes a hierarchical multi-head self-attention and gate channel BERT, which is an optimized BERT model. The model is composed of three modules: the hierarchical multi-head self-attention module realizes the hierarchical extraction process of features; the gate channel module replaces BERT’s original Feed Forward layer to realize information filtering; and the tensor fusion model based on a self-attention mechanism is utilized to implement the fusion process of different modal features. Experiments show that our method achieves promising results and improves accuracy by 5–6% when compared with traditional models on the CMU-MOSI dataset.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W4306408041",
    "type": "article"
  },
  {
    "title": "A Comprehensive Study of Deep Learning-based Covert Communication",
    "doi": "https://doi.org/10.1145/3508365",
    "publication_date": "2022-05-16",
    "publication_year": 2022,
    "authors": "Ashima Anand; Amit Kumar Singh",
    "corresponding_authors": "",
    "abstract": "Deep learning-based methods have been popular in multimedia analysis tasks, including classification, detection, segmentation, and so on. In addition to conventional applications, this model can be widely used for cover communication, i.e., information hiding. This article presents a review of deep learning-based covert communication scheme for protecting digital contents, devices, and models. In particular, we discuss the background knowledge, current applications, and constraints of existing deep learning-based information hiding schemes, identify recent challenges, and highlight possible research directions. Further, major role of deep learning in the area of information hiding are highlighted. Then, the contribution of surveyed scheme is also summarized and compared in the context of estimation of design objectives, approaches, evaluation metric, and weaknesses. We believe that this survey can pave the way to new research in this crucial field of information hiding in deep-learning environment.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W4280547956",
    "type": "article"
  },
  {
    "title": "Learning from Temporal Spatial Cubism for Cross-Dataset Skeleton-based Action Recognition",
    "doi": "https://doi.org/10.1145/3472722",
    "publication_date": "2022-02-16",
    "publication_year": 2022,
    "authors": "Yansong Tang; Xingyu Liu; Xumin Yu; Danyang Zhang; Jiwen Lu; Jie Zhou",
    "corresponding_authors": "",
    "abstract": "Rapid progress and superior performance have been achieved for skeleton-based action recognition recently. In this article, we investigate this problem under a cross-dataset setting, which is a new, pragmatic, and challenging task in real-world scenarios. Following the unsupervised domain adaptation (UDA) paradigm, the action labels are only available on a source dataset, but unavailable on a target dataset in the training stage. Different from the conventional adversarial learning-based approaches for UDA, we utilize a self-supervision scheme to reduce the domain shift between two skeleton-based action datasets. Our inspiration is drawn from Cubism, an art genre from the early 20th century, which breaks and reassembles the objects to convey a greater context. By segmenting and permuting temporal segments or human body parts, we design two self-supervised learning classification tasks to explore the temporal and spatial dependency of a skeleton-based action and improve the generalization ability of the model. We conduct experiments on six datasets for skeleton-based action recognition, including three large-scale datasets (NTU RGB+D, PKU-MMD, and Kinetics) where new cross-dataset settings and benchmarks are established. Extensive results demonstrate that our method outperforms state-of-the-art approaches. The source codes of our model and all the compared methods are available at https://github.com/shanice-l/st-cubism.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W4283396563",
    "type": "article"
  },
  {
    "title": "Exploiting Manifold Feature Representation for Efficient Classification of 3D Point Clouds",
    "doi": "https://doi.org/10.1145/3539611",
    "publication_date": "2022-07-29",
    "publication_year": 2022,
    "authors": "Dinghao Yang; Wei Gao; Ge Li; Hui Yuan; Junhui Hou; Sam Kwong",
    "corresponding_authors": "",
    "abstract": "In this paper, we propose an efficient point cloud classification method via manifold learning based feature representation. Different from conventional methods, we use manifold learning algorithms to embed point cloud features for better considering the geometric continuity on the surface. Then, the nature of point cloud can be acquired in low dimensional space, and after being concatenated with features in the original three-dimensional (3D) space, both the capability of feature representation and the classification network performance can be improved. We explore three traditional manifold algorithms (i.e., Isomap, Locally-Linear Embedding, and Laplacian eigenmaps) in detail, and finally, we select the Locally-Linear Embedding (LLE) algorithm due to its low complexity and locality consistency preservation. Furthermore, we propose a neural network based manifold learning (NNML) method to implement manifold learning based non-linear projection. Experiments demonstrate that the proposed two manifold learning methods can obtain better performances than the state-of-the-art methods, and the obtained mean class accuracy (mA) and overall accuracy (oA) can reach 91.4% and 94.4%, respectively. Moreover, because of the improved feature learning capability, the proposed NNML method can also have better classification accuracy on models with prominent geometric shapes. To further demonstrate the advantages of PointManifold, we extend it as a plug and play method for point cloud classification task, which can be directly used with existing methods and gain a significant improvement.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W4288705760",
    "type": "article"
  },
  {
    "title": "Deep Self-Supervised Hyperspectral Image Reconstruction",
    "doi": "https://doi.org/10.1145/3510373",
    "publication_date": "2022-08-23",
    "publication_year": 2022,
    "authors": "Zhe Liu; Xian‐Hua Han",
    "corresponding_authors": "",
    "abstract": "Reconstructing a high-resolution hyperspectral (HR-HS) image via merging a low-resolution hyperspectral (LR-HS) image and a high-resolution RGB (HR-RGB) image has become a hot research topic, and can greatly benefit for different subsequent high-level vision tasks. Recently, deep learning–based approaches have evolved for HS image reconstruction and validated impressive performance. However, to learn a good reconstruction model in the deep learning–based methods, it is mandatory to previously collect large-scale training triplets consisting of the LR-HS, HR-RGB, and HR-HS images, which is difficult to be collected in real applications. This study proposes a deep self-supervised HS image reconstruction framework (DSSH), which does not have to depend on any handcrafted prior and previously collected training triplets at all. The proposed DSSH method leverages the designed network architecture itself for capturing the prior of the underlying structure in the latent HR-HS image and employs the observed LR-HS and HR-RGB images only for network parameter learning. Experiments on two benchmark HS image datasets validated that the proposed DSSH method manifests very impressive reconstruction performance, and is even better than some state-of-the-art supervised learning approaches.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W4292737915",
    "type": "article"
  },
  {
    "title": "Multi-Guidance CNNs for Salient Object Detection",
    "doi": "https://doi.org/10.1145/3570507",
    "publication_date": "2022-11-03",
    "publication_year": 2022,
    "authors": "Shuaixiong Hui; Qiang Guo; Xiaoyu Geng; Caiming Zhang",
    "corresponding_authors": "",
    "abstract": "Feature refinement and feature fusion are two key steps in convolutional neural networks–based salient object detection (SOD). In this article, we investigate how to utilize multiple guidance mechanisms to better refine and fuse extracted multi-level features and propose a novel multi-guidance SOD model dubbed as MGuid-Net. Since boundary information is beneficial for locating and sharpening salient objects, edge features are utilized in our network together with saliency features for SOD. Specifically, a self-guidance module is applied to multi-level saliency features and edge features, respectively, which aims to gradually guide the refinement of lower-level features by higher-level features. After that, a cross-guidance module is devised to mutually refine saliency features and edge features via the complementarity between them. Moreover, to better integrate refined multi-level features, we also present an accumulative guidance module, which exploits multiple high-level features to guide the fusion of different features in a hierarchical manner. Finally, a pixelwise contrast loss function is adopted as an implicit guidance to help our network retain more details in salient objects. Extensive experiments on five benchmark datasets demonstrate our model can identify salient regions of an image more effectively compared to most of state-of-the-art models.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W4308449898",
    "type": "article"
  },
  {
    "title": "Hybrid Modality Metric Learning for Visible-Infrared Person Re-Identification",
    "doi": "https://doi.org/10.1145/3473341",
    "publication_date": "2022-01-25",
    "publication_year": 2022,
    "authors": "Zhang La; Haiyun Guo; Kuan Zhu; Honglin Qiao; Gaopan Huang; Sen Zhang; Huichen Zhang; Jian Sun; Jinqiao Wang",
    "corresponding_authors": "",
    "abstract": "Visible-infrared person re-identification (Re-ID) has received increasing research attention for its great practical value in night-time surveillance scenarios. Due to the large variations in person pose, viewpoint, and occlusion in the same modality, as well as the domain gap brought by heterogeneous modality, this hybrid modality person matching task is quite challenging. Different from the metric learning methods for visible person re-ID, which only pose similarity constraints on class level, an efficient metric learning approach for visible-infrared person Re-ID should take both the class-level and modality-level similarity constraints into full consideration to learn sufficiently discriminative and robust features. In this article, the hybrid modality is divided into two types, within modality and cross modality. We first fully explore the variations that hinder the ranking results of visible-infrared person re-ID and roughly summarize them into three types: within-modality variation, cross-modality modality-related variation, and cross-modality modality-unrelated variation. Then, we propose a comprehensive metric learning framework based on four kinds of paired-based similarity constraints to address all the variations within and cross modality. This framework focuses on both class-level and modality-level similarity relationships between person images. Furthermore, we demonstrate the compatibility of our framework with any paired-based loss functions by giving detailed implementation of combing it with triplet loss and contrastive loss separately. Finally, extensive experiments of our approach on SYSU-MM01 and RegDB demonstrate the effectiveness and superiority of our proposed metric learning framework for visible-infrared person Re-ID.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W4206903995",
    "type": "article"
  },
  {
    "title": "ECCNAS: Efficient Crowd Counting Neural Architecture Search",
    "doi": "https://doi.org/10.1145/3465455",
    "publication_date": "2022-01-25",
    "publication_year": 2022,
    "authors": "Yabin Wang; Zhiheng Ma; Xing Wei; Shuai Zheng; Yaowei Wang; Xiaopeng Hong",
    "corresponding_authors": "",
    "abstract": "Recent solutions to crowd counting problems have already achieved promising performance across various benchmarks. However, applying these approaches to real-world applications is still challenging, because they are computation intensive and lack the flexibility to meet various resource budgets. In this article, we propose an efficient crowd counting neural architecture search (ECCNAS) framework to search efficient crowd counting network structures, which can fill this research gap. A novel search from pre-trained strategy enables our cross-task NAS to explore the significantly large and flexible search space with less search time and get more proper network structures. Moreover, our well-designed search space can intrinsically provide candidate neural network structures with high performance and efficiency. In order to search network structures according to hardwares with different computational performance, we develop a novel latency cost estimation algorithm in our ECCNAS. Experiments show our searched models get an excellent trade-off between computational complexity and accuracy and have the potential to deploy in practical scenarios with various resource budgets. We reduce the computational cost, in terms of multiply-and-accumulate (MACs), by up to 96% with comparable accuracy. And we further designed experiments to validate the efficiency and the stability improvement of our proposed search from pre-trained strategy.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W4206948829",
    "type": "article"
  },
  {
    "title": "Multi-feature Fusion VoteNet for 3D Object Detection",
    "doi": "https://doi.org/10.1145/3462219",
    "publication_date": "2022-01-27",
    "publication_year": 2022,
    "authors": "Zhoutao Wang; Qian Xie; Mingqiang Wei; Kun Long; Jun Wang",
    "corresponding_authors": "",
    "abstract": "In this article, we propose a Multi-feature Fusion VoteNet (MFFVoteNet) framework for improving the 3D object detection performance in cluttered and heavily occluded scenes. Our method takes the point cloud and the synchronized RGB image as inputs to provide object detection results in 3D space. Our detection architecture is built on VoteNet with three key designs. First, we augment the VoteNet input with point color information to enhance the difference of various instances in a scene. Next, we integrate an image feature module into the VoteNet to provide a strong object class signal that can facilitate deterministic detections in occlusion. Moreover, we propose a Projection Non-Maximum Suppression (PNMS) method in 3D object detection to eliminate redundant proposals and hence provide more accurate positioning of 3D objects. We evaluate the proposed MFFVoteNet on two challenging 3D object detection datasets, i.e., ScanNetv2 and SUN RGB-D. Extensive experiments show that our framework can effectively improve the performance of 3D object detection.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W4210352603",
    "type": "article"
  },
  {
    "title": "Response Generation by Jointly Modeling Personalized Linguistic Styles and Emotions",
    "doi": "https://doi.org/10.1145/3475872",
    "publication_date": "2022-02-16",
    "publication_year": 2022,
    "authors": "Teng Sun; Wang Chun; Xuemeng Song; Fuli Feng; Liqiang Nie",
    "corresponding_authors": "",
    "abstract": "Natural language generation (NLG) has been an essential technique for various applications, like XiaoIce and Siri, and engaged increasing attention recently. To improve the user experience, several emotion-aware NLG methods have been developed to generate responses coherent with a pre-designated emotion (e.g., the positive or negative). Nevertheless, existing methods cannot generate personalized responses as they frequently overlook the personalized linguistic style. Apparently, different human responsers tend to have different linguistic styles. Inspired by this, in this work, we focus on a novel research theme of personalized emotion-aware NLG ( PENLG ), whereby the generated responses should be coherent with the linguistic style of a pre-designated responser and emotion. In particular, we study PENLG under a scenario of generating personalized emotion-aware response for social media post. Yet it faces certain research challenges: (1) the user linguistic styles are implicit and complex by nature, and hence it is hard to learn their representations; and (2) linguistic styles and emotions are usually expressed in different manners in a response, and thus how to convey them properly in the generated responses is not easy. Toward this end, we present a novel scheme of PENLG, named CRobot, which consists of a personalized emotion-aware response generator and two discriminators, i.e., general discriminator and personalized emotion-aware discriminator . To be more specific, the post-based and avatar-based user linguistic style modeling methods are incorporated into the encoder-decoder–based generator, while the discriminators are devised to ensure that the generated response is fluent and consistent with both the emotion and the linguistic style of the user. Different from the traditional adversarial networks, we embed adversarial learning under the umbrella of reinforcement learning. In this way, the response generation problem can be tackled by the generator taking a sequence of actions on selecting the proper word of each timestep for output. To justify our model, we construct a large-scale response generation dataset based on Twitter, consisting of 6,763 tweets with a corresponding 1,461,713 response created by 153,664 users. Extensive experiments demonstrate that CRobot surpasses the state-of-the-art baselines regarding both subjective and objective evaluation.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W4213227052",
    "type": "article"
  },
  {
    "title": "Blockchain-Based Audio Watermarking Technique for Multimedia Copyright Protection in Distribution Networks",
    "doi": "https://doi.org/10.1145/3492803",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Iynkaran Natgunanathan; Purathani Praitheeshan; Longxiang Gao; Yong Xiang; Lei Pan",
    "corresponding_authors": "",
    "abstract": "Copyright protection in multimedia protection distribution is a challenging problem. To protect multimedia data, many watermarking methods have been proposed in the literature. However, most of them cannot be used effectively in a multimedia distribution network (MDN) as they are not designed to support multi-layer watermark embedding. Multi-layer watermarking mechanisms were developed to protect multimedia data across different layers in an MDN. However, in those mechanisms, we need to trust the entities in the MDN, such as regional and country distributors. To overcome this potential drawback, in this article, we propose a novel privacy protection mechanism for MDNs by combining the advantages of both blockchain and watermarking technologies. A specifically designed watermarking algorithm is used to link the copyright information with the audio file, while a novel blockchain-based smart contract mechanism is developed to enforce the proper functioning of each entity in the distribution network. Moreover, the new audio mechanism is computationally efficient. Although audio signals are used to show the effectiveness of the proposed mechanism, the proposed approach can easily be extended to other multimedia objects, such as an image. The validity of the proposed mechanism is demonstrated by our simulation results. The proposed mechanism can benefit multimedia production companies and other entities in the MDN.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W4214882459",
    "type": "article"
  },
  {
    "title": "Tell, Imagine, and Search: End-to-end Learning for Composing Text and Image to Image Retrieval",
    "doi": "https://doi.org/10.1145/3478642",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Feifei Zhang; Mingliang Xu; Changsheng Xu",
    "corresponding_authors": "",
    "abstract": "Composing Text and Image to Image Retrieval ( CTI-IR ) is an emerging task in computer vision, which allows retrieving images relevant to a query image with text describing desired modifications to the query image. Most conventional cross-modal retrieval approaches usually take one modality data as the query to retrieve relevant data of another modality. Different from the existing methods, in this article, we propose an end-to-end trainable network for simultaneous image generation and CTI-IR . The proposed model is based on Generative Adversarial Network (GAN) and enjoys several merits. First, it can learn a generative and discriminative feature for the query (a query image with text description) by jointly training a generative model and a retrieval model. Second, our model can automatically manipulate the visual features of the reference image in terms of the text description by the adversarial learning between the synthesized image and target image. Third, global-local collaborative discriminators and attention-based generators are exploited, allowing our approach to focus on both the global and local differences between the query image and the target image. As a result, the semantic consistency and fine-grained details of the generated images can be better enhanced in our model. The generated image can also be used to interpret and empower our retrieval model. Quantitative and qualitative evaluations on three benchmark datasets demonstrate that the proposed algorithm performs favorably against state-of-the-art methods.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W4214920941",
    "type": "article"
  },
  {
    "title": "PRNU-based Image Forgery Localization with Deep Multi-scale Fusion",
    "doi": "https://doi.org/10.1145/3548689",
    "publication_date": "2022-07-14",
    "publication_year": 2022,
    "authors": "Yushu Zhang; Qing Mei Tan; Shuren Qi; Mingfu Xue",
    "corresponding_authors": "",
    "abstract": "Photo-response non-uniformity (PRNU), as a class of device fingerprint, plays a key role in the forgery detection/localization for visual media. The state-of-the-art PRNU-based forensics methods generally rely on the multi-scale trace analysis and result fusion, with Markov random field model. However, such hand-crafted strategies are difficult to provide satisfactory multi-scale decision, exhibiting a high false-positive rate. Motivated by this, we propose an end-to-end multi-scale decision fusion strategy, where a mapping from multi-scale forgery probabilities to binary decision is achieved by a supervised deep fully connected neural network. As the first time, the deep learning technology is employed in PRNU-based forensics for more flexible and reliable integration of multi-scale information. The benchmark experiments exhibit the state-of-the-art accuracy performance of our method in both pixel-level and image-level, especially for false positives. Additional robustness experiments also demonstrate the benefits of the proposed method in resisting noise and compression attacks.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W4285390910",
    "type": "article"
  },
  {
    "title": "A Sorting Fuzzy Min-Max Model in an Embedded System for Atrial Fibrillation Detection",
    "doi": "https://doi.org/10.1145/3554737",
    "publication_date": "2022-06-30",
    "publication_year": 2022,
    "authors": "Wei Huang; Yuze Zhang; Shaohua Wan",
    "corresponding_authors": "",
    "abstract": "Atrial fibrillation detection (AFD) has attracted much attention in the field of embedded systems. In this study, we propose a sorting fuzzy min-max (SFMM) model, and then develop an SFMM-based embedded system for AF detection. The proposed SFMM model is essentially enhanced the fuzzy min-max (FMM) model that have been successfully applied in many classification fields. In comparison with the typical FMM model, the proposed SFMM model can overcome the limitation of the input order problem encountered in the typical FMM model. The embedded system consists of a control chip and an analog-digital conversion (ADC) chip. The STM32F407 chip is used as the control chip and the ADS1292 chip, which has a high common-mode rejection ratio (CMRR), is used as the ADC chip. A series of machine learning benchmarks are included to evaluate the performance of the SFMM model. Experimental results on AF data further demonstrate the effectiveness of the SFMM-based embedded system.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W4289938749",
    "type": "article"
  },
  {
    "title": "Semantics and Non-Fungible Tokens for Copyright Management on the Metaverse and Beyond",
    "doi": "https://doi.org/10.1145/3585387",
    "publication_date": "2023-02-24",
    "publication_year": 2023,
    "authors": "Roberto Garcı́a; Ana Cediel; Mercè Teixidó; Rosa Gil",
    "corresponding_authors": "",
    "abstract": "Recent initiatives related to the Metaverse focus on better visualisation, like augmented or virtual reality, but also persistent digital objects. To guarantee real ownership of these digital objects, open systems based on public blockchains and Non-Fungible Tokens (NFTs) are emerging together with a nascent decentralized and open creator economy. To manage this emerging economy in a more organised way, and fight the so common NFT plagiarism, we propose CopyrightLY, a decentralized application for authorship and copyright management. It provides means to claim content authorship, including supporting evidence. Content and metadata are stored in decentralized storage and registered on the blockchain. A token is used to curate these claims, and potential complaints, by staking it on them. Staking is incentivized by the fact that the token is minted using a bonding curve. The tokenomics include the resolution of complaints and enabling the monetization of curated claims. Monetization is achieved through licensing NFTs with metadata enhanced by semantic technologies. Semantic data makes explicit the reuse conditions transferred with the token while keeping the connection to the underlying copyright claims to improve the trustability of the NFTs. Moreover, the semantic metadata is flexible enough to enable licensing not just in the real world. Licenses can refer to reuses in specific locations in a metaverse, thus facilitating the emergence of creative economies in them.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4321790408",
    "type": "article"
  },
  {
    "title": "A Comprehensive Survey on Methods for Image Integrity",
    "doi": "https://doi.org/10.1145/3633203",
    "publication_date": "2023-11-16",
    "publication_year": 2023,
    "authors": "Paola Capasso; Giuseppe Cattaneo; Maria De Marsico",
    "corresponding_authors": "",
    "abstract": "The outbreak of digital devices on the Internet, the exponential diffusion of data (images, video, audio, and text), along with their manipulation/generation also by artificial intelligence models, such as generative adversarial networks, have created a great deal of concern in the field of forensics. A malicious use can affect relevant application domains, which often include counterfeiting biomedical images and deceiving biometric authentication systems, as well as their use in scientific publications, in the political world, and even in school activities. It has been demonstrated that manipulated pictures most likely represent indications of malicious behavior, such as photos of minors to promote child prostitution or false political statements. Following this widespread behavior, various forensic techniques have been proposed in the scientific literature over time both to defeat these spoofing attacks as well as to guarantee the integrity of the information. Focusing on image forensics, which is currently a very hot topic area in multimedia forensics, this article will present the whole scenario in which a target image could be modified. The aim of this comprehensive survey will be (1) to provide an overview of the types of attacks and contrasting techniques and (2) to evaluate to what extent the former can deceive prevention methods and the latter can identify counterfeit images. The results of this study highlight how forgery detection techniques, sometimes limited to a single type of real scenario, are not able to provide exhaustive countermeasures and could/should therefore be combined. Currently, the use of neural networks, such as convolutional neural networks, is already heading, synergistically, in this direction.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4388733965",
    "type": "article"
  },
  {
    "title": "JPEG-compatible Joint Image Compression and Encryption Algorithm with File Size Preservation",
    "doi": "https://doi.org/10.1145/3633459",
    "publication_date": "2023-11-20",
    "publication_year": 2023,
    "authors": "Yuxiang Peng; Chong Fu; Guixing Cao; Wei Song; Junxin Chen; Chiu‐Wing Sham",
    "corresponding_authors": "",
    "abstract": "Joint image compression and encryption algorithms are intensively investigated due to their powerful capability of simultaneous image data compression and sensitive information protection. Unfortunately, most of the existing algorithms suffered from either poor compression efficiency or weak encryption strength, making them vulnerable to cryptanalysis. To address these limitations, we propose a chaos-based JPEG-compatible joint image compression and encryption algorithm. We separate the luminance and chrominance coefficients to preserve file size and encrypt the discrete cosine transform (DCT) coefficients in parallel. The proposed inter-block DC encryption strategy achieves high encryption intensity based on the permutation-substitution structure. In addition, we apply both inter- and intra-block permutations to AC coefficients and strengthen the encryption using an inter-block substitution for non-zero AC coefficients. The results of security and performance analyses demonstrate that the proposed algorithm offers robust encryption of image data while maintaining compression efficiency for real-time transmission.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4388827065",
    "type": "article"
  },
  {
    "title": "Graph Pooling Inference Network for Text-based VQA",
    "doi": "https://doi.org/10.1145/3634918",
    "publication_date": "2023-11-29",
    "publication_year": 2023,
    "authors": "Sheng Zhou; Dan Guo; Xun Yang; Jianfeng Dong; Meng Wang",
    "corresponding_authors": "",
    "abstract": "Effectively leveraging objects and optical character recognition (OCR) tokens to reason out pivotal scene text is critical for the challenging Text-based Visual Question Answering (TextVQA) task. Graph-based models can effectively capture the semantic relationship among visual entities (objects and tokens) and report remarkable performance in TextVQA. However, previous efforts usually leverage all visual entities and ignore the negative effect of superfluous entities. This article presents a Graph Pooling Inference Network (GPIN), which is an evolutionary graph learning method to purify the visual entities and capture the core semantics. It is observed that the dense distribution of reduplicative objects and the crowd of semantically dependent OCR tokens usually co-exist in the image. Motivated by this, GPIN adopts an adaptive node dropping strategy to dynamically downscale semantically closed nodes for graph evolution and update. To deepen the comprehension of scene text, GPIN is a dual-path hierarchical graph architecture that progressively aggregates the evolved object graph and the evolved token graph semantics into a graph vector that serves as visual cues to facilitate the answer reasoning. It can effectively eliminate object redundancy and enhance the association of semantically continuous tokens. Experiments conducted on TextVQA and ST-VQA datasets show that GPIN achieves promising performance compared with state-of-the-art methods.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4389140516",
    "type": "article"
  },
  {
    "title": "RAC-Chain: An Asynchronous Consensus-based Cross-chain Approach to Scalable Blockchain for Metaverse",
    "doi": "https://doi.org/10.1145/3586011",
    "publication_date": "2023-03-02",
    "publication_year": 2023,
    "authors": "Tianxiu Xie; Keke Gai; Liehuang Zhu; Shuo Wang; Zijian Zhang",
    "corresponding_authors": "",
    "abstract": "The metaverse, as an emerging technical term, conceptually aims to construct a virtual digital space that runs parallel to the physical world. Due to human behaviors and interactions being represented in the virtual world, security in the metaverse is a challenging issue in which the traditional centralized service model is one of the threat sources. To conquer the obstacle caused by centralized computing, blockchain-based solutions are potential problem-solving methods. However, it is difficult for a single blockchain to support large-scale data and business services in the metaverse, due to the scalability restrictions. Moreover, multi-chain settings also encounter the interoperability issues. In this work, we propose a Relay chain and Asynchronous consensus-based Consortium blockchain cross-Chain model, which realizes message transmission and cross-chain transactions in multiple chains by adopting the relay chain and cross-chain gateways. All nodes of the application chains and the relay chain execute cross-chain transactions in sequence and reach a consensus on transactions at any transmission delay. Our experiment evaluations demonstrate that our approach performs well in atomicity, security, and functionality (cross-chain transactions), such that the performance of blockchain scalability in the metaverse can be improved, compared with the traditional relay chain schemes.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4322775925",
    "type": "article"
  },
  {
    "title": "VISCOUNTH: A Large-scale Multilingual Visual Question Answering Dataset for Cultural Heritage",
    "doi": "https://doi.org/10.1145/3590773",
    "publication_date": "2023-04-04",
    "publication_year": 2023,
    "authors": "Federico Becattini; Pietro Bongini; Luana Bulla; Alberto Del Bimbo; Ludovica Marinucci; Misael Mongiovı̀; Valentina Presutti",
    "corresponding_authors": "",
    "abstract": "Visual question answering has recently been settled as a fundamental multi-modal reasoning task of artificial intelligence that allows users to get information about visual content by asking questions in natural language. In the cultural heritage domain, this task can contribute to assisting visitors in museums and cultural sites, thus increasing engagement. However, the development of visual question answering models for cultural heritage is prevented by the lack of suitable large-scale datasets. To meet this demand, we built a large-scale heterogeneous and multilingual (Italian and English) dataset for cultural heritage that comprises approximately 500K Italian cultural assets and 6.5M question-answer pairs. We propose a novel formulation of the task that requires reasoning over both the visual content and an associated natural language description, and present baselines for this task. Results show that the current state of the art is reasonably effective but still far from satisfactory; therefore, further research in this area is recommended. Nonetheless, we also present a holistic baseline to address visual and contextual questions and foster future research on the topic.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4362554621",
    "type": "article"
  },
  {
    "title": "Semantic Map Guided Identity Transfer GAN for Person Re-identification",
    "doi": "https://doi.org/10.1145/3631355",
    "publication_date": "2023-11-02",
    "publication_year": 2023,
    "authors": "Wu Tian; Rongbo Zhu; Shaohua Wan",
    "corresponding_authors": "",
    "abstract": "Generative adversarial networks (GANs)-based person re-identification (re-id) schemes provide potential ways to augment data in practical applications. However, existing solutions perform poorly because of the separation of data generation and re-id training and a lack of diverse data in real-world scenarios. In this paper, a person re-id model (IDGAN) based on semantic map guided identity transfer GAN is proposed to improve the person re-id performance. With the aid of the semantic map, IDGAN generates pedestrian images with varying poses, perspectives, and backgrounds efficiently and accurately, improving the diversity of training data. To increase the visual realism, IDGAN utilizes a gradient augmentation method based on local quality attention to refine the generated image locally. Then, a two-stage joint training framework is employed to allow the GAN and the person re-id network to learn from each other to better use the generated data. Detailed experimental results demonstrate that, compared with the existing state-of-the-art methods, IDGAN is capable of producing high-quality images and significantly enhancing re-id performance, with the FID of generated images on the Market-1501 dataset being reduced by 1.15, and mAP on the Market-1501 and DukeMTMC-reID datasets being increased by 3.3% and 2.6%, respectively.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4388230444",
    "type": "article"
  },
  {
    "title": "Efficient Video Transformers via Spatial-Temporal Token Merging for Action Recognition",
    "doi": "https://doi.org/10.1145/3633781",
    "publication_date": "2024-01-11",
    "publication_year": 2024,
    "authors": "Zhanzhou Feng; Jiaming Xu; Лей Ма; Shiliang Zhang",
    "corresponding_authors": "",
    "abstract": "Transformer has exhibited promising performance in various video recognition tasks but brings a huge computational cost in modeling spatial-temporal cues. This work aims to boost the efficiency of existing video transformers for action recognition through eliminating redundancies in their tokens and efficiently learning motion cues of moving objects. We propose a lightweight and plug-and-play module, namely Spatial-temporal Token Merger (STTM), to merge the tokens belonging to the same object into a more compact representation. STTM first adaptively identifies crucial object clues underlying the video as meta tokens. Similarity scores between input tokens and meta tokens are hence computed and used to guide the fusion of similar tokens in both spatial and temporal domains, respectively. To compensate for motion cues lost in the merging procedure, we compute the linear aggregation of spatial-temporal positions of tokens as motion features. STTM hence outputs a compact set of tokens fusing both appearance and motion features of moving objects. This procedure substantially decreases the number of tokens that need to be processed by each Transformer block and boosts the efficiency. As a general module, STTM can be applied to different layers of various video Transformers. Extensive experiments on the action recognition datasets Kinectics-400 and SSv2 demonstrate its promising performance. For example, it reduces the computation complexity of ViT by 38% while maintaining a similar performance on Kinectics-400. It also brings 1.7% gains of top-1 accuracy on SSv2 under the same computational cost.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4389299497",
    "type": "article"
  },
  {
    "title": "Head3D: Complete 3D Head Generation via Tri-plane Feature Distillation",
    "doi": "https://doi.org/10.1145/3635717",
    "publication_date": "2024-01-25",
    "publication_year": 2024,
    "authors": "Yuhao Cheng; Yichao Yan; Wenhan Zhu; Ye Pan; Bowen Pan; Xiaokang Yang",
    "corresponding_authors": "",
    "abstract": "Head generation with diverse identities is an important task in computer vision and computer graphics, widely used in multimedia applications. However, current full-head generation methods require a large number of three-dimensional (3D) scans or multi-view images to train the model, resulting in expensive data acquisition costs. To address this issue, we propose Head3D, a method to generate full 3D heads with limited multi-view images. Specifically, our approach first extracts facial priors represented by tri-planes learned in EG3D, a 3D-aware generative model, and then proposes feature distillation to deliver the 3D frontal faces within complete heads without compromising head integrity. To mitigate the domain gap between the face and head models, we present a dual-discriminator to guide the frontal and back head generation. Our model achieves cost-efficient and diverse complete head generation with photo-realistic renderings and high-quality geometry representations. Extensive experiments demonstrate the effectiveness of our proposed Head3D, both qualitatively and quantitatively.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4391230384",
    "type": "article"
  },
  {
    "title": "Invisible Adversarial Watermarking: A Novel Security Mechanism for Enhancing Copyright Protection",
    "doi": "https://doi.org/10.1145/3652608",
    "publication_date": "2024-03-14",
    "publication_year": 2024,
    "authors": "Jinwei Wang; Haihua Wang; Jiawei Zhang; Hao Wu; Xiangyang Luo; Bin Ma",
    "corresponding_authors": "",
    "abstract": "Invisible watermarking can be used as an important tool for copyright certification in the Metaverse. However, with the advent of deep learning, Deep Neural Networks (DNNs) have posed new threats to this technique. For example, artificially trained DNNs can perform unauthorized content analysis and achieve illegal access to protected images. Furthermore, some specially crafted DNNs may even erase invisible watermarks embedded within the protected images, which eventually leads to the collapse of this protection and certification mechanism. To address these issues, inspired by the adversarial attack, we introduce Invisible Adversarial Watermarking (IAW), a novel security mechanism to enhance the copyright protection efficacy of watermarks. Specifically, we design an Adversarial Watermarking Fusion Model (AWFM) to efficiently generate Invisible Adversarial Watermark Images (IAWIs). By modeling the embedding of watermarks and adversarial perturbations as a unified task, the generated IAWIs can effectively defend against unauthorized identification, access, and erase via DNNs, and identify the ownership by extracting the embedded watermark. Experimental results show that the proposed IAW presents superior extraction accuracy, attack ability, and robustness on different DNNs, and the protected images maintain good visual quality, which ensures its effectiveness as an image protection mechanism.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4392806484",
    "type": "article"
  },
  {
    "title": "Generative Adversarial Networks with Learnable Auxiliary Module for Image Synthesis",
    "doi": "https://doi.org/10.1145/3653021",
    "publication_date": "2024-03-17",
    "publication_year": 2024,
    "authors": "Yan Gan; Chenxue Yang; Mao Ye; Renjie Huang; Deqiang Ouyang",
    "corresponding_authors": "",
    "abstract": "Training generative adversarial networks (GANs) for noise-to-image synthesis is a challenge task, primarily due to the instability of GANs’ training process. One of the key issues is the generator’s sensitivity to input data, which can cause sudden fluctuations in the generator’s loss value with certain inputs. This sensitivity suggests an inadequate ability to resist disturbances in the generator, causing the discriminator’s loss value to oscillate and negatively impacting the discriminator. Then, the negative feedback of discriminator is also not conducive to updating generator’s parameters, leading to suboptimal image generation quality. In response to this challenge, we present an innovative GANs model equipped with a learnable auxiliary module that processes auxiliary noise. The core objective of this module is to enhance the stability of both the generator and discriminator throughout the training process. To achieve this target, we incorporate a learnable auxiliary penalty and an augmented discriminator, designed to control the generator and reinforce the discriminator’s stability, respectively. We further apply our method to the Hinge and LSGANs loss functions, illustrating its efficacy in reducing the instability of both the generator and the discriminator. The tests we conducted on LSUN, CelebA, Market-1501 and Creative Senz3D datasets serve as proof of our method’s ability to improve the training stability and overall performance of the baseline methods.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4392895456",
    "type": "article"
  },
  {
    "title": "The Price of Unlearning: Identifying Unlearning Risk in Edge Computing",
    "doi": "https://doi.org/10.1145/3662184",
    "publication_date": "2024-05-06",
    "publication_year": 2024,
    "authors": "Lefeng Zhang; Tianqing Zhu; Ping Xiong; Wanlei Zhou",
    "corresponding_authors": "",
    "abstract": "Machine unlearning is an emerging paradigm that aims to make machine learning models “forget” what they have learned about particular data. It fulfills the requirements of privacy legislation (e.g., GDPR), which stipulates that individuals have the autonomy to determine the usage of their personal data. However, alongside all the achievements, there are still loopholes in machine unlearning that may cause significant losses for the system, especially in edge computing. Edge computing is a distributed computing paradigm with the purpose of migrating data processing tasks closer to terminal devices. While various machine unlearning approaches have been proposed to erase the influence of data sample(s), we claim that it might be dangerous to directly apply them in the realm of edge computing. A malicious edge node may broadcast (possibly fake) unlearning requests to a target data sample (s) and then analyze the behavior of edge devices to infer useful information. In this paper, we exploited the vulnerabilities of current machine unlearning strategies in edge computing and proposed a new inference attack to highlight the potential privacy risk. Furthermore, we developed a defense method against this particular type of attack and proposed the price of unlearning ( PoU ) as a means to evaluate the inefficiency it brings to an edge computing system. We provide theoretical analyses to show the upper bound of the PoU using tools borrowed from game theory. The experimental results on real-world datasets demonstrate that the proposed defense strategy is effective and capable of preventing an adversary from deducing useful information.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4396677864",
    "type": "article"
  },
  {
    "title": "Spatiotemporal Inconsistency Learning and Interactive Fusion for Deepfake Video Detection",
    "doi": "https://doi.org/10.1145/3664654",
    "publication_date": "2024-05-13",
    "publication_year": 2024,
    "authors": "Dengyong Zhang; Wenjie Zhu; Xin Liao; Feifan Qi; Gaobo Yang; Xiangling Ding",
    "corresponding_authors": "",
    "abstract": "With the rise of the metaverse, the rapid advancement of Deepfakes technology has become closely intertwined. Within the metaverse, individuals exist in digital form and engage in interactions, transactions, and communications through virtual avatars. However, the development of Deepfakes technology has led to the proliferation of forged information disseminated under the guise of users’ virtual identities, posing significant security risks to the metaverse. Hence, there is an urgent need to research and develop more robust methods for detecting deep forgeries to address these challenges. This paper explores deepfake video detection by leveraging the spatiotemporal inconsistencies generated by deepfake generation techniques, and thereby proposing the interactive spatioTemporal inconsistency learning and interactive fusion (ST-ILIF) detection method, which consists of phase-aware and sequence streams. The spatial inconsistencies exhibited in frames of deepfake videos are primarily attributed to variations in the structural information contained within the phase component of the Fourier domain. To mitigate the issue of overfitting the content information, a phase-aware stream is introduced to learn the spatial inconsistencies from the phase-based reconstructed frames. Additionally, considering that deepfake videos are generated frame-by-frame and lack temporal consistency between frames, a sequence stream is proposed to extract temporal inconsistency features from the spatiotemporal difference information between consecutive frames. Finally, through feature interaction and fusion of the two streams, the representation ability of intermediate and classification features is further enhanced. The proposed method, which was evaluated on four mainstream datasets, outperformed most existing methods, and extensive experimental results demonstrated its effectiveness in identifying deepfake videos. Our source code is available at https://github.com/qff98/Deepfake-Video-Detection",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4396861045",
    "type": "article"
  },
  {
    "title": "Performance Evaluation in Multimedia Retrieval",
    "doi": "https://doi.org/10.1145/3678881",
    "publication_date": "2024-10-14",
    "publication_year": 2024,
    "authors": "Loris Sauter; Ralph Gasser; Heiko Schuldt; Abraham Bernstein; Luca Rossetto",
    "corresponding_authors": "",
    "abstract": "Performance evaluation in multimedia retrieval, as in the information retrieval domain at large, relies heavily on retrieval experiments, employing a broad range of techniques and metrics. These can involve human-in-the-loop and machine-only settings for the retrieval process itself and the subsequent verification of results. Such experiments can be elaborate and use-case-specific, which can make them difficult to compare or replicate. In this paper, we present a formal model to express all relevant aspects of such retrieval experiments, as well as a flexible open-source evaluation infrastructure that implements the model. These contributions intend to make a step towards lowering the hurdles for conducting retrieval experiments and improving their reproducibility.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4403345346",
    "type": "article"
  },
  {
    "title": "A SAM-guided Two-stream Lightweight Model for Anomaly Detection",
    "doi": "https://doi.org/10.1145/3706574",
    "publication_date": "2024-11-29",
    "publication_year": 2024,
    "authors": "Chenghao Li; Lei Qi; Xin Geng",
    "corresponding_authors": "",
    "abstract": "In industrial anomaly detection, model efficiency and mobile-friendliness become the primary concerns in real-world applications. Simultaneously, the impressive generalization capabilities of Segment Anything (SAM) have garnered broad academic attention, making it an ideal choice for localizing unseen anomalies and diverse real-world patterns. In this paper, considering these two critical factors, we propose a SAM-guided Two-stream Lightweight Model for unsupervised anomaly detection (STLM) that not only aligns with the two practical application requirements but also harnesses the robust generalization capabilities of SAM. We employ two lightweight image encoders, i.e. , our two-stream lightweight module, guided by SAM's knowledge. To be specific, one stream is trained to generate discriminative and general feature representations in both normal and anomalous regions, while the other stream reconstructs the same images without anomalies, which effectively enhances the differentiation of two-stream representations when facing anomalous regions. Furthermore, we employ a shared mask decoder and a feature aggregation module to generate anomaly maps. Our experiments conducted on MVTec AD benchmark show that STLM, with about 16M parameters and achieving an inference time in 20ms, competes effectively with state-of-the-art methods in terms of performance, 98.26% on pixel-level AUC and 94.92% on PRO. We further experiment on more difficult datasets, e.g. , VisA and DAGM, to demonstrate the effectiveness and generalizability of STLM. Codes are available online at https://github.com/Qi5Lei/STLM .",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4404852513",
    "type": "article"
  },
  {
    "title": "Evaluating Adaptive Video Streaming over Multipath QUIC with Shared Bottleneck Detection",
    "doi": "https://doi.org/10.1145/3711862",
    "publication_date": "2025-01-09",
    "publication_year": 2025,
    "authors": "Bruno Yuji Lino Kimura; Simone Ferlin; Thomas William do Prado Paiva; Toktam Mahmoodi; Anna Brunström; Özgü Alay",
    "corresponding_authors": "",
    "abstract": "The promises of multipath transport are to aggregate bandwidth, improve resource utilization, and enhance reliability. In this paper, we demonstrate that the way multipath coupled congestion control is defined today leads to a suboptimal resource utilisation when network paths are disjoint, i.e., they do not share a bottleneck link. With growing interest in standardising Multipath QUIC (MPQUIC), we have implemented the practical shared bottleneck detection (SBD) algorithm from RFC8382 in MPQUIC (MPQUIC-SBD). Through extensive experiments, we evaluate MPQUIC-SBD in the context of video streaming with various Adaptive Bitrate (ABR) algorithms, addressing both ABR classes of rule-based and learning-based solutions. We demonstrate that MPQUIC-SBD accurately detects shared bottlenecks over 90% of the time, depending on the ABR algorithm, as the size of the video segments increases. In non-shared bottleneck scenarios, when MPQUIC-SBD detects that its QUIC subflows do not share the same network resources, it decouples their congestion windows accordingly, enabling video throughput gains of up to 37% compared to MPQUIC. These gains translate directly into improved video quality metrics, including higher bitrate, better resolution, and reduced buffering, resulting in an enhanced quality of experience for users.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4406219658",
    "type": "article"
  },
  {
    "title": "Domain-Separated Bottleneck Attention Fusion Framework for Multimodal Emotion Recognition",
    "doi": "https://doi.org/10.1145/3711865",
    "publication_date": "2025-01-10",
    "publication_year": 2025,
    "authors": "Peng He; Jun Yu; Chengjie Ge; Wei Jia; W. L. Xu; Lei Wang; Tianyu Liu; Zhen Kan",
    "corresponding_authors": "",
    "abstract": "As a focal point of research in various fields, human body language understanding has long been a subject of intense interest. Within this realm, the exploration of emotion recognition through the analysis of facial expressions, voice patterns, and physiological signals, holds significant practical value. Compared with unimodal approaches, multimodal emotion recognition models leverage complementary information from vision, acoustic, and language modalities to robust perceive the human sentiment attitudes. However, the heterogeneity among modality signals leads to significant domain shifts, posing challenges for achieving balanced fusion. In this paper, we propose a Domain-separated Bottleneck Attention fusion framework (DBA Framework) for human multimodal emotion recognition with lower computational complexity. Specifically, we partition each modality into two distinct domains: the invariant/private domain. The invariant domain contains crucial shared information, while the private domain aims to capture modality-specific representations. For the decomposed features, we introduce two sets of bottleneck cross-attention modules to effectively utilize the complementarity between domains to reduce redundant information. In each module, we interweave two Fusion Adapter blocks into the Self-Attention Transformer backbone. Each Fusion Adapter block integrates a small group of latent tokens as bridges for inter-modal and inter-domain interactions, mitigating the adverse effects of modality distribution differences and lowering computational costs. Extensive experimental results demonstrate that our method outperforms state-of-the-art (SOTA) approaches across three widely used benchmark datasets.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4406233700",
    "type": "article"
  },
  {
    "title": "DriveDiTFit: Fine-tuning Diffusion Transformers for Autonomous Driving Data Generation",
    "doi": "https://doi.org/10.1145/3712064",
    "publication_date": "2025-01-13",
    "publication_year": 2025,
    "authors": "Jiahang Tu; Wei Ji; Hanbin Zhao; Chao Zhang; Roger Zimmermann; Hui Qian",
    "corresponding_authors": "",
    "abstract": "In autonomous driving, deep models have shown remarkable performance across various visual perception tasks with the demand of high-quality and huge-diversity training datasets. Such datasets are expected to cover various driving scenarios with adverse weather, lighting conditions and diverse moving objects. However, manually collecting these data presents huge challenges and expensive cost. With the rapid development of large generative models, we propose DriveDiTFit, a novel method for efficiently generating autonomous Driv ing data by Fi ne- t uning pre-trained Di ffusion T ransformers (DiTs). Specifically, DriveDiTFit utilizes a gap-driven modulation technique to carefully select and efficiently fine-tune a few parameters in DiTs according to the discrepancy between the pre-trained source data and the target driving data. Additionally, DriveDiTFit develops an effective weather and lighting condition embedding module to ensure diversity in the generated data, which is initialized by a nearest-semantic-similarity initialization approach. Through progressive tuning scheme to refined the process of detail generation in early diffusion process and enlarging the weights corresponding to small objects in training loss, DriveDiTFit ensures high-quality generation of small moving objects in the generated data. Extensive experiments conducted on driving datasets confirm that our method could efficiently produce diverse real driving data.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4406325054",
    "type": "article"
  },
  {
    "title": "Multigranularity Feature Aggregation and Cross-level Boundary Modeling for Temporal Action Detection",
    "doi": "https://doi.org/10.1145/3712598",
    "publication_date": "2025-01-17",
    "publication_year": 2025,
    "authors": "Qiang Li; Di Liu; Guang Zu; S.H. Li; Hui Sun; Jianzhong Wang",
    "corresponding_authors": "",
    "abstract": "This paper presents a temporal action detection (TAD) method with multi-granularity feature aggregation and cross-level boundary modeling (MGCBM). Compared with other methods, our proposed approach has the following advantages. First, different from most existing works which only consider the local temporal context, a simple and computationally efficient Multi-Granularity (MG) module is proposed to comprehensively extract video features in instant, local and global temporal granularities. Second, unlike the methods that only employ the information from single feature pyramid level for action boundary regression, a cross-level boundary modeling (CBM) strategy that integrates the relative information from both the same and higher level features is designed to improve the accuracy of boundary prediction. At last, benefiting from the MG module and CBM strategy, our method outperforms other state-of-the-art approaches on five challenging TAD datasets: THUMOS14, MultiTHUMOS, EPIC-KITCHENS-100, ActivityNet-1.3 and HACS. We make our code and pre-trained model publicly available at: https://github.com/MGCBM/TAL-MGCBM .",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4406526561",
    "type": "article"
  },
  {
    "title": "Temporal Boundary Awareness Network for Repetitive Action Counting",
    "doi": "https://doi.org/10.1145/3712602",
    "publication_date": "2025-01-21",
    "publication_year": 2025,
    "authors": "Z Zhang; Kun Li; Shengeng Tang; Yanyan Wei; Fei Wang; Jinxing Zhou; Dan Guo",
    "corresponding_authors": "",
    "abstract": "Repetitive Action Counting (RAC) is a critical and challenging task in video analysis, aiming to count the number of repeated actions in videos accurately. Existing methods typically generate a Temporal Self-similarity Matrix (TSM) as an intermediate representation to predict the number of repetitive actions. While this simplifies the process, it often overlooks the variable lengths between action cycles and the phenomenon of motion interruptions. The period inconsistency problem caused by the change in the action period and the motion interruption problem resulting from the motion pause are the two main challenges that affect the accuracy of RAC in complex scenes. To address these challenges, we propose a novel framework. First, we construct a boundary-aware encoder equipped with a temporal pyramid structure to build multi-scale video features, capturing the period information of different lengths of repetitive actions to solve the period inconsistency problem. Next, a cycle and boundary attention module is followed by each layer in the pyramid to enhance these multi-scale features with periodic and event boundary information. Finally, we design a gated density estimator to generate the actionness score for each frame that reflects the probability of the corresponding time point being within the motion cycle. These scores are used to weight features to reduce the impact of noise frames without actions present and solve the motion interruption problem for better density prediction. Extensive experiments conducted on public datasets demonstrate the effectiveness of our method. The source code will be available at https://github.com/zqzhang2023/TBANRAC .",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4406676671",
    "type": "article"
  },
  {
    "title": "Towards Scene-Centric Multi-Level Interest Mining for Video Recommendation",
    "doi": "https://doi.org/10.1145/3712600",
    "publication_date": "2025-01-23",
    "publication_year": 2025,
    "authors": "J. Chen; Xiaomeng Wang; Tong Xu; Shiwei Wu",
    "corresponding_authors": "",
    "abstract": "Knowledge-aware video recommendation requires the ability of associating external knowledge to capture high-order connectivities between users and videos. One limitation of existing methods is that they only extract user interests at a granular level of relational paths by modeling high-order connectivities, which are coarse-grained in user interest modeling, failing to identify user-video relations at a finer-grained level of semantics. In this paper, we investigate the utility of semantic scene graphs in video recommendation scenario, which provide detailed, graph-based annotations of social situations depicted in video clips. We propose a new method named Scene-Centric multi-level Interest Miner (SCIMiner) which explicitly models multi-level user interests. Specifically, we construct user-video graph, knowledge graph and semantic scene graphs as hierarchical heterogeneous graphs. On top of the hierarchical graph representation, we propose two information aggregation strategies to capture user interests from different levels. Knowledge-aware aggregation scheme extracts coarse-grained user interests by aggregating relational paths in high-order connectivities, while scene-aware aggregation scheme models fine-grained user interests by capturing clip-level semantic commonality of user favorite videos. Furthermore, we adaptively distill complementary information about multi-level user interests extracted by different aggregation schemes and encode them into the representations of users and videos. Empirical results show that SCIMiner significantly outperforms the state-of-the-art methods. Further studies verify the complementary benefits of knowledge graph with semantic scene graphs in video recommendation scenario and the finer-grained explainability for predictions.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4406778829",
    "type": "article"
  },
  {
    "title": "A 360-degree Video Player for Dynamic Video Editing Applications",
    "doi": "https://doi.org/10.1145/3715135",
    "publication_date": "2025-01-28",
    "publication_year": 2025,
    "authors": "Gabriel De Castro Araújo; Henrique Domingues Garcia; Mylène C. Q. Farias; Ravi Prakash; Marcelo M. Carvalho",
    "corresponding_authors": "",
    "abstract": "360-degree videos introduce unique challenges to storytelling, especially if filmmakers want to draw the user’s attention to specific regions of the surrounding sphere to ensure the proper flow of narrative at any given time. For instance, at scene cuts, a user might lose track of a story if he/she looks towards a direction that prevents an intended region of interest (RoI) to appear in the viewport of the user’s head-mounted display (HMD). Because of that, new editing techniques for 360-degree videos are being investigated, and the use of dynamic (i.e., real-time) alignment of intended RoI across scene cuts is receiving growing attention. To support research on this topic, we introduce 360EAVP, an open source web-based application for streaming and visualization of 360-degree edited videos on HMDs. The 360EAVP is built on top of the VR DASH Tile Player, which is based on cube mapping projection, and adds new capabilities to handle dynamic video editing. Specifically, 360EAVP introduces 1) real-time track of a user’s viewport direction on the HMD; 2) support for dynamic editing via “snap-change” or “fade-rotation” effects; 3) visibility mapping of the user’s Field of View (FoV) with respect to the player’s cube mapping projection (for purposes of video tile requests during streaming); 4) incorporation of editing timing information into the operation of an ABR algorithm; 5) viewport prediction module based on linear regression or ridge regression models; and 6) video playback data collection and log module. To showcase the use of 360EAVP, we present results of a small-scale subjective experiment that aimed to evaluate the impact of the “snap-change” and “fade-rotation” editing techniques on user’s Quality of Experience (QoE), comfort, and head movement. Our findings indicate that these editing techniques do not compromise the overall QoE; in fact, in certain scenarios, an improvement is observed. In addition, some subjects significantly reduced their head movements with the editing techniques implemented.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4406903267",
    "type": "article"
  },
  {
    "title": "Securing Vehicle-to-Digital Twin Communications in the Internet of Vehicles",
    "doi": "https://doi.org/10.1145/3711863",
    "publication_date": "2025-01-28",
    "publication_year": 2025,
    "authors": "Sadia Jabeen Siddiqi; Abdulraheem H. Alobaidi; Mian Ahmad Jan; Muhammad Tariq",
    "corresponding_authors": "",
    "abstract": "The current landscape of data-centric Internet of Vehicles (IoVs) encompasses a fusion of Human-driven Vehicles (HVs), Autonomous Vehicles (AVs), Road-Side Units (RSUs), and edge-based devices engaged in periodic communication. Given the stringent latency requirements inherent in vehicular communications, the emergence of edge-based vehicular Digital Twins (DTs) plays a pivotal role in problem-solving, ensuring rapid response, regulatory compliance, and seamless availability. While these communications serve as the backbone of IoV, they also create an opportune environment for cybercriminals to exploit. Vulnerabilities at the network layer facilitate intrusions, resulting in a surge of data falsification attacks in recent years. Addressing this challenge demands resilient and intelligent threat detection schemes capable of adapting to the dynamic nature of IoV. This study conducts a comprehensive examination of the vulnerabilities in Vehicle-to-Digital twin (V2DT) data communication through the lens of an attacker utilizing False Data Injection Attack (FDIA). It utilizes cutting-edge Blockchain-based decentralized storage and buffering mechanisms for vehicle dynamics data en route to edge-based DTs. Further, deep learning-powered sensor data analysis serves as an additional layer of security. Evaluation of the proposed threat detection and mitigation model demonstrates 100% tamper detection in V2DT communication, coupled with a 96% accurate classification of anomalous driving behaviors, including aggressive driving or FDIAs.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4406903610",
    "type": "article"
  },
  {
    "title": "CapST: Leveraging Capsule Networks and Temporal Attention for Accurate Model Attribution in Deep-fake Videos",
    "doi": "https://doi.org/10.1145/3715138",
    "publication_date": "2025-01-29",
    "publication_year": 2025,
    "authors": "W. Haj Ahmad; Yan-Tsung Peng; Yuan-Hao Chang; Gaddisa Olani Ganfure; Shujhat Khan",
    "corresponding_authors": "",
    "abstract": "Deep-fake videos, generated through AI face-swapping techniques, have garnered considerable attention due to their potential for impactful impersonation attacks. While existing research primarily distinguishes real from fake videos, attributing a deep-fake to its specific generation model or encoder is crucial for forensic investigation, enabling precise source tracing and tailored countermeasures. This approach not only enhances detection accuracy by leveraging unique model-specific artifacts but also provides insights essential for developing proactive defenses against evolving deep-fake techniques. Addressing this gap, this paper investigates the model attribution problem for deep-fake videos using two datasets: Deepfakes from Different Models (DFDM) and GANGen-Detection, which comprise deep-fake videos and images generated by GAN models. We select only fake images from the GANGen-Detection dataset to align with the DFDM dataset, which specifies the goal of this study, focusing on model attribution rather than real/fake classification. This study formulates deep-fake model attribution as a multiclass classification task, introducing a novel Capsule-Spatial-Temporal (CapST) model that effectively integrates a modified VGG19 (utilizing only the first 26 out of 52 layers) for feature extraction, combined with Capsule Networks and a Spatio-Temporal attention mechanism. The Capsule module captures intricate feature hierarchies, enabling robust identification of deep-fake attributes, while a video-level fusion technique leverages temporal attention mechanisms to process concatenated feature vectors and capture temporal dependencies in deep-fake videos. By aggregating insights across frames, our model achieves a comprehensive understanding of video content, resulting in more precise predictions. Experimental results on the DFDM and GANGen-Detection datasets demonstrate the efficacy of CapST, achieving substantial improvements in accurately categorizing deep-fake videos over baseline models, all while demanding fewer computational resources.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4406958624",
    "type": "article"
  },
  {
    "title": "Language-guided Bias Generation Contrastive Strategy for Visual Question Answering",
    "doi": "https://doi.org/10.1145/3715141",
    "publication_date": "2025-01-30",
    "publication_year": 2025,
    "authors": "Enyuan Zhao; Ning Song; Ze Zhang; Jie Nie; Xinyue Liang; Zhiqiang Wei",
    "corresponding_authors": "",
    "abstract": "Visual question answering (VQA) is a challenging task that requires models to understand both visual and linguistic inputs and produce accurate answers. However, VQA models often exploit biases in datasets to make predictions rather than reasoning based on the inputs. Prior approaches to debiasing have suggested the implementation of a supplementary model, deliberately designed to exhibit bias, which subsequently informs the training of a resilient target model. Nevertheless, such techniques merely quantify the model’s divergence based on the statistical distribution of labels within the training dataset or in relation to unimodal branches. In this work, we propose a novel method of generating bias from the target model itself, called LEGO, which aims to combat the language guidance bias. Specifically, LEGO framework employs a generative network that assimilates the biases inherent in the target model by integrating adversarial goals with the principles of knowledge distillation. Then, we use a debiased contrastive learning strategy to model the language guidance bias of caption and question. In the process of modelling, in order to obtain robust semantic coreference, the multi-modal representations of two semantic granularity are modelled by mutual information fusion and contrast learning difference modelling. We evaluate our method on various VQA-biased datasets, including VQA-CP2, GQA-OOD, and RSICD, and show that it outperforms similar methods.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4406987633",
    "type": "article"
  },
  {
    "title": "SOEDiff: Efficient Distillation for Small Object Editing",
    "doi": "https://doi.org/10.1145/3715915",
    "publication_date": "2025-01-31",
    "publication_year": 2025,
    "authors": "Yiming Wu; Qihe Pan; Zhen Zhao; Zicheng Wang; Sifan Long; Ronghua Liang",
    "corresponding_authors": "",
    "abstract": "In this paper, we delve into a new task known as small object editing (SOE), which focuses on text-based image inpainting within a constrained, small-sized area. Despite the remarkable success have been achieved by current image inpainting approaches, their application to the SOE task generally results in failure cases such as Object Missing, Text-Image Mismatch, and Distortion . These failures stem from the limited use of small-sized objects in training datasets and the downsampling operations employed by U-Net models, which hinders accurate generation. To overcome these challenges, we introduce a novel training-based approach, SOEDiff, aimed at enhancing the capability of baseline models like StableDiffusion in editing small-sized objects while minimizing training costs. Specifically, our method involves two key components: SO-LoRA , which efficiently fine-tunes low-rank matrices, and Cross-scale score distillation , which leverages high-resolution predictions from the pre-trained teacher diffusion model. Our method presents significant improvements on the test dataset collected from MSCOCO and OpenImage, validating the effectiveness of our proposed method in small object editing. In particular, when comparing SOEDiff with SD-I model on the OpenImage-small-val dataset, we observe a 0.99 improvement in CLIP-Score and a reduction of 2.87 in FID.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4407031113",
    "type": "article"
  },
  {
    "title": "Multi-Modal Sarcasm Detection via Knowledge-aware Focused Graph Convolutional Networks",
    "doi": "https://doi.org/10.1145/3722115",
    "publication_date": "2025-03-07",
    "publication_year": 2025,
    "authors": "X. Zhuang; Fengling Zhou; Zhixin Li",
    "corresponding_authors": "",
    "abstract": "Multi-modal Sarcasm Detection (MSD) aims to combine multiple modal information to identify implicit sarcastic sentiment. However, the significance of commonsense knowledge in implicit emotion recognition has been frequently overlooked. Additionally, the important visual emotions associated with textual modal sarcastic cues are typically dispersed throughout the entire image, making it difficult to accurately focus on crucial sarcastic features. Therefore, we propose a Knowledge-aware Focused Graph Convolutional Networks (KFGC-Net) to tackle these issues. Specifically, we first construct a cross-modal knowledge-aware graph based on commonsense concepts for each instance. This graph explicitly establishes connections between significant visual sentiments and relevant textual tokens of knowledge. Next, we integrate the transformer encoder optimized with convolutional operations with the Convolutional Block Attention Module to compensate for the model’s lack of attention to important features. Finally, we design a Global Modality Synergistic Fusion (GMSF) block, aiming to model the global relationships in each modality for complementing global sarcasm detection result. Notably, we analyze the proposed framework by testing it on several benchmark datasets, and the results outperform the existing state-of-the-art.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4408213186",
    "type": "article"
  },
  {
    "title": "A Survey on Composed Image Retrieval",
    "doi": "https://doi.org/10.1145/3723879",
    "publication_date": "2025-03-17",
    "publication_year": 2025,
    "authors": "Longbiao Du; S. Deng; Ying Li; Jun Li; Qi Tian",
    "corresponding_authors": "",
    "abstract": "Composed Image Retrieval (CIR) processes a query consisting of a reference image and a modification text, aiming to retrieve target images that not only resemble the reference image visually but also reflect the modification described in the caption. Unlike traditional image retrieval methods that rely on a single modality, CIR integrates visual and textual information, enabling more nuanced and constraint-based query representations. This unique capability has garnered growing interest from researchers. Despite its potential, the field lacks a systematic review that comprehensively examines its advancements and trends. This article seeks to fill this gap by providing a detailed review of CIR research developments over the past 5 years. It categorizes existing methods into supervised approaches, which leverage triplet-labeled data for model training, and zero-shot approaches, which utilize unlabeled data to address CIR challenges. Additionally, the widely used benchmark datasets and evaluation indicators are comprehensively introduced. A comparative analysis of state-of-the-art methods across five datasets is also conducted, providing insights into their strengths and limitations. Ultimately, this article also outlines potential research directions for the future.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4408503287",
    "type": "article"
  },
  {
    "title": "Learning Visual-Semantic Embedding for Generalizable Person Re-identification: A Unified Perspective",
    "doi": "https://doi.org/10.1145/3726528",
    "publication_date": "2025-03-28",
    "publication_year": 2025,
    "authors": "Suncheng Xiang; Jingsheng Gao; Mingye Xie; Mengyuan Guan; Jiacheng Ruan; Yuzhuo Fu",
    "corresponding_authors": "",
    "abstract": "Generalizable person re-identification (Re-ID) is a very hot research topic in machine learning and computer vision, which plays a significant role in realistic scenarios due to its various applications in public security and video surveillance. However, previous methods mainly focus on the visual representation learning, while neglect to explore the potential of semantic features during training, which easily leads to poor generalization capability when adapted to the new domain. In this paper, we present a unified perspective called MMET for more robust visual-semantic embedding learning on generalizable Re-ID. To further enhance the robust feature learning in the context of transformer, a dynamic masking mechanism called M asked M ultimodal M odeling strategy ( MMM ) is introduced to mask both the image patches and the text tokens, which can jointly work on multimodal or unimodal data and significantly boost the performance of generalizable person Re-ID. Extensive experiments on benchmark datasets demonstrate the competitive performance of our method over previous approaches. We hope this method could advance the research towards visual-semantic representation learning. Our source code is also publicly available at https://github.com/JeremyXSC/MMET .",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4408930147",
    "type": "article"
  },
  {
    "title": "Beyond Visual Cues: Synchronously Exploring Target-Centric Semantics for Vision-Language Tracking",
    "doi": "https://doi.org/10.1145/3726529",
    "publication_date": "2025-03-28",
    "publication_year": 2025,
    "authors": "Jiawei Ge; Jiuxin Cao; Xiangmei Chen; Xuelin Zhu; Weijia Liu; Chang Liu; Kun Wang; Bo Liu",
    "corresponding_authors": "",
    "abstract": "Single object tracking aims to locate one specific target in video sequences, given its initial state. Classical trackers rely solely on visual cues, restricting their ability to handle challenges such as appearance variations, ambiguity, and distractions. Hence, Vision-Language Tracking (VLT) has emerged as a promising approach, incorporating language descriptions to directly provide high-level semantics and enhance tracking performance. However, current Vision-Language (VL) trackers have not fully exploited the power of multi-modal learning, as they suffer from limitations such as heavily relying on off-the-shelf backbones for feature extraction, ineffective asynchronous fusion designs, and the absence of VL-related loss functions for optimizing multi-modal representation. Consequently, we present a novel tracker that progressively explores target-centric semantics for Vision-Language Tracking. Specifically, we propose the first Synchronous Learning Backbone (SLB) for VLT, which consists of two novel modules: the Target Enhance Module (TEM) and the Semantic Aware Module (SAM). These modules together ensure the multi-modal feature extraction and interaction at the same pace, facilitating the tracker to synchronously perceive target-related semantics from both visual and textual modalities. Moreover, we devise the dense matching loss to further strengthen multi-modal representation learning. Extensive experiments on VLT datasets demonstrate the superiority and effectiveness of our methods.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4408930345",
    "type": "article"
  },
  {
    "title": "Generating Higher-Quality Anti-Forensics DeepFakes with Adversarial Sharpening Mask",
    "doi": "https://doi.org/10.1145/3729233",
    "publication_date": "2025-04-15",
    "publication_year": 2025,
    "authors": "Bing Fan; Feng Ding; Guopu Zhu; Jiwu Huang; Sam Kwong; Pradeep K. Atrey; Siwei Lyu",
    "corresponding_authors": "",
    "abstract": "DeepFake, an artificial intelligence technology that can automatically synthesize facial forgeries, has recently attracted worldwide attention. While DeepFakes can be entertaining, they can also be used to spread falsified information or be weaponized as cognition warfare. Forensic researchers have been dedicated to designing defensive algorithms to combat such disinformation. However, attacking technologies have been developed to make DeepFake products more aggressive. For example, by launching anti-forensics and adversarial attacks, DeepFakes can be disguised as authentic media to evade forensic detectors. However, such manipulations often sacrifice image quality for satisfactory undetectability. To address this issue, we propose a method to generate a novel adversarial sharpening mask for launching black-box anti-forensics attacks. Unlike many existing methods, our approach injects perturbations that allow DeepFakes to achieve high anti-forensics performance while maintaining pleasant sharpening visual effects. Experimental evaluations demonstrate that our method successfully disrupts state-of-the-art DeepFake detectors. Moreover, compared to images processed by existing DeepFake anti-forensics methods, our method's quality of anti-forensics DeepFakes rendered is significantly improved. Our code is available at https://github.com/fb-reps/HQ-AF_GAN .",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4409478246",
    "type": "article"
  },
  {
    "title": "Temporal and Semantic Correlation Network for Weakly-Supervised Temporal Action Localization",
    "doi": "https://doi.org/10.1145/3721433",
    "publication_date": "2025-04-17",
    "publication_year": 2025,
    "authors": "Kang Lin; Wei Zhou; Zhijie Zheng; Dihu Chen; Tao Su",
    "corresponding_authors": "",
    "abstract": "Weakly-supervised Temporal Action Localization (WTAL) aims to identify the temporal boundaries and classify actions in untrimmed videos using only video-level labels during training. Despite recent progress, many existing approaches primarily follow a localization-by-classification pipeline, treating snippets as independent instances and thus exploiting only limited contextual information. Besides, these methods struggle to capture multi-scale temporal information and neglect both the internal temporal structures within videos and the semantic consistency between videos, resulting in misclassification and inaccurate localization. To address these limitations, we introduce a novel T emporal and S emantic C orrelation Net work (TSC-Net) for WTAL task, which can be trained end-to-end. Firstly, we propose a Multi-scale Features Integration Pyramid (MFIP) module to integrate multi-scale temporal features, effectively addressing the challenge of missed detections caused by short action durations. Furthermore, we design a Temporal Correlation Enhancement (TCE) branch to enhance segment correlations by video-level temporal structures to improve the completeness of action localization. Finally, a Dataset-wide Semantic Awareness (DSA) branch is designed to construct and propagate a dataset-level action semantics bank, enhancing the model's awareness of semantic consistency in actions. Extensive experiments show that TSC-Net outperforms most existing WTAL methods, achieving an average mAP of 46.3% on the THUMOS-14 dataset and 26.5% on the ActivityNet1.2 dataset. Detailed ablation studies further confirm the effectiveness of each component in our model. The code and models are publicly available at https://github.com/linkang-els/TSC-Net-main .",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4409538236",
    "type": "article"
  },
  {
    "title": "TextBlockV2: Towards Precise-Detection-Free Scene Text Spotting with Pre-trained Language Model",
    "doi": "https://doi.org/10.1145/3734872",
    "publication_date": "2025-05-08",
    "publication_year": 2025,
    "authors": "Jiahao Lyu; Wei Jin; Gangyan Zeng; Li Zeng; Enze Xie; Wei Wang; Can Ma; Yu Zhou",
    "corresponding_authors": "",
    "abstract": "Existing scene text spotters are designed to locate and transcribe texts from images. However, it is challenging for a spotter to achieve precise detection and recognition of scene texts simultaneously. Inspired by the glimpse-focus spotting pipeline of human beings and impressive performances of Pre-trained Language Models (PLMs) on visual tasks, we ask: (1) “Can machines spot texts without precise detection just like human beings?”, and if yes, (2) “Is text block another alternative for scene text spotting other than word or character?” To this end, our proposed scene text spotter leverages advanced PLMs to enhance performance without fine-grained detection. Specifically, we first use a simple detector for block-level text detection to obtain rough positional information. Then, we fine-tune a PLM using a large-scale OCR dataset to achieve accurate recognition. Benefiting from the comprehensive language knowledge gained during the pre-training phase, the PLM-based recognition module effectively handles complex scenarios, including multi-line, reversed, occluded, and incomplete-detection texts. Taking advantage of the fine-tuned language model on scene recognition benchmarks and the paradigm of text block detection, extensive experiments demonstrate the superior performance of our scene text spotter across multiple public benchmarks. Additionally, we attempt to spot texts directly from an entire scene image to demonstrate the potential of PLMs, even Large Language Models (LLMs).",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4410192162",
    "type": "article"
  },
  {
    "title": "Brain-Machine Cross-Modal Alignment via Sample Relational Learning for Visual Classification",
    "doi": "https://doi.org/10.1145/3736416",
    "publication_date": "2025-05-20",
    "publication_year": 2025,
    "authors": "Dongjun Liu; Weichen Dai; Honggang Liu; Hangjie Yi; Wanzeng Kong",
    "corresponding_authors": "",
    "abstract": "Recent works on visual classification tasks have leveraged EEG signals to provide additional supervisory information, further improving the performance of the models on natural images. However, previous methods often force machine models to directly match EEG signals, which involves the transfer of modal-specific representations, leading to potentially distorted alignment of modal-shared representations. Moreover, focusing solely on aligning individual sample features neglects the alignment of relationships between samples, making it difficult to capture the potential relational reasoning capabilities in EEG signals. This relational reasoning ability is key to the human brain’s outstanding performance in visual classification tasks. Similarly, for a machine model, the complex relationships between instances are more critical than individual instances. Inspired by this, our idea is to enhance machine visual classification capabilities by imparting human-like relational reasoning, encouraging machine models to focus on the relational structure within EEG signals. To this end, we propose a brain-machine relation alignment method that constructs a cognitive model and a visual model to process EEG signals and visual images, respectively. Instead of forcing the visual model to mimic the output of an individual EEG data sample represented by the cognitive model, we encourage it to learn the mutual relations of EEG data samples. By penalizing the difference in relational structures between EEG signals and visual images, we facilitate the transfer of relational knowledge. Experiments demonstrate that the proposed method significantly improves the classification performance of the visual model. This highlights the potential of relational alignment as a robust mechanism for integrating human relational reasoning into machine learning models.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4410515520",
    "type": "article"
  },
  {
    "title": "Alleviating Hallucination in Large Vision-Language Models with Active Retrieval Augmentation",
    "doi": "https://doi.org/10.1145/3742434",
    "publication_date": "2025-06-10",
    "publication_year": 2025,
    "authors": "Xiaoye Qu; Qiyuan Chen; Wei Wei; Jiashuo Sun; Daizong Liu; Jianfeng Dong",
    "corresponding_authors": "",
    "abstract": "Despite the remarkable ability of large vision-language models (LVLMs) in image comprehension, these models frequently generate plausible yet factually incorrect responses, a phenomenon known as hallucination. Recently, in large language models (LLMs), augmenting LLMs by retrieving information from external knowledge resources has been proven as a promising solution to mitigate hallucinations. However, the retrieval augmentation in LVLM significantly lags behind the widespread applications of LVLM. Moreover, when transferred to augmenting LVLMs, sometimes the hallucination degree of the model is even exacerbated. Motivated by the research gap and counter-intuitive phenomenon, we introduce a novel framework, the Active Retrieval-Augmented large vision-language model (ARA), specifically designed to address hallucinations by incorporating three critical dimensions: (i) dissecting the retrieval targets based on the inherent hierarchical structures of images. (ii) pinpointing the most effective retrieval methods and filtering out the reliable retrieval results. (iii) timing the retrieval process to coincide with episodes of low certainty, while circumventing unnecessary retrieval during periods of high certainty. To assess the capability of our proposed ARA model in reducing hallucination, we employ three widely used LVLM models (LLaVA-1.5, Qwen-VL, and mPLUG-Owl2) across four benchmarks. Our empirical observations suggest that by utilizing fitting retrieval mechanisms and timing the retrieval judiciously, we can effectively mitigate the hallucination problem. We hope that this study can provide deeper insights into how to adapt the retrieval augmentation to LVLMs for reducing hallucinations with more effective retrieval and minimal retrieval occurrences.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4411177432",
    "type": "article"
  },
  {
    "title": "A Multimodal Semantic Fusion Network with Cross-Modal Alignment for Multimodal Sentiment Analysis",
    "doi": "https://doi.org/10.1145/3744648",
    "publication_date": "2025-06-17",
    "publication_year": 2025,
    "authors": "Shunxiang Zhang; Jiajia Liu; Yuyang Jiao; Yulei Zhang; Lei Chen; Kuan‐Ching Li",
    "corresponding_authors": "",
    "abstract": "User-generated multimodal data can provide powerful sentiment clues for sentiment analysis task. Existing works have aligned common sentiment features in different modalities through various multimodal fusion methods. However, these works have certain limitations: (1) Previous research works only align common sentiment features between image and text, without fully exploring interactions among these features, leading to suboptimal analysis results. (2) Redundant noise in image and text increases the risk of feature misalignment during cross-modal alignment. To address these issues, we propose a multimodal semantic fusion network (MSFN) to deeply explore the semantic relationship between image and text for Multimodal Sentiment Analysis (MSA). Specifically, we align image region and text word features related to sentiment by using a gated attention mechanism. Subsequently, we employ graph convolutional networks to model the interactions among these features to obtain explicit sentiment semantics. The proposed gated attention mechanism corrects potential feature misalignment during cross-modal alignment using a gating mechanism. Moreover, considering not all image-text pairs have explicit corresponding sentiment features, we integrate implicit sentiment semantics to our model for enhancing reliability in analysis. Experimental results on benchmark datasets demonstrate the effectiveness of our proposed model compared to baselines.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4411371823",
    "type": "article"
  },
  {
    "title": "ER-Depth: Enhancing the Robustness of Self-Supervised Monocular Depth Estimation in Challenging Scenes",
    "doi": "https://doi.org/10.1145/3750050",
    "publication_date": "2025-07-23",
    "publication_year": 2025,
    "authors": "Ziyang Song; Ruijie Zhu; Jing Wang; Chuxin Wang; Jianfeng He; Jiacheng Deng; Wenfei Yang; Tianzhu Zhang",
    "corresponding_authors": "",
    "abstract": "Self-supervised monocular depth estimation holds significant importance in the fields of autonomous driving and robotics. However, existing methods are typically trained and evaluated on clear, sunny datasets, overlooking the impact of various adverse conditions commonly encountered in real-world applications, such as rainy weather, low visibility, and motion blur. As a result, they often struggle in challenging scenarios and produce artifacts. To address this issue, we propose ER-Depth, a novel two-stage self-supervised framework designed for robust depth estimation. In the first stage, we propose perturbation-invariant depth consistency regularization to propagate reliable supervision from standard to challenging scenes. In the second stage, we adopt the Mean Teacher paradigm for self-distillation and present a novel consistency-based pseudo-label filtering strategy to improve the quality of pseudo-labels. Extensive experiments demonstrate that our method exhibits exceptional robustness in challenging scenarios while maintaining high performance in standard scenes, significantly outperforming existing state-of-the-art methods on challenging KITTI-C, DrivingStereo, and NuScenes-Night benchmarks. Project page: https://ruijiezhu94.github.io/ERDepth_page .",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4412602629",
    "type": "article"
  },
  {
    "title": "FAST: Flexibly Controllable Arbitrary Style Transfer via Latent Diffusion models",
    "doi": "https://doi.org/10.1145/3748655",
    "publication_date": "2025-07-29",
    "publication_year": 2025,
    "authors": "Hanzhang Wang; Haoran Wang; Zhongrui Yu; M.-T. Sun; Junjun Jiang; Xianming Liu; Deming Zhai",
    "corresponding_authors": "",
    "abstract": "The goal of Arbitrary Style Transfer (AST) is injecting the artistic features of a style reference into a given image/video. Existing methods usually pursue the balance between style and content by adjusting general coarse-level stylized strength, thereby leading to unsatisfactory results and hindering their practical application. To address this critical issue, a novel AST approach namely F lexibly Controllable A rbitrary S tyle T ransfer ( FAST ) is proposed, which is capable of explicitly customizing the stylization results according to various source of semantic clues. In the specific, our model is constructed based on Latent Diffusion Model (LDM) and elaborately designed to absorb content and style instance as conditions of LDM. It is characterized by introducing of Style-Adapter , which allows users to flexibly manipulate the stylization results via aligning multi-level style control information and intrinsic knowledge in LDM, meanwhile enhancing the model with improved capacity to harmonize content detail retention and stylization strength. Lastly, our model is extended to handle video AST task. A novel learning objective is leveraged for video diffusion model training, which considerably improve cross-frame temporal consistency on the premise of maintaining stylization strength. Qualitative and quantitative comparisons as well as user studies demonstrate our presented approach outperforms the existing SoTA methods in generating visually plausible stylization results. The project homepage for the paper is available at: https://fast-ldm.github.io/.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4412708776",
    "type": "article"
  },
  {
    "title": "MuralAgent: Enhancing Ancient Mural Outpainting with RAG-Based Texts and Multimodal Integration",
    "doi": "https://doi.org/10.1145/3743679",
    "publication_date": "2025-08-26",
    "publication_year": 2025,
    "authors": "Zishan Xu; Xiaofeng Zhang; Yuqing Yang; Wei Chen; Jueting Liu; Tingting Xu; Zehua Wang; Abdulmotaleb El Saddik",
    "corresponding_authors": "",
    "abstract": "In the context of the digital age, utilizing cutting-edge technology for the digitization and creative expansion of ancient murals is crucial, aimed at preserving and passing on cultural heritage. Existing image outpainting techniques suffer from a lack of semantic guidance. This paper introduces MuralAgent, a multimodal model based on Retrieval-Augmented Generation (RAG) technology. It precisely extracts key information from mural images and integrates it with a constructed ancient texts knowledge base to ensure the cultural and semantic consistency of the expanded images. Moreover, fine-tuning the Stable Diffusion model ensures the fidelity of the generated image styles. Specifically, this study involves constructing an ancient texts knowledge base for accurate matching, designing specific prompts for GPT-4V(ision) to extract key information, and innovatively expanding artworks through Stable Diffusion, providing a novel way for the public to reinterpret ancient murals.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4413682358",
    "type": "article"
  },
  {
    "title": "Fusion of AV features and external information sources for event detection in team sports video",
    "doi": "https://doi.org/10.1145/1126004.1126007",
    "publication_date": "2006-02-01",
    "publication_year": 2006,
    "authors": "Huaxin Xu; Tat‐Seng Chua",
    "corresponding_authors": "",
    "abstract": "The use of AV features alone is insufficient to induce high-level semantics. This article proposes a framework that utilizes both internal AV features and various types of external information sources for event detection in team sports video. Three schemes are also proposed to tackle the asynchronism between the fusion of AV and external information. The framework is extensible as it can provide increasing functionalities given more detailed external information and domain knowledge. By demonstrating its effectiveness on soccer and American football, we believe that with the availability of appropriate domain knowledge, the framework is applicable to other team sports.",
    "cited_by_count": 55,
    "openalex_id": "https://openalex.org/W1981758193",
    "type": "article"
  },
  {
    "title": "Detecting social interactions of the elderly in a nursing home environment",
    "doi": "https://doi.org/10.1145/1198302.1198308",
    "publication_date": "2007-02-01",
    "publication_year": 2007,
    "authors": "Datong Chen; Jie Yang; Robert Malkin; Howard D. Wactlar",
    "corresponding_authors": "",
    "abstract": "Social interaction plays an important role in our daily lives. It is one of the most important indicators of physical or mental changes in aging patients. In this article, we investigate the problem of detecting social interaction patterns of patients in a skilled nursing facility using audio/visual records. Our studies consist of both a “Wizard of Oz” style study and an experimental study of various sensors and detection models for detecting and summarizing social interactions among aging patients and caregivers. We first simulate plausible sensors using human labeling on top of audio and visual data collected from a skilled nursing facility. The most useful sensors and robust detection models are determined using the simulated sensors. We then present the implementation of some real sensors based on video and audio analysis techniques and evaluate the performance of these implementations in detecting interactions. We conclude the article with discussions and future work.",
    "cited_by_count": 48,
    "openalex_id": "https://openalex.org/W1970025976",
    "type": "article"
  },
  {
    "title": "Segmentation and recognition of motion streams by similarity search",
    "doi": "https://doi.org/10.1145/1236471.1236475",
    "publication_date": "2007-08-01",
    "publication_year": 2007,
    "authors": "Chuanjun Li; Si Zheng; Balakrishnan Prabhakaran",
    "corresponding_authors": "",
    "abstract": "Fast and accurate recognition of motion data streams from gesture sensing and motion capture devices has many applications and is the focus of this article. Based on the analysis of the geometric structures revealed by singular value decompositions (SVD) of motion data, a similarity measure is proposed for simultaneously segmenting and recognizing motion streams. A direction identification approach is explored to further differentiate motions with similar data geometric structures. Experiments show that the proposed similarity measure can segment and recognize motion streams of variable lengths with high accuracy, without knowing beforehand the number of motions in a stream.",
    "cited_by_count": 48,
    "openalex_id": "https://openalex.org/W2065460079",
    "type": "article"
  },
  {
    "title": "Dynamic privacy assessment in a smart house environment using multimodal sensing",
    "doi": "https://doi.org/10.1145/1413862.1413863",
    "publication_date": "2008-11-01",
    "publication_year": 2008,
    "authors": "Simon Moncrieff; Svetha Venkatesh; Geoff West",
    "corresponding_authors": "",
    "abstract": "Surveillance applications in private environments such as smart houses require a privacy management policy if such systems are to be accepted by the occupants of the environment. This is due to the invasive nature of surveillance, and the private nature of the home. In this article, we propose a framework for dynamically altering the privacy policy applied to the monitoring of a smart house based on the situation within the environment. Initially the situation, or context, within the environment is determined; we identify several factors for determining environmental context, and propose methods to quantify the context using audio and binary sensor data. The context is then mapped to an appropriate privacy policy, which is implemented by applying data hiding techniques to control access to data gathered from various information sources. The significance of this work lies in the examination of privacy issues related to assisted-living smart house environments. A single privacy policy in such applications would be either too restrictive for an observer, for example, a carer, or too invasive for the occupants. We address this by proposing a dynamic method, with the aim of decreasing the invasiveness of the technology, while retaining the purpose of the system.",
    "cited_by_count": 47,
    "openalex_id": "https://openalex.org/W2052534480",
    "type": "article"
  },
  {
    "title": "Clustering and searching WWW images using link and page layout analysis",
    "doi": "https://doi.org/10.1145/1230812.1230816",
    "publication_date": "2007-05-01",
    "publication_year": 2007,
    "authors": "Xiaofei He; Deng Cai; Ji-Rong Wen; Wei‐Ying Ma; Hao Zhang",
    "corresponding_authors": "",
    "abstract": "Due to the rapid growth of the number of digital images on the Web, there is an increasing demand for an effective and efficient method for organizing and retrieving the available images. This article describes iFind, a system for clustering and searching WWW images. By using a vision-based page segmentation algorithm, a Web page is partitioned into blocks, and the textual and link information of an image can be accurately extracted from the block containing that image. The textual information is used for image indexing. By extracting the page-to-block, block-to-image, block-to-page relationships through link structure and page layout analysis, we construct an image graph. Our method is less sensitive to noisy links than previous methods like PageRank, HITS, and PicASHOW, and hence the image graph can better reflect the semantic relationship between images. Using the notion of Markov Chain, we can compute the limiting probability distributions of the images, ImageRanks, which characterize the importance of the images. The ImageRanks are combined with the relevance scores to produce the final ranking for image search. With the graph models, we can also use techniques from spectral graph theory for image clustering and embedding, or 2-D visualization. Some experimental results on 11.6 million images downloaded from the Web are provided in the article.",
    "cited_by_count": 46,
    "openalex_id": "https://openalex.org/W1963959122",
    "type": "article"
  },
  {
    "title": "Content-adaptive digital music watermarking based on music structure analysis",
    "doi": "https://doi.org/10.1145/1198302.1198303",
    "publication_date": "2007-02-01",
    "publication_year": 2007,
    "authors": "Changsheng Xu; Namunu C. Maddage; Xi Shao; Qi Tian",
    "corresponding_authors": "",
    "abstract": "A novel content-adaptive music watermarking technique is proposed in this article. To optimally balance inaudibility and robustness when embedding and extracting watermarks, the embedding scheme is highly related to the music structure and human auditory system (HAS). A note-based segmentation method is proposed and used for music vocal/instrumental boundary detection. A multiple bit hopping and hiding scheme with different embedding parameters is applied to vocal and instrumental frames of the music. The experimental results in inaudibility and robustness are provided to support all novel features in the proposed watermarking scheme.",
    "cited_by_count": 46,
    "openalex_id": "https://openalex.org/W1975181437",
    "type": "article"
  },
  {
    "title": "Sensing and using social context",
    "doi": "https://doi.org/10.1145/1413862.1413864",
    "publication_date": "2008-11-01",
    "publication_year": 2008,
    "authors": "Brett Adams; Dinh Phung; Svetha Venkatesh",
    "corresponding_authors": "",
    "abstract": "We present online algorithms to extract social context: Social spheres are labeled locations of significance, represented as convex hulls extracted from GPS traces. Colocation is determined from Bluetooth and GPS to extract social rhythms, patterns in time, duration, place, and people corresponding to real-world activities. Social ties are formulated from proximity and shared spheres and rhythms. Quantitative evaluation is performed for 10+ million samples over 45 man-months. Applications are presented with assessment of perceived utility: Socio-Graph , a video and photo browser with filters for social metadata, and Jive , a blog browser that uses rhythms to discover similarity between entries automatically.",
    "cited_by_count": 46,
    "openalex_id": "https://openalex.org/W2076104882",
    "type": "article"
  },
  {
    "title": "Authoring, viewing, and generating hypervideo",
    "doi": "https://doi.org/10.1145/1413862.1413868",
    "publication_date": "2008-11-01",
    "publication_year": 2008,
    "authors": "Frank Shipman; Andreas Girgensohn; Lynn Wilcox",
    "corresponding_authors": "",
    "abstract": "Hyper-Hitchcock consists of three components for creating and viewing a form of interactive video called detail-on-demand video: a hypervideo editor, a hypervideo player, and algorithms for automatically generating hypervideo summaries. Detail-on-demand video is a form of hypervideo that supports one hyperlink at a time for navigating between video sequences. The Hyper-Hitchcock editor enables authoring of detail-on-demand video without programming and uses video processing to aid in the authoring process. The Hyper-Hitchcock player uses labels and keyframes to support navigation through and back hyperlinks. Hyper-Hitchcock includes techniques for automatically generating hypervideo summaries of one or more videos that take the form of multiple linear summaries of different lengths with links from the shorter to the longer summaries. User studies on authoring and viewing provided insight into the various roles of links in hypervideo and found that player interface design greatly affects people's understanding of hypervideo structure and the video they access.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W2071206385",
    "type": "article"
  },
  {
    "title": "Server selection in large-scale video-on-demand systems",
    "doi": "https://doi.org/10.1145/1671954.1671955",
    "publication_date": "2010-02-01",
    "publication_year": 2010,
    "authors": "Niklas Carlsson; Derek L. Eager",
    "corresponding_authors": "",
    "abstract": "Video on demand, particularly with user-generated content, is emerging as one of the most bandwidth-intensive applications on the Internet. Owing to content control and other issues, some video-on-demand systems attempt to prevent downloading and peer-to-peer content delivery. Instead, such systems rely on server replication, such as via third-party content distribution networks, to support video streaming (or pseudostreaming) to their clients. A major issue with such systems is the cost of the required server resources. By synchronizing the video streams for clients that make closely spaced requests for the same video from the same server, server costs (such as for retrieval of the video data from disk) can be amortized over multiple requests. A fundamental trade-off then arises, however, with respect to server selection. Network delivery cost is minimized by selecting the nearest server, while server cost is minimized by directing closely spaced requests for the same video to a common server. This article compares classes of server selection policies within the context of a simple system model. We conclude that: (i) server selection using dynamic system state information (rather than only proximities and average loads) can yield large improvements in performance, (ii) deferring server selection for a request as late as possible (i.e., until just before streaming is to begin) can yield additional large improvements, and (iii) within the class of policies using dynamic state information and deferred selection, policies using only “local” (rather than global) request information are able to achieve most of the potential performance gains.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W2085356565",
    "type": "article"
  },
  {
    "title": "Touchable 3D video system",
    "doi": "https://doi.org/10.1145/1596990.1596993",
    "publication_date": "2009-10-01",
    "publication_year": 2009,
    "authors": "Jongeun Cha; Mohamad Eid; Abdulmotaleb El Saddik",
    "corresponding_authors": "",
    "abstract": "Multimedia technologies are reaching the limits of providing audio-visual media that viewers consume passively. An important factor, which will ultimately enhance the user's experience in terms of impressiveness and immersion, is interaction. Among daily life interactions, haptic interaction plays a prominent role in enhancing the quality of experience of users, and in promoting physical and emotional development. Therefore, a critical step in multimedia research is expected to bring the sense of touch, or haptics, into multimedia systems and applications. This article proposes a touchable 3D video system where viewers can actively touch a video scene through a force-feedback device, and presents the underlying technologies in three functional components: (1) contents generation, (2) contents transmission, and (3) viewing and interaction. First of all, we introduce a depth image-based haptic representation (DIBHR) method that adds haptic and heightmap images, in addition to the traditional depth image-based representation (DIBR), to encode the haptic surface properties of the video media. In this representation, the haptic image contains the stiffness, static friction, and dynamic friction, whereas the heightmap image contains roughness of the video contents. Based on this representation method, we discuss how to generate synthetic and natural (real) video media through a 3D modeling tool and a depth camera, respectively. Next, we introduce a transmission mechanism based on the MPEG-4 framework where new MPEG-4 BIFS nodes are designed to describe the haptic scene. Finally, a haptic rendering algorithm to compute the interaction force between the scene and the viewer is described. As a result, the performance of the haptic rendering algorithm is evaluated in terms of computational time and smooth contact force. It operates marginally within a 1 kHz update rate that is required to provide stable interaction force and provide smoother contact force with the depth image that has high frequency geometrical noise using a median filter.",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W1988872807",
    "type": "article"
  },
  {
    "title": "Examining presence and lightweight messaging in a social television experience",
    "doi": "https://doi.org/10.1145/1412196.1412200",
    "publication_date": "2008-10-01",
    "publication_year": 2008,
    "authors": "Crysta Metcalf; Gunnar Harboe; Joe Tullio; Noel Massey; Guy Romano; Elaine M. Huang; Frank Bentley",
    "corresponding_authors": "",
    "abstract": "We report on a field evaluation of a prototype social television system (Social TV) that incorporates lightweight messaging as well as ambient awareness of user presence on the system. This evaluation was conducted over a two-week period and involved the participation of ten households. Participants appreciated the ability to see their buddies' presence on the system, the ability to see or suggest the programs they were currently watching, and the ability to send short messages to one another. The presence facilities available in Social TV also allowed participants to learn more about one another's TV viewing habits and preferences, and fostered a sense of connectedness between them. However, they also felt constrained by the limitations of the communication options available to them and demanded free-form text or voice chat to be able to fully express themselves.",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W2012977491",
    "type": "article"
  },
  {
    "title": "Multigranularity reuse of learning resources",
    "doi": "https://doi.org/10.1145/1870121.1870122",
    "publication_date": "2011-01-01",
    "publication_year": 2011,
    "authors": "Marek Meyer; Christoph Rensing; Ralf Steinmetz",
    "corresponding_authors": "",
    "abstract": "This article investigates a scenario of reuse in which existing learning resources serve as preliminary products for the creation of new learning resources. Authors should be able to reuse learning resources and also parts of them at different levels of granularity in a modular way. The requirements of multigranularity reuse are analyzed and compared to existing solutions. A concept for modular, multigranularity reuse is presented in this article. It is also shown how this kind of reuse can be achieved in practise.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W1997987543",
    "type": "article"
  },
  {
    "title": "Towards decrypting attractiveness via multi-modality cues",
    "doi": "https://doi.org/10.1145/2501643.2501650",
    "publication_date": "2013-08-01",
    "publication_year": 2013,
    "authors": "Tam Nguyen; Si Liu; Bingbing Ni; Jun Tan; Yong Rui; Shuicheng Yan",
    "corresponding_authors": "",
    "abstract": "Decrypting the secret of beauty or attractiveness has been the pursuit of artists and philosophers for centuries. To date, the computational model for attractiveness estimation has been actively explored in computer vision and multimedia community, yet with the focus mainly on facial features. In this article, we conduct a comprehensive study on female attractiveness conveyed by single/multiple modalities of cues, that is, face, dressing and/or voice, and aim to discover how different modalities individually and collectively affect the human sense of beauty. To extensively investigate the problem, we collect the Multi-Modality Beauty (M 2 B) dataset, which is annotated with attractiveness levels converted from manual k -wise ratings and semantic attributes of different modalities. Inspired by the common consensus that middle-level attribute prediction can assist higher-level computer vision tasks, we manually labeled many attributes for each modality. Next, a tri-layer Dual-supervised Feature-Attribute-Task (DFAT) network is proposed to jointly learn the attribute model and attractiveness model of single/multiple modalities. To remedy possible loss of information caused by incomplete manual attributes, we also propose a novel Latent Dual-supervised Feature-Attribute-Task (LDFAT) network, where latent attributes are combined with manual attributes to contribute to the final attractiveness estimation. The extensive experimental evaluations on the collected M 2 B dataset well demonstrate the effectiveness of the proposed DFAT and LDFAT networks for female attractiveness prediction.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W2093212601",
    "type": "article"
  },
  {
    "title": "Automatic creation of photo books from stories in social media",
    "doi": "https://doi.org/10.1145/2037676.2037684",
    "publication_date": "2011-10-01",
    "publication_year": 2011,
    "authors": "Mohamad Rabbath; Philipp Sandhaus; Susanne Boll",
    "corresponding_authors": "",
    "abstract": "Photos are a special way to tell stories of our best memories and moments. The representation of those photos in appealing physical photo books is highly appreciated by many people. Today, many photos are shared via social networking sites, where people upload their photos and share their stories with their friends. The members of social networks comment on each other's photos, add tags or descriptions and upload new photos of the same events to their albums. While the media of different personal events are available on the social network, there is no easy way to collect and bundle them into a story and print this story as a photo book. We propose an approach to automatically detect media elements that match a query (where, when, what, who) in the user's social network and intelligently arrange and compose them into a printable photo book. We combine content analysis of text and images to automatically and semi-automatically select photos of a specific story. We calculate the probabilities of each two photos to belong to the same event using an Expectation-Maximization algorithm that we propose in order to be able to retrieve them easily when receiving the user queries, and we address the differences between our model and other models that use similar proposed algorithms. People's tags and the interaction between the users and the photos as well as other semantic information are exploited to select important photos that are suitable to create the photo book. The selected photos and derived semantics are then employed to automatically create an appealing layout for the photo book.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W1974835320",
    "type": "article"
  },
  {
    "title": "SMIL builder",
    "doi": "https://doi.org/10.1145/1870121.1870123",
    "publication_date": "2011-01-01",
    "publication_year": 2011,
    "authors": "Samia Bouyakoub; Abdelkader Belkhir",
    "corresponding_authors": "",
    "abstract": "We present in this article a temporal SMIL editor with incremental verification capabilities, based on a formal Petri Net--based model. Our authoring tool, named SMIL Builder, allows the author to “build” his document step by step, while insuring at every stage the validity of the current state of the document. These incremental authoring and consistency checking features are based on the H-SMIL-Net model (Hierarchical SMIL Petri Net), a temporal extension of Petri Nets. Our aim is to propose an easy-to-use temporal environment which can satisfy a wide range of users; so we opted for an interface combining simplicity and ergonomics.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W2010985784",
    "type": "article"
  },
  {
    "title": "Game-on-demand:",
    "doi": "https://doi.org/10.1145/2000486.2000493",
    "publication_date": "2011-08-01",
    "publication_year": 2011,
    "authors": "Frederick W. B. Li; Rynson W. H. Lau; Danny Kilis; Lewis W. F. Li",
    "corresponding_authors": "",
    "abstract": "In recent years, online gaming has become very popular. In contrast to stand-alone games, online games tend to be large-scale and typically support interactions among users. However, due to the high network latency of the Internet, smooth interactions among the users are often difficult. The huge and dynamic geometry data sets also make it difficult for some machines, such as handheld devices, to run those games. These constraints have stimulated some research interests on online gaming, which may be broadly categorized into two areas: technological support and user-perceived visual quality . Technological support concerns the performance issues while user-perceived visual quality concerns the presentation quality and accuracy of the game. In this article, we propose a game-on-demand engine that addresses both research areas. The engine distributes game content progressively to each client based on the player's location in the game scene. It comprises a two-level content management scheme and a prioritized content delivery scheme to help identify and deliver relevant game content at appropriate quality to each client dynamically. To improve the effectiveness of the prioritized content delivery scheme, it also includes a synchronization scheme to minimize the location discrepancy of avatars (game players). We demonstrate the performance of the proposed engine through numerous experiments.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W2016608234",
    "type": "article"
  },
  {
    "title": "Evolution of temporal multimedia synchronization principles",
    "doi": "https://doi.org/10.1145/2490821",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Zixia Huang; Klara Nahrstedt; Ralf Steinmetz",
    "corresponding_authors": "",
    "abstract": "The evolution of multimedia applications has drastically changed human life and behaviors. New communication technologies lead to new requirements for multimedia synchronization. This article presents a historical view of temporal synchronization studies focusing on continuous multimedia. We demonstrate how the development of multimedia systems has created new challenges for synchronization technologies. We conclude with a new application-dependent, multilocation, multirequirement synchronization framework to address these new challenges.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W2023657454",
    "type": "article"
  },
  {
    "title": "A 3D-HEVC Fast Mode Decision Algorithm for Real-Time Applications",
    "doi": "https://doi.org/10.1145/2700298",
    "publication_date": "2015-02-05",
    "publication_year": 2015,
    "authors": "Liquan Shen; Ping An; Zhaoyang Zhang; Qianqian Hu; Zhengchuan Chen",
    "corresponding_authors": "",
    "abstract": "3D High Efficiency Video Coding (3D-HEVC) is an extension of the HEVC standard for coding of multiview videos and depth maps. It inherits the same quadtree coding structure as HEVC for both components, which allows recursively splitting into four equal-sized coding units (CU). One of 11 different prediction modes is chosen to code a CU in inter-frames. Similar to the joint model of H.264/AVC, the mode decision process in HM (reference software of HEVC) is performed using all the possible depth levels and prediction modes to find the one with the least rate distortion cost using a Lagrange multiplier. Furthermore, both motion estimation and disparity estimation need to be performed in the encoding process of 3D-HEVC. Those tools achieve high coding efficiency, but lead to a significant computational complexity. In this article, we propose a fast mode decision algorithm for 3D-HEVC. Since multiview videos and their associated depth maps represent the same scene, at the same time instant, their prediction modes are closely linked. Furthermore, the prediction information of a CU at the depth level X is strongly related to that of its parent CU at the depth level X-1 in the quadtree coding structure of HEVC since two corresponding CUs from two neighboring depth levels share similar video characteristics. The proposed algorithm jointly exploits the inter-view coding mode correlation, the inter-component (texture-depth) correlation and the inter-level correlation in the quadtree structure of 3D-HEVC. Experimental results show that our algorithm saves 66% encoder runtime on average with only a 0.2% BD-Rate increase on coded views and 1.3% BD-Rate increase on synthesized views.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W2029519528",
    "type": "article"
  },
  {
    "title": "An Advanced Visibility Restoration Algorithm for Single Hazy Images",
    "doi": "https://doi.org/10.1145/2726947",
    "publication_date": "2015-06-02",
    "publication_year": 2015,
    "authors": "Bo‐Hao Chen; Shih-Chia Huang",
    "corresponding_authors": "",
    "abstract": "Haze removal is the process by which horizontal obscuration is eliminated from hazy images captured during inclement weather. Images captured in natural environments with varied weather conditions frequently exhibit localized light sources or color-shift effects. The occurrence of these effects presents a difficult challenge for hazy image restoration, with which many traditional restoration methods cannot adequately contend. In this article, we present a new image haze removal approach based on Fisher's linear discriminant-based dual dark channel prior scheme in order to solve the problems associated with the presence of localized light sources and color shifts, and thereby achieve effective restoration. Experimental restoration results via qualitative and quantitative evaluations show that our proposed approach can provide higher haze-removal efficacy for images captured in varied weather conditions than can the other state-of-the-art approaches.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W2254711202",
    "type": "article"
  },
  {
    "title": "Secure Cloud-Based Image Tampering Detection and Localization Using POB Number System",
    "doi": "https://doi.org/10.1145/3077140",
    "publication_date": "2017-06-28",
    "publication_year": 2017,
    "authors": "Priyanka Singh; Balasubramanian Raman; Nishant Agarwal; Pradeep K. Atrey",
    "corresponding_authors": "",
    "abstract": "The benefits of high-end computation infrastructure facilities provided by cloud-based multimedia systems are attracting people all around the globe. However, such cloud-based systems possess security issues as third party servers become involved in them. Rendering data in an unreadable form so that no information is revealed to the cloud data centers will serve as the best solution to these security issues. One such image encryption scheme based on a Permutation Ordered Binary Number System has been proposed in this work. It distributes the image information in totally random shares, which can be stored at the cloud data centers. Further, the proposed scheme authenticates the shares at the pixel level. If any tampering is done at the cloud servers, the scheme can accurately identify the altered pixels via authentication bits and localizes the tampered area. The tampered portion is also reflected back in the reconstructed image that is obtained at the authentic user end. The experimental results validate the efficacy of the proposed scheme against various kinds of possible attacks, tested with a variety of images. The tamper detection accuracy has been computed on a pixel basis and found to be satisfactorily high for most of the tampering scenarios.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W2727850327",
    "type": "article"
  },
  {
    "title": "Design and Performance Evaluation of Network-assisted Control Strategies for HTTP Adaptive Streaming",
    "doi": "https://doi.org/10.1145/3092836",
    "publication_date": "2017-06-28",
    "publication_year": 2017,
    "authors": "Giuseppe Cofano; Luca De Cicco; Thomas Zinner; Anh Vu Nguyen‐Ngoc; Phuoc Tran‐Gia; Saverio Mascolo",
    "corresponding_authors": "",
    "abstract": "This article investigates several network-assisted streaming approaches that rely on active cooperation between video streaming applications and the network. We build a Video Control Plane that enforces Video Quality Fairness among concurrent video flows generated by heterogeneous client devices. For this purpose, a max-min fairness optimization problem is solved at runtime. We compare two approaches to actuate the optimal solution in an Software Defined Networking network: The first one allocates network bandwidth slices to video flows, and the second one guides video players in the video bitrate selection. We assess performance through several QoE-related metrics, such as Video Quality Fairness, video quality, and switching frequency. The impact of client-side adaptation algorithms is also investigated.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W2732567092",
    "type": "article"
  },
  {
    "title": "Spatial-Temporal Tag Mining for Automatic Geospatial Video Annotation",
    "doi": "https://doi.org/10.1145/2658981",
    "publication_date": "2015-01-07",
    "publication_year": 2015,
    "authors": "Yifang Yin; Zhijie Shen; Luming Zhang; Roger Zimmermann",
    "corresponding_authors": "",
    "abstract": "Videos are increasingly geotagged and used in practical and powerful GIS applications. However, video search and management operations are typically supported by manual textual annotations, which are subjective and laborious. Therefore, research has been conducted to automate or semi-automate this process. Since a diverse vocabulary for video annotations is of paramount importance towards good search results, this article proposes to leverage crowdsourced data from social multimedia applications that host tags of diverse semantics to build a spatio-temporal tag repository, consequently acting as input to our auto-annotation approach. In particular, to build the tag store, we retrieve the necessary data from several social multimedia applications, mine both the spatial and temporal features of the tags, and then refine and index them accordingly. To better integrate the tag repository, we extend our previous approach by leveraging the temporal characteristics of videos as well. Moreover, we set up additional ranking criteria on the basis of tag similarity, popularity and location bias. Experimental results demonstrate that, by making use of such a tag repository, the generated tags have a wide range of semantics, and the resulting rankings are more consistent with human perception.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2059156133",
    "type": "article"
  },
  {
    "title": "Attribute-Augmented Semantic Hierarchy",
    "doi": "https://doi.org/10.1145/2637291",
    "publication_date": "2014-10-01",
    "publication_year": 2014,
    "authors": "Hanwang Zhang; Zheng-Jun Zha; Yang Yang; Shuicheng Yan; Yue Gao; Tat‐Seng Chua",
    "corresponding_authors": "",
    "abstract": "This article presents a novel attribute-augmented semantic hierarchy (A 2 SH) and demonstrates its effectiveness in bridging both the semantic and intention gaps in content-based image retrieval (CBIR). A 2 SH organizes semantic concepts into multiple semantic levels and augments each concept with a set of related attributes. The attributes are used to describe the multiple facets of the concept and act as the intermediate bridge connecting the concept and low-level visual content. An hierarchical semantic similarity function is learned to characterize the semantic similarities among images for retrieval. To better capture user search intent, a hybrid feedback mechanism is developed, which collects hybrid feedback on attributes and images. This feedback is then used to refine the search results based on A 2 SH. We use A 2 SH as a basis to develop a unified content-based image retrieval system. We conduct extensive experiments on a large-scale dataset of over one million Web images. Experimental results show that the proposed A 2 SH can characterize the semantic affinities among images accurately and can shape user search intent quickly, leading to more accurate search results as compared to state-of-the-art CBIR solutions.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2124065797",
    "type": "article"
  },
  {
    "title": "An Efficient Motion Detection and Tracking Scheme for Encrypted Surveillance Videos",
    "doi": "https://doi.org/10.1145/3131342",
    "publication_date": "2017-09-18",
    "publication_year": 2017,
    "authors": "Jianting Guo; Peijia Zheng; Jiwu Huang",
    "corresponding_authors": "",
    "abstract": "Performing detection on surveillance videos contributes significantly to the goals of safety and security. However, performing detection on unprotected surveillance video may reveal the privacy of innocent people in the video. Therefore, striking a proper balance between maintaining personal privacy while enhancing the feasibility of detection is an important issue. One promising solution to this problem is to encrypt the surveillance videos and perform detection on the encrypted videos. Most existing encrypted signal processing methods focus on still images or small data volumes; however, because videos are typically much larger, investigating how to process encrypted videos is a significant challenge. In this article, we propose an efficient motion detection and tracking scheme for encrypted H.264/AVC video bitstreams, which does not require the previous decryption on the encrypted video. The main idea is to first estimate motion information from the bitstream structure and codeword length and, then, propose a region update (RU) algorithm to deal with the loss and error drifting of motion caused by the video encryption. The RU algorithm is designed based on the prior knowledge that the object motion in the video is continuous in space and time. Compared to the existing scheme, which is based on video encryption that occurs at the pixel level, the proposed scheme has the advantages of requiring only a small storage of the encrypted video and has a low computational cost for both encryption and detection. Experimental results show that our scheme performs better regarding detection accuracy and execution speed. Moreover, the proposed scheme can work with more than one format-compliant video encryption method, provided that the positions of the macroblocks can be extracted from the encrypted video bitstream. Due to the coupling of video stream encryption and detection algorithms, our scheme can be directly connected to the video stream output (e.g., surveillance cameras) without requiring any camera modifications.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2754279782",
    "type": "article"
  },
  {
    "title": "Representation, Analysis, and Recognition of 3D Humans",
    "doi": "https://doi.org/10.1145/3182179",
    "publication_date": "2018-03-06",
    "publication_year": 2018,
    "authors": "Stefano Berretti; Mohamed Daoudi; Pavan Turaga; Anup Basu",
    "corresponding_authors": "",
    "abstract": "Computer Vision and Multimedia solutions are now offering an increasing number of applications ready for use by end users in everyday life. Many of these applications are centered for detection, representation, and analysis of face and body. Methods based on 2D images and videos are the most widespread, but there is a recent trend that successfully extends the study to 3D human data as acquired by a new generation of 3D acquisition devices. Based on these premises, in this survey, we provide an overview on the newly designed techniques that exploit 3D human data and also prospect the most promising current and future research directions. In particular, we first propose a taxonomy of the representation methods, distinguishing between spatial and temporal modeling of the data. Then, we focus on the analysis and recognition of 3D humans from 3D static and dynamic data, considering many applications for body and face.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2782660655",
    "type": "article"
  },
  {
    "title": "Delay-Sensitive Video Computing in the Cloud",
    "doi": "https://doi.org/10.1145/3212804",
    "publication_date": "2018-06-27",
    "publication_year": 2018,
    "authors": "Maha Abdallah; Carsten Griwodz; Kuan‐Ta Chen; Gwendal Simon; Pin-Chun Wang; Cheng-Hsin Hsu",
    "corresponding_authors": "",
    "abstract": "While cloud servers provide a tremendous amount of resources for networked video applications, most successful stories of cloud-assisted video applications are presentational video services, such as YouTube and NetFlix. This article surveys the recent advances on delay-sensitive video computations in the cloud, which are crucial to cloud-assisted conversational video services, such as cloud gaming, Virtual Reality (VR), Augmented Reality (AR), and telepresence. Supporting conversational video services with cloud resources is challenging because most cloud servers are far away from the end users while these services incur the following stringent requirements: high bandwidth, short delay, and high heterogeneity. In this article, we cover the literature with a top-down approach: from applications and experience, to architecture and management, and to optimization in and outside of the cloud. We also point out major open challenges, hoping to stimulate more research activities in this emerging and exciting direction.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2811481541",
    "type": "article"
  },
  {
    "title": "Correspondence Autoencoders for Cross-Modal Retrieval",
    "doi": "https://doi.org/10.1145/2808205",
    "publication_date": "2015-10-21",
    "publication_year": 2015,
    "authors": "Fangxiang Feng; Xiaojie Wang; Ruifan Li; Ibrar Ahmad",
    "corresponding_authors": "",
    "abstract": "This article considers the problem of cross-modal retrieval, such as using a text query to search for images and vice-versa. Based on different autoencoders, several novel models are proposed here for solving this problem. These models are constructed by correlating hidden representations of a pair of autoencoders. A novel optimal objective, which minimizes a linear combination of the representation learning errors for each modality and the correlation learning error between hidden representations of two modalities, is used to train the model as a whole. Minimizing the correlation learning error forces the model to learn hidden representations with only common information in different modalities, while minimizing the representation learning error makes hidden representations good enough to reconstruct inputs of each modality. To balance the two kind of errors induced by representation learning and correlation learning, we set a specific parameter in our models. Furthermore, according to the modalities the models attempt to reconstruct they are divided into two groups. One group including three models is named multimodal reconstruction correspondence autoencoder since it reconstructs both modalities. The other group including two models is named unimodal reconstruction correspondence autoencoder since it reconstructs a single modality. The proposed models are evaluated on three publicly available datasets. And our experiments demonstrate that our proposed correspondence autoencoders perform significantly better than three canonical correlation analysis based models and two popular multimodal deep models on cross-modal retrieval tasks.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2032001302",
    "type": "article"
  },
  {
    "title": "Aesthetics-Guided Summarization from Multiple User Generated Videos",
    "doi": "https://doi.org/10.1145/2659520",
    "publication_date": "2015-01-07",
    "publication_year": 2015,
    "authors": "Ying Zhang; Luming Zhang; Roger Zimmermann",
    "corresponding_authors": "",
    "abstract": "In recent years, with the rapid development of camera technology and portable devices, we have witnessed a flourish of user generated videos, which are gradually reshaping the traditional professional video oriented media market. The volume of user generated videos in repositories is increasing at a rapid rate. In today's video retrieval systems, a simple query will return many videos which seriously increase the viewing burden. To manage these video retrievals and provide viewers with an efficient way to browse, we introduce a system to automatically generate a summarization from multiple user generated videos and present their salience to viewers in an enjoyable manner. Among multiple consumer videos, we find their qualities to be highly diverse due to various factors such as a photographer's experience or environmental conditions at the time of capture. Such quality inspires us to include a video quality evaluation component into the video summarization since videos with poor qualities can seriously degrade the viewing experience. We first propose a probabilistic model to evaluate the aesthetic quality of each user generated video. This model compares the rich aesthetics information from several well-known photo databases with generic unlabeled consumer videos, under a human perception component indicating the correlation between a video and its constituting frames. Subjective studies were carried out with the results indicating that our method is reliable. Then a novel graph-based formulation is proposed for the multi-video summarization task. Desirable summarization criteria is incorporated as the graph attributes and the problem is solved through a dynamic programming framework. Comparisons with several state-of-the-art methods demonstrate that our algorithm performs better than other methods in generating a skimming video in preserving the essential scenes from the original multiple input videos, with smooth transitions among consecutive segments and appealing aesthetics overall.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2054904802",
    "type": "article"
  },
  {
    "title": "Improved Audio Steganalytic Feature and Its Applications in Audio Forensics",
    "doi": "https://doi.org/10.1145/3190575",
    "publication_date": "2018-04-25",
    "publication_year": 2018,
    "authors": "Weiqi Luo; Haodong Li; Qi Yan; Rui Yang; Jiwu Huang",
    "corresponding_authors": "",
    "abstract": "Digital multimedia steganalysis has attracted wide attention over the past decade. Currently, there are many algorithms for detecting image steganography. However, little research has been devoted to audio steganalysis. Since the statistical properties of image and audio files are quite different, features that are effective in image steganalysis may not be effective for audio. In this article, we design an improved audio steganalytic feature set derived from both the time and Mel-frequency domains for detecting some typical steganography in the time domain, including LSB matching, Hide4PGP, and Steghide. The experiment results, evaluated on different audio sources, including various music and speech clips of different complexity, have shown that the proposed features significantly outperform the existing ones. Moreover, we use the proposed features to detect and further identify some typical audio operations that would probably be used in audio tampering. The extensive experiment results have shown that the proposed features also outperform the related forensic methods, especially when the length of the audio clip is small, such as audio clips with 800 samples. This is very important in real forensic situations.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2802024536",
    "type": "article"
  },
  {
    "title": "Analysis and Detection of Fake Views in Online Video Services",
    "doi": "https://doi.org/10.1145/2700290",
    "publication_date": "2015-02-24",
    "publication_year": 2015,
    "authors": "Liang Chen; Yipeng Zhou; Dah Ming Chiu",
    "corresponding_authors": "",
    "abstract": "Online video-on-demand(VoD) services invariably maintain a view count for each video they serve, and it has become an important currency for various stakeholders, from viewers, to content owners, advertizers, and the online service providers themselves. There is often significant financial incentive to use a robot (or a botnet) to artificially create fake views. How can we detect fake views? Can we detect them (and stop them) efficiently? What is the extent of fake views with current VoD service providers? These are the questions we study in this article. We develop some algorithms and show that they are quite effective for this problem.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W2094871756",
    "type": "article"
  },
  {
    "title": "Video Retrieval with Similarity-Preserving Deep Temporal Hashing",
    "doi": "https://doi.org/10.1145/3356316",
    "publication_date": "2019-11-30",
    "publication_year": 2019,
    "authors": "Ling Shen; Richang Hong; Haoran Zhang; Xinmei Tian; Meng Wang",
    "corresponding_authors": "",
    "abstract": "Despite the fact that remarkable progress has been made in recent years, Content-based Video Retrieval (CBVR) is still an appealing research topic due to increasing search demands in the Internet era of big data. This article aims to explore an efficient CBVR system by discriminately hashing videos into short binary codes. Existing video hashing methods usually encounter two weaknesses originating from the following sources: (1) Most works adopt the separated stages method or the frame-pooling based end-to-end architecture. However, the spatial-temporal properties of videos cannot be fully explored or kept well in the follow-up hashing step. (2) Discriminative learning based on pairwise or triplet constraints often suffers from slow convergence and poor local optimization, mainly because of the limited samples for each update. To alleviate these problems, we propose an end-to-end video retrieval framework called the Similarity-Preserving Deep Temporal Hashing (SPDTH) network. Specifically, we equip the model with the ability to capture spatial-temporal properties of videos and to generate binary codes by stacked Gated Recurrent Units (GRUs). It unifies video temporal modeling and learning to hash into one step to allow for maximum retention of information. We also introduce a deep metric learning objective called ℓ 2 All _ loss for network training by preserving intra-class similarity and inter-class separability, and a quantization loss between the real-valued outputs and the binary codes is minimized. Extensive experiments on several challenging datasets demonstrate that SPDTH can consistently outperform state-of-the-art methods.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W2996754048",
    "type": "article"
  },
  {
    "title": "A Unified Video Recommendation by Cross-Network User Modeling",
    "doi": "https://doi.org/10.1145/2957755",
    "publication_date": "2016-08-03",
    "publication_year": 2016,
    "authors": "Ming Yan; Jitao Sang; Changsheng Xu; M. Shamim Hossain",
    "corresponding_authors": "",
    "abstract": "Online video sharing sites are increasingly encouraging their users to connect to the social network venues such as Facebook and Twitter, with goals to boost user interaction and better disseminate the high-quality video content. This in turn provides huge possibilities to conduct cross-network collaboration for personalized video recommendation. However, very few efforts have been devoted to leveraging users’ social media profiles in the auxiliary network to capture and personalize their video preferences, so as to recommend videos of interest. In this article, we propose a unified YouTube video recommendation solution by transferring and integrating users’ rich social and content information in Twitter network. While general recommender systems often suffer from typical problems like cold-start and data sparsity, our proposed recommendation solution is able to effectively learn from users’ abundant auxiliary information on Twitter for enhanced user modeling and well address the typical problems in a unified framework. In this framework, two stages are mainly involved: (1) auxiliary-network data transfer, where user preferences are transferred from an auxiliary network by learning cross-network knowledge associations; and (2) cross-network data integration, where transferred user preferences are integrated with the observed behaviors on a target network in an adaptive fashion. Experimental results show that the proposed cross-network collaborative solution achieves superior performance not only in terms of accuracy, but also in improving the diversity and novelty of the recommended videos.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2484372584",
    "type": "article"
  },
  {
    "title": "Emotion Recognition Using Multiple Kernel Learning toward E-learning Applications",
    "doi": "https://doi.org/10.1145/3131287",
    "publication_date": "2018-01-04",
    "publication_year": 2018,
    "authors": "Oryina Kingsley Akputu; Kah Phooi Seng; Yunli Lee; Li-Minn Ang",
    "corresponding_authors": "",
    "abstract": "Adaptive Educational Hypermedia (AEH) e-learning models aim to personalize educational content and learning resources based on the needs of an individual learner. The Adaptive Hypermedia Architecture (AHA) is a specific implementation of the AEH model that exploits the cognitive characteristics of learner feedback to adapt resources accordingly. However, beside cognitive feedback, the learning realm generally includes both the affective and emotional feedback of the learner, which is often neglected in the design of e-learning models. This article aims to explore the potential of utilizing affect or emotion recognition research in AEH models. The framework is referred to as Multiple Kernel Learning Decision Tree Weighted Kernel Alignment (MKLDT-WFA). The MKLDT-WFA has two merits over classical MKL. First, the WFA component only preserves the relevant kernel weights to reduce redundancy and improve the discrimination for emotion classes. Second, training via the decision tree reduces the misclassification issues associated with the SimpleMKL. The proposed work has been evaluated on different emotion datasets and the results confirm the good performances. Finally, the conceptual Emotion-based E-learning Model (EEM) with the proposed emotion recognition framework is proposed for future work.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2782211501",
    "type": "article"
  },
  {
    "title": "From Selective Deep Convolutional Features to Compact Binary Representations for Image Retrieval",
    "doi": "https://doi.org/10.1145/3314051",
    "publication_date": "2019-05-31",
    "publication_year": 2019,
    "authors": "Thanh-Toan Do; Tuan Hoang; Dang-Khoa Le Tan; Huu Le; Tam Nguyen; Ngai‐Man Cheung",
    "corresponding_authors": "",
    "abstract": "In the large-scale image retrieval task, the two most important requirements are the discriminability of image representations and the efficiency in computation and storage of representations. Regarding the former requirement, Convolutional Neural Network is proven to be a very powerful tool to extract highly discriminative local descriptors for effective image search. Additionally, to further improve the discriminative power of the descriptors, recent works adopt fine-tuned strategies. In this article, taking a different approach, we propose a novel, computationally efficient, and competitive framework. Specifically, we first propose various strategies to compute masks, namely, SIFT-masks , SUM-mask , and MAX-mask , to select a representative subset of local convolutional features and eliminate redundant features. Our in-depth analyses demonstrate that proposed masking schemes are effective to address the burstiness drawback and improve retrieval accuracy. Second, we propose to employ recent embedding and aggregating methods that can significantly boost the feature discriminability. Regarding the computation and storage efficiency, we include a hashing module to produce very compact binary image representations. Extensive experiments on six image retrieval benchmarks demonstrate that our proposed framework achieves the state-of-the-art retrieval performances.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2962941391",
    "type": "article"
  },
  {
    "title": "Image Encryption Based on Compressive Sensing and Scrambled Index for Secure Multimedia Transmission",
    "doi": "https://doi.org/10.1145/2903717",
    "publication_date": "2016-09-15",
    "publication_year": 2016,
    "authors": "Yinghua Li; Bin Song; Rong Cao; Yue Zhang; Hao Qin",
    "corresponding_authors": "",
    "abstract": "With the rapid growth of multimedia message exchange and digital communication, multimedia big data has become a research hotspot in various fields. The storage and transmission of multimedia big data have high requirements for security. Images, covering the highest proportion of multimedia data, should be processed and transmitted with high security. Compressive sensing (CS) has a beneficial property for the encryption that the image can be recovered with fewer samples than conventional approaches use. In recent years, CS has been studied not only to reduce the resource requirements for signal acquisition but also to ensure the security of data. It is still an open challenge to improve security and enhance the quality of the decrypted image simultaneously using the key with small size. In this article, a CS-based encryption method is presented that associates the quantization with random measurement permutation. An enormous number of experiments have been conducted on both standard test images and face images chosen from the big database LFW. Experimental results show that our proposal has dramatic improvements on ensuring the security, enhancing the quality of the decrypted image, and raising the efficiency. Additionally, this proposal remarkably reduces storage and transmission resources. Accordingly, this encryption scheme can be applied to ensure the security of multimedia transmission.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2519913657",
    "type": "article"
  },
  {
    "title": "PPHOCFS",
    "doi": "https://doi.org/10.1145/2886779",
    "publication_date": "2016-10-12",
    "publication_year": 2016,
    "authors": "Qingchen Zhang; Hua Zhong; Laurence T. Yang; Zhikui Chen; Fanyu Bu",
    "corresponding_authors": "",
    "abstract": "Clustering is a commonly used technique for multimedia data analysis and management. In this article, we propose a high-order clustering algorithm by fast search and find of density peaks (HOCFS) by extending the traditional clustering scheme by fast search and find of density peaks (CFS) algorithm from the vector space to the tensor space for multimedia data clustering. Furthermore, we propose a privacy preserving HOCFS algorithm (PPHOCFS) which improves the efficiency of the HOCFS algorithm by using the cloud computing to perform most of the clustering operations. To protect the private data in the multimedia data sets during the clustering process on the cloud, the raw data is encrypted by the Brakerski-Gentry-Vaikun-tanathan (BGV) strategy before being uploaded to the cloud for performing the HOCFS clustering algorithm efficiently. In the proposed method, the client is required to only execute the encryption/decryption operations and the cloud servers are employed to perform all the computing operations. Finally, the performance of our scheme is evaluated on two representative multimedia data sets, namely NUS-WIDE and SNAE2, in terms of clustering accuracy, execution time, and speedup in the experiments. The results demonstrate that the proposed PPHOCFS scheme can save at least 40% running time compared with HOCFS, without disclosing the private data on the cloud, making our scheme securely suitable for multimedia big data clustering.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2530307160",
    "type": "article"
  },
  {
    "title": "Spatially Coherent Feature Learning for Pose-Invariant Facial Expression Recognition",
    "doi": "https://doi.org/10.1145/3176646",
    "publication_date": "2018-03-06",
    "publication_year": 2018,
    "authors": "Feifei Zhang; Qirong Mao; Xiang‐Jun Shen; Yongzhao Zhan; Ming Dong",
    "corresponding_authors": "",
    "abstract": "Feature learning has enjoyed much attention and achieved good performance in recent studies of image processing. Unlike the required training conditions often assumed there, far less labeled data is available for training emotion classification systems. In addition, current feature learning is typically performed on an entire face image without considering the dependency between features. These approaches ignore the fact that faces are structured and the neighboring features are dependent. Thus, the learned features lack the power to describe visually coherent facial images. Our method is therefore designed with the goal of simplifying the problem domain by removing expression-irrelevant factors from the input images, with a key region-based mechanism, which is an effort to reduce the amount of data required to effectively train the feature-learning methods. Meanwhile, we can construct geometric constraints between the key regions and its detected positions. To this end, we introduce a Spatially Coherent featurelearning method for Pose-invariant Facial Expression Recognition (SC-PFER). In our model, we first perform face frontalization through a 3D pose-normalization technique, which could normalize poses while preserving the identity information through synthesizing frontal faces for facial images with arbitrary views. Subsequently, we select a sequence of key regions around 51 key points in the synthetic frontal face images for efficient unsupervised feature learning. Finally, we introduce a linkage structure over the learning-based features and the corresponding geometry information of each key region to encode the dependencies of the regions. Our method, on the whole, does not require training multiple models for each specific pose and avoids separating training and parameter tuning for each pose. The proposed framework has been evaluated on two benchmark databases, BU-3DFE and SFEW, for pose-invariant Facial Expression Recognition (FER). The experimental results demonstrate that our algorithm outperforms current state-of-the-art FER methods. Specifically, our model achieves an improvement of 1.72% and 1.11% FER accuracy, on average, on BU-3DFE and SFEW, respectively.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2790989868",
    "type": "article"
  },
  {
    "title": "Deep Semantic Mapping for Heterogeneous Multimedia Transfer Learning Using Co-Occurrence Data",
    "doi": "https://doi.org/10.1145/3241055",
    "publication_date": "2019-01-24",
    "publication_year": 2019,
    "authors": "Liang Zhao; Zhikui Chen; Laurence T. Yang; M. Jamal Deen; Z. Jane Wang",
    "corresponding_authors": "",
    "abstract": "Transfer learning, which focuses on finding a favorable representation for instances of different domains based on auxiliary data, can mitigate the divergence between domains through knowledge transfer. Recently, increasing efforts on transfer learning have employed d eep n eural n etworks (DNN) to learn more robust and higher level feature representations to better tackle cross-media disparities. However, only a few articles consider the correction and semantic matching between multi-layer heterogeneous domain networks. In this article, we propose a d eep semantic mapping model for h eterogeneous multimedia t ransfer l earning (DHTL) using co-occurrence data. More specifically, we integrate the DNN with c anonical c orrelation a nalysis (CCA) to derive a deep correlation subspace as the joint semantic representation for associating data across different domains. In the proposed DHTL, a multi-layer correlation matching network across domains is constructed, in which the CCA is combined to bridge each pair of domain-specific hidden layers. To train the network, a joint objective function is defined and the optimization processes are presented. When the deep semantic representation is achieved, the shared features of the source domain are transferred for task learning in the target domain. Extensive experiments for three multimedia recognition applications demonstrate that the proposed DHTL can effectively find deep semantic representations for heterogeneous domains, and it is superior to the several existing state-of-the-art methods for deep transfer learning.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2913283038",
    "type": "article"
  },
  {
    "title": "Hybrid Wolf-Bat Algorithm for Optimization of Connection Weights in Multi-layer Perceptron",
    "doi": "https://doi.org/10.1145/3350532",
    "publication_date": "2020-01-31",
    "publication_year": 2020,
    "authors": "Utkarsh Agrawal; Jatin Arora; Rahul Singh; Deepak Gupta; Ashish Khanna; Aditya Khamparia",
    "corresponding_authors": "",
    "abstract": "In a neural network, the weights act as parameters to determine the output(s) from a set of inputs. The weights are used to find the activation values of nodes of a layer from the values of the previous layer. Finding the ideal set of these weights for training a Multi-layer Perceptron neural network such that it minimizes the classification error is a widely known optimization problem. The presented article proposes a Hybrid Wolf-Bat algorithm, a novel optimization algorithm, as a solution to solve the discussed problem. The proposed algorithm is a hybrid of two already existing nature-inspired algorithms, Grey Wolf Optimization algorithm and Bat algorithm. The novel introduced approach is tested on ten different datasets of the medical field, obtained from the UCI machine learning repository. The performance of the proposed algorithm is compared with the recently developed nature-inspired algorithms: Grey Wolf Optimization algorithm, Cuckoo Search, Bat Algorithm, and Whale Optimization Algorithm, along with the standard Back-propagation training method available in the literature. The obtained results demonstrate that the proposed method outperforms other bio-inspired algorithms in terms of both speed of convergence and accuracy.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W3016998144",
    "type": "article"
  },
  {
    "title": "Shuffled ImageNet Banks for Video Event Detection and Search",
    "doi": "https://doi.org/10.1145/3377875",
    "publication_date": "2020-05-22",
    "publication_year": 2020,
    "authors": "Pascal Mettes; D.C. Koelma; Cees G. M. Snoek",
    "corresponding_authors": "",
    "abstract": "This article aims for the detection and search of events in videos, where video examples are either scarce or even absent during training. To enable such event detection and search, ImageNet concept banks have shown to be effective. Rather than employing the standard concept bank of 1,000 ImageNet classes, we leverage the full 21,841-class dataset. We identify two problems with using the full dataset: (i) there is an imbalance between the number of examples per concept, and (ii) not all concepts are equally relevant for events. In this article, we propose to balance large-scale image hierarchies for pre-training. We shuffle concepts based on bottom-up and top-down operations to overcome the problems of example imbalance and concept relevance. Using this strategy, we arrive at the shuffled ImageNet bank, a concept bank with an order of magnitude more concepts compared to standard ImageNet banks. Compared to standard ImageNet pre-training, our shuffles result in more discriminative representations to train event models from the limited video event examples. For event search, the broad range of concepts enable a closer match between textual queries of events and concept detections in videos. Experimentally, we show the benefit of the proposed bank for event detection and event search, with state-of-the-art performance for both tasks on the challenging TRECVID Multimedia Event Detection and Ad-Hoc Video Search benchmarks.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W3028831795",
    "type": "article"
  },
  {
    "title": "Deep Patch Representations with Shared Codebook for Scene Classification",
    "doi": "https://doi.org/10.1145/3231738",
    "publication_date": "2019-01-24",
    "publication_year": 2019,
    "authors": "Shuqiang Jiang; Gongwei Chen; Xinhang Song; Linhu Liu",
    "corresponding_authors": "",
    "abstract": "Scene classification is a challenging problem. Compared with object images, scene images are more abstract, as they are composed of objects. Object and scene images have different characteristics with different scales and composition structures. How to effectively integrate the local mid-level semantic representations including both object and scene concepts needs to be investigated, which is an important aspect for scene classification. In this article, the idea of a sharing codebook is introduced by organically integrating deep learning, concept feature, and local feature encoding techniques. More specifically, the shared local feature codebook is generated from the combined ImageNet1K and Places365 concepts (Mixed1365) using convolutional neural networks. As the Mixed1365 features cover all the semantic information including both object and scene concepts, we can extract a shared codebook from the Mixed1365 features, which only contain a subset of the whole 1,365 concepts with the same codebook size. The shared codebook can not only provide complementary representations without additional codebook training but also be adaptively extracted toward different scene classification tasks. A method of fusing the encoded features with both the original codebook and the shared codebook is proposed for scene classification. In this way, more comprehensive and representative image features can be generated for classification. Extensive experimentations conducted on two public datasets validate the effectiveness of the proposed method. Besides, some useful observations are also revealed to show the advantage of shared codebook.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2913467217",
    "type": "article"
  },
  {
    "title": "Learning Shared Semantic Space with Correlation Alignment for Cross-Modal Event Retrieval",
    "doi": "https://doi.org/10.1145/3374754",
    "publication_date": "2020-02-17",
    "publication_year": 2020,
    "authors": "Zhenguo Yang; Zehang Lin; Peipei Kang; Jianming Lv; Qing Li; Wenyin Liu",
    "corresponding_authors": "",
    "abstract": "In this article, we propose to learn shared semantic space with correlation alignment (S3CA) for multimodal data representations, which aligns nonlinear correlations of multimodal data distributions in deep neural networks designed for heterogeneous data. In the context of cross-modal (event) retrieval, we design a neural network with convolutional layers and fully connected layers to extract features for images, including images on Flickr-like social media. Simultaneously, we exploit a fully connected neural network to extract semantic features for text documents, including news articles from news media. In particular, nonlinear correlations of layer activations in the two neural networks are aligned with correlation alignment during the joint training of the networks. Furthermore, we project the multimodal data into a shared semantic space for cross-modal (event) retrieval, where the distances between heterogeneous data samples can be measured directly. In addition, we contribute a Wiki-Flickr Event dataset, where the multimodal data samples are not describing each other in pairs like the existing paired datasets, but all of them are describing semantic events. Extensive experiments conducted on both paired and unpaired datasets manifest the effectiveness of S3CA, outperforming the state-of-the-art methods.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W3010262726",
    "type": "article"
  },
  {
    "title": "Multichannel Attention Refinement for Video Question Answering",
    "doi": "https://doi.org/10.1145/3366710",
    "publication_date": "2020-01-31",
    "publication_year": 2020,
    "authors": "Yueting Zhuang; Dejing Xu; Xin Yan; Wenzhuo Cheng; Zhou Zhao; Shiliang Pu; Jun Xiao",
    "corresponding_authors": "",
    "abstract": "Video Question Answering (VideoQA) is the extension of image question answering (ImageQA) in the video domain. Methods are required to give the correct answer after analyzing the provided video and question in this task. Comparing to ImageQA, the most distinctive part is the media type. Both tasks require the understanding of visual media, but VideoQA is much more challenging, mainly because of the complexity and diversity of videos. Particularly, working with the video needs to model its inherent temporal structure and analyze the diverse information it contains. In this article, we propose to tackle the task from a multichannel perspective. Appearance, motion, and audio features are extracted from the video, and question-guided attentions are refined to generate the expressive clues that support the correct answer. We also incorporate the relevant text information acquired from Wikipedia as an attempt to extend the capability of the method. Experiments on TGIF-QA and ActivityNet-QA datasets show the advantages of our method compared to existing methods. We also demonstrate the effectiveness and interpretability of our method by analyzing the refined attention weights during the question-answering procedure.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W3016658915",
    "type": "article"
  },
  {
    "title": "Controlling Neural Learning Network with Multiple Scales for Image Splicing Forgery Detection",
    "doi": "https://doi.org/10.1145/3408299",
    "publication_date": "2020-11-30",
    "publication_year": 2020,
    "authors": "Wei Yang; Zhuzhu Wang; Bin Xiao; Ximeng Liu; Zheng Yan; Jianfeng Ma",
    "corresponding_authors": "",
    "abstract": "The guarantee of social stability comes from many aspects of life, and image information security as one of them is being subjected to various malicious attacks. As a means of information attack, image splicing forgery refers to copying some areas of an image to another image to hide the traces of the original information and leads to grave consequences. Image splicing forgery is extremely complex since the attributes of the two images subjected to the pasting and copying operations are greatly different. In order to solve the issue mentioned above, we propose a method by applying a neural learning network controlled by multiple scales (MCNL-Net) based on U-Net to identify whether an image has been tampered and to locate the tampered regions. Firstly, the learning capacity of MCNL-Net is enhanced by the combination of a residual propagation module and a residual feedback module. An ingenious strategy is designed to control the size of local receptive field in each building block of MCNL-Net. The strategy makes MCNL-Net able to achieve properties and superiorities of multi-scale structure and learn specified features. For further improving the detection performance of MCNL-Net, a block attention mechanism is proposed to control the advanced degree of the input information in each building block. In addition, a MaxBlurPool method is applied into image splicing forgery detection for the first time, preserving the shift-equivariance of a convolutional neural network. Through experiments, we demonstrate that MCNL-Net can achieve more promising results and offer stronger robustness than the state-of-the-art splicing forgery detection methods.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W3111381080",
    "type": "article"
  },
  {
    "title": "Kernel Attention Network for Single Image Super-Resolution",
    "doi": "https://doi.org/10.1145/3398685",
    "publication_date": "2020-07-05",
    "publication_year": 2020,
    "authors": "Dongyang Zhang; Jie Shao; Heng Tao Shen",
    "corresponding_authors": "",
    "abstract": "Recently, attention mechanisms have shown a developing tendency toward convolutional neural network (CNN), and some representative attention mechanisms, i.e., channel attention (CA) and spatial attention (SA) have been fully applied to single image super-resolution (SISR) tasks. However, the existing architectures directly apply these attention mechanisms to SISR without much consideration of the nature characteristic, resulting in less strong representational power. In this article, we propose a novel kernel attention module (KAM) for SISR, which enables the network to adjust its receptive field size corresponding to various scales of input by dynamically selecting the appropriate kernel. Based on this, we stack multiple kernel attention modules with group and residual connection to constitute a novel architecture for SISR, which enables our network to learn more distinguishing representations through filtering the information under different receptive fields. Thus, our network is more sensitive to multi-scale features, which enables our single network to deal with multi-scale SR task by predefining the upscaling modules. Besides, other attention mechanisms in super-resolution are also investigated and illustrated in detail in this article. Thanks to the kernel attention mechanism, the extensive benchmark evaluation shows that our method outperforms the other state-of-the-art methods.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W3038753503",
    "type": "article"
  },
  {
    "title": "Entropy Slicing Extraction and Transfer Learning Classification for Early Diagnosis of Alzheimer Diseases with sMRI",
    "doi": "https://doi.org/10.1145/3383749",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "S. Sambath Kumar; M. Nandhini",
    "corresponding_authors": "",
    "abstract": "Alzheimer’s Disease (AD) is an irreversible neurogenerative disorder that undergoes progressive decline in memory and cognitive function and is characterized by structural brain Magnetic Resonance Images (sMRI). In recent years, sMRI data has played a vital role in the evaluation of brain anatomical changes, leading to early detection of AD through deep networks. The existing AD problems such as preprocessing complexity and unreliability are major concerns at present. To overcome these, a model ( FE ES C TL ) has been proposed with an entropy slicing for feature extraction and Transfer Learning for classification. In the present study, the entropy image slicing method is attempted for selecting the most informative MRI slices during training stages. The ADNI dataset is trained on Transfer Learning adopted by VGG-16 network for classifying the AD with normal individuals. The experimental results reveal that the proposed model has achieved an accuracy level of 93.05%, 86.39%, 92.00% for binary classifications (AD/MCI, MCI/CN, AD/CN) and 93.12% for ternary classification (AD/MCI/CN), respectively, and henceforth the efficiency in diagnosing AD is proved through comparative analysis.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W3041975743",
    "type": "article"
  },
  {
    "title": "QoE-Fair DASH Video Streaming Using Server-side Reinforcement Learning",
    "doi": "https://doi.org/10.1145/3397227",
    "publication_date": "2020-04-30",
    "publication_year": 2020,
    "authors": "Sa’di Altamimi; Shervin Shirmohammadi",
    "corresponding_authors": "",
    "abstract": "To design an optimal adaptive video streaming method, video service providers need to consider both the efficiency and the fairness of the Quality of Experience (QoE) of their users. In Reference [8], we proposed a server-side QoE-fair rate adaptation method that considers both efficiency and fairness of the QoE. The server uses Reinforcement Learning (RL) to select a bitrate for each client sharing the same bottleneck link to the server in a way that achieves fairness among concurrent DASH clients and imposes that bitrate by dynamically modifying the client’s Media Presentation Description (MPD) file. In this article, we extend that work to minimize the number of actions the server needs to take to keep the system in its equilibrium state. By incorporating a Recurrent Neural Network, specifically an LSTM model, we modify the server’s training algorithm to achieve improvements in both the quality and the quantity of actions the server takes to guide the client. Performance evaluation of the modified algorithm for clients running both homogeneous and heterogeneous adaptation algorithms showed that the number of server actions dropped by 14% and 22%, respectively, while QoE-fairness improved by at least 6% and 10%, respectively.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W3080338915",
    "type": "article"
  },
  {
    "title": "Market2Dish: Health-aware Food Recommendation",
    "doi": "https://doi.org/10.1145/3418211",
    "publication_date": "2021-02-28",
    "publication_year": 2021,
    "authors": "Wenjie Wang; Ling‐Yu Duan; Hao Jiang; Peiguang Jing; Xuemeng Song; Liqiang Nie",
    "corresponding_authors": "",
    "abstract": "With the rising incidence of some diseases, such as obesity and diabetes, the healthy diet is arousing increasing attention. However, most existing food-related research efforts focus on recipe retrieval, user-preference-based food recommendation, cooking assistance, or the nutrition and calorie estimation of dishes, ignoring the personalized health-aware food recommendation. Therefore, in this work, we present a personalized health-aware food recommendation scheme, namely, Market2Dish, mapping the ingredients displayed in the market to the healthy dishes eaten at home. The proposed scheme comprises three components, namely, recipe retrieval, user health profiling, and health-aware food recommendation. In particular, recipe retrieval aims to acquire the ingredients available to the users and then retrieve recipe candidates from a large-scale recipe dataset. User health profiling is to characterize the health conditions of users by capturing the textual health-related information crawled from social networks. Specifically, to solve the issue that the health-related information is extremely sparse, we incorporate a word-class interaction mechanism into the proposed deep model to learn the fine-grained correlations between the textual tweets and pre-defined health concepts. For the health-aware food recommendation, we present a novel category-aware hierarchical memory network–based recommender to learn the health-aware user-recipe interactions for better food recommendation. Moreover, extensive experiments demonstrate the effectiveness of the health-aware food recommendation scheme.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W3111340645",
    "type": "article"
  },
  {
    "title": "A Semi-supervised Learning Approach Based on Adaptive Weighted Fusion for Automatic Image Annotation",
    "doi": "https://doi.org/10.1145/3426974",
    "publication_date": "2021-02-28",
    "publication_year": 2021,
    "authors": "Zhixin Li; Lan Lin; Canlong Zhang; Huifang Ma; Weizhong Zhao; Zhiping Shi",
    "corresponding_authors": "",
    "abstract": "To learn a well-performed image annotation model, a large number of labeled samples are usually required. Although the unlabeled samples are readily available and abundant, it is a difficult task for humans to annotate large numbers of images manually. In this article, we propose a novel semi-supervised approach based on adaptive weighted fusion for automatic image annotation that can simultaneously utilize the labeled data and unlabeled data to improve the annotation performance. At first, two different classifiers, constructed based on support vector machine and covolutional neural network, respectively, are trained by different features extracted from the labeled data. Therefore, these two classifiers are independently represented as different feature views. Then, the corresponding features of unlabeled images are extracted and input into these two classifiers, and the semantic annotation of images can be obtained respectively. At the same time, the confidence of corresponding image annotation can be measured by an adaptive weighted fusion strategy. After that, the images and its semantic annotations with high confidence are submitted to the classifiers for retraining until a certain stop condition is reached. As a result, we can obtain a strong classifier that can make full use of unlabeled data. Finally, we conduct experiments on four datasets, namely, Corel 5K, IAPR TC12, ESP Game, and NUS-WIDE. In addition, we measure the performance of our approach with standard criteria, including precision, recall, F-measure, N+, and mAP. The experimental results show that our approach has superior performance and outperforms many state-of-the-art approaches.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W3154314403",
    "type": "article"
  },
  {
    "title": "Leveraging Deep Statistics for Underwater Image Enhancement",
    "doi": "https://doi.org/10.1145/3489520",
    "publication_date": "2021-10-26",
    "publication_year": 2021,
    "authors": "Yang Wang; Yang Cao; Jing Zhang; Feng Wu; Zheng-Jun Zha",
    "corresponding_authors": "",
    "abstract": "Underwater imaging often suffers from color cast and contrast degradation due to range-dependent medium absorption and light scattering. Introducing image statistics as prior has been proved to be an effective solution for underwater image enhancement. However, relative to the modal divergence of light propagation and underwater scenery, the existing methods are limited in representing the inherent statistics of underwater images resulting in color artifacts and haze residuals. To address this problem, this article proposes a convolutional neural network (CNN)-based framework to learn hierarchical statistical features related to color cast and contrast degradation and to leverage them for underwater image enhancement. Specifically, a pixel disruption strategy is first proposed to suppress intrinsic colors’ influence and facilitate modeling a unified statistical representation of underwater image. Then, considering the local variation of depth of field, two parallel sub-networks: Color Correction Network (CC-Net) and Contrast Enhancement Network (CE-Net) are presented. The CC-Net and CE-Net can generate pixel-wise color cast and transmission map and achieve spatial-varied color correction and contrast enhancement. Moreover, to address the issue of insufficient training data, an imaging model-based synthesis method that incorporates pixel disruption strategy is presented to generate underwater patches with global degradation consistency. Quantitative and subjective evaluations demonstrate that our proposed method achieves state-of-the-art performance.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W3209760397",
    "type": "article"
  },
  {
    "title": "A Multi-instance Multi-label Dual Learning Approach for Video Captioning",
    "doi": "https://doi.org/10.1145/3446792",
    "publication_date": "2021-06-14",
    "publication_year": 2021,
    "authors": "Wanting Ji; Ruili Wang",
    "corresponding_authors": "",
    "abstract": "Video captioning is a challenging task in the field of multimedia processing, which aims to generate informative natural language descriptions/captions to describe video contents. Previous video captioning approaches mainly focused on capturing visual information in videos using an encoder-decoder structure to generate video captions. Recently, a new encoder-decoder-reconstructor structure was proposed for video captioning, which captured the information in both videos and captions. Based on this, this article proposes a novel multi-instance multi-label dual learning approach (MIMLDL) to generate video captions based on the encoder-decoder-reconstructor structure. Specifically, MIMLDL contains two modules: caption generation and video reconstruction modules. The caption generation module utilizes a lexical fully convolutional neural network (Lexical FCN) with a weakly supervised multi-instance multi-label learning mechanism to learn a translatable mapping between video regions and lexical labels to generate video captions. Then the video reconstruction module synthesizes visual sequences to reproduce raw videos using the outputs of the caption generation module. A dual learning mechanism fine-tunes the two modules according to the gap between the raw and the reproduced videos. Thus, our approach can minimize the semantic gap between raw videos and the generated captions by minimizing the differences between the reproduced and the raw visual sequences. Experimental results on a benchmark dataset demonstrate that MIMLDL can improve the accuracy of video captioning.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W3167051144",
    "type": "article"
  },
  {
    "title": "Fully Unsupervised Person Re-Identification via Selective Contrastive Learning",
    "doi": "https://doi.org/10.1145/3485061",
    "publication_date": "2022-02-16",
    "publication_year": 2022,
    "authors": "Bo Pang; Deming Zhai; Junjun Jiang; Xianming Liu",
    "corresponding_authors": "",
    "abstract": "Person re-identification (ReID) aims at searching the same identity person among images captured by various cameras. Existing fully supervised person ReID methods usually suffer from poor generalization capability caused by domain gaps. Unsupervised person ReID has attracted a lot of attention recently, because it works without intensive manual annotation and thus shows great potential in adapting to new conditions. Representation learning plays a critical role in unsupervised person ReID. In this work, we propose a novel selective contrastive learning framework for fully unsupervised feature learning. Specifically, different from traditional contrastive learning strategies, we propose to use multiple positives and adaptively selected negatives for defining the contrastive loss, enabling to learn a feature embedding model with stronger identity discriminative representation. Moreover, we propose to jointly leverage global and local features to construct three dynamic memory banks, among which the global and local ones are used for pairwise similarity computation and the mixture memory bank are used for contrastive loss definition. Experimental results demonstrate the superiority of our method in unsupervised person ReID compared with the state of the art. Our code is available at https://github.com/pangbo1997/Unsup_ReID.git .",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W3134591196",
    "type": "article"
  },
  {
    "title": "An End-to-end Heterogeneous Restraint Network for RGB-D Cross-modal Person Re-identification",
    "doi": "https://doi.org/10.1145/3506708",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Jingjing Wu; Jian‐Guo Jiang; Meibin Qi; Cuiqun Chen; Jingjing Zhang",
    "corresponding_authors": "",
    "abstract": "The RGB-D cross-modal person re-identification (re-id) task aims to identify the person of interest across the RGB and depth image modes. The tremendous discrepancy between these two modalities makes this task difficult to tackle. Few researchers pay attention to this task, and the deep networks of existing methods still cannot be trained in an end-to-end manner. Therefore, this article proposes an end-to-end module for RGB-D cross-modal person re-id. This network introduces a cross-modal relational branch to narrow the gaps between two heterogeneous images. It models the abundant correlations between any cross-modal sample pairs, which are constrained by heterogeneous interactive learning. The proposed network also exploits a dual-modal local branch, which aims to capture the common spatial contexts in two modalities. This branch adopts shared attentive pooling and mutual contextual graph networks to extract the spatial attention within each local region and the spatial relations between distinct local parts, respectively. Experimental results on two public benchmark datasets, that is, the BIWI and RobotPKU datasets, demonstrate that our method is superior to the state-of-the-art. In addition, we perform thorough experiments to prove the effectiveness of each component in the proposed method.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W4214865104",
    "type": "article"
  },
  {
    "title": "Learning Video-Text Aligned Representations for Video Captioning",
    "doi": "https://doi.org/10.1145/3546828",
    "publication_date": "2022-07-07",
    "publication_year": 2022,
    "authors": "Yaya Shi; Haiyang Xu; Chunfeng Yuan; Bing Li; Weiming Hu; Zheng-Jun Zha",
    "corresponding_authors": "",
    "abstract": "Video captioning requires that the model has the abilities of video understanding, video-text alignment, and text generation. Due to the semantic gap between vision and language, conducting video-text alignment is a crucial step to reduce the semantic gap, which maps the representations from the visual to the language domain. However, the existing methods often overlook this step, so the decoder has to directly take the visual representations as input, which increases the decoder’s workload and limits its ability to generate semantically correct captions. In this paper, we propose a video-text alignment module with a retrieval unit and an alignment unit to learn video-text aligned representations for video captioning. Specifically, we firstly propose a retrieval unit to retrieve sentences as additional input which is used as the semantic anchor between visual scene and language description. Then, we employ an alignment unit with the input of the video and retrieved sentences to conduct the video-text alignment. The representations of two modal inputs are aligned in a shared semantic space. The obtained video-text aligned representations are used to generate semantically correct captions. Moreover, retrieved sentences provide rich semantic concepts which are helpful for generating distinctive captions. Experiments on two public benchmarks, i.e., VATEX and MSR-VTT, demonstrate that our method outperforms state-of-the-art performances by a large margin. The qualitative analysis shows that our method generates correct and distinctive captions.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W4284692156",
    "type": "article"
  },
  {
    "title": "Binary Representation via Jointly Personalized Sparse Hashing",
    "doi": "https://doi.org/10.1145/3558769",
    "publication_date": "2022-09-06",
    "publication_year": 2022,
    "authors": "Xiaoqin Wang; Chen Chen; Rushi Lan; Licheng Liu; Zhenbing Liu; Huiyu Zhou; Xiaonan Luo",
    "corresponding_authors": "",
    "abstract": "Unsupervised hashing has attracted much attention for binary representation learning due to the requirement of economical storage and efficiency of binary codes. It aims to encode high-dimensional features in the Hamming space with similarity preservation between instances. However, most existing methods learn hash functions in manifold-based approaches. Those methods capture the local geometric structures (i.e., pairwise relationships) of data, and lack satisfactory performance in dealing with real-world scenarios that produce similar features (e.g., color and shape) with different semantic information. To address this challenge, in this work, we propose an effective unsupervised method, namely, Jointly Personalized Sparse Hashing (JPSH), for binary representation learning. To be specific, first, we propose a novel personalized hashing module, i.e., Personalized Sparse Hashing (PSH). Different personalized subspaces are constructed to reflect category-specific attributes for different clusters, adaptively mapping instances within the same cluster to the same Hamming space. In addition, we deploy sparse constraints for different personalized subspaces to select important features. We also collect the strengths of the other clusters to build the PSH module with avoiding over-fitting. Then, to simultaneously preserve semantic and pairwise similarities in our proposed JPSH, we incorporate the proposed PSH and manifold-based hash learning into the seamless formulation. As such, JPSH not only distinguishes the instances from different clusters but also preserves local neighborhood structures within the cluster. Finally, an alternating optimization algorithm is adopted to iteratively capture analytical solutions of the JPSH model. We apply the proposed representation learning algorithm JPSH to the similarity search task. Extensive experiments on four benchmark datasets verify that the proposed JPSH outperforms several state-of-the-art unsupervised hashing algorithms.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W4294733375",
    "type": "article"
  },
  {
    "title": "Harmonious Multi-branch Network for Person Re-identification with Harder Triplet Loss",
    "doi": "https://doi.org/10.1145/3501405",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Zengming Tang; Jun Huang",
    "corresponding_authors": "",
    "abstract": "Recently, advances in person re-identification (Re-ID) has benefitted from use of the popular multi-branch network. However, performing feature learning in a single branch with uniform partitioning is likely to separate meaningful local regions, and correlation among different branches is not well established. In this article, we propose a novel harmonious multi-branch network (HMBN) to relieve these intra-branch and inter-branch problems harmoniously. HMBN is a multi-branch network with various stripes on different branches to learn coarse-to-fine pedestrian information. We first replace the uniform partition with a horizontal overlapped partition to cover meaningful local regions between adjacent stripes in a single branch. We then incorporate a novel attention module to make all branches interact by modeling spatial contextual dependencies across branches. Finally, in order to train the HMBN more effectively, a harder triplet loss is introduced to optimize triplets in a harder manner. Extensive experiments are conducted on three benchmark datasets — DukeMTMC-reID, CUHK03, and Market-1501 — demonstrating the superiority of our proposed HMBN over state-of-the-art methods.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W4214839753",
    "type": "article"
  },
  {
    "title": "A Format-compatible Searchable Encryption Scheme for JPEG Images Using Bag-of-words",
    "doi": "https://doi.org/10.1145/3492705",
    "publication_date": "2022-03-15",
    "publication_year": 2022,
    "authors": "Zhihua Xia; Qiuju Ji; Qi Gu; Chengsheng Yuan; Fengjun Xiao",
    "corresponding_authors": "",
    "abstract": "The development of cloud computing attracts enterprises and individuals to outsource their data, such as images, to the cloud server. However, direct outsourcing causes the extensive concern of privacy leakage, as images often contain rich sensitive information. A straightforward way to protect privacy is to encrypt the images using the standard cryptographic tools before outsourcing. However, in such a way the possible usage of the outsourced images would be strongly limited together with the services provided to users, like the Content-Based Image Retrieval (CBIR). In this article, we propose a secure outsourced CBIR scheme, in which an encryption scheme is designed for the widely used JPEG-format images, and the secure features can be directly extracted from such encrypted images. Specifically, the JPEG images are encrypted by the block permutation, intra-block permutation, polyalphabetic cipher, and stream cipher. Then secure local histograms are extracted from the encrypted DCT blocks and the Bag-Of-Words (BOW) model is further used to organize the encrypted local features to represent the image. The proposed image encryption gets all of the image data protected and the experimental results show that the proposed scheme achieves improved accuracy with a small file size expansion.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W4220660149",
    "type": "article"
  },
  {
    "title": "Meta-MMFNet: Meta-learning-based Multi-model Fusion Network for Micro-expression Recognition",
    "doi": "https://doi.org/10.1145/3539576",
    "publication_date": "2022-06-02",
    "publication_year": 2022,
    "authors": "Wenjuan Gong; Yue Zhang; Wei Wang; Peng Cheng; Jordi Gonzàlez",
    "corresponding_authors": "",
    "abstract": "Despite its wide applications in criminal investigations and clinical communications with patients suffering from autism, automatic micro-expression recognition remains a challenging problem because of the lack of training data and imbalanced classes problems. In this study, we proposed a meta-learning-based multi-model fusion network (Meta-MMFNet) to solve the existing problems. The proposed method is based on the metric-based meta-learning pipeline, which is specifically designed for few-shot learning and is suitable for model-level fusion. The frame difference and optical flow features were fused, deep features were extracted from the fused feature, and finally in the meta-learning-based framework, weighted sum model fusion method was applied for micro-expression classification. Meta-MMFNet achieved better results than state-of-the-art methods on four datasets. The code is available at https://github.com/wenjgong/meta-fusion-based-method .",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W4281913048",
    "type": "article"
  },
  {
    "title": "Aligning Image Semantics and Label Concepts for Image Multi-Label Classification",
    "doi": "https://doi.org/10.1145/3550278",
    "publication_date": "2022-07-21",
    "publication_year": 2022,
    "authors": "Wei Zhou; Zhiwu Xia; Peng Dou; Tao Su; Haifeng Hu",
    "corresponding_authors": "",
    "abstract": "Image multi-label classification task is mainly to correctly predict multiple object categories in the images. To capture the correlation between labels, graph convolution network based methods have to manually count the label co-occurrence probability from training data to construct a pre-defined graph as the input of graph network, which is inflexible and may degrade model generalizability. Moreover, most of the current methods cannot effectively align the learned salient object features with the label concepts, so that the predicted results of model may not be consistent with the image content. Therefore, how to learn the salient semantic features of images and capture the correlation between labels, and then effectively align them is one of the key to improve the performance of image multi-label classification task. To this end, we propose a novel image multi-label classification framework which aims to align I mage S emantics with L abel C oncepts ( ISLC ). Specifically, we propose a residual encoder to learn salient object features in the images, and exploit the self-attention layer in aligned decoder to automatically capture the correlation between labels. Then, we leverage the cross-attention layers in aligned decoder to align image semantic features with label concepts, so as to make the labels predicted by model more consistent with image content. Finally, the output features of the last layer of residual encoder and aligned decoder are fused to obtain the final output feature for classification. The proposed ISLC model achieves good performance on various prevalent multi-label image datasets such as MS-COCO 2014, PASCAL VOC 2007, VG-500, and NUS-WIDE with 87.2%, 96.9%, 39.4%, and 64.2%, respectively.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W4286499389",
    "type": "article"
  },
  {
    "title": "Multimodal Graph for Unaligned Multimodal Sequence Analysis via Graph Convolution and Graph Pooling",
    "doi": "https://doi.org/10.1145/3542927",
    "publication_date": "2022-06-09",
    "publication_year": 2022,
    "authors": "Sijie Mai; Songlong Xing; Jiaxuan He; Ying Zeng; Haifeng Hu",
    "corresponding_authors": "",
    "abstract": "Multimodal sequence analysis aims to draw inferences from visual, language, and acoustic sequences. A majority of existing works focus on the aligned fusion of three modalities to explore inter-modal interactions, which is impractical in real-world scenarios. To overcome this issue, we seek to focus on analyzing unaligned sequences, which is still relatively underexplored and also more challenging. We propose Multimodal Graph, whose novelty mainly lies in transforming the sequential learning problem into graph learning problem. The graph-based structure enables parallel computation in time dimension (as opposed to recurrent neural network) and can effectively learn longer intra- and inter-modal temporal dependency in unaligned sequences. First, we propose multiple ways to construct the adjacency matrix for sequence to perform sequence to graph transformation. To learn intra-modal dynamics, a graph convolution network is employed for each modality based on the defined adjacency matrix. To learn inter-modal dynamics, given that the unimodal sequences are unaligned, the commonly considered word-level fusion does not pertain. To this end, we innovatively devise graph pooling algorithms to automatically explore the associations between various time slices from different modalities and learn high-level graph representation hierarchically. Multimodal Graph outperforms state-of-the-art models on three datasets under the same experimental setting.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W4293081695",
    "type": "article"
  },
  {
    "title": "Dilated Convolution-based Feature Refinement Network for Crowd Localization",
    "doi": "https://doi.org/10.1145/3571134",
    "publication_date": "2022-12-20",
    "publication_year": 2022,
    "authors": "Xingyu Gao; Jinyang Xie; Zhenyu Chen; An-An Liu; Zhenan Sun; Lei Lyu",
    "corresponding_authors": "",
    "abstract": "As an emerging computer vision task, crowd localization has received increasing attention due to its ability to produce more accurate spatially predictions. However, continuous scale variations in complex crowd scenes lead to tiny individuals at the edges, so that existing methods cannot achieve precise crowd localization. Aiming at alleviating the above problems, we propose a novel Dilated Convolution-based Feature Refinement Network (DFRNet) to enhance the representation learning capability. Specifically, the DFRNet is built with three branches that can capture the information of each individual in crowd scenes more precisely. More specifically, we introduce a Feature Perception Module to model long-range contextual information at different scales by adopting multiple dilated convolutions, thus providing sufficient feature information to perceive tiny individuals at the edge of images. Afterwards, a Feature Refinement Module is deployed at multiple stages of the three branches to facilitate the mutual refinement of feature information at different scales, thus further improving the expression capability of multi-scale contextual information. By incorporating the above modules, DFRNet can locate individuals in complex scenes more precisely. Extensive experiments on multiple datasets demonstrate that the proposed method has more advanced performance compared to existing methods and can be more accurately adapted to complex crowd scenes.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W4312037474",
    "type": "article"
  },
  {
    "title": "DDIFN: A Dual-discriminator Multi-modal Medical Image Fusion Network",
    "doi": "https://doi.org/10.1145/3574136",
    "publication_date": "2023-01-17",
    "publication_year": 2023,
    "authors": "Hui Liu; Shanshan Li; Jicheng Zhu; Kai Deng; Meng Liu; Liqiang Nie",
    "corresponding_authors": "",
    "abstract": "Multi-modal medical image fusion is a long-standing important research topic that can obtain informative medical images and assist doctors diagnose and treat diseases more efficiently. However, most fusion methods extract and fuse features by subjectively defining constraints, which easily distorts the unique information of source images. In this work, we present a novel end-to-end unsupervised network to fuse multi-modal medical images. It is composed of a generator and two symmetrical discriminators. The former aims to generate a ”real-like” fused image based on a specifically designed content and structure loss, while the latter are devoted to distinguishing the differences between the fused image and the source ones. They are trained alternately until discriminators cannot distinguish the fused image from the source ones. In addition, the symmetrical discriminator scheme is conducive to maintaining the feature consistency among different modalities. More importantly, to enhance the retention degree of texture details, U-Net is adopted as the generator heuristically, where the up-sampling method is modified to bilinear interpolation for avoiding checkerboard artifacts. As for the optimization, we define the content loss function, which preserves the gradient information and pixel activity of source images. Both visual analysis and quantitative evaluation of experimental results show the superiority of our method as compared to the cutting-edge baselines.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4317036091",
    "type": "article"
  },
  {
    "title": "Learning Semantic Representation on Visual Attribute Graph for Person Re-identification and Beyond",
    "doi": "https://doi.org/10.1145/3487044",
    "publication_date": "2023-03-27",
    "publication_year": 2023,
    "authors": "Geyu Tang; Xingyu Gao; Zhenyu Chen",
    "corresponding_authors": "",
    "abstract": "Person re-identification (re-ID) aims to match pedestrian pairs captured from different cameras. Recently, various attribute-based models have been proposed to combine the pedestrian attribute as an auxiliary semantic information to learn a more discriminative pedestrian representation. However, these methods usually directly concatenate the visual branch and attribute branch embeddings as the final pedestrian representation, which ignores the semantic relation between the pedestrian revealed by attribute similarity. To capture and explore such semantic relation, we propose a unified pedestrian representation framework, called Visual Attribute Graph Embedding Network (VAGEN), to simultaneously learn attribute and visual representation. We unify the visual embedding and attribute similarity into a Visual Attribute Graph, where pedestrian is considered as a node and attribute similarity as an edge. Then, we learn graph node embedding to generate pedestrian representation through Graph Neural Network. Except for this unified representation for visual and attribute embeddings, VAGEN also conducts implicitly hard example mining for visual similar false-positive results, which has not been explored yet among existing attribute-based methods. We conduct extensive empirical studies on several person re-ID datasets to evaluate our proposed algorithm from different aspects. The results show that our proposed method outperforms state-of-the-art techniques with considerable margins.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4361003624",
    "type": "article"
  },
  {
    "title": "Shot Boundary Detection Using Color Clustering and Attention Mechanism",
    "doi": "https://doi.org/10.1145/3595923",
    "publication_date": "2023-05-03",
    "publication_year": 2023,
    "authors": "Ye Yuan; Jiawan Zhang",
    "corresponding_authors": "",
    "abstract": "Shot boundary detection (SBD) is widely used in scene segmentation, semantic analysis, and video retrieval. However, existing SBD algorithms have certain applications in video processing, but they have the following three problems. First, these algorithms cannot effectively handle shot boundaries caused by sudden lighting changes. Second, when there are dimly lighting frames in the video, these algorithms cannot perform boundary detection well. Third, when there is object or camera motion in the video, these algorithms also fail to work. To resolve these issues, we propose an SBD algorithm with color clustering changes in small regions (CCSR) to detect the shot transitions, which are abrupt changes and gradual transitions (dissolve and fade). The main idea behind the CCSR algorithm is to compute the distance of color features and to preserve the spatio-temporal information as much as possible. This model has relatively less dependence on the threshold parameters and sliding windows. Unlike other SBD algorithms, the clustering results of CCSR weaken factors such as object motion and illumination changes between adjacent frames in the video, which is helpful for reducing false detections. Furthermore, we utilize an attention mechanism in the gradual transitions to improve detection efficiency and accuracy. Finally, we evaluated the SBD algorithm, which was tested on a standard TRECVID dataset. The experimental results suggest that our algorithm yields significant improvements in precision and recall compared to the current techniques, with an average improvement of 10.35% and 8.85%, respectively. Moreover, compared with state-of-the-art algorithms, the results prove that the proposed method improves the F-score by more than 2.64% and the computation time efficiency by over 10%.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4367847529",
    "type": "article"
  },
  {
    "title": "Color-Unrelated Head-Shoulder Networks for Fine-Grained Person Re-identification",
    "doi": "https://doi.org/10.1145/3599730",
    "publication_date": "2023-05-25",
    "publication_year": 2023,
    "authors": "Boqiang Xu; Jian Liang; Lingxiao He; Jinlin Wu; Chao Fan; Zhenan Sun",
    "corresponding_authors": "",
    "abstract": "Person re-identification (re-id) attempts to match pedestrian images with the same identity across non-overlapping cameras. Existing methods usually study person re-id by learning discriminative features based on the clothing attributes (e.g., color, texture). However, the clothing appearance is not sufficient to distinguish different persons especially when they are in similar clothes, which is known as the fine-grained (FG) person re-id problem. By contrast, this paper proposes to exploit the color-unrelated feature along with the head-shoulder feature for FG person re-id. Specifically, a color-unrelated head-shoulder network (CUHS) is developed, which is featured in three aspects: (1) It consists of a lightweight head-shoulder segmentation layer for localizing the head-shoulder region and learning the corresponding feature. (2) It exploits instance normalization (IN) for learning color-unrelated features. (3) As IN inevitably reduces inter-class differences, we propose to explore richer visual cues for IN by an attention exploration mechanism to ensure high discrimination. We evaluate our model on the FG-reID, Market1501, and DukeMTMC-reID datasets, and the results show that CUHS surpasses previous methods on both the FG and conventional person re-id problems.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4378218572",
    "type": "article"
  },
  {
    "title": "Cloth Interactive Transformer for Virtual Try-On",
    "doi": "https://doi.org/10.1145/3617374",
    "publication_date": "2023-09-05",
    "publication_year": 2023,
    "authors": "Bin Ren; Hao Tang; Fanyang Meng; Runwei Ding; Philip H. S. Torr; Nicu Sebe",
    "corresponding_authors": "",
    "abstract": "The 2D image-based virtual try-on has aroused increased interest from the multimedia and computer vision fields due to its enormous commercial value. Nevertheless, most existing image-based virtual try-on approaches directly combine the person-identity representation and the in-shop clothing items without taking their mutual correlations into consideration. Moreover, these methods are commonly established on pure convolutional neural networks (CNNs) architectures which are not simple to capture the long-range correlations among the input pixels. As a result, it generally results in inconsistent results. To alleviate these issues, in this article, we propose a novel two-stage cloth interactive transformer (CIT) method for the virtual try-on task. During the first stage, we design a CIT matching block, aiming at precisely capturing the long-range correlations between the cloth-agnostic person information and the in-shop cloth information. Consequently, it makes the warped in-shop clothing items look more natural in appearance. In the second stage, we put forth a CIT reasoning block for establishing global mutual interactive dependencies among person representation, the warped clothing item, and the corresponding warped cloth mask. The empirical results, based on mutual dependencies, demonstrate that the final try-on results are more realistic. Substantial empirical results on a public fashion dataset illustrate that the suggested CIT attains competitive virtual try-on performance.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4386443265",
    "type": "article"
  },
  {
    "title": "TA-Detector: A GNN-based Anomaly Detector via Trust Relationship",
    "doi": "https://doi.org/10.1145/3672401",
    "publication_date": "2024-06-19",
    "publication_year": 2024,
    "authors": "Jie Wen; Nan Jiang; Lang Li; Jie Zhou; Y.Y Li; Hualin Zhan; Guang Kou; Weihao Gu; Jiahui Zhao",
    "corresponding_authors": "",
    "abstract": "With the rise of mobile Internet and AI, social media integrating short messages, images, and videos has developed rapidly. As a guarantee for the stable operation of social media, information security, especially graph anomaly detection (GAD), has become a hot issue inspired by the extensive attention of researchers. Most GAD methods are mainly limited to enhancing the homophily or considering homophily and heterophilic connections. Nevertheless, due to the deceptive nature of homophily connections among anomalies, the discriminative information of the anomalies can be eliminated. To alleviate the issue, we explore a novel method TA-Detector in GAD by introducing the concept of trust into the classification of connections. In particular, the proposed approach adopts a designed trust classier to distinguish trust and distrust connections with the supervision of labeled nodes. Then, we capture the latent factors related to GAD by graph neural networks (GNN), which integrate node interaction type information and node representation. Finally, to identify anomalies in the graph, we use the residual network mechanism to extract the deep semantic embedding information related to GAD. Experimental results on two real benchmark datasets verify that our proposed approach boosts the overall GAD performance in comparison to benchmark baselines.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4399804785",
    "type": "article"
  },
  {
    "title": "Learning rich semantics from news video archives by style analysis",
    "doi": "https://doi.org/10.1145/1142020.1142021",
    "publication_date": "2006-05-01",
    "publication_year": 2006,
    "authors": "Cees G. M. Snoek; Marcel Worring; Alexander G. Hauptmann",
    "corresponding_authors": "",
    "abstract": "We propose a generic and robust framework for news video indexing which we founded on a broadcast news production model. We identify within this model four production phases, each providing useful metadata for annotation. In contrast to semiautomatic indexing approaches which exploit this information at production time, we adhere to an automatic data-driven approach. To that end, we analyze a digital news video using a separate set of multimodal detectors for each production phase. By combining the resulting production-derived features into a statistical classifier ensemble, the framework facilitates robust classification of several rich semantic concepts in news video; rich meaning that concepts share many similarities in their production process. Experiments on an archive of 120 hours of news video from the 2003 TRECVID benchmark show that a combined analysis of production phases yields the best results. In addition, we demonstrate that the accuracy of the proposed style analysis framework for classification of several rich semantic concepts is state-of-the-art.",
    "cited_by_count": 46,
    "openalex_id": "https://openalex.org/W2144163791",
    "type": "article"
  },
  {
    "title": "Optimization of interactive visual-similarity-based search",
    "doi": "https://doi.org/10.1145/1324287.1324294",
    "publication_date": "2008-01-01",
    "publication_year": 2008,
    "authors": "Nguyen Thi Phuong Giang; Marcel Worring",
    "corresponding_authors": "",
    "abstract": "At one end of the spectrum, research in interactive content-based retrieval concentrates on machine learning methods for effective use of relevance feedback. On the other end, the information visualization community focuses on effective methods for conveying information to the user. What is lacking is research considering the information visualization and interactive retrieval as truly integrated parts of one content-based search system. In such an integrated system, there are many degrees of freedom like the similarity function, the number of images to display, the image size, different visualization modes, and possible feedback modes. To base the optimal values for all of those on user studies is unfeasible. We therefore develop search scenarios in which tasks and user actions are simulated. From there, the proposed scheme is optimized based on objective constraints and evaluation criteria. In such a manner, the degrees of freedom are reduced and the remaining degrees can be evaluated in user studies. In this article, we present a system that integrates advanced similarity based visualization with active learning. We have performed extensive experimentation on interactive category search with different image collections. The results using the proposed simulation scheme show that indeed the use of advanced visualization and active learning pays off in all of these datasets.",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W2170092354",
    "type": "article"
  },
  {
    "title": "Low-latency adaptive streaming over tcp",
    "doi": "https://doi.org/10.1145/1386109.1386113",
    "publication_date": "2008-08-01",
    "publication_year": 2008,
    "authors": "Ashvin Goel; Charles Krasic; Jonathan Walpole",
    "corresponding_authors": "",
    "abstract": "Media streaming over TCP has become increasingly popular because TCP's congestion control provides remarkable stability to the Internet. Streaming over TCP requires adapting to bandwidth availability, but unforunately, TCP can introduce significant latency at the application level, which causes unresponsive and poor adaptation. This article shows that this latency is not inherent in TCP but occurs as a result of throughput-optimized TCP implementations. We show that this latency can be minimized by dynamically tuning TCP's send buffer. Our evaluation shows that this approach leads to better application-level adaptation and it allows supporting interactive and other low-latency applications over TCP.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W2104158305",
    "type": "article"
  },
  {
    "title": "Ubiquitous device personalization and use",
    "doi": "https://doi.org/10.1145/1230812.1230818",
    "publication_date": "2007-05-01",
    "publication_year": 2007,
    "authors": "Ron Shacham; Henning Schulzrinne; Srisakul Thakolsri; Wolfgang Kellerer",
    "corresponding_authors": "",
    "abstract": "Service usage in emerging ubiquitous environments includes seamless and personalized usage of public and private devices discovered in the vicinity of a user. In our work, we describe an architecture for device discovery, device configuration, and the transfer of active sessions between devices. The presented architecture uses the Session Initiation Protocol (SIP) as a standardized, widely used signaling protocol for IP-based multimedia services. Our solution includes support of simple existing devices, split of sessions between devices, user-control of location-based behavior, and handling of security and privacy concerns. We present the implementation and show the feasibility of our work with analytical evaluation and measurements.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W2040282734",
    "type": "article"
  },
  {
    "title": "Incorporating feature hierarchy and boosting to achieve more effective classifier training and concept-oriented video summarization and skimming",
    "doi": "https://doi.org/10.1145/1324287.1324288",
    "publication_date": "2008-01-01",
    "publication_year": 2008,
    "authors": "Hangzai Luo; Yuli Gao; Xiangyang Xue; Jinye Peng; Jianping Fan",
    "corresponding_authors": "",
    "abstract": "For online medical education purposes, we have developed a novel scheme to incorporate the results of semantic video classification to select the most representative video shots for generating concept-oriented summarization and skimming of surgery education videos . First, salient objects are used as the video patterns for feature extraction to achieve a good representation of the intermediate video semantics. The salient objects are defined as the salient video compounds that can be used to characterize the most significant perceptual properties of the corresponding real world physical objects in a video, and thus the appearances of such salient objects can be used to predict the appearances of the relevant semantic video concepts in a specific video domain. Second, a novel multi-modal boosting algorithm is developed to achieve more reliable video classifier training by incorporating feature hierarchy and boosting to dramatically reduce both the training cost and the size of training samples, thus it can significantly speed up SVM (support vector machine) classifier training. In addition, the unlabeled samples are integrated to reduce the human efforts on labeling large amount of training samples. Finally, the results of semantic video classification are incorporated to enable concept-oriented video summarization and skimming. Experimental results in a specific domain of surgery education videos are provided.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W1968300426",
    "type": "article"
  },
  {
    "title": "The big picture on small screens delivering acceptable video quality in mobile TV",
    "doi": "https://doi.org/10.1145/1556134.1556137",
    "publication_date": "2009-08-01",
    "publication_year": 2009,
    "authors": "Hendrik Knoche; M. Angela Sasse",
    "corresponding_authors": "",
    "abstract": "Mobile TV viewers can change the viewing distance and (on some devices) scale the picture to their preferred viewing ratio, trading off size for angular resolution. We investigated optimal trade-offs between size and resolution through a series of studies. Participants selected their preferred size and rated the acceptability of the visual experience on a 200ppi device at a 4:3 aspect ratio. They preferred viewing ratios similar to living room TV setups regardless of the much lower resolution: at a minimum 14 pixels per degree. While traveling on trains people required videos with a height larger than 35mm.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W2031936741",
    "type": "article"
  },
  {
    "title": "Authentication schemes for multimedia streams",
    "doi": "https://doi.org/10.1145/1671954.1671960",
    "publication_date": "2010-02-01",
    "publication_year": 2010,
    "authors": "Mohamed Hefeeda; Kianoosh Mokhtarian",
    "corresponding_authors": "",
    "abstract": "With the rapid increase in the demand for multimedia services, securing the delivery of multimedia content has become an important issue. Accordingly, the problem of multimedia stream authentication has received considerable attention by previous research and various solutions have been proposed. However, these solutions have not been rigorously analyzed and contrasted to each other, and thus their relative suitability for different streaming environments is not clear. This article presents comprehensive analysis and comparison among different schemes proposed in the literature to authenticate multimedia streams. Authentication schemes for nonscalable and scalable multimedia streams are analyzed. To conduct this analysis, we define five important performance metrics, which are computation cost, communication overhead, receiver buffer size, delay, and tolerance to packet losses. We derive analytic formulas for these metrics for all considered authentication schemes to numerically analyze their performance. In addition, we implement all schemes in a simulator to study and compare their performance in different environments. The parameters for the simulator are carefully chosen to mimic realistic settings. We draw several conclusions on the advantages and disadvantages of each scheme. We extend our analysis to authentication techniques for scalable streams. We pay careful attention to the flexibility of scalable streams and analyze its impacts on the authentication schemes. Our analysis and comparison reveal the merits and shortcomings of each scheme, provide guidelines on choosing the most appropriate scheme for a given multimedia streaming application, and could stimulate designing new authentication schemes or improving existing ones. For example, our detailed analysis has led us to design a new authentication scheme that combines the best features of two previous schemes.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2017167863",
    "type": "article"
  },
  {
    "title": "ELVIS",
    "doi": "https://doi.org/10.1145/1823746.1823751",
    "publication_date": "2010-08-01",
    "publication_year": 2010,
    "authors": "Arthur Money; Harry Agius",
    "corresponding_authors": "",
    "abstract": "Video summaries present the user with a condensed and succinct representation of the content of a video stream. Usually this is achieved by attaching degrees of importance to low-level image, audio and text features. However, video content elicits strong and measurable physiological responses in the user, which are potentially rich indicators of what video content is memorable to or emotionally engaging for an individual user. This article proposes a technique that exploits such physiological responses to a given video stream by a given user to produce Entertainment-Led VIdeo Summaries (ELVIS). ELVIS is made up of five analysis phases which correspond to the analyses of five physiological response measures: electro-dermal response (EDR), heart rate (HR), blood volume pulse (BVP), respiration rate (RR), and respiration amplitude (RA). Through these analyses, the temporal locations of the most entertaining video subsegments, as they occur within the video stream as a whole, are automatically identified. The effectiveness of the ELVIS technique is verified through a statistical analysis of data collected during a set of user trials. Our results show that ELVIS is more consistent than RANDOM, EDR, HR, BVP, RR and RA selections in identifying the most entertaining video subsegments for content in the comedy, horror/comedy, and horror genres. Subjective user reports also reveal that ELVIS video summaries are comparatively easy to understand, enjoyable, and informative.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W1964981210",
    "type": "article"
  },
  {
    "title": "Collaborative video reindexing via matrix factorization",
    "doi": "https://doi.org/10.1145/2168996.2169003",
    "publication_date": "2012-05-01",
    "publication_year": 2012,
    "authors": "Ming-Fang Weng; Yung‐Yu Chuang",
    "corresponding_authors": "",
    "abstract": "Concept-based video indexing generates a matrix of scores predicting the possibilities of concepts occurring in video shots. Based on the idea of collaborative filtering, this article presents unsupervised methods to refine the initial scores generated by concept classifiers by taking into account the concept-to-concept correlation and shot-to-shot similarity embedded within the score matrix. Given a noisy matrix, we refine the inaccurate scores via matrix factorization. This method is further improved by learning multiple local models and incorporating contextual-temporal structures. Experiments on the TRECVID 2006--2008 datasets demonstrate relative performance gains ranging from 13% to 52% without using any user annotations or external knowledge resources.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2080943432",
    "type": "article"
  },
  {
    "title": "ImageSense",
    "doi": "https://doi.org/10.1145/2071396.2071402",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Tao Mei; Lusong Li; Xian‐Sheng Hua; Shipeng Li",
    "corresponding_authors": "",
    "abstract": "The daunting volumes of community-contributed media contents on the Internet have become one of the primary sources for online advertising. However, conventional advertising treats image and video advertising as general text advertising by displaying relevant ads based on the contents of the Web page, without considering the inherent characteristics of visual contents. This article presents a contextual advertising system driven by images, which automatically associates relevant ads with an image rather than the entire text in a Web page and seamlessly inserts the ads in the nonintrusive areas within each individual image. The proposed system, called ImageSense , supports scalable advertising of, from root to node, Web sites, pages, and images. In ImageSense, the ads are selected based on not only textual relevance but also visual similarity, so that the ads yield contextual relevance to both the text in the Web page and the image content. The ad insertion positions are detected based on image salience, as well as face and text detection, to minimize intrusiveness to the user. We evaluate ImageSense on a large-scale real-world images and Web pages, and demonstrate the effectiveness of ImageSense for online image advertising.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W2030602513",
    "type": "article"
  },
  {
    "title": "In-video product annotation with web information mining",
    "doi": "https://doi.org/10.1145/2379790.2379797",
    "publication_date": "2012-11-01",
    "publication_year": 2012,
    "authors": "Guangda Li; Meng Wang; Zheng Lu; Richang Hong; Tat‐Seng Chua",
    "corresponding_authors": "",
    "abstract": "Product annotation in videos is of great importance for video browsing, search, and advertisement. However, most of the existing automatic video annotation research focuses on the annotation of high-level concepts, such as events, scenes, and object categories. This article presents a novel solution to the annotation of specific products in videos by mining information from the Web. It collects a set of high-quality training data for each product by simultaneously leveraging Amazon and Google image search engine. A visual signature for each product is then built based on the bag-of-visual-words representation of the training images. A correlative sparsification approach is employed to remove noisy bins in the visual signatures. These signatures are used to annotate video frames. We conduct experiments on more than 1,000 videos and the results demonstrate the feasibility and effectiveness of our approach.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W1998049905",
    "type": "article"
  },
  {
    "title": "Structured Streaming Skeleton -- A New Feature for Online Human Gesture Recognition",
    "doi": "https://doi.org/10.1145/2648583",
    "publication_date": "2014-10-01",
    "publication_year": 2014,
    "authors": "Xin Zhao; Xue Li; Chaoyi Pang; Quan Z. Sheng; Sen Wang; Mao Ye",
    "corresponding_authors": "",
    "abstract": "Online human gesture recognition has a wide range of applications in computer vision, especially in human-computer interaction applications. The recent introduction of cost-effective depth cameras brings a new trend of research on body-movement gesture recognition. However, there are two major challenges: (i) how to continuously detect gestures from unsegmented streams, and (ii) how to differentiate different styles of the same gesture from other types of gestures. In this article, we solve these two problems with a new effective and efficient feature extraction method—Structured Streaming Skeleton (SSS)—which uses a dynamic matching approach to construct a feature vector for each frame. Our comprehensive experiments on MSRC-12 Kinect Gesture, Huawei/3DLife-2013, and MSR-Action3D datasets have demonstrated superior performances than the state-of-the-art approaches. We also demonstrate model selection based on the proposed SSS feature, where the classifier of squared loss regression with l 2,1 norm regularization is a recommended classifier for best performance.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W1968073106",
    "type": "article"
  },
  {
    "title": "From 3D Sensing to Printing",
    "doi": "https://doi.org/10.1145/2818710",
    "publication_date": "2015-10-20",
    "publication_year": 2015,
    "authors": "Longyu Zhang; Haiwei Dong; Abdulmotaleb El Saddik",
    "corresponding_authors": "",
    "abstract": "Three-dimensional (3D) sensing and printing technologies have reshaped our world in recent years. In this article, a comprehensive overview of techniques related to the pipeline from 3D sensing to printing is provided. We compare the latest 3D sensors and 3D printers and introduce several sensing, postprocessing, and printing techniques available from both commercial deployments and published research. In addition, we demonstrate several devices, software, and experimental results of our related projects to further elaborate details of this process. A case study is conducted to further illustrate the possible tradeoffs during the process of this pipeline. Current progress, future research trends, and potential risks of 3D technologies are also discussed.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2086707973",
    "type": "article"
  },
  {
    "title": "Convergence of interactive displays with smart mobile devices for effective advertising",
    "doi": "https://doi.org/10.1145/2557450",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "James She; Jon Crowcroft; Hao Fu; Flora Li",
    "corresponding_authors": "",
    "abstract": "The trend of replacing public static signages with digital displays creates opportunities for interactive display systems, which can be used in collaborative workspaces, social gaming platforms and advertising. Based on marketing communication concepts and existing models for consumer behavior, three stages, namely attraction, interaction and conation, are defined in this article to analyze the effectiveness of interactive display advertising. By reviewing various methods and strategies employed by existing systems with attraction, interaction and conation stages, this article concludes that smart mobile devices should be integrated as a component to increase the effectiveness of interactive displays as advertising tools. Future research challenges related to this topic are also discussed.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2123599291",
    "type": "article"
  },
  {
    "title": "Drift-Compensated Robust Watermarking Algorithm for H.265/HEVC Video Stream",
    "doi": "https://doi.org/10.1145/3009910",
    "publication_date": "2017-01-17",
    "publication_year": 2017,
    "authors": "Sibaji Gaj; Aditya Kanetkar; Arijit Sur; Prabin Kumar Bora",
    "corresponding_authors": "",
    "abstract": "It has been observed in the recent literature that the drift error due to watermarking degrades the visual quality of the embedded video. The existing drift error handling strategies for recent video standards such as H.264 may not be directly applicable for upcoming high-definition video standards (such as High Efficiency Video Coding (HEVC)) due to different compression architecture. In this article, a compressed domain watermarking scheme is proposed for H.265/HEVC bit stream that can handle drift error propagation both for intra- and interprediction process. Additionally, the proposed scheme shows adequate robustness against recompression attack as well as common image processing attacks while maintaining decent visual quality. A comprehensive set of experiments has been carried out to justify the efficacy of the proposed scheme over the existing literature.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2575471480",
    "type": "article"
  },
  {
    "title": "Performance Analysis of Game Engines on Mobile and Fixed Devices",
    "doi": "https://doi.org/10.1145/3115934",
    "publication_date": "2017-09-18",
    "publication_year": 2017,
    "authors": "Farouk Messaoudi; Adlen Ksentini; Gwendal Simon; Philippe Bertin",
    "corresponding_authors": "",
    "abstract": "Mobile gaming is an emerging concept wherein gamers are using mobile devices, like smartphones and tablets, to play best-seller games. Compared to dedicated gaming boxes or PCs, these devices still fall short of executing newly complex 3D video games with a rich immersion. Three novel solutions, relying on cloud computing infrastructure, namely, computation offloading, cloud gaming, and client-server architecture, will represent the next generation of game engine architecture aiming at improving the gaming experience. The basis of these aforementioned solutions is the distribution of the game code over different devices (including set-top boxes, PCs, and servers). In order to know how the game code should be distributed, advanced knowledge of game engines is required. By consequence, dissecting and analyzing game engine performances will surely help to better understand how to move in these new directions (i.e., distribute game code), which is so far missing in the literature. Aiming at filling this gap, we propose in this article to analyze and evaluate one of the famous engines in the market, that is, “Unity 3D.” We begin by detailing the architecture and the game logic of game engines. Then, we propose a test-bed to evaluate the CPU and GPU consumption per frame and per module for nine representative games on three platforms, namely, a stand-alone computer, embedded systems, and web players. Based on the obtained results and observations, we build a valued graph of each module, composing the Unity 3D architecture, which reflects the internal flow and CPU consumption. Finally, we made a comparison in terms of CPU consumption between these architectures.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2754631256",
    "type": "article"
  },
  {
    "title": "Speaker-Following Video Subtitles",
    "doi": "https://doi.org/10.1145/2632111",
    "publication_date": "2015-01-07",
    "publication_year": 2015,
    "authors": "Yongtao Hu; Jan Kautz; Yizhou Yu; Wenping Wang",
    "corresponding_authors": "",
    "abstract": "We propose a new method for improving the presentation of subtitles in video (e.g. TV and movies). With conventional subtitles, the viewer has to constantly look away from the main viewing area to read the subtitles at the bottom of the screen, which disrupts the viewing experience and causes unnecessary eyestrain. Our method places on-screen subtitles next to the respective speakers to allow the viewer to follow the visual content while simultaneously reading the subtitles. We use novel identification algorithms to detect the speakers based on audio and visual information. Then the placement of the subtitles is determined using global optimization. A comprehensive usability study indicated that our subtitle placement method outperformed both conventional fixed-position subtitling and another previous dynamic subtitling method in terms of enhancing the overall viewing experience and reducing eyestrain.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W3125983324",
    "type": "article"
  },
  {
    "title": "Auction-based P2P VoD streaming",
    "doi": "https://doi.org/10.1145/2089085.2089091",
    "publication_date": "2012-02-01",
    "publication_year": 2012,
    "authors": "Chuan Wu; Zongpeng Li; Xuanjia Qiu; Francis C. M. Lau",
    "corresponding_authors": "",
    "abstract": "Real-world large-scale Peer-to-Peer (P2P) Video-on-Demand (VoD) streaming applications face more design challenges as compared to P2P live streaming, due to higher peer dynamics and less buffer overlap. The situation is further complicated when we consider the selfish nature of peers, who in general wish to download more and upload less, unless otherwise motivated. Taking a new perspective of distributed dynamic auctions, we design efficient P2P VoD streaming algorithms with simultaneous consideration of peer incentives and streaming optimality. In our solution, media block exchanges among peers are carried out through local auctions, in which budget-constrained peers bid for desired blocks from their neighbors, which in turn deliver blocks to the winning bidders and collect revenue. With strategic design of a discriminative second price auction with seller reservation, a supplying peer has full incentive to maximally contribute its bandwidth to increase its budget; requesting peers are also motivated to bid in such a way that optimal media block scheduling is achieved effectively in a fully decentralized fashion. Applying techniques from convex optimization and mechanism design, we prove (a) the incentive compatibility at the selling and buying peers, and (b) the optimality of the induced media block scheduling in terms of social welfare maximization. Large-scale empirical studies are conducted to investigate the behavior of the proposed auction mechanisms in dynamic P2P VoD systems based on real-world settings.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2011308849",
    "type": "article"
  },
  {
    "title": "Personalized Video Recommendation through Graph Propagation",
    "doi": "https://doi.org/10.1145/2598779",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "Qinghua Huang; Bisheng Chen; Jingdong Wang; Tao Mei",
    "corresponding_authors": "",
    "abstract": "The rapid growth of the number of videos on the Internet provides enormous potential for users to find content of interest. However, the vast quantity of videos also turns the finding process into a difficult task. In this article, we address the problem of providing personalized video recommendation for users. Rather than only exploring the user-video bipartite graph that is formulated using click information, we first combine the clicks and queries information to build a tripartite graph. In the tripartite graph, the query nodes act as bridges to connect user nodes and video nodes. Then, to further enrich the connections between users and videos, three subgraphs between the same kinds of nodes are added to the tripartite graph by exploring content-based information (video tags and textual queries). We propose an iterative propagation algorithm over the enhanced graph to compute the preference information of each user. Experiments conducted on a dataset with 1,369 users, 8,765 queries, and 17,712 videos collected from a commercial video search engine demonstrate the effectiveness of the proposed method.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2014421179",
    "type": "article"
  },
  {
    "title": "NextSlidePlease",
    "doi": "https://doi.org/10.1145/2379790.2379795",
    "publication_date": "2012-11-01",
    "publication_year": 2012,
    "authors": "Ryan Spicer; Yu‐Ru Lin; Aisling Kelliher; Hari Sundaram",
    "corresponding_authors": "",
    "abstract": "Presentation support tools, such as Microsoft PowerPoint, pose challenges both in terms of creating linear presentations from complex data and fluidly navigating such linear structures when presenting to diverse audiences. NextSlidePlease is a slideware application that addresses these challenges using a directed graph structure approach for authoring and delivering multimedia presentations. The application combines novel approaches for searching and analyzing presentation datasets, composing meaningfully structured presentations, and efficiently delivering material under a variety of time constraints. We introduce and evaluate a presentation analysis algorithm intended to simplify the process of authoring dynamic presentations, and a time management and path selection algorithm that assists users in prioritizing content during the presentation process. Results from two comparative user studies indicate that the directed graph approach promotes the creation of hyperlinks, the consideration of connections between content items, and a richer understanding of the time management consequences of including and selecting presentation material.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2042925766",
    "type": "article"
  },
  {
    "title": "Bilateral Correspondence Model for Words-and-Pictures Association in Multimedia-Rich Microblogs",
    "doi": "https://doi.org/10.1145/2611388",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "Zhiyu Wang; Peng Cui; Lexing Xie; Wenwu Zhu; Yong Rui; Shiqiang Yang",
    "corresponding_authors": "",
    "abstract": "Nowadays, the amount of multimedia contents in microblogs is growing significantly. More than 20% of microblogs link to a picture or video in certain large systems. The rich semantics in microblogs provides an opportunity to endow images with higher-level semantics beyond object labels. However, this raises new challenges for understanding the association between multimodal multimedia contents in multimedia-rich microblogs. Disobeying the fundamental assumptions of traditional annotation, tagging, and retrieval systems, pictures and words in multimedia-rich microblogs are loosely associated and a correspondence between pictures and words cannot be established. To address the aforementioned challenges, we present the first study analyzing and modeling the associations between multimodal contents in microblog streams, aiming to discover multimodal topics from microblogs by establishing correspondences between pictures and words in microblogs. We first use a data-driven approach to analyze the new characteristics of the words, pictures, and their association types in microblogs. We then propose a novel generative model called the Bilateral Correspondence Latent Dirichlet Allocation (BC-LDA) model. Our BC-LDA model can assign flexible associations between pictures and words and is able to not only allow picture-word co-occurrence with bilateral directions, but also single modal association. This flexible association can best fit the data distribution, so that the model can discover various types of joint topics and generate pictures and words with the topics accordingly. We evaluate this model extensively on a large-scale real multimedia-rich microblogs dataset. We demonstrate the advantages of the proposed model in several application scenarios, including image tagging, text illustration, and topic discovery. The experimental results demonstrate that our proposed model can significantly and consistently outperform traditional approaches.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2046945311",
    "type": "article"
  },
  {
    "title": "Using Paired Distances of Signal Peaks in Stereo Channels as Fingerprints for Copy Identification",
    "doi": "https://doi.org/10.1145/2742059",
    "publication_date": "2015-08-24",
    "publication_year": 2015,
    "authors": "Shingchern D. You; Yihan Pu",
    "corresponding_authors": "",
    "abstract": "This article proposes to use the relative distances between adjacent envelope peaks detected in stereo audio as fingerprints for copy identification. The matching algorithm used is the rough longest common subsequence (RLCS) algorithm. The experimental results show that the proposed approach has better identification accuracy than an MPEG-7 based scheme for distorted and noisy audio. When compared with other schemes, the proposed scheme uses fewer bits with comparable performance. The proposed fingerprints can also be used in conjunction with the MPEG-7 based scheme for lower computational burden.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2198557826",
    "type": "article"
  },
  {
    "title": "Intensifying Emotional Reactions via Tactile Gestures in Immersive Films",
    "doi": "https://doi.org/10.1145/3092840",
    "publication_date": "2017-06-28",
    "publication_year": 2017,
    "authors": "Georgios Karafotias; Akiko Teranishi; Γεώργιος Κορρές; Friederike Eyssel; Scandar Copti; Mohamad Eid",
    "corresponding_authors": "",
    "abstract": "The film industry continuously strives to make visitors’ movie experience more immersive and thus, more captivating. This is realized through larger screens, sophisticated speaker systems, and high quality 2D and 3D content. Moreover, a recent trend in the film industry is to incorporate multiple interaction modalities, such as 4D film, to simulate rain, wind, vibration, and heat, in order to intensify viewers’ emotional reactions. In this context, humans’ sense of touch possesses significant potential for intensifying emotional reactions for the film experience beyond audio-visual sensory modalities. This article presents a framework for authoring tactile cues (tactile gestures as used in this article) and enabling automatic rendering of said gestures to intensify emotional reactions in an immersive film experience. To validate the proposed framework, we conducted an experimental study where tactile gestures are designed and evaluated for the ability to intensify four emotional reactions: high valence-high arousal, high valence-low arousal, low valence-high arousal, and low valence-low arousal. Using a haptic jacket, participants felt tactile gestures that are synchronized with the audio-visual contents of a film. Results demonstrated that (1) any tactile feedback generated a positive user experience; (2) the tactile feedback intensifies emotional reactions when the audio-visual stimuli elicit clear emotional responses, except for low arousal emotional response since tactile gestures seem to always generate excitement; (3) purposed tactile gestures do not seem to significantly outperform randomized tactile gesture for intensifying specific emotional reactions; and (4) using a haptic jacket is not distracting for the users.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2731261099",
    "type": "article"
  },
  {
    "title": "Robust Privacy-Preserving Image Sharing over Online Social Networks (OSNs)",
    "doi": "https://doi.org/10.1145/3165265",
    "publication_date": "2018-01-04",
    "publication_year": 2018,
    "authors": "Weiwei Sun; Jiantao Zhou; Shuyuan Zhu; Yuan Yan Tang",
    "corresponding_authors": "",
    "abstract": "Sharing images online has become extremely easy and popular due to the ever-increasing adoption of mobile devices and online social networks (OSNs). The privacy issues arising from image sharing over OSNs have received significant attention in recent years. In this article, we consider the problem of designing a secure, robust, high-fidelity, storage-efficient image-sharing scheme over Facebook, a representative OSN that is widely accessed. To accomplish this goal, we first conduct an in-depth investigation on the manipulations that Facebook performs to the uploaded images. Assisted by such knowledge, we propose a DCT-domain image encryption/decryption framework that is robust against these lossy operations. As verified theoretically and experimentally, superior performance in terms of data privacy, quality of the reconstructed images, and storage cost can be achieved.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2782363563",
    "type": "article"
  },
  {
    "title": "Game Categorization for Deriving QoE-Driven Video Encoding Configuration Strategies for Cloud Gaming",
    "doi": "https://doi.org/10.1145/3132041",
    "publication_date": "2018-06-27",
    "publication_year": 2018,
    "authors": "Ivan Slivar; Mirko Sužnjević; Lea Skorin‐Kapov",
    "corresponding_authors": "",
    "abstract": "Cloud gaming has been recognized as a promising shift in the online game industry, with the aim of implementing the “on demand” service concept that has achieved market success in other areas of digital entertainment such as movies and TV shows. The concepts of cloud computing are leveraged to render the game scene as a video stream that is then delivered to players in real-time. The main advantage of this approach is the capability of delivering high-quality graphics games to any type of end user device; however, at the cost of high bandwidth consumption and strict latency requirements. A key challenge faced by cloud game providers lies in configuring the video encoding parameters so as to maximize player Quality of Experience (QoE) while meeting bandwidth availability constraints. In this article, we tackle one aspect of this problem by addressing the following research question: Is it possible to improve service adaptation based on information about the characteristics of the game being streamed? To answer this question, two main challenges need to be addressed: the need for different QoE-driven video encoding (re-)configuration strategies for different categories of games, and how to determine a relevant game categorization to be used for assigning appropriate configuration strategies. We investigate these problems by conducting two subjective laboratory studies with a total of 80 players and three different games. Results indicate that different strategies should likely be applied for different types of games, and show that existing game classifications are not necessarily suitable for differentiating game types in this context. We thus further analyze objective video metrics of collected game play video traces as well as player actions per minute and use this as input data for clustering of games into two clusters. Subjective results verify that different video encoding configuration strategies may be applied to games belonging to different clusters.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2810014379",
    "type": "article"
  },
  {
    "title": "Soundscape of an Archaeological Site Recreated with Audio Augmented Reality",
    "doi": "https://doi.org/10.1145/3230652",
    "publication_date": "2018-07-24",
    "publication_year": 2018,
    "authors": "Marjan Sikora; Mladen Russo; Jurica Đerek; Ante Jurčević",
    "corresponding_authors": "",
    "abstract": "This article investigates the use of an audio augmented reality (AAR) system to recreate the soundscape of a medieval archaeological site. The aim of our work was to explore whether it is possible to enhance a tourist's archaeological experience, which is often derived from only scarce remains. We developed a smartphone-based AAR system, which uses location and orientation sensors to synthesize the soundscape of a site and plays it to the user via headphones. We recreated the ancient soundscape of a medieval archaeological site in Croatia and tested it in situ on two groups of participants using the soundwalk method. One test group performed the soundwalk while listening to the recreated soundscape using the AAR system, while the second control group did not use the AAR equipment. We measured the experiences of the participants using two methods: the standard soundwalk questionnaire and affective computing equipment for detecting the emotional state of participants. The results of both test methods show that participants who were listening to the ancient soundscape using our AAR system experienced higher arousal than those visiting the site without AAR.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2884119805",
    "type": "article"
  },
  {
    "title": "Mixed image-keyword query adaptive hashing over multilabel images",
    "doi": "https://doi.org/10.1145/2540990",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "Xianglong Liu; Yadong Mu; Bo Lang; Shih‐Fu Chang",
    "corresponding_authors": "",
    "abstract": "This article defines a new hashing task motivated by real-world applications in content-based image retrieval, that is, effective data indexing and retrieval given mixed query (query image together with user-provided keywords). Our work is distinguished from state-of-the-art hashing research by two unique features: (1) Unlike conventional image retrieval systems, the input query is a combination of an exemplar image and several descriptive keywords, and (2) the input image data are often associated with multiple labels. It is an assumption that is more consistent with the realistic scenarios. The mixed image-keyword query significantly extends traditional image-based query and better explicates the user intention. Meanwhile it complicates semantics-based indexing on the multilabel data. Though several existing hashing methods can be adapted to solve the indexing task, unfortunately they all prove to suffer from low effectiveness. To enhance the hashing efficiency, we propose a novel scheme “boosted shared hashing”. Unlike prior works that learn the hashing functions on either all image labels or a single label, we observe that the hashing function can be more effective if it is designed to index over an optimal label subset. In other words, the association between labels and hash bits are moderately sparse. The sparsity of the bit-label association indicates greatly reduced computation and storage complexities for indexing a new sample, since only limited number of hashing functions will become active for the specific sample. We develop a Boosting style algorithm for simultaneously optimizing both the optimal label subsets and hashing functions in a unified formulation, and further propose a query-adaptive retrieval mechanism based on hash bit selection for mixed queries, no matter whether or not the query words exist in the training data. Moreover, we show that the proposed method can be easily extended to the case where the data similarity is gauged by nonlinear kernel functions. Extensive experiments are conducted on standard image benchmarks like CIFAR-10, NUS-WIDE and a-TRECVID. The results validate both the sparsity of the bit-label association and the convergence of the proposed algorithm, and demonstrate that the proposed hashing scheme achieves substantially superior performances over state-of-the-art methods under the same hash bit budget.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2060306405",
    "type": "article"
  },
  {
    "title": "MOWL",
    "doi": "https://doi.org/10.1145/2542205.2542210",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Anupama Mallik; Hiranmay Ghosh; Santanu Chaudhury; Gaurav Harit",
    "corresponding_authors": "",
    "abstract": "Several multimedia applications need to reason with concepts and their media properties in specific domain contexts. Media properties of concepts exhibit some unique characteristics that cannot be dealt with conceptual modeling schemes followed in the existing ontology representation and reasoning schemes. We have proposed a new perceptual modeling technique for reasoning with media properties observed in multimedia instances and the latent concepts. Our knowledge representation scheme uses a causal model of the world where concepts manifest in media properties with uncertainties. We introduce a probabilistic reasoning scheme for belief propagation across domain concepts through observation of media properties. In order to support the perceptual modeling and reasoning paradigm, we propose a new ontology language, Multimedia Web Ontology Language (MOWL). Our primary contribution in this article is to establish the need for the new ontology language and to introduce the semantics of its novel language constructs. We establish the generality of our approach with two disperate knowledge-intensive applications involving reasoning with media properties of concepts.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2128094052",
    "type": "article"
  },
  {
    "title": "Privacy-Preserving Multimedia Big Data Aggregation in Large-Scale Wireless Sensor Networks",
    "doi": "https://doi.org/10.1145/2978570",
    "publication_date": "2016-09-15",
    "publication_year": 2016,
    "authors": "Dapeng Wu; Boran Yang; Honggang Wang; Chonggang Wang; Ruyan Wang",
    "corresponding_authors": "",
    "abstract": "To preserve the privacy of multimedia big data and achieve the efficient data aggregation in wireless multimedia sensor networks (WMSNs), a distributed compressed sensing--based privacy-preserving data aggregation (DCSPDA) approach is proposed in this article. First, in this approach, the original multimedia sensor data are compressed and measured by distributed compressed sensing (DCS) and the compressed data measurements are uploaded to the sink, by which the inherent characteristics between sensor data can be obtained. Second, the original multimedia data are jointly recovered and the common and innovation sparse components are obtained through solving the optimization problem and linear equations at the sink. Third, through least squares support vector machine (LSSVM) learning of the sparse components, the sparse position configuration can be determined and disseminated for each node to conduct the privacy-preserving data configuration. After receiving the configuration message, original multimedia sensor data are accordingly customized, compressed, and measured by the common measurement matrix, aggregated at the cluster heads, and transmitted to the sink. Finally, the aggregated multimedia sensor data are recovered by the sink according to the data configuration to achieve the privacy-preserving data aggregation and transmission. Our comparative simulation results validate the efficiency and scalability of DCSPDA and demonstrate that the proposed approach can effectively reduce the communication overheads and provide reliable privacy-preserving with low computational complexity for WMSNs.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2520191464",
    "type": "article"
  },
  {
    "title": "From Annotation to Computer-Aided Diagnosis",
    "doi": "https://doi.org/10.1145/3079765",
    "publication_date": "2017-05-31",
    "publication_year": 2017,
    "authors": "Michael A. Riegler; Konstantin Pogorelov; Sigrun Losada Eskeland; Peter T. Schmidt; Zeno Albisser; Dag Johansen; Carsten Griwodz; Pål Halvorsen; Thomas de Lange",
    "corresponding_authors": "",
    "abstract": "Holistic medical multimedia systems covering end-to-end functionality from data collection to aided diagnosis are highly needed, but rare. In many hospitals, the potential value of multimedia data collected through routine examinations is not recognized. Moreover, the availability of the data is limited, as the health care personnel may not have direct access to stored data. However, medical specialists interact with multimedia content daily through their everyday work and have an increasing interest in finding ways to use it to facilitate their work processes. In this article, we present a novel, holistic multimedia system aiming to tackle automatic analysis of video from gastrointestinal (GI) endoscopy. The proposed system comprises the whole pipeline, including data collection, processing, analysis, and visualization. It combines filters using machine learning, image recognition, and extraction of global and local image features. The novelty is primarily in this holistic approach and its real-time performance, where we automate a complete algorithmic GI screening process. We built the system in a modular way to make it easily extendable to analyze various abnormalities, and we made it efficient in order to run in real time. The conducted experimental evaluation proves that the detection and localization accuracy are comparable or even better than existing systems, but it is by far leading in terms of real-time performance and efficient resource consumption.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2623206548",
    "type": "article"
  },
  {
    "title": "Cross-Domain Multi-Event Tracking via CO-PMHT",
    "doi": "https://doi.org/10.1145/2602633",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "Tianzhu Zhang; Changsheng Xu",
    "corresponding_authors": "",
    "abstract": "With the massive growth of events on the Internet, efficient organization and monitoring of events becomes a practical challenge. To deal with this problem, we propose a novel CO-PMHT (CO-Probabilistic Multi-Hypothesis Tracking) algorithm for cross-domain multi-event tracking to obtain their informative summary details and evolutionary trends over time. We collect a large-scale dataset by searching keywords on two domains (Gooogle News and Flickr) and downloading both images and textual content for an event. Given the input data, our algorithm can track multiple events in the two domains collaboratively and boost the tracking performance. Specifically, the bridge between two domains is a semantic posterior probability, that avoids the domain gap. After tracking, we can visualize the whole evolutionary process of the event over time and mine the semantic topics of each event for deep understanding and event prediction. The extensive experimental evaluations on the collected dataset well demonstrate the effectiveness of the proposed algorithm for cross-domain multi-event tracking.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2044686187",
    "type": "article"
  },
  {
    "title": "Context-Aware Photography Learning for Smart Mobile Devices",
    "doi": "https://doi.org/10.1145/2808199",
    "publication_date": "2015-10-21",
    "publication_year": 2015,
    "authors": "Yogesh Singh Rawat; Mohan Kankanhalli",
    "corresponding_authors": "",
    "abstract": "In this work we have developed a photography model based on machine learning which can assist a user in capturing high quality photographs. As scene composition and camera parameters play a vital role in aesthetics of a captured image, the proposed method addresses the problem of learning photographic composition and camera parameters. Further, we observe that context is an important factor from a photography perspective, we therefore augment the learning with associated contextual information. The proposed method utilizes publicly available photographs along with social media cues and associated metainformation in photography learning. We define context features based on factors such as time, geolocation, environmental conditions and type of image, which have an impact on photography. We also propose the idea of computing the photographic composition basis, eigenrules and baserules , to support our composition learning. The proposed system can be used to provide feedback to the user regarding scene composition and camera parameters while the scene is being captured. It can also recommend position in the frame where people should stand for better composition. Moreover, it also provides camera motion guidance for pan, tilt and zoom to the user for improving scene composition.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2085948715",
    "type": "article"
  },
  {
    "title": "Applying Seamful Design in Location-Based Mobile Museum Applications",
    "doi": "https://doi.org/10.1145/2962720",
    "publication_date": "2016-08-24",
    "publication_year": 2016,
    "authors": "Tommy Nilsson; Carl Hogsden; Charith Perera; Saeed Aghaee; David Scruton; Andreas Lund; Alan F. Blackwell",
    "corresponding_authors": "",
    "abstract": "The application of mobile computing is currently altering patterns of our behavior to a greater degree than perhaps any other invention. In combination with the introduction of power-efficient wireless communication technologies, such as Bluetooth Low Energy (BLE), designers are today increasingly empowered to shape the way we interact with our physical surroundings and thus build entirely new experiences. However, our evaluations of BLE and its abilities to facilitate mobile location-based experiences in public environments revealed a number of potential problems. Most notably, the position and orientation of the user in combination with various environmental factors, such as crowds of people traversing the space, were found to cause major fluctuations of the received BLE signal strength. These issues are rendering a seamless functioning of any location-based application practically impossible. Instead of achieving seamlessness by eliminating these technical issues, we thus choose to advocate the use of a seamful approach, that is, to reveal and exploit these problems and turn them into a part of the actual experience. In order to demonstrate the viability of this approach, we designed, implemented, and evaluated the Ghost Detector —an educational location-based museum game for children. By presenting a qualitative evaluation of this game and by motivating our design decisions, this article provides insight into some of the challenges and possible solutions connected to the process of developing location-based BLE-enabled experiences for public cultural spaces.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2402588664",
    "type": "article"
  },
  {
    "title": "SecSIFT",
    "doi": "https://doi.org/10.1145/2978574",
    "publication_date": "2016-09-15",
    "publication_year": 2016,
    "authors": "Zhan Qin; Jingbo Yan; Kui Ren; Chang Wen Chen; Cong Wang",
    "corresponding_authors": "",
    "abstract": "The image and multimedia data produced by individuals and enterprises is increasing every day. Motivated by the advances in cloud computing, there is a growing need to outsource such computational intensive image feature detection tasks to cloud for its economic computing resources and on-demand ubiquitous access. However, the concerns over the effective protection of private image and multimedia data when outsourcing it to cloud platform become the major barrier that impedes the further implementation of cloud computing techniques over massive amount of image and multimedia data. To address this fundamental challenge, we study the state-of-the-art image feature detection algorithms and focus on Scalar Invariant Feature Transform (SIFT), which is one of the most important local feature detection algorithms and has been broadly employed in different areas, including object recognition, image matching, robotic mapping, and so on. We analyze and model the privacy requirements in outsourcing SIFT computation and propose Secure Scalar Invariant Feature Transform (SecSIFT), a high-performance privacy-preserving SIFT feature detection system. In contrast to previous works, the proposed design is not restricted by the efficiency limitations of current homomorphic encryption scheme. In our design, we decompose and distribute the computation procedures of the original SIFT algorithm to a set of independent, co-operative cloud servers and keep the outsourced computation procedures as simple as possible to avoid utilizing a computationally expensive homomorphic encryption scheme. The proposed SecSIFT enables implementation with practical computation and communication complexity. Extensive experimental results demonstrate that SecSIFT performs comparably to original SIFT on image benchmarks while capable of preserving the privacy in an efficient way.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2519511029",
    "type": "article"
  },
  {
    "title": "A Deployment Optimization Scheme Over Multimedia Big Data for Large-Scale Media Streaming Application",
    "doi": "https://doi.org/10.1145/2983642",
    "publication_date": "2016-10-12",
    "publication_year": 2016,
    "authors": "Taotao Wu; Wanchun Dou; Fan Wu; Shaojie Tang; Chunhua Hu; Jinjun Chen",
    "corresponding_authors": "",
    "abstract": "With the prosperity of media streaming applications over the Internet in the past decades, multimedia data has sharply increased (categorized as multimedia big data), which exerts more pressure on the infrastructure, such as networking of the application provider. In order to move this hurdle, an increasing number of traditional media streaming applications have migrated from a private server cluster onto the cloud. With the elastic resource provisioning and centralized management of the cloud, the operational costs of media streaming application providers can decrease dramatically. However, to the best of our knowledge, existing migration solutions do not fully take viewer information such as hardware condition into consideration. In this article, we consider the deployment optimization problem named ODP by leveraging local memories at each viewer. Considering the NP-hardness of calculating the optimal solution, we turn to propose computationally tractable algorithms. Specifically, we unfold the original problem into two interactive subproblems: coarse-grained migration subproblem and fine-grained scheduling subproblem. Then, the corresponding offline approximation algorithms with performance guarantee and computational efficiency are given. The results of extensive evaluation show that compared with the baseline algorithm without leveraging local memories at viewers, our proposed algorithms and their online versions can decrease total bandwidth reservation and enhance the utilization of bandwidth reservation dramatically.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2530869753",
    "type": "article"
  },
  {
    "title": "Image Captioning with Affective Guiding and Selective Attention",
    "doi": "https://doi.org/10.1145/3226037",
    "publication_date": "2018-07-24",
    "publication_year": 2018,
    "authors": "Anqi Wang; Haifeng Hu; Liang Yang",
    "corresponding_authors": "",
    "abstract": "Image captioning is an increasingly important problem associated with artificial intelligence, computer vision, and natural language processing. Recent works revealed that it is possible for a machine to generate meaningful and accurate sentences for images. However, most existing methods ignore latent emotional information in an image. In this article, we propose a novel image captioning model with Affective Guiding and Selective Attention Mechanism named AG-SAM. In our method, we aim to bridge the affective gap between image captioning and the emotional response elicited by the image. First, we introduce affective components that capture higher-level concepts encoded in images into AG-SAM. Hence, our language model can be adapted to generate sentences that are more passionate and emotive. In addition, a selective gate acting on the attention mechanism controls the degree of how much visual information AG-SAM needs. Experimental results have shown that our model outperforms most existing methods, clearly reflecting an association between images and emotional components that is usually ignored in existing works.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2883112830",
    "type": "article"
  },
  {
    "title": "Thinking Like a Director",
    "doi": "https://doi.org/10.1145/3241057",
    "publication_date": "2018-10-23",
    "publication_year": 2018,
    "authors": "Hui-Yin Wu; Francesca Palù; Roberto Ranon; Marc Christie",
    "corresponding_authors": "",
    "abstract": "This article introduces Film Editing Patterns (FEP) , a language to formalize film editing practices and stylistic choices found in movies. FEP constructs are constraints, expressed over one or more shots from a movie sequence, that characterize changes in cinematographic visual properties, such as shot sizes, camera angles, or layout of actors on the screen. We present the vocabulary of the FEP language, introduce its usage in analyzing styles from annotated film data, and describe how it can support users in the creative design of film sequences in 3D. More specifically, (i) we define the FEP language, (ii) we present an application to craft filmic sequences from 3D animated scenes that uses FEPs as a high level mean to select cameras and perform cuts between cameras that follow best practices in cinema, and (iii) we evaluate the benefits of FEPs by performing user experiments in which professional filmmakers and amateurs had to create cinematographic sequences. The evaluation suggests that users generally appreciate the idea of FEPs, and that it can effectively help novice and medium experienced users in crafting film sequences with little training.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2897493023",
    "type": "article"
  },
  {
    "title": "Emotion Recognition During Speech Using Dynamics of Multiple Regions of the Face",
    "doi": "https://doi.org/10.1145/2808204",
    "publication_date": "2015-10-21",
    "publication_year": 2015,
    "authors": "Yelin Kim; Emily Mower Provost",
    "corresponding_authors": "",
    "abstract": "The need for human-centered, affective multimedia interfaces has motivated research in automatic emotion recognition. In this article, we focus on facial emotion recognition. Specifically, we target a domain in which speakers produce emotional facial expressions while speaking. The main challenge of this domain is the presence of modulations due to both emotion and speech. For example, an individual's mouth movement may be similar when he smiles and when he pronounces the phoneme /IY/, as in “cheese”. The result of this confusion is a decrease in performance of facial emotion recognition systems. In our previous work, we investigated the joint effects of emotion and speech on facial movement. We found that it is critical to employ proper temporal segmentation and to leverage knowledge of spoken content to improve classification performance. In the current work, we investigate the temporal characteristics of specific regions of the face, such as the forehead, eyebrow, cheek, and mouth. We present methodology that uses the temporal patterns of specific regions of the face in the context of a facial emotion recognition system. We test our proposed approaches on two emotion datasets, the IEMOCAP and SAVEE datasets. Our results demonstrate that the combination of emotion recognition systems based on different facial regions improves overall accuracy compared to systems that do not leverage different characteristics of individual regions.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W1994348490",
    "type": "article"
  },
  {
    "title": "Progressive Visual Cryptography with Unexpanded Meaningful Shares",
    "doi": "https://doi.org/10.1145/2935618",
    "publication_date": "2016-08-24",
    "publication_year": 2016,
    "authors": "Shivendra Shivani; Suneeta Agarwal",
    "corresponding_authors": "",
    "abstract": "The traditional k -out-of- n Visual Cryptography (VC) scheme is the conception of “all or nothing” for n participants to share a secret image. The original secret image can be visually revealed only when a subset of k or more shares are superimposed together, but if the number of stacked shares are less than k , nothing will be revealed. On the other hand, a Progressive Visual Cryptography (PVC) scheme differs from the traditional VC with respect to decoding. In PVC, clarity and contrast of the decoded secret image will be increased progressively with the number of stacked shares. Much of the existing state-of-the-art research on PVC has problems with pixel expansion and random pattern of the shares. In this article, a novel scheme of progressive visual cryptography with four or more number of unexpanded as well as meaningful shares has been proposed. For this, a novel and efficient Candidate Block Replacement preprocessing approach and a basis matrix creation algorithm have also been introduced. The proposed method also eliminates many unnecessary encryption constraints like a predefined codebook for encoding and decoding the secret image, restriction on the number of participants, and so on. From the experiments, it is observed that the reconstruction probability of black pixels in the decoded image corresponding to the black pixel in the secret image is always 1, whereas that of white pixels is 0.5 irrespective of the meaningful contents visible in the shares, thus ensuring the value of contrast to alwasys be 50%. Therefore, a reconstructed image can be easily identified by a human visual system without any computation.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2512984096",
    "type": "article"
  },
  {
    "title": "A Network-Based Virtual Reality Simulation Training Approach for Orthopedic Surgery",
    "doi": "https://doi.org/10.1145/3232678",
    "publication_date": "2018-08-22",
    "publication_year": 2018,
    "authors": "J. Cecil; Avinash Gupta; Miguel Pirela-Cruz; Parmesh Ramanathan",
    "corresponding_authors": "",
    "abstract": "The focus of this article is on the adoption of immersive and haptic simulators for training of medical residents in a surgical process called Less Invasive Stabilization System (LISS) plating surgery . LISS surgery is an orthopedic surgical procedure to treat fractures of the femur bone. Development of such simulators is a complex task which involves multiple systems, technologies, and human experts. Emerging Next Generation Internet technologies were used to develop the standalone on-line haptic-based simulator accessible to the students 24/7. A standalone immersive surgical simulator was also developed using HTC Vive. Expert surgeons played an important role in developing the simulator system; use cases of the target surgical processes were built using a modeling language called the engineering Enterprise Modeling Language (eEML) . A detailed study presenting the comparison between the haptic-based simulator and the immersive simulator has been also presented. The outcomes of this study underscore the potential of using such simulators in surgical training.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2888468864",
    "type": "article"
  },
  {
    "title": "Cross-Modality Retrieval by Joint Correlation Learning",
    "doi": "https://doi.org/10.1145/3314577",
    "publication_date": "2019-04-30",
    "publication_year": 2019,
    "authors": "Shuo Wang; Dan Guo; Xin Xu; Zhuo Li; Meng Wang",
    "corresponding_authors": "",
    "abstract": "As an indispensable process of cross-media analyzing, comprehending heterogeneous data faces challenges in the fields of visual question answering (VQA), visual captioning, and cross-modality retrieval. Bridging the semantic gap between the two modalities is still difficult. In this article, to address the problem in cross-modality retrieval, we propose a cross-modal learning model with joint correlative calculation learning. First, an auto-encoder is used to embed the visual features by minimizing the error of feature reconstruction and a multi-layer perceptron (MLP) is utilized to model the textual features embedding. Then we design a joint loss function to optimize both the intra- and the inter-correlations among the image-sentence pairs, i.e., the reconstruction loss of visual features, the relevant similarity loss of paired samples, and the triplet relation loss between positive and negative examples. In the proposed method, we optimize the joint loss based on a batch score matrix and utilize all mutual mismatched paired samples to enhance its performance. Our experiments in the retrieval tasks demonstrate the effectiveness of the proposed method. It achieves comparable performance to the state-of-the-art on three benchmarks, i.e., Flickr8k, Flickr30k, and MS-COCO.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2962529143",
    "type": "article"
  },
  {
    "title": "Deep Learning at Scale and at Ease",
    "doi": "https://doi.org/10.1145/2996464",
    "publication_date": "2016-11-02",
    "publication_year": 2016,
    "authors": "Wei Wang; Gang Chen; Haibo Chen; Tien Tuan Anh Dinh; Jinyang Gao; Beng Chin Ooi; Kian‐Lee Tan; Sheng Wang; Meihui Zhang",
    "corresponding_authors": "",
    "abstract": "Recently, deep learning techniques have enjoyed success in various multimedia applications, such as image classification and multimodal data analysis. Large deep learning models are developed for learning rich representations of complex data. There are two challenges to overcome before deep learning can be widely adopted in multimedia and other applications. One is usability, namely the implementation of different models and training algorithms must be done by nonexperts without much effort, especially when the model is large and complex. The other is scalability, namely the deep learning system must be able to provision for a huge demand of computing resources for training large models with massive datasets. To address these two challenges, in this article we design a distributed deep learning platform called SINGA , which has an intuitive programming model based on the common layer abstraction of deep learning models. Good scalability is achieved through flexible distributed training architecture and specific optimization techniques. SINGA runs on both GPUs and CPUs, and we show that it outperforms many other state-of-the-art deep learning systems. Our experience with developing and training deep learning models for real-life multimedia applications in SINGA shows that the platform is both usable and scalable.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2274162699",
    "type": "article"
  },
  {
    "title": "HGAN",
    "doi": "https://doi.org/10.1145/3344684",
    "publication_date": "2019-11-30",
    "publication_year": 2019,
    "authors": "Weizhi Nie; Weijie Wang; An-An Liu; Jie Nie; Yuting Su",
    "corresponding_authors": "",
    "abstract": "In this article, we propose a novel method to address the two-dimensional (2D) image-based 3D object retrieval problem. First, we extract a set of virtual views to represent each 3D object. Then, a soft-attention model is utilized to find the weight of each view to select one characteristic view for each 3D object. Second, we propose a novel Holistic Generative Adversarial Network (HGAN) to solve the cross-domain feature representation problem and make the feature space of virtual characteristic view more inclined to the feature space of the real picture. This will effectively mitigate the distribution discrepancies across the 2D image domains and 3D object domains. Finally, we utilize the generative model of the HGAN to obtain the “virtual real image” of each 3D object and make the characteristic view of the 3D object and real picture possess the same feature space for retrieval. To demonstrate the performance of our approach, We established a new dataset that includes pairs of 2D images and 3D objects, where the 3D objects are based on the ModelNet40 dataset. The experimental results demonstrate the superiority of our proposed method over the state-of-the-art methods.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2996049010",
    "type": "article"
  },
  {
    "title": "A Hierarchical CNN-RNN Approach for Visual Emotion Classification",
    "doi": "https://doi.org/10.1145/3359753",
    "publication_date": "2019-11-30",
    "publication_year": 2019,
    "authors": "Liang Li; Xinge Zhu; Yiming Hao; Shuhui Wang; Xingyu Gao; Qingming Huang",
    "corresponding_authors": "",
    "abstract": "Visual emotion classification is predicting emotional reactions of people for the given visual content. Psychological studies show that human emotions are affected by various visual stimuli from low level to high level, including contrast, color, texture, scene, object, and association, among others. Traditional approaches regarded different levels of stimuli as independent components and ignored to effectively fuse different stimuli. This article proposes a hierarchical convolutional neural network (CNN)-recurrent neural network (RNN) approach to predict the emotion based on the fused stimuli by exploiting the dependency among different-level features. First, we introduce a dual CNN to extract different levels of visual stimulus, where two related loss functions are designed to learn the stimuli representation under a multi-task learning structure. Further, to model the dependency between the low- and high-level stimulus, a stacked bi-directional RNN is proposed to fuse the preceding learned features from the dual CNN. Comparison experiments on one large-scale and three small scale datasets show that the proposed approach brings significant improvement. Ablation experiments demonstrate the effectiveness of different modules from our model.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2997383919",
    "type": "article"
  },
  {
    "title": "Frame-level Bit Allocation Optimization Based on&lt;?brk?&gt; Video Content Characteristics for HEVC",
    "doi": "https://doi.org/10.1145/3380827",
    "publication_date": "2020-02-29",
    "publication_year": 2020,
    "authors": "Zhaoqing Pan; Xiaokai Yi; Yun Zhang; Hui Yuan; Fu Lee Wang; Sam Kwong",
    "corresponding_authors": "",
    "abstract": "Rate control plays an important role in high efficiency video coding (HEVC), and bit allocation is the foundation of rate control. The video content characteristics are significant for bit allocation, and modeling an accurate relationship between video content characteristics and bit allocation is essential for bit allocation optimization. Therefore, in this article, a video content characteristics–based frame-level optimal bit allocation algorithm is proposed for improving the rate distortion (RD) performance of HEVC. First, the number of search points of motion estimation is used to evaluate the motion activity of video content, and the relationship between the search points and bit allocation is modeled as the search-points model. Second, the grey level co-occurrence matrix and temporal perceptual information are used to evaluate the spatial and temporal texture complexity, and the relationship between the video content texture complexity and bit allocation is modeled as the texture-complexity model. Then, the search-points model and texture-complexity model are jointly employed to allocate the coding bits for the second and third layers of the HEVC hierarchical coding structure. Finally, the remaining coding bits of a group-of-pictures (GOP) are allocated to the first layer of HEVC coding structure. To evaluate the performance of the proposed algorithm, the RD performance and bitrate accuracy are used as evaluation criteria, and the experimental results show that when compared with the popularly used R-λ model–based bit allocation algorithm, the proposed algorithm achieves an average of -3.43% BDBR reduction and 0.13 dB BDPSNR gains with only 0.02% loss of bitrate accuracy.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W3033971658",
    "type": "article"
  },
  {
    "title": "A Novel ( <i>t</i> , <i>s</i> , <i>k</i> , <i>n</i> )-Threshold Visual Secret Sharing Scheme Based on Access Structure Partition",
    "doi": "https://doi.org/10.1145/3418212",
    "publication_date": "2020-11-30",
    "publication_year": 2020,
    "authors": "Zuquan Liu; Guopu Zhu; Yuan‐Gen Wang; Jianquan Yang; Sam Kwong",
    "corresponding_authors": "",
    "abstract": "Visual secret sharing (VSS) is a new technique for sharing a binary image into multiple shadows. For VSS, the original image can be reconstructed from the shadows in any qualified set, but cannot be reconstructed from those in any forbidden set. In most traditional VSS schemes, the shadows held by participants have the same importance. However, in practice, a certain number of shadows are given a higher importance due to the privileges of their owners. In this article, a novel ( t , s , k , n )-threshold VSS scheme is proposed based on access structure partition. First, we construct the basis matrix of the proposed ( t , s , k , n )-threshold VSS scheme by utilizing a new access structure partition method and sub-access structure merging method. Then, the secret image is shared by the basis matrix as n shadows, which are divided into s essential shadows and n - s non-essential shadows. To reconstruct the secret image, k or more shadows should be collected, which include at least t essential shadows; otherwise, no information about the secret image can be obtained. Compared with related schemes, our scheme achieves a smaller shadow size and a higher visual quality of the reconstructed image. Theoretical analysis and experiments indicate the effectiveness of the proposed scheme.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W3111746241",
    "type": "article"
  },
  {
    "title": "Multi-human Parsing with a Graph-based Generative Adversarial Model",
    "doi": "https://doi.org/10.1145/3418217",
    "publication_date": "2021-02-28",
    "publication_year": 2021,
    "authors": "Jianshu Li; Jian Zhao; Congyan Lang; Yidong Li; Yunchao Wei; Guodong Guo; Terence Sim; Shuicheng Yan; Jiashi Feng",
    "corresponding_authors": "",
    "abstract": "Human parsing is an important task in human-centric image understanding in computer vision and multimedia systems. However, most existing works on human parsing mainly tackle the single-person scenario, which deviates from real-world applications where multiple persons are present simultaneously with interaction and occlusion. To address such a challenging multi-human parsing problem, we introduce a novel multi-human parsing model named MH-Parser, which uses a graph-based generative adversarial model to address the challenges of close-person interaction and occlusion in multi-human parsing. To validate the effectiveness of the new model, we collect a new dataset named Multi-Human Parsing (MHP), which contains multiple persons with intensive person interaction and entanglement. Experiments on the new MHP dataset and existing datasets demonstrate that the proposed method is effective in addressing the multi-human parsing problem compared with existing solutions in the literature.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W3154503495",
    "type": "article"
  },
  {
    "title": "Medical Image Classification based on an Adaptive Size Deep Learning Model",
    "doi": "https://doi.org/10.1145/3465220",
    "publication_date": "2021-10-26",
    "publication_year": 2021,
    "authors": "Xiangbin Liu; Jiesheng He; Liping Song; Shuai Liu; Gautam Srivastava",
    "corresponding_authors": "",
    "abstract": "With the rapid development of Artificial Intelligence (AI), deep learning has increasingly become a research hotspot in various fields, such as medical image classification. Traditional deep learning models use Bilinear Interpolation when processing classification tasks of multi-size medical image dataset, which will cause the loss of information of the image, and then affect the classification effect. In response to this problem, this work proposes a solution for an adaptive size deep learning model. First, according to the characteristics of the multi-size medical image dataset, the optimal size set module is proposed in combination with the unpooling process. Next, an adaptive deep learning model module is proposed based on the existing deep learning model. Then, the model is fused with the size fine-tuning module used to process multi-size medical images to obtain a solution of the adaptive size deep learning model. Finally, the proposed solution model is applied to the pneumonia CT medical image dataset. Through experiments, it can be seen that the model has strong robustness, and the classification effect is improved by about 4% compared with traditional algorithms.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W3209855943",
    "type": "article"
  },
  {
    "title": "Robust Secret Image Sharing Resistant to Noise in Shares",
    "doi": "https://doi.org/10.1145/3419750",
    "publication_date": "2021-02-28",
    "publication_year": 2021,
    "authors": "Xuehu Yan; Lintao Liu; Longlong Li; Yuliang Lu",
    "corresponding_authors": "",
    "abstract": "A secret image is split into <?TeX $n$?> shares in the generation phase of secret image sharing (SIS) for a <?TeX $(k,n)$?> threshold. In the recovery phase, the secret image is recovered when any <?TeX $k$?> or more shares are collected, and each collected share is generally assumed to be lossless in conventional SIS during storage and transmission. However, noise will arise during real-world storage and transmission; thus, shares will experience data loss, which will also lead to data loss in the secret image being recovered. Secret image recovery in the case of lossy shares is an important issue that must be addressed in practice, which is the overall subject of this article. An SIS scheme that can recover the secret image from lossy shares is proposed in this article. First, robust SIS and its definition are introduced. Next, a robust SIS scheme for a <?TeX $(k,n)$?> threshold without pixel expansion is proposed based on the Chinese remainder theorem (CRT) and error-correcting codes (ECC). By screening the random numbers, the share generation phase of the proposed robust SIS is designed to implement the error correction capability without increasing the share size. Particularly in the case of collecting noisy shares, our recovery method is to some degree robust to some noise types, such as least significant bit (LSB) noise, JPEG compression, and salt-and-pepper noise. A theoretical proof is presented, and experimental results are examined to evaluate the effectiveness of our proposed method.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W3155924340",
    "type": "article"
  },
  {
    "title": "Pinball Loss Twin Support Vector Clustering",
    "doi": "https://doi.org/10.1145/3409264",
    "publication_date": "2021-06-21",
    "publication_year": 2021,
    "authors": "M. Tanveer; Tarun Gupta; Miten Shah",
    "corresponding_authors": "",
    "abstract": "Twin Support Vector Clustering (TWSVC) is a clustering algorithm inspired by the principles of Twin Support Vector Machine (TWSVM). TWSVC has already outperformed other traditional plane based clustering algorithms. However, TWSVC uses hinge loss, which maximizes shortest distance between clusters and hence suffers from noise-sensitivity and low re-sampling stability. In this article, we propose Pinball loss Twin Support Vector Clustering (pinTSVC) as a clustering algorithm. The proposed pinTSVC model incorporates the pinball loss function in the plane clustering formulation. Pinball loss function introduces favorable properties such as noise-insensitivity and re-sampling stability. The time complexity of the proposed pinTSVC remains equivalent to that of TWSVC. Extensive numerical experiments on noise-corrupted benchmark UCI and artificial datasets have been provided. Results of the proposed pinTSVC model are compared with TWSVC, Twin Bounded Support Vector Clustering (TBSVC) and Fuzzy c-means clustering (FCM). Detailed and exhaustive comparisons demonstrate the better performance and generalization of the proposed pinTSVC for noise-corrupted datasets. Further experiments and analysis on the performance of the above-mentioned clustering algorithms on structural MRI (sMRI) images taken from the ADNI database, face clustering, and facial expression clustering have been done to demonstrate the effectiveness and feasibility of the proposed pinTSVC model.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W3175244446",
    "type": "article"
  },
  {
    "title": "Learning to Fool the Speaker Recognition",
    "doi": "https://doi.org/10.1145/3468673",
    "publication_date": "2021-10-31",
    "publication_year": 2021,
    "authors": "Jiguo Li; Xinfeng Zhang; Jizheng Xu; Siwei Ma; Wen Gao",
    "corresponding_authors": "",
    "abstract": "Due to the widespread deployment of fingerprint/face/speaker recognition systems, the risk in these systems, especially the adversarial attack, has drawn increasing attention in recent years. Previous researches mainly studied the adversarial attack to the vision-based systems, such as fingerprint and face recognition. While the attack for speech-based systems has not been well studied yet, although it has been widely used in our daily life. In this article, we attempt to fool the state-of-the-art speaker recognition model and present speaker recognition attacker , a lightweight multi-layer convolutional neural network to fool the well-trained state-of-the-art speaker recognition model by adding imperceptible perturbations onto the raw speech waveform. We find that the speaker recognition system is vulnerable to the adversarial attack, and achieve a high success rate on both the non-targeted attack and targeted attack. Besides, we present an effective method by leveraging a pretrained phoneme recognition model to optimize the speaker recognition attacker to obtain a tradeoff between the attack success rate and the perceptual quality. Experimental results on the TIMIT and LibriSpeech datasets demonstrate the effectiveness and efficiency of our proposed model. And the experiments for frequency analysis indicate that high-frequency attack is more effective than low-frequency attack, which is different from the conclusion drawn in previous image-based works. Additionally, the ablation study gives more insights into our model.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W3213643395",
    "type": "article"
  },
  {
    "title": "A Convolutional Neural Network Model Using Weighted Loss Function to Detect Diabetic Retinopathy",
    "doi": "https://doi.org/10.1145/3470976",
    "publication_date": "2022-01-25",
    "publication_year": 2022,
    "authors": "Mehedi Masud; Mohammed F. Alhamid; Yin Zhang⋆",
    "corresponding_authors": "",
    "abstract": "Nowadays, artificial intelligence (AI) provides tremendous prospects for driving future healthcare while empowering patients and service providers. The extensive use of digital healthcare produces a massive amount of multimedia healthcare data continuously (e.g., MRI, X-Ray, ultrasound images, etc.). Hence, it needs special data analytics techniques to provide a smart diagnosis to the patients. Recent advancements in artificial intelligence and machine learning techniques, particularly Deep learning (DL) methods, have demonstrated tremendous medical diagnosis progress and achievements. Diabetic Retinopathy (DR), cataract, macular degeneration, and glaucoma are the most common eye problems due to diabetes. Numerous models have been proposed using deep learning models to diagnose diabetic retinopathy, but no model is perfect for detecting DR diseases. This article presents a deep learning model to analyze diabetic retinopathy images to classify DR patients’ severity levels. The model applies a custom-weighted loss function in the model’s training and achieves 92.49% accuracy and a 0.945 Cohen Kappa score on test data. The model’s weighted average precision was 93%, recall 92%, and f1 score 93%. The model is compared with several state-of-the-art pre-trained models. We observe that the proposed model performs better in accuracy results and Cohen Kappa score.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W4206936205",
    "type": "article"
  },
  {
    "title": "JoT-GAN: A Framework for Jointly Training GAN and Person Re-Identification Model",
    "doi": "https://doi.org/10.1145/3491225",
    "publication_date": "2022-01-25",
    "publication_year": 2022,
    "authors": "Zhongwei Zhao; Ran Song; Qian Zhang; Peng Duan; Youmei Zhang",
    "corresponding_authors": "",
    "abstract": "To cope with the problem caused by inadequate training data, many person re-identification (re-id) methods exploit generative adversarial networks (GAN) for data augmentation, where the training of GAN is typically independent of that of the re-id model. The coupling relation between them that probably brings in a performance gain of re-id is thus ignored. In this work, we propose a general framework, namely JoT-GAN, to jointly train GAN and the re-id model. It can simultaneously achieve the optima of both the generator and the re-id model, where the training is guided by each other through a discriminator. The re-id model is boosted for two reasons: (1) the adversarial training encourages it to fool the discriminator, and (2) the generated samples augment the training data. Extensive results on benchmark datasets show that for the re-id model trained with the identification loss as well as the triplet loss, the proposed joint training framework outperforms existing methods with separate training and achieves state-of-the-art re-id performance.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W4207058911",
    "type": "article"
  },
  {
    "title": "Structure-aware Meta-fusion for Image Super-resolution",
    "doi": "https://doi.org/10.1145/3477553",
    "publication_date": "2022-02-16",
    "publication_year": 2022,
    "authors": "Haoyu Ma; Bingchen Gong; Yizhou Yu",
    "corresponding_authors": "",
    "abstract": "There are two main categories of image super-resolution algorithms: distortion oriented and perception oriented. Recent evidence shows that reconstruction accuracy and perceptual quality are typically in disagreement with each other. In this article, we present a new image super-resolution framework that is capable of striking a balance between distortion and perception. The core of our framework is a deep fusion network capable of generating a final high-resolution image by fusing a pair of deterministic and stochastic images using spatially varying weights. To make a single fusion model produce images with varying degrees of stochasticity, we further incorporate meta-learning into our fusion network. Once equipped with the kernel produced by a kernel prediction module, our meta fusion network is able to produce final images at any desired level of stochasticity. Experimental results indicate that our meta fusion network outperforms existing state-of-the-art SISR algorithms on widely used datasets, including PIRM-val, DIV2K-val, Set5, Set14, Urban100, Manga109, and B100. In addition, it is capable of producing high-resolution images that achieve low distortion and high perceptual quality simultaneously.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W4212780023",
    "type": "article"
  },
  {
    "title": "Rank-in-Rank Loss for Person Re-identification",
    "doi": "https://doi.org/10.1145/3532866",
    "publication_date": "2022-04-30",
    "publication_year": 2022,
    "authors": "Xin Xu; Xin Yuan; Zheng Wang; Kai Zhang; Ruimin Hu",
    "corresponding_authors": "",
    "abstract": "Person re-identification (re-ID) is commonly investigated as a ranking problem. However, the performance of existing re-ID models drops dramatically, when they encounter extreme positive-negative class imbalance (e.g., very small ratio of positive and negative samples) during training. To alleviate this problem, this article designs a rank-in-rank loss to optimize the distribution of feature embeddings. Specifically, we propose a Differentiable Retrieval-Sort Loss (DRSL) to optimize the re-ID model by ranking each positive sample ahead of the negative samples according to the distance and sorting the positive samples according to the angle (e.g., similarity score). The key idea of the proposed DRSL lies in minimizing the distance between samples of the same category along with the angle between them. Considering that the ranking and sorting operations are non-differentiable and non-convex, the DRSL also performs the optimization of automatic derivation and backpropagation. In addition, the analysis of the proposed DRSL is provided to illustrate that the DRSL not only maintains the inter-class distance distribution but also preserves the intra-class similarity structure in terms of angle constraints. Extensive experimental results indicate that the proposed DRSL can improve the performance of the state-of-the-art re-ID models, thus demonstrating its effectiveness and superiority in the re-ID task.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W4225149712",
    "type": "article"
  },
  {
    "title": "AMSA: Adaptive Multimodal Learning for Sentiment Analysis",
    "doi": "https://doi.org/10.1145/3572915",
    "publication_date": "2022-12-01",
    "publication_year": 2022,
    "authors": "Jingyao Wang; Luntian Mou; Лей Ма; Tiejun Huang; Wen Gao",
    "corresponding_authors": "",
    "abstract": "Efficient recognition of emotions has attracted extensive research interest, which makes new applications in many fields possible, such as human-computer interaction, disease diagnosis, service robots, and so forth. Although existing work on sentiment analysis relying on sensors or unimodal methods performs well for simple contexts like business recommendation and facial expression recognition, it does far below expectations for complex scenes, such as sarcasm, disdain, and metaphors. In this article, we propose a novel two-stage multimodal learning framework, called AMSA, to adaptively learn correlation and complementarity between modalities for dynamic fusion, achieving more stable and precise sentiment analysis results. Specifically, a multiscale attention model with a slice positioning scheme is proposed to get stable quintuplets of sentiment in images, texts, and speeches in the first stage. Then a Transformer-based self-adaptive network is proposed to assign weights flexibly for multimodal fusion in the second stage and update the parameters of the loss function through compensation iteration. To quickly locate key areas for efficient affective computing, a patch-based selection scheme is proposed to iteratively remove redundant information through a novel loss function before fusion. Extensive experiments have been conducted on both machine weakly labeled and manually annotated datasets of self-made Video-SA, CMU-MOSEI, and CMU-MOSI. The results demonstrate the superiority of our approach through comparison with baselines.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W4311057028",
    "type": "article"
  },
  {
    "title": "Rectified Meta-learning from Noisy Labels for Robust Image-based Plant Disease Classification",
    "doi": "https://doi.org/10.1145/3472809",
    "publication_date": "2022-01-25",
    "publication_year": 2022,
    "authors": "Deming Zhai; Ruifeng Shi; Junjun Jiang; Xianming Liu",
    "corresponding_authors": "",
    "abstract": "Plant diseases serve as one of main threats to food security and crop production. It is thus valuable to exploit recent advances of artificial intelligence to assist plant disease diagnosis. One popular approach is to transform this problem as a leaf image classification task, which can be then addressed by the powerful convolutional neural networks (CNNs). However, the performance of CNN-based classification approach depends on a large amount of high-quality manually labeled training data, which inevitably introduce noise on labels in practice, leading to model overfitting and performance degradation. To overcome this problem, we propose a novel framework that incorporates rectified meta-learning module into common CNN paradigm to train a noise-robust deep network without using extra supervision information. The proposed method enjoys the following merits: (i) A rectified meta-learning is designed to pay more attention to unbiased samples, leading to accelerated convergence and improved classification accuracy. (ii) Our method is free on assumption of label noise distribution, which works well on various kinds of noise. (iii) Our method serves as a plug-and-play module, which can be embedded into any deep models optimized by gradient descent-based method. Extensive experiments are conducted to demonstrate the superior performance of our algorithm over the state-of-the-arts.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4207007505",
    "type": "article"
  },
  {
    "title": "Uni-EDEN: Universal Encoder-Decoder Network by Multi-Granular Vision-Language Pre-training",
    "doi": "https://doi.org/10.1145/3473140",
    "publication_date": "2022-02-16",
    "publication_year": 2022,
    "authors": "Yehao Li; Jiahao Fan; Yingwei Pan; Ting Yao; Weiyao Lin; Tao Mei",
    "corresponding_authors": "",
    "abstract": "Vision-language pre-training has been an emerging and fast-developing research topic, which transfers multi-modal knowledge from rich-resource pre-training task to limited-resource downstream tasks. Unlike existing works that predominantly learn a single generic encoder, we present a pre-trainable Universal Encoder-DEcoder Network (Uni-EDEN) to facilitate both vision-language perception (e.g., visual question answering) and generation (e.g., image captioning). Uni-EDEN is a two-stream Transformer-based structure, consisting of three modules: object and sentence encoders that separately learns the representations of each modality and sentence decoder that enables both multi-modal reasoning and sentence generation via inter-modal interaction. Considering that the linguistic representations of each image can span different granularities in this hierarchy including, from simple to comprehensive, individual label, a phrase, and a natural sentence, we pre-train Uni-EDEN through multi-granular vision-language proxy tasks: Masked Object Classification, Masked Region Phrase Generation, Image-Sentence Matching, and Masked Sentence Generation. In this way, Uni-EDEN is endowed with the power of both multi-modal representation extraction and language modeling. Extensive experiments demonstrate the compelling generalizability of Uni-EDEN by fine-tuning it to four vision-language perception and generation downstream tasks.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4212804468",
    "type": "article"
  },
  {
    "title": "Inner Knowledge-based Img2Doc Scheme for Visual Question Answering",
    "doi": "https://doi.org/10.1145/3489142",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Qun Li; Fu Xiao; Bir Bhanu; Biyun Sheng; Richang Hong",
    "corresponding_authors": "",
    "abstract": "Visual Question Answering (VQA) is a research topic of significant interest at the intersection of computer vision and natural language understanding. Recent research indicates that attributes and knowledge can effectively improve performance for both image captioning and VQA. In this article, an inner knowledge-based Img2Doc algorithm for VQA is presented. The inner knowledge is characterized as the inner attribute relationship in visual images. In addition to using an attribute network for inner knowledge-based image representation, VQA scheme is associated with a question-guided Doc2Vec method for question–answering. The attribute network generates inner knowledge-based features for visual images, while a novel question-guided Doc2Vec method aims at converting natural language text to vector features. After the vector features are extracted, they are combined with visual image features into a classifier to provide an answer. Based on our model, the VQA problem is resolved by textual question answering. The experimental results demonstrate that the proposed method achieves superior performance on multiple benchmark datasets.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4214824212",
    "type": "article"
  },
  {
    "title": "LCSNet: End-to-end Lipreading with Channel-aware Feature Selection",
    "doi": "https://doi.org/10.1145/3524620",
    "publication_date": "2022-03-17",
    "publication_year": 2022,
    "authors": "Feng Xue; Tian Yang; Kang Liu; Zikun Hong; Mingwei Cao; Dan Guo; Richang Hong",
    "corresponding_authors": "",
    "abstract": "Lipreading is a task of decoding the movement of the speaker’s lip region into text. In recent years, lipreading methods based on deep neural network have attracted widespread attention, and the accuracy has far surpassed that of experienced human lipreaders. The visual differences in some phonemes are extremely subtle and pose a great challenge to lipreading. Most of the lipreading existing methods do not process the extracted visual features, which mainly suffer from two problems. First, the extracted features contain lot of useless information such as noise caused by differences in speech speed and lip shape, for example. In addition, the extracted features are not abstract enough to distinguish phonemes with similar pronunciation. These problems have a bad effect on the performance of lipreading. To extract features from the lip regions that are more distinguishable and more relevant to the speech content, this article proposes an end-to-end deep neural network-based lipreading model (LCSNet). The proposed model extracts the short-term spatio-temporal features and the motion trajectory features from the lip region in the video clips. The extracted features are filtered by the channel attention module to eliminate the useless features and then used as input to the proposed Selective Feature Fusion Module (SFFM) to extract the high-level abstract features. Afterwards, these features are used as input to the bidirectional GRU network in time order for temporal modeling to obtain the long-term spatio-temporal features. Finally, a Connectionist Temporal Classification (CTC) decoder is used to generate the output text. The experimental results show that the proposed model achieves a 1.0% CER and 2.3% WER on the GRID corpus database, which, respectively, represents an improvement of 52% and 47% compared to LipNet.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4220663529",
    "type": "article"
  },
  {
    "title": "Clustering Matters: Sphere Feature for Fully Unsupervised Person Re-identification",
    "doi": "https://doi.org/10.1145/3501404",
    "publication_date": "2022-03-15",
    "publication_year": 2022,
    "authors": "Yi Zheng; Yong Zhou; Jiaqi Zhao; Ying Chen; Rui Yao; Bing Liu; Abdulmotaleb El Saddik",
    "corresponding_authors": "",
    "abstract": "In person re-identification (Re-ID) , the data annotation cost of supervised learning, is huge and it cannot adapt well to complex situations. Therefore, compared with supervised deep learning methods, unsupervised methods are more in line with actual needs. In unsupervised learning, a key to solving Re-ID is to find a standard that can effectively distinguish the difference (distance) between the features of images belonging to different pedestrian identities. However, there are some differences in the images captured by different cameras (such as brightness, angle, etc.). It is well known that the training of neural networks is mainly based on the distance between features, while in unsupervised learning, especially in unsupervised learning methods based on hierarchical clustering, the distance between features plays a more important role in the clustering phase. We improve the accuracy of a deep learning method based on hierarchical clustering under fully unsupervised conditions, starting from both feature and distance metrics. First, we propose to use spherical features, by normalizing the images in the feature space, to weaken the structural differences (length) between features, while saving the feature differences (direction) between different identities. Then, we use the sum of squared errors (SSE) as a regularization term to balance different cluster states. We evaluate our method on four large-scale Re-ID datasets, and experiments show that our method achieves better results than the state-of-the-art unsupervised methods.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4220913778",
    "type": "article"
  },
  {
    "title": "Deep Uncoupled Discrete Hashing via Similarity Matrix Decomposition",
    "doi": "https://doi.org/10.1145/3524021",
    "publication_date": "2022-03-12",
    "publication_year": 2022,
    "authors": "Dayan Wu; Qi Dai; Bo Li; Weiping Wang",
    "corresponding_authors": "",
    "abstract": "Hashing has been drawing increasing attention in the task of large-scale image retrieval owing to its storage and computation efficiency, especially the recent asymmetric deep hashing methods. These approaches treat the query and database in an asymmetric way and can take full advantage of the whole training data. Though it has achieved state-of-the-art performance, asymmetric deep hashing methods still suffer from the large quantization error and efficiency problem on large-scale datasets due to the tight coupling between the query and database. In this article, we propose a novel asymmetric hashing method, called D eep U ncoupled D iscrete H ashing (DUDH), for large-scale approximate nearest neighbor search. Instead of directly preserving the similarity between the query and database, DUDH first exploits a small similarity-transfer image set to transfer the underlying semantic structures from the database to the query and implicitly keep the desired similarity. As a result, the large similarity matrix is decomposed into two relatively small ones and the query is decoupled from the database. Then both database codes and similarity-transfer codes are directly learned during optimization. The quantization error of DUDH only exists in the process of preserving similarity between the query and similarity-transfer set. By uncoupling the query from the database, the training cost of optimizing the CNN model for the query is no longer related to the size of the database. Besides, to further accelerate the training process, we propose to optimize the similarity-transfer codes with a constant-approximation solution. In doing so, the training cost of optimizing similarity-transfer codes can be almost ignored. Extensive experiments on four widely used image retrieval benchmarks demonstrate that DUDH can achieve state-of-the-art retrieval performance with remarkable training cost reduction (30× - 50× relative).",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4221096774",
    "type": "article"
  },
  {
    "title": "SETTI: A <u>S</u> elf-supervised Adv <u>E</u> rsarial Malware De <u>T</u> ection Archi <u>T</u> ecture in an <u>I</u> oT Environment",
    "doi": "https://doi.org/10.1145/3536425",
    "publication_date": "2022-05-17",
    "publication_year": 2022,
    "authors": "Marjan Golmaryami; Rahim Taheri; Zahra Pooranian; Mohammad Shojafar; Pei Xiao",
    "corresponding_authors": "",
    "abstract": "In recent years, malware detection has become an active research topic in the area of Internet of Things (IoT) security. The principle is to exploit knowledge from large quantities of continuously generated malware. Existing algorithms practise available malware features for IoT devices and lack real-time prediction behaviours. More research is thus required on malware detection to cope with real-time misclassification of the input IoT data. Motivated by this, in this article, we propose an adversarial self-supervised architecture for detecting malware in IoT networks, SETTI, considering samples of IoT network traffic that may not be labeled. In the SETTI architecture, we design three self-supervised attack techniques, namely, Self-MDS , GSelf-MDS, and ASelf-MDS . The Self-MDS method considers the IoT input data and the adversarial sample generation in real-time. The GSelf-MDS builds a generative adversarial network model to generate adversarial samples in the self-supervised structure. Finally, ASelf-MDS utilises three well-known perturbation sample techniques to develop adversarial malware and inject it over the self-supervised architecture. Also, we apply a defence method to mitigate these attacks, namely, adversarial self-supervised training, to protect the malware detection architecture against injecting the malicious samples. To validate the attack and defence algorithms, we conduct experiments on two recent IoT datasets: IoT23 and NBIoT. Comparison of the results shows that in the IoT23 dataset, the Self-MDS method has the most damaging consequences from the attacker’s point of view by reducing the accuracy rate from 98% to 74%. In the NBIoT dataset, the ASelf-MDS method is the most devastating algorithm that can plunge the accuracy rate from 98% to 77%.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4224305170",
    "type": "article"
  },
  {
    "title": "HIFGAN: A High-Frequency Information-Based Generative Adversarial Network for Image Super-Resolution",
    "doi": "https://doi.org/10.1145/3578934",
    "publication_date": "2023-01-04",
    "publication_year": 2023,
    "authors": "Xin Yang; Hengrui Li; Xiaochuan Li; Tao Li",
    "corresponding_authors": "",
    "abstract": "Since the neural network was introduced into the super-resolution (SR) field, many SR deep models have been proposed and have achieved excellent results. However, there are two main drawbacks: one is that the methods based on the best peak-signal-to-noise ratio (PSNR) do not have enough comfortable visual quality; the other is that although the SR models based on generative adversarial network (GAN) have satisfactory visual quality, the structure of the reconstructed image has apparent defects. Therefore, according to the characteristics that human eyes are sensitive to high-frequency components in images, this article proposes an improved image SRGAN model based on high-frequency information fusion (HIFGAN). It builds a feature extraction network for high-frequency information fusion by designing a lightweight spatial attention module and improving the network architecture of enhanced super-resolution GAN (ESRGAN). It makes the generator in the GAN network have better feature recovery ability, reduces the dependence of the later training on the decider and loss function, and makes the generated image structure more consistent with the real situation. In addition, we build a high-frequency loss function to optimize the training of the generator network. Detailed experimental results show that HIFGAN performs excellently in both objective criterion evaluation and subjective visual effect. Compared with the state-of-the-art GAN-based SR networks, the reconstructed image by our model is more precise and complete in texture details.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4313591094",
    "type": "article"
  },
  {
    "title": "Self-Supervised Learning of Depth and Ego-Motion for 3D Perception in Human Computer Interaction",
    "doi": "https://doi.org/10.1145/3588571",
    "publication_date": "2023-03-23",
    "publication_year": 2023,
    "authors": "Shanbao Qiao; Naixue Xiong; Yongbin Gao; Zhijun Fang; Wenjun Yu; Juan Zhang; Xiaoyan Jiang",
    "corresponding_authors": "",
    "abstract": "3D perception of depth and ego-motion is of vital importance in intelligent agent and Human Computer Interaction (HCI) tasks, such as robotics and autonomous driving. There are different kinds of sensors that can directly obtain 3D depth information. However, the commonly used Lidar sensor is expensive, and the effective range of RGB-D cameras is limited. In the field of computer vision, researchers have done a lot of work on 3D perception. While traditional geometric algorithms require a lot of manual features for depth estimation, Deep Learning methods have achieved great success in this field. In this work, we proposed a novel self-supervised method based on Vision Transformer (ViT) with Convolutional Neural Network (CNN) architecture, which is referred to as ViT-Depth . The image reconstruction losses computed by the estimated depth and motion between adjacent frames are treated as supervision signal to establish a self-supervised learning pipeline. This is an effective solution for tasks that need accurate and low-cost 3D perception, such as autonomous driving, robotic navigation, 3D reconstruction, and so on. Our method could leverage both the ability of CNN and Transformer to extract deep features and capture global contextual information. In addition, we propose a cross-frame loss that could constrain photometric error and scale consistency among multi-frames, which lead the training process to be more stable and improve the performance. Extensive experimental results on autonomous driving dataset demonstrate the proposed approach is competitive with the state-of-the-art depth and motion estimation methods.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4360612554",
    "type": "article"
  },
  {
    "title": "Robust Hashing via Global and Local Invariant Features for Image Copy Detection",
    "doi": "https://doi.org/10.1145/3600234",
    "publication_date": "2023-05-27",
    "publication_year": 2023,
    "authors": "Xiaoping Liang; Zhenjun Tang; Zhixin Li; Mengzhu Yu; Hanyun Zhang; Xianquan Zhang",
    "corresponding_authors": "",
    "abstract": "Robust hashing is a powerful technique for processing large-scale images. Currently, many reported image hashing schemes do not perform well in balancing the performances of discrimination and robustness, and thus they cannot efficiently detect image copies, especially the image copies with multiple distortions. To address this, we exploit global and local invariant features to develop a novel robust hashing for image copy detection. A critical contribution is the global feature calculation by gray level co-occurrence moment learned from the saliency map determined by the phase spectrum of quaternion Fourier transform, which can significantly enhance discrimination without reducing robustness. Another essential contribution is the local invariant feature computation via Kernel Principal Component Analysis (KPCA) and vector distances. As KPCA can maintain the geometric relationships within image, the local invariant features learned with KPCA and vector distances can guarantee discrimination and compactness. Moreover, the global and local invariant features are encrypted to ensure security. Finally, the hash is produced via the ordinal measures of the encrypted features for making a short length of hash. Numerous experiments are conducted to show efficiency of our scheme. Compared with some well-known hashing schemes, our scheme demonstrates a preferable classification performance of discrimination and robustness. The experiments of detecting image copies with multiple distortions are tested and the results illustrate the effectiveness of our scheme.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4378533318",
    "type": "article"
  },
  {
    "title": "A Privacy-preserving Auction Mechanism for Learning Model as an NFT in Blockchain-driven Metaverse",
    "doi": "https://doi.org/10.1145/3599971",
    "publication_date": "2023-06-07",
    "publication_year": 2023,
    "authors": "Qinnan Zhang; Zehui Xiong; Jianming Zhu; Sheng Gao; Wanting Yang",
    "corresponding_authors": "",
    "abstract": "The Metaverse, envisioned as the next-generation Internet, will be constructed via twining a practical world in a virtual form, wherein Meterverse service providers (MSPs) are required to collect massive data from Meterverse users (MUs). In this regard, a critical demand exists for MSPs to motivate MUs to contribute computing resources and data while preserving user privacy. Federated learning (FL), as a privacy-preserving collaborative machine learning paradigm, can support distributed intensive computation in the Metaverse. In this work, we first investigate minting the machine learning models into NFT with FL assistance (referred to as FL-NFT), such that MUs as stakeholders can control the ownership and share the economic value of user-generated content (UGC). Specifically, MUs are encouraged to establish a decentralized autonomous organization (i.e., MU-DAO) to aggregate local models and mint FL-NFT. MUs and MSPs optimize the strategies by formulating an imperfect information Stackelberg game to trade off the cost and benefit. We apply the backward induction to derive the equilibrium solution. Then, we construct a privacy-preserving multi-winner sealed-bid auction mechanism (PMS-AM), in which the Hidden Markov Model assists MSPs in choosing rational bidding strategies according to historical bids, and the double auction mechanism determines the winners and price of FL-NFT. Finally, the numerical results based on theoretical analysis and simulations demonstrate that the proposed PMS-AM can increase the quality of FL-NFT and achieve the economic properties of incentive mechanisms such as individual rationality and incentive compatibility.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4379646818",
    "type": "article"
  },
  {
    "title": "Diverse Image Captioning via Conditional Variational Autoencoder and Dual Contrastive Learning",
    "doi": "https://doi.org/10.1145/3614435",
    "publication_date": "2023-08-11",
    "publication_year": 2023,
    "authors": "Jing Xu; Bing Liu; Yong Zhou; Mingming Liu; Rui Yao; Zhiwen Shao",
    "corresponding_authors": "",
    "abstract": "Diverse image captioning has achieved substantial progress in recent years. However, the discriminability of generative models and the limitation of cross entropy loss are generally overlooked in the traditional diverse image captioning models, which seriously hurts both the diversity and accuracy of image captioning. In this article, aiming to improve diversity and accuracy simultaneously, we propose a novel Conditional Variational Autoencoder (DCL-CVAE) framework for diverse image captioning by seamlessly integrating sequential variational autoencoder with contrastive learning. In the encoding stage, we first build conditional variational autoencoders to separately learn the sequential latent spaces for a pair of captions. Then, we introduce contrastive learning in the sequential latent spaces to enhance the discriminability of latent representations for both image-caption pairs and mismatched pairs. In the decoding stage, we leverage the captions sampled from the pre-trained Long Short-Term Memory (LSTM), LSTM decoder as the negative examples and perform contrastive learning with the greedily sampled positive examples, which can restrain the generation of common words and phrases induced by the cross entropy loss. By virtue of dual constrastive learning, DCL-CVAE is capable of encouraging the discriminability and facilitating the diversity, while promoting the accuracy of the generated captions. Extensive experiments are conducted on the challenging MSCOCO dataset, showing that our proposed methods can achieve a better balance between accuracy and diversity compared to the state-of-the-art diverse image captioning models.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4385762291",
    "type": "article"
  },
  {
    "title": "<b> P <sup>2</sup> ANet </b> : A Large-Scale Benchmark for Dense Action Detection from Table Tennis Match Broadcasting Videos",
    "doi": "https://doi.org/10.1145/3633516",
    "publication_date": "2023-11-28",
    "publication_year": 2023,
    "authors": "Jiang Bian; Xuhong Li; Tao Wang; Qingzhong Wang; Jun Huang; Chen Liu; Jun Zhao; Feixiang Lu; Dejing Dou; Haoyi Xiong",
    "corresponding_authors": "",
    "abstract": "While deep learning has been widely used for video analytics, such as video classification and action detection, dense action detection with fast-moving subjects from sports videos is still challenging. In this work, we release yet another sports video benchmark P 2 ANet for P ing P ong- A ction detection, which consists of 2,721 video clips collected from the broadcasting videos of professional table tennis matches in World Table Tennis Championships and Olympiads. We work with a crew of table tennis professionals and referees on a specially designed annotation toolbox to obtain fine-grained action labels (in 14 classes) for every ping-pong action that appeared in the dataset, and formulate two sets of action detection problems— action localization and action recognition . We evaluate a number of commonly-seen action recognition (e.g., TSM, TSN, Video SwinTransformer, and Slowfast) and action localization models (e.g., BSN, BSN++, BMN, TCANet), using P 2 ANet for both problems, under various settings. These models can only achieve 48% area under the AR-AN curve for localization and 82% top-one accuracy for recognition since the ping-pong actions are dense with fast-moving subjects but broadcasting videos are with only 25 FPS. The results confirm that P 2 ANet is still a challenging task and can be used as a special benchmark for dense action detection from videos. We invite readers to examine our dataset by visiting the following link: https://github.com/Fred1991/P2ANET.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4389089672",
    "type": "article"
  },
  {
    "title": "Exploring Visual Relationships via Transformer-based Graphs for Enhanced Image Captioning",
    "doi": "https://doi.org/10.1145/3638558",
    "publication_date": "2023-12-25",
    "publication_year": 2023,
    "authors": "Jingyu Li; Zhendong Mao; Hao Li; Weidong Chen; Yongdong Zhang",
    "corresponding_authors": "",
    "abstract": "Image captioning (IC), bringing vision to language, has drawn extensive attention. A crucial aspect of IC is the accurate depiction of visual relations among image objects. Visual relations encompass two primary facets: content relations and structural relations. Content relations, which comprise geometric positions content (i.e., distances and sizes) and semantic interactions content (i.e., actions and possessives), unveil the mutual correlations between objects. In contrast, structural relations pertain to the topological connectivity of object regions. Existing Transformer-based methods typically resort to geometric positions to enhance the visual relations, yet only using the shallow geometric content is unable to precisely cover actional content correlations and structural connection relations. In this article, we adopt a comprehensive perspective to examine the correlations between objects, incorporating both content relations (i.e., geometric and semantic relations) and structural relations, with the aim of generating plausible captions. To achieve this, first, we construct a geometric graph from bounding box features and a semantic graph from the scene graph parser to model the content relations. Innovatively, we construct a topology graph that amalgamates the sparsity characteristics of the geometric and semantic graphs, enabling the representation of image structural relations. Second, we propose a novel unified approach to enrich image relation representations by integrating semantic, geometric, and structural relations into self-attention. Finally, in the language decoding stage, we further leverage the semantic relation as prior knowledge to generate accurate words. Extensive experiments on MS-COCO dataset demonstrate the effectiveness of our model, with improvements of CIDEr from 128.6% to 136.6%. Codes have been released at https://github.com/CrossmodalGroup/ER-SAN/tree/main/VG-Cap .",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4390195594",
    "type": "article"
  },
  {
    "title": "Synthetic Data and Hierarchical Object Detection in Overhead Imagery",
    "doi": "https://doi.org/10.1145/3635309",
    "publication_date": "2024-01-11",
    "publication_year": 2024,
    "authors": "Nathan Clement; Alan Schoen; Arnold Boedihardjo; Andrew Jenkins",
    "corresponding_authors": "",
    "abstract": "The performance of neural network models is often limited by the availability of big datasets. To treat this problem, we survey and develop novel synthetic data generation and augmentation techniques for enhancing low/zero-sample learning in satellite imagery. In addition to extending synthetic data generation approaches, we propose a hierarchical detection architecture to improve the utility of synthetic training samples. We consider existing techniques for producing synthetic imagery–3D models and neural style transfer–as well as introducing our own adversarially trained reskinning network, the GAN-Reskinner, to blend 3D models. Additionally, we test the value of synthetic data in a two-stage, hierarchical detection/classification model of our own construction. To test the effectiveness of synthetic imagery, we employ it in the training of detection models and our two stage model, and evaluate the resulting models on real satellite images. All modalities of synthetic data are tested extensively on practical, geospatial analysis problems. Our experiments show that synthetic data developed using our approach can often enhance detection performance, particularly when combined with some real training images. When the only source of data is synthetic, our GAN-Reskinner often boosts performance over conventionally rendered 3D models and in all cases, the hierarchical model outperforms the baseline end-to-end detection architecture.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3128570648",
    "type": "article"
  },
  {
    "title": "IoT-enabled Biometric Security: Enhancing Smart Car Safety with Depth-based Head Pose Estimation",
    "doi": "https://doi.org/10.1145/3639367",
    "publication_date": "2024-01-02",
    "publication_year": 2024,
    "authors": "Carmen Bisogni; Lucia Cascone; Michele Nappi; Chiara Pero",
    "corresponding_authors": "",
    "abstract": "Advanced Driver Assistance Systems (ADAS) are experiencing higher levels of automation, facilitated by the synergy among various sensors integrated within vehicles, thereby forming an Internet of Things (IoT) framework. Among these sensors, cameras have emerged as valuable tools for detecting driver fatigue and distraction. This study introduces HYDE-F, a Head Pose Estimation (HPE) system exclusively utilizing depth cameras. HYDE-F adeptly identifies critical driver head poses associated with risky conditions, thus enhancing the safety of IoT-enabled ADAS. The core of HYDE-F’s innovation lies in its dual-process approach: it employs a fractal encoding technique and keypoint intensity analysis in parallel. These two processes are then fused using an optimization algorithm, enabling HYDE-F to blend the strengths of both methods for enhanced accuracy. Evaluations conducted on a specialized driving dataset, Pandora, demonstrate HYDE-F’s competitive performance compared to existing methods, surpassing current techniques in terms of average Mean Absolute Error (MAE) by nearly 1 ∘ . Moreover, case studies highlight the successful integration of HYDE-F with vehicle sensors. Additionally, HYDE-F exhibits robust generalization capabilities, as evidenced by experiments conducted on standard laboratory-based HPE datasets, i.e., Biwi and ICT-3DHP databases, achieving an average MAE of 4.9 ∘ and 5 ∘ , respectively.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4390490829",
    "type": "article"
  },
  {
    "title": "Privacy-preserving Multi-source Cross-domain Recommendation Based on Knowledge Graph",
    "doi": "https://doi.org/10.1145/3639706",
    "publication_date": "2024-01-05",
    "publication_year": 2024,
    "authors": "Jing Liu; L. Shang; Yuting Su; Weizhi Nie; Xin Wen; An-An Liu",
    "corresponding_authors": "",
    "abstract": "The cross-domain recommender systems aim to alleviate the data sparsity problem in the target domain by transferring knowledge from the auxiliary domain. However, existing works ignore the fact that the data sparsity problem may also exist in the single auxiliary domain, and sharing user behavior data is restricted by the privacy policy. In addition, their cross-domain models lack interpretability. To address these concerns, we propose a novel multi-source cross-domain model based on knowledge graph. Specifically, to avoid the insufficiency of single auxiliary domain, we construct a knowledge graph comprehensively leveraging items from multiple auxiliary domains. To avoid the leakage of user privacy when user information is transferred to multiple domains, we construct graph for information transfer between items to effectively avoid the propagation of users’ private information between different domains. We implicitly integrate the user–item interaction by transferring the learned item embeddings. To improve the interpretability of cross-domain knowledge transfer, we propose a knowledge graph-based retrieval and fusion method to transfer knowledge derived from multiple auxiliary domains. An attention-based fusion network is designed to enhance the representation of the targeted user and items with the transferred item embedding. We perform extensive experiments on three real-world datasets, demonstrating that our model outperforms the states of the art.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4390605524",
    "type": "article"
  },
  {
    "title": "Cross-Modal Attention Preservation with Self-Contrastive Learning for Composed Query-Based Image Retrieval",
    "doi": "https://doi.org/10.1145/3639469",
    "publication_date": "2024-01-09",
    "publication_year": 2024,
    "authors": "Shenshen Li; Xing Xu; Xun Jiang; Fumin Shen; Zhe Sun; Andrzej Cichocki",
    "corresponding_authors": "",
    "abstract": "In this article, we study the challenging cross-modal image retrieval task, Composed Query-Based Image Retrieval (CQBIR) , in which the query is not a single text query but a composed query, i.e., a reference image, and a modification text. Compared with the conventional cross-modal image-text retrieval task, the CQBIR is more challenging as it requires properly preserving and modifying the specific image region according to the multi-level semantic information learned from the multi-modal query. Most recent works focus on extracting preserved and modified information and compositing it into a unified representation. However, we observe that the preserved regions learned by the existing methods contain redundant modified information, inevitably degrading the overall retrieval performance. To this end, we propose a novel method termed C ross- M odal A ttention P reservation (CMAP) . Specifically, we first leverage the cross-level interaction to fully account for multi-granular semantic information, which aims to supplement the high-level semantics for effective image retrieval. Furthermore, different from conventional contrastive learning, our method introduces self-contrastive learning into learning preserved information, to prevent the model from confusing the attention for the preserved part with the modified part. Extensive experiments on three widely used CQBIR datasets, i.e., FashionIQ, Shoes, and Fashion200k, demonstrate that our proposed CMAP method significantly outperforms the current state-of-the-art methods on all the datasets. The anonymous implementation code of our CMAP method is available at https://github.com/CFM-MSG/Code_CMAP.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4390724303",
    "type": "article"
  },
  {
    "title": "Multiply Complementary Priors for Image Compressive Sensing Reconstruction in Impulsive Noise",
    "doi": "https://doi.org/10.1145/3643032",
    "publication_date": "2024-01-24",
    "publication_year": 2024,
    "authors": "Yunyi Li; Fu Xiao; Liang Wei; Linqing Gui",
    "corresponding_authors": "",
    "abstract": "Impulsive noise is always present in real-world image Compressive Sensing (CS) acquisition systems, where existing CS reconstruction performance may seriously deteriorate. In this article, we propose a robust CS formulation for image reconstruction to suppress outliers in the presence of impulsive noise. To address this issue, we consider a novel truncated-Cauchy loss function as the metric of residual error to elevate the reconstruction robustness. Specifically, we design a complementary priors model to incorporate nonconvex nonlocal low-rank prior and deep denoiser prior for high-accuracy image reconstruction. By means of the half-quadratic optimization theory and generalized soft-thresholding technique, we also develop an alternative optimization algorithm for solving the induced nonconvex optimization problem. Numerical simulations demonstrate the robustness and accuracy of the proposed robust CS method compared to some recent CS methods for image reconstruction in impulsive noise.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4391174937",
    "type": "article"
  },
  {
    "title": "Exploring the Facets of the Multiplayer VR Gaming Experience",
    "doi": "https://doi.org/10.1145/3649897",
    "publication_date": "2024-02-29",
    "publication_year": 2024,
    "authors": "Sara Vlahović; Ivan Slivar; Matko Silic; Lea Skorin‐Kapov; Mirko Sužnjević",
    "corresponding_authors": "",
    "abstract": "While the topic of investigating user experience with immersive services, such as Social Virtual Reality (VR), is starting to gain traction in the research community, the unique case of multiplayer VR games requires a more specific approach. Attempts to investigate user experiences with this complex, multidimensional service are hindered by the absence of specific standards and guidelines going beyond what we know about non-immersive gaming. In this article, we present the results of a user study (N = 32) exploring participants’ experience of playing two competitive VR games of different genres (shooter, sports game), as we focus on three distinct facets of multiplayer VR gaming—network, interpersonal competitiveness, and social interaction. Furthermore, approaching the issue from the perspective of Quality of Experience researchers looking to conduct future user studies on the topic, we also present our findings as a way to shed light on factors that need further consideration, especially in the context of participant recruitment and methodology design, such as the choice of access network to be used for testing, and the issue of matchmaking study participants based on skill and prior relationship.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4392295061",
    "type": "article"
  },
  {
    "title": "Inter-camera Identity Discrimination for Unsupervised Person Re-identification",
    "doi": "https://doi.org/10.1145/3652858",
    "publication_date": "2024-04-03",
    "publication_year": 2024,
    "authors": "Mingfu Xiong; Kaikang Hu; Zhihan Lyu; Fei Fang; Zhongyuan Wang; Ruimin Hu; Khan Muhammad",
    "corresponding_authors": "",
    "abstract": "Unsupervised person re-identification (Re-ID) has garnered significant attention because of its data-friendly nature, as it does not require labeled data. Existing approaches primarily address this challenge by employing feature-clustering techniques to generate pseudo-labels. In addition, camera-proxy-based methods have emerged because of their impressive ability to cluster sample identities. However, these methods often blur the distinctions between individuals within inter-camera views, which is crucial for effective person re-ID. To address this issue, this study introduces an inter-camera-identity-difference-based contrastive learning framework for unsupervised person Re-ID. The proposed framework comprises two key components: (1) a different sample cross-view close-range penalty module and (2) the same sample cross-view long-range constraint module. The former aims at penalizing excessive similarity among different subjects across inter-camera views, whereas the latter mitigates the challenge of excessive dissimilarity among the same subject across camera views. To validate the performance of our method, we conducted extensive experiments on three existing person Re-ID datasets (Market-1501, MSMT17, and PersonX). The results demonstrate the effectiveness of the proposed method, which shows a promising performance. The code is available at https://github.com/hooldylan/IIDCL .",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4393871767",
    "type": "article"
  },
  {
    "title": "Learned Video Compression with Adaptive Temporal Prior and Decoded Motion-aided Quality Enhancement",
    "doi": "https://doi.org/10.1145/3661824",
    "publication_date": "2024-04-27",
    "publication_year": 2024,
    "authors": "Jiayu Yang; Chunhui Yang; Fei Xiong; Yongqi Zhai; Ronggang Wang",
    "corresponding_authors": "",
    "abstract": "Learned video compression has drawn great attention and shown promising compression performance recently. In this article, we focus on the two components in the learned video compression framework, the conditional entropy model and quality enhancement module, to improve compression performance. Specifically, we propose an adaptive spatial-temporal entropy model for image, motion, and residual compression, which introduces a temporal prior to reduce temporal redundancy of latents and an additional modulated mask to evaluate the similarity and perform refinement. In addition, a quality enhancement module is proposed for predicted frame and reconstructed frame to improve frame quality and reduce the bitrate cost of residual coding. The module reuses decoded optical flow as a motion prior and utilizes deformable convolution to mine high-quality information from the reference frame in a bit-free manner. The two proposed coding tools are integrated into a pixel-domain residual coding–based compression framework to evaluate their effectiveness. Experimental results demonstrate that our framework achieves competitive compression performance in the low-delay scenario compared with recent learning-based methods and traditional H.265/HEVC in terms of Peak Signal-to-Noise Ratio (PSNR) and Multi-Scale Structural Similarity Index (MS-SSIM). The code is available at OpenLVC.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4395691450",
    "type": "article"
  },
  {
    "title": "Self-Supervised Monocular Depth Estimation via Binocular Geometric Correlation Learning",
    "doi": "https://doi.org/10.1145/3663570",
    "publication_date": "2024-05-08",
    "publication_year": 2024,
    "authors": "Bo Peng; Lin Sun; Jianjun Lei; Bingzheng Liu; Haifeng Shen; Wanqing Li; Qingming Huang",
    "corresponding_authors": "",
    "abstract": "Monocular depth estimation aims to infer a depth map from a single image. Although supervised learning-based methods have achieved remarkable performance, they generally rely on a large amount of labor-intensively annotated data. Self-supervised methods, on the other hand, do not require any annotation of ground-truth depth and have recently attracted increasing attention. In this work, we propose a self-supervised monocular depth estimation network via binocular geometric correlation learning. Specifically, considering the inter-view geometric correlation, a binocular cue prediction module is presented to generate the auxiliary vision cue for the self-supervised learning of monocular depth estimation. Then, to deal with the occlusion in depth estimation, an occlusion interference attenuated constraint is developed to guide the supervision of the network by inferring the occlusion region and producing paired occlusion masks. Experimental results on two popular benchmark datasets have demonstrated that the proposed network obtains competitive results compared to state-of-the-art self-supervised methods and achieves comparable results to some popular supervised methods.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4396723387",
    "type": "article"
  },
  {
    "title": "A Multi-task Adversarial Attack Against Face Authentication",
    "doi": "https://doi.org/10.1145/3665496",
    "publication_date": "2024-05-21",
    "publication_year": 2024,
    "authors": "Hanrui Wang; Shuo Wang; Cunjian Chen; Mássimo Tistarelli; Zhe Jin",
    "corresponding_authors": "",
    "abstract": "Deep-learning-based identity management systems, such as face authentication systems, are vulnerable to adversarial attacks. However, existing attacks are typically designed for single-task purposes, which means they are tailored to exploit vulnerabilities unique to the individual target rather than being adaptable for multiple users or systems. This limitation makes them unsuitable for certain attack scenarios, such as morphing, universal, transferable, and counter attacks. In this paper, we propose a multi-task adversarial attack algorithm called MTADV that are adaptable for multiple users or systems. By interpreting these scenarios as multi-task attacks, MTADV is applicable to both single- and multi-task attacks, and feasible in the white- and gray-box settings. Furthermore, MTADV is effective against various face datasets, including LFW, CelebA, and CelebA-HQ, and can work with different deep learning models, such as FaceNet, InsightFace, and CurricularFace. Importantly, MTADV retains its feasibility as a single-task attack targeting a single user/system. To the best of our knowledge, MTADV is the first adversarial attack method that can target all of the aforementioned scenarios in one algorithm.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4398159858",
    "type": "article"
  },
  {
    "title": "Towards Attribute-Controlled Fashion Image Captioning",
    "doi": "https://doi.org/10.1145/3671000",
    "publication_date": "2024-06-05",
    "publication_year": 2024,
    "authors": "Chen Cai; Kim–Hui Yap; Suchen Wang",
    "corresponding_authors": "",
    "abstract": "Fashion image captioning is a critical task in the fashion industry that aims to automatically generate product descriptions for fashion items. However, existing fashion image captioning models predict a fixed caption for a particular fashion item once deployed, which does not cater to unique preferences. We explore a controllable way of fashion image captioning that allows the users to specify a few semantic attributes to guide the caption generation. Our approach utilizes semantic attributes as a control signal, giving users the ability to specify particular fashion attributes (e.g., stitch, knit, sleeve, etc.) and styles (e.g., cool, classic, fresh, etc.) that they want the model to incorporate when generating captions. By providing this level of customization, our approach creates more personalized and targeted captions that suit individual preferences. To evaluate the effectiveness of our proposed approach, we clean, filter, and assemble a new fashion image caption dataset called FACAD170K from the current FACAD dataset. This dataset facilitates learning and enables us to investigate the effectiveness of our approach. Our results demonstrate that our proposed approach outperforms existing fashion image captioning models as well as conventional captioning methods. Besides, we further validate the effectiveness of the proposed method on the MSCOCO and Flickr30K captioning datasets and achieve competitive performance.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4399360161",
    "type": "article"
  },
  {
    "title": "Multimodal PEAR Chain-of-Thought Reasoning for Multimodal Sentiment Analysis",
    "doi": "https://doi.org/10.1145/3672398",
    "publication_date": "2024-06-11",
    "publication_year": 2024,
    "authors": "Yan Li; Xiangyuan Lan; Haifeng Chen; Ke Lü; Dongmei Jiang",
    "corresponding_authors": "",
    "abstract": "Multimodal sentiment analysis aims to predict sentiments from multimodal signals such as audio, video, and text. Existing methods often rely on Pre-trained Language Models (PLMs) to extract semantic information from textual data, lacking an in-depth understanding of the logical relationships within the text modality . This paper introduces the Multimodal PEAR Chain-of-Thought (MM-PEAR-CoT) reasoning for multimodal sentiment analysis. Inspired by the human thought process when solving complex problems, the PEAR (Preliminaries, quEstion, Answer, Reason) chain-of-thought prompt is first proposed to induce Large Language Models (LLMs) to generate text-based reasoning processes and zero-shot sentiment prediction results. However, text-based chain-of-thought reasoning is not always reliable and might contain irrational steps due to the hallucinations of large language models . To address this, we further design the Cross-Modal Filtering and Fusion (CMFF) module. The filtering submodule utilizes audio and visual modalities to suppress irrational steps in the chain of thought, while the fusion submodule integrates high-level reasoning information and cross-modal complementary information in the process of semantic representation learning. Experimental results on two multimodal sentiment analysis benchmark datasets show that high-level reasoning information can help learn discriminative text representation, and cross-modal complementary information can avoid misleading by unreasonable steps in the chain of thought. MM-PEAR-CoT achieves the best results on both datasets, with improvements of 2.2% and 1.7% in binary classification accuracy on the CMU-MOSI and CMU-MOSEI datasets, respectively. To the best of our knowledge, this is the first study to apply chain-of-thought reasoning to multimodal sentiment analysis.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4399541205",
    "type": "article"
  },
  {
    "title": "Learning Domain Invariant Features for Unsupervised Indoor Depth Estimation Adaptation",
    "doi": "https://doi.org/10.1145/3672397",
    "publication_date": "2024-06-13",
    "publication_year": 2024,
    "authors": "Jiehua Zhang; Liang Li; Chenggang Yan; Zhan Wang; Changliang Xu; Jiyong Zhang; Chuqiao Chen",
    "corresponding_authors": "",
    "abstract": "Predicting depth maps from monocular images has made an impressive performance in the past years. However, most depth estimation methods are trained with paired image-depth map data or multi-view images (e.g., stereo pair and monocular sequence), which suffer from expensive annotation costs and poor transferability. Although unsupervised domain adaptation methods are introduced to mitigate the reliance on annotated data, rare works focus on the unsupervised cross-scenario indoor monocular depth estimation. In this paper, we propose to study the generalization of depth estimation models across different indoor scenarios in an adversarial-based domain adaptation paradigm. Concretely, a domain discriminator is designed for discriminating the representation from source and target domains, while the feature extractor aims to confuse the domain discriminator by capturing domain-invariant features. Further, we reconstruct depth maps from latent representations with the supervision of labeled source data. As a result, the feature extractor learned features possess the merit of both domain-invariant and low source risk, and the depth estimator can deal with the domain shift between source and target domains. We conduct the cross-scenario and cross-dataset experiments on the ScanNet and NYU-Depth-v2 datasets to verify the effectiveness of our method and achieve impressive performance.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4399631183",
    "type": "article"
  },
  {
    "title": "Decoupling Deep Learning for Enhanced Image Recognition Interpretability",
    "doi": "https://doi.org/10.1145/3674837",
    "publication_date": "2024-07-10",
    "publication_year": 2024,
    "authors": "Yitao Peng; Lianghua He; Die Hu; Yihang Liu; Longzhen Yang; Shaohua Shang",
    "corresponding_authors": "",
    "abstract": "The quest for enhancing the interpretability of neural networks has become a prominent focus in recent research endeavors. Prototype-based neural networks have emerged as a promising avenue for imbuing models with interpretability by gauging the similarity between image components and category prototypes to inform decision-making. However, these networks face challenges as they share similarity activations during both the inference and explanation processes, creating a tradeoff between accuracy and interpretability. To address this issue and ensure that a network achieves high accuracy and robust interpretability in the classification process, this article introduces a groundbreaking prototype-based neural network termed the “Decoupling Prototypical Network” (DProtoNet). This novel architecture comprises encoder, inference, and interpretation modules. In the encoder module, we introduce decoupling feature masks to facilitate the generation of feature vectors and prototypes, enhancing the generalization capabilities of the model. The inference module leverages these feature vectors and prototypes to make predictions based on similarity comparisons, thereby preserving an interpretable inference structure. Meanwhile, the interpretation module advances the field by presenting a novel approach: a “multiple dynamic masks decoder” that replaces conventional upsampling similarity activations. This decoder operates by perturbing images with mask vectors of varying sizes and learning saliency maps through consistent activation. This methodology offers a precise and innovative means of interpreting prototype-based networks. DProtoNet effectively separates the inference and explanation components within prototype-based networks. By eliminating the constraints imposed by shared similarity activations during the inference and explanation phases, our approach concurrently elevates accuracy and interpretability. Experimental evaluations on diverse public natural datasets, including CUB-200-2011, Stanford Cars, and medical datasets like RSNA and iChallenge-PM, corroborate the substantial enhancements achieved by our method compared to previous state-of-the-art approaches. Furthermore, ablation studies are conducted to provide additional evidence of the effectiveness of our proposed components.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4400493363",
    "type": "article"
  },
  {
    "title": "Edit Temporal-Consistent Videos with Image Diffusion Model",
    "doi": "https://doi.org/10.1145/3691344",
    "publication_date": "2024-08-31",
    "publication_year": 2024,
    "authors": "Yuanzhi Wang; Yong Li; Xiaoya Zhang; Xin Liu; A.J. Dai; Antoni B. Chan; Zhen Cui",
    "corresponding_authors": "",
    "abstract": "Large-scale text-to-image (T2I) diffusion models have been extended for text-guided video editing, yielding impressive zero-shot video editing performance. Nonetheless, the generated videos usually show spatial irregularities and temporal inconsistencies as the temporal characteristics of videos have not been faithfully modeled. In this paper, we propose an elegant yet effective Temporal-Consistent Video Editing (TCVE) method to mitigate the temporal inconsistency challenge for robust text-guided video editing. In addition to the utilization of a pretrained T2I 2D Unet for spatial content manipulation, we establish a dedicated temporal Unet architecture to faithfully capture the temporal coherence of the input video sequences. Furthermore, to establish coherence and interrelation between the spatial-focused and temporal-focused components, a cohesive spatial-temporal modeling unit is formulated. This unit effectively interconnects the temporal Unet with the pretrained 2D Unet, thereby enhancing the temporal consistency of the generated videos while preserving the capacity for video content manipulation. Quantitative experimental results and visualization results demonstrate that TCVE achieves state-of-the-art performance in both video temporal consistency and video editing capability, surpassing existing benchmarks in the field. Codes are released at https://github.com/mdswyz/TCVE .",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4402090583",
    "type": "article"
  },
  {
    "title": "Exploring Generative Adversarial Networks for Augmenting Network Intrusion Detection Tasks",
    "doi": "https://doi.org/10.1145/3689636",
    "publication_date": "2024-09-18",
    "publication_year": 2024,
    "authors": "Mihai Gabriel Constantin; Dan-Cristian Stanciu; Liviu–Daniel Stefan; Mihai Dogariu; Dan Mihai Mihailescu; George Ciobanu; M Bergeron; Winston Liu; Konstantin Belov; Octavian Radu; Bogdan Ionescu",
    "corresponding_authors": "",
    "abstract": "The advent of generative networks and their adoption in numerous domains and communities have led to a wave of innovation and breakthroughs in artificial intelligence and machine learning. Generative Adversarial Networks (GANs) have expanded the scope of what is possible with machine learning, allowing for new applications in areas such as computer vision, natural language processing, and creative AI. GANs, in particular, have been used for a wide range of tasks, including image and video generation, data augmentation, style transfer, and anomaly detection. They have also been used for medical imaging and drug discovery, where they can generate synthetic data to augment small datasets, reduce the need for expensive experiments, and lower the number of real patients that must be included in medical trials. Given these developments, we propose using the power of generative adversarial networks to create and augment flow-based network traffic datasets. We evaluate a series of GAN architectures, including Wasserstein, conditional, energy-based, gradient penalty, and LSTM GANs. We evaluate their performance on a set of flow-based network traffic data collected from 16 subjects who used their computers for home, work, and study purposes. The performance of these GAN architectures is described according to metrics that involve networking principles, data distribution among a collection of flows, and temporal data distribution. Given the tendency of network intrusion detection datasets to have a very imbalanced data distribution, i.e., a large number of samples in the “normal traffic” category and a comparatively low number of samples assigned to the “intrusion” categories, we test our GANs by augmenting the intrusion data and checking whether this helps intrusion detection neural networks in their task. We publish the resulting UPBFlow dataset and code on GitHub 1 .",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4402597159",
    "type": "article"
  },
  {
    "title": "Robust Hashing with Deep Features and Meixner Moments for Image Copy Detection",
    "doi": "https://doi.org/10.1145/3696669",
    "publication_date": "2024-09-23",
    "publication_year": 2024,
    "authors": "Mengzhu Yu; Zhenjun Tang; Xiaoping Liang; Xianquan Zhang; Zhixin Li; Xinpeng Zhang",
    "corresponding_authors": "",
    "abstract": "Copy detection is a key task of image copyright protection. Most robust hashing schemes do not make satisfied performance of image copy detection yet. To address this, a robust hashing scheme with deep features and Meixner moments is proposed for image copy detection. In the proposed hashing, global deep features are extracted by applying tensor Singular Value Decomposition (t-SVD) to the three-order tensor constructed in the DWT domain of the feature maps calculated by the pre-trained VGG16. Since the feature maps in the DWT domain are slightly disturbed by digital operations, the constructed three-order tensor is stable and thus the desirable robustness is guaranteed. Moreover, since t-SVD can decompose a three-order tensor into multiple low-dimensional matrices reflecting intrinsic structure, the global deep feature calculation from the low-dimensional matrices can provide good discrimination. Local features are calculated by the block-based Meixner moments. As the Meixner moments are resistant to geometric transformation and can efficiently discriminate various images, the use of the block-based Meixner moments can make discriminative and robust local features. Hash is ultimately determined by quantifying and combining global deep features and local features. The results of extensive experiments on open image datasets demonstrate that the proposed robust hashing outperforms some state-of-the-art robust hashing schemes in terms of classification and copy detection performances.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4402732806",
    "type": "article"
  },
  {
    "title": "Special section from the ACM multimedia conference 2007",
    "doi": "https://doi.org/10.1145/1404880.1404881",
    "publication_date": "2008-10-01",
    "publication_year": 2008,
    "authors": "Brian P. Bailey; Nicu Sebe; Alan Hanjalić",
    "corresponding_authors": "",
    "abstract": "research-article Share on Special section from the ACM multimedia conference 2007 Authors: Brian P. Bailey University of Illinois University of IllinoisView Profile , Nicu Sebe University of Amsterdam University of AmsterdamView Profile , Alan Hanjalic Delft University of Technology Delft University of TechnologyView Profile Authors Info & Claims ACM Transactions on Multimedia Computing, Communications, and ApplicationsVolume 5Issue 1October 2008 Article No.: 1pp 1–3https://doi.org/10.1145/1404880.1404881Published:30 October 2008Publication History 0citation198DownloadsMetricsTotal Citations0Total Downloads198Last 12 Months4Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W1992528333",
    "type": "article"
  },
  {
    "title": "Computational approaches to temporal sampling of video sequences",
    "doi": "https://doi.org/10.1145/1230812.1230813",
    "publication_date": "2007-05-01",
    "publication_year": 2007,
    "authors": "Tiecheng Liu; John R. Kender",
    "corresponding_authors": "",
    "abstract": "Video key frame extraction is one of the most important research problems for video summarization, indexing, and retrieval. For a variety of applications such as ubiquitous media access and video streaming, the temporal boundaries between video key frames are required for synchronizing visual content with audio. In this article, we define temporal video sampling as a unified process of extracting video key frames and computing their temporal boundaries, and formulate it as an optimization problem. We first provide an optimal approach that minimizes temporal video sampling error using a dynamic programming process. The optimal approach retrieves a key frame hierarchy and all temporal boundaries in O ( n 4 ) time and O ( n 2 ) space. To further reduce computational complexity, we also provide a suboptimal greedy algorithm that exploits the data structure of a binary heap and uses a novel “look-ahead” computational technique, enabling all levels of key frames to be extracted with an average-case computational time of O ( n log n ) and memory usage of O ( n ). Both the optimal and the greedy methods are free of parameters, thus avoiding the threshold-selection problem that exists in other approaches. We empirically compare the proposed optimal and greedy methods with several existing methods in terms of video sampling error, computational cost, and subjective quality. An evaluation of eight videos of different genres shows that the greedy approach achieves performance very close to that of the optimal approach while drastically reducing computational cost, making it suitable for processing long video sequences in large video databases.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W1965890211",
    "type": "article"
  },
  {
    "title": "A neural-network-based context-aware handoff algorithm for multimedia computing",
    "doi": "https://doi.org/10.1145/1386109.1386110",
    "publication_date": "2008-08-01",
    "publication_year": 2008,
    "authors": "Tsung-Nan Lin; Chiapin Wang; Po-Chiang Lin",
    "corresponding_authors": "",
    "abstract": "The access of multimedia computing in wireless networks is concerned with the performance of handoff because of the irretrievable property of real-time data delivery. To lessen throughput degradation incurred by unnecessary handoffs or handoff latencies leading to media disruption perceived by users, this paper presents a link quality based handoff algorithm. Neural networks are used to learn the cross-layer correlation between the link quality estimator such as packet success rate and the corresponding context metric indictors, for example, the transmitting packet length, received signal strength, and signal to noise ratio. Based on a pre-processed learning of link quality profile, neural networks make essential handoff decisions efficiently with the evaluations of link quality instead of the comparisons between relative signal strength. The experiment and simulation results show that the proposed algorithm improves the user perceived qualities in a transmission scenario of VoIP applications by minimizing both the number of lost packets and unnecessary handoffs.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W1998436072",
    "type": "article"
  },
  {
    "title": "Video game design using an eye-movement-dependent model of visual attention",
    "doi": "https://doi.org/10.1145/1386109.1386115",
    "publication_date": "2008-08-01",
    "publication_year": 2008,
    "authors": "Jie Li; James J. Clark",
    "corresponding_authors": "",
    "abstract": "Eye movements can be used to infer the allocation of covert attention. In this article, we propose to model the allocation of attention in a task-dependent manner based on different eye movement conditions, specifically fixation and pursuit. We show that the image complexity at eye fixation points during fixation, and the pursuit direction during pursuit are significant factors in attention allocation. Results of the study are applied to the design of an interactive computer game. Real-time eye movement information is taken as one of inputs for the game. The utility of such eye information for controlling game difficulty is shown.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W2083780703",
    "type": "article"
  },
  {
    "title": "A framework for cross-layer optimization of video streaming in wireless networks",
    "doi": "https://doi.org/10.1145/1870121.1870126",
    "publication_date": "2011-01-01",
    "publication_year": 2011,
    "authors": "Cheng-Hsin Hsu; Mohamed Hefeeda",
    "corresponding_authors": "",
    "abstract": "We present a general framework for optimizing the quality of video streaming in wireless networks that are composed of multiple wireless stations. The framework is general because: (i) it can be applied to different wireless networks, such as IEEE 802.11e WLAN and IEEE 802.16 WiMAX, (ii) it can employ different objective functions for the optimization, and (iii) it can adopt various models for the wireless channel, the link layer, and the distortion of the video streams in the application layer. The optimization framework controls parameters in different layers to optimally allocate the wireless network resources among all stations. More specifically, we address this video optimization problem in two steps. First, we formulate an abstract optimization problem for video streaming in wireless networks in general. This formulation exposes the important interaction between parameters belonging to different layers in the network stack. Then, we instantiate and solve the general problem for the recent IEEE 802.11e WLANs, which support prioritized traffic classes. We show how the calculated optimal solutions can efficiently be implemented in the distributed mode of the IEEE 802.11e standard. We evaluate our proposed solution using extensive simulations in the OPNET simulator, which captures most features of realistic wireless networks. In addition, to show the practicality of our solution, we have implemented it in the driver of an off-the-shelf wireless adapter that complies with the IEEE 802.11e standard. Our experimental and simulation results show that significant quality improvement in video streams can be achieved using our solution, without incurring any significant communication or computational overhead. We also explain how the general video optimization problem can be applied to other wireless networks, in particular, to the IEEE 802.16 WiMAX networks, which are becoming very popular.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2025879359",
    "type": "article"
  },
  {
    "title": "Exposing MP3 audio forgeries using frame offsets",
    "doi": "https://doi.org/10.1145/2344436.2344441",
    "publication_date": "2012-09-01",
    "publication_year": 2012,
    "authors": "Rui Yang; Zhenhua Qu; Jiwu Huang",
    "corresponding_authors": "",
    "abstract": "Audio recordings should be authenticated before they are used as evidence. Although audio watermarking and signature are widely applied for authentication, these two techniques require accessing the original audio before it is published. Passive authentication is necessary for digital audio, especially for the most popular audio format: MP3. In this article, we propose a passive approach to detect forgeries of MP3 audio. During the process of MP3 encoding the audio samples are divided into frames, and thus each frame has its own frame offset after encoding. Forgeries lead to the breaking of framing grids. So the frame offset is a good indication for locating forgeries, and it can be retrieved by the identification of the quantization characteristic. In this way, the doctored positions can be automatically located. Experimental results demonstrate that the proposed approach is effective in detecting some common forgeries, such as deletion, insertion, substitution, and splicing. Even when the bit rate is as low as 32 kbps, the detection rate is above 99%.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2022975157",
    "type": "article"
  },
  {
    "title": "Enhancing news organization for convenient retrieval and browsing",
    "doi": "https://doi.org/10.1145/2488732",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Zechao Li; Jing Liu; Meng Wang; Changsheng Xu; Hanqing Lu",
    "corresponding_authors": "",
    "abstract": "To facilitate users to access news quickly and comprehensively, we design a news search and browsing system named GeoVisNews, in which the news elements of “Where”, “Who”, “What” and “When” are enhanced via news geo-localization, image enrichment and joint ranking, respectively. For news geo-localization, an Ordinal Correlation Consistent Matrix Factorization (OCCMF) model is proposed to maintain the relevance rankings of locations to a specific news document and simultaneously capture intra-relations among locations and documents. To visualize news, we develop a novel method to enrich news documents with appropriate web images. Specifically, multiple queries are first generated from news documents for image search, and then the appropriate images are selected from the collected web images by an intelligent fusion approach based on multiple features. Obtaining the geo-localized and image enriched news resources, we further employ a joint ranking strategy to provide relevant, timely and popular news items as the answer of user searching queries. Extensive experiments on a large-scale news dataset collected from the web demonstrate the superior performance of the proposed approaches over related methods.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2070103877",
    "type": "article"
  },
  {
    "title": "A generalized tamper localization approach for reversible watermarking algorithms",
    "doi": "https://doi.org/10.1145/2487268.2487272",
    "publication_date": "2013-06-01",
    "publication_year": 2013,
    "authors": "Ruchira Naskar; Rajat Subhra Chakraborty",
    "corresponding_authors": "",
    "abstract": "In general reversible watermarking algorithms, the convention is to reject the entire cover image at the receiver end if it fails authentication, since there is no way to detect the exact locations of tampering. This feature may be exploited by an adversary to bring about a form of DoS attack. Here we provide a solution to this problem in form of a tamper localization mechanism for reversible watermarking algorithms, which allows selective rejection of distorted cover image regions in case of authentication failure, thus avoiding rejection of the complete image. Additionally it minimizes the bandwidth requirement of the communication channel.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2062774123",
    "type": "article"
  },
  {
    "title": "Contextual tag inference",
    "doi": "https://doi.org/10.1145/2037676.2037689",
    "publication_date": "2011-10-01",
    "publication_year": 2011,
    "authors": "Michael Mandel; Razvan Pascanu; Douglas Eck; Yoshua Bengio; Luca Maria Aiello; Rossano Schifanella; Filippo Menczer",
    "corresponding_authors": "",
    "abstract": "This article examines the use of two kinds of context to improve the results of content-based music taggers: the relationships between tags and between the clips of songs that are tagged. We show that users agree more on tags applied to clips temporally “closer” to one another; that conditional restricted Boltzmann machine models of tags can more accurately predict related tags when they take context into account; and that when training data is “smoothed” using context, support vector machines can better rank these clips according to the original, unsmoothed tags and do this more accurately than three standard multi-label classifiers.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2081631398",
    "type": "article"
  },
  {
    "title": "Browse by chunks",
    "doi": "https://doi.org/10.1145/2037676.2037687",
    "publication_date": "2011-10-01",
    "publication_year": 2011,
    "authors": "Jitao Sang; Changsheng Xu",
    "corresponding_authors": "",
    "abstract": "The overwhelming amount of Web videos returned from search engines makes effective browsing and search a challenging task. Rather than conventional ranked list, it becomes necessary to organize the retrieved videos in alternative ways. In this article, we explore the issue of topic mining and organizing of the retrieved web videos in semantic clusters. We present a framework for clustering-based video retrieval and build a visualization user interface. A hierarchical topic structure is exploited to encode the characteristics of the retrieved video collection and a semi-supervised hierarchical topic model is proposed to guide the topic hierarchy discovery. Carefully designed experiments on web-scale video dataset collected from video sharing websites validate the proposed method and demonstrate that clustering-based video retrieval is practical to facilitate users for effective browsing.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2099914810",
    "type": "article"
  },
  {
    "title": "QoE-oriented 3D video transcoding for mobile streaming",
    "doi": "https://doi.org/10.1145/2348816.2348821",
    "publication_date": "2012-09-01",
    "publication_year": 2012,
    "authors": "Yanwei Liu; Song Ci; Hui Tang; Yun Ye; Jinxia Liu",
    "corresponding_authors": "",
    "abstract": "With advance in mobile 3D display, mobile 3D video is already enabled by the wireless multimedia networking, and it will be gradually popular since it can make people enjoy the natural 3D experience anywhere and anytime. In current stage, mobile 3D video is generally delivered over the heterogeneous network combined by wired and wireless channels. How to guarantee the optimal 3D visual quality of experience (QoE) for the mobile 3D video streaming is one of the important topics concerned by the service provider. In this article, we propose a QoE-oriented transcoding approach to enhance the quality of mobile 3D video service. By learning the pre-controlled QoE patterns of 3D contents, the proposed 3D visual QoE inferring model can be utilized to regulate the transcoding configurations in real-time according to the feedbacks of network and user-end device information. In the learning stage, we propose a piecewise linear mean opinion score (MOS) interpolation method to further reduce the cumbersome manual work of preparing QoE patterns. Experimental results show that the proposed transcoding approach can provide the adapted 3D stream to the heterogeneous network, and further provide superior QoE performance to the fixed quantization parameter (QP) transcoding and mean squared error (MSE) optimized transcoding for mobile 3D video streaming.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2169434382",
    "type": "article"
  },
  {
    "title": "O-Mopsi",
    "doi": "https://doi.org/10.1145/3115935",
    "publication_date": "2017-08-12",
    "publication_year": 2017,
    "authors": "Pasi Fränti; Radu Mariescu-Istodor; Lahari Sengupta",
    "corresponding_authors": "",
    "abstract": "Location-based games have been around already since 2000 but only recently when PokemonGo came to markets it became clear that they can reach wide popularity. In this article, we perform a literature-based analytical study of what kind of issues location-based game design faces, and how they can be solved. We study how to use and verify the location, the role of the games as exergames, use in education, and study technical and safety issues. As a case study, we present O-Mopsi game that combines physical activity with problem solving. It includes three challenges: (1) navigating to the next target, (2) deciding the order of targets, (3) physical movement. All of them are unavoidable and relevant. For guiding the players, we use three types of multimedia: images (targets and maps), sound (user guidance), and GPS (for positioning). We discuss motivational aspects, analysis of the playing, and content creation. The quality of experiences is reported based on playing in SciFest Science festivals during 2011--2016.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2748310782",
    "type": "article"
  },
  {
    "title": "Learning Label Preserving Binary Codes for Multimedia Retrieval",
    "doi": "https://doi.org/10.1145/3152126",
    "publication_date": "2017-12-20",
    "publication_year": 2017,
    "authors": "Kai Li; Guo-Jun Qi; Kien A. Hua",
    "corresponding_authors": "",
    "abstract": "Learning-based hashing has been researched extensively in the past few years due to its great potential in fast and accurate similarity search among huge volumes of multimedia data. In this article, we present a novel multimedia hashing framework, called Label Preserving Multimedia Hashing (LPMH) for multimedia similarity search. In LPMH, a general optimization method is used to learn the joint binary codes of multiple media types by explicitly preserving semantic label information. Compared with existing hashing methods which are typically developed under and thus restricted to some specific objective functions, the proposed optimization strategy is not tied to any specific loss function and can easily incorporate bit balance constraints to produce well-balanced binary codes. Specifically, our formulation leads to a set of Binary Integer Programming (BIP) problems that have exact solutions both with and without bit balance constraints. These problems can be solved extremely fast and the solution can easily scale up to large-scale datasets. In the hash function learning stage, the boosted decision trees algorithm is utilized to learn multiple media-specific hash functions that can map heterogeneous data sources into a homogeneous Hamming space for cross-media retrieval. We have comprehensively evaluated the proposed method using a range of large-scale datasets in both single-media and cross-media retrieval tasks. The experimental results demonstrate that LPMH is competitive with state-of-the-art methods in both speed and accuracy.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2778018998",
    "type": "article"
  },
  {
    "title": "Saving Energy in Mobile Devices for On-Demand Multimedia Streaming -- A Cross-Layer Approach",
    "doi": "https://doi.org/10.1145/2556942",
    "publication_date": "2014-04-01",
    "publication_year": 2014,
    "authors": "Mohammad Asharful Hoque; Matti Siekkinen; Jukka K. Nurminen; Sasu Tarkoma; Mika Aalto",
    "corresponding_authors": "",
    "abstract": "This article proposes a novel energy-efficient multimedia delivery system called EStreamer. First, we study the relationship between buffer size at the client, burst-shaped TCP-based multimedia traffic, and energy consumption of wireless network interfaces in smartphones. Based on the study, we design and implement EStreamer for constant bit rate and rate-adaptive streaming. EStreamer can improve battery lifetime by 3x, 1.5x, and 2x while streaming over Wi-Fi, 3G, and 4G, respectively.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W3121960391",
    "type": "article"
  },
  {
    "title": "Dynamic load balancing in distributed virtual environments using heat diffusion",
    "doi": "https://doi.org/10.1145/2499906",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "Yunhua Deng; Rynson W. H. Lau",
    "corresponding_authors": "",
    "abstract": "Distributed virtual environments (DVEs) are attracting a lot of attention in recent years, due to the increasing popularity of online gaming and social networks. As the number of concurrent users of a DVE increases, a critical problem is on how the workload among multiple servers can be balanced in order to maintain real-time performance. Although a number of load balancing methods have been proposed, they either try to produce high quality load balancing results and become too slow or emphasize on efficiency and the load balancing results become less effective. In this article, we propose a new approach to address this problem based on heat diffusion. Our work has two main contributions. First, we propose a local and a global load balancing methods for DVEs based on heat diffusion. Second, we investigate two performance factors of the proposed methods, the convergence threshold and the load balancing interval. We have conducted a number of experiments to extensively evaluate the performance of the proposed methods. Our experimental results show that the proposed methods outperform existing methods in that our methods are effective in reducing server overloading while at the same time being efficient.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2034979708",
    "type": "article"
  },
  {
    "title": "Identifying Compression History of Wave Audio and Its Applications",
    "doi": "https://doi.org/10.1145/2575978",
    "publication_date": "2014-04-01",
    "publication_year": 2014,
    "authors": "Da Luo; Weiqi Luo; Rui Yang; Jiwu Huang",
    "corresponding_authors": "",
    "abstract": "Audio signal is sometimes stored and/or processed in WAV (waveform) format without any knowledge of its previous compression operations. To perform some subsequent processing, such as digital audio forensics, audio enhancement and blind audio quality assessment, it is necessary to identify its compression history. In this article, we will investigate how to identify a decompressed wave audio that went through one of three popular compression schemes, including MP3, WMA (windows media audio) and AAC (advanced audio coding). By analyzing the corresponding frequency coefficients, including modified discrete cosine transform (MDCT) and Mel-frequency cepstral coefficients (MFCCs), of those original audio clips and their decompressed versions with different compression schemes and bit rates, we propose several statistics to identify the compression scheme as well as the corresponding bit rate previously used for a given WAV signal. The experimental results evaluated on 8,800 audio clips with various contents have shown the effectiveness of the proposed method. In addition, some potential applications of the proposed method are discussed.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2035268175",
    "type": "article"
  },
  {
    "title": "PLACID",
    "doi": "https://doi.org/10.1145/3131289",
    "publication_date": "2017-09-18",
    "publication_year": 2017,
    "authors": "Mohammad Motamedi; Philipp Gysel; Soheil Ghiasi",
    "corresponding_authors": "",
    "abstract": "Deep Convolutional Neural Networks (DCNNs) exhibit remarkable performance in a number of pattern recognition and classification tasks. Modern DCNNs involve many millions of parameters and billions of operations. Inference using such DCNNs, if implemented as software running on an embedded processor, results in considerable execution time and energy consumption, which is prohibitive in many mobile applications. Field-programmable gate array (FPGA)-based acceleration of DCNN inference is a promising approach to improve both energy consumption and classification throughput. However, the engineering effort required for development and verification of an optimized FPGA-based architecture is significant. In this article, we present PLACID, an automated PLatform for Accelerator CreatIon for DCNNs. PLACID uses an analytical approach to characterization and exploration of the implementation space. PLACID enables generation of an accelerator with the highest throughput for a given DCNN on a specific target FPGA platform. Subsequently, it generates an RTL level architecture in Verilog, which can be passed onto commercial tools for FPGA implementation. PLACID is fully automated, and reduces the accelerator design time from a few months down to a few hours. Experimental results show that architectures synthesized by PLACID yield 2× higher throughput density than the best competing approach.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2756009169",
    "type": "article"
  },
  {
    "title": "Cost-Efficient Server Provisioning for Cloud Gaming",
    "doi": "https://doi.org/10.1145/3190838",
    "publication_date": "2018-06-27",
    "publication_year": 2018,
    "authors": "Yusen Li; Yunhua Deng; Xueyan Tang; Wentong Cai; Xiaoguang Liu; Gang Wang",
    "corresponding_authors": "",
    "abstract": "Cloud gaming has gained significant popularity recently due to many important benefits such as removal of device constraints, instant-on, and cross-platform. The properties of intensive resource demands and dynamic workloads make cloud gaming appropriate to be supported by an elastic cloud platform. Facing a large user population, a fundamental problem is how to provide satisfactory cloud gaming service at modest cost. We observe that the software storage cost could be substantial compared to the server running cost in cloud gaming using elastic cloud resources. Therefore, in this article, we address the server provisioning problem for cloud gaming to optimize both the server running cost and the software storage cost. We find that the distribution of game software among servers and the selection of server types both trigger tradeoffs between the software storage cost and the server running cost in cloud gaming. We formulate the problem with a stochastic model and employ queueing theory to conduct a solid theoretical analysis of the system behaviors under different request dispatching policies. We then propose several classes of algorithms to approximate the optimal solution. The proposed algorithms are evaluated by extensive experiments using real-world parameters. The results show that the proposed Ordered and Genetic algorithms are computationally efficient, nearly cost-optimal, and highly robust to dynamic changes.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2810494620",
    "type": "article"
  },
  {
    "title": "Combining Acoustic and Multilevel Visual Features for Music Genre Classification",
    "doi": "https://doi.org/10.1145/2801127",
    "publication_date": "2015-08-24",
    "publication_year": 2015,
    "authors": "Ming-Ju Wu; Jyh‐Shing Roger Jang",
    "corresponding_authors": "",
    "abstract": "Most music genre classification approaches extract acoustic features from frames to capture timbre information, leading to the common framework of bag-of-frames analysis. However, time-frequency analysis is also vital for modeling music genres. This article proposes multilevel visual features for extracting spectrogram textures and their temporal variations. A confidence-based late fusion is proposed for combining the acoustic and visual features. The experimental results indicated that the proposed method achieved an accuracy improvement of approximately 14% and 2% in the world's largest benchmark dataset (MASD) and Unique dataset, respectively. In particular, the proposed approach won the Music Information Retrieval Evaluation eXchange (MIREX) music genre classification contests from 2011 to 2013, demonstrating the feasibility and necessity of combining acoustic and visual features for classifying music genres.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W1146734363",
    "type": "article"
  },
  {
    "title": "CamMark",
    "doi": "https://doi.org/10.1145/2700295",
    "publication_date": "2015-02-24",
    "publication_year": 2015,
    "authors": "Philipp Schaber; Stephan Kopf; Sina Wetzel; Tyler Ballast; Christoph Wesch; Wolfgang Effelsberg",
    "corresponding_authors": "",
    "abstract": "To support the development of any system that includes the generation and evaluation of camcorder copies, as well as to provide a common benchmark for robustness against camcorder copies, we present a tool to simulate digital video re-acquisition using a digital video camera. By resampling each video frame, we simulate the typical artifacts occurring in a camcorder copy: geometric modifications (aspect ratio changes, cropping, perspective and lens distortion), temporal sampling artifacts (due to different frame rates, shutter speeds, rolling shutters, or playback), spatial and color subsampling (rescaling, filtering, Bayer color filter array), and processing steps (automatic gain control, automatic white balance). We also support the simulation of camera movement (e.g., a hand-held camera) and background insertion. Furthermore, we allow for an easy setup and calibration of all the simulated artifacts, using sample/reference pairs of images and videos. Specifically temporal subsampling effects are analyzed in detail to create realistic frame blending artifacts in the simulated copies. We carefully evaluated our entire camcorder simulation system and found that the models we developed describe and match the real artifacts quite well.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2115330164",
    "type": "article"
  },
  {
    "title": "Personalized Photograph Ranking and Selection System Considering Positive and Negative User Feedback",
    "doi": "https://doi.org/10.1145/2584105",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "Che-Hua Yeh; Brian A. Barsky; Ming Ouhyoung",
    "corresponding_authors": "",
    "abstract": "In this article, we propose a novel personalized ranking system for amateur photographs. The proposed framework treats the photograph assessment as a ranking problem and we introduce the idea of personalized ranking , which ranks photographs considering both their aesthetic qualities and personal preferences. Photographs are described using three types of features: photo composition , color and intensity distribution , and personalized features . An aesthetic prediction model is learned from labeled photographs by using the proposed image features and RBF-ListNet learning algorithm. The experimental results show that the proposed framework outperforms in the ranking performance: a Kendall's tau value of 0.432 is significantly higher than those obtained by the features proposed in one of the state-of-the-art approaches (0.365) and by learning based on support vector regression (0.384). To realize personalization in ranking, three approaches are proposed: the feature-based approach allows users to select photographs with specific rules, the example-based approach takes the positive feedback from users to rerank the photograph, and the list-based approach takes both positive and negative feedback from users into consideration. User studies indicate that all three approaches are effective in both aesthetic and personalized ranking.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2119996282",
    "type": "article"
  },
  {
    "title": "Complexity Correlation-Based CTU-Level Rate Control with Direction Selection for HEVC",
    "doi": "https://doi.org/10.1145/3107616",
    "publication_date": "2017-08-12",
    "publication_year": 2017,
    "authors": "Mingliang Zhou; Yongfei Zhang; Bo Li; Xupeng Lin",
    "corresponding_authors": "",
    "abstract": "Rate control is a crucial consideration in high-efficiency video coding (HEVC). The estimation of model parameters is very important for coding tree unit (CTU)-level rate control, as it will significantly affect bit allocation and thus coding performance. However, the model parameters in the CTU-level rate control sometimes fails because of inadequate consideration of the correlation between model parameters and complexity characteristic. In this study, we establish a novel complexity correlation-based CTU-level rate control for HEVC. First, we formulate the model parameter estimation scheme as a multivariable estimation problem; second, based on the complexity correlation of the neighbouring CTU, an optimal direction is selected in five directions for reference CTU set selection during model parameter estimation to further improve the prediction accuracy of the complexity of the current CTU. Third, to improve their precision, the relationship between the model parameters and the complexity of the reference CTU set in the optimal direction is established by using least square method (LS), and the model parameters are solved via the estimated complexity of the current CTU. Experimental results show that the proposed algorithm can significantly improve the accuracy of the CTU-level rate control and thus the coding performance; the proposed scheme consistently outperforms HM 16.0 and other state-of-the-art algorithms in a variety of testing configurations. More specifically, up to 8.4% and on average 6.4% BD-Rate reduction is achieved compared to HM 16.0 and up to 4.7% and an average of 3.4% BD-Rate reduction is achieved compared to other algorithms, with only a slight complexity overhead.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2749432756",
    "type": "article"
  },
  {
    "title": "Game Input with Delay—Moving Target Selection with a Game Controller Thumbstick",
    "doi": "https://doi.org/10.1145/3187288",
    "publication_date": "2018-06-27",
    "publication_year": 2018,
    "authors": "Mark Claypool",
    "corresponding_authors": "Mark Claypool",
    "abstract": "Hosting interactive video-based services, such as computer games, in the Cloud poses particular challenges given user sensitivity to delay. A better understanding of the impact of delay on player-game interactions can help design cloud systems and games that accommodate delays inherent in cloud systems. Previous top-down studies of delay using full-featured games have helped understand the impact of delay, but often do not generalize or lend themselves to analytic modeling. Bottom-up studies isolating user input and delay can better generalize and be used in models, but have yet to be applied to cloud-hosted computer games. In order to better understand delay impact in cloud-hosted computer games, we conduct a large bottom-up user study centered on a fundamental game interaction—selecting a moving target with user input impeded by delay. Our work builds a custom game that controls both the target speed and input delay and has players select the target using a game controller analog thumbstick. Analysis of data from over 50 users shows target selection time exponentially increases with delay and target speed and is well-fit by an exponential model that includes a delay and target speed interaction term. A comparison with two previous studies, both using a mouse instead of a thumbstick, suggests the model’s relationship between selection time, delay, and target speed holds more broadly, providing a foundation for a potential law explaining moving target selection with delay encountered in cloud-hosted games.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2810283007",
    "type": "article"
  },
  {
    "title": "User Behavior Analysis and Video Popularity Prediction on a Large-Scale VoD System",
    "doi": "https://doi.org/10.1145/3226035",
    "publication_date": "2018-06-27",
    "publication_year": 2018,
    "authors": "Lei Huang; Bowen Ding; Aining Wang; Yuedong Xu; Yipeng Zhou; Xiang Li",
    "corresponding_authors": "",
    "abstract": "Understanding streaming user behavior is crucial to the design of large-scale Video-on-Demand (VoD) systems. In this article, we begin with the measurement of individual viewing behavior from two aspects: the temporal characteristics and user interest. We observe that active users spend more hours on each active day, and their daily request time distribution is more scattered than that of the less active users, while the inter-view time distribution differs negligibly between two groups. The common interest in popular videos and the latest uploaded videos is observed in both groups. We then investigate the predictability of video popularity as a collective user behavior through early views. In the light of the limitations of classical approaches, the Autoregressive-Moving-Average (ARMA) model is employed to forecast the popularity dynamics of individual videos at fine-grained time scales, thus achieving much higher prediction accuracy. When applied to video caching, the ARMA-assisted Least Frequently Used (LFU) algorithm can outperform the Least Recently Used (LRU) by 11--16%, the well-tuned LFU by 6--13%, and the LFU is only 2--4% inferior to the offline LFU in terms of hit ratio.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2810843207",
    "type": "article"
  },
  {
    "title": "A Survey on Content-Aware Image and Video Retargeting",
    "doi": "https://doi.org/10.1145/3231598",
    "publication_date": "2018-07-24",
    "publication_year": 2018,
    "authors": "Johannes Kiess; Stephan Kopf; Benjamin Guthier; Wolfgang Effelsberg",
    "corresponding_authors": "",
    "abstract": "This survey introduces the current state of the art in image and video retargeting and describes important ideas and technologies that have influenced the recent work. Retargeting is the process of adapting an image or video from one screen resolution to another to fit different displays, for example, when watching a wide screen movie on a normal television screen or a mobile device. As there has been considerable work done in this field already, this survey provides an overview of the techniques. It is meant to be a starting point for new research in the field. We include explanations of basic terms and operators, as well as the basic workflow of the different methods.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2883222749",
    "type": "article"
  },
  {
    "title": "Image Captioning With Visual-Semantic Double Attention",
    "doi": "https://doi.org/10.1145/3292058",
    "publication_date": "2019-01-23",
    "publication_year": 2019,
    "authors": "Chen He; Haifeng Hu",
    "corresponding_authors": "",
    "abstract": "In this article, we propose a novel Visual-Semantic Double Attention (VSDA) model for image captioning. In our approach, VSDA consists of two parts: a modified visual attention model is used to extract sub-region image features, then a new SEmantic Attention (SEA) model is proposed to distill semantic features. Traditional attribute-based models always neglect the distinctive importance of each attribute word and fuse all of them into recurrent neural networks, resulting in abundant irrelevant semantic features. In contrast, at each timestep, our model selects the most relevant word that aligns with current context. In other words, the real power of VSDA lies in the ability of not only leveraging semantic features but also eliminating the influence of irrelevant attribute words to make the semantic guidance more precise. Furthermore, our approach solves the problem that visual attention models cannot boost generating non-visual words. Considering that visual and semantic features are complementary to each other, our model can leverage both of them to strengthen the generations of visual and non-visual words. Extensive experiments are conducted on famous datasets: MS COCO and Flickr30k. The results show that VSDA outperforms other methods and achieves promising performance.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2911285743",
    "type": "article"
  },
  {
    "title": "Modeling Dyadic and Group Impressions with Intermodal and Interperson Features",
    "doi": "https://doi.org/10.1145/3265754",
    "publication_date": "2019-01-24",
    "publication_year": 2019,
    "authors": "Shogo Okada; Laurent Son Nguyen; Oya Aran; Daniel Gática-Pérez",
    "corresponding_authors": "",
    "abstract": "This article proposes a novel feature-extraction framework for inferring impression personality traits, emergent leadership skills, communicative competence, and hiring decisions. The proposed framework extracts multimodal features, describing each participant’s nonverbal activities. It captures intermodal and interperson relationships in interactions and captures how the target interactor generates nonverbal behavior when other interactors also generate nonverbal behavior. The intermodal and interperson patterns are identified as frequent co-occurring events based on clustering from multimodal sequences. The proposed framework is applied to the SONVB corpus, which is an audiovisual dataset collected from dyadic job interviews, and the ELEA audiovisual data corpus, which is a dataset collected from group meetings. We evaluate the framework on a binary classification task involving 15 impression variables from the two data corpora. The experimental results show that the model trained with co-occurrence features is more accurate than previous models for 14 out of 15 traits.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2912188122",
    "type": "article"
  },
  {
    "title": "Multi-source Multi-level Attention Networks for Visual Question Answering",
    "doi": "https://doi.org/10.1145/3316767",
    "publication_date": "2019-04-30",
    "publication_year": 2019,
    "authors": "Dongfei Yu; Jianlong Fu; Xinmei Tian; Tao Mei",
    "corresponding_authors": "",
    "abstract": "In recent years, Visual Question Answering (VQA) has attracted increasing attention due to its requirement on cross-modal understanding and reasoning of vision and language. VQA is proposed to automatically answer natural language questions with reference to a given image. VQA is challenging, because the reasoning process on a visual domain needs a full understanding of the spatial relationship, semantic concepts, as well as the common sense for a real image. However, most existing approaches jointly embed the abstract low-level visual features and high-level question features to infer answers. These works have limited reasoning ability due to the lack of modeling of the rich spatial context of regions, high-level semantics of images, and knowledge across multiple sources. To solve the challenges, we propose multi-source multi-level attention networks for visual question answering that can benefit both spatial inferences by visual attention on context-aware region representation and reasoning by semantic attention on concepts as well as external knowledge. Indeed, we learn to reason on image representation by question-guided attention at different levels across multiple sources, including region and concept level representation from image source as well as sentence level representation from the external knowledge base. First, we encode region-based middle-level outputs from Convolutional Neural Networks (CNNs) into spatially embedded representation by a multi-directional two-dimensional recurrent neural network and, further, locate the answer-related regions by Multiple Layer Perceptron as visual attention. Second, we generate semantic concepts from high-level semantics in CNNs and select those question-related concepts as concept attention. Third, we query semantic knowledge from the general knowledge base by concepts and selected question-related knowledge as knowledge attention. Finally, we jointly optimize visual attention, concept attention, knowledge attention, and question embedding by a softmax classifier to infer the final answer. Extensive experiments show the proposed approach achieved significant improvement on two very challenging VQA datasets.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2963760481",
    "type": "article"
  },
  {
    "title": "Investigating On-Screen Gamepad Designs for Smartphone-Controlled Video Games",
    "doi": "https://doi.org/10.1145/2808202",
    "publication_date": "2015-10-21",
    "publication_year": 2015,
    "authors": "Matthias Baldauf; Peter Fröhlich; Florence Adegeye; Stefan Suette",
    "corresponding_authors": "",
    "abstract": "On-screen gamepads are increasingly used as controllers for video games on distant screens, yet lack the typical tactile feedback known from hardware controllers. We conducted a comparative lab study to investigate four smartphone gamepads inspired by traditional game controllers and mobile game controls (directional buttons, directional pad, floating joystick, tilt control). The study consisted of both completing a formal control test as well as controlling two popular video games of different genres ( Pac-Man and Super Mario Bros. ). The results indicate that the directional buttons require the most attention of the user, however, work precisely for direction-restricted navigational tasks. Directional pad and joystick showed a similar performance, yet they encourage drifting and unintended operations when the user is focused on the remote screen. While currently unfamiliar to many users, the floating joystick can reduce the glances at the device. Tilt turned out to be not sufficiently precise and quick for the investigated tasks. The article concludes with derived design guidelines with easily realizable measures for typical contexts such as casual gaming at home or spontaneous gaming on public displays.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W1981028852",
    "type": "article"
  },
  {
    "title": "Dissecting User Behaviors for a Simultaneous Live and VoD IPTV System",
    "doi": "https://doi.org/10.1145/2568194",
    "publication_date": "2014-04-01",
    "publication_year": 2014,
    "authors": "Ning Liu; Huajie Cui; S.-H. Gary Chan; Zhipeng Chen; Yirong Zhuang",
    "corresponding_authors": "",
    "abstract": "IPTV services deployed nowadays often consist of both live TV and Video-on-Demand (VoD), offered by the same service provider to the same pool of users over the same managed network. Understanding user behaviors in such a setting is hence an important step for system modelling and optimization. Previous studies on user behavior on video services were on either live TV or VoD. For the first time, we conduct an in-depth large-scale behavior study for IPTV users offering simultaneously live TV and VoD choices at the same time. Our data is from the largest IPTV service provider in China, offering hundreds of live channels and hundreds of thousands of VoD files, with traces covering more than 1.9 million users over a period of 5 months. This large dataset provides us a unique opportunity to cross-compare user viewing behaviors for these services on the same platform, and sheds valuable insights on how users interact with such a simultaneous system. Our results lead to new understanding on IPTV user behaviors which have strong implications on system design. For example, we find that the average holding time for VoD is significantly longer than live TV. live TV users tend to surf more. However, if such channel surfing is discounted, the holding times of both services are not much different. While users in VoD tend to view HD longer, channel popularity for live TV is much less dependent on its video quality. In contrast to some popular assumptions on user interactivity, the transitions among live TV, VoD, and offline modes are far from a Markov model.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2046671874",
    "type": "article"
  },
  {
    "title": "Measuring Collectiveness via Refined Topological Similarity",
    "doi": "https://doi.org/10.1145/2854000",
    "publication_date": "2016-03-03",
    "publication_year": 2016,
    "authors": "Xuelong Li; Mulin Chen; Qi Wang",
    "corresponding_authors": "",
    "abstract": "Crowd system has motivated a surge of interests in many areas of multimedia, as it contains plenty of information about crowd scenes. In crowd systems, individuals tend to exhibit collective behaviors, and the motion of all those individuals is called collective motion. As a comprehensive descriptor of collective motion, collectiveness has been proposed to reflect the degree of individuals moving as an entirety. Nevertheless, existing works mostly have limitations to correctly find the individuals of a crowd system and precisely capture the various relationships between individuals, both of which are essential to measure collectiveness. In this article, we propose a collectiveness-measuring method that is capable of quantifying collectiveness accurately. Our main contributions are threefold: (1) we compute relatively accurate collectiveness by making the tracked feature points represent the individuals more precisely with a point selection strategy; (2) we jointly investigate the spatial-temporal information of individuals and utilize it to characterize the topological relationship between individuals by manifold learning; (3) we propose a stability descriptor to deal with the irregular individuals, which influence the calculation of collectiveness. Intensive experiments on the simulated and real world datasets demonstrate that the proposed method is able to compute relatively accurate collectiveness and keep high consistency with human perception.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2293336468",
    "type": "article"
  },
  {
    "title": "SABR",
    "doi": "https://doi.org/10.1145/3183516",
    "publication_date": "2018-04-25",
    "publication_year": 2018,
    "authors": "Divyashri Bhat; Amr Rizk; Michael Zink; Ralf Steinmetz",
    "corresponding_authors": "",
    "abstract": "State-of-the-art software-defined wide area networks (SD-WANs) provide the foundation for flexible and highly resilient networking. In this work, we design, implement, and evaluate a novel architecture (denoted as SABR) that leverages the benefits of software-defined networking (SDN) to provide network-assisted adaptive bitrate streaming. With clients retaining full control of their streaming algorithms, we clearly show that by this network assistance, both the clients and the content providers benefit significantly in terms of quality of experience (QoE) and content origin offloading. SABR utilizes information on available bandwidths per link and network cache contents to guide video streaming clients with the goal of improving the viewer’s QoE. In addition, SABR uses SDN capabilities to dynamically program flows to optimize the utilization of content delivery network caches. Backed by our study of SDN-assisted streaming, we discuss the change in the requirements for network-to-player APIs that enables flexible video streaming. We illustrate the difficulty of the problem and the impact of SDN-assisted streaming on QoE metrics using various well-established player algorithms. We evaluate SABR together with state-of-the-art dynamic adaptive streaming over HTTP (DASH) quality adaptation algorithms through a series of experiments performed on a real-world, SDN-enabled testbed network with minimal modifications to an existing DASH client. In addition, we compare the performance of different caching strategies in combination with SABR. Our trace-based measurements show the substantial improvement in cache hit rates and QoE metrics in conjunction with SABR indicating a rich design space for jointly optimized SDN-assisted caching architectures for adaptive bitrate video streaming applications.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2802272336",
    "type": "article"
  },
  {
    "title": "Deep Learning–Based Multimedia Analytics",
    "doi": "https://doi.org/10.1145/3279952",
    "publication_date": "2019-01-24",
    "publication_year": 2019,
    "authors": "Wei Zhang; Ting Yao; Shiai Zhu; Abdulmotaleb El Saddik",
    "corresponding_authors": "",
    "abstract": "The multimedia community has witnessed the rise of deep learning–based techniques in analyzing multimedia content more effectively. In the past decade, the convergence of deep-learning and multimedia analytics has boosted the performance of several traditional tasks, such as classification, detection, and regression, and has also fundamentally changed the landscape of several relatively new areas, such as semantic segmentation, captioning, and content generation. This article aims to review the development path of major tasks in multimedia analytics and take a look into future directions. We start by summarizing the fundamental deep techniques related to multimedia analytics, especially in the visual domain, and then review representative high-level tasks powered by recent advances. Moreover, the performance review of popular benchmarks gives a pathway to technology advancement and helps identify both milestone works and future directions.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2913694129",
    "type": "article"
  },
  {
    "title": "Rich Visual and Language Representation with Complementary Semantics for Video Captioning",
    "doi": "https://doi.org/10.1145/3303083",
    "publication_date": "2019-05-31",
    "publication_year": 2019,
    "authors": "Pengjie Tang; Hanli Wang; Qinyu Li",
    "corresponding_authors": "",
    "abstract": "It is interesting and challenging to translate a video to natural description sentences based on the video content. In this work, an advanced framework is built to generate sentences with coherence and rich semantic expressions for video captioning. A long short term memory (LSTM) network with an improved factored way is first developed, which takes the inspiration of LSTM with a conventional factored way and a common practice to feed multi-modal features into LSTM at the first time step for visual description. Then, the incorporation of the LSTM network with the proposed improved factored way and un-factored way is exploited, and a voting strategy is utilized to predict candidate words. In addition, for robust and abstract visual and language representation, residuals are employed to enhance the gradient signals that are learned from the residual network (ResNet), and a deeper LSTM network is constructed. Furthermore, three convolutional neural network based features extracted from GoogLeNet, ResNet101, and ResNet152, are fused to catch more comprehensive and complementary visual information. Experiments are conducted on two benchmark datasets, including MSVD and MSR-VTT2016, and competitive performances are obtained by the proposed techniques as compared to other state-of-the-art methods.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2996817764",
    "type": "article"
  },
  {
    "title": "An Evaluation of Tile Selection Methods for Viewport-Adaptive Streaming of 360-Degree Video",
    "doi": "https://doi.org/10.1145/3373359",
    "publication_date": "2020-02-29",
    "publication_year": 2020,
    "authors": "Duc V. Nguyen; Huyen T. T. Tran; Truong Cong Thang",
    "corresponding_authors": "",
    "abstract": "360-degree video has become increasingly popular nowadays. For effective transmission of bandwidth-intensive 360-degree video over networks, viewport-adaptive streaming has been introduced. In this article, we evaluate, for the first time, ten existing methods to understand the effectiveness of tile-based viewport adaptive streaming of 360-degree video. Experimental results show that tile-based methods can improve the average V-PSNR by up to 4.3 dB compared to a non-tiled method under low delay settings. Here, the V-PSNR is computed as the peak signal-to-noise ratio of the adapted viewport compared to the corresponding origin viewport. Also, different methods show different tradeoffs between average viewport quality and viewport quality variations. Especially, the performances of most tile-based methods decrease quickly as the segment duration and/or buffer size increase for the content with no main focus. Even, under long delay settings like HTTP Adaptive Streaming, it is found that the simple non-tiled method appears to be the best one. For the content with a strong viewing focus, it is found that the tile-based methods are less influenced by the segment duration and the buffer size. In addition, a comparison of the performances of the tile selection methods using two popular viewport estimation methods is conducted. It is interesting that there is only little difference found in performances of tile selection methods. The findings of this study are useful for service providers to make decisions on deployment of streaming solutions.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W3010862052",
    "type": "article"
  },
  {
    "title": "Image Captioning with a Joint Attention Mechanism by Visual Concept Samples",
    "doi": "https://doi.org/10.1145/3394955",
    "publication_date": "2020-07-05",
    "publication_year": 2020,
    "authors": "Jin Yuan; Lei Zhang; Songrui Guo; Yi Xiao; Zhiyong Li",
    "corresponding_authors": "",
    "abstract": "The attention mechanism has been established as an effective method for generating caption words in image captioning; it explores one noticed subregion in an image to predict a related caption word. However, even though the attention mechanism could offer accurate subregions to train a model, the learned captioner may predict wrong, especially for visual concept words, which are the most important parts to understand an image. To tackle the preceding problem, in this article we propose Visual Concept Enhanced Captioner, which employs a joint attention mechanism with visual concept samples to strengthen prediction abilities for visual concepts in image captioning. Different from traditional attention approaches that adopt one LSTM to explore one noticed subregion each time, Visual Concept Enhanced Captioner introduces multiple virtual LSTMs in parallel to simultaneously receive multiple subregions from visual concept samples. Then, the model could update parameters by jointly exploring these subregions according to a composite loss function. Technically, this joint learning is helpful in finding the common characters of a visual concept, and thus it enhances the prediction accuracy for visual concepts. Moreover, by integrating diverse visual concept samples from different domains, our model can be extended to bridge visual bias in cross-domain learning for image captioning, which saves the cost for labeling captions. Extensive experiments have been conducted on two image datasets (MSCOCO and Flickr30K), and superior results are reported when comparing to state-of-the-art approaches. It is impressive that our approach could significantly increase BLUE-1 and F1 scores, which demonstrates an accuracy improvement for visual concepts in image captioning.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W3038593179",
    "type": "article"
  },
  {
    "title": "Meta-path Augmented Sequential Recommendation with Contextual Co-attention Network",
    "doi": "https://doi.org/10.1145/3382180",
    "publication_date": "2020-05-22",
    "publication_year": 2020,
    "authors": "Xiaowen Huang; Shengsheng Qian; Quan Fang; Jitao Sang; Changsheng Xu",
    "corresponding_authors": "",
    "abstract": "It is critical to comprehensively and efficiently learn user preferences for an effective sequential recommender system. Existing sequential recommendation methods mainly focus on modeling local preference from users’ historical behaviors, which largely ignore the global context information from the heterogeneous information network. This prevents a comprehensive user preference representation. To address these issues, we propose a joint learning approach to incorporate global context with local preferences efficiently. The proposed approach introduces meta-paths from a heterogeneous information network to capture the global context information, and the position-based self-attention mechanism is adopted to model the local preference representation efficiently. Compared with the methods that only consider the local preference, our proposed method takes the advantages of incorporating global context information, which extracts structural features that captures relevant semantics to construct users’ global preference representation for the sequential recommendation. We further adopt a co-attention mechanism to model complex interactions between global context and users’ historical behaviors for better user representations. Quantitative and qualitative experimental evaluations are conducted on nine large-scale Amazon datasets and a multi-modal Zhihu dataset. The promising results demonstrate the effectiveness of the proposed model.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W3087006136",
    "type": "article"
  },
  {
    "title": "GuessUNeed",
    "doi": "https://doi.org/10.1145/3410441",
    "publication_date": "2020-11-30",
    "publication_year": 2020,
    "authors": "Zhongying Zhao; Yonghao Yang; Chao Li; Liqiang Nie",
    "corresponding_authors": "",
    "abstract": "Massive Open Online Courses, offering millions of high-quality courses from prestigious universities and prominent experts, are picking up momentum in popularity. Although users enrolling on MOOCs have free access to abundant knowledge, they may easily get overwhelmed by information overload. Therefore, there is a need of recommending technology as a fundamental and well-accepted effective solution. However, differing from many other online recommendations, recommending courses to users on MOOCs faces two challenges. First, users’ knowledge background differs, so does their purpose of learning. Second, online courses are not independent but intertwined with prerequisite relations. Therefore, it is necessary to take these two challenges into account when designing a recommending method. To tackle this issue, in this article, we first propose two algorithms for extracting concept-level and course-level prerequisite relations. We then present the recommending method GuessUNeed based on neural attention network and course prerequisite relation embeddings. The experimental results on real-world datasets demonstrate the superiority of the proposed GuessUNeed method.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W3111620502",
    "type": "article"
  },
  {
    "title": "Secure Nonlocal Denoising in Outsourced Images",
    "doi": "https://doi.org/10.1145/2886777",
    "publication_date": "2016-03-08",
    "publication_year": 2016,
    "authors": "Xianjun Hu; Weiming Zhang; Ke Li; Honggang Hu; Nenghai Yu",
    "corresponding_authors": "",
    "abstract": "Signal processing in the encrypted domain becomes a desired technique to protect privacy of outsourced data in cloud. In this article, we propose a double-cipher scheme to implement nonlocal means (NLM) denoising in encrypted images. In this scheme, one ciphertext is generated by the Paillier scheme, which enables the mean filter, and the other is obtained by a privacy-preserving transform, which enables the nonlocal search. By the privacy-preserving transform, the cloud server can search the similar pixel blocks in the ciphertexts with the same speed as in the plaintexts; thus, the proposed method can be executed fast. To enhance the security, we randomly permutate both ciphertexts. To reduce the denoising complexity caused by random permutation, a random NLM method is exploited in the encrypted domain. The experimental results show that the quality of denoised images in the encrypted domain is comparable to that obtained in the plain domain.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2298286999",
    "type": "article"
  },
  {
    "title": "Statistical Early Termination and Early Skip Models for Fast Mode Decision in HEVC INTRA Coding",
    "doi": "https://doi.org/10.1145/3321510",
    "publication_date": "2019-07-29",
    "publication_year": 2019,
    "authors": "Yun Zhang; Na Li; Sam Kwong; Gangyi Jiang; Huanqiang Zeng",
    "corresponding_authors": "",
    "abstract": "In this article, statistical Early Termination (ET) and Early Skip (ES) models are proposed for fast Coding Unit (CU) and prediction mode decision in HEVC INTRA coding, in which three categories of ET and ES sub-algorithms are included. First, the CU ranges of the current CU are recursively predicted based on the texture and CU depth of the spatial neighboring CUs. Second, the statistical model based ET and ES schemes are proposed and applied to optimize the CU and INTRA prediction mode decision, in which the coding complexities over different decision layers are jointly minimized subject to acceptable rate-distortion degradation. Third, the mode correlations among the INTRA prediction modes are exploited to early terminate the full rate-distortion optimization in each CU decision layer. Extensive experiments are performed to evaluate the coding performance of each sub-algorithm and the overall algorithm. Experimental results reveal that the overall proposed algorithm can achieve 45.47% to 74.77%, and 58.09% on average complexity reduction, while the overall Bjøntegaard delta bit rate increase and Bjøntegaard delta peak signal-to-noise ratio degradation are 2.29% and −0.11 dB, respectively.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2965924161",
    "type": "article"
  },
  {
    "title": "Moving Foreground-Aware Visual Attention and Key Volume Mining for Human Action Recognition",
    "doi": "https://doi.org/10.1145/3321511",
    "publication_date": "2019-08-08",
    "publication_year": 2019,
    "authors": "Junxuan Zhang; Haifeng Hu; Xinlong Lu",
    "corresponding_authors": "",
    "abstract": "Recently, many deep learning approaches have shown remarkable progress on human action recognition. However, it remains unclear how to extract the useful information in videos since only video-level labels are available in the training phase. To address this limitation, many efforts have been made to improve the performance of action recognition by applying the visual attention mechanism in the deep learning model. In this article, we propose a novel deep model called Moving Foreground Attention (MFA) that enhances the performance of action recognition by guiding the model to focus on the discriminative foreground targets. In our work, MFA detects the moving foreground through a proposed variance-based algorithm. Meanwhile, an unsupervised proposal is utilized to mine the action-related key volumes and generate corresponding correlation scores. Based on these scores, a newly proposed stochastic-out scheme is exploited to train the MFA. Experiment results show that action recognition performance can be significantly improved by using our proposed techniques, and our model achieves state-of-the-art performance on UCF101 and HMDB51.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2966879646",
    "type": "article"
  },
  {
    "title": "Video Question Answering via Knowledge-based Progressive Spatial-Temporal Attention Network",
    "doi": "https://doi.org/10.1145/3321505",
    "publication_date": "2019-04-30",
    "publication_year": 2019,
    "authors": "Weike Jin; Zhou Zhao; Yimeng Li; Jie Li; Jun Xiao; Yueting Zhuang",
    "corresponding_authors": "",
    "abstract": "Visual Question Answering (VQA) is a challenging task that has gained increasing attention from both the computer vision and the natural language processing communities in recent years. Given a question in natural language, a VQA system is designed to automatically generate the answer according to the referenced visual content. Though there recently has been much intereset in this topic, the existing work of visual question answering mainly focuses on a single static image, which is only a small part of the dynamic and sequential visual data in the real world. As a natural extension, video question answering (VideoQA) is less explored. Because of the inherent temporal structure in the video, the approaches of ImageQA may be ineffectively applied to video question answering. In this article, we not only take the spatial and temporal dimension of video content into account but also employ an external knowledge base to improve the answering ability of the network. More specifically, we propose a knowledge-based progressive spatial-temporal attention network to tackle this problem. We obtain both objects and region features of the video frames from a region proposal network. The knowledge representation is generated by a word-level attention mechanism using the comment information of each object that is extracted from DBpedia. Then, we develop a question-knowledge-guided progressive spatial-temporal attention network to learn the joint video representation for video question answering task. We construct a large-scale video question answering dataset. The extensive experiments based on two different datasets validate the effectiveness of our method.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2969663619",
    "type": "article"
  },
  {
    "title": "Sequential Cross-Modal Hashing Learning via Multi-scale Correlation Mining",
    "doi": "https://doi.org/10.1145/3356338",
    "publication_date": "2019-11-30",
    "publication_year": 2019,
    "authors": "Zhaoda Ye; Yuxin Peng",
    "corresponding_authors": "",
    "abstract": "Cross-modal hashing aims to map heterogeneous multimedia data into a common Hamming space through hash function, and achieves fast and flexible cross-modal retrieval. Most existing cross-modal hashing methods learn hash function by mining the correlation among multimedia data, but ignore the important property of multimedia data: Each modality of multimedia data has features of different scales, such as texture, object, and scene features in the image, which can provide complementary information for boosting retrieval task. The correlations among the multi-scale features are more abundant than the correlations between single features of multimedia data, which reveal finer underlying structures of the multimedia data and can be used for effective hashing function learning. Therefore, we propose the M ulti-scale C orrelation S equential C ross-modal H ashing ( MCSCH ) approach, and its main contributions can be summarized as follows: (1) Multi-scale feature guided sequential hashing learning method is proposed to share the information from features of different scales through an RNN-based network and generate the hash codes sequentially. The features of different scales are used to guide the hash codes generation, which can enhance the diversity of the hash codes and weaken the influence of errors in specific features, such as false object features caused by occlusion. (2) Multi-scale correlation mining strategy is proposed to align the features of different scales in different modalities and mine the correlations among aligned features. These correlations reveal the finer underlying structure of multimedia data and can help to boost the hash function learning. (3) Correlation evaluation network evaluates the importance of the correlations to select the worthwhile correlations, and increases the impact of these correlations for hash function learning. Experiments on two widely-used 2-media datasets and a 5-media dataset demonstrate the effectiveness of our proposed MCSCH approach.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W3014697397",
    "type": "article"
  },
  {
    "title": "Requet",
    "doi": "https://doi.org/10.1145/3394498",
    "publication_date": "2020-04-30",
    "publication_year": 2020,
    "authors": "Craig Gutterman; Katherine Guo; Sarthak Arora; Trey Gilliland; Xiaoyang Wang; Les Wu; Ethan Katz-Bassett; Gil Zussman",
    "corresponding_authors": "",
    "abstract": "As video traffic dominates the Internet, it is important for operators to detect video quality of experience (QoE) to ensure adequate support for video traffic. With wide deployment of end-to-end encryption, traditional deep packet inspection--based traffic monitoring approaches are becoming ineffective. This poses a challenge for network operators to monitor user QoE and improve upon their experience. To resolve this issue, we develop and present a system for RE al-time QU ality of experience metric detection for &lt;underline&gt; E &lt;/underline&gt;ncrypted T raffic— Requet —which is suitable for network middlebox deployment. Requet uses a detection algorithm that we develop to identify video and audio chunks from the IP headers of encrypted traffic. Features extracted from the chunk statistics are used as input to a machine learning algorithm to predict QoE metrics, specifically buffer warning (low buffer, high buffer), video state (buffer increase, buffer decay, steady, stall), and video resolution. We collect a large YouTube dataset consisting of diverse video assets delivered over various WiFi and LTE network conditions to evaluate the performance. We compare Requet with a baseline system based on previous work and show that Requet outperforms the baseline system in accuracy of predicting buffer low warning, video state, and video resolution by 1.12×, 1.53×, and 3.14×, respectively.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W3041221479",
    "type": "article"
  },
  {
    "title": "SDN Enabled QoE and Security Framework for Multimedia Applications in 5G Networks",
    "doi": "https://doi.org/10.1145/3377390",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "Prabhakar Krishnan; Kurunandan Jain; Pramod George Jose; Krishnashree Achuthan; Rajkumar Buyya",
    "corresponding_authors": "",
    "abstract": "The technologies for real-time multimedia transmission and immersive 3D gaming applications are rapidly emerging, posing challenges in terms of performance, security, authentication, data privacy, and encoding. The communication channel for these multimedia applications must be secure and reliable from network attack vectors and data-contents must employ strong encryption to preserve privacy and confidentiality. Towards delivering secure multimedia application environment for 5G networks, we propose an SDN/NFV (Software-Defined-Networking/Network-Function-Virtualization) framework called STREK , which attempts to deliver highly adaptable Quality-of-Experience (QoE), Security, and Authentication functions for multi-domain Cloud to Edge networks. The STREK architecture consists of a holistic SDNFV dataplane, NFV service-chaining and network slicing, a lightweight adaptable hybrid cipher scheme called TREK, and an open RESTful API for applications to deploy custom policies at runtime for multimedia services. For multi-domain/small-cell deployments, the key-generation scheme is dynamic at flow/session-level, and the handover authentication scheme uses a novel method to exchange security credentials with the Access Points (APs) of neighborhood cells. This scheme is designed to improve authentication function during handover with low overhead, delivering the 5G ultra-low latency requirements. We present the experiments with both software and hardware-based implementations and compare our solution with popular lightweight cryptographic solutions, standard open source software, and SDN-based research proposals for 5G multimedia. In the microbenchmarks, STREK achieves smaller hardware, low overhead, low computation, higher attack resistance, and offers better network performance for multimedia streaming applications. In real-time multimedia use-cases, STREK shows greater level of quality distortion for multimedia contents with minimal encryption bitrate overhead to deliver data confidentiality, immunity to common cryptanalysis, and significant resistance to communication channel attacks, in the context of low-latency 5G networks.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W3086709401",
    "type": "article"
  },
  {
    "title": "Knowledge-driven Egocentric Multimodal Activity Recognition",
    "doi": "https://doi.org/10.1145/3409332",
    "publication_date": "2020-11-30",
    "publication_year": 2020,
    "authors": "Yi Huang; Xiaoshan Yang; Junyu Gao; Jitao Sang; Changsheng Xu",
    "corresponding_authors": "",
    "abstract": "Recognizing activities from egocentric multimodal data collected by wearable cameras and sensors, is gaining interest, as multimodal methods always benefit from the complementarity of different modalities. However, since high-dimensional videos contain rich high-level semantic information while low-dimensional sensor signals describe simple motion patterns of the wearer, the large modality gap between the videos and the sensor signals raises a challenge for fusing the raw data. Moreover, the lack of large-scale egocentric multimodal datasets due to the cost of data collection and annotation processes makes another challenge for employing complex deep learning models. To jointly deal with the above two challenges, we propose a knowledge-driven multimodal activity recognition framework that exploits external knowledge to fuse multimodal data and reduce the dependence on large-scale training samples. Specifically, we design a dual-GCLSTM (Graph Convolutional LSTM) and a multi-layer GCN (Graph Convolutional Network) to collectively model the relations among activities and intermediate objects. The dual-GCLSTM is designed to fuse temporal multimodal features with top-down relation-aware guidance. In addition, we apply a co-attention mechanism to adaptively attend to the features of different modalities at different timesteps. The multi-layer GCN aims to learn relation-aware classifiers of activity categories. Experimental results on three publicly available egocentric multimodal datasets show the effectiveness of the proposed model.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W3111490429",
    "type": "article"
  },
  {
    "title": "Part-based Structured Representation Learning for Person Re-identification",
    "doi": "https://doi.org/10.1145/3412384",
    "publication_date": "2020-11-30",
    "publication_year": 2020,
    "authors": "Yaoyu Li; Hantao Yao; Tianzhu Zhang; Changsheng Xu",
    "corresponding_authors": "",
    "abstract": "Person re-identification aims to match person of interest under non-overlapping camera views. Therefore, how to generate a robust and discriminative representation is crucial for person re-identification. Mining local clues from human body parts to describe pedestrians has been extensively studied in existing methods. However, existing methods locate human body parts coarsely and do not consider the relations among different local parts. To address the above problem, we propose a Part-based Structured Representation Learning (PSRL) for better exploiting local clues to improve the person representation. There are two important modules in our architecture: Local Semantic Feature Extraction and Structured Person Representation Learning. The Local Semantic Feature Extraction module is designed to extract local features from human body semantic regions. After obtaining the local features, the Structured Person Representation Learning is proposed to fuse the local features by considering the person structure. To model the underlying person structure, a graph convolutional network is employed to capture the relations of different semantic regions. The generated structured feature encodes underlying person structure information, and local semantic feature can solve the misalignment problem caused by pose variations in feature matching. By combining them together, we can improve the descriptive ability of the generated representation. Extensive evaluations on four standard benchmarks show that our proposed method achieves competitive performance against state-of-the-art methods.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W3112116512",
    "type": "article"
  },
  {
    "title": "Motion-Aware Structured Matrix Factorization for Foreground Detection in Complex Scenes",
    "doi": "https://doi.org/10.1145/3407188",
    "publication_date": "2020-11-30",
    "publication_year": 2020,
    "authors": "Lin Zhu; Xiurong Jiang; Jianing Li; Yuanhong Hao; Yonghong Tian",
    "corresponding_authors": "",
    "abstract": "Foreground detection is one of the key steps in computer vision applications. Many foreground and background models have been proposed and achieved promising performance in static scenes. However, due to challenges such as dynamic background, irregular movement, and noise, most algorithms degrade sharply in complex scenes. To address the problem, we propose a motion-aware structured matrix factorization approach (MSMF), which integrates the structural and spatiotemporal motion information into a unified sparse-low-rank matrix factorization framework. Technologically, it has three main contributions: First, a variant of structured sparsity-inducing norm is proposed to constrain both structure and sparsity of foreground. The model is robust to the statistical variability of the underlying foreground pixels in complex scenes. Second, to capture the ambiguous pixels, a spatiotemporal cube-based motion trajectory is extracted for assisting matrix factorization. Finally, to solve the optimization problem of structured matrix factorization, we develop an augmented Lagrange multiplier method with the alternating direction strategy and Douglas-Rachford monotone operator splitting algorithm. Experiments demonstrate that the proposed approach achieves impressive performance in separating irregular moving foreground while suppressing the dynamic background and the noise, and outperforms some state-of-the-art algorithms.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W3113302257",
    "type": "article"
  },
  {
    "title": "Textual Entailment--Based Figure Summarization for Biomedical Articles",
    "doi": "https://doi.org/10.1145/3357334",
    "publication_date": "2020-01-31",
    "publication_year": 2020,
    "authors": "Naveen Saini; Sriparna Saha; Pushpak Bhattacharyya; Himanshu Tuteja",
    "corresponding_authors": "",
    "abstract": "This article proposes a novel unsupervised approach (FigSum++) for automatic figure summarization in biomedical scientific articles using a multi-objective evolutionary algorithm. The problem is treated as an optimization problem where relevant sentences in the summary for a given figure are selected based on various sentence scoring features (or objective functions), such as the textual entailment score between sentences in the summary and a figure’s caption, the number of sentences referring to that figure, semantic similarity between sentences and a figure’s caption, and the number of overlapping words between sentences and a figure’s caption. These objective functions are optimized simultaneously using multi-objective binary differential evolution (MBDE). MBDE consists of a set of solutions, and each solution represents a subset of sentences to be selected in the summary. MBDE generally uses a single differential evolution variant, but in the current study, an ensemble of two different differential evolution variants measuring diversity among solutions and convergence toward global optimal solution, respectively, is employed for efficient search. Usually, in any summarization system, diversity among sentences (called anti-redundancy ) in the summary is a very critical feature, and it is calculated in terms of similarity (like cosine similarity) among sentences. In this article, a new way of measuring diversity in terms of textual entailment is proposed. To represent the sentences of the article in the form of numeric vectors, the recently proposed BioBERT pre-trained language model in biomedical text mining is utilized. An ablation study has also been presented to determine the importance of different objective functions. For evaluation of the proposed technique, two benchmark biomedical datasets containing 91 and 84 figures are considered. Our proposed system obtains 5% and 11% improvements in terms of the F -measure metric over two datasets, compared to the state-of-the-art unsupervised methods.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W3016294469",
    "type": "article"
  },
  {
    "title": "Spatio-temporal Saliency-based Motion Vector Refinement for Frame Rate Up-conversion",
    "doi": "https://doi.org/10.1145/3382506",
    "publication_date": "2020-05-22",
    "publication_year": 2020,
    "authors": "Jiale He; Gaobo Yang; Xin Liu; Xiangling Ding",
    "corresponding_authors": "",
    "abstract": "A spatio-temporal saliency-based frame rate up-conversion (FRUC) approach is proposed, which achieves better quality of interpolated frames and invalidates existing texture variation-based FRUC detectors. A spatio-temporal saliency model is designed to select salient frames. After obtaining initial motion vector field by texture- and color-based bilateral motion estimation, two motion vector refining (MVR) schemes are adopted for high and low saliency frames to hierarchically refine the motion vectors, respectively. To produce high-quality interpolated frames, image enhancement are performed for salient frames after frame interpolation. Due to distinct MVR schemes, there are different degrees of texture information in interpolated frames. Some edge and texture information is supplemented into salient frames as post-processing, which can invalidate existing texture variation-based FRUC detectors. Experimental results show that the proposed approach outperforms state-of-the-art works in both objective and subjective qualities of interpolated frames, and achieves the purpose of FRUC anti-forensics.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W3034232500",
    "type": "article"
  },
  {
    "title": "Do Users Behave Similarly in VR? Investigation of the User Influence on the System Design",
    "doi": "https://doi.org/10.1145/3381846",
    "publication_date": "2020-05-22",
    "publication_year": 2020,
    "authors": "Silvia Rossi; Cagri Ozcinar; Aljoša Smolić; Laura Toni",
    "corresponding_authors": "",
    "abstract": "With the overarching goal of developing user-centric Virtual Reality (VR) systems, a new wave of studies focused on understanding how users interact in VR environments has recently emerged. Despite the intense efforts, however, current literature still does not provide the right framework to fully interpret and predict users’ trajectories while navigating in VR scenes. This work advances the state-of-the-art on both the study of users’ behaviour in VR and the user-centric system design. In more detail, we complement current datasets by presenting a publicly available dataset that provides navigation trajectories acquired for heterogeneous omnidirectional videos and different viewing platforms—namely, head-mounted display, tablet, and laptop. We then present an exhaustive analysis on the collected data to better understand navigation in VR across users, content, and, for the first time, across viewing platforms. The novelty lies in the user-affinity metric, proposed in this work to investigate users’ similarities when navigating within the content. The analysis reveals useful insights on the effect of device and content on the navigation, which could be precious considerations from the system design perspective. As a case study of the importance of studying users’ behaviour when designing VR systems, we finally propose a user-centric server optimisation. We formulate an integer linear program that seeks the best stored set of omnidirectional content that minimises encoding and storage cost while maximising the user’s experience. This is posed while taking into account network dynamics, type of video content, and also user population interactivity. Experimental results prove that our solution outperforms common company recommendations in terms of experienced quality but also in terms of encoding and storage, achieving a savings up to 70%. More importantly, we highlight a strong correlation between the storage cost and the user-affinity metric, showing the impact of the latter in the system architecture design.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W3046007855",
    "type": "article"
  },
  {
    "title": "Am I Done? Predicting Action Progress in Videos",
    "doi": "https://doi.org/10.1145/3402447",
    "publication_date": "2020-11-30",
    "publication_year": 2020,
    "authors": "Federico Becattini; Tiberio Uricchio; Lorenzo Seidenari; Lamberto Ballan; Alberto Del Bimbo",
    "corresponding_authors": "",
    "abstract": "In this article, we deal with the problem of predicting action progress in videos. We argue that this is an extremely important task, since it can be valuable for a wide range of interaction applications. To this end, we introduce a novel approach, named ProgressNet, capable of predicting when an action takes place in a video, where it is located within the frames, and how far it has progressed during its execution. To provide a general definition of action progress, we ground our work in the linguistics literature, borrowing terms and concepts to understand which actions can be the subject of progress estimation. As a result, we define a categorization of actions and their phases. Motivated by the recent success obtained from the interaction of Convolutional and Recurrent Neural Networks, our model is based on a combination of the Faster R-CNN framework, to make framewise predictions, and LSTM networks, to estimate action progress through time. After introducing two evaluation protocols for the task at hand, we demonstrate the capability of our model to effectively predict action progress on the UCF-101 and J-HMDB datasets.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W3113321693",
    "type": "article"
  },
  {
    "title": "Automatic Comic Generation with Stylistic Multi-page Layouts and Emotion-driven Text Balloon Generation",
    "doi": "https://doi.org/10.1145/3440053",
    "publication_date": "2021-05-29",
    "publication_year": 2021,
    "authors": "Xin Yang; Zongliang Ma; Letian Yu; Ying Cao; Baocai Yin; Xiaopeng Wei; Qiang Zhang; Rynson W. H. Lau",
    "corresponding_authors": "",
    "abstract": "In this article, we propose a fully automatic system for generating comic books from videos without any human intervention. Given an input video along with its subtitles, our approach first extracts informative keyframes by analyzing the subtitles and stylizes keyframes into comic-style images. Then, we propose a novel automatic multi-page layout framework that can allocate the images across multiple pages and synthesize visually interesting layouts based on the rich semantics of the images (e.g., importance and inter-image relation). Finally, as opposed to using the same type of balloon as in previous works, we propose an emotion-aware balloon generation method to create different types of word balloons by analyzing the emotion of subtitles and audio. Our method is able to vary balloon shapes and word sizes in balloons in response to different emotions, leading to more enriched reading experience. Once the balloons are generated, they are placed adjacent to their corresponding speakers via speaker detection. Our results show that our method, without requiring any user inputs, can generate high-quality comic pages with visually rich layouts and balloons. Our user studies also demonstrate that users prefer our generated results over those by state-of-the-art comic generation systems.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W3164212494",
    "type": "article"
  },
  {
    "title": "Single-shot Semantic Matching Network for Moment Localization in Videos",
    "doi": "https://doi.org/10.1145/3441577",
    "publication_date": "2021-07-22",
    "publication_year": 2021,
    "authors": "Xinfang Liu; Xiushan Nie; Junya Teng; Lian Li; Yilong Yin",
    "corresponding_authors": "",
    "abstract": "Moment localization in videos using natural language refers to finding the most relevant segment from videos given a natural language query. Most of the existing methods require video segment candidates for further matching with the query, which leads to extra computational costs, and they may also not locate the relevant moments under any length evaluated. To address these issues, we present a lightweight single-shot semantic matching network (SSMN) to avoid the complex computations required to match the query and the segment candidates, and the proposed SSMN can locate moments of any length theoretically. Using the proposed SSMN, video features are first uniformly sampled to a fixed number, while the query sentence features are generated and enhanced by GloVe, long-term short memory (LSTM), and soft-attention modules. Subsequently, the video features and sentence features are fed to an enhanced cross-modal attention model to mine the semantic relationships between vision and language. Finally, a score predictor and a location predictor are designed to locate the start and stop indexes of the query moment. We evaluate the proposed method on two benchmark datasets and the experimental results demonstrate that SSMN outperforms state-of-the-art methods in both precision and efficiency.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W3183800903",
    "type": "article"
  },
  {
    "title": "Secure Chaff-less Fuzzy Vault for Face Identification Systems",
    "doi": "https://doi.org/10.1145/3442198",
    "publication_date": "2021-07-22",
    "publication_year": 2021,
    "authors": "Xingbo Dong; Soohyong Kim; Zhe Jin; Jung Yeon Hwang; Sangrae Cho; Andrew Beng Jin Teoh",
    "corresponding_authors": "",
    "abstract": "Biometric cryptosystems such as fuzzy vaults represent one of the most popular approaches for secret and biometric template protection. However, they are solely designed for biometric verification, where the user is required to input both identity credentials and biometrics. Several practical questions related to the implementation of biometric cryptosystems remain open, especially in regard to biometric template protection. In this article, we propose a face cryptosystem for identification (FCI) in which only biometric input is needed. Our FCI is composed of a one-to-N search subsystem for template protection and a one-to-one match chaff-less fuzzy vault (CFV) subsystem for secret protection. The first subsystem stores N facial features, which are protected by index-of-maximum (IoM) hashing, enhanced by a fusion module for search accuracy. When a face image of the user is presented, the subsystem returns the top k matching scores and activates the corresponding vaults in the CFV subsystem. Then, one-to-one matching is applied to the k vaults based on the probe face, and the identifier or secret associated with the user is retrieved from the correct matched vault. We demonstrate that coupling between the IoM hashing and the CFV resolves several practical issues related to fuzzy vault schemes. The FCI system is evaluated on three large-scale public unconstrained face datasets (LFW, VGG2, and IJB-C) in terms of its accuracy, computation cost, template protection criteria, and security.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W3186503362",
    "type": "article"
  },
  {
    "title": "Visual Semantic-Based Representation Learning Using Deep CNNs for Scene Recognition",
    "doi": "https://doi.org/10.1145/3436494",
    "publication_date": "2021-05-11",
    "publication_year": 2021,
    "authors": "Shikha Gupta; Krishan Sharma; Dileep Aroor Dinesh; Veena Thenkanidiyoor",
    "corresponding_authors": "",
    "abstract": "In this work, we address the task of scene recognition from image data. A scene is a spatially correlated arrangement of various visual semantic contents also known as concepts, e.g., “chair,” “car,” “sky,” etc. Representation learning using visual semantic content can be regarded as one of the most trivial ideas as it mimics the human behavior of perceiving visual information. Semantic multinomial (SMN) representation is one such representation that captures semantic information using posterior probabilities of concepts. The core part of obtaining SMN representation is the building of concept models. Therefore, it is necessary to have ground-truth (true) concept labels for every concept present in an image. Moreover, manual labeling of concepts is practically not feasible due to the large number of images in the dataset. To address this issue, we propose an approach for generating pseudo-concepts in the absence of true concept labels. We utilize the pre-trained deep CNN-based architectures where activation maps (filter responses) from convolutional layers are considered as initial cues to the pseudo-concepts. The non-significant activation maps are removed using the proposed filter-specific threshold-based approach that leads to the removal of non-prominent concepts from data. Further, we propose a grouping mechanism to group the same pseudo-concepts using subspace modeling of filter responses to achieve a non-redundant representation. Experimental studies show that generated SMN representation using pseudo-concepts achieves comparable results for scene recognition tasks on standard datasets like MIT-67 and SUN-397 even in the absence of true concept labels.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W3162529291",
    "type": "article"
  },
  {
    "title": "Synthesising Privacy by Design Knowledge Toward Explainable Internet of Things Application Designing in Healthcare",
    "doi": "https://doi.org/10.1145/3434186",
    "publication_date": "2021-06-14",
    "publication_year": 2021,
    "authors": "Lamya Alkhariji; Nada Alhirabi; Mansour Naser Alraja; Mahmoud Barhamgi; Omer Rana; Charith Perera",
    "corresponding_authors": "",
    "abstract": "Privacy by Design (PbD) is the most common approach followed by software developers who aim to reduce risks within their application designs, yet it remains commonplace for developers to retain little conceptual understanding of what is meant by privacy. A vision is to develop an intelligent privacy assistant to whom developers can easily ask questions to learn how to incorporate different privacy-preserving ideas into their IoT application designs. This article lays the foundations toward developing such a privacy assistant by synthesising existing PbD knowledge to elicit requirements. It is believed that such a privacy assistant should not just prescribe a list of privacy-preserving ideas that developers should incorporate into their design. Instead, it should explain how each prescribed idea helps to protect privacy in a given application design context—this approach is defined as “Explainable Privacy.” A total of 74 privacy patterns were analysed and reviewed using ten different PbD schemes to understand how each privacy pattern is built and how each helps to ensure privacy. Due to page limitations, we have presented a detailed analysis in Reference [3]. In addition, different real-world Internet of Things (IoT) use-cases, including a healthcare application, were used to demonstrate how each privacy pattern could be applied to a given application design. By doing so, several knowledge engineering requirements were identified that need to be considered when developing a privacy assistant. It was also found that, when compared to other IoT application domains, privacy patterns can significantly benefit healthcare applications. In conclusion, this article identifies the research challenges that must be addressed if one wishes to construct an intelligent privacy assistant that can truly augment software developers’ capabilities at the design phase.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W3171991246",
    "type": "article"
  },
  {
    "title": "MILL: Channel Attention–based Deep Multiple Instance Learning for Landslide Recognition",
    "doi": "https://doi.org/10.1145/3454009",
    "publication_date": "2021-06-21",
    "publication_year": 2021,
    "authors": "Xiaochuan Tang; Mingzhe Liu; Hao Zhong; Yuanzhen Ju; Weile Li; Qiang Xu",
    "corresponding_authors": "",
    "abstract": "Landslide recognition is widely used in natural disaster risk management. Traditional landslide recognition is mainly conducted by geologists, which is accurate but inefficient. This article introduces multiple instance learning (MIL) to perform automatic landslide recognition. An end-to-end deep convolutional neural network is proposed, referred to as Multiple Instance Learning–based Landslide classification (MILL). First, MILL uses a large-scale remote sensing image classification dataset to build pre-train networks for landslide feature extraction. Second, MILL extracts instances and assign instance labels without pixel-level annotations. Third, MILL uses a new channel attention–based MIL pooling function to map instance-level labels to bag-level label. We apply MIL to detect landslides in a loess area. Experimental results demonstrate that MILL is effective in identifying landslides in remote sensing images.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W3174836611",
    "type": "article"
  },
  {
    "title": "Transform, Warp, and Dress: A New Transformation-guided Model for Virtual Try-on",
    "doi": "https://doi.org/10.1145/3491226",
    "publication_date": "2022-02-16",
    "publication_year": 2022,
    "authors": "Matteo Fincato; Marcella Cornia; Federico Landi; Fabio Cesari; Rita Cucchiara",
    "corresponding_authors": "",
    "abstract": "Virtual try-on has recently emerged in computer vision and multimedia communities with the development of architectures that can generate realistic images of a target person wearing a custom garment. This research interest is motivated by the large role played by e-commerce and online shopping in our society. Indeed, the virtual try-on task can offer many opportunities to improve the efficiency of preparing fashion catalogs and to enhance the online user experience. The problem is far to be solved: current architectures do not reach sufficient accuracy with respect to manually generated images and can only be trained on image pairs with a limited variety. Existing virtual try-on datasets have two main limits: they contain only female models, and all the images are available only in low resolution. This not only affects the generalization capabilities of the trained architectures but makes the deployment to real applications impractical. To overcome these issues, we present Dress Code , a new dataset for virtual try-on that contains high-resolution images of a large variety of upper-body clothes and both male and female models. Leveraging this enriched dataset, we propose a new model for virtual try-on capable of generating high-quality and photo-realistic images using a three-stage pipeline. The first two stages perform two different geometric transformations to warp the desired garment and make it fit into the target person’s body pose and shape. Then, we generate the new image of that same person wearing the try-on garment using a generative network. We test the proposed solution on the most widely used dataset for this task as well as on our newly collected dataset and demonstrate its effectiveness when compared to current state-of-the-art methods. Through extensive analyses on our Dress Code dataset, we show the adaptability of our model, which can generate try-on images even with a higher resolution.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W3196515900",
    "type": "article"
  },
  {
    "title": "ESRNet: Efficient Search and Recognition Network for Image Manipulation Detection",
    "doi": "https://doi.org/10.1145/3506853",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Ruyong Ren; Shaozhang Niu; Hua Ren; Shubin Zhang; Tengyue Han; Xiaohai Tong",
    "corresponding_authors": "",
    "abstract": "With the widespread use of smartphones and the rise of intelligent software, we can manipulate captured photos anytime and anywhere, so the fake photos finally obtained look “Real.” If these intelligent operation methods are maliciously applied to our daily life, then fake news, fake photos, rumors, slander, fraud, threats, and other information security issues around us can happen all the time. Today’s intelligent retouching software can make various modifications to photos, some of which do not change the content that the photos themselves want to express, such as retouching, contrast improvement, and so on. In this article, we mainly study the three operation modes of changing the authenticity of photo contents, which are Copy-move, Splicing, and Removal. Few scholars have done relevant research due to the lack of a corresponding dataset. To address this issue, we elaborately collect a novel dataset, called the multi-realistic scene manipulation dataset ( MSM30K ), which consists of 30,000 images, including three types of tampering methods, and covering 32 different tampering scenes in life. In addition, we propose a unified detection network: the efficient search and recognition network ( ESRNet ) for three tampering methods. It mainly includes four main modules: Efficient feature pyramid network ( EFPN ), Residual receptive field block with attention ( RFBA ), Hierarchical decoding identification ( HDI ), and Cascaded group-reversal attention ( GRA ) blocks. On these three datasets, ESRNet can reach 0.81 on the S-measure, 0.72 on the F-measure, and 0.85 on the E-measure. The inference speed is ~53 fps on a single GPU without I/O time. ESRNet outperforms various state-of-the-art manipulation detection baselines on three image manipulation datasets.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W4214845801",
    "type": "article"
  },
  {
    "title": "Image Super-Resolution via Lightweight Attention-Directed Feature Aggregation Network",
    "doi": "https://doi.org/10.1145/3546076",
    "publication_date": "2022-06-30",
    "publication_year": 2022,
    "authors": "Wang Li; Ke Li; Jingjing Tang; Yuying Liang",
    "corresponding_authors": "",
    "abstract": "The advent of convolutional neural networks (CNNs) has brought substantial progress in image super-resolution (SR) reconstruction. However, most SR methods pursue deep architectures to boost performance, and the resulting large model sizes are impractical for real-world applications. Furthermore, they insufficiently explore the internal structural information of image features, disadvantaging the restoration of fine texture details. To solve these challenges, we propose a lightweight architecture based on a CNN named attention-directed feature aggregation network (AFAN), consisting of chained stacking multi-aware attention modules (MAAMs) and a simple channel attention module (SCAM), for image SR. Specifically, in each MAAM, we construct a space-aware attention block (SAAB) and a dimension-aware attention block (DAAB) that individually yield unique three-dimensional modulation coefficients to adaptively recalibrate structural information from an asymmetric convolution residual block (ACRB). The synergistic strategy captures multiple content features that are both space-aware and dimension-aware to preserve more fine-grained details. In addition, to further enhance the accuracy and robustness of the network, SCAM is embedded in the last MAAM to highlight channels with high activated values at low computational load. Comprehensive experiments verify that our proposed network attains high qualitative accuracy while employing fewer parameters and moderate computational requirements, exceeding most state-of-the-art lightweight approaches.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W4283721091",
    "type": "article"
  },
  {
    "title": "Learning Pixel Affinity Pyramid for Arbitrary-Shaped Text Detection",
    "doi": "https://doi.org/10.1145/3524617",
    "publication_date": "2022-07-04",
    "publication_year": 2022,
    "authors": "Zilong Fu; Hongtao Xie; Shancheng Fang; Yuxin Wang; Mengting Xing; Yongdong Zhang",
    "corresponding_authors": "",
    "abstract": "Arbitrary-shaped text detection in natural images is a challenging task due to the complexity of the background and the diversity of text properties. The difficulty lies in two aspects: accurate separation of adjacent texts and sufficient text feature representation. To handle these problems, we consider text detection as instance segmentation and propose a novel text detection framework, which jointly learns semantic segmentation and a pixel affinity pyramid in a unified fully convolutional network. Specifically, the pixel affinity pyramid is proposed to encode multi-scale instance affiliation relationships of pixels, which is not only robust to varying shapes of text but also provides an accurate boundary description for separating closely located texts. In the inference phase, a simple but effective post-processing is presented to reconstruct text instances from the semantic segmentation results under the guidance of the learned pixel affinity pyramid, achieving good accuracy and efficiency. Furthermore, to enhance the representation of text features in the neural network, two modules — the Region Enhancement Module (REM) and Attentional Fusion Module (AFM) — are proposed. The REM models the semantic correlations of regional features to enhance the features from the text area, which effectively suppresses false-positive detection. The AFM adaptively fuses multi-scale textual information through an attention mechanism to obtain abundant text semantic features, which benefits multi-sized text detection. Extensive ablation experiments are conducted demonstrating the effectiveness of the REM and AFM. Evaluation results on standard benchmarks, including Total-Text, ICDAR2015, SCUT-CTW1500, and MSRA-TD500, show that our method surpasses most existing text detectors and achieves state-of-the-art performance, denoting its superior capability in detecting arbitrary-shaped texts.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W4283802017",
    "type": "article"
  },
  {
    "title": "A Decoupled Kernel Prediction Network Guided by Soft Mask for Single Image HDR Reconstruction",
    "doi": "https://doi.org/10.1145/3550277",
    "publication_date": "2022-07-22",
    "publication_year": 2022,
    "authors": "Gaofeng Cao; Fei Zhou; Kanglin Liu; Anjie Wang; Leidong Fan",
    "corresponding_authors": "",
    "abstract": "Recent works on single image high dynamic range (HDR) reconstruction fail to hallucinate plausible textures, resulting in information missing and artifacts in large-scale under/over-exposed regions. In this article, a decoupled kernel prediction network is proposed to infer an HDR image from a low dynamic range (LDR) image. Specifically, we first adopt a simple module to generate a preliminary result, which can precisely estimate well-exposed HDR regions. Meanwhile, an encoder-decoder backbone network with a soft mask guidance module is presented to predict pixel-wise kernels, which is further convolved with the preliminary result to obtain the final HDR output. Instead of traditional kernels, our predicted kernels are decoupled along the spatial and channel dimensions. The advantages of our method are threefold at least. First, our model is guided by the soft mask so that it can focus on the most relevant information for under/over-exposed regions. Second, pixel-wise kernels are able to adaptively solve the different degradations for differently exposed regions. Third, decoupled kernels can avoid information redundancy across channels and reduce the solution space of our model. Thus, our method is able to hallucinate fine details in the under/over-exposed regions and renders visually pleasing results. Extensive experiments demonstrate that our model outperforms state-of-the-art ones.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W4286488874",
    "type": "article"
  },
  {
    "title": "Robust Searching-Based Gradient Collaborative Management in Intelligent Transportation System",
    "doi": "https://doi.org/10.1145/3549939",
    "publication_date": "2022-07-21",
    "publication_year": 2022,
    "authors": "Hongjian Shi; Hao Wang; Ruhui Ma; Hua Yang; Tao Song; Honghao Gao; Haibing Guan",
    "corresponding_authors": "",
    "abstract": "With the rapid development of big data and the Internet of Things (IoT), traffic data from an Intelligent Transportation System (ITS) is becoming more and more accessible. To understand and simulate the traffic patterns from the traffic data, Multimedia Cognitive Computing (MCC) is an efficient and practical approach. Distributed Machine Learning (DML) has been the trend to provide sufficient computing resources and efficiency for MCC tasks to handle massive data and complex models. DML can speed up computation with those computing resources but introduces communication overhead. Gradient collaborative management or gradient aggregation in DML for MCC tasks is a critical task. An efficient managing algorithm of the communication schedules for gradient aggregation in ITS can improve the performance of MCC tasks. However, existing communication schedules typically rely on specific physical connection matrices, which have low robustness when a malfunction occurs. In this article, we propose Robust Searching-based Gradient Collaborative Management (RSGCM) in Intelligent Transportation System, a practical ring-based gradient managing algorithm for communication schedules across devices to deal with ITS malfunction. RSGCM provides solutions of communication schedules to various kinds of connection matrices with an acceptable amount of training time. Our experimental results have shown that RSGCM can deal with more varieties of connection matrices than existing state-of-the-art communication schedules. RSGCM also increases the robustness of ITS since it can restore the system’s functionality in an acceptable time when device or connection breakdown happens.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W4286493958",
    "type": "article"
  },
  {
    "title": "TCSD: Triple Complementary Streams Detector for Comprehensive Deepfake Detection",
    "doi": "https://doi.org/10.1145/3558004",
    "publication_date": "2022-08-22",
    "publication_year": 2022,
    "authors": "Xiaolong Liu; Yang Yu; Xiaolong Li; Yao Zhao; Guodong Guo",
    "corresponding_authors": "",
    "abstract": "Advancements in computer vision and deep learning have made it difficult to distinguish deepfake visual media. While existing detection frameworks have achieved significant performance on challenging deepfake datasets, these approaches consider only a single perspective. More importantly, in urban scenes, neither complex scenarios can be covered by a single view nor can the correlation between multiple datasets of information be well utilized. In this article, to mine the new view for deepfake detection and utilize the correlation of multi-view information contained in images, we propose a novel triple complementary streams detector (TCSD). First, a novel depth estimator is designed to extract depth information (DI), which has not been used in previous methods. Then, to supplement depth information for obtaining comprehensive forgery clues, we consider the incoherence between image foreground and background information (FBI) and the inconsistency between local and global information (LGI). In addition, we designed an attention-based multi-scale feature extraction (MsFE) module to extract more complementary features from DI, FBI, and LGI. Finally, two attention-based feature fusion modules are proposed to adaptively fuse information. Extensive experiment results show that the proposed approach achieves state-of-the-art performance on detecting deepfakes.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W4292596009",
    "type": "article"
  },
  {
    "title": "An Interaction-process-guided Framework for Small-group Performance Prediction",
    "doi": "https://doi.org/10.1145/3558768",
    "publication_date": "2022-08-26",
    "publication_year": 2022,
    "authors": "Yun-Shao Lin; Yi‐Ching Liu; Chi-Chun Lee",
    "corresponding_authors": "",
    "abstract": "A small group is a fundamental interaction unit for achieving a shared goal. Group performance can be automatically predicted using computational methods to analyze members’ verbal behavior in task-oriented interactions, as has been proven in several recent works. Most of the prior works focus on lower-level verbal behaviors, such as acoustics and turn-taking patterns, using either hand-crafted features or even advanced end-to-end methods. However, higher-level group-based communicative functions used between group members during conversations have not yet been considered. In this work, we propose a two-stage training framework that effectively integrates the communication function, as defined using Bales’s interaction process analysis (IPA) coding system, with the embedding learned from the low-level features in order to improve the group performance prediction. Our result shows a significant improvement compared to the state-of-the-art methods (4.241 MSE and 0.341 Pearson’s correlation on NTUBA-task1 and 3.794 MSE and 0.291 Pearson’s correlation on NTUBA-task2) on the National Taiwan University Business Administration (NTUBA) small-group interaction database. Furthermore, based on the design of IPA, our computational framework can provide a time-grained analysis of the group communication process and interpret the beneficial communicative behaviors for achieving better group performance.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W4293182302",
    "type": "article"
  },
  {
    "title": "NumCap: A Number-controlled Multi-caption Image Captioning Network",
    "doi": "https://doi.org/10.1145/3576927",
    "publication_date": "2022-12-16",
    "publication_year": 2022,
    "authors": "Amr Abdussalam; Zhongfu Ye; Ammar Hawbani; Majjed Al-Qatf; Rashid Khan",
    "corresponding_authors": "",
    "abstract": "Image captioning is a promising task that attracted researchers in the last few years. Existing image captioning models are primarily trained to generate one caption per image. However, an image may contain rich contents, and one caption cannot express its full details. A better solution is to describe an image with multiple captions, with each caption focusing on a specific aspect of the image. In this regard, we introduce a new number-based image captioning model that describes an image with multiple sentences. An image is annotated with multiple ground-truth captions; thus, we assign an external number to each caption to distinguish its order. Given an image-number pair as input, we could achieve different captions for the same image under different numbers. First, a number is attached to the image features to form an image-number vector (INV). Then, this vector and the corresponding caption are embedded using the order-embedding approach. Afterward, the INV’s embedding is fed to a language model to generate the caption. To show the efficiency of the numbers incorporation strategy, we conduct extensive experiments using MS-COCO, Flickr30K, and Flickr8K datasets. The proposed model attains 24.1 in METEOR on MS-COCO. The achieved results demonstrate that our method is competitive with a range of state-of-the-art models and validate its ability to produce different descriptions under different given numbers.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W4313476584",
    "type": "article"
  },
  {
    "title": "Non-Acted Text and Keystrokes Database and Learning Methods to Recognize Emotions",
    "doi": "https://doi.org/10.1145/3480968",
    "publication_date": "2022-02-16",
    "publication_year": 2022,
    "authors": "Madiha Tahir; Zahid Halim; Atta Ur Rahman; Muhammad Waqas; Shanshan Tu; Sheng Chen; Han Zhu",
    "corresponding_authors": "",
    "abstract": "The modern computing applications are presently adapting to the convenient availability of huge and diverse data for making their pattern recognition methods smarter. Identification of dominant emotion solely based on the text data generated by humans is essential for the modern human–computer interaction. This work presents a multimodal text-keystrokes dataset and associated learning methods for the identification of human emotions hidden in small text. For this, a text-keystrokes data of 69 participants is collected in multiple scenarios. Stimuli are induced through videos in a controlled environment. After the stimuli induction, participants write their reviews about the given scenario in an unguided manner. Afterward, keystroke and in-text features are extracted from the dataset. These are used with an assortment of learning methods to identify emotion hidden in the short text. An accuracy of 86.95% is achieved by fusing text and keystroke features. Whereas, 100% accuracy is obtained for pleasure-displeasure classes of emotions using the fusion of keystroke/text features, tree-based feature selection method, and support vector machine classifier. The present work is also compared with four state-of-the-art techniques for the same task, where the results suggest that the present proposal performs better in terms of accuracy.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W3201580996",
    "type": "article"
  },
  {
    "title": "MMSUM Digital Twins: A Multi-view Multi-modality Summarization Framework for Sporting Events",
    "doi": "https://doi.org/10.1145/3462777",
    "publication_date": "2022-01-27",
    "publication_year": 2022,
    "authors": "Samah Aloufi; Abdulmotaleb El Saddik",
    "corresponding_authors": "",
    "abstract": "Sporting events generate a massive amount of traffic on social media with live moment-to-moment accounts as any given situation unfolds. The generated data are intensified by fans feelings, reactions, and subjective opinions towards what happens during the event, all of which are based on their individual points of view. Analyzing and summarizing this data will generate a comprehensive overview of the event in terms of how the event evolves and how fans react and view the event based on their perspectives. Previously, most of the summarization works ignore fan reactions and subjective opinions, and focus primarily on generating an objective-view summary. We believe that an effective and useful summary should consider human reactions, sentiment, and point of view, as opposed to simply describing what happens during the event. Accordingly, in this work, we propose MMSUM Digital Twins: a summarization framework that is capable of generating a multi-view multi-modal summary for sporting events in real-time. The proposed digital twins-based framework consists of four main components: sub-event recognition which detects the event’s key moments, tweet categorization, which determines which team the tweets’ writers support and assigns tweets to their teams, sentiment analysis to track fans’ state of mind, and image popularity prediction for selecting representative images. Furthermore, the MMSUM employs a visual-filtering model to address the issue of noisy images that inundate social media, compromising the summarization quality. We leverage the knowledge of sport fans to evaluate the generated multi-view summarization through an online user study. The experiment results confirm the effectiveness of our proposed approach for summarizing sporting events by considering multimedia data, sentiment, and subjective views of the event.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4210324837",
    "type": "article"
  },
  {
    "title": "Learning Transferable Perturbations for Image Captioning",
    "doi": "https://doi.org/10.1145/3478024",
    "publication_date": "2022-02-16",
    "publication_year": 2022,
    "authors": "Hanjie Wu; Yongtuo Liu; Hongmin Cai; Shengfeng He",
    "corresponding_authors": "",
    "abstract": "Present studies have discovered that state-of-the-art deep learning models can be attacked by small but well-designed perturbations. Existing attack algorithms for the image captioning task is time-consuming, and their generated adversarial examples cannot transfer well to other models. To generate adversarial examples faster and stronger, we propose to learn the perturbations by a generative model that is governed by three novel loss functions. Image feature distortion loss is designed to maximize the encoded image feature distance between original images and the corresponding adversarial examples at the image domain, and local-global mismatching loss is introduced to separate the mapping encoding representation of the adversarial images and the ground true captions from a local and global perspective in the common semantic space as far as possible cross image and caption domain. Language diversity loss is to make the image captions generated by the adversarial examples as different as possible from the correct image caption at the language domain. Extensive experiments show that our proposed generative model can efficiently generate adversarial examples that successfully generalize to attack image captioning models trained on unseen large-scale datasets or with different architectures, or even the image captioning commercial service.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4213031069",
    "type": "article"
  },
  {
    "title": "Multi-view Shape Generation for a 3D Human-like Body",
    "doi": "https://doi.org/10.1145/3514248",
    "publication_date": "2022-02-18",
    "publication_year": 2022,
    "authors": "Hang Yu; Chilam Cheang; Yanwei Fu; Xiangyang Xue",
    "corresponding_authors": "",
    "abstract": "Three-dimensional (3D) human-like body reconstruction via a single RGB image has attracted significant research attention recently. Most of the existing methods rely on the Skinned Multi-Person Linear model and thus can only predict unified human bodies. Moreover, meshes reconstructed by current methods sometimes perform well from a canonical view but not from other views, as the reconstruction process is commonly supervised by only a single view. To address these limitations, this article proposes a multi-view shape generation network for a 3D human-like body. Particularly, we propose a coarse-to-fine learning model that gradually deforms a template body toward the ground truth body. Our model utilizes the information of multi-view renderings and corresponding 3D vertex transformation as supervision. Such supervision will help to generate 3D bodies well aligned to all views. To accurately operate mesh deformation, a graph convolutional network structure is introduced to support the shape generation from 3D vertex representation. Additionally, a graph up-pooling operation is designed over the intermediate representations of the graph convolutional network, and thus our model can generate 3D shapes with higher resolution. Novel loss functions are employed to help optimize the whole multi-view generation model, resulting in smoother surfaces. In addition, two multi-view human body datasets are produced and contributed to the community. Extensive experiments conducted on the benchmark datasets demonstrate the efficacy of our model over the competitors.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4213386995",
    "type": "article"
  },
  {
    "title": "Multi-granularity Brushstrokes Network for Universal Style Transfer",
    "doi": "https://doi.org/10.1145/3506710",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Quan Wang; Sheng Li; Xinpeng Zhang; Guorui Feng",
    "corresponding_authors": "",
    "abstract": "Neural style transfer has been developed in recent years, where both performance and efficiency have been greatly improved. However, most existing methods do not transfer the brushstrokes information of style images well. In this article, we address this issue by training a multi-granularity brushstrokes network based on a parallel coding structure. Specifically, we first adopt the content parsing module to obtain the spatial distribution of content image and the smoothness of different regions. Then, different brushstrokes features are transformed by a multi-granularity style-swap module guided by the region content map. Finally, the stylized features of the two branches are fused to enhance the stylized results. The multi-granularity brushstrokes network is jointly supervised by a new multi-layer brushstroke loss and pre-existing loss. The proposed method is close to the artistic drawing process. In addition, we can control whether the color of the stylized results tend to be the style image or the content image. Experimental results demonstrate the advantage of our proposed method compare with the existing schemes.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4214896365",
    "type": "article"
  },
  {
    "title": "Affective Interaction: Attentive Representation Learning for Multi-Modal Sentiment Classification",
    "doi": "https://doi.org/10.1145/3527175",
    "publication_date": "2022-03-25",
    "publication_year": 2022,
    "authors": "Yazhou Zhang; Prayag Tiwari; Lu Rong; Rui Chen; Nojoom A. Alnajem; M. Shamim Hossain",
    "corresponding_authors": "",
    "abstract": "The recent booming of artificial intelligence (AI) applications, e.g., affective robots, human-machine interfaces, autonomous vehicles, and so on, has produced a great number of multi-modal records of human communication. Such data often carry latent subjective users’ attitudes and opinions, which provides a practical and feasible path to realize the connection between human emotion and intelligence services. Sentiment and emotion analysis of multi-modal records is of great value to improve the intelligence level of affective services. However, how to find an optimal manner to learn people’s sentiments and emotional representations has been a difficult problem, since both of them involve subtle mind activity. To solve this problem, a lot of approaches have been published, but most of them are insufficient to mine sentiment and emotion, since they have treated sentiment analysis and emotion recognition as two separate tasks. The interaction between them has been neglected, which limits the efficiency of sentiment and emotion representation learning. In this work, emotion is seen as the external expression of sentiment, while sentiment is the essential nature of emotion. We thus argue that they are strongly related to each other where one’s judgment helps the decision of the other. The key challenges are multi-modal fused representation and the interaction between sentiment and emotion. To solve such issues, we design an external knowledge enhanced multi-task representation learning network, termed KAMT. The major elements contain two attention mechanisms, which are inter-modal and inter-task attentions and an external knowledge augmentation layer. The external knowledge augmentation layer is used to extract the vector of the participant’s gender, age, occupation, and of overall color or shape. The main use of inter-modal attention is to capture effective multi-modal fused features. Inter-task attention is designed to model the correlation between sentiment analysis and emotion classification. We perform experiments on three widely used datasets, and the experimental performance proves the effectiveness of the KAMT model.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4220923545",
    "type": "article"
  },
  {
    "title": "BMIF: Privacy-preserving Blockchain-based Medical Image Fusion",
    "doi": "https://doi.org/10.1145/3531016",
    "publication_date": "2022-04-20",
    "publication_year": 2022,
    "authors": "Tao Xiang; Honghong Zeng; Biwen Chen; Shangwei Guo",
    "corresponding_authors": "",
    "abstract": "Medical image fusion generates a fused image containing multiple features extracted from different source images, and it is of great help in clinical analysis and diagnosis. However, training a deep learning model for image fusion usually requires enormous computing power, especially for large volumes of medical data. Meanwhile, the privacy of images is also a critical issue. In this article, we propose a privacy-preserving blockchain-based medical image fusion (BMIF) framework. First, to ensure fusion performance, we design a new medical image fusion model based on convolutional neural network and Inception network and integrate the proposed model into the consensus process of blockchain. Next, to save computing power of blockchain, we design a consensus mechanism by requesting consensus nodes to train the fusion model instead of calculating useless hash values in traditional blockchain. Then, to protect data privacy, we further present an efficient homomorphic encryption to realize the training of fusion model on encrypted medical data. Finally, we conduct theoretical analysis and extensive experiments on public datasets to evaluate the feasibility and the performance of our proposed BMIF. The results exhibit that BMIF is efficient and secure, and our medical image fusion network performs better than state-of-the-art approaches.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4224292479",
    "type": "article"
  },
  {
    "title": "JDAN: Joint Detection and Association Network for Real-Time Online Multi-Object Tracking",
    "doi": "https://doi.org/10.1145/3533253",
    "publication_date": "2022-05-02",
    "publication_year": 2022,
    "authors": "Haidong Wang; Xuan He; Zhiyong Li; Jin Yuan; Shutao Li",
    "corresponding_authors": "",
    "abstract": "In the last few years, enormous strides have been made for object detection and data association, which are vital subtasks for one-stage online multi-object tracking (MOT). However, the two separated submodules involved in the whole MOT pipeline are processed or optimized separately, resulting in a complex method design and requiring manual settings. In addition, few works integrate the two subtasks into a single end-to-end network to optimize the overall task. In this study, we propose an end-to-end MOT network called joint detection and association network (JDAN) that is trained and inferred in a single network. All layers in JDAN are differentiable, and can be optimized jointly to detect targets and output an association matrix for robust multi-object tracking. What’s more, we generate suitable pseudo-labels to address the data inconsistency between object detection and association. The detection and association submodules could be optimized by the composite loss function that is derived from the detection results and the generated pseudo association labels, respectively. The proposed approach is evaluated on two MOT challenge datasets, and achieves promising performance compared with classic and latest methods.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4225272991",
    "type": "article"
  },
  {
    "title": "From Coarse to Fine: Hierarchical Structure-aware Video Summarization",
    "doi": "https://doi.org/10.1145/3485472",
    "publication_date": "2022-01-25",
    "publication_year": 2022,
    "authors": "Wenxu Li; Gang Pan; Chen Wang; Zhen Xing; Zhenjun Han",
    "corresponding_authors": "",
    "abstract": "Hierarchical structure is a common characteristic for some kinds of videos (e.g., sports videos, game videos): The videos are composed of several actions hierarchically and there exist temporal dependencies among segments with different scales, where action labels can be enumerated. Our ideas are based on two observations: First, the actions are the fundamental units for people to understand these videos. Second, the humans summarize a video by iteratively observing and refining, i.e., observing segments in video and hierarchically refining the boundaries of important actions. Based on the above insights, we generate action proposals to construct the structure of the video and formulate the summarization process as a hierarchical refining process. We also train a hierarchical summarization network with deep Q-learning (HQSN) to achieve the refining process and explore temporal dependency. Besides, we collect a new dataset that consists of structured game videos with fine-grain actions and importance annotations. The experimental results demonstrate the effectiveness of the proposed method.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4226133174",
    "type": "article"
  },
  {
    "title": "Perturbation-enabled Deep Federated Learning for Preserving Internet of Things-based Social Networks",
    "doi": "https://doi.org/10.1145/3537899",
    "publication_date": "2022-05-23",
    "publication_year": 2022,
    "authors": "Sara Salim; Nour Moustafa; Benjamin Turnbull; Imran Razzak",
    "corresponding_authors": "",
    "abstract": "Federated Learning (FL), as an emerging form of distributed machine learning (ML), can protect participants’ private data from being substantially disclosed to cyber adversaries. It has potential uses in many large-scale, data-rich environments, such as the Internet of Things (IoT), Industrial IoT, Social Media (SM), and the emerging SM 3.0. However, federated learning is susceptible to some forms of data leakage through model inversion attacks. Such attacks occur through the analysis of participants’ uploaded model updates. Model inversion attacks can reveal private data and potentially undermine some critical reasons for employing federated learning paradigms. This article proposes novel differential privacy (DP)-based deep federated learning framework. We theoretically prove that our framework can fulfill DP’s requirements under distinct privacy levels by appropriately adjusting scaled variances of Gaussian noise. We then develop a Differentially Private Data-Level Perturbation (DP-DLP) mechanism to conceal any single data point’s impact on the training phase. Experiments on real-world datasets, specifically the social media 3.0, Iris, and Human Activity Recognition (HAR) datasets, demonstrate that the proposed mechanism can offer high privacy, enhanced utility, and elevated efficiency. Consequently, it simplifies the development of various DP-based FL models with different tradeoff preferences on data utility and privacy levels.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4281294363",
    "type": "article"
  },
  {
    "title": "Adversarial Multi-Grained Embedding Network for Cross-Modal Text-Video Retrieval",
    "doi": "https://doi.org/10.1145/3483381",
    "publication_date": "2022-02-16",
    "publication_year": 2022,
    "authors": "Ning Han; Jingjing Chen; Hao Zhang; Huan-Wen Wang; Hao Chen",
    "corresponding_authors": "",
    "abstract": "Cross-modal retrieval between texts and videos has received consistent research interest in the multimedia community. Existing studies follow a trend of learning a joint embedding space to measure the distance between text and video representations. In common practice, video representation is constructed by feeding clips into 3D convolutional neural networks for a coarse-grained global visual feature extraction. In addition, several studies have attempted to align the local objects of video with the text. However, these representations share a drawback of neglecting rich fine-grained relation features capturing spatial-temporal object interactions that benefits mapping textual entities in the real-world retrieval system. To tackle this problem, we propose an adversarial multi-grained embedding network (AME-Net), a novel cross-modal retrieval framework that adopts both fine-grained local relation and coarse-grained global features in bridging text-video modalities. Additionally, with the newly proposed visual representation, we also integrate an adversarial learning strategy into AME-Net, to further narrow the domain gap between text and video representations. In summary, we contribute AME-Net with an adversarial learning strategy for learning a better joint embedding space, and experimental results on MSR-VTT and YouCook2 datasets demonstrate that our proposed framework consistently outperforms the state-of-the-art method.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4283384437",
    "type": "article"
  },
  {
    "title": "Referring Expression Comprehension Via Enhanced Cross-modal Graph Attention Networks",
    "doi": "https://doi.org/10.1145/3548688",
    "publication_date": "2022-07-15",
    "publication_year": 2022,
    "authors": "Jia Wang; Jingcheng Ke; Hong-Han Shuai; Yung‐Hui Li; Wen-Huang Cheng",
    "corresponding_authors": "",
    "abstract": "Referring expression comprehension aims to localize a specific object in an image according to a given language description. It is still challenging to comprehend and mitigate the gap between various types of information in the visual and textual domains. Generally, it needs to extract the salient features from a given expression and match the features of expression to an image. One challenge in referring expression comprehension is the number of region proposals generated by object detection methods is far more than the number of entities in the corresponding language description. Remarkably, the candidate regions without described by the expression will bring a severe impact on referring expression comprehension. To tackle this problem, we first propose a novel Enhanced Cross-modal Graph Attention Networks (ECMGANs) that boosts the matching between the expression and the entity position of an image. Then, an effective strategy named Graph Node Erase (GNE) is proposed to assist ECMGANs in eliminating the effect of irrelevant objects on the target object. Experiments on three public referring expression comprehension datasets show unambiguously that our ECMGANs framework achieves better performance than other state-of-the-art methods. Moreover, GNE is able to obtain higher accuracies of visual-expression matching effectively.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4285594363",
    "type": "article"
  },
  {
    "title": "DRL based Joint Affective Services Computing and Resource Allocation in ISTN",
    "doi": "https://doi.org/10.1145/3561821",
    "publication_date": "2022-09-10",
    "publication_year": 2022,
    "authors": "Kexin Xu; Haijun Zhang; Keping Long; Jianquan Wang; Lei Sun",
    "corresponding_authors": "",
    "abstract": "Affective services will become a research hotspot in artificial intelligence (AI) in the next decade. In this paper, a novel service paradigm combined with wireless communication in integrated satellite-terrestrial network (ISTN) is proposed. On this basis, an affective services computing offloading and transmission network (ASCTN) with a three-tier computation architecture is proposed, which is able to assist users to obtain affective computing services and regulate emotions. The optimization problem is investigated in the ASCTN, which is a discrete, non-linear, and non-convex problem with the limitation of computation ability of satellite and transmit power. Specifically, with the objective to minimize the cost utility related to latency and energy consumption, a joint affective services tasks computing offloading strategy, sub-channel, and power allocation algorithm based on dueling deep Q-network (Dueling-DQN) is proposed, which is in possession of better stability. The simulation results reveal the effectiveness of the optimization algorithm in terms of the cost utility in the ASCTN system.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4296270202",
    "type": "article"
  },
  {
    "title": "Semi-supervised Learning for Mars Imagery Classification and Segmentation",
    "doi": "https://doi.org/10.1145/3572916",
    "publication_date": "2022-12-01",
    "publication_year": 2022,
    "authors": "Wenjing Wang; Lilang Lin; Zejia Fan; Jiaying Liu",
    "corresponding_authors": "",
    "abstract": "With the progress of Mars exploration, numerous Mars image data are being collected and need to be analyzed. However, due to the severe train-test gap and quality distortion of Martian data, the performance of existing computer vision models is unsatisfactory. In this article, we introduce a semi-supervised framework for machine vision on Mars and try to resolve two specific tasks: classification and segmentation. Contrastive learning is a powerful representation learning technique. However, there is too much information overlap between Martian data samples, leading to a contradiction between contrastive learning and Martian data. Our key idea is to reconcile this contradiction with the help of annotations and further take advantage of unlabeled data to improve performance. For classification, we propose to ignore inner-class pairs on labeled data as well as neglect negative pairs on unlabeled data, forming supervised inter-class contrastive learning and unsupervised similarity learning. For segmentation, we extend supervised inter-class contrastive learning into an element-wise mode and use online pseudo labels for supervision on unlabeled areas. Experimental results show that our learning strategies can improve the classification and segmentation models by a large margin and outperform state-of-the-art approaches.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4310535571",
    "type": "article"
  },
  {
    "title": "Perceptual Hashing of Deep Convolutional Neural Networks for Model Copy Detection",
    "doi": "https://doi.org/10.1145/3572777",
    "publication_date": "2022-12-06",
    "publication_year": 2022,
    "authors": "H.F. Chen; Hang Zhou; Jie Zhang; Dongdong Chen; Weiming Zhang; Kejiang Chen; Gang Hua; Nenghai Yu",
    "corresponding_authors": "",
    "abstract": "In recent years, many model intellectual property (IP) proof methods for IP protection have been proposed, such as model watermarking and model fingerprinting. However, with the increasing number of models transmitted and deployed on the Internet, quickly finding the suspect model among thousands of models on model-sharing platforms such as GitHub is in great demand, which concurrently triggers the new security problem of model copy detection for IP protection. As an important part of the model IP protection system, the model copy detection task has not received enough attention. Due to the high computational complexity, both model watermarking and model fingerprinting lack the capability to efficiently find suspected infringing models among tens of millions of models. In this article, inspired by the hash-based image retrieval methods, we introduce a novel model copy detection mechanism: perceptual hashing for convolutional neural networks (CNNs). The proposed perceptual hashing algorithm can convert the weights of CNNs to fixed-length binary hash codes so that the lightly modified version has the similar hash code as the original model. By comparing the similarity of a pair of hash codes between a query model and a test model in the model library, similar versions of a query model can be retrieved efficiently. To the best of our knowledge, this is the first perceptual hashing algorithm for deep neural network models. Specifically, we first select the important model weights based on the model compression theory, then calculate the normal test statistics (NTS) on the segments of important weights, and finally encode the NTS features into hash codes. The experiment performed on a model library containing 3,565 models indicates that our perceptual hashing scheme has a superior copy detection performance.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4311623198",
    "type": "article"
  },
  {
    "title": "Aesthetic Attribute Assessment of Images Numerically on Mixed Multi-attribute Datasets",
    "doi": "https://doi.org/10.1145/3547144",
    "publication_date": "2022-10-31",
    "publication_year": 2022,
    "authors": "Xin Jin; Xinning Li; Hao Lou; Chenyu Fan; Qiang Deng; Chaoen Xiao; Shuai Cui; Amit Kumar Singh",
    "corresponding_authors": "",
    "abstract": "With the continuous development of social software and multimedia technology, images have become a kind of important carrier for spreading information and socializing. How to evaluate an image comprehensively has become the focus of recent researches. The traditional image aesthetic assessment methods often adopt single numerical overall assessment scores, which has certain subjectivity and can no longer meet the higher aesthetic requirements. In this article, we construct an new image attribute dataset called aesthetic mixed dataset with attributes (AMD-A) and design external attribute features for fusion. Besides, we propose an efficient method for image aesthetic attribute assessment on mixed multi-attribute dataset and construct a multitasking network architecture by using the EfficientNet-B0 as the backbone network. Our model can achieve aesthetic classification, overall scoring, and attribute scoring. In each sub-network, we improve the feature extraction through ECA channel attention module. As for the final overall scoring, we adopt the idea of the teacher-student network and use the classification sub-network to guide the aesthetic overall fine-grain regression. Experimental results, using the MindSpore, show that our proposed method can effectively improve the performance of the aesthetic overall and attribute assessment.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4319316596",
    "type": "article"
  },
  {
    "title": "Advanced Predictive Tile Selection Using Dynamic Tiling for Prioritized 360° Video VR Streaming",
    "doi": "https://doi.org/10.1145/3603146",
    "publication_date": "2023-06-01",
    "publication_year": 2023,
    "authors": "Abid Yaqoob; Gabriel‐Miro Muntean",
    "corresponding_authors": "",
    "abstract": "The widespread availability of smart computing and display devices such as mobile phones, gaming consoles, laptops, and tethered/untethered head-mounted displays has fueled an increase in demand for omnidirectional (360°) videos. 360° video applications enable users to change their viewing angles while interacting with the video during playback. This allows users to have a more personalized and interactive viewing experience. Unfortunately, these applications require substantial network and computational resources that the conventional infrastructure and end devices cannot support. Recently proposed viewport adaptive fixed tiling solutions stream only relevant video tiles based on user interaction with the virtual reality (VR) space to use existing transmission resources more efficiently. However, achieving real-time accurate viewport extraction and transmission in response to both head movements and bandwidth dynamics can be challenging, which can impact the user’s Quality of Experience (QoE). This article proposes innovative dynamic tiling-based adaptive 360° video streaming solutions in order to achieve high viewer QoE. First, novel and easy-to-scale tiling layout selection methods are introduced, and the best tiling layouts are employed in each adaptation interval based on the prediction-assisted visual quality metric and the observed viewport divergence. Second, a novel proactive tile selection approach is presented, which adaptively extracts tiles for each selected tiling layout based on two low-complex viewport prediction mechanisms. Finally, a practical dynamic tile priority-oriented bitrate adaptation scheme is introduced, which uniformly distributes the bitrate budget among different tiles during 360° video streaming. Extensive trace-driven experiments are conducted to evaluate the proposed solutions using head motion traces from 48 VR users for five 360° videos with tiling layouts of 4 × 3, 6 × 4, and 8 × 6 and segment durations of 1s, 1.5s, and 2s. The experimental evaluations show that the dynamic video tiling solutions achieve up to 11.2% more viewport matches and an average improvement in QoE of 9.7% to 18% compared to state-of-the-art 360° streaming approaches.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4379014710",
    "type": "article"
  },
  {
    "title": "Deep Modular Co-Attention Shifting Network for Multimodal Sentiment Analysis",
    "doi": "https://doi.org/10.1145/3634706",
    "publication_date": "2023-11-27",
    "publication_year": 2023,
    "authors": "Piao Shi; Min Hu; Xuefeng Shi; Fuji Ren",
    "corresponding_authors": "",
    "abstract": "Human Multimodal Sentiment Analysis (MSA) is an attractive research that studies sentiment expressed from multiple heterogeneous modalities. While transformer-based methods have achieved great success, designing an effective ”co-attention” model to associate text modality with nonverbal modalities remains challenging. There are two main problems: 1) the dominant role of the text in modalities is underutilization, and 2) the interaction between modalities is not sufficiently explored. This paper proposes a deep modular Co-Attention Shifting Network (CoASN) for MSA. A Cross-modal Modulation Module based on Co-attention (CMMC) and an Advanced Modality-mixing Adaptation Gate (AMAG) are constructed. The CMMC consists of the Text-guided Co-Attention (TCA) and Interior Transformer Encoder (ITE) units to capture inter-modal features and intra-modal features. With text modality as the core, the CMMC module aims to guide and promote the expression of emotion in nonverbal modalities, and the nonverbal modalities increase the richness of the text-based multimodal sentiment information. In addition, the AMAG module is introduced to explore the dynamical correlations among all modalities. Particularly, this efficient module first captures the nonverbal shifted representations and then combines them to calculate the shifted word embedding representations for the final MSA tasks. Extensive experiments on two commonly used datasets, CMU-MOSI and CMU-MOSEI, demonstrate that our proposed method is superior to the state-of-the-art performance.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4389047389",
    "type": "article"
  },
  {
    "title": "Context-Detail-Aware United Network for Single Image Deraining",
    "doi": "https://doi.org/10.1145/3639407",
    "publication_date": "2024-01-02",
    "publication_year": 2024,
    "authors": "Wei‐Yen Hsu; Hsien-Wen Lin",
    "corresponding_authors": "",
    "abstract": "Images captured outdoors are often affected by rainy days, resulting in a severe deterioration in the visual quality of the captured images and a decrease in the performance of related applications. Therefore, single image deraining has attracted attention as a challenging research topic. Nowadays, there are two common deraining architectures in single image deraining. The first one is to restore the rain-free image by deducting rain streaks learned by the model from the rain image, but the background structure is easily mistaken for rain streaks and subtracted. The other one is to directly learn the clean background structure through the model using rain images, but it is difficult to completely remove the rain streaks due to the complexity of the information in images with rain. Therefore, current methods cannot balance rain streak removal and rain-free image background restoration in a single architecture and achieve good results. To address this issue, we propose a novel framework, namely, Context-Detail-Aware United Network (CDaUNet), which combines the above two architectures in this study. More specifically, we divide the restoration of the background structure of rain-free images and the learning of rain streaks into two independent sub-networks. The proposed Structure-Aware Rain Removal Network (SaRRN) is to learn the background structure in images to reconstruct clean rain-free images, whereas Detail-Aware Rain Streak Learning Network (DaRLN) is proposed to learn the details of rain streaks in images. Finally, we fuse the results generated by the two sub-networks through our designed Dual Architecture Fusion Network (DAFN) to reconstruct original rain images to effectively fuse the results of the two sub-networks. The experimental results show that CDaUNet achieves satisfactory performance in comparison with the state-of-the-art approaches included in rain streak removal and rain-free image structure restoration architectures on both synthetic and real image datasets, confirming the effectiveness of our method.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4390490907",
    "type": "article"
  },
  {
    "title": "An Underwater Organism Image Dataset and a Lightweight Module Designed for Object Detection Networks",
    "doi": "https://doi.org/10.1145/3640465",
    "publication_date": "2024-01-11",
    "publication_year": 2024,
    "authors": "Jiafeng Huang; Tianjun Zhang; Shengjie Zhao; Lin Zhang; Yicong Zhou",
    "corresponding_authors": "",
    "abstract": "Long-term monitoring and recognition of underwater organism objects are of great significance in marine ecology, fisheries science and many other disciplines. Traditional techniques in this field, including manual fishing-based ones and sonar-based ones, are usually flawed. Specifically, the method based on manual fishing is time-consuming and unsuitable for scientific researches, while the sonar-based one, has the defects of low acoustic image accuracy and large echo errors. In recent years, the rapid development of deep learning and its excellent performance in computer vision tasks make vision-based solutions feasible. However, the researches in this area are still relatively insufficient in mainly two aspects. First, to our knowledge, there is still a lack of large-scale datasets of underwater organism images with accurate annotations. Second, in consideration of the limitation on hardware resources of underwater devices, an underwater organism detection algorithm that is both accurate and lightweight enough to be able to infer in real time is still lacking. As an attempt to fill in the aforementioned research gaps to some extent, we established the Multiple Kinds of Underwater Organisms (MKUO) dataset with accurate bounding box annotations of taxonomic information, which consists of 10,043 annotated images, covering eighty-four underwater organism categories. Based on our benchmark dataset, we evaluated a series of existing object detection algorithms to obtain their accuracy and complexity indicators as the baseline for future reference. In addition, we also propose a novel lightweight module, namely Sparse Ghost Module, designed especially for object detection networks. By substituting the standard convolution with our proposed one, the network complexity can be significantly reduced and the inference speed can be greatly improved without obvious detection accuracy loss. To make our results reproducible, the dataset and the source code are available online at https://cslinzhang.github.io/MKUO-and-Sparse-Ghost-Module/ .",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4390740999",
    "type": "article"
  },
  {
    "title": "Modality-collaborative Transformer with Hybrid Feature Reconstruction for Robust Emotion Recognition",
    "doi": "https://doi.org/10.1145/3640343",
    "publication_date": "2024-01-11",
    "publication_year": 2024,
    "authors": "Chengxin Chen; Pengyuan Zhang",
    "corresponding_authors": "",
    "abstract": "As a vital aspect of affective computing, Multimodal Emotion Recognition has been an active research area in the multimedia community. Despite recent progress, this field still confronts two major challenges in real-world applications: (1) improving the efficiency of constructing joint representations from unaligned multimodal features and (2) relieving the performance decline caused by random modality feature missing. In this article, we propose a unified framework, Modality-Collaborative Transformer with Hybrid Feature Reconstruction (MCT-HFR), to address these issues. The crucial component of MCT is a novel attention-based encoder that concurrently extracts and dynamically balances the intra- and inter-modality relations for all associated modalities. With additional modality-wise parameter sharing, a more compact representation can be encoded with less time and space complexity. To improve the robustness of MCT, we further introduce HFR, which consists of two modules: Local Feature Imagination (LFI) and Global Feature Alignment (GFA). During model training, LFI leverages complete features as supervisory signals to recover local missing features, while GFA is designed to reduce the global semantic gap between pairwise-complete and -incomplete representations. Experimental evaluations on two popular benchmark datasets demonstrate that our proposed method consistently outperforms advanced baselines in both complete and incomplete data scenarios.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4390751814",
    "type": "article"
  },
  {
    "title": "Diverse Visual Question Generation Based on Multiple Objects Selection",
    "doi": "https://doi.org/10.1145/3640014",
    "publication_date": "2024-01-15",
    "publication_year": 2024,
    "authors": "Wenhao Fang; Jiayuan Xie; Hongfei Liu; Jiali Chen; Yi Cai",
    "corresponding_authors": "",
    "abstract": "Visual question generation task aims at generating high-quality questions about a given image. To make this tak applicable to various scenarios, e.g., the growing demand for exams, it is important to generate diverse questions. The existing methods for this task control diverse question generation based on different question types, e.g., “what” and “when.” Although different question types lead to description diversity, they cannot guarantee semantic diversity when asking the same objects. Research in the field of psychology shows that humans pay attention to different objects in an image based on their preferences, which is beneficial to constructing semantically diverse questions. According to the research, we propose a multi-selector visual question generation (MS-VQG) model that aims to focus on different objects to generate diverse questions. Specifically, our MS-VQG model employs multiple selectors to imitate different humans to select different objects in a given image. Based on these different selected objects, our MS-VQG model can generate diverse questions corresponding to each selector. Extensive experiments on two datasets show that our proposed model outperforms the baselines in generating diverse questions.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4390878756",
    "type": "article"
  },
  {
    "title": "Exploiting Spatial-Temporal Context for Interacting Hand Reconstruction on Monocular RGB Video",
    "doi": "https://doi.org/10.1145/3639707",
    "publication_date": "2024-01-24",
    "publication_year": 2024,
    "authors": "Weichao Zhao; Hezhen Hu; Wengang Zhou; Li Li; Houqiang Li",
    "corresponding_authors": "",
    "abstract": "Reconstructing interacting hands from monocular RGB data is a challenging task, as it involves many interfering factors, e.g., self- and mutual occlusion and similar textures. Previous works only leverage information from a single RGB image without modeling their physically plausible relation, which leads to inferior reconstruction results. In this work, we are dedicated to explicitly exploiting spatial-temporal information to achieve better interacting hand reconstruction. On the one hand, we leverage temporal context to complement insufficient information provided by the single frame and design a novel temporal framework with a temporal constraint for interacting hand motion smoothness. On the other hand, we further propose an interpenetration detection module to produce kinetically plausible interacting hands without physical collisions. Extensive experiments are performed to validate the effectiveness of our proposed framework, which achieves new state-of-the-art performance on public benchmarks.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4391169637",
    "type": "article"
  },
  {
    "title": "Iterative Temporal-spatial Transformer-based Cardiac T1 Mapping MRI Reconstruction",
    "doi": "https://doi.org/10.1145/3643640",
    "publication_date": "2024-01-29",
    "publication_year": 2024,
    "authors": "Jun Lyu; Guangming Wang; M. Shamim Hossain",
    "corresponding_authors": "",
    "abstract": "The precise reconstruction of accelerated magnetic resonance imaging (MRI) brings about notable advantages, such as enhanced diagnostic precision and decreased examination costs. In contrast, traditional cardiac MRI necessitates repetitive acquisitions across multiple heartbeats, resulting in prolonged acquisition times. Significant strides have been made in accelerating MRI through deep learning-based reconstruction methods. However, these existing methods encounter certain limitations: (1) The intricate nature of heart reconstruction involving multiple complex time-series data poses a challenge in exploring nonlinear dependencies between temporal contexts. (2) Existing research often overlooks weight sharing in iterative frameworks, impeding the effective capturing of non-local information and, consequently, limiting improvements in model performance. In order to improve cardiac MRI reconstruction, we propose a novel temporal-spatial transformer with a strategy in this study. Based on the multi-level encoder and decoder transformer architecture, we conduct multi-level spatiotemporal information feature aggregation over several adjacent views, that create nonlinear dependencies among features and efficiently learn important information among adjacent cardiac temporal frames. Additionally, in order to improve contextual awareness between neighboring views, we add cross-view attention for temporal information fusion. Furthermore, we introduce an iterative strategy for training weights during the reconstruction process, which improves feature fusion in critical locations and reduces the number of computations required to calculate global feature dependencies. Extensive experiments have demonstrated the substantial superiority of this procedure over the most advanced techniques, suggesting that it has broad potential for clinical use.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4391315829",
    "type": "article"
  },
  {
    "title": "Multi-Content Interaction Network for Few-Shot Segmentation",
    "doi": "https://doi.org/10.1145/3643850",
    "publication_date": "2024-02-03",
    "publication_year": 2024,
    "authors": "Hao Chen; Yunlong Yu; Yonghan Dong; Zhe‐Ming Lu; Yingming Li; Zhongfei Zhang",
    "corresponding_authors": "",
    "abstract": "Few-Shot Segmentation (FSS) poses significant challenges due to limited support images and large intra-class appearance discrepancies. Most existing approaches focus on aligning the support-query correlations from the same layer of the frozen backbone while neglecting the bias between different tasks and different layers. In this article, we propose a Multi-Content Interaction Network (MCINet) to remedy these issues by fully exploiting and interacting with the different contextual information contained in distinct branches. Specifically, MCINet improves FSS from three perspectives: (1) boosting the query representations through incorporating the independent information from another learnable branch into the features from the frozen backbone, (2) enhancing the support-query correlations by exploiting both the same-layer and adjacent-layer features, and (3) refining the predicted results with a multi-scale mask prediction strategy. Experiments on three benchmarks demonstrate that our approach reaches state-of-the-art performances and outperforms the best competitors with many desirable advantages, especially on the challenging COCO dataset. Code will be released on GitHub ( https://github.com/chenhao-zju/mcinet ).",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4391514780",
    "type": "article"
  },
  {
    "title": "Encrypted Video Search with Single/Multiple Writers",
    "doi": "https://doi.org/10.1145/3643887",
    "publication_date": "2024-02-05",
    "publication_year": 2024,
    "authors": "Yu Zheng; Wenchao Zhang; Wei Song; Xiuhua Wang; Chong Fu",
    "corresponding_authors": "",
    "abstract": "Video-based services have become popular. Clients often outsource their videos to the cloud to relieve local maintenance. However, privacy has become a major concern, since many videos contain sensitive information. Although retrieving (unencrypted) videos has been extensively investigated, retrieving encrypted multimedia has received relatively rare attention, at best in a limitation of image-based similarity searches. We initiate the study of scalable encrypted video search, enabling clients to query videos similar to an image search. Our modular framework leverages intrinsic attributes of videos, such as semantics and visuals, to effectively capture their contents. We propose a two-step approach whereby lightweight searchable encryption techniques are used for pre-screening, followed by an interactive approach for fine-grained search. Furthermore, we present three instantiations, including one centralized-writer instantiation and two distributed-writer instantiations, to effectively cater to varying needs and scenarios: (1) The centralized one employs forward and backward private searchable encryption [CCS 2017] over deep hashing [CVPR 2020]. (2) Motivated by distributed computing, the multi-writer instantiations building atop HSE [Usenix Security 2022] allows searching the relevant videos contributed by multiple intuitions collaboratively. Our experimental results illustrate their practical performance over multiple real-world datasets, whether in a centralized setting or distributed setting.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4391540370",
    "type": "article"
  },
  {
    "title": "Multimodal Attentive Representation Learning for Micro-video Multi-label Classification",
    "doi": "https://doi.org/10.1145/3643888",
    "publication_date": "2024-02-06",
    "publication_year": 2024,
    "authors": "Peiguang Jing; Xianyi Liu; Lijuan Zhang; Yun Li; Yu Liu; Yuting Su",
    "corresponding_authors": "",
    "abstract": "As one of the representative types of user-generated contents (UGCs) in social platforms, micro-videos have been becoming popular in our daily life. Although micro-videos naturally exhibit multimodal features that are rich enough to support representation learning, the complex correlations across modalities render valuable information difficult to integrate. In this paper, we introduced a multimodal attentive representation network (MARNET) to learn complete and robust representations to benefit micro-video multi-label classification. To address the commonly missing modality issue, we presented a multimodal information aggregation mechanism module to integrate multimodal information, where latent common representations are obtained by modeling the complementarity and consistency in terms of visual-centered modality groupings instead of single modalities. For the label correlation issue, we designed an attentive graph neural network module to adaptively learn the correlation matrix and representations of labels for better compatibility with training data. In addition, a cross-modal multi-head attention module is developed to make the learned common representations label-aware for multi-label classification. Experiments conducted on two micro-video datasets demonstrate the superior performance of MARNET compared with state-of-the-art methods.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4391576337",
    "type": "article"
  },
  {
    "title": "Region-Focused Network for Dense Captioning",
    "doi": "https://doi.org/10.1145/3648370",
    "publication_date": "2024-02-15",
    "publication_year": 2024,
    "authors": "Qingbao Huang; Pijian Li; Youji Huang; Feng Shuang; Yi Cai",
    "corresponding_authors": "",
    "abstract": "Dense captioning is a very critical but under-explored task, which aims to densely detect localized regions-of-interest (RoIs) and describe them with natural language in a given image. Although recent studies tried to fuse multi-scale features from different visual instances to generate more accurate descriptions, their methods still suffer from the lack of exploration of relation semantic information in images, leading to less informative descriptions. Furthermore, indiscriminately fusing all visual instance features will introduce redundant information, resulting in poor matching between descriptions and corresponding regions. In this work, we propose a Region-Focused Network (RFN) to address these issues. Specifically, to fully comprehend the images, we first extract the object-level features, and encode the interaction and position relations between objects to enhance the object representations. Then, to decrease the interference from redundant information about the target region, we extract the most relevant information to the region. Finally, a region-based Transformer is employed to compose and align the previous mined information and generate the corresponding descriptions. Extensive experiments on Visual Genome V1.0 and V1.2 datasets show that our RFN model outperforms the state-of-the-art methods, thus verifying its effectiveness. Our code is available at https://github.com/VILAN-Lab/DesCap .",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4391837206",
    "type": "article"
  },
  {
    "title": "Continuous Image Outpainting with Neural ODE",
    "doi": "https://doi.org/10.1145/3648367",
    "publication_date": "2024-03-02",
    "publication_year": 2024,
    "authors": "Penglei Gao; Xi Yang; Rui Zhang; Kaizhu Huang",
    "corresponding_authors": "",
    "abstract": "Generalised image outpainting is an important and active research topic in computer vision, which aims to extend appealing content all-side around a given image. Existing state-of-the-art outpainting methods often rely on discrete extrapolation to extend the feature map in the bottleneck. They thus suffer from content unsmoothness, especially in circumstances where the outlines of objects in the extrapolated regions are incoherent with the input sub-images. To mitigate this issue, we design a novel bottleneck with Neural ODEs to make continuous extrapolation in latent space, which could be a plug-in for many deep learning frameworks. Our ODE-based network continuously transforms the state and makes accurate predictions by learning the incremental relationship among latent points, leading to both smooth and structured feature representation. Experimental results on three real-world datasets both applied on transformer-based and CNN-based frameworks show that our methods could generate more realistic and coherent images against the state-of-the-art image outpainting approaches. Our code is available at https://github.com/PengleiGao/Continuous-Image-Outpainting-with-Neural-ODE .",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4392347534",
    "type": "article"
  },
  {
    "title": "Delay Threshold for Social Interaction in Volumetric eXtended Reality Communication",
    "doi": "https://doi.org/10.1145/3651164",
    "publication_date": "2024-03-06",
    "publication_year": 2024,
    "authors": "Carlos Cortés; Irene Viola; Jesús Gutiérrez; Jack Jansen; Shishir Subramanyam; Evangelos Alexiou; Pablo Pérez; Narciso Garcı́a; Pablo César",
    "corresponding_authors": "",
    "abstract": "Immersive technologies like eXtended Reality (XR) are the next step in videoconferencing. In this context, understanding the effect of delay on communication is crucial. This article presents the first study on the impact of delay on collaborative tasks using a realistic Social XR system. Specifically, we design an experiment and evaluate the impact of end-to-end delays of 300, 600, 900, 1,200, and 1,500 ms on the execution of a standardized task involving the collaboration of two remote users that meet in a virtual space and construct block-based shapes. To measure the impact of the delay in this communication scenario, objective and subjective data were collected. As objective data, we measured the time required to execute the tasks and computed conversational characteristics by analyzing the recorded audio signals. As subjective data, a questionnaire was prepared and completed by every user to evaluate different factors such as overall quality, perception of delay, annoyance using the system, level of presence, cybersickness, and other subjective factors associated with social interaction. The results show a clear influence of the delay on the perceived quality and a significant negative effect as the delay increases. Specifically, the results indicate that the acceptable threshold for end-to-end delay should not exceed 900 ms. This article additionally provides guidelines for developing standardized XR tasks for assessing interaction in Social XR environments.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4392511609",
    "type": "article"
  },
  {
    "title": "A Quality of Experience and Visual Attention Evaluation for 360° videos with non-spatial and spatial audio",
    "doi": "https://doi.org/10.1145/3650208",
    "publication_date": "2024-08-19",
    "publication_year": 2024,
    "authors": "Amit Hirway; Yuansong Qiao; Niall Murray",
    "corresponding_authors": "",
    "abstract": "This article presents the results of an empirical study that aimed to investigate the influence of various types of audio (spatial and non-spatial) on the user quality of experience (QoE) of and visual attention in 360° videos. The study compared the head pose, eye gaze, pupil dilations, heart rate, and subjective responses of 73 users who watched ten 360° videos with different sound configurations. The configurations evaluated were no sound; non-spatial (stereo) audio; and two spatial sound conditions (first- and third-order ambisonics). The videos covered various categories and presented both indoor and outdoor scenarios. The subjective responses were analyzed using an ANOVA (Analysis of Variance) to assess mean differences between sound conditions. Data visualization was also employed to enhance the interpretability of the results. The findings reveal diverse viewing patterns, physiological responses, and subjective experiences among users watching 360° videos with different sound conditions. Spatial audio, in particular third-order ambisonics, garnered heightened attention. This is evident in increased pupil dilation and heart rate. Furthermore, the presence of spatial audio led to more diverse head poses when sound sources were distributed across the scene. These findings have important implications for the development of effective techniques for optimizing processing, encoding, distributing, and rendering content in virtual reality (VR) and 360° videos with spatialized audio. These insights are also relevant in the creative realms of content design and enhancement. They provide valuable guidance on how spatial audio influences user attention, physiological responses, and overall subjective experiences. Understanding these dynamics can assist content creators and designers in crafting immersive experiences that leverage spatialized audio to captivate users, enhance engagement, and optimize the overall quality of VR and 360° video content. The dataset, scripts used for data collection, ffmpeg commands used for processing the videos, and the subjective questionnaire and its statistical analysis are publicly available.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4392518508",
    "type": "article"
  },
  {
    "title": "Building Category Graphs Representation with Spatial and Temporal Attention for Visual Navigation",
    "doi": "https://doi.org/10.1145/3653714",
    "publication_date": "2024-03-22",
    "publication_year": 2024,
    "authors": "Xiaobo Hu; Youfang Lin; Hehe Fan; Shuo Wang; Zhihao Wu; Kai Lv",
    "corresponding_authors": "",
    "abstract": "Given an object of interest, visual navigation aims to reach the object’s location based on a sequence of partial observations. To this end, an agent needs to (1) acquire specific knowledge about the relations of object categories in the world during training and (2) locate the target object based on the pre-learned object category relations and its trajectory in the current unseen environment. In this article, we propose a Category Relation Graph (CRG) to learn the knowledge of object category layout relations and a Temporal-Spatial-Region attention (TSR) architecture to perceive the long-term spatial-temporal dependencies of objects, aiding navigation. We establish CRG to learn prior knowledge of object layout and deduce the positions of specific objects. Subsequently, we propose the TSR architecture to capture relationships among objects in temporal, spatial, and regions within observation trajectories. Specifically, we implement a Temporal attention module (T) to model the temporal structure of the observation sequence, implicitly encoding historical moving or trajectory information. Then, a Spatial attention module (S) uncovers the spatial context of the current observation objects based on CRG and past observations. Last, a Region attention module (R) shifts the attention to the target-relevant region. Leveraging the visual representation extracted by our method, the agent accurately perceives the environment and easily learns a superior navigation policy. Experiments on AI2-THOR demonstrate that our CRG-TSR method significantly outperforms existing methods in both effectiveness and efficiency. The supplementary material includes the code and will be publicly available.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4393096665",
    "type": "article"
  },
  {
    "title": "ISF-GAN: Imagine, Select, and Fuse with GPT-Based Text Enrichment for Text-to-Image Synthesis",
    "doi": "https://doi.org/10.1145/3650033",
    "publication_date": "2024-03-28",
    "publication_year": 2024,
    "authors": "Yefei Sheng; Ming Tao; Jie Wang; Bing‐Kun Bao",
    "corresponding_authors": "",
    "abstract": "Text-to-Image synthesis aims to generate an accurate and semantically consistent image from a given text description. However, it is difficult for existing generative methods to generate semantically complete images from a single piece of text. Some works try to expand the input text to multiple captions via retrieving similar descriptions of the input text from the training set but still fail to fill in missing image semantics. In this article, we propose a GAN-based approach to Imagine, Select, and Fuse for Text-to-image synthesis, named ISF-GAN. The proposed ISF-GAN contains Imagine Stage and Select and Fuse Stage to solve the above problems. First, the Imagine Stage proposes a text completion and enrichment module. This module guides a GPT-based model to enrich the text expression beyond the original dataset. Second, the Select and Fuse Stage selects qualified text descriptions and then introduces a cross-modal attentional mechanism to interact these different sentence embeddings with the image features at different scales. In short, our proposed model enriches the input text information for completing missing semantics and introduces a cross-modal attentional mechanism to maximize the utilization of enriched text information to generate semantically consistent images. Experimental results on CUB, Oxford-102, and CelebA-HQ datasets prove the effectiveness and superiority of the proposed network. Code is available at https://github.com/Feilingg/ISF-GAN",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4393260012",
    "type": "article"
  },
  {
    "title": "Paying Attention to Vehicles: A Systematic Review on Transformer-Based Vehicle Re-Identification",
    "doi": "https://doi.org/10.1145/3655623",
    "publication_date": "2024-03-30",
    "publication_year": 2024,
    "authors": "Yan Qian; Johan Barthélemy; Bo Du; Jun Shen",
    "corresponding_authors": "",
    "abstract": "Vehicle re-identification (v-reID) is a crucial and challenging task in the intelligent transportation systems (ITS). While vehicle re-identification plays a role in analysing traffic behaviour, criminal investigation, or automatic toll collection, it is also a key component for the construction of smart cities. With the recent introduction of transformer models and their rapid development in computer vision, vehicle re-identification has also made significant progress in performance and development over 2021-2023. This bite-sized review is the first to summarize existing works in vehicle re-identification using pure transformer models and examine their capabilities. We introduce the various applications and challenges, different datasets, evaluation strategies and loss functions in v-reID. A comparison between existing state-of-the-art methods based on different research areas is then provided. Finally, we discuss possible future research directions and provide a checklist on how to implement a v-reID model. This checklist is useful for an interested researcher or practitioner who is starting their work in this field, and also for anyone who seeks an insight into how to implement an AI model in computer vision using v-reID.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4393342230",
    "type": "review"
  },
  {
    "title": "A Quality-Aware and Obfuscation-Based Data Collection Scheme for Cyber-Physical Metaverse Systems",
    "doi": "https://doi.org/10.1145/3659582",
    "publication_date": "2024-04-16",
    "publication_year": 2024,
    "authors": "Jianheng Tang; Kejia Fan; Wenjie Yin; Shihao Yang; Yajiang Huang; Anfeng Liu; Naixue Xiong; Mianxiong Dong; Tian Wang; Shaobo Zhang",
    "corresponding_authors": "",
    "abstract": "In pursuit of an immersive virtual experience within the Cyber-Physical Metaverse Systems (CPMS), the construction of Avatars often requires a significant amount of real-world data. Mobile Crowd Sensing (MCS) has emerged as an efficient method for collecting data for CPMS. While progress has been made in protecting the privacy of workers, little attention has been given to safeguarding task privacy, potentially exposing the intentions of applications and posing risks to the development of the Metaverse. Additionally, existing privacy protection schemes hinder the exchange of information among entities, inadvertently compromising the quality of the collected data. To this end, we propose a Quality-aware and Obfuscation-based Task Privacy-Preserving (QOTPP) scheme, which protects task privacy and enhances data quality without third-party involvement. The QOTPP scheme initially employs the insight of “showing the fake, and hiding the real” by employing differential privacy techniques to create fake tasks and conceal genuine ones. Additionally, we introduce a two-tier truth discovery mechanism using Deep Matrix Factorization (DMF) to efficiently identify high-quality workers. Furthermore, we propose a Combinatorial Multi-Armed Bandit (CMAB)-based worker incentive and selection mechanism to improve the quality of data collection. Theoretical analysis confirms that our QOTPP scheme satisfies essential properties such as truthfulness, individual rationality, and ϵ-differential privacy. Extensive simulation experiments validate the state-of-the-art performance achieved by QOTPP.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4394844889",
    "type": "article"
  },
  {
    "title": "EdiTor: Edge-guided Transformer for Ghost-free High Dynamic Range Imaging",
    "doi": "https://doi.org/10.1145/3657293",
    "publication_date": "2024-04-27",
    "publication_year": 2024,
    "authors": "Yuanshen Guan; Ruikang Xu; Mingde Yao; Jie Huang; Zhiwei Xiong",
    "corresponding_authors": "",
    "abstract": "Synthesizing the high dynamic range (HDR) image from multi-exposure images has been extensively studied by exploiting convolutional neural networks (CNNs) recently. Despite the remarkable progress, existing CNN-based methods have the intrinsic limitation of local receptive field, which hinders the model’s capability of capturing long-range correspondence and large motions across under/over-exposure images, resulting in ghosting artifacts of dynamic scenes. To address the above challenge, we propose a novel Ed ge-gu i ded T ransf or mer framework (EdiTor) customized for ghost-free HDR reconstruction, where the long-range motions across different exposures can be delicately modeled by incorporating the edge prior. Specifically, EdiTor calculates patch-wise correlation maps on both image and edge domains, enabling the network to effectively model the global movements and the fine-grained shifts across multiple exposures. Based on this framework, we further propose an exposure-masked loss to adaptively compensate for the severely distorted regions ( e.g. , highlights and shadows). Experiments demonstrate that EdiTor outperforms state-of-the-art methods both quantitatively and qualitatively, achieving appealing HDR visualization with unified textures and colors.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4395691463",
    "type": "article"
  },
  {
    "title": "AGAR - Attention Graph-RNN for Adaptative Motion Prediction of Point Clouds of Deformable Objects",
    "doi": "https://doi.org/10.1145/3662183",
    "publication_date": "2024-05-01",
    "publication_year": 2024,
    "authors": "Pedro Gomes; Silvia Rossi; Laura Toni",
    "corresponding_authors": "",
    "abstract": "This article focuses on motion prediction for point cloud sequences in the challenging case of deformable 3D objects, such as human body motion. First, we investigate the challenges caused by deformable shapes and complex motions present in this type of representation, with the ultimate goal of understanding the technical limitations of state-of-the-art models. From this understanding, we propose an improved architecture for point cloud prediction of deformable 3D objects. Specifically, to handle deformable shapes, we propose a graph-based approach that learns and exploits the spatial structure of point clouds to extract more representative features. Then, we propose a module able to combine the learned features in a adaptative manner according to the point cloud movements. The proposed adaptative module controls the composition of local and global motions for each point, enabling the network to model complex motions in deformable 3D objects more effectively. We tested the proposed method on the following datasets: MNIST moving digits, the Mixamo human bodies motions [ 15 ], JPEG [ 5 ] and CWIPC-SXR [ 32 ] real-world dynamic bodies. Simulation results demonstrate that our method outperforms the current baseline methods given its improved ability to model complex movements as well as preserve point cloud shape. Furthermore, we demonstrate the generalizability of the proposed framework for dynamic feature learning by testing the framework for action recognition on the MSRAction3D dataset [ 19 ] and achieving results on par with state-of-the-art methods.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4396560657",
    "type": "article"
  },
  {
    "title": "Style Variable and Irrelevant Learning for Generalizable Person Re-identification",
    "doi": "https://doi.org/10.1145/3671003",
    "publication_date": "2024-06-06",
    "publication_year": 2024,
    "authors": "Kai Lv; H. Y. Chen; Chuyang Zhao; Kai Tu; Junru Chen; Y. Li; Boxun Li; Youfang Lin",
    "corresponding_authors": "",
    "abstract": "Domain generalization person re-identification (DG-ReID) has gained much attention recently due to the poor performance of supervised re-identification on unseen domains. The goal of domain generalization is to develop a model that is insensitive to domain bias and can perform well across different domains. In this article, We conduct experiments to verify the importance of style factors in domain bias. Specifically, the experiments are to affirm that style bias across different domains significantly contributes to domain bias. Based on this observation, we propose style variable and irrelevant learning (SVIL) to eliminate the influence of style factors on the model. Specifically, we employ a style jitter module (SJM) that enhances the style diversity of a specific source domain and reduces the style differences among various source domains. This allows the model to focus on identity-relevant information and be robust to style changes. We also integrate the SJM module with a meta-learning algorithm to further enhance the model’s generalization ability. Notably, our SJM module is easy to implement and does not add any inference cost. Our extensive experiments demonstrate the effectiveness of our approach, which outperforms existing methods on DG-ReID benchmarks.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4399388452",
    "type": "article"
  },
  {
    "title": "SCAE: Structural Contrastive Auto-encoder for Incomplete Multi-view Representation Learning",
    "doi": "https://doi.org/10.1145/3672078",
    "publication_date": "2024-06-07",
    "publication_year": 2024,
    "authors": "Mengran Li; Ronghui Zhang; Yong Zhang; Xinglin Piao; Shiyu Zhao; Baocai Yin",
    "corresponding_authors": "",
    "abstract": "Describing an object from multiple perspectives often leads to incomplete data representation. Consequently, learning consistent representations for missing data from multiple views has emerged as a key focus in the realm of Incomplete Multi-view Representation Learning (IMRL). In recent years, various strategies such as subspace learning, matrix decomposition, and deep learning have been harnessed to develop numerous IMRL methods. In this paper, our primary research revolves around IMRL, with a particular emphasis on addressing two main challenges. Firstly, we delve into the effective integration of intra-view similarity and contextual structure into a unified framework. Secondly, we explore the effective facilitation of information exchange and fusion across multiple views. To tackle these issues, we propose a deep learning approach known as Structural Contrastive Auto-encoder (SCAE) to solve the challenges of IMRL. SCAE comprises two major components: Intra-View Structural Representation Learning and Inter-View Contrastive Representation Learning. The former involves capturing intra-view similarity by minimizing the Dirichlet energy of the feature matrix, while also applying spatial dispersion regularization to capture intra-view contextual structure. The latter encourages maximizing the mutual information of inter-view representations, facilitating information exchange and fusion across views. Experimental results demonstrate the efficacy of our approach in significantly enhancing model accuracy and robustly addressing IMRL problems. The code is available at https://github.com/limengran98/SCAE .",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4399431904",
    "type": "article"
  },
  {
    "title": "Privacy-Enhanced Prototype-based Federated Cross-modal Hashing for Cross-modal Retrieval",
    "doi": "https://doi.org/10.1145/3674507",
    "publication_date": "2024-06-25",
    "publication_year": 2024,
    "authors": "Ruifan Zuo; C. Zheng; Fengling Li; Lei Zhu; Zheng Zhang",
    "corresponding_authors": "",
    "abstract": "Cross-modal hashing is widely used for efficient similarity searches, improving data processing efficiency, and reducing storage costs. Existing cross-modal hashing methods primarily focus on centralized training scenarios, where fixed-scale and fixed-category multi-modal data is collected beforehand. However, these methods often face challenges associated with the potential risk of privacy breaches and high data communication costs during data transmission in real-world multimedia retrieval tasks. To tackle these challenges, in this article, we propose an efficient privacy-enhanced prototype-based federated cross-modal hashing (PEPFCH). In PEPFCH, we integrate local and global prototypes in order to effectively capture the distinctive traits of individual clients, while also harnessing the collective intelligence of the entire federated learning system. Moreover, to ensure the security of prototype information and prevent its disclosure during the aggregation process, we use a prototype encryption transmission mechanism to encrypt the prototype information before transmission, making it challenging for attackers to gain access to sensitive data. Additionally, to facilitate personalized federated learning and alleviate the issue of parametric catastrophic forgetting, we establish the image and text hyper-networks for each client and adopt a hyper-network extension strategy to selectively preserve and update previously acquired knowledge when acquiring new concepts or categories. Comprehensive experiments highlight the efficiency and superiority of our proposed method. To enhance research and accessibility, we have publicly released our source codes at: https://github.com/vindahi/PEPFCH .",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4400002930",
    "type": "article"
  },
  {
    "title": "Multi-modal LiDAR Point Cloud Semantic Segmentation with Salience Refinement and Boundary Perception",
    "doi": "https://doi.org/10.1145/3674979",
    "publication_date": "2024-07-01",
    "publication_year": 2024,
    "authors": "Yong Zhou; Zeming Xie; Jiaqi Zhao; Wenliang Du; Rui Yao; Abdulmotaleb El Saddik",
    "corresponding_authors": "",
    "abstract": "Point cloud segmentation is essential for scene understanding, which provides advanced information for many applications, such as autonomous driving, robots, and virtual reality. To improve the accuracy and robustness of point cloud segmentation, many researchers have attempted to fuze camera images to complement the color and texture information. The common fusion strategy is the combination of convolutional operations with concatenation, element-wise addition or element-wise multiplication. However, conventional convolutional operators tend to confine the fusion of modal features within their receptive fields, which can be incomplete and limited. In addition, the inability of encoder–decoder segmentation networks to explicitly perceive segmentation boundary information results in semantic ambiguity and classification errors at object edges. These errors are further amplified in point cloud segmentation tasks, significantly affecting the accuracy of point cloud segmentation. To address the above issues, we propose a novel self-attention multi-modal fusion semantic segmentation network for point cloud semantic segmentation. Firstly, to effectively fuze different modal features, we propose a self-cross fusion module (SCF), which models long-range modality dependencies and transfers complementary image information to the point cloud to fully leverage the modality-specific advantages. Secondly, we design the salience refinement module (SR), which calculates the importance of channels in the feature maps and global descriptors to enhance the representation capability of salient modal features. Finally, we propose the local-aware anisotropy loss measure the element-level importance in the data and explicitly provide boundary information for the model, which alleviates the inherent semantic ambiguity problem in segmentation networks. Extensive experiments on two benchmark datasets demonstrate that our proposed method surpasses current state-of-the-art methods.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4400199679",
    "type": "article"
  },
  {
    "title": "Multi-Level Fusion for Robust RGBT Tracking via Enhanced Thermal Representation",
    "doi": "https://doi.org/10.1145/3678176",
    "publication_date": "2024-07-15",
    "publication_year": 2024,
    "authors": "Zhangyong Tang; Tianyang Xu; Xiao-Jun Wu; Josef Kittler",
    "corresponding_authors": "",
    "abstract": "Due to the limitations of visible (RGB) sensors in challenging scenarios, such as nighttime and foggy environments, the thermal infrared (TIR) modality draws increasing attention as an auxiliary source for robust tracking systems. Currently, the existing methods extract both the RGB and TIR (RGBT) clues in a similar approach, i.e., utilising RGB-pretrained models with or without finetuning, and then aggregate the multi-modal information through a fusion block embedded in a single level. However, the different imaging principles of RGB and TIR data raise questions about the suitability of RGB-pretrained models for thermal data. In this article, it is argued that the modality gap is overlooked, and an alternative training paradigm is proposed for TIR data to ensure consistency between the training and test data, which is achieved by optimising the TIR feature extractor with only TIR data involved. Furthermore, with the goal of making better use of the enhanced thermal representations, a multi-level fusion strategy is inspired by the observation that various fusion strategies at different levels can contribute to a better performance. Specifically, fusion modules at both the feature and decision levels are derived for a comprehensive fusion procedure while the pixel-level fusion strategy is not considered due to the misalignment of multi-modal image pairs. The effectiveness of our method is demonstrated by extensive qualitative and quantitative experiments conducted on several challenging benchmarks. Code will be released at https://github.com/Zhangyong-Tang/MELT .",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4400668214",
    "type": "article"
  },
  {
    "title": "Creating High-quality 3D Content by Bridging the Gap Between Text-to-2D and Text-to-3D Generation",
    "doi": "https://doi.org/10.1145/3687475",
    "publication_date": "2024-08-28",
    "publication_year": 2024,
    "authors": "Yiwei Ma; Yijun Fan; Jiayi Ji; Haowei Wang; Haibing Yin; Xiaoshuai Sun; Rongrong Ji",
    "corresponding_authors": "",
    "abstract": "In recent times, automatic text-to-3D content creation has made significant progress, driven by the development of pretrained 2D diffusion models. Existing text-to-3D methods typically optimize the 3D representation to ensure that the rendered image aligns well with the given text, as evaluated by the pretrained 2D diffusion model. Nevertheless, a substantial domain gap exists between 2D images and 3D assets, primarily attributed to variations in camera-related attributes and the exclusive presence of foreground objects. Consequently, employing 2D diffusion models directly for optimizing 3D representations may lead to suboptimal outcomes. To address this issue, we present X-Dreamer, a novel approach for high-quality text-to-3D content creation that effectively bridges the gap between text-to-2D and text-to-3D synthesis. The key components of X-Dreamer are two innovative designs: Camera-Guided Low-Rank Adaptation (CG-LoRA) and Attention-Mask Alignment (AMA) Loss. CG-LoRA dynamically incorporates camera information into the pretrained diffusion models by employing camera-dependent generation for trainable parameters. This integration makes the 2D diffusion model camera-sensitive. AMA loss guides the attention map of the pretrained diffusion model using the binary mask of the 3D object, prioritizing the creation of the foreground object. This module ensures that the model focuses on generating accurate and detailed foreground objects. Extensive evaluations demonstrate the effectiveness of our proposed method compared to existing text-to-3D approaches. Our project webpage: https://anonymous-11111.github.io/ . Our code is available at https://github.com/xmu-xiaoma666/X-Dreamer .",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4401946592",
    "type": "article"
  },
  {
    "title": "Can Linguistic Knowledge Improve Multimodal Alignment in Vision-Language Pretraining?",
    "doi": "https://doi.org/10.1145/3690640",
    "publication_date": "2024-08-29",
    "publication_year": 2024,
    "authors": "F. Wang; Liang Ding; Jun Rao; Ye Liu; Li Shen; Changxing Ding",
    "corresponding_authors": "",
    "abstract": "The field of multimedia research has witnessed significant interest in leveraging multimodal pretrained neural network models to perceive and represent the physical world. Among these models, vision-language pretraining (VLP) has emerged as a captivating topic. Currently, the prevalent approach in VLP involves supervising the training process with paired image-text data. However, limited efforts have been dedicated to exploring the extraction of essential linguistic knowledge, such as semantics and syntax, during VLP and understanding its impact on multimodal alignment. In response, our study aims to shed light on the influence of comprehensive linguistic knowledge encompassing semantic expression and syntactic structure on multimodal alignment. To achieve this, we introduce SNARE , a large-scale multimodal alignment probing benchmark designed specifically for the detection of vital linguistic components, including lexical, semantic, and syntax knowledge. SNARE offers four distinct tasks: Semantic Structure, Negation Logic, Attribute Ownership, and Relationship Composition. Leveraging SNARE , we conduct holistic analyses of six advanced VLP models (BLIP, CLIP, Flava, X-VLM, BLIP2, and GPT-4), along with human performance, revealing key characteristics of the VLP model: (i) Insensitivity to complex syntax structures, relying primarily on content words for sentence comprehension. (ii) Limited comprehension of sentence combinations and negations. (iii) Challenges in determining actions or spatial relations within visual information, as well as difficulties in verifying the correctness of ternary relationships. Based on these findings, we propose the following strategies to enhance multimodal alignment in VLP: (1) Utilize a large generative language model as the language backbone in VLP to facilitate the understanding of complex sentences. (2) Establish high-quality datasets that emphasize content words and employ simple syntax, such as short-distance semantic composition, to improve multimodal alignment. (3) Incorporate more fine-grained visual knowledge, such as spatial relationships, into pretraining objectives. 1",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4401993157",
    "type": "article"
  },
  {
    "title": "Meta-Review on Brain-Computer Interface (BCI) in the Metaverse",
    "doi": "https://doi.org/10.1145/3696109",
    "publication_date": "2024-09-14",
    "publication_year": 2024,
    "authors": "Kamran Gholizadeh HamlAbadi; Fedwa Laamarti; Abdulmotaleb El Saddik",
    "corresponding_authors": "",
    "abstract": "This paper presents a comprehensive meta-review of the intersection between Brain-Computer Interface (BCI) technologies and the Metaverse, emphasizing the enhancement of immersive experiences through VR, AR, MR, XR, Digital Twin, and haptic interfaces. The study classifies BCI devices into wearable and non-wearable categories, with a focus on their applications in robotics. It explores BCI user feedback mechanisms and their impact on medical and non-medical settings, including personalized rehabilitation and immersive gaming. The review introduces two frameworks for leveraging the Metaverse to navigate multisensory integration between BCI and Assistive Devices. Applications such as VR therapies for stroke patients and neuro-responsive multiplayer gaming environments showcase the potential of BCIs to enhance Metaverse interactions. To the best of our knowledge, this is the first meta-review on the integration of BCI and the Metaverse, identifying key challenges and research gaps, and serves as a foundational reference for future research and development in this interdisciplinary field.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4402534277",
    "type": "article"
  },
  {
    "title": "Middleware for streaming 3D progressive meshes over lossy networks",
    "doi": "https://doi.org/10.1145/1201730.1201733",
    "publication_date": "2006-11-01",
    "publication_year": 2006,
    "authors": "Huimin Li; Mingqing Li; Balakrishnan Prabhakaran",
    "corresponding_authors": "",
    "abstract": "Streaming 3D graphics have been widely used in multimedia applications such as online gaming and virtual reality. However, a gap exists between the zero-loss-tolerance of the existing compression schemes and the lossy network transmissions. In this article, we propose a generic 3D middleware between the 3D application layer and the transport layer for the transmission of triangle-based progressively compressed 3D models. Significant features of the proposed middleware include. 1) handling 3D compressed data streams from multiple progressive compression techniques. 2) considering end user hardware capabilities for effectively saving the data size for network delivery. 3) a minimum cost dynamic reliable set selector to choose the transport protocol for each sublayer based on the real-time network traffic. Extensive simulations with TCP/UDP and SCTP show that the proposed 3D middleware can achieve the dual objectives of maintaining low transmission delay and small distortion, and thus supporting high quality 3D streaming with high flexibility.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2076920864",
    "type": "article"
  },
  {
    "title": "A Semantic Web ontology for context-based classification and retrieval of music resources",
    "doi": "https://doi.org/10.1145/1152149.1152151",
    "publication_date": "2006-08-01",
    "publication_year": 2006,
    "authors": "Alfio Ferrara; Luca A. Ludovico; Stefano Montanelli; Silvana Castano; Goffredo Haus",
    "corresponding_authors": "",
    "abstract": "In this article, we describe the MX-Onto ontology for providing a Semantic Web compatible representation of music resources based on their context. The context representation is realized by means of an OWL ontology that describes music information and that defines rules and classes for a flexible genre classification. By flexible classification we mean that the proposed approach enables capturing the subjective interpretation of music genres by defining multiple membership relations between a music resource and the corresponding music genres, thus supporting context-based and proximity-based search of music resources.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2093661684",
    "type": "article"
  },
  {
    "title": "Introduction to special issue on eye-tracking applications in multimedia systems",
    "doi": "https://doi.org/10.1145/1314303.1314304",
    "publication_date": "2007-12-01",
    "publication_year": 2007,
    "authors": "Gheorghiță Ghinea; Chaabane Djéraba; Stephen R. Gulliver; Kara Pernice Coyne",
    "corresponding_authors": "",
    "abstract": "introduction Introduction to special issue on eye-tracking applications in multimedia systems Editors: Gheorghita Ghinea View Profile , Chabane Djeraba View Profile , Stephen Gulliver View Profile , Kara Pernice Coyne View Profile Authors Info & Claims ACM Transactions on Multimedia Computing, Communications, and ApplicationsVolume 3Issue 4December 2007 Article No.: 1pp 1–4https://doi.org/10.1145/1314303.1314304Published:12 December 2007Publication History 5citation519DownloadsMetricsTotal Citations5Total Downloads519Last 12 Months8Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W1974744554",
    "type": "article"
  },
  {
    "title": "Goal-oriented optimal subset selection of correlated multimedia streams",
    "doi": "https://doi.org/10.1145/1198302.1198304",
    "publication_date": "2007-02-01",
    "publication_year": 2007,
    "authors": "Pradeep K. Atrey; Mohan Kankanhalli; B. John Oommen",
    "corresponding_authors": "",
    "abstract": "A multimedia analysis system utilizes a set of correlated media streams, each of which, we assume, has a confidence level and a cost associated with it, and each of which partially helps in achieving the system goal. However, the fact that at any instant, not all of the media streams contribute towards a system goal brings up the issue of finding the best subset from the available set of media streams. For example, a subset of two video cameras and two microphones could be better than any other subset of sensors at some time instance to achieve a surveillance goal (e.g. event detection). This article presents a novel framework that finds the optimal subset of media streams so as to achieve the system goal under specified constraints. The proposed framework uses a dynamic programming approach to find the optimal subset of media streams based on three different criteria: first, by maximizing the probability of achieving the goal under the specified cost and confidence; second, by maximizing the confidence in the achieved goal under the specified cost and probability with which the goal is achieved; and third, by minimizing the cost to achieve the goal with a specified probability and confidence. Each of these problems is proven to be NP-Complete. From an AI point of view, the solution we propose is heuristic-based, and for each criterion, utilizes a heuristic function which for a given problem, combines optimal solutions of small-sized subproblems to yield a potential near-optimal solution to the original problem. The proposed framework allows for a tradeoff among the aforementioned three criteria, and offers the flexibility to compare whether any one set of media streams of low cost would be better than any other set of higher cost, or whether any one set of media streams of high confidence would be better than any other set of low confidence. To show the utility of our framework, we provide the experimental results for event detection in a surveillance scenario.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2110291364",
    "type": "article"
  },
  {
    "title": "An assessment of eye-gaze potential within immersive virtual environments",
    "doi": "https://doi.org/10.1145/1314303.1314311",
    "publication_date": "2007-12-01",
    "publication_year": 2007,
    "authors": "Norman Murray; Dave Roberts; Anthony Steed; Paul Sharkey; Paul Dickerson; John Rae",
    "corresponding_authors": "",
    "abstract": "In collaborative situations, eye gaze is a critical element of behavior which supports and fulfills many activities and roles. In current computer-supported collaboration systems, eye gaze is poorly supported. Even in a state-of-the-art video conferencing system such as the access grid, although one can see the face of the user, much of the communicative power of eye gaze is lost. This article gives an overview of some preliminary work that looks towards integrating eye gaze into an immersive collaborative virtual environment and assessing the impact that this would have on interaction between the users of such a system. Three experiments were conducted to assess the efficacy of eye gaze within immersive virtual environments. In each experiment, subjects observed on a large screen the eye-gaze behavior of an avatar. The eye-gaze behavior of that avatar had previously been recorded from a user with the use of a head-mounted eye tracker. The first experiment was conducted to assess the difference between users' abilities to judge what objects an avatar is looking at with only head gaze being viewed and also with eye- and head-gaze data being displayed. The results from the experiment show that eye gaze is of vital importance to the subjects, correctly identifying what a person is looking at in an immersive virtual environment. The second experiment examined whether a monocular or binocular eye-tracker would be required. This was examined by testing subjects' ability to identify where an avatar was looking from their eye direction alone, or by eye direction combined with convergence. This experiment showed that convergence had a significant impact on the subjects' ability to identify where the avatar was looking. The final experiment looked at the effects of stereo and mono-viewing of the scene, with the subjects being asked to identify where the avatar was looking. This experiment showed that there was no difference in the subjects' ability to detect where the avatar was gazing. This is followed by a description of how the eye-tracking system has been integrated into an immersive collaborative virtual environment and some preliminary results from the use of such a system.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2040813014",
    "type": "article"
  },
  {
    "title": "Constructing visual phrases for effective and efficient object-based image retrieval",
    "doi": "https://doi.org/10.1145/1404880.1404887",
    "publication_date": "2008-10-01",
    "publication_year": 2008,
    "authors": "Qingfang Zheng; Wen Gao",
    "corresponding_authors": "",
    "abstract": "The explosion of multimedia data necessitates effective and efficient ways for us to get access to our desired ones. In this article, we draw an analogy between image retrieval and text retrieval and propose a visual phrase-based approach to retrieve images containing desired objects (object-based image retrieval). The visual phrase is defined as a pair of frequently co-occurred adjacent local image patches and is constructed using data mining. We design methods on how to construct visual phrase and how to index/search images based on visual phrase. We demonstrate experiments to show our visual phrase-based approach can be very efficient and more effective than current visual word-based approach.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2000033362",
    "type": "article"
  },
  {
    "title": "Real-time H.264 video encoding in software with fast mode decision and dynamic complexity control",
    "doi": "https://doi.org/10.1145/1671954.1671959",
    "publication_date": "2010-02-01",
    "publication_year": 2010,
    "authors": "Yuri V. Ivanov; Chris J. Bleakley",
    "corresponding_authors": "",
    "abstract": "This article presents a novel real-time algorithm for reducing and dynamically controlling the computational complexity of an H.264 video encoder implemented in software. A fast mode decision algorithm, based on a Pareto-optimal macroblock classification scheme, is combined with a dynamic complexity control algorithm that adjusts the MB class decisions such that a constant frame rate is achieved. The average coding efficiency of the proposed algorithm was found to be similar to that of conventional encoding operating at half the frame rate. The proposed algorithm was found to provide lower average bitrate and distortion than static complexity scaling.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2054414024",
    "type": "article"
  },
  {
    "title": "Recognition of adult images, videos, and web page bags",
    "doi": "https://doi.org/10.1145/2037676.2037685",
    "publication_date": "2011-10-01",
    "publication_year": 2011,
    "authors": "Weiming Hu; Haiqiang Zuo; Ou Wu; Yunfei Chen; Zhongfei Zhang; David Suter",
    "corresponding_authors": "",
    "abstract": "In this article, we develop an integrated adult-content recognition system which can detect adult images, adult videos, and adult Web page bags, where a Web page bag consists of a Web page and a predefined number of Web pages linked to it through hyperlinks. In our adult image-recognition algorithm, we model skin patches rather than skin pixels, resulting in better results than state-of-the-art algorithms which model skin pixels. In our adult video-recognition algorithm, information from the accompanying audio section around an image in an adult video is used to obtain a prior classification of the image. The algorithm achieves a better performance than the ones which use image information alone or audio information alone. The adult Web page bag recognition is carried out using multi-instance learning based on the combination of classifying texts, images and videos in Web pages. Both the speed and the accuracy for recognizing the Web adult content are increased, in contrast to recognizing Web pages one-by-one.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2010373969",
    "type": "article"
  },
  {
    "title": "Towards optimizing human labeling for interactive image tagging",
    "doi": "https://doi.org/10.1145/2501643.2501651",
    "publication_date": "2013-08-01",
    "publication_year": 2013,
    "authors": "Jinhui Tang; Qiang Chen; Meng Wang; Shuicheng Yan; Tat‐Seng Chua; Ramesh Jain",
    "corresponding_authors": "",
    "abstract": "Interactive tagging is an approach that combines human and computer to assign descriptive keywords to image contents in a semi-automatic way. It can avoid the problems in automatic tagging and pure manual tagging by achieving a compromise between tagging performance and manual cost. However, conventional research efforts on interactive tagging mainly focus on sample selection and models for tag prediction. In this work, we investigate interactive tagging from a different aspect. We introduce an interactive image tagging framework that can more fully make use of human's labeling efforts. That means, it can achieve a specified tagging performance by taking less manual labeling effort or achieve better tagging performance with a specified labeling cost. In the framework, hashing is used to enable a quick clustering of image regions and a dynamic multiscale clustering labeling strategy is proposed such that users can label a large group of similar regions each time. We also employ a tag refinement method such that several inappropriate tags can be automatically corrected. Experiments on a large dataset demonstrate the effectiveness of our approach",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W1970007001",
    "type": "article"
  },
  {
    "title": "How far we've come",
    "doi": "https://doi.org/10.1145/2491844",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Shih‐Fu Chang",
    "corresponding_authors": "Shih‐Fu Chang",
    "abstract": "This article reviews the major research trends that emerged in the last two decades within the broad area of multimedia information retrieval, with a focus on the ACM Multimedia community. Trends are defined (nonscientifically) to be topics that appeared in ACM multimedia publications and have had a significant number of citations. The article also assesses the impacts of these trends on real-world applications. The views expressed are subjective and likely biased but hopefully useful for understanding the heritage of the community and stimulating new research direction.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W1989160572",
    "type": "article"
  },
  {
    "title": "Mobile haptic e-book system to support 3D immersive reading in ubiquitous environments",
    "doi": "https://doi.org/10.1145/2501643.2501649",
    "publication_date": "2013-08-01",
    "publication_year": 2013,
    "authors": "Kazi Masudul Alam; A. Rahman; Abdulmotaleb El Saddik",
    "corresponding_authors": "",
    "abstract": "In order to leverage the use of various modalities such as audio-visual materials in instilling effective learning behavior we present an intuitive approach of annotation based hapto-audio-visual interaction with the traditional digital learning materials such as e-books. By integrating the home entertainment system in the user's reading experience combined with haptic interfaces we want to examine whether such augmentation of modalities influence the user's learning patterns. The proposed Haptic E--Book (HE-Book) system leverages the haptic jacket, haptic arm band as well as haptic sofa interfaces to receive haptic emotive signals wirelessly in the form of patterned vibrations of the actuators and expresses the learning material by incorporating image, video, 3D environment based augmented display in order to pave ways for intimate reading experience in the popular mobile e-book platform.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2050539945",
    "type": "article"
  },
  {
    "title": "Multimedia retrieval that matters",
    "doi": "https://doi.org/10.1145/2490827",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Alan Hanjalić",
    "corresponding_authors": "Alan Hanjalić",
    "abstract": "This article emphasizes the need to refocus multimedia information retrieval (MIR) research towards bridging the utility gap , the gap between the expected and defacto usefulness of MIR solutions. This requires us to revisit the notion of relevance, but also to consider other criteria for assessing MIR solutions, like the informativeness of the retrieved results and how helpful they are for the users. The article also states that this focus shift cannot be realized incrementally, but by revisiting the foundations of MIR solutions, that is, by a utility-by-design approach. In this respect, a number of research challenges are proposed.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2040001953",
    "type": "article"
  },
  {
    "title": "Scheduling a Video Transcoding Server to Save Energy",
    "doi": "https://doi.org/10.1145/2700282",
    "publication_date": "2015-02-24",
    "publication_year": 2015,
    "authors": "Minseok Song; Yeongju Lee; Jin-Han Park",
    "corresponding_authors": "",
    "abstract": "Recent popular streaming services such as TV Everywhere, N-Screen, and dynamic adaptive streaming over HTTP (DASH) need to deliver content to the wide range of devices, requiring video content to be transcoded into different versions. Transcoding tasks require a lot of computation, and each task typically has its own real-time constraint. These make it difficult to manage transcoding, but the more efficient use of energy in servers is an imperative. We characterize transcoding workloads in terms of deadlines and computation times, and propose a new dynamic voltage and frequency scaling (DVFS) scheme that allocates a frequency and a workload to each CPU with the aim of minimizing power consumption while meeting all transcoding deadlines. This scheme has been simulated, and also implemented in a Linux transcoding server, in which a frontend node distributes transcoding requests to heterogeneous backend nodes. This required a new protocol for communication between nodes, a DVFS management scheme to reduce power consumption and thread management and scheduling schemes which ensure that transcoding deadlines are met. Power measurements show that this approach can reduce system-wide energy consumption by 17% to 31%, compared with the Linux Ondemand governor.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2092732709",
    "type": "article"
  },
  {
    "title": "A Video Bitrate Adaptation and Prediction Mechanism for HTTP Adaptive Streaming",
    "doi": "https://doi.org/10.1145/3052822",
    "publication_date": "2017-03-21",
    "publication_year": 2017,
    "authors": "Ashkan Sobhani; Abdulsalam Yassine; Shervin Shirmohammadi",
    "corresponding_authors": "",
    "abstract": "The Hypertext Transfer Protocol (HTTP) Adaptive Streaming (HAS) has now become ubiquitous and accounts for a large amount of video delivery over the Internet. But since the Internet is prone to bandwidth variations, HAS's up and down switching between different video bitrates to keep up with bandwidth variations leads to a reduction in Quality of Experience (QoE). In this article, we propose a video bitrate adaptation and prediction mechanism based on Fuzzy logic for HAS players, which takes into consideration the estimate of available network bandwidth as well as the predicted buffer occupancy level in order to proactively and intelligently respond to current conditions. This leads to two contributions: First, it allows HAS players to take appropriate actions, sooner than existing methods, to prevent playback interruptions caused by buffer underrun, reducing the ON-OFF traffic phenomena associated with current approaches and increasing the QoE. Second, it facilitates fair sharing of bandwidth among competing players at the bottleneck link. We present the implementation of our proposed mechanism and provide both empirical/QoE analysis and performance comparison with existing work. Our results show that, compared to existing systems, our system has (1) better fairness among multiple competing players by almost 50% on average and as much as 80% as indicated by Jain's fairness index and (2) better perceived quality of video by almost 8% on average and as much as 17%, according to the estimate the Mean Opinion Score (eMOS) model.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2602467456",
    "type": "article"
  },
  {
    "title": "Enhancing Transmission Collision Detection for Distributed TDMA in Vehicular Networks",
    "doi": "https://doi.org/10.1145/3092833",
    "publication_date": "2017-07-14",
    "publication_year": 2017,
    "authors": "Sailesh Bharati; Hassan Aboubakr Omar; Weihua Zhuang",
    "corresponding_authors": "",
    "abstract": "The increasing number of road accidents has led to the evolution of vehicular ad hoc networks (VANETs), which allow vehicles and roadside infrastructure to continuously broadcast safety messages, including necessary information to avoid undesired events on the road. To support reliable broadcast of safety messages, distributed time division multiple access (D-TDMA) protocols are proposed for medium access control in VANETs. Existing D-TDMA protocols react to a transmission failure without distinguishing whether the failure comes from a transmission collision or from a poor radio channel condition, resulting in degraded performance. In this article, we present the importance of transmission failure differentiation due to a poor channel or due to a transmission collision for D-TDMA protocols in vehicular networks. We study the effects of such a transmission failure differentiation on the performance of a node when reserving a time slot to access the transmission channel. Furthermore, we propose a method for transmission failure differentiation, employing the concept of deep-learning techniques, for a node to decide whether to release or continue using its acquired time slot. The proposed method is based on the application of a Markov chain model to estimate the channel state when a transmission failure occurs. The Markov model parameters are dynamically updated by each node (i.e., vehicle or roadside unit) based on information included in the safety messages that are periodically received from neighboring nodes. In addition, from the D-TDMA protocol headers of received messages, a node approximately determines the error in estimating the channel state based on the proposed Markov model and then uses this channel estimation error to further improve subsequent channel state estimations. Through mathematical analysis, we show that transmission failure differentiation, or transmission collision detection, helps a node to efficiently reserve a time slot even with a large number of nodes contending for time slots. Furthermore, through extensive simulations in a highway scenario, we demonstrate that the proposed solution significantly improves the performance of D-TDMA protocols by reducing unnecessary contention on the available time slots, thus increasing the number of nodes having unique time slots for successful broadcast of safety messages.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2735210822",
    "type": "article"
  },
  {
    "title": "Deep Bidirectional Cross-Triplet Embedding for Online Clothing Shopping",
    "doi": "https://doi.org/10.1145/3152114",
    "publication_date": "2018-01-04",
    "publication_year": 2018,
    "authors": "Shuhui Jiang; Yue Wu; Yun Fu",
    "corresponding_authors": "",
    "abstract": "In this article, we address the cross-domain (i.e., street and shop) clothing retrieval problem and investigate its real-world applications for online clothing shopping. It is a challenging problem due to the large discrepancy between street and shop domain images. We focus on learning an effective feature-embedding model to generate robust and discriminative feature representation across domains. Existing triplet embedding models achieve promising results by finding an embedding metric in which the distance between negative pairs is larger than the distance between positive pairs plus a margin. However, existing methods do not address the challenges in the cross-domain clothing retrieval scenario sufficiently. First, the intradomain and cross-domain data relationships need to be considered simultaneously. Second, the number of matched and nonmatched cross-domain pairs are unbalanced. To address these challenges, we propose a deep cross-triplet embedding algorithm together with a cross-triplet sampling strategy. The extensive experimental evaluations demonstrate the effectiveness of the proposed algorithms well. Furthermore, we investigate two novel online shopping applications, clothing trying on and accessories recommendation, based on a unified cross-domain clothing retrieval framework.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2782510554",
    "type": "article"
  },
  {
    "title": "Boosted Multifeature Learning for Cross-Domain Transfer",
    "doi": "https://doi.org/10.1145/2700286",
    "publication_date": "2015-02-05",
    "publication_year": 2015,
    "authors": "Xiaoshan Yang; Tianzhu Zhang; Changsheng Xu; Ming–Hsuan Yang",
    "corresponding_authors": "",
    "abstract": "Conventional learning algorithm assumes that the training data and test data share a common distribution. However, this assumption will greatly hinder the practical application of the learned model for cross-domain data analysis in multimedia. To deal with this issue, transfer learning based technology should be adopted. As a typical version of transfer learning, domain adaption has been extensively studied recently due to its theoretical value and practical interest. In this article, we propose a boosted multifeature learning (BMFL) approach to iteratively learn multiple representations within a boosting procedure for unsupervised domain adaption. The proposed BMFL method has a number of properties. (1) It reuses all instances with different weights assigned by the previous boosting iteration and avoids discarding labeled instances as in conventional methods. (2) It models the instance weight distribution effectively by considering the classification error and the domain similarity, which facilitates learning new feature representation to correct the previously misclassified instances. (3) It learns multiple different feature representations to effectively bridge the source and target domains. We evaluate the BMFL by comparing its performance on three applications: image classification, sentiment classification and spam filtering. Extensive experimental results demonstrate that the proposed BMFL algorithm performs favorably against state-of-the-art domain adaption methods.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W1972370969",
    "type": "article"
  },
  {
    "title": "Human perception of haptic-to-video and haptic-to-audio skew in multimedia applications",
    "doi": "https://doi.org/10.1145/2457450.2457451",
    "publication_date": "2013-05-01",
    "publication_year": 2013,
    "authors": "Juan M. Silva; Mauricio Orozco; Jongeun Cha; Abdulmotaleb El Saddik; Emil M. Petriu",
    "corresponding_authors": "",
    "abstract": "The purpose of this research is to assess the sensitivity of humans to perceive asynchrony among media signals coming from a computer application. Particularly we examine haptic-to-video and haptic-to-audio skew. For this purpose we have designed an experimental setup, where users are exposed to a basic multimedia presentation resembling a ping-pong game. For every collision between a ball and a racket, the user is able to perceive auditory, visual, and haptic cues about the collision event. We artificially introduce negative and positive delay to the auditory and visual cues with respect to the haptic stream. We subjectively evaluate the perception of inter-stream asynchrony perceived by the users using two types of haptic devices. The statistical results of our evaluation show perception rates of around 100 ms regardless of modality and type of device.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W1989222389",
    "type": "article"
  },
  {
    "title": "Social influence analysis and application on multimedia sharing websites",
    "doi": "https://doi.org/10.1145/2502436",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Jitao Sang; Changsheng Xu",
    "corresponding_authors": "",
    "abstract": "Social media is becoming popular these days, where users necessarily interact with each other to form social networks. Influence network, as one special case of social network, has been recognized as significantly impacting social activities and user decisions. We emphasize in this article that the inter-user influence is essentially topic-sensitive, as for different tasks users tend to trust different influencers and be influenced most by them. While existing research focuses on global influence modeling and applies to text-based networks, this work investigates the problem of topic-sensitive influence modeling in the multimedia domain. According to temporal data justification, we propose a multimodal probabilistic model, considering both users' textual annotation and uploaded visual images. This model is capable of simultaneously extracting user topic distributions and topic-sensitive influence strengths. By identifying the topic-sensitive influencer, we are able to conduct applications, like collective search and collaborative recommendation. A risk minimization-based general framework for personalized image search is further presented, where the image search task is transferred to measure the distance of image and personalized query language models. The framework considers the noisy tag issue and enables easy incorporation of social influence. We have conducted experiments on a large-scale Flickr dataset. Qualitative as well as quantitative evaluation results have validated the effectiveness of the topic-sensitive influencer mining model, and demonstrated the advantage of incorporating topic-sensitive influence in personalized image search and topic-based image recommendation.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2053562101",
    "type": "article"
  },
  {
    "title": "Secure randomized image watermarking based on singular value decomposition",
    "doi": "https://doi.org/10.1145/2542205.2542207",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Gaurav Bhatnagar; Q. M. Jonathan Wu; Pradeep K. Atrey",
    "corresponding_authors": "",
    "abstract": "In this article, a novel logo watermarking scheme is proposed based on wavelet frame transform, singular value decomposition and automatic thresholding. The proposed scheme essentially rectifies the ambiguity problem in the SVD-based watermarking. The core idea is to randomly upscale the size of host image using reversible random extension transform followed by the embedding of logo watermark in the wavelet frame domain. After embedding, a verification phase is casted with the help of a binary watermark and toral automorphism. At the extraction end, the binary watermark is first extracted followed by the verification of watermarked image. The logo watermark is extracted if and only if the watermarked image is verified. The security, attack and comparative analysis confirm high security, efficiency and robustness of the proposed watermarking system.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2054501915",
    "type": "article"
  },
  {
    "title": "Propagation-based social-aware multimedia content distribution",
    "doi": "https://doi.org/10.1145/2523001.2523005",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Zhi Wang; Wenwu Zhu; Xiangwen Chen; Lifeng Sun; Jiangchuan Liu; Minghua Chen; Peng Cui; Shiqiang Yang",
    "corresponding_authors": "",
    "abstract": "Online social networks have reshaped how multimedia contents are generated, distributed, and consumed on today's Internet. Given the massive number of user-generated contents shared in online social networks, users are moving to directly access these contents in their preferred social network services. It is intriguing to study the service provision of social contents for global users with satisfactory quality of experience. In this article, we conduct large-scale measurement of a real-world online social network system to study the social content propagation. We have observed important propagation patterns, including social locality, geographical locality, and temporal locality. Motivated by the measurement insights, we propose a propagation-based social-aware delivery framework using a hybrid edge-cloud and peer-assisted architecture. We also design replication strategies for the architecture based on three propagation predictors designed by jointly considering user, content, and context information. In particular, we design a propagation region predictor and a global audience predictor to guide how the edge-cloud servers backup the contents, and a local audience predictor to guide how peers cache the contents for their friends. Our trace-driven experiments further demonstrate the effectiveness and superiority of our design.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2069118956",
    "type": "article"
  },
  {
    "title": "Measurements and Analysis of a Major Adult Video Portal",
    "doi": "https://doi.org/10.1145/2854003",
    "publication_date": "2016-01-28",
    "publication_year": 2016,
    "authors": "Gareth Tyson; Yehia Elkhatib; Nishanth Sastry; Steve Uhlig",
    "corresponding_authors": "",
    "abstract": "Today, the Internet is a large multimedia delivery infrastructure, with websites such as YouTube appearing at the top of most measurement studies. However, most traffic studies have ignored an important domain: adult multimedia distribution. Whereas, traditionally, such services were provided primarily via bespoke websites, recently these have converged towards what is known as “Porn 2.0”. These services allow users to upload, view, rate, and comment on videos for free (much like YouTube). Despite their scale, we still lack even a basic understanding of their operation. This article addresses this gap by performing a large-scale study of one of the most popular Porn 2.0 websites: YouPorn. Our measurements reveal a global delivery infrastructure that we have repeatedly crawled to collect statistics (on 183k videos). We use this data to characterise the corpus, as well as to inspect popularity trends and how they relate to other features, for example, categories and ratings. To explore our discoveries further, we use a small-scale user study, highlighting key system implications.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2296542253",
    "type": "article"
  },
  {
    "title": "Semantic Reasoning in Zero Example Video Event Retrieval",
    "doi": "https://doi.org/10.1145/3131288",
    "publication_date": "2017-10-04",
    "publication_year": 2017,
    "authors": "Maaike de Boer; Yijie Lu; Hao Zhang; Klamer Schutte; Chong‐Wah Ngo; Wessel Kraaij",
    "corresponding_authors": "",
    "abstract": "Searching in digital video data for high-level events, such as a parade or a car accident, is challenging when the query is textual and lacks visual example images or videos. Current research in deep neural networks is highly beneficial for the retrieval of high-level events using visual examples, but without examples it is still hard to (1) determine which concepts are useful to pre-train ( Vocabulary challenge ) and (2) which pre-trained concept detectors are relevant for a certain unseen high-level event ( Concept Selection challenge ). In our article, we present our Semantic Event Retrieval System which (1) shows the importance of high-level concepts in a vocabulary for the retrieval of complex and generic high-level events and (2) uses a novel concept selection method ( i-w2v ) based on semantic embeddings. Our experiments on the international TRECVID Multimedia Event Detection benchmark show that a diverse vocabulary including high-level concepts improves performance on the retrieval of high-level events in videos and that our novel method outperforms a knowledge-based concept selection method.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2761419673",
    "type": "article"
  },
  {
    "title": "Joint Estimation of Age and Expression by Combining Scattering and Convolutional Networks",
    "doi": "https://doi.org/10.1145/3152118",
    "publication_date": "2018-01-04",
    "publication_year": 2018,
    "authors": "Huei‐Fang Yang; Bo-Yao Lin; Kuang-Yu Chang; Chu‐Song Chen",
    "corresponding_authors": "",
    "abstract": "This article tackles the problem of joint estimation of human age and facial expression. This is an important yet challenging problem because expressions can alter face appearances in a similar manner to human aging. Different from previous approaches that deal with the two tasks independently, our approach trains a convolutional neural network (CNN) model that unifies ordinal regression and multi-class classification in a single framework. We demonstrate experimentally that our method performs more favorably against state-of-the-art approaches.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2781942174",
    "type": "article"
  },
  {
    "title": "Interpretable Partitioned Embedding for Intelligent Multi-item Fashion Outfit Composition",
    "doi": "https://doi.org/10.1145/3326332",
    "publication_date": "2019-04-30",
    "publication_year": 2019,
    "authors": "Zunlei Feng; Zhenyun Yu; Yongcheng Jing; Sai Wu; Mingli Song; Yezhou Yang; Junxiao Jiang",
    "corresponding_authors": "",
    "abstract": "Intelligent fashion outfit composition has become more popular in recent years. Some deep-learning-based approaches reveal competitive composition. However, the uninterpretable characteristic makes such a deep-learning-based approach fail to meet the businesses’, designers’, and consumers’ urges to comprehend the importance of different attributes in an outfit composition. To realize interpretable and intelligent multi-item fashion outfit compositions, we propose a partitioned embedding network to learn interpretable embeddings from clothing items. The network contains two vital components: attribute partition module and partition adversarial module. In the attribute partition module, multiple attribute labels are adopted to ensure that different parts of the overall embedding correspond to different attributes. In the partition adversarial module, adversarial operations are adopted to achieve the independence of different parts. With the interpretable and partitioned embedding, we then construct an outfit-composition graph and an attribute matching map. Extensive experiments demonstrate that (1) the partitioned embedding have unmingled parts that correspond to different attributes and (2) outfits recommended by our model are more desirable in comparison with the existing methods.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2965424936",
    "type": "article"
  },
  {
    "title": "AB-LSTM",
    "doi": "https://doi.org/10.1145/3356728",
    "publication_date": "2019-11-30",
    "publication_year": 2019,
    "authors": "Zhandong Liu; Wengang Zhou; Houqiang Li",
    "corresponding_authors": "",
    "abstract": "Detection of scene text in arbitrary shapes is a challenging task in the field of computer vision. Most existing scene text detection methods exploit the rectangle/quadrangular bounding box to denote the detected text, which fails to accurately fit text with arbitrary shapes, such as curved text. In addition, recent progress on scene text detection has benefited from Fully Convolutional Network. Text cues contained in multi-level convolutional features are complementary for detecting scene text objects. How to explore these multi-level features is still an open problem. To tackle the above issues, we propose an Attention-based Bidirectional Long Short-Term Memory (AB-LSTM) model for scene text detection. First, word stroke regions (WSRs) and text center blocks (TCBs) are extracted by two AB-LSTM models, respectively. Then, the union of WSRs and TCBs are used to represent text objects. To verify the effectiveness of the proposed method, we perform experiments on four public benchmarks: CTW1500, Total-text, ICDAR2013, and MSRA-TD500, and compare it with existing state-of-the-art methods. Experiment results demonstrate that the proposed method can achieve competitive results, and well handle scene text objects with arbitrary shapes (i.e., curved, oriented, and horizontal forms).",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2996609910",
    "type": "article"
  },
  {
    "title": "Understanding Video Sharing Propagation in Social Networks",
    "doi": "https://doi.org/10.1145/2594440",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "Haitao Li; Xu Cheng; Jiangchuan Liu",
    "corresponding_authors": "",
    "abstract": "Modern online social networking has drastically changed the information distribution landscape. Recently, video has become one of the most important types of objects spreading among social networking service users. The sheer and ever-increasing data volume, the broader coverage, and the longer access durations of video objects, however, present significantly more challenges than other types of objects. This article takes an initial step toward understanding the unique characteristics of video sharing propagation in social networks. Based on realworld data traces from a large-scale online social network, we examine the user behavior from diverse aspects and identify different types of users involved in video propagation. We closely investigate the temporal distribution during propagation as well as the typical propagation structures, revealing more details beyond stationary coverage. We further extend the conventional epidemic models to accommodate diverse types of users and their probabilistic viewing and sharing behaviors. The model, effectively capturing the essentials of the propagation process, serves as a valuable basis for such applications as workload synthesis, traffic prediction, and resource provision of video servers.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2152980153",
    "type": "article"
  },
  {
    "title": "Semantic Feature Mining for Video Event Understanding",
    "doi": "https://doi.org/10.1145/2962719",
    "publication_date": "2016-08-03",
    "publication_year": 2016,
    "authors": "Xiaoshan Yang; Tianzhu Zhang; Changsheng Xu",
    "corresponding_authors": "",
    "abstract": "Content-based video understanding is extremely difficult due to the semantic gap between low-level vision signals and the various semantic concepts (object, action, and scene) in videos. Though feature extraction from videos has achieved significant progress, most of the previous methods rely only on low-level features, such as the appearance and motion features. Recently, visual-feature extraction has been improved significantly with machine-learning algorithms, especially deep learning. However, there is still not enough work focusing on extracting semantic features from videos directly. The goal of this article is to adopt unlabeled videos with the help of text descriptions to learn an embedding function, which can be used to extract more effective semantic features from videos when only a few labeled samples are available for video recognition. To achieve this goal, we propose a novel embedding convolutional neural network (ECNN). We evaluate our algorithm by comparing its performance on three challenging benchmarks with several popular state-of-the-art methods. Extensive experimental results show that the proposed ECNN consistently and significantly outperforms the existing methods.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2499974702",
    "type": "article"
  },
  {
    "title": "Cross-Modality Feature Learning via Convolutional Autoencoder",
    "doi": "https://doi.org/10.1145/3231740",
    "publication_date": "2019-01-24",
    "publication_year": 2019,
    "authors": "Xueliang Liu; Meng Wang; Zheng-Jun Zha; Richang Hong",
    "corresponding_authors": "",
    "abstract": "Learning robust and representative features across multiple modalities has been a fundamental problem in machine learning and multimedia fields. In this article, we propose a novel MUltimodal Convolutional AutoEncoder (MUCAE) approach to learn representative features from visual and textual modalities. For each modality, we integrate the convolutional operation into an autoencoder framework to learn a joint representation from the original image and text content. We optimize the convolutional autoencoders of different modalities jointly by exploiting the correlation between the hidden representations from the convolutional autoencoders, in particular by minimizing both the reconstructing error of each modality and the correlation divergence between the hidden feature of different modalities. Compared to the conventional solutions relying on hand-crafted features, the proposed MUCAE approach encodes features from image pixels and text characters directly and produces more representative and robust features. We evaluate MUCAE on cross-media retrieval as well as unimodal classification tasks over real-world large-scale multimedia databases. Experimental results have shown that MUCAE performs better than the state-of-the-arts methods.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2913790388",
    "type": "article"
  },
  {
    "title": "Increasing Image Memorability with Neural Style Transfer",
    "doi": "https://doi.org/10.1145/3311781",
    "publication_date": "2019-05-31",
    "publication_year": 2019,
    "authors": "Aliaksandr Siarohin; Gloria Zen; Cveta Majtanovic; Xavier Alameda-Pineda; Elisa Ricci; Nicu Sebe",
    "corresponding_authors": "",
    "abstract": "Recent works in computer vision and multimedia have shown that image memorability can be automatically inferred exploiting powerful deep-learning models. This article advances the state of the art in this area by addressing a novel and more challenging issue: “ Given an arbitrary input image, can we make it more memorable? ” To tackle this problem, we introduce an approach based on an editing-by-applying-filters paradigm: given an input image, we propose to automatically retrieve a set of “style seeds,” i.e., a set of style images that, applied to the input image through a neural style transfer algorithm, provide the highest increase in memorability. We show the effectiveness of the proposed approach with experiments on the publicly available LaMem dataset, performing both a quantitative evaluation and a user study. To demonstrate the flexibility of the proposed framework, we also analyze the impact of different implementation choices, such as using different state-of-the-art neural style transfer methods. Finally, we show several qualitative results to provide additional insights on the link between image style and memorability.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2951848755",
    "type": "article"
  },
  {
    "title": "Art by Computing Machinery",
    "doi": "https://doi.org/10.1145/3326338",
    "publication_date": "2019-04-30",
    "publication_year": 2019,
    "authors": "Eugene Ch’ng",
    "corresponding_authors": "Eugene Ch’ng",
    "abstract": "When does a machine-created work becomes art? What is art? Can machine artworks fit in to the historical and present discourse? Do machine artworks demonstrate creativity, or are they a type of new media from which artists extend their creativity with? Will solely machine-created artworks be acceptable by our artworlds? This article probes these questions by first identifying the frameworks for defining and explaining art and evaluating its suitability for explaining machine artworks. It then explores how artworks have a necessary relationship with their human artists and the wider context of history, institutions, styles, and approaches and with audiences and artworlds. The article then questions whether machines have such a relational context and whether machines will ever live up to our standard of what constitutes an artwork as defined by us or whether machines are good only for assisting creativity. The question of intellectual property, rights, and ownership are also discussed for human--machine artworks and purely machine-produced works of art. The article critically assesses the viability of machines as artists as the central question in the historical discourse, extended through art and the artworld and evaluates machine-produced work from such a basis.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2969020539",
    "type": "article"
  },
  {
    "title": "Features-Enhanced Multi-Attribute Estimation with Convolutional Tensor Correlation Fusion Network",
    "doi": "https://doi.org/10.1145/3355542",
    "publication_date": "2019-10-15",
    "publication_year": 2019,
    "authors": "Mingxing Duan; Kenli Li; Xiangke Liao; Keqin Li; Qi Tian",
    "corresponding_authors": "",
    "abstract": "To achieve robust facial attribute estimation, a hierarchical prediction system referred to as tensor correlation fusion network (TCFN) is proposed for attribute estimation. The system includes feature extraction, correlation excavation among facial attribute features, score fusion, and multi-attribute prediction. Subnetworks (Age-Net, Gender-Net, Race-Net, and Smile-Net) are used to extract corresponding features while Main-Net extracts features not only from an input image but also from corresponding pooling layers of subnetworks. Dynamic tensor canonical correlation analysis (DTCCA) is proposed to explore the correlation of different targets’ features in the F7 layers. Then, for binary classifications of gender, race, and smile, corresponding robust decisions are achieved by fusing the results of subnetworks with those of TCFN while for age prediction, facial image into one of age groups, and then ELM regressor performs the final age estimation. Experimental results on benchmarks with multiple face attributes (MORPH-II, Adience Benchmark datasets, LAP-2016, and CelebA) show that the proposed approach has superior performance compared to state of the art.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2981018152",
    "type": "article"
  },
  {
    "title": "Dissecting the Performance of VR Video Streaming through the VR-EXP Experimentation Platform",
    "doi": "https://doi.org/10.1145/3360286",
    "publication_date": "2019-11-30",
    "publication_year": 2019,
    "authors": "Roberto Irajá Tavares da Costa Filho; Marcelo Caggiani Luizelli; Stefano Petrangeli; Maria Torres Vega; Jeroen van der Hooft; Tim Wauters; Filip De Turck; Luciano Paschoal Gaspary",
    "corresponding_authors": "",
    "abstract": "To cope with the massive bandwidth demands of Virtual Reality (VR) video streaming, both the scientific community and the industry have been proposing optimization techniques such as viewport-aware streaming and tile-based adaptive bitrate heuristics. As most of the VR video traffic is expected to be delivered through mobile networks, a major problem arises: both the network performance and VR video optimization techniques have the potential to influence the video playout performance and the Quality of Experience (QoE). However, the interplay between them is neither trivial nor has it been properly investigated. To bridge this gap, in this article, we introduce VR-EXP, an open-source platform for carrying out VR video streaming performance evaluation. Furthermore, we consolidate a set of relevant VR video streaming techniques and evaluate them under variable network conditions, contributing to an in-depth understanding of what to expect when different combinations are employed. To the best of our knowledge, this is the first work to propose a systematic approach, accompanied by a software toolkit, which allows one to compare different optimization techniques under the same circumstances. Extensive evaluations carried out using realistic datasets demonstrate that VR-EXP is instrumental in providing valuable insights regarding the interplay between network performance and VR video streaming optimization techniques.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2995166892",
    "type": "article"
  },
  {
    "title": "U-Net Conditional GANs for Photo-Realistic and Identity-Preserving Facial Expression Synthesis",
    "doi": "https://doi.org/10.1145/3355397",
    "publication_date": "2019-10-15",
    "publication_year": 2019,
    "authors": "Xueping Wang; Yunhong Wang; Weixin Li",
    "corresponding_authors": "",
    "abstract": "Facial expression synthesis (FES) is a challenging task since the expression changes are highly non-linear and depend on the facial appearance. Person identity should also be well preserved in the synthesized face. In this article, we present a novel U-Net Conditional Generative Adversarial Network for FES. U-Net helps retain the property of the input face, including the identity information and facial details. Category condition is added to the U-Net model so that one-to-many expression synthesis can be achieved simultaneously. We also design constraints for identity preservation during FES to further guarantee that the identity of the input face can be well preserved in the generated face image. Specifically, we pair the generated output with condition image of other identities for the discriminator, so as to encourage it to learn the distinctions between the synthesized and natural images, as well as between input and other identities, which can help improve its discriminating ability. Additionally, we utilize the triplet loss to maintain the generated face images closer to the same identity person by imposing a margin between the positive pairs and negative pairs in feature space. Both qualitative and quantitative evaluations are conducted on the Oulu-CASIA NIR8VIS facial expression database, the Radboud Faces Database, and the Karolinska Directed Emotional Faces database, and the experimental results show that our method can generate faces with natural and realistic expressions while preserving identity information.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W3008970820",
    "type": "article"
  },
  {
    "title": "An Adaptive Two-Layer Light Field Compression Scheme Using GNN-Based Reconstruction",
    "doi": "https://doi.org/10.1145/3395620",
    "publication_date": "2020-04-30",
    "publication_year": 2020,
    "authors": "Xinjue Hu; Jingming Shan; Yu Liu; Lin Zhang; Shervin Shirmohammadi",
    "corresponding_authors": "",
    "abstract": "As a new form of volumetric media, Light Field (LF) can provide users with a true six degrees of freedom immersive experience because LF captures the scene with photo-realism, including aperture-limited changes in viewpoint. But uncompressed LF data is too large for network transmission, which is the reason why LF compression has become an important research topic. One of the more recent approaches for LF compression is to reduce the angular resolution of the input LF during compression and to use LF reconstruction to recover the discarded viewpoints during decompression. Following this approach, we propose a new LF reconstruction algorithm based on Graph Neural Networks; we show that it can achieve higher compression and better quality compared to existing reconstruction methods, although suffering from the same problem as those methods—the inability to deal effectively with high-frequency image components. To solve this problem, we propose an adaptive two-layer compression architecture that separates high-frequency and low-frequency components and compresses each with a different strategy so that the performance can become robust and controllable. Experiments with multiple datasets 1 show that our proposed scheme is capable of providing a decompression quality of above 40 dB, and can significantly improve compression efficiency compared with similar LF reconstruction schemes.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W3077527200",
    "type": "article"
  },
  {
    "title": "Correlation Discrepancy Insight Network for Video Re-identification",
    "doi": "https://doi.org/10.1145/3402666",
    "publication_date": "2020-11-30",
    "publication_year": 2020,
    "authors": "Weijian Ruan; Chao Liang; Yi Yu; Zheng Wang; Wu Liu; Jun Chen; Jiayi Ma",
    "corresponding_authors": "",
    "abstract": "Video-based person re-identification (ReID) aims at re-identifying a specified person sequence from videos that were captured by disjoint cameras. Most existing works on this task ignore the quality discrepancy across frames by using all video frames to develop a ReID method. Additionally, they adopt only the person self-characteristic as the representation, which cannot adapt to cross-camera variation effectively. To that end, we propose a novel correlation discrepancy insight network for video-based person ReID, which consists of an unsupervised correlation insight model (CIM) for video purification and a discrepancy description network (DDN) for person representation. Concretely, CIM is constructed by using kernelized correlation filters to encode person half-parts, which evaluates the frame quality by the cross correlation across frames for selecting discriminative video fragments. Furthermore, DDN exploits the selected video fragments to generate a discrepancy descriptor using a compression network, which aims at employing the discrepancies with other persons’ to facilitate the representation of the target person rather than only using the self-characteristic. Due to the advantage in handling cross-domain variation, the discrepancy descriptor is expected to provide a new pattern for the object representation in cross-camera tasks. Experimental results on three public benchmarks demonstrate that the proposed method outperforms several state-of-the-art methods.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W3113160875",
    "type": "article"
  },
  {
    "title": "Covert Voice over Internet Protocol Communications with Packet Loss Based on Fractal Interpolation",
    "doi": "https://doi.org/10.1145/2961053",
    "publication_date": "2016-08-24",
    "publication_year": 2016,
    "authors": "Yijing Jiang; Shanyu Tang; Liping Zhang; Muzhou Xiong; Yau Jim Yip",
    "corresponding_authors": "",
    "abstract": "The last few years have witnessed an explosive growth in the research of information hiding in multimedia objects, but few studies have taken into account packet loss in multimedia networks. As one of the most popular real-time services in the Internet, Voice over Internet Protocol (VoIP) contributes to a large part of network traffic for its advantages of real time, high flow, and low cost. So packet loss is inevitable in multimedia networks and affects the performance of VoIP communications. In this study, a fractal-based VoIP steganographic approach was proposed to realize covert VoIP communications in the presence of packet loss. In the proposed scheme, secret data to be hidden were divided into blocks after being encrypted with the block cipher, and each block of the secret data was then embedded into VoIP streaming packets. The VoIP packets went through a packet-loss system based on Gilbert model which simulates a real network situation. And a prediction model based on fractal interpolation was built to decide whether a VoIP packet was suitable for data hiding. The experimental results indicated that the speech quality degradation increased with the escalating packet-loss level. The average variance of speech quality metrics (PESQ score) between the “no-embedding” speech samples and the “with-embedding” stego-speech samples was about 0.717, and the variances narrowed with the increasing packet-loss level. Both the average PESQ scores and the SNR values of stego-speech samples and the data-retrieving rates had almost the same varying trends when the packet-loss level increased, indicating that the success rate of the fractal prediction model played an important role in the performance of covert VoIP communications.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2513409064",
    "type": "article"
  },
  {
    "title": "DQ-DASH",
    "doi": "https://doi.org/10.1145/3371040",
    "publication_date": "2020-02-29",
    "publication_year": 2020,
    "authors": "Abdelhak Bentaleb; Praveen Kumar Yadav; Wei Tsang Ooi; Roger Zimmermann",
    "corresponding_authors": "",
    "abstract": "The significant popularity of HTTP adaptive video streaming (HAS), such as Dynamic Adaptive Streaming over HTTP (DASH), over the Internet has led to a stark increase in user expectations in terms of video quality and delivery robustness. This situation creates new challenges for content providers who must satisfy the Quality-of-Experience (QoE) requirements and demands of their customers over a best-effort network infrastructure. Unlike traditional single server DASH, we developed a D istributed Q ueuing theory bitrate adaptation algorithm for DASH (DQ-DASH) that leverages the availability of multiple servers by downloading segments in parallel. DQ-DASH uses a M x /D/1/K queuing theory based bitrate selection in conjunction with the request scheduler to download subsequent segments of the same quality through parallel requests to reduce quality fluctuations. DQ-DASH facilitates the aggregation of bandwidth from different servers and increases fault-tolerance and robustness through path diversity. The resulting resilience prevents clients from suffering QoE degradations when some of the servers become congested. DQ-DASH also helps to fully utilize the aggregate bandwidth from the servers and download the imminently required segment from the server with the highest throughput. We have also analyzed the effect of buffer capacity and segment duration for multi-source video streaming.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W3009248904",
    "type": "article"
  },
  {
    "title": "LFGAN",
    "doi": "https://doi.org/10.1145/3366371",
    "publication_date": "2020-02-17",
    "publication_year": 2020,
    "authors": "Bin Chen; Lingyan Ruan; Miu Ling Lam",
    "corresponding_authors": "",
    "abstract": "We present a deep neural network called the light field generative adversarial network (LFGAN) that synthesizes a 4D light field from a single 2D RGB image. We generate light fields using a single image super-resolution (SISR) technique based on two important observations. First, the small baseline gives rise to the high similarity between the full light field image and each sub-aperture view. Second, the occlusion edge at any spatial coordinate of a sub-aperture view has the same orientation as the occlusion edge at the corresponding angular patch, implying that the occlusion information in the angular domain can be inferred from the sub-aperture local information. We employ the Wasserstein GAN with gradient penalty (WGAN-GP) to learn the color and geometry information from the light field datasets. The network can generate a plausible 4D light field comprising 8×8 angular views from a single sub-aperture 2D image. We propose new loss terms, namely epipolar plane image (EPI) and brightness regularization (BRI) losses, as well as a novel multi-stage training framework to feed the loss terms at different time to generate superior light fields. The EPI loss can reinforce the network to learn the geometric features of the light fields, and the BRI loss can preserve the brightness consistency across different sub-aperture views. Two datasets have been used to evaluate our method: in addition to an existing light field dataset capturing scenes of flowers and plants, we have built a large dataset of toy animals consisting of 2,100 light fields captured with a plenoptic camera. We have performed comprehensive ablation studies to evaluate the effects of individual loss terms and the multi-stage training strategy, and have compared LFGAN to other state-of-the-art techniques. Qualitative and quantitative evaluation demonstrates that LFGAN can effectively estimate complex occlusions and geometry in challenging scenes, and outperform other existing techniques.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W3010610204",
    "type": "article"
  },
  {
    "title": "A Decision Support System with Intelligent Recommendation for Multi-disciplinary Medical Treatment",
    "doi": "https://doi.org/10.1145/3352573",
    "publication_date": "2020-01-31",
    "publication_year": 2020,
    "authors": "Nengjun Zhu; Jian Cao; Kunwei Shen; Xiaosong Chen; Siji Zhu",
    "corresponding_authors": "",
    "abstract": "Recent years have witnessed an emerging trend for improving disease treatment by forming multi-disciplinary medical teams. The collaboration among specialists from multiple medical domains has been shown to be significantly helpful for designing comprehensive and reliable regimens, especially for incurable diseases. Although this kind of multi-disciplinary treatment has been increasingly adopted by healthcare providers, a new challenge has been introduced to the decision-making process—how to efficiently and effectively develop final regimens by searching for candidate treatments and considering inputs from every expert. In this article, we present a sophisticated decision support system called MdtDSS (a decision support system (DSS) for multi-disciplinary treatment (Mdt)), which is particularly developed to guide the collaborative decision-making in multi-disciplinary treatment scenarios. The system integrates a recommender system that aims to search for personalized candidates from a large-scale high-quality regimen pool and a voting system that helps collect feedback from multiple specialists without potential bias. Our decision support system optimally combines machine intelligence and human experience and helps medical practitioners make informed and accountable regimen decisions. We deployed the proposed system in a large hospital in Shanghai, China, and collected real-world data on large-scale patient cases. The evaluation shows that the proposed system achieves outstanding results in terms of high-quality multi-disciplinary treatment.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W3019129093",
    "type": "article"
  },
  {
    "title": "Smart Scribbles for Image Matting",
    "doi": "https://doi.org/10.1145/3408323",
    "publication_date": "2020-11-30",
    "publication_year": 2020,
    "authors": "Xin Yang; Yu Qiao; Shaozhe Chen; Shengfeng He; Baocai Yin; Qiang Zhang; Xiaopeng Wei; Rynson W. H. Lau",
    "corresponding_authors": "",
    "abstract": "Image matting is an ill-posed problem that usually requires additional user input, such as trimaps or scribbles. Drawing a fne trimap requires a large amount of user effort, while using scribbles can hardly obtain satisfactory alpha mattes for non-professional users. Some recent deep learning-based matting networks rely on large-scale composite datasets for training to improve performance, resulting in the occasional appearance of obvious artifacts when processing natural images. In this article, we explore the intrinsic relationship between user input and alpha mattes and strike a balance between user effort and the quality of alpha mattes. In particular, we propose an interactive framework, referred to as smart scribbles, to guide users to draw few scribbles on the input images to produce high-quality alpha mattes. It frst infers the most informative regions of an image for drawing scribbles to indicate different categories (foreground, background, or unknown) and then spreads these scribbles (i.e., the category labels) to the rest of the image via our well-designed two-phase propagation. Both neighboring low-level afnities and high-level semantic features are considered during the propagation process. Our method can be optimized without large-scale matting datasets and exhibits more universality in real situations. Extensive experiments demonstrate that smart scribbles can produce more accurate alpha mattes with reduced additional input, compared to the state-of-the-art matting methods.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W3111868107",
    "type": "article"
  },
  {
    "title": "Adaptive Attention-based High-level Semantic Introduction for Image Caption",
    "doi": "https://doi.org/10.1145/3409388",
    "publication_date": "2020-11-30",
    "publication_year": 2020,
    "authors": "Xiaoxiao Liu; Qingyang Xu",
    "corresponding_authors": "",
    "abstract": "There have been several attempts to integrate a spatial visual attention mechanism into an image caption model and introduce semantic concepts as the guidance of image caption generation. High-level semantic information consists of the abstractedness and generality indication of an image, which is beneficial to improve the model performance. However, the high-level information is always static representation without considering the salient elements. Therefore, a semantic attention mechanism is used for the high-level information instead of conventional of static representation in this article. The salient high-level semantic information can be considered as redundant semantic information for image caption generation. Additionally, the generation of visual words and non-visual words can be separated, and an adaptive attention mechanism is employed to realize the guidance information of image caption generation switching between new fusion information (fusion of image feature and high-level semantics) and a language model. Therefore, visual words can be generated according to the image features and high-level semantic information, and non-visual words can be predicted by the language model. The semantics attention, adaptive attention, and previous generated words are fused to construct a special attention module for the input and output of long short-term memory. An image caption can be generated as a concise sentence on the basis of accurately grasping the rich content of the image. The experimental results show that the performance of the proposed model is promising for the evaluation metrics, and the captions can achieve logical and rich descriptions.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W3111947517",
    "type": "article"
  },
  {
    "title": "Perceptual Image Compression with Block-Level Just Noticeable Difference Prediction",
    "doi": "https://doi.org/10.1145/3408320",
    "publication_date": "2020-11-30",
    "publication_year": 2020,
    "authors": "Tao Tian; Hanli Wang; Sam Kwong; C.‐C. Jay Kuo",
    "corresponding_authors": "",
    "abstract": "A block-level perceptual image compression framework is proposed in this work, including a block-level just noticeable difference (JND) prediction model and a preprocessing scheme. Specifically speaking, block-level JND values are first deduced by utilizing the OTSU method based on the variation of block-level structural similarity values between two adjacent picture-level JND values in the MCL-JCI dataset. After the JND value for each image block is generated, a convolutional neural network–based prediction model is designed to forecast block-level JND values for a given target image. Then, a preprocessing scheme is devised to modify the discrete cosine transform coefficients during JPEG compression on the basis of the distribution of block-level JND values of the target test image. Finally, the test image is compressed by the max JND value across all of its image blocks in the light of the initial quality factor setting. The experimental results demonstrate that the proposed block-level perceptual image compression method is able to achieve 16.75% bit saving as compared to the state-of-the-art method with similar subjective quality. The project page can be found at https://mic.tongji.edu.cn/43/3f/c9778a148287/page.htm.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W3127585865",
    "type": "article"
  },
  {
    "title": "Lightweight Single Image Super-resolution with Dense Connection Distillation Network",
    "doi": "https://doi.org/10.1145/3414838",
    "publication_date": "2021-01-31",
    "publication_year": 2021,
    "authors": "Yanchun Li; Jiang-lian Cao; Zhetao Li; Sangyoon Oh; Nobuyoshi Komuro",
    "corresponding_authors": "",
    "abstract": "Single image super-resolution attempts to reconstruct a high-resolution (HR) image from its corresponding low-resolution (LR) image, which has been a research hotspot in computer vision and image processing for decades. To improve the accuracy of super-resolution images, many works adopt very deep networks to model the translation from LR to HR, resulting in memory and computation consumption. In this article, we design a lightweight dense connection distillation network by combining the feature fusion units and dense connection distillation blocks (DCDB) that include selective cascading and dense distillation components. The dense connections are used between and within the distillation block, which can provide rich information for image reconstruction by fusing shallow and deep features. In each DCDB, the dense distillation module concatenates the remaining feature maps of all previous layers to extract useful information, the selected features are then assessed by the proposed layer contrast-aware channel attention mechanism, and finally the cascade module aggregates the features. The distillation mechanism helps to reduce training parameters and improve training efficiency, and the layer contrast-aware channel attention further improves the performance of model. The quality and quantity experimental results on several benchmark datasets show the proposed method performs better tradeoff in term of accuracy and efficiency.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W3145048986",
    "type": "article"
  },
  {
    "title": "Smart Director: An Event-Driven Directing System for Live Broadcasting",
    "doi": "https://doi.org/10.1145/3448981",
    "publication_date": "2021-11-12",
    "publication_year": 2021,
    "authors": "Yingwei Pan; Yue Chen; Qian Bao; Ning Zhang; Ting Yao; Jingen Liu; Tao Mei",
    "corresponding_authors": "",
    "abstract": "Live video broadcasting normally requires a multitude of skills and expertise with domain knowledge to enable multi-camera productions. As the number of cameras keeps increasing, directing a live sports broadcast has now become more complicated and challenging than ever before. The broadcast directors need to be much more concentrated, responsive, and knowledgeable, during the production. To relieve the directors from their intensive efforts, we develop an innovative automated sports broadcast directing system, called Smart Director, which aims at mimicking the typical human-in-the-loop broadcasting process to automatically create near-professional broadcasting programs in real-time by using a set of advanced multi-view video analysis algorithms. Inspired by the so-called “three-event” construction of sports broadcast [ 14 ], we build our system with an event-driven pipeline consisting of three consecutive novel components: (1) the Multi-View Event Localization to detect events by modeling multi-view correlations, (2) the Multi-View Highlight Detection to rank camera views by the visual importance for view selection, and (3) the Auto-Broadcasting Scheduler to control the production of broadcasting videos. To our best knowledge, our system is the first end-to-end automated directing system for multi-camera sports broadcasting, completely driven by the semantic understanding of sports events. It is also the first system to solve the novel problem of multi-view joint event detection by cross-view relation modeling. We conduct both objective and subjective evaluations on a real-world multi-camera soccer dataset, which demonstrate the quality of our auto-generated videos is comparable to that of the human-directed videos. Thanks to its faster response, our system is able to capture more fast-passing and short-duration events which are usually missed by human directors.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W3211461682",
    "type": "article"
  },
  {
    "title": "Where Are They Going? Predicting Human Behaviors in Crowded Scenes",
    "doi": "https://doi.org/10.1145/3449359",
    "publication_date": "2021-11-12",
    "publication_year": 2021,
    "authors": "Bo Zhang; Rui Zhang; Niccoló Bisagno; Nicola Conci; Francesco G. B. De Natale; Hongbo Liu",
    "corresponding_authors": "",
    "abstract": "In this article, we propose a framework for crowd behavior prediction in complicated scenarios. The fundamental framework is designed using the standard encoder-decoder scheme, which is built upon the long short-term memory module to capture the temporal evolution of crowd behaviors. To model interactions among humans and environments, we embed both the social and the physical attention mechanisms into the long short-term memory. The social attention component can model the interactions among different pedestrians, whereas the physical attention component helps to understand the spatial configurations of the scene. Since pedestrians’ behaviors demonstrate multi-modal properties, we use the generative model to produce multiple acceptable future paths. The proposed framework not only predicts an individual’s trajectory accurately but also forecasts the ongoing group behaviors by leveraging on the coherent filtering approach. Experiments are carried out on the standard crowd benchmarks (namely, the ETH, the UCY, the CUHK crowd, and the CrowdFlow datasets), which demonstrate that the proposed framework is effective in forecasting crowd behaviors in complex scenarios.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W3212852649",
    "type": "article"
  },
  {
    "title": "GreyReID: A Novel Two-stream Deep Framework with RGB-grey Information for Person Re-identification",
    "doi": "https://doi.org/10.1145/3419439",
    "publication_date": "2021-02-28",
    "publication_year": 2021,
    "authors": "Lei Qi; Lei Wang; Jing Huo; Yinghuan Shi; Yang Gao",
    "corresponding_authors": "",
    "abstract": "In this article, we observe that most false positive images (i.e., different identities with query images) in the top ranking list usually have the similar color information with the query image in person re-identification (Re-ID). Meanwhile, when we use the greyscale images generated from RGB images to conduct the person Re-ID task, some hard query images can obtain better performance compared with using RGB images. Therefore, RGB and greyscale images seem to be complementary to each other for person Re-ID. In this article, we aim to utilize both RGB and greyscale images to improve the person Re-ID performance. To this end, we propose a novel two-stream deep neural network with RGB-grey information, which can effectively fuse RGB and greyscale feature representations to enhance the generalization ability of Re-ID. First, we convert RGB images to greyscale images in each training batch. Based on these RGB and greyscale images, we train the RGB and greyscale branches, respectively. Second, to build up connections between RGB and greyscale branches, we merge the RGB and greyscale branches into a new joint branch. Finally, we concatenate the features of all three branches as the final feature representation for Re-ID. Moreover, in the training process, we adopt the joint learning scheme to simultaneously train each branch by the independent loss function, which can enhance the generalization ability of each branch. Besides, a global loss function is utilized to further fine-tune the final concatenated feature. The extensive experiments on multiple benchmark datasets fully show that the proposed method can outperform the state-of-the-art person Re-ID methods. Furthermore, using greyscale images can indeed improve the person Re-ID performance in the proposed deep framework.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W3153082147",
    "type": "article"
  },
  {
    "title": "Distribution Aligned Multimodal and Multi-domain Image Stylization",
    "doi": "https://doi.org/10.1145/3450525",
    "publication_date": "2021-07-22",
    "publication_year": 2021,
    "authors": "Minxuan Lin; Fan Tang; Weiming Dong; Xiao Li; Changsheng Xu; Chongyang Ma",
    "corresponding_authors": "",
    "abstract": "Multimodal and multi-domain stylization are two important problems in the field of image style transfer. Currently, there are few methods that can perform multimodal and multi-domain stylization simultaneously. In this study, we propose a unified framework for multimodal and multi-domain style transfer with the support of both exemplar-based reference and randomly sampled guidance. The key component of our method is a novel style distribution alignment module that eliminates the explicit distribution gaps between various style domains and reduces the risk of mode collapse. The multimodal diversity is ensured by either guidance from multiple images or random style codes, while the multi-domain controllability is directly achieved by using a domain label. We validate our proposed framework on painting style transfer with various artistic styles and genres. Qualitative and quantitative comparisons with state-of-the-art methods demonstrate that our method can generate high-quality results of multi-domain styles and multimodal instances from reference style guidance or a random sampled style.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W3183258422",
    "type": "article"
  },
  {
    "title": "Full-reference Screen Content Image Quality Assessment by Fusing Multilevel Structure Similarity",
    "doi": "https://doi.org/10.1145/3447393",
    "publication_date": "2021-07-22",
    "publication_year": 2021,
    "authors": "Chenglizhao Chen; Hongmeng Zhao; Huan Yang; Teng Yu; Chong Peng; Hong Qin",
    "corresponding_authors": "",
    "abstract": "Screen content images (SCIs) usually comprise various content types with sharp edges, in which artifacts or distortions can be effectively sensed by a vanilla structure similarity measurement in a full-reference manner. Nonetheless, almost all of the current state-of-the-art (SOTA) structure similarity metrics are “locally” formulated in a single-level manner, while the true human visual system (HVS) follows the multilevel manner; such mismatch could eventually prevent these metrics from achieving reliable quality assessment. To ameliorate this issue, this article advocates a novel solution to measure structure similarity “globally” from the perspective of sparse representation. To perform multilevel quality assessment in accordance with the real HVS, the abovementioned global metric will be integrated with the conventional local ones by resorting to the newly devised selective deep fusion network. To validate its efficacy and effectiveness, we have compared our method with 12 SOTA methods over two widely used large-scale public SCI datasets, and the quantitative results indicate that our method yields significantly higher consistency with subjective quality scores than the current leading works. Both the source code and data are also publicly available to gain widespread acceptance and facilitate new advancement and validation.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W3184906298",
    "type": "article"
  },
  {
    "title": "Explainable AI: A Multispectral Palm-Vein Identification System with New Augmentation Features",
    "doi": "https://doi.org/10.1145/3468873",
    "publication_date": "2021-10-31",
    "publication_year": 2021,
    "authors": "Yung-Yao Chen; Sin-Ye Jhong; Chih‐Hsien Hsia; Kai‐Lung Hua",
    "corresponding_authors": "",
    "abstract": "Recently, as one of the most promising biometric traits, the vein has attracted the attention of both academia and industry because of its living body identification and the convenience of the acquisition process. State-of-the-art techniques can provide relatively good performance, yet they are limited to specific light sources. Besides, it still has poor adaptability to multispectral images. Despite the great success achieved by convolutional neural networks (CNNs) in various image understanding tasks, they often require large training samples and high computation that are infeasible for palm-vein identification. To address this limitation, this work proposes a palm-vein identification system based on lightweight CNN and adaptive multi-spectral method with explainable AI. The principal component analysis on symmetric discrete wavelet transform (SMDWT-PCA) technique for vein images augmentation method is adopted to solve the problem of insufficient data and multispectral adaptability. The depth separable convolution (DSC) has been applied to reduce the number of model parameters in this work. To ensure that the experimental result demonstrates accurately and robustly, a multispectral palm image of the public dataset (CASIA) is also used to assess the performance of the proposed method. As result, the palm-vein identification system can provide superior performance to that of the former related approaches for different spectrums.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W3212194243",
    "type": "article"
  },
  {
    "title": "Variational Autoencoder with CCA for Audio–Visual Cross-modal Retrieval",
    "doi": "https://doi.org/10.1145/3575658",
    "publication_date": "2022-12-08",
    "publication_year": 2022,
    "authors": "Jiwei Zhang; Yi Yu; Suhua Tang; Jianming Wu; Wei Li",
    "corresponding_authors": "",
    "abstract": "Cross-modal retrieval is to utilize one modality as a query to retrieve data from another modality, which has become a popular topic in information retrieval, machine learning, and databases. Finding a method to effectively measure the similarity between different modality data is the major challenge of cross-modal retrieval. Although several research works have calculated the correlation between different modality data via learning a common subspace representation, the encoder’s ability to extract features from multi-modal information is not satisfactory. In this article, we present a novel variational autoencoder architecture for audio–visual cross-modal retrieval by learning paired audio–visual correlation embedding and category correlation embedding as constraints to reinforce the mutuality of audio–visual information. On the one hand, audio encoder and visual encoder separately encode audio data and visual data into two different latent spaces. Further, two mutual latent spaces are respectively constructed by canonical correlation analysis. On the other hand, probabilistic modeling methods are used to deal with possible noise and missing information in the data. Additionally, in this way, the cross-modal discrepancies from intra-modal and inter-modal information are simultaneously eliminated in the joint embedding subspace. We conduct extensive experiments over two benchmark datasets. The experimental results confirm that the proposed architecture is effective in learning audio–visual correlation and is appreciably better than the existing cross-modal retrieval methods.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4200631412",
    "type": "article"
  },
  {
    "title": "Improving Feature Discrimination for Object Tracking by Structural-similarity-based Metric Learning",
    "doi": "https://doi.org/10.1145/3497746",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Jingjing Wu; Jianguo Jiang; Meibin Qi; Cuiqun Chen; Yimin Liu",
    "corresponding_authors": "",
    "abstract": "Existing approaches usually form the tracking task as an appearance matching procedure. However, the discrimination ability of appearance features is insufficient in these trackers, which is caused by their weak feature supervision constraints and inadequate exploitation of spatial contexts. To tackle this issue, this article proposes a novel appearance matching tracking (AMT) method to strengthen the feature restraints and capture discriminative spatial representations. Specifically, we first utilize a triplet structural loss function, which improves the learning capability of features by applying a structural similarity constraint with a triplet metric format on the features. It leverages feature statistics to capture the complex interactions of visual parts. Second, we put forward an adaptive matching module that exploits the dual spatial enhancement module to reinforce target feature discrimination. This not only boosts the representation ability of spatial context but also realizes spatially dynamic feature selection by attending to target deformation information. Moreover, this model introduces a simple but effective matching unit to intuitively evaluate the relative appearance differences between the target and the proposals. In addition, with the obtained discriminative features, AMT is capable of providing precise localization for the target. Therefore, the impact of spatial suppression imposed by window functions can be alleviated, allowing for effective tracking of high-speed moving objects. Extensive experiments prove that AMT outperforms state-of-the-art methods on six public datasets and demonstrate the effectiveness of each component in AMT.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4214810297",
    "type": "article"
  },
  {
    "title": "Scribble-Supervised Meibomian Glands Segmentation in Infrared Images",
    "doi": "https://doi.org/10.1145/3497747",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Xiaoming Liu; Shuo Wang; Ying Zhang; Quan Yuan",
    "corresponding_authors": "",
    "abstract": "Infrared imaging is currently the most effective clinical method to evaluate the morphology of the meibomian glands (MGs) in patients. As an important indicator for monitoring the development of MG dysfunction, it is necessary to accurately measure gland-drop and gland morphology. Although there are existing methods for automatic segmentation of MGs using deep learning frameworks, they require fully annotated ground-truth labels for training, which is time-consuming and laborious. In this article, we proposed a new scribble-supervised deep learning framework for segmenting the MGs, which only requires easily attainable scribble annotations for training. To cope with the shortage of supervision and regularize the network, a transformation consistent strategy is incorporated, which requires the prediction to follow the same transformation if a transform is performed on an input image of the network. The proposed segmentation method consists of two stages. In the first stage, a U-Net network is used to obtain the meibomian region segmentation map. In the second stage, we concentrate on segmenting glands in the meibomian region. We utilize the gradient prior information of the original image at the decoder part of the segmentation network, which can coarsely locate the target contour. We automatically generate reliable labels using the exponential moving average of the predictions during training and filter out the unreliable pseudo-label by uncertainty threshold. Experimental results on a local MG dataset and two other public medical image datasets demonstrate the effectiveness of the proposed segmentation framework.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4214918224",
    "type": "article"
  },
  {
    "title": "iDAM: Iteratively Trained Deep In-loop Filter with Adaptive Model Selection",
    "doi": "https://doi.org/10.1145/3529107",
    "publication_date": "2022-04-01",
    "publication_year": 2022,
    "authors": "Yue Li; Li Zhang; Kai Zhang",
    "corresponding_authors": "",
    "abstract": "As a rapid development of neural-network-based machine learning algorithms, deep learning methods are being tentatively used in a much wider range than well-known artificial intelligence applications such as face recognition or auto-driving. Recently, deep learning models are investigated intensively to improve the compression efficiency for video coding, especially at the in-loop filtering stage. Although deep learning-based in-loop filtering methods in prior arts have already shown a remarkable potential capability in video coding, content propagation issue is still not well recognized and addressed yet. Content propagation is the fact that contents of reference frames are propagated to frames referring to them, which typically leads to over-filtering issues. In this article, we develop an iteratively trained deep in-loop filter with adaptive model selection (iDAM) to address the content propagation issue. First, we propose an iterative training scheme, which enables the network to gradually take into account the impacts of content propagation. Second, we propose a filter selection mechanism, i.e., allowing a block to select from a set of candidate filters with different filtering strengths. Besides, we propose a novel approach to design a conditional in-loop filtering method that can deal with multiple quality levels with a single model and serve the functionality of filter selection by modifying the input parameters. Extensive experiments on top of the latest video coding standard (Versatile Video Coding, VVC) have been conducted to evaluate the proposed techniques. Compared with VTM-11.0, our scheme achieves a new state-of-the-art, leading to {7.91%, 20.25%, 20.44%}, {11.64%, 26.40%, 26.50%}, and {10.97%, 26.63%, 26.77%} BD-rate reductions on average for {Y, Cb, Cr} under all-intra, random-access, and low-delay configurations, respectively. As far as we know, our proposed iDAM scheme provides the highest coding performance compared to all existing solutions. In addition, the syntax elements of the proposed scheme were adopted at the 76th meeting of Audio Video coding Standard (AVS) held this year.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4220715488",
    "type": "article"
  },
  {
    "title": "GLPose: Global-Local Representation Learning for Human Pose Estimation",
    "doi": "https://doi.org/10.1145/3519305",
    "publication_date": "2022-03-12",
    "publication_year": 2022,
    "authors": "Yingying Jiao; Haipeng Chen; Runyang Feng; Haoming Chen; Sifan Wu; Yifang Yin; Zhenguang Liu",
    "corresponding_authors": "",
    "abstract": "Multi-frame human pose estimation is at the core of many computer vision tasks. Although state-of-the-art approaches have demonstrated remarkable results for human pose estimation on static images, their performances inevitably come short when being applied to videos. A central issue lies in the visual degeneration of video frames induced by rapid motion and pose occlusion in dynamic environments. This problem, by nature, is insurmountable for a single frame. Therefore, incorporating complementary visual cues from other video frames becomes an intuitive paradigm. Current state-of-the-art methods usually leverage information from adjacent frames, which unfortunately place excessive focus on only the temporally nearby frames. In this paper, we argue that combining global semantically similar information and local temporal visual context will deliver more comprehensive and more robust representations for human pose estimation. Towards this end, we present an effective framework, namely global-local enhanced pose estimation ( GLPose ) network. Our framework consists of a feature processing module that conditionally incorporates global semantic information and local visual context to generate a robust human representation and a feature enhancement module that excavates complementary information from this aggregated representation to enhance keyframe features for precise estimation. We empirically find that the proposed GLpose outperforms existing methods by a large margin and achieves new state-of-the-art results on large benchmark datasets.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4221025196",
    "type": "article"
  },
  {
    "title": "An <i>l</i> <sub>½</sub> and Graph Regularized Subspace Clustering Method for Robust Image Segmentation",
    "doi": "https://doi.org/10.1145/3476514",
    "publication_date": "2022-02-16",
    "publication_year": 2022,
    "authors": "J. E. Francis; M Baburaj; Sudhish N. George",
    "corresponding_authors": "",
    "abstract": "Segmenting meaningful visual structures from an image is a fundamental and most-addressed problem in image analysis algorithms. However, among factors such as diverse visual patterns, noise, complex backgrounds, and similar textures present in foreground and background, image segmentation still stands as a challenging research problem. In this article, the proposed method employs an unsupervised method that addresses image segmentation as subspace clustering of image feature vectors. Initially, an image is partitioned into a set of homogeneous regions called superpixels, from which Local Spectral Histogram features are computed. Subsequently, a feature data matrix is created whereupon subspace clustering methodology is applied. A single-stage optimization model is formulated with enhanced segmentation capabilities by the combined action of l ½ and l 2 norm minimization. Robustness of l ½ regularization toward both the noise and overestimation of sparsity provides simultaneous noise robustness and better subspace selection, respectively. While l 2 norm facilitates grouping effect. Hence, the designed optimization model ensures an improved sparse solution and a sparse representation matrix with an accurate block diagonal structure, which thereby favours getting properly segmented images. Then, experimental results of the proposed method are compared with the state-of-art algorithms. Results demonstrate the improved performance of our method over the state-of-art algorithms.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4283391360",
    "type": "article"
  },
  {
    "title": "Monocular Vision-aided Depth Measurement from RGB Images for Autonomous UAV Navigation",
    "doi": "https://doi.org/10.1145/3550485",
    "publication_date": "2022-07-29",
    "publication_year": 2022,
    "authors": "Ram Prasad Padhy; Pankaj Kumar; Fabio Narducci; Carmen Bisogni; Sambit Bakshi",
    "corresponding_authors": "",
    "abstract": "Monocular vision-based 3D scene understanding has been an integral part of many machine vision applications. Always, the objective is to measure the depth using a single RGB camera, which is at par with the depth cameras. In this regard, monocular vision-guided autonomous navigation of robots is rapidly gaining popularity among the research community. We propose an effective monocular vision-assisted method to measure the depth of an Unmanned Aerial Vehicle (UAV) from an impending frontal obstacle. This is followed by collision-free navigation in unknown GPS-denied environments. Our approach deals upon the fundamental principle of perspective vision that the size of an object relative to its field of view (FoV) increases as the center of projection moves closer towards the object. Our contribution involves modeling the depth followed by its realization through scale-invariant SURF features. Noisy depth measurements arising due to external wind, or the turbulence in the UAV, are rectified by employing a constant velocity-based Kalman filter model. Necessary control commands are then designed based on the rectified depth value to avoid the obstacle before collision. Rigorous experiments with SURF scale-invariant features reveal an overall accuracy of 88.6% with varying obstacles, in both indoor and outdoor environments.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4288707159",
    "type": "article"
  },
  {
    "title": "Sensor-based Human Activity Recognition Using Graph LSTM and Multi-task Classification Model",
    "doi": "https://doi.org/10.1145/3561387",
    "publication_date": "2022-09-08",
    "publication_year": 2022,
    "authors": "Jie Cao; Youquan Wang; Haicheng Tao; Xiang Guo",
    "corresponding_authors": "",
    "abstract": "This paper explores human activities recognition from sensor-based multi-dimensional streams. Recently, deep learning-based methods such as LSTM and CNN have achieved important progress in practical application scenarios. However, in most previous deep learning-based methods exist potential challenges such as class imbalance and multi-modal heterogeneity with time and sensor signals. To handle those problems, we propose a graph LSTM and Metric Learning model (GLML) with multiple construction graph fusion by modeling the sensor-aspect signals and the graph-aspect activities. GLML is a semi-supervised co-training architecture, which can be seen as several iteratively pseudo-labels sampling processing in the unlabeled data. Specifically, we construct three graphs to capture the different relations in each timestamp. Meanwhile, the graph attention model and attention mechanism are proposed to integrate multiple graph interactions for different sensor signals. Furthermore, to obtain a fixed representation of hidden state units and their neighboring nodes, we introduce the Graph LSTM to learn the graph-aspect relations from graph-structured constructed graphs. Notably, we propose a multi-task classification model combining loss function for classification distribution with deep metric learning to enhance the representation ability of the multi-modal sensor data. Experimental results on three public datasets demonstrate that our proposed GLML model has at least 2.44% improved in average against the state-of-the-art methods.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4296117617",
    "type": "article"
  },
  {
    "title": "Multi-scale Edge-guided Learning for 3D Reconstruction",
    "doi": "https://doi.org/10.1145/3568678",
    "publication_date": "2022-10-20",
    "publication_year": 2022,
    "authors": "Lei Li; Zhiyuan Zhou; Suping Wu; Yongrong Cao",
    "corresponding_authors": "",
    "abstract": "Single-view three-dimensional (3D) object reconstruction has always been a long-term challenging task. Objects with complex topologies are hard to accurately reconstruct, which makes existing methods suffer from blurring of shape boundaries between multiple components in the object. Moreover, most of them cannot balance learning between global geometric structure information and local detail information. In this article, we propose a multi-scale edge-guided learning network (MEGLN) to utilize the global edge information guiding the network to better capture and recover local details. The goal is to exploit the multi-scale learning strategy to learn global edge information and local details, thus achieving robust 3D object reconstruction. We first design a multi-scale Gaussian difference block (MGDB) to extract global edge geometry features for input images of different scales and adopt the attention mechanism to aggregate the extracted global edge geometry features of different scales. Second, we design a multi-scale feature interaction block (MFIB) to learn local details, which utilizes the multi-scale feature interaction to capture the features of multiple objects or components at multiple scales. The MFIB can learn and capture better as much local detail information as possible under the guidance of global edge information. Finally, we dynamically fuse the predicted probabilities of the MGDB and MFIB to obtain the final predicted result, which makes our MEGLN able to recover 3D shapes with global complex topological structures and rich local details via the multi-scale learning strategy. Extensive qualitative and quantitative experimental results on the ShapeNet dataset demonstrate that our approach achieves competitive performance compared with state-of-the-art methods. Code is available at https://github.com/Ray-tju/MEGLN .",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4306873679",
    "type": "article"
  },
  {
    "title": "Lightweight Feature De-redundancy and Self-calibration Network for Efficient Image Super-resolution",
    "doi": "https://doi.org/10.1145/3569900",
    "publication_date": "2022-10-29",
    "publication_year": 2022,
    "authors": "Zhengxue Wang; Guangwei Gao; Juncheng Li; Hui Yan; Hao Zheng; Huimin Lu",
    "corresponding_authors": "",
    "abstract": "In recent years, thanks to the inherent powerful feature representation and learning abilities of the convolutional neural network (CNN), deep CNN-steered single image super-resolution approaches have achieved remarkable performance improvements. However, these methods are often accompanied by large consumption of computing and memory resources, which is difficult to be adopted in real-world application scenes. To handle this issue, we design an efficient Feature De-redundancy and Self-calibration Super-resolution network (FDSCSR). In particular, a Feature De-redundancy and Self-calibration Block (FDSCB) is proposed to reduce the repetitive feature information extracted by the model and further enhance the efficiency of the model. Then, based on FDSCB, a Local Feature Fusion Module is presented to elaborately utilize and fuse the feature information extracted by each FDSCB. Abundant experiments on benchmarks have demonstrated that our FDSCSR achieves superior performance with relatively less computational consumption and storage resource than other state-of-the-art approaches. The code is available at https://github.com/IVIPLab/FDSCSR .",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4307812570",
    "type": "article"
  },
  {
    "title": "Talking Face Generation via Facial Anatomy",
    "doi": "https://doi.org/10.1145/3571746",
    "publication_date": "2022-11-17",
    "publication_year": 2022,
    "authors": "Shiguang Liu; Huixin Wang",
    "corresponding_authors": "",
    "abstract": "To generate the corresponding talking face from a speech audio and a face image, it is essential to match the variations in the facial appearance with the speech audio in subtle movements of different face regions. Nevertheless, the facial movements generated by the existing methods lack detail and vividness, or the methods are only oriented toward a specific person. In this article, we propose a novel two-stage network to generate talking faces for any target identity through annotations of the action units (AUs). In the first stage, the relationship between the audio and the AUs in the audio-to-AU network is learned. The audio-to-AU network needs to produce the consistent AU group for the input audio. In the second stage, the AU group in the first stage and a face image are fed into the generation network to output the resulting talking face image. Various results confirm that, compared to state-of-the-art methods, our approach is able to produce more realistic and vivid talking faces for arbitrary targets with richer details of facial movements, such as the cheek motion and eyebrow motion.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4309617235",
    "type": "article"
  },
  {
    "title": "Semantic Completion and Filtration for Image–Text Retrieval",
    "doi": "https://doi.org/10.1145/3572844",
    "publication_date": "2022-11-23",
    "publication_year": 2022,
    "authors": "Song Soo Yang; Qiang Li; Wenhui Li; Xuanya Li; Ran Jin; Bo Lv; Rui Wang; An-An Liu",
    "corresponding_authors": "",
    "abstract": "Image–text retrieval is a vital task in computer vision and has received growing attention, since it connects cross-modality data. It comes with the critical challenges of learning unified representations and eliminating the large gap between visual and textual domains. Over the past few decades, although many works have made significant progress in image–text retrieval, they are still confronted with the challenge of incomplete text descriptions of images, i.e., how to fully learn the correlations between relevant region–word pairs with semantic diversity. In this article, we propose a novel semantic completion and filtration (SCAF) method to alleviate the above issue. Specifically, the text semantic completion module is presented to generate a complete semantic description of an image using multi-view text descriptions, guiding the model to explore the correlations of relevant region–word pairs fully. Meanwhile, the adaptive structural semantic matching module is presented to filter irrelevant region–word pairs by considering the relevance score of each region–word pair, which facilitates the model to focus on learning the relevance of matching pairs. Extensive experiments show that our SCAF outperforms the existing methods on Flickr30K and MSCOCO datasets, which demonstrates the superiority of our proposed method.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4309764765",
    "type": "article"
  },
  {
    "title": "Hierarchical and Progressive Image Matting",
    "doi": "https://doi.org/10.1145/3540201",
    "publication_date": "2022-06-11",
    "publication_year": 2022,
    "authors": "Yu Qiao; Yuhao Liu; Ziqi Wei; Yuxin Wang; Qiang Cai; Guofeng Zhang; Xin Yang",
    "corresponding_authors": "",
    "abstract": "Most matting research resorts to advanced semantics to achieve high-quality alpha mattes, and a direct low-level features combination is usually explored to complement alpha details. However, we argue that appearance-agnostic integration can only provide biased foreground (FG) details and that alpha mattes require different-level feature aggregation for better pixel-wise opacity perception. In this article, we propose an end-to-end hierarchical and progressive attention matting network (HAttMatting++), which can better predict the opacity of the FG from single RGB images without additional input. Specifically, we utilize channel-wise attention (CA) to distill pyramidal features and employ spatial attention (SA) at different levels to filter appearance cues. This progressive attention mechanism can estimate alpha mattes from adaptive semantics and semantics-indicated boundaries. We also introduce a hybrid loss function fusing structural similarity, mean square error, adversarial loss, and sentry supervision to guide the network to further improve the overall FG structure. In addition, we construct a large-scale and challenging image matting dataset comprised of 59,000 training images and 1,000 test images (a total of 646 distinct FG alpha mattes), which can further improve the robustness of our hierarchical and progressive aggregation model. Extensive experiments demonstrate that the proposed HAttMatting++ can capture sophisticated FG structures and achieve state-of-the-art performance with single RGB images as input.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4313426825",
    "type": "article"
  },
  {
    "title": "A Siamese Inverted Residuals Network Image Steganalysis Scheme based on Deep Learning",
    "doi": "https://doi.org/10.1145/3579166",
    "publication_date": "2023-01-11",
    "publication_year": 2023,
    "authors": "Hao Li; Jinwei Wang; Naixue Xiong; Yi Zhang; Athanasios V. Vasilakos; Xiangyang Luo",
    "corresponding_authors": "",
    "abstract": "With the rapid proliferation of urbanization, massive data in social networks are collected and aggregated in real time, making it possible for criminals to use images as a cover to spread secret information on the Internet. How to determine whether these images contain secret information is a huge challenge for multimedia computing security. The steganalysis method based on deep learning can effectively judge whether the pictures transmitted on the Internet in urban scenes contain secret information, which is of great significance to safeguarding national and social security. Image steganalysis based on deep learning has powerful learning ability and classification ability, and its detection accuracy of steganography images has surpassed that of traditional steganalysis based on manual feature extraction. In recent years, it has become a hot topic of the information hiding technology. However, the detection accuracy of existing deep learning based steganalysis methods still needs to be improved, especially when detecting arbitrary-size and multi-source images, their detection efficientness is easily affected by cover mismatch. In this manuscript, we propose a steganalysis method based on Inverse Residuals structured Siamese network (abbreviated as SiaIRNet method, Sia mese- I nverted- R esiduals- Net work Based method). The SiaIRNet method uses a siamese convolutional neural network (CNN) to obtain the residual features of subgraphs, including three stages of preprocessing, feature extraction, and classification. Firstly, a preprocessing layer with high-pass filters combined with depth-wise separable convolution is designed to more accurately capture the correlation of residuals between feature channels, which can help capture rich and effective residual features. Then, a feature extraction layer based on the Inverse Residuals structure is proposed, which improves the ability of the model to obtain residual features by expanding channels and reusing features. Finally, a fully connected layer is used to classify the cover image and the stego image features. Utilizing three general datasets, BossBase-1.01, BOWS2, and ALASKA#2, as cover images, a large number of experiments are conducted comparing with the state-of-the-art steganalysis methods. The experimental results show that compared with the classical SID method and the latest SiaStegNet method, the detection accuracy of the proposed method for 15 arbitrary-size images is improved by 15.96% and 5.86% on average, respectively, which verifies the higher detection accuracy and better adaptability of the proposed method to multi-source and arbitrary-size images in urban scenes.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4315647004",
    "type": "article"
  },
  {
    "title": "Compressed Screen Content Image Super Resolution",
    "doi": "https://doi.org/10.1145/3589963",
    "publication_date": "2023-03-30",
    "publication_year": 2023,
    "authors": "Meng Wang; Jizheng Xu; Li Zhang; Junru Li; Kai Zhang; Shiqi Wang; Siwei Ma",
    "corresponding_authors": "",
    "abstract": "Screen content has become one of the prominent mediums in the increasingly connected world. With the prevalence of remote collaboration and communication such as virtual conferences and online education, recent years have witnessed a dramatic increase in the data volume of the screen content. Screen content compression serves as the fundamental technology in fostering the storage, transmission, and exhibition of screen content. In this article, we target the super-resolution of the compressed screen content images, intending to tackle the real-world challenge problems. A dataset is proposed for the super-resolution of the screen contents contaminated with different compression distortion levels. Subsequently, we introduce the principle of the multi-hypothesis into the super resolution and propose a new paradigm for the restoration of the compressed screen content images. The luminance and sharpness similarity metric is adopted in the network learning to better adapt to the screen content characteristic and ensure perceptual fidelity. Experimental results verify the superiority and effectiveness of the proposed method.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4361271623",
    "type": "article"
  },
  {
    "title": "Identity Feature Disentanglement for Visible-Infrared Person Re-Identification",
    "doi": "https://doi.org/10.1145/3595183",
    "publication_date": "2023-04-28",
    "publication_year": 2023,
    "authors": "Xiumei Chen; Xiangtao Zheng; Xiaoqiang Lu",
    "corresponding_authors": "",
    "abstract": "Visible-infrared person re-identification (VI-ReID) task aims to retrieve persons from different spectrum cameras (i.e., visible and infrared images). The biggest challenge of VI-ReID is the huge cross-modal discrepancy caused by different imaging mechanisms. Many VI-ReID methods have been proposed by embedding different modal person images into a shared feature space to narrow the cross-modal discrepancy. However, these methods ignore the purification of identity features, which results in identity features containing different modal information and failing to align well. In this article, an identity feature disentanglement method is proposed to disentangle the identity features from identity-irrelevant information, such as pose and modality. Specifically, images of different modalities are first processed to extract shared features that reduce the cross-modal discrepancy preliminarily. Then the extracted feature of each image is disentangled into a latent identity variable and an identity-irrelevant variable. In order to enforce the latent identity variable to contain as much identity information as possible and as little identity-irrelevant information, an ID-discriminative loss and an ID-swapping reconstruction process are additionally designed. Extensive quantitative and qualitative experiments on two popular public VI-ReID datasets, RegDB and SYSU-MM01, demonstrate the efficacy and superiority of the proposed method.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4367314371",
    "type": "article"
  },
  {
    "title": "Scale-Semantic Joint Decoupling Network for Image-Text Retrieval in Remote Sensing",
    "doi": "https://doi.org/10.1145/3603628",
    "publication_date": "2023-06-07",
    "publication_year": 2023,
    "authors": "Chengyu Zheng; Ning Song; Ruoyu Zhang; Lei Huang; Zhiqiang Wei; Jie Nie",
    "corresponding_authors": "",
    "abstract": "Image-text retrieval in remote sensing aims to provide flexible information for data analysis and application. In recent years, state-of-the-art methods are dedicated to “scale decoupling” and “semantic decoupling” strategies to further enhance the capability of representation. However, these previous approaches focus on either the disentangling scale or semantics but ignore merging these two ideas in a union model, which extremely limits the performance of cross-modal retrieval models. To address these issues, we propose a novel Scale-Semantic Joint Decoupling Network (SSJDN) for remote sensing image-text retrieval. Specifically, we design the Bidirectional Scale Decoupling (BSD) module, which exploits Salience Extraction Map (SEM) and Salience Suppression Map (SSM) units to adaptively extract potential features and suppress cumbersome features at other scales in a bidirectional pattern to yield different scale clues. Besides, we design the Label-supervised Semantic Decoupling (LSD) module by leveraging the category semantic labels as prior knowledge to supervise images and texts probing significant semantic-related information. Finally, we design a Semantic-guided Triple Loss (STL), which adaptively generates a constant to adjust the loss function to improve the probability of matching the same semantic image and text and shorten the convergence time of the retrieval model. Our proposed SSJDN outperforms state-of-the-art approaches in numerical experiments conducted on four benchmark remote sensing datasets.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4379795525",
    "type": "article"
  },
  {
    "title": "GJFusion: A Channel-Level Correlation Construction Method for Multimodal Physiological Signal Fusion",
    "doi": "https://doi.org/10.1145/3617503",
    "publication_date": "2023-08-25",
    "publication_year": 2023,
    "authors": "Wuliang Huang; Yiqiang Chen; Xinlong Jiang; Teng Zhang; Qian Chen",
    "corresponding_authors": "",
    "abstract": "Physiological signal based ubiquitous computing has garnered significant attention. However, the heterogeneity among multimodal physiological signals poses a critical challenge to practical applications. To traverse this heterogeneity gap, recent studies have focused on establishing inter-modality correlations. Early works only consider coarse-level correlations between the embeddings of each modality. More recent graph-based approaches incorporate prior knowledge-based correlations, although they may not be entirely accurate. In this article, we propose the Graph Joint Fusion (GJFusion) network, which leverages channel-level inter-modality correlations based on a graph joint to mitigate the heterogeneous gap. Our proposed GJFusion first represents each modality as a graph, with each vertex corresponding to a signal channel, and the edges denoting their functional connectivity. We then join each modality by constructing inter-modality correlations for each salient channel using a sampling-based matching method. Discarded channels are transformed into a virtual vertex through a lightweight pooling operation. Subsequently, the fusion network integrates intra- and inter-modality features, enabling multimodal physiological signal fusion. To validate the effectiveness of our method, we select emotional state recognition as the downstream task and conduct comprehensive experiments on two benchmark datasets. The results demonstrate that our proposed GJFusion network surpasses the latest state-of-the-art methods, achieving relative accuracy improvements of 1.22% and 0.81% on the DEAP and MAHNOB-HCI datasets, respectively. Furthermore, visualization experiments of the salient brain regions reveal the presence of interpretable knowledge within the proposed GJFusion model.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4386162685",
    "type": "article"
  },
  {
    "title": "PLACE Dropout: A Progressive Layer-wise and Channel-wise Dropout for Domain Generalization",
    "doi": "https://doi.org/10.1145/3624015",
    "publication_date": "2023-09-13",
    "publication_year": 2023,
    "authors": "Jintao Guo; Lei Qi; Yinghuan Shi; Yang Gao",
    "corresponding_authors": "",
    "abstract": "Domain generalization (DG) aims to learn a generic model from multiple observed source domains that generalizes well to arbitrary unseen target domains without further training. The major challenge in DG is that the model inevitably faces a severe overfitting issue due to the domain gap between source and target domains. To mitigate this problem, some dropout-based methods have been proposed to resist overfitting by discarding part of the representation of the intermediate layers. However, we observe that most of these methods only conduct the dropout operation in some specific layers, leading to an insufficient regularization effect on the model. We argue that applying dropout at multiple layers can produce stronger regularization effects, which could alleviate the overfitting problem on source domains more adequately than previous layer-specific dropout methods. In this article, we develop a novel layer-wise and channel-wise dropout for DG, which randomly selects one layer and then randomly selects its channels to conduct dropout. Particularly, the proposed method can generate a variety of data variants to better deal with the overfitting issue. We also provide theoretical analysis for our dropout method and prove that it can effectively reduce the generalization error bound. Besides, we leverage the progressive scheme to increase the dropout ratio with the training progress, which can gradually boost the difficulty of training the model to enhance its robustness. Extensive experiments on three standard benchmark datasets have demonstrated that our method outperforms several state-of-the-art DG methods. Our code is available at https://github.com/lingeringlight/PLACEdropout .",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4386710192",
    "type": "article"
  },
  {
    "title": "Incomplete Cross-Modal Retrieval with Deep Correlation Transfer",
    "doi": "https://doi.org/10.1145/3637442",
    "publication_date": "2023-12-13",
    "publication_year": 2023,
    "authors": "Dan Shi; Lei Zhu; Jingjing Li; Guohua Dong; Huaxiang Zhang",
    "corresponding_authors": "",
    "abstract": "Most cross-modal retrieval methods assume the multi-modal training data is complete and has a one-to-one correspondence. However, in the real world, multi-modal data generally suffers from missing modality information due to the uncertainty of data collection and storage processes, which limits the practical application of existing cross-modal retrieval methods. Although some solutions have been proposed to generate the missing modality data using a single pseudo sample, this may lead to incomplete semantic restoration and sub-optimal retrieval results due to the limited semantic information it provides. To address this challenge, this article proposes an Incomplete Cross-Modal Retrieval with Deep Correlation Transfer (ICMR-DCT) method that can robustly model incomplete multi-modal data and dynamically capture the adjacency semantic correlation for cross-modal retrieval. Specifically, we construct intra-modal graph attention-based auto-encoder to learn modality-invariant representations by performing semantic reconstruction through intra-modality adjacency correlation mining. Then, we design dual cross-modal alignment constraints to project multi-modal representations into a common semantic space, thus bridging the heterogeneous modality gap and enhancing the discriminability of the common representation. We further introduce semantic preservation to enhance adjacency semantic information and achieve cross-modal semantic correlation. Moreover, we propose a nearest-neighbor weighting integration strategy with cross-modal correlation transfer to generate the missing modality data according to inter-modality mapping relations and adjacency correlations between each sample and its neighbors, which improves the robustness of our method against incomplete multi-modal training data. Extensive experiments on three widely tested benchmark datasets demonstrate the superior performance of our method in cross-modal retrieval tasks under both complete and incomplete retrieval scenarios. Our used datasets and source codes are available at https://github.com/shidan0122/DCT.git .",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4389675233",
    "type": "article"
  },
  {
    "title": "Knowledge-integrated Multi-modal Movie Turning Point identification",
    "doi": "https://doi.org/10.1145/3638557",
    "publication_date": "2023-12-23",
    "publication_year": 2023,
    "authors": "Depei Wang; Ruifeng Xu; Lianglun Cheng; Zhuowei Wang",
    "corresponding_authors": "",
    "abstract": "The rapid development of artificial intelligence provides rich technologies and tools for the automated understanding of literary works. As a comprehensive carrier of storylines, movies are natural multimodal data sources that provide sufficient data foundations, and how to fully leverage the benefits of data remains a sustainable research hotspot. In addition, the efficient representation of multi-source data also poses new challenges for information fusion technology. Therefore, we propose a knowledge-enhanced turning points identification (KTPi) method for multimodal scene recognition. First, the BiLSTM method is used to encode scene text and integrate contextual information into scene representations to complete text sequence modeling. Then, the graph structure is used to model all scenes, which strengthens long-range semantic dependencies between scenes and enhances scene representations using graph convolution network. After, the self-supervised method is used to obtain the optimal number of neighboring nodes in sparse graph. Next, actor and verb knowledge involved in the scene text are added to the multimodal data to enhance the diversity of scene feature expressions. Finally, the teacher-student network strategy is used to train the KTPi model. Experimental results show that KTPi outperforms baseline methods in scene role recognition tasks, and ablation experiments show that incorporating knowledge into multimodal model can improve its performance.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4390143357",
    "type": "article"
  },
  {
    "title": "Adjusting forward error correction with temporal scaling for TCP-friendly streaming MPEG",
    "doi": "https://doi.org/10.1145/1111604.1111605",
    "publication_date": "2005-11-01",
    "publication_year": 2005,
    "authors": "Huahui Wu; Mark Claypool; Robert Kinicki",
    "corresponding_authors": "",
    "abstract": "New TCP-friendly constraints require multimedia flows to reduce their data rates under packet loss to that of a conformant TCP flow. To reduce data rates while preserving real-time playout, temporal scaling can be used to discard the encoded multimedia frames that have the least impact on perceived video quality. To limit the impact of lost packets, Forward Error Correction (FEC) can be used to repair frames damaged by packet loss. However, adding FEC requires further reduction of multimedia data, making the decision of how much FEC to use of critical importance. Current approaches use either inflexible FEC patterns or adapt to packet loss on the network without regard to TCP-friendly data rate constraints. In this article, we analytically model the playable frame rate of a TCP-friendly MPEG stream with FEC and temporal scaling, capturing the impact of distributing FEC within MPEG frame types with interframe dependencies. For a given network condition and MPEG video encoding, we use our model to exhaustively search for the optimal combination of FEC and temporal scaling that yields the highest playable frame rate within TCP-friendly constraints. Analytic experiments over a range of network and application conditions indicate that adjustable FEC with temporal scaling can provide a significant performance improvement over current approaches. Extensive simulation experiments based on Internet traces show that our model can be effective as part of a streaming protocol that chooses FEC and temporal scaling patterns that meet dynamically-changing application and network conditions.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W2158603205",
    "type": "article"
  },
  {
    "title": "Process prioritization using output production",
    "doi": "https://doi.org/10.1145/1201730.1201734",
    "publication_date": "2006-11-01",
    "publication_year": 2006,
    "authors": "Yoav Etsion; Dan Tsafrir; Dror G. Feitelson",
    "corresponding_authors": "",
    "abstract": "Desktop operating systems such as Windows and Linux base scheduling decisions on CPU consumption; processes that consume fewer CPU cycles are prioritized, assuming that interactive processes gain from this since they spend most of their time waiting for user input. However, this doesn't work for modern multimedia applications which require significant CPU resources. We therefore suggest a new metric to identify interactive processes by explicitly measuring interactions with the user, and we use it to design and implement a process scheduler. Measurements using a variety of applications indicate that this scheduler is very effective in distinguishing between competing interactive and noninteractive processes.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W1968954610",
    "type": "article"
  },
  {
    "title": "Using geometric properties of topographic manifold to detect and track eyes for human-computer interaction",
    "doi": "https://doi.org/10.1145/1314303.1314306",
    "publication_date": "2007-12-01",
    "publication_year": 2007,
    "authors": "Jun Wang; Lijun Yin; Jason Moore",
    "corresponding_authors": "",
    "abstract": "Automatic eye detection and tracking is an important component for advanced human-computer interface design. Accurate eye localization can help develop a successful system for face recognition and emotion identification. In this article, we propose a novel approach to detect and track eyes using geometric surface features on topographic manifold of eye images. First, in the joint spatial-intensity domain, a facial image is treated as a 3D terrain surface or image topographic manifold. In particular, eye regions exhibit certain intrinsic geometric traits on this topographic manifold, namely, the pit -labeled center and hillside -like surround regions. Applying a terrain classification procedure on the topographic manifold of facial images, each location of the manifold can be labeled to generate a terrain map. We use the distribution of terrain labels to represent the eye terrain pattern. The Bhattacharyya affinity is employed to measure the distribution similarity between two topographic manifolds. Based on the Bhattacharyya kernel, a support vector machine is applied for selecting proper eye pairs from the pit-labeled candidates. Second, given detected eyes on the first frame of a video sequence, a mutual-information-based fitting function is defined to describe the similarity between two terrain surfaces of neighboring frames. By optimizing the fitting function, eye locations are updated for subsequent frames. The distinction of the proposed approach lies in that both eye detection and eye tracking are performed on the derived topographic manifold, rather than on an original-intensity image domain. The robustness of the approach is demonstrated under various imaging conditions and with different facial appearances, using both static images and video sequences without background constraints.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2032646771",
    "type": "article"
  },
  {
    "title": "Detecting eye fixations by projection clustering",
    "doi": "https://doi.org/10.1145/1314303.1314308",
    "publication_date": "2007-12-01",
    "publication_year": 2007,
    "authors": "Thierry Urruty; Stanislas Lew; Nacim Ihadaddene; Dan A. Simovici",
    "corresponding_authors": "",
    "abstract": "Eye movements are certainly the most natural and repetitive movement of a human being. The most mundane activity, such as watching television or reading a newspaper, involves this automatic activity which consists of shifting our gaze from one point to another. Identification of the components of eye movements (fixations and saccades) is an essential part in the analysis of visual behavior because these types of movements provide the basic elements used by further investigations of human vision. However, many of the algorithms that detect fixations present a number of problems. In this article, we present a new fixation identification technique that is based on clustering of eye positions, using projections and projection aggregation applied to static pictures. We also present a new method that computes dispersion of eye fixations in videos considering a multiuser environment. To demonstrate the performance and usefulness of our approach we discuss our experimental work with two different applications: on fixed image and video.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2150855095",
    "type": "article"
  },
  {
    "title": "A dynamic decision network framework for online media adaptation in stroke rehabilitation",
    "doi": "https://doi.org/10.1145/1404880.1404884",
    "publication_date": "2008-10-01",
    "publication_year": 2008,
    "authors": "Yinpeng Chen; Weiwei Xu; Hari Sundaram; Thanassis Rikakis; Sheng-Min Liu",
    "corresponding_authors": "",
    "abstract": "In this article, we present a media adaptation framework for an immersive biofeedback system for stroke patient rehabilitation. In our biofeedback system, media adaptation refers to changes in audio/visual feedback as well as changes in physical environment. Effective media adaptation frameworks help patients recover generative plans for arm movement with potential for significantly shortened therapeutic time. The media adaptation problem has significant challenges—(a) high dimensionality of adaptation parameter space; (b) variability in the patient performance across and within sessions; (c) the actual rehabilitation plan is typically a non-first-order Markov process, making the learning task hard. Our key insight is to understand media adaptation as a real-time feedback control problem. We use a mixture-of-experts based Dynamic Decision Network (DDN) for online media adaptation. We train DDN mixtures per patient, per session. The mixture models address two basic questions—(a) given a specific adaptation suggested by the domain experts, predict the patient performance, and (b) given the expected performance, determine the optimal adaptation decision. The questions are answered through an optimality criterion based search on DDN models trained in previous sessions. We have also developed new validation metrics and have very good results for both questions on actual stroke rehabilitation data.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W1967922755",
    "type": "article"
  },
  {
    "title": "End-to-end delay control of multimedia applications over multihop wireless links",
    "doi": "https://doi.org/10.1145/1413862.1413869",
    "publication_date": "2008-11-01",
    "publication_year": 2008,
    "authors": "Wenbo He; Klara Nahrstedt; Xue Liu",
    "corresponding_authors": "",
    "abstract": "The proliferation of multimedia applications over mobile, resource-constrained wireless networks has raised the need for techniques that adapt these applications both to clients' Quality of Service (QoS) requirements and to network resource constraints. This article investigates the upper-layer adaptation mechanisms to achieve end-to-end delay control for multimedia applications. The proposed adaptation approach spans application layer, middleware layer and network layer. In application layer, the requirement adaptor dynamically changes the requirement levels according to end-to-end delay measurement and acceptable QoS requirements for the end-users. In middleware layer, the priority adaptor is used to dynamically adjust the service classes for applications using feedback control theory. In network layer, the service differentiation scheduler assigns different network resources (e.g., bandwidth) to different service classes. With the coordination of these three layers, our approach can adaptively assign resources to multimedia applications. To evaluate the impact of our adaptation scheme, we built a real IEEE 802.11 ad hoc network testbed. The test-bed experiments show that the proposed upper-layer adaptation for end-to-end delay control successfully adjusts multimedia applications to meet delay requirements in many scenarios.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2137320174",
    "type": "article"
  },
  {
    "title": "Distributed musical performances",
    "doi": "https://doi.org/10.1145/1352012.1352018",
    "publication_date": "2008-05-01",
    "publication_year": 2008,
    "authors": "Roger Zimmermann; Elaine Chew; Sakire Arslan Ay; Moses Pawar",
    "corresponding_authors": "",
    "abstract": "An increasing number of novel applications produce a rich set of different data types that need to be managed efficiently and coherently. In this article we present our experience with designing and implementing a data management infrastructure for a distributed immersive performance (DIP) application. The DIP project investigates a versatile framework for the capture, recording, and replay of video, audio, and MIDI (Musical Instrument Digital Interface) streams in an interactive environment for collaborative music performance. We are focusing on two classes of data streams that are generated within this environment. The first category consists of high-resolution isochronous media streams, namely audio and video. The second class comprises MIDI data produced by electronic instruments. MIDI event sequences are alphanumeric in nature and fall into the category of the data streams that have been of interest to data management researchers in recent years. We present our data management architecture, which provides a repository for all DIP data. Streams of both categories need to be acquired, transmitted, stored, and replayed in real time. Data items are correlated across different streams with temporal indices. The audio and video streams are managed in our own High-performance Data Recording Architecture (HYDRA), which integrates multistream recording and retrieval in a consistent manner. This paper reports on the practical issues and challenges that we encountered during the design, implementation and experimental phases of our prototype. We also present some analysis results and discuss future extensions for the architecture.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2006612371",
    "type": "article"
  },
  {
    "title": "Robust tracking and remapping of eye appearance with passive computer vision",
    "doi": "https://doi.org/10.1145/1314303.1314305",
    "publication_date": "2007-12-01",
    "publication_year": 2007,
    "authors": "Carlo Colombo; Dario Comanducci; Alberto Del Bimbo",
    "corresponding_authors": "",
    "abstract": "A single-camera iris-tracking and remapping approach based on passive computer vision is presented. Tracking is aimed at obtaining accurate and robust measurements of the iris/pupil position. To this purpose, a robust method for ellipse fitting is used, employing search constraints so as to achieve better performance with respect to the standard RANSAC algorithm. Tracking also embeds an iris localization algorithm (working as a bootstrap multiple-hypotheses generation step), and a blink detector that can detect voluntary eye blinks in human-computer interaction applications. On-screen remapping incorporates a head-tracking method capable of compensating for small user-head movements. The approach operates in real time under different light conditions and in the presence of distractors. An extensive set of experiments is presented and discussed. In particular, an evaluation method for the choice of layout of both hardware components and calibration points is described. Experiments also investigate the importance of providing a visual feedback to the user, and the benefits gained from performing head compensation, especially during image-to-screen map calibration.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2042088881",
    "type": "article"
  },
  {
    "title": "Fast min-hashing indexing and robust spatio-temporal matching for detecting video copies",
    "doi": "https://doi.org/10.1145/1671962.1671966",
    "publication_date": "2010-03-01",
    "publication_year": 2010,
    "authors": "Chih‐Yi Chiu; Hsin‐Min Wang; Chu‐Song Chen",
    "corresponding_authors": "",
    "abstract": "The increase in the number of video copies, both legal and illegal, has become a major problem in the multimedia and Internet era. In this article, we propose a novel method for detecting various video copies in a video sequence. To achieve fast and robust detection, the method fully integrates several components, namely the min-hashing signature to compactly represent a video sequence, a spatio-temporal matching scheme to accurately evaluate video similarity compiled from the spatial and temporal aspects, and some speedup techniques to expedite both min-hashing indexing and spatio-temporal matching. The results of experiments demonstrate that, compared to several baseline methods with different feature descriptors and matching schemes, the proposed method which combines both global and local feature descriptors yields the best performance when encountering a variety of video transformations. The method is very fast, requiring approximately 0.06 seconds to search for copies of a thirty-second video clip in a six-hour video sequence.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W1987264320",
    "type": "article"
  },
  {
    "title": "Detecting malicious nodes in peer-to-peer streaming by peer-based monitoring",
    "doi": "https://doi.org/10.1145/1671962.1671965",
    "publication_date": "2010-03-01",
    "publication_year": 2010,
    "authors": "Xing Jin; S.-H. Gary Chan",
    "corresponding_authors": "",
    "abstract": "Current peer-to-peer (P2P) streaming systems often assume that nodes cooperate to upload and download data. However, in the open environment of the Internet, this is not necessarily true and there exist malicious nodes in the system. In this article, we study malicious actions of nodes that can be detected through peer-based monitoring. We require each node to monitor the data received and to periodically send monitoring messages about its neighbors to some trustworthy nodes. To efficiently store and search messages among multiple trustworthy nodes, we organize trustworthy nodes into a threaded binary tree. Trustworthy nodes also dynamically redistribute monitoring messages among themselves to achieve load balancing. Our simulation results show that this scheme can efficiently detect malicious nodes with high accuracy, and that the dynamic redistribution method can achieve good load balancing among trustworthy nodes.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2065343891",
    "type": "article"
  },
  {
    "title": "Effect of compressed offline foveated video on viewing behavior and subjective quality",
    "doi": "https://doi.org/10.1145/1671954.1671958",
    "publication_date": "2010-02-01",
    "publication_year": 2010,
    "authors": "Marcus Nyström; Kenneth Holmqvist",
    "corresponding_authors": "",
    "abstract": "Offline foveation is a technique to improve the compression efficiency of digitized video. The general idea behind offline foveation is to blur video regions where no or a small number of previewers look without decreasing the subjective quality for later viewers. It relies on the fact that peripheral vision is reduced compared to central vision, and the observation that during free-viewing humans' gaze positions generally coincide when watching video. In this article, we conduct two experiments to assess how offline foveation affects viewing behavior and subjective quality. In the first experiment, 15 subjects free-viewed six video clips before and after offline foveation whereas in the second experiment we had 17 subjects assessing the quality of these videos after one, two, and three consecutive viewings. Eye movements were measured during the experiments. Results showed that, although offline foveation prior to encoding with H.264 yielded data reductions up to 52% (20% average) on the tested videos, it had little or no effect on where people looked, their intersubject dispersion, fixation duration, saccade amplitude, or the experienced quality during first-time viewing. However, seeing the videos more than once increased the intersubject dispersion and decreased the subjective quality. In view of these results, we discuss the usage of offline foveated video in practical applications.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W1990963501",
    "type": "article"
  },
  {
    "title": "Energy-efficient multicasting of multiview 3D videos to mobile devices",
    "doi": "https://doi.org/10.1145/2348816.2348824",
    "publication_date": "2012-09-01",
    "publication_year": 2012,
    "authors": "Ahmed M. Hamza; Mohamed Hefeeda",
    "corresponding_authors": "",
    "abstract": "Multicasting multiple video streams over wireless broadband access networks enables the delivery of multimedia content to large-scale user communities in a cost-efficient manner. Three dimensional (3D) videos are the next natural step in the evolution of digital media technologies. In order to provide 3D perception, 3D video streams contain one or more views that greatly increase their bandwidth requirements. Due to the limited channel capacity and variable bit rate of the videos, multicasting multiple 3D videos over wireless broadband networks is a challenging problem. In this article, we consider a 4G wireless access network in which a number of 3D videos represented in two-view plus depth format and encoded using scalable video coders are multicast. We formulate the optimal 3D video multicasting problem to maximize the quality of rendered virtual views on the receivers' displays. We show that this problem is NP-complete and present a polynomial time approximation algorithm to solve it. We then extend the proposed algorithm to efficiently schedule the transmission of the chosen substreams from each video in order to maximize the power saving on the mobile receivers. Our simulation-based experimental results show that our algorithm provides solutions that are within 0.3 dB of the optimal solutions while satisfying real-time requirements of multicast systems. In addition, our algorithm results in an average power consumption reduction of 86%.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2056368207",
    "type": "article"
  },
  {
    "title": "Editorial note and call for nominations",
    "doi": "https://doi.org/10.1145/2071396.2071397",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Ralf Steinmetz",
    "corresponding_authors": "Ralf Steinmetz",
    "abstract": "No abstract available.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2114569624",
    "type": "editorial"
  },
  {
    "title": "Are we in the middle of a video streaming revolution?",
    "doi": "https://doi.org/10.1145/2490826",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Viswanathan Swaminathan",
    "corresponding_authors": "Viswanathan Swaminathan",
    "abstract": "It has been roughly 20 years since the beginning of video streaming over the Internet. Until very recently, video streaming experiences left much to be desired. Over the last few years, this has significantly improved making monetization of streaming, possible. Recently, there has been an explosion of commercial video delivery services over the Internet, sometimes referred to as over-the-top (OTT) delivery. All these services invariably use streaming technologies. Initially, streaming had all the promise, then for a long time, it was download and play, later progressive download for short content, and now it is streaming again. Did streaming win the download versus streaming contest? Did the best technology win? The improvement in streaming experience has been possible through a variety of new streaming technologies, some proprietary and others extensions to standard protocols. The primary delivery mechanism for entertainment video, both premium content like movies and user generated content (UGC), tends to be HTTP streaming. Is HTTP streaming the panacea for all problems? The goal of this article is to give an industry perspective of what fundamentally changed in video streaming that makes it commercially viable now. This article outlines how a blend of technology choices between download and streaming makes the current wave of ubiquitous streaming possible for entertainment video delivery. After identifying problems that still need to be solved, the article concludes with the lessons learnt from the video streaming evolution.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2007330003",
    "type": "article"
  },
  {
    "title": "A fuzzy algorithm for dynamically adaptive multimedia streaming",
    "doi": "https://doi.org/10.1145/1925101.1925106",
    "publication_date": "2011-02-01",
    "publication_year": 2011,
    "authors": "Susmit Bagchi",
    "corresponding_authors": "Susmit Bagchi",
    "abstract": "The QoS-aware delivery model of multimedia is an interesting research area. The wireless networking systems connecting mobile clients and media servers have created the paradigm of mobile multimedia. In mobile multimedia systems, the media delivery model has to maintain two diagonally opposite objectives, such as maintaining QoS of playback and saving energy consumption of the mobile devices. The traditional pull, push, and the hybrid push-pull models of media delivery are not completely suitable to offer consistent QoS of playback while saving the energy consumptions at mobile devices. This article proposes a novel multimedia delivery system based on the Fuzzy Adaptive Buffering (FAB) algorithm using pull model. The FAB algorithm employs a fuzzy inference technique and dynamically adapts to the execution environments. The experimental results illustrate that the FAB algorithm successfully adapts to dynamic execution contexts while maintaining playback-QoS and saving the energy consumption of the mobile clients by keeping the data prefetching thread in the sleeping mode from 31.44% to 97.4% of streaming time depending on the execution environments.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2029993887",
    "type": "article"
  },
  {
    "title": "Discovering multirelational structure in social media streams",
    "doi": "https://doi.org/10.1145/2071396.2071400",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Yu‐Ru Lin; Hari Sundaram; Munmun De Choudhury; Aisling Kelliher",
    "corresponding_authors": "",
    "abstract": "In this article, we present a novel algorithm to discover multirelational structures from social media streams. A media item such as a photograph exists as part of a meaningful interrelationship among several attributes, including time, visual content, users, and actions. Discovery of such relational structures enables us to understand the semantics of human activity and has applications in content organization, recommendation algorithms, and exploratory social network analysis. We are proposing a novel nonnegative matrix factorization framework to characterize relational structures of group photo streams. The factorization incorporates image content features and contextual information. The idea is to consider a cluster as having similar relational patterns; each cluster consists of photos relating to similar content or context. Relations represent different aspects of the photo stream data, including visual content, associated tags, photo owners, and post times. The extracted structures minimize the mutual information of the predicted joint distribution. We also introduce a relational modularity function to determine the structure cost penalty, and hence determine the number of clusters. Extensive experiments on a large Flickr dataset suggest that our approach is able to extract meaningful relational patterns from group photo streams. We evaluate the utility of the discovered structures through a tag prediction task and through a user study. Our results show that our method based on relational structures, outperforms baseline methods, including feature and tag frequency based techniques, by 35%--420%. We have conducted a qualitative user study to evaluate the benefits of our framework in exploring group photo streams. The study indicates that users found the extracted clustering results clearly represent major themes in a group; the clustering results not only reflect how users describe the group data but often lead the users to discover the evolution of the group activity.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2047871136",
    "type": "article"
  },
  {
    "title": "Congestion Control for Network-Aware Telehaptic Communication",
    "doi": "https://doi.org/10.1145/3052821",
    "publication_date": "2017-03-21",
    "publication_year": 2017,
    "authors": "Vineet Gokhale; Jayakrishnan Nair; Subhasis Chaudhuri",
    "corresponding_authors": "",
    "abstract": "Telehaptic applications involve delay-sensitive multimedia communication between remote locations with distinct Quality of Service (QoS) requirements for different media components. These QoS constraints pose a variety of challenges, especially when the communication occurs over a shared network, with unknown and time-varying cross-traffic. In this work, we propose a transport layer congestion control protocol for telehaptic applications operating over shared networks, termed as Dynamic Packetization Module (DPM). DPM is a lossless, network-aware protocol that tunes the telehaptic packetization rate based on the level of congestion in the network. To monitor the network congestion, we devise a novel network feedback module , which communicates the end-to-end delays encountered by the telehaptic packets to the respective transmitters with negligible overhead. Via extensive simulations, we show that DPM meets the QoS requirements of telehaptic applications over a wide range of network cross-traffic conditions. We also report qualitative results of a real-time telepottery experiment with several human subjects, which reveal that DPM preserves the quality of telehaptic activity even under heavily congested network scenarios. Finally, we compare the performance of DPM with several previously proposed telehaptic communication protocols and demonstrate that DPM outperforms these protocols.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2581145950",
    "type": "article"
  },
  {
    "title": "Machine Learning--Based Parametric Audiovisual Quality Prediction Models for Real-Time Communications",
    "doi": "https://doi.org/10.1145/3051482",
    "publication_date": "2017-03-15",
    "publication_year": 2017,
    "authors": "Edip Demirbilek; Jean‐Charles Grégoire",
    "corresponding_authors": "",
    "abstract": "In order to mechanically predict audiovisual quality in interactive multimedia services, we have developed machine learning--based no-reference parametric models. We have compared Decision Trees--based ensemble methods, Genetic Programming and Deep Learning models that have one and more hidden layers. We have used the Institut national de la recherche scientifique (INRS) audiovisual quality dataset specifically designed to include ranges of parameters and degradations typically seen in real-time communications. Decision Trees--based ensemble methods have outperformed both Deep Learning-- and Genetic Programming--based models in terms of Root-Mean-Square Error (RMSE) and Pearson correlation values. We have also trained and developed models on various publicly available datasets and have compared our results with those of these original models. Our studies show that Random Forests--based prediction models achieve high accuracy for both the INRS audiovisual quality dataset and other publicly available comparable datasets.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2600357261",
    "type": "article"
  },
  {
    "title": "Multimodal Retrieval with Diversification and Relevance Feedback for Tourist Attraction Images",
    "doi": "https://doi.org/10.1145/3103613",
    "publication_date": "2017-08-12",
    "publication_year": 2017,
    "authors": "Duc‐Tien Dang‐Nguyen; Luca Piras; Giorgio Giacinto; Giulia Boato; Francesco G. B. De Natale",
    "corresponding_authors": "",
    "abstract": "In this article, we present a novel framework that can produce a visual description of a tourist attraction by choosing the most diverse pictures from community-contributed datasets, which describe different details of the queried location. The main strength of the proposed approach is its flexibility that permits us to filter out non-relevant images and to obtain a reliable set of diverse and relevant images by first clustering similar images according to their textual descriptions and their visual content and then extracting images from different clusters according to a measure of the user’s credibility. Clustering is based on a two-step process, where textual descriptions are used first and the clusters are then refined according to the visual features. The degree of diversification can be further increased by exploiting users’ judgments on the results produced by the proposed algorithm through a novel approach, where users not only provide a relevance feedback but also a diversity feedback. Experimental results performed on the MediaEval 2015 “Retrieving Diverse Social Images” dataset show that the proposed framework can achieve very good performance both in the case of automatic retrieval of diverse images and in the case of the exploitation of the users’ feedback. The effectiveness of the proposed approach has been also confirmed by a small case study involving a number of real users.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2746646173",
    "type": "article"
  },
  {
    "title": "Early Recognition of 3D Human Actions",
    "doi": "https://doi.org/10.1145/3131344",
    "publication_date": "2018-03-26",
    "publication_year": 2018,
    "authors": "Sheng Li; Kang Li; Yun Fu",
    "corresponding_authors": "",
    "abstract": "Action recognition is an important research problem of human motion analysis (HMA). In recent years, 3D observation-based action recognition has been receiving increasing interest in the multimedia and computer vision communities, due to the recent advent of cost-effective sensors, such as depth camera Kinect. This work takes this one step further, focusing on early recognition of ongoing 3D human actions, which is beneficial for a large variety of time-critical applications, e.g., gesture-based human machine interaction, somatosensory games, and so forth. Our goal is to infer the class label information of 3D human actions with partial observation of temporally incomplete action executions. By considering 3D action data as multivariate time series (m.t.s.) synchronized to a shared common clock (frames), we propose a stochastic process called dynamic marked point process (DMP) to model the 3D action as temporal dynamic patterns, where both timing and strength information are captured. To achieve even more early and better accuracy of recognition, we also explore the temporal dependency patterns between feature dimensions. A probabilistic suffix tree is constructed to represent sequential patterns among features in terms of the variable-order Markov model (VMM). Our approach and several baselines are evaluated on five 3D human action datasets. Extensive results show that our approach achieves superior performance for early recognition of 3D human actions.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2794984788",
    "type": "article"
  },
  {
    "title": "DeepProduct",
    "doi": "https://doi.org/10.1145/3184745",
    "publication_date": "2018-04-25",
    "publication_year": 2018,
    "authors": "Yu–Gang Jiang; Minjun Li; Xi Wang; Wei Liu; Xian‐Sheng Hua",
    "corresponding_authors": "",
    "abstract": "Features extracted by deep networks have been popular in many visual search tasks. This article studies deep network structures and training schemes for mobile visual search. The goal is to learn an effective yet portable feature representation that is suitable for bridging the domain gap between mobile user photos and (mostly) professionally taken product images while keeping the computational cost acceptable for mobile-based applications. The technical contributions are twofold. First, we propose an alternative of the contrastive loss popularly used for training deep Siamese networks, namely robust contrastive loss, where we relax the penalty on some positive and negative pairs to alleviate overfitting. Second, a simple multitask fine-tuning scheme is leveraged to train the network, which not only utilizes knowledge from the provided training photo pairs but also harnesses additional information from the large ImageNet dataset to regularize the fine-tuning process. Extensive experiments on challenging real-world datasets demonstrate that both the robust contrastive loss and the multitask fine-tuning scheme are effective, leading to very promising results with a time cost suitable for mobile product search scenarios.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2800923548",
    "type": "article"
  },
  {
    "title": "Using rich social media information for music recommendation via hypergraph model",
    "doi": "https://doi.org/10.1145/2037676.2037679",
    "publication_date": "2011-10-01",
    "publication_year": 2011,
    "authors": "Shulong Tan; Jiajun Bu; Chun Chen; Bin Xu; Can Wang; Xiaofei He",
    "corresponding_authors": "",
    "abstract": "There are various kinds of social media information, including different types of objects and relations among these objects, in music social communities such as Last.fm and Pandora. This information is valuable for music recommendation. However, there are two main challenges to exploit this rich social media information: (a) There are many different types of objects and relations in music social communities, which makes it difficult to develop a unified framework taking into account all objects and relations. (b) In these communities, some relations are much more sophisticated than pairwise relation, and thus cannot be simply modeled by a graph. We propose a novel music recommendation algorithm by using both multiple kinds of social media information and music acoustic-based content. Instead of graph, we use hypergraph to model the various objects and relations, and consider music recommendation as a ranking problem on this hypergraph. While an edge of an ordinary graph connects only two objects, a hyperedge represents a set of objects. In this way, hypergraph can be naturally used to model high-order relations.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W4377200789",
    "type": "article"
  },
  {
    "title": "Exploring interest correlation for peer-to-peer socialized video sharing",
    "doi": "https://doi.org/10.1145/2071396.2071401",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Xu Cheng; Jiangchuan Liu",
    "corresponding_authors": "",
    "abstract": "The last five years have witnessed an explosion of networked video sharing, represented by YouTube, as a new killer Internet application. Their sustainable development however is severely hindered by the intrinsic limit of their client/server architecture. A shift to the peer-to-peer paradigm has been widely suggested with success already shown in live video streaming and movie-on-demand. Unfortunately, our latest measurement demonstrates that short video clips exhibit drastically different statistics, which would simply render these existing solutions suboptimal, if not entirely inapplicable. Our long-term measurement over five million YouTube videos, on the other hand, reveals interesting social networks with strong correlation among the videos, thus opening new opportunities to explore. In this article, we present NetTube, a novel peer-to-peer assisted delivering framework that explores the user interest correlation for short video sharing. We address a series of key design issues to realize the system, including a bi-layer overlay, an efficient indexing scheme, a delay-aware scheduling mechanism, and a prefetching strategy leveraging interest correlation. We evaluate NetTube through both simulations and prototype experiments, which show that it greatly reduces the server workload, improves the playback quality and scales well.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2001149961",
    "type": "article"
  },
  {
    "title": "Content-based copy detection through multimodal feature representation and temporal pyramid matching",
    "doi": "https://doi.org/10.1145/2542205.2542208",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Luntian Mou; Tiejun Huang; Yonghong Tian; Menglin Jiang; Wen Gao",
    "corresponding_authors": "",
    "abstract": "Content-based copy detection (CBCD) is drawing increasing attention as an alternative technology to watermarking for video identification and copyright protection. In this article, we present a comprehensive method to detect copies that are subjected to complicated transformations. A multimodal feature representation scheme is designed to exploit the complementarity of audio features, global and local visual features so that optimal overall robustness to a wide range of complicated modifications can be achieved. Meanwhile, a temporal pyramid matching algorithm is proposed to assemble frame-level similarity search results into sequence-level matching results through similarity evaluation over multiple temporal granularities. Additionally, inverted indexing and locality sensitive hashing (LSH) are also adopted to speed up similarity search. Experimental results over benchmarking datasets of TRECVID 2010 and 2009 demonstrate that the proposed method outperforms other methods for most transformations in terms of copy detection accuracy. The evaluation results also suggest that our method can achieve competitive copy localization preciseness.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2091154729",
    "type": "article"
  },
  {
    "title": "Detecting profilable and overlapping communities with user-generated multimedia contents in LBSNs",
    "doi": "https://doi.org/10.1145/2502415",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Yiliang Zhao; Qiang Chen; Shuicheng Yan; Tat‐Seng Chua; Daqing Zhang",
    "corresponding_authors": "",
    "abstract": "In location-based social networks (LBSNs), users implicitly interact with each other by visiting places, issuing comments and/or uploading photos. These heterogeneous interactions convey the latent information for identifying meaningful user groups, namely social communities, which exhibit unique location-oriented characteristics. In this work, we aim to detect and profile social communities in LBSNs by representing the heterogeneous interactions with a multimodality nonuniform hypergraph. Here, the vertices of the hypergraph are users, venues, textual comments or photos and the hyperedges characterize the k -partite heterogeneous interactions such as posting certain comments or uploading certain photos while visiting certain places. We then view each detected social community as a dense subgraph within the heterogeneous hypergraph, where the user community is constructed by the vertices and edges in the dense subgraph and the profile of the community is characterized by the vertices related with venues, comments and photos and their inter-relations. We present an efficient algorithm to detect the overlapped dense subgraphs, where the profile of each social community is guaranteed to be available by constraining the minimal number of vertices in each modality. Extensive experiments on Foursquare data well validated the effectiveness of the proposed framework in terms of detecting meaningful social communities and uncovering their underlying profiles in LBSNs.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2169813567",
    "type": "article"
  },
  {
    "title": "Selecting vantage objects for similarity indexing",
    "doi": "https://doi.org/10.1145/2000486.2000490",
    "publication_date": "2011-08-01",
    "publication_year": 2011,
    "authors": "Reinier H. van Leuken; Remco C. Veltkamp",
    "corresponding_authors": "",
    "abstract": "Indexing has become a key element in the pipeline of a multimedia retrieval system, due to continuous increases in database size, data complexity, and complexity of similarity measures. The primary goal of any indexing algorithm is to overcome high computational costs involved with comparing the query to every object in the database. This is achieved by efficient pruning in order to select only a small set of candidate matches. Vantage indexing is an indexing technique that belongs to the category of embedding or mapping approaches, because it maps a dissimilarity space onto a vector space such that traditional access methods can be used for querying. Each object is represented by a vector of dissimilarities to a small set of m reference objects, called vantage objects. Querying takes place within this vector space. The retrieval performance of a system based on this technique can be improved significantly through a proper choice of vantage objects. We propose a new technique for selecting vantage objects that addresses the retrieval performance directly, and present extensive experimental results based on three data sets of different size and modality, including a comparison with other selection strategies. The results clearly demonstrate both the efficacy and scalability of the proposed approach.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2621048125",
    "type": "article"
  },
  {
    "title": "A Generic Approach to Video Buffer Modeling Using Discrete-Time Analysis",
    "doi": "https://doi.org/10.1145/3183511",
    "publication_date": "2018-04-25",
    "publication_year": 2018,
    "authors": "Valentin Burger; Thomas Zinner; Lam Dinh-Xuan; Florian Wamser; Phuoc Tran‐Gia",
    "corresponding_authors": "",
    "abstract": "The large share of traffic in the Internet generated by video streaming services puts high loads on access and aggregation networks, resulting in high costs for the content delivery infrastructure. To reduce the bandwidth consumed while maintaining a high playback quality, video players use policies that control and limit the buffer level by using thresholds for pausing and continuing the video download. This allows shaping the bandwidth consumed by video streams and limiting the traffic wasted in case of playback abortion. Especially in mobile scenarios, where the throughput can be highly variant, the buffer policy can have a high impact on the probability of interruptions during video playback. To find the optimal setting for the buffer policy in each network condition, the relationship between the parameters of the buffer policy, the network throughput dynamics, and the corresponding video playback behavior needs to be understood. To this end, we model the video buffer as GI/GI/1 queue with pq -policy using discrete-time analysis. By studying the stochastic properties of the buffer-level distribution, we are able to accurately evaluate the impact of network and video bitrate dynamics on the video playback quality based on the buffer policy. We find a fundamental relationship between the bandwidth variation and the expected interarrival time of segments, meaning that overproportionately more bandwidth is necessary to prevent stalling events for high bandwidth variation. The proposed model further allows to optimize the trade-off between the traffic wasted in case of video abortion and video streaming quality experienced by the user.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2802889123",
    "type": "article"
  },
  {
    "title": "User-Click-Data-Based Fine-Grained Image Recognition via Weakly Supervised Metric Learning",
    "doi": "https://doi.org/10.1145/3209666",
    "publication_date": "2018-07-24",
    "publication_year": 2018,
    "authors": "Min Tan; Jun Yu; Yu Zhou; Fei Gao; Yong Rui; Dacheng Tao",
    "corresponding_authors": "",
    "abstract": "We present a novel fine-grained image recognition framework using user click data, which can bridge the semantic gap in distinguishing categories that are similar in visual. As query set in click data is usually large-scale and redundant, we first propose a click-feature-based query-merging approach to merge queries with similar semantics and construct a compact click feature. Afterward, we utilize this compact click feature and convolutional neural network (CNN)-based deep visual feature to jointly represent an image. Finally, with the combined feature, we employ the metriclearning-based template-matching scheme for efficient recognition. Considering the heavy noise in the training data, we introduce a reliability variable to characterize the image reliability, and propose a weakly-supervised metric and template leaning with smooth assumption and click prior (WMTLSC) method to jointly learn the distance metric, object templates, and image reliability. Extensive experiments are conducted on a public Clickture-Dog dataset and our newly established Clickture-Bird dataset. It is shown that the click-data-based query merging helps generating a highly compact (the dimension is reduced to 0.9%) and dense click feature for images, which greatly improves the computational efficiency. Also, introducing this click feature into CNN feature further boosts the recognition accuracy. The proposed framework performs much better than previous state-of-the-arts in fine-grained recognition tasks.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2883776044",
    "type": "article"
  },
  {
    "title": "Using Eye Tracking and Heart-Rate Activity to Examine Crossmodal Correspondences QoE in Mulsemedia",
    "doi": "https://doi.org/10.1145/3303080",
    "publication_date": "2019-05-31",
    "publication_year": 2019,
    "authors": "Gebremariam Mesfin; Nadia Hussain; Alexandra Covaci; Gheorghiță Ghinea",
    "corresponding_authors": "",
    "abstract": "Different senses provide us with information of various levels of precision and enable us to construct a more precise representation of the world. Rich multisensory simulations are thus beneficial for comprehension, memory reinforcement, or retention of information. Crossmodal mappings refer to the systematic associations often made between different sensory modalities (e.g., high pitch is matched with angular shapes) and govern multisensory processing. A great deal of research effort has been put into exploring cross-modal correspondences in the field of cognitive science. However, the possibilities they open in the digital world have been relatively unexplored. Multiple sensorial media (mulsemedia) provides a highly immersive experience to the users and enhances their Quality of Experience (QoE) in the digital world. Thus, we consider that studying the plasticity and the effects of cross-modal correspondences in a mulsemedia setup can bring interesting insights about improving the human computer dialogue and experience. In our experiments, we exposed users to videos with certain visual dimensions (brightness, color, and shape), and we investigated whether the pairing with a cross-modal matching sound (high and low pitch) and the corresponding auto-generated vibrotactile effects (produced by a haptic vest) lead to an enhanced QoE. For this, we captured the eye gaze and the heart rate of users while experiencing mulsemedia, and we asked them to fill in a set of questions targeting their enjoyment and perception at the end of the experiment. Results showed differences in eye-gaze patterns and heart rate between the experimental and the control group, indicating changes in participants’ engagement when videos were accompanied by matching cross-modal sounds (this effect was the strongest for the video displaying angular shapes and high-pitch audio) and transitively generated cross-modal vibrotactile effects.&lt;?vsp -1pt?&gt;",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2950754238",
    "type": "article"
  },
  {
    "title": "Content vs. Context",
    "doi": "https://doi.org/10.1145/2700287",
    "publication_date": "2015-02-05",
    "publication_year": 2015,
    "authors": "Yifang Yin; Beomjoo Seo; Roger Zimmermann",
    "corresponding_authors": "",
    "abstract": "Due to the ubiquity of sensor-equipped smartphones, it has become increasingly feasible for users to capture videos together with associated geographic metadata, for example the location and the orientation of the camera. Such contextual information creates new opportunities for the organization and retrieval of geo-referenced videos. In this study we explore the task of landmark retrieval through the analysis of two types of state-of-the-art techniques, namely media-content-based and geocontext-based retrievals. For the content-based method, we choose the Spatial Pyramid Matching (SPM) approach combined with two advanced coding methods: Sparse Coding (SC) and Locality-Constrained Linear Coding (LLC). For the geo-based method, we present the Geo Landmark Visibility Determination (GeoLVD) approach which computes the visibility of a landmark based on intersections of a camera's field-of-view (FOV) and the landmark's geometric information available from Geographic Information Systems (GIS) and services. We first compare the retrieval results of the two methods, and discuss the strengths and weaknesses of each approach in terms of precision, recall and execution time. Next we analyze the factors that affect the effectiveness for the content-based and the geo-based methods, respectively. Finally we propose a hybrid retrieval method based on the integration of the visual (content) and geographic (context) information, which is shown to achieve significant improvements in our experiments. We believe that the results and observations in this work will enlighten the design of future geo-referenced video retrieval systems, improve our understanding of selecting the most appropriate visual features for indexing and searching, and help in selecting between the most suitable methods for retrieval based on different conditions.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2001085473",
    "type": "article"
  },
  {
    "title": "DeepSearch",
    "doi": "https://doi.org/10.1145/3152127",
    "publication_date": "2017-12-13",
    "publication_year": 2017,
    "authors": "Peisong Wang; Qinghao Hu; Zhiwei Fang; Chaoyang Zhao; Jian Cheng",
    "corresponding_authors": "",
    "abstract": "Content-based image retrieval (CBIR) is one of the most important applications of computer vision. In recent years, there have been many important advances in the development of CBIR systems, especially Convolutional Neural Networks (CNNs) and other deep-learning techniques. On the other hand, current CNN-based CBIR systems suffer from high computational complexity of CNNs. This problem becomes more severe as mobile applications become more and more popular. The current practice is to deploy the entire CBIR systems on the server side while the client side only serves as an image provider. This architecture can increase the computational burden on the server side, which needs to process thousands of requests per second. Moreover, sending images have the potential of personal information leakage. As the need of mobile search expands, concerns about privacy are growing. In this article, we propose a fast image search framework, named DeepSearch, which makes complex image search based on CNNs feasible on mobile phones. To implement the huge computation of CNN models, we present a tensor Block Term Decomposition (BTD) approach as well as a nonlinear response reconstruction method to accelerate the CNNs involving in object detection and feature extraction. The extensive experiments on the ImageNet dataset and Alibaba Large-scale Image Search Challenge dataset show that the proposed accelerating approach BTD can significantly speed up the CNN models and further makes CNN-based image search practical on common smart phones.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2777402613",
    "type": "article"
  },
  {
    "title": "QoE-Aware OTT-ISP Collaboration in Service Management",
    "doi": "https://doi.org/10.1145/3183517",
    "publication_date": "2018-04-25",
    "publication_year": 2018,
    "authors": "Alessandro Floris; Arslan Ahmad; Luigi Atzori",
    "corresponding_authors": "",
    "abstract": "It is a matter of fact that quality of experience (QoE) has become one of the key factors determining whether a new multimedia service will be successfully accepted by the final users. Accordingly, several QoE models have been developed with the aim of capturing the perception of the user by considering as many influencing factors as possible. However, when it comes to adopting these models in the management of the services and networks, it frequently happens that no single provider has access to all of the tools to either measure all influencing factors parameters or control over the delivered quality. In particular, it often happens to the over-the-top (OTT) and Internet service providers (ISPs), which act with complementary roles in the service delivery over the Internet. On the basis of this consideration, in this article we first highlight the importance of a possible OTT-ISP collaboration for a joint service management in terms of technical and economic aspects. Then we propose a general reference architecture for a possible collaboration and information exchange among them. Finally, we define three different approaches, namely joint venture, customer lifetime value based, and QoE fairness based. The first aims to maximize the revenue by providing better QoE to customers paying more. The second aims to maximize the profit by providing better QoE to the most profitable customers (MPCs). The third aims to maximize QoE fairness among all customers. Finally, we conduct simulations to compare the three approaches in terms of QoE provided to the users, profit generated for the providers, and QoE fairness.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2802568342",
    "type": "article"
  },
  {
    "title": "Cloud Baking",
    "doi": "https://doi.org/10.1145/3206431",
    "publication_date": "2018-06-15",
    "publication_year": 2018,
    "authors": "Chang Liu; Wei Tsang Ooi; Jinyuan Jia; Lei Zhao",
    "corresponding_authors": "",
    "abstract": "We propose Cloud Baking, a collaborative rendering architecture for dynamic Web3D scenes. In our architecture, the cloud renderer renders the scene with the global illumination (GI) information in a GI map; the web-based client renderer renders the scene with ambient lighting only and blends it with the GI map received from the cloud for the final scene. This approach allows the users to interact with the web scene and change the scene dynamically through the web interface end, yet move the computationally heavy tasks of global illumination computation to the cloud. A challenge we face is the interaction delay that causes the frames rendered on the cloud and the client to go out of sync. We propose to use 3D warping and a hole-filling algorithm designed for GI map to predict the late GI map. We show both quantitatively and visually the quality of the GI map produced using our method. Our prediction algorithm allows us to further reduce the frequency at which the GI map is computed and sent from the server, reducing both computational needs and bandwidth usage.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2809603942",
    "type": "article"
  },
  {
    "title": "A Framework for Adaptive Residual Streaming for Single-Player Cloud Gaming",
    "doi": "https://doi.org/10.1145/3336498",
    "publication_date": "2019-04-30",
    "publication_year": 2019,
    "authors": "Deyu Chen; Magda El-Zarki",
    "corresponding_authors": "",
    "abstract": "Applying cloud technology to 3D interactive multimedia applications is a promising way to provide flexible and cost-efficient online high-bandwidth immersive services to a large population of end users. One main reason cloud systems are popular among users is the fact that it relaxes the hardware requirements for high-end interactive visual applications. As most of the computational tasks are done on cloud servers, users no longer need to upgrade their hardware as frequently to keep up with the ever-increasing high-end computing requirements of the latest applications. Moreover, cloud systems make it easier for a user to enjoy applications on different platforms, including mobile devices that are usually not powerful enough to run high-end, memory-intensive services. In short, applying cloud technology to high-end immersive applications has advantages in cost-efficiency and flexibility both for the end users and the service providers. However, there are two main drawbacks to applying cloud technology to 3D interactive multimedia services: (1) high-bandwidth utilization and (2) latency. In this article, we propose a framework that addresses the two problems for single-player cloud gaming by using a combination of collaborative rendering, progressive meshes, and 3D image warping techniques. The experimental results show that the proposed system can reduce the bandwidth usage and improve the visual quality by utilizing local computing power on the client. The results also show that the interaction latency can be reduced somewhat by sacrificing some degree of visual quality in the end system.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2964318714",
    "type": "article"
  },
  {
    "title": "Output-Bounded and RBFNN-Based Position Tracking and Adaptive Force Control for Security Tele-Surgery",
    "doi": "https://doi.org/10.1145/3394920",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "Ting Wang; Xiangjun Ji; Aiguo Song; Kurosh Madani; Amine Chohra; Huimin Lu; Ramon Monero",
    "corresponding_authors": "",
    "abstract": "In security e-health brain neurosurgery, one of the important processes is to move the electrocoagulation to the appropriate position in order to excavate the diseased tissue. 1 However, it has been problematic for surgeons to freely operate the electrocoagulation, as the workspace is very narrow in the brain. Due to the precision, vulnerability, and important function of brain tissues, it is essential to ensure the precision and safety of brain tissues surrounding the diseased part. The present study proposes the use of a robot-assisted tele-surgery system to accomplish the process. With the aim to achieve accuracy, an output-bounded and RBF neural network–based bilateral position control method was designed to guarantee the stability and accuracy of the operation process. For the purpose of accomplishing a minimal amount of bleeding and damage, an adaptive force control of the slave manipulator was proposed, allowing it to be appropriate to contact the susceptible vessels, nerves, and brain tissues. The stability was analyzed, and the numerical simulation results revealed the high performance of the proposed controls.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W3041570459",
    "type": "article"
  },
  {
    "title": "Sketch-guided Deep Portrait Generation",
    "doi": "https://doi.org/10.1145/3396237",
    "publication_date": "2020-07-05",
    "publication_year": 2020,
    "authors": "Trang-Thi Ho; John Jethro Virtusio; Yung-Yao Chen; Chih-Ming Hsu; Kai‐Lung Hua",
    "corresponding_authors": "",
    "abstract": "Generating a realistic human class image from a sketch is a unique and challenging problem considering that the human body has a complex structure that must be preserved. Additionally, input sketches often lack important details that are crucial in the generation process, hence making the problem more complicated. In this article, we present an effective method for synthesizing realistic images from human sketches. Our framework incorporates human poses corresponding to locations of key semantic components (e.g., arm, eyes, nose), seeing that its a strong prior for generating human class images. Our sketch-image synthesis framework consists of three stages: semantic keypoint extraction, coarse image generation, and image refinement. First, we extract the semantic keypoints using Part Affinity Fields (PAFs) and a convolutional autoencoder. Then, we integrate the sketch with semantic keypoints to generate a coarse image of a human. Finally, in the image refinement stage, the coarse image is enhanced by a Generative Adversarial Network (GAN) that adopts an architecture carefully designed to avoid checkerboard artifacts and to generate photo-realistic results. We evaluate our method on 6,300 sketch-image pairs and show that our proposed method generates realistic images and compares favorably against state-of-the-art image synthesis methods.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W3080441985",
    "type": "article"
  },
  {
    "title": "Fog-based Secure Service Discovery for Internet of Multimedia Things",
    "doi": "https://doi.org/10.1145/3415151",
    "publication_date": "2020-10-31",
    "publication_year": 2020,
    "authors": "Haoran Liang; Jun Wu; Xi Zheng; Mengshi Zhang; Jianhua Li; Alireza Jolfaei",
    "corresponding_authors": "",
    "abstract": "The Internet of Multimedia Things (IoMT) has become the backbone of innumerable multimedia applications in various fields. The wide application of IoMT not only makes our life convenient but also brings challenges to service discovery. Service discovery aims to leverage location information and trust evidence scattered in a variety of multimedia applications to find trusted IoMT devices that can provide specific service in target areas. However, the eavesdropping and tampering to these sensitive IoMT data during the trust propagation process invalidate the service discovery process. To address these challenges, we propose Secure Service Discovery (SSD) for IoMT using cross-blockchain-enabled fog computing. To resist the tampering and eavesdropping during the trust propagation process, a scalable cross-blockchain structure consisting of multiple parallel blockchains is first proposed based on fog, in which different parallel blockchains can be orchestrated to propagate encrypted location information and trust evidence of different applications. Moreover, to enable a cross-blockchain structure to leverage encrypted location information and trust evidence to find trusted IoMT devices in preset areas, a novel privacy-preserving range query is proposed to query and aggregate trust evidence. Security analysis and simulations are carried out to demonstrate the effectiveness and security of the proposed SSD.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W3112285133",
    "type": "article"
  },
  {
    "title": "Accessing Tape Music Documents on Mobile Devices",
    "doi": "https://doi.org/10.1145/2808200",
    "publication_date": "2015-10-21",
    "publication_year": 2015,
    "authors": "Sergio Canazza; Carlo Fantozzi; Niccolò Pretto",
    "corresponding_authors": "",
    "abstract": "The aim of this article is to present and discuss an innovative methodology aimed at accessing digitized copies of historical tape music audio documents; the methodology leverages on the multimedia and multisensory capabilities of mobile devices to provide an unprecedented level of fruition. In addition to the methodology, and stemming from it, we present an actual software application for Android tablet devices. This novel piece of software was designed and developed in a multidisciplinary team involving engineers as well as musicians, composers, and archivists. The strongest element in our work is the fact that it follows a rigorous process and it is based on the principles of philological awareness ; thus, it also takes into consideration the critical points in the musicologist's domain such as (i) the definition of preservation (i.e., master) copy, (ii) the importance of secondary information, (iii) the history of production and transmission of audio documents.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W1967261126",
    "type": "article"
  },
  {
    "title": "Launching an Efficient Participatory Sensing Campaign",
    "doi": "https://doi.org/10.1145/2808198",
    "publication_date": "2015-10-21",
    "publication_year": 2015,
    "authors": "Fei Hao; Mingjie Jiao; Geyong Min; Laurence T. Yang",
    "corresponding_authors": "",
    "abstract": "Participatory sensing is a promising sensing paradigm that enables collection, processing, dissemination and analysis of the phenomena of interest by ordinary citizens through their handheld sensing devices. Participatory sensing has huge potential in many applications, such as smart transportation and air quality monitoring. However, participants may submit low-quality, misleading, inaccurate, or even malicious data if a participatory sensing campaign is not launched effectively. Therefore, it has become a significant issue to establish an efficient participatory sensing campaign for improving the data quality. This article proposes a novel five-tier framework of participatory sensing and addresses several technical challenges in this proposed framework including: (1) optimized deployment of data collection points (DC-points); and (2) efficient recruitment strategy of participants. Toward this end, the deployment of DC-points is formulated as an optimization problem with maximum utilization of sensor and then a Wise-Dynamic DC-points Deployment (WD3) algorithm is designed for high-quality sensing. Furthermore, to guarantee the reliable sensing data collection and communication, a trajectory-based strategy for participant recruitment is proposed to enable campaign organizers to identify well-suited participants for data sensing based on a joint consideration of temporal availability, trust, and energy. Extensive experiments and performance analysis of the proposed framework and associated algorithms are conducted. The results demonstrate that the proposed algorithm can achieve a good sensing coverage with a smaller number of DC-points, and the participants that are termed as social sensors are easily selected, to evaluate the feasibility and extensibility of the proposed recruitment strategies.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2072600107",
    "type": "article"
  },
  {
    "title": "Field Effect Deep Networks for Image Recognition with Incomplete Data",
    "doi": "https://doi.org/10.1145/2957754",
    "publication_date": "2016-08-03",
    "publication_year": 2016,
    "authors": "Sheng-hua Zhong; Yan Liu; Kien A. Hua",
    "corresponding_authors": "",
    "abstract": "Image recognition with incomplete data is a well-known hard problem in computer vision and machine learning. This article proposes a novel deep learning technique called Field Effect Bilinear Deep Networks (FEBDN) for this problem. To address the difficulties of recognizing incomplete data, we design a novel second-order deep architecture with the Field Effect Restricted Boltzmann Machine, which models the reliability of the delivered information according to the availability of the features. Based on this new architecture, we propose a new three-stage learning procedure with field effect bilinear initialization, field effect abstraction and estimation, and global fine-tuning with missing features adjustment. By integrating the reliability of features into the new learning procedure, the proposed FEBDN can jointly determine the classification boundary and estimate the missing features. FEBDN has demonstrated impressive performance on recognition and estimation tasks in various standard datasets.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2476608201",
    "type": "article"
  },
  {
    "title": "AMIL",
    "doi": "https://doi.org/10.1145/3355612",
    "publication_date": "2020-01-31",
    "publication_year": 2020,
    "authors": "Pourya Shamsolmoali; Masoumeh Zareapoor; Huiyu Zhou; Jie Yang",
    "corresponding_authors": "",
    "abstract": "Human pose estimation has an important impact on a wide range of applications, from human-computer interface to surveillance and content-based video retrieval. For human pose estimation, joint obstructions and overlapping upon human bodies result in departed pose estimation. To address these problems, by integrating priors of the structure of human bodies, we present a novel structure-aware network to discreetly consider such priors during the training of the network. Typically, learning such constraints is a challenging task. Instead, we propose generative adversarial networks as our learning model in which we design two residual Multiple-Instance Learning (MIL) models with identical architecture—one is used as the generator, and the other one is used as the discriminator. The discriminator task is to distinguish the actual poses from the fake ones. If the pose generator generates results that the discriminator is not able to distinguish from the real ones, then the model has successfully learned the priors. In the proposed model, the discriminator differentiates the ground-truth heatmaps from the generated ones, and later the adversarial loss back-propagates to the generator. Such procedure assists the generator to learn reasonable body configurations and is proved to be advantageous to improve the pose estimation accuracy. Meanwhile, we propose a novel function for MIL. It is an adjustable structure for both instance selection and modeling to appropriately pass the information between instances in a single bag. In the proposed residual MIL neural network, the pooling action adequately updates the instance contribution to its bag. The proposed adversarial residual multi-instance neural network that is based on pooling has been validated on two datasets for the human pose estimation task and successfully outperforms the other state-of-the-art models. The code will be made available on https://github.com/pshams55/AMIL.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W3022162718",
    "type": "article"
  },
  {
    "title": "CovLets",
    "doi": "https://doi.org/10.1145/3357525",
    "publication_date": "2020-01-31",
    "publication_year": 2020,
    "authors": "Zhaoxin Zhang; Changyong Guo; Fanzhi Meng; Taizhong Xu; Junkai Huang",
    "corresponding_authors": "",
    "abstract": "State-of-the-art techniques for image and video classification take a bottom-up approach where local features are aggregated into a global final representation. Existing frameworks (i.e., bag of words or Fisher vectors) are specifically designed to aggregate vector-valued features such as SIFT descriptors. In this article, we propose a technique to aggregate local descriptors in the form of covariance descriptors (CovDs) into a rich descriptor, which in essence benefit from the second-order statistics along the coding pipeline. The difficulty in aggregating CovDs arises from the fact that CovDs lie on the Riemannian manifold of symmetric positive definite (SPD) matrices. Therefore, the aggregating scheme must take advantage of metrics and the geometry of the SPD manifolds. In our proposal, we make use of the Stein divergence and Nyström method to embed the SPD manifold into a Hilbert space. We compare our proposal, dubbed CovLets, against state-of-the-art methods on several image and video classification problems including facial expression recognition and action recognition.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W3023245712",
    "type": "article"
  },
  {
    "title": "Performance Analysis of ACTE",
    "doi": "https://doi.org/10.1145/3387921",
    "publication_date": "2020-04-30",
    "publication_year": 2020,
    "authors": "Abdelhak Bentaleb; Christian Timmerer; Ali C. Begen; Roger Zimmermann",
    "corresponding_authors": "",
    "abstract": "HTTP adaptive streaming with chunked transfer encoding can offer low-latency streaming without sacrificing the coding efficiency. This allows media segments to be delivered while still being packaged. However, conventional schemes often make widely inaccurate bandwidth measurements due to the presence of idle periods between the chunks and hence this is causing sub-optimal adaptation decisions. To address this issue, we earlier proposed ACTE (ABR for Chunked Transfer Encoding) [6], a bandwidth prediction scheme for low-latency chunked streaming. While ACTE was a significant step forward, in this study we focus on two still remaining open areas, namely, (i) quantifying the impact of encoding parameters, including chunk and segment durations, bitrate levels, minimum interval between IDR-frames and frame rate on ACTE, and (ii) exploring the impact of video content complexity on ACTE. We thoroughly investigate these questions and report on our findings. We also discuss some additional issues that arise in the context of pursuing very low latency HTTP video streaming.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W3033886763",
    "type": "article"
  },
  {
    "title": "Dynamic Graph Learning Convolutional Networks for Semi-supervised Classification",
    "doi": "https://doi.org/10.1145/3412846",
    "publication_date": "2021-01-31",
    "publication_year": 2021,
    "authors": "Sichao Fu; Weifeng Liu; Weili Guan; Yicong Zhou; Dapeng Tao; Changsheng Xu",
    "corresponding_authors": "",
    "abstract": "Over the past few years, graph representation learning (GRL) has received widespread attention on the feature representations of the non-Euclidean data. As a typical model of GRL, graph convolutional networks (GCN) fuse the graph Laplacian-based static sample structural information. GCN thus generalizes convolutional neural networks to acquire the sample representations with the variously high-order structures. However, most of existing GCN-based variants depend on the static data structural relationships. It will result in the extracted data features lacking of representativeness during the convolution process. To solve this problem, dynamic graph learning convolutional networks (DGLCN) on the application of semi-supervised classification are proposed. First, we introduce a definition of dynamic spectral graph convolution operation. It constantly optimizes the high-order structural relationships between data points according to the loss values of the loss function, and then fits the local geometry information of data exactly. After optimizing our proposed definition with the one-order Chebyshev polynomial, we can obtain a single-layer convolution rule of DGLCN. Due to the fusion of the optimized structural information in the learning process, multi-layer DGLCN can extract richer sample features to improve classification performance. Substantial experiments are conducted on citation network datasets to prove the effectiveness of DGLCN. Experiment results demonstrate that the proposed DGLCN obtains a superior classification performance compared to several existing semi-supervised classification models.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W3141262995",
    "type": "article"
  },
  {
    "title": "Alignment Enhancement Network for Fine-grained Visual Categorization",
    "doi": "https://doi.org/10.1145/3446208",
    "publication_date": "2021-01-31",
    "publication_year": 2021,
    "authors": "Yutao Hu; Xuhui Liu; Baochang Zhang; Jungong Han; Xianbin Cao",
    "corresponding_authors": "",
    "abstract": "Fine-grained visual categorization (FGVC) aims to automatically recognize objects from different sub-ordinate categories. Despite attracting considerable attention from both academia and industry, it remains a challenging task due to subtle visual differences among different classes. Cross-layer feature aggregation and cross-image pairwise learning become prevailing in improving the performance of FGVC by extracting discriminative class-specific features. However, they are still inefficient to fully use the cross-layer information based on the simple aggregation strategy, while existing pairwise learning methods also fail to explore long-range interactions between different images. To address these problems, we propose a novel Alignment Enhancement Network (AENet), including two-level alignments, Cross-layer Alignment (CLA) and Cross-image Alignment (CIA). The CLA module exploits the cross-layer relationship between low-level spatial information and high-level semantic information, which contributes to cross-layer feature aggregation to improve the capacity of feature representation for input images. The new CIA module is further introduced to produce the aligned feature map, which can enhance the relevant information as well as suppress the irrelevant information across the whole spatial region. Our method is based on an underlying assumption that the aligned feature map should be closer to the inputs of CIA when they belong to the same category. Accordingly, we establish Semantic Affinity Loss to supervise the feature alignment within each CIA block. Experimental results on four challenging datasets show that the proposed AENet achieves the state-of-the-art results over prior arts.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W3151762391",
    "type": "article"
  },
  {
    "title": "WTRPNet: An Explainable Graph Feature Convolutional Neural Network for Epileptic EEG Classification",
    "doi": "https://doi.org/10.1145/3460522",
    "publication_date": "2021-10-31",
    "publication_year": 2021,
    "authors": "Xin Qi; Shaohao Hu; Shuaiqi Liu; Ling Zhao; Shuihua Wang",
    "corresponding_authors": "",
    "abstract": "As one of the important tools of epilepsy diagnosis, the electroencephalogram (EEG) is noninvasive and presents no traumatic injury to patients. It contains a lot of physiological and pathological information that is easy to obtain. The automatic classification of epileptic EEG is important in the diagnosis and therapeutic efficacy of epileptics. In this article, an explainable graph feature convolutional neural network named WTRPNet is proposed for epileptic EEG classification. Since WTRPNet is constructed by a recurrence plot in the wavelet domain, it can fully obtain the graph feature of the EEG signal, which is established by an explainable graph features extracted layer called WTRP block . The proposed method shows superior performance over state-of-the-art methods. Experimental results show that our algorithm has achieved an accuracy of 99.67% in classification of focal and nonfocal epileptic EEG, which proves the effectiveness of the classification and detection of epileptic EEG.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4200474188",
    "type": "article"
  },
  {
    "title": "Synergy between Semantic Segmentation and Image Denoising via Alternate Boosting",
    "doi": "https://doi.org/10.1145/3548459",
    "publication_date": "2022-07-14",
    "publication_year": 2022,
    "authors": "Shunxin Xu; Ke Sun; Dong Liu; Zhiwei Xiong; Zheng-Jun Zha",
    "corresponding_authors": "",
    "abstract": "The capability of image semantic segmentation may be deteriorated due to the noisy input image, where image denoising prior to segmentation may help. Both image denoising and semantic segmentation have been developed significantly with the advance of deep learning. In this work, we are interested in the synergy between these two tasks by using a holistic deep model. We observe that not only denoising helps combat the drop of segmentation accuracy due to the noisy input, but also pixel-wise semantic information boosts the capability of denoising. We then propose a boosting network to perform denoising and segmentation alternately. The proposed network is composed of multiple segmentation and denoising blocks (SDBs), each of which estimates a semantic map and then uses the map to regularize denoising. Experimental results show that the denoised image quality is improved substantially and the segmentation accuracy is improved to close to that on clean images, and segmentation and denoising are both boosted as the number of SDBs increases. On the Cityscapes dataset, using three SDBs improves the denoising quality to 34.42 dB in PSNR, and the segmentation accuracy to 66.5 in mIoU, when the additive white Gaussian noise level is 50.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W3133004360",
    "type": "article"
  },
  {
    "title": "Mask or Non-Mask? Robust Face Mask Detector via Triplet-Consistency Representation Learning",
    "doi": "https://doi.org/10.1145/3472623",
    "publication_date": "2022-01-25",
    "publication_year": 2022,
    "authors": "Chun‐Wei Yang; Thanh Hai Phung; Hong-Han Shuai; Wen-Huang Cheng",
    "corresponding_authors": "",
    "abstract": "In the absence of vaccines or medicines to stop COVID-19, one of the effective methods to slow the spread of the coronavirus and reduce the overloading of healthcare is to wear a face mask. Nevertheless, to mandate the use of face masks or coverings in public areas, additional human resources are required, which is tedious and attention-intensive. To automate the monitoring process, one of the promising solutions is to leverage existing object detection models to detect the faces with or without masks. As such, security officers do not have to stare at the monitoring devices or crowds, and only have to deal with the alerts triggered by the detection of faces without masks. Existing object detection models usually focus on designing the CNN-based network architectures for extracting discriminative features. However, the size of training datasets of face mask detection is small, while the difference between faces with and without masks is subtle. Therefore, in this article, we propose a face mask detection framework that uses the context attention module to enable the effective attention of the feed-forward convolution neural network by adapting their attention maps’ feature refinement. Moreover, we further propose an anchor-free detector with Triplet-Consistency Representation Learning by integrating the consistency loss and the triplet loss to deal with the small-scale training data and the similarity between masks and occlusions. Extensive experimental results show that our method outperforms the other state-of-the-art methods. The source code is released as a public download to improve public health at https://github.com/wei-1006/MaskFaceDetection .",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W3202299883",
    "type": "article"
  },
  {
    "title": "Instance Correlation Graph for Unsupervised Domain Adaptation",
    "doi": "https://doi.org/10.1145/3486251",
    "publication_date": "2022-01-25",
    "publication_year": 2022,
    "authors": "Lei Wu; Hefei Ling; Yuxuan Shi; Baiyan Zhang",
    "corresponding_authors": "",
    "abstract": "In recent years, deep neural networks have emerged as a dominant machine learning tool for a wide variety of application fields. Due to the expensive cost of manual labeling efforts, it is important to transfer knowledge from a label-rich source domain to an unlabeled target domain. The core problem is how to learn a domain-invariant representation to address the domain shift challenge, in which the training and test samples come from different distributions. First, considering the geometry of space probability distributions, we introduce an effective Hellinger Distance to match the source and target distributions on statistical manifold. Second, the data samples are not isolated individuals, and they are interrelated. The correlation information of data samples should not be neglected for domain adaptation. Distinguished from previous works, we pay attention to the correlation distributions over data samples. We design elaborately a Residual Graph Convolutional Network to construct the Instance Correlation Graph (ICG). The correlation information of data samples is exploited to reduce the domain shift. Therefore, a novel Instance Correlation Graph for Unsupervised Domain Adaptation is proposed, which is trained end-to-end by jointly optimizing three types of losses, i.e., Supervised Classification loss for source domain, Centroid Alignment loss to measure the centroid difference between source and target domain, ICG Alignment loss to match Instance Correlation Graph over two related domains. Extensive experiments are conducted on several hard transfer tasks to learn domain-invariant representations on three benchmarks: Office-31, Office-Home, and VisDA2017. Compared with other state-of-the-art techniques, our method achieves superior performance.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4206967050",
    "type": "article"
  },
  {
    "title": "TT-TSVD: A Multi-modal Tensor Train Decomposition with Its Application in Convolutional Neural Networks for Smart Healthcare",
    "doi": "https://doi.org/10.1145/3491223",
    "publication_date": "2022-01-25",
    "publication_year": 2022,
    "authors": "Debin Liu; Laurence T. Yang; Puming Wang; Ruonan Zhao; Qingchen Zhang",
    "corresponding_authors": "",
    "abstract": "Smart healthcare systems are generating a large scale of heterogenous high-dimensional data with complex relationships. It is hard for current methods to analyze such high-dimensional healthcare data. Specifically, the traditional data reduction methods can not keep the correlation among different modalities of data objects, while the latest methods based on tensor singular value decomposition are not effective for data reduction, although they can keep the correlation. This article presents a tensor train-tensor singular value decomposition (TT-TSVD) algorithm for data reduction. Particularly, the presented algorithm balances the correlation-preservation ability of modalities and data reduction ability by combining the advantages of the train structure of the tensor train decomposition and the association relationship between the tensor singular value decomposition retention mode. Extensive experiments are conducted on the convolutional neural network and the results clearly show that the presented algorithm performs effectively for data reduction with a low-loss classification accuracy; what is more, classification accuracy on medical image dataset has been improved a little.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4207006620",
    "type": "article"
  },
  {
    "title": "SDCN2: A Shallow Densely Connected CNN for Multi-Purpose Image Manipulation Detection",
    "doi": "https://doi.org/10.1145/3510462",
    "publication_date": "2022-03-16",
    "publication_year": 2022,
    "authors": "Gurinder Singh; Puneet Goyal",
    "corresponding_authors": "",
    "abstract": "Digital image information can be easily tampered with to harm the integrity of someone. Thus, recognizing the truthfulness and processing history of an image is one of the essential concerns in multimedia forensics. Numerous forensic methods have been developed by researchers with the ability to detect targeted editing operations. However, creating a unified forensic approach capable of detecting multiple image manipulations remains a challenging problem. In this article, a new general-purpose forensic approach is designed based on a shallow densely connected convolutional neural network (SDCN2) that exploits local dense connections and global residual learning. The residual domain is considered in the proposed network rather than the spatial domain to analyze the image manipulation artifacts because the residual domain is less dependent on image content information. To attain this purpose, a residual convolutional layer is employed at the beginning of the proposed model to adaptively learn the image manipulation features by suppressing the image content information. Then, the obtained image residuals or prediction error features are further processed by the shallow densely connected convolutional neural network for high-level feature extraction. In addition, the hierarchical features produced by the densely connected blocks and prediction error features are fused globally for better information flow across the network. The extensive experiment results show that the proposed scheme outperforms the existing state-of-the-art general-purpose forensic schemes even under anti-forensic attacks, when tested on large-scale datasets. The proposed model offers overall detection accuracies of 98.34% and 99.22% for BOSSBase and Dresden datasets, respectively, for multiple image manipulation detection. Moreover, the proposed network is highly efficient in terms of computational complexity as compared to the existing approaches.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4220827504",
    "type": "article"
  },
  {
    "title": "Category-Stitch Learning for Union Domain Generalization",
    "doi": "https://doi.org/10.1145/3524136",
    "publication_date": "2022-03-17",
    "publication_year": 2022,
    "authors": "Yajing Liu; Zhiwei Xiong; Ya Li; Yuning Lu; Xinmei Tian; Zheng-Jun Zha",
    "corresponding_authors": "",
    "abstract": "Domain generalization aims at generalizing the network trained on multiple domains to unknown but related domains. Under the assumption that different domains share the same classes, previous works can build relationships across domains. However, in realistic scenarios, the change of domains is always followed by the change of categories, which raises a difficulty for collecting sufficient aligned categories across domains. Bearing this in mind, this article introduces union domain generalization (UDG) as a new domain generalization scenario, in which the label space varies across domains, and the categories in unknown domains belong to the union of all given domain categories. The absence of categories in given domains is the main obstacle to aligning different domain distributions and obtaining domain-invariant information. To address this problem, we propose category-stitch learning (CSL), which aims at jointly learning the domain-invariant information and completing missing categories in all domains through an improved variational autoencoder and generators. The domain-invariant information extraction and sample generation cross-promote each other to better generalizability. Additionally, we decouple category and domain information and propose explicitly regularizing the semantic information by the classification loss with transferred samples. Thus our method can breakthrough the category limit and generate samples of missing categories in each domain. Extensive experiments and visualizations are conducted on MNIST, VLCS, PACS, Office-Home, and DomainNet datasets to demonstrate the effectiveness of our proposed method.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4220834004",
    "type": "article"
  },
  {
    "title": "Toward High-quality Face-Mask Occluded Restoration",
    "doi": "https://doi.org/10.1145/3524137",
    "publication_date": "2022-03-25",
    "publication_year": 2022,
    "authors": "Feihong Lu; Hang Chen; Kang Li; Deng Qiliang; Jian Zhao; Kaipeng Zhang; Hong Han",
    "corresponding_authors": "",
    "abstract": "Face-mask occluded restoration aims at restoring the masked region of a human face, which has attracted increasing attention in the context of the COVID-19 pandemic. One major challenge of this task is the large visual variance of masks in the real world. To solve it we first construct a large-scale Face-mask Occluded Restoration (FMOR) dataset, which contains 5,500 unmasked images and 5,500 face-mask occluded images with various illuminations, and involves 1,100 subjects of different races, face orientations, and mask types. Moreover, we propose a Face-Mask Occluded Detection and Restoration (FMODR) framework, which can detect face-mask regions with large visual variations and restore them to realistic human faces. In particular, our FMODR contains a self-adaptive contextual attention module specifically designed for this task, which is able to exploit the contextual information and correlations of adjacent pixels for achieving high realism of the restored faces, which are however often neglected in existing contextual attention models. Our framework achieves state-of-the-art results of face restoration on three datasets, including CelebA, AR, and our FMOR datasets. Moreover, experimental results on AR and FMOR datasets demonstrate that our framework can significantly improve masked face recognition and verification performance.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4221049315",
    "type": "article"
  },
  {
    "title": "Using Four Hypothesis Probability Estimators for CABAC in Versatile Video Coding",
    "doi": "https://doi.org/10.1145/3531015",
    "publication_date": "2022-05-12",
    "publication_year": 2022,
    "authors": "Ka‐Hou Chan; Sio‐Kei Im",
    "corresponding_authors": "",
    "abstract": "This article introduces the key technologies involved in four hypothetical probability estimators for Context-based Adaptive Binary Arithmetic Coding (CABAC). The focus is on the selected adaptation rate performed in these estimators, which are selected based on coding efficiency and memory considerations, and also the relationship with the current size of the coding block. The proposed scheme can linearly realize the quantitative representation of probabilistic prediction and describes the scalability potential for higher accuracy. Besides a description of the design concept, this work also discusses motivation and implementation aspects, which are based on simple operations such as bitwise operations and single subsampling for subinterval updates. The experimental results verify the effectiveness of the proposed CABAC method specified in Versatile Video Coding (VVC).",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4280564083",
    "type": "article"
  },
  {
    "title": "Weakly Supervised Text-based Actor-Action Video Segmentation by Clip-level Multi-instance Learning",
    "doi": "https://doi.org/10.1145/3514250",
    "publication_date": "2022-07-18",
    "publication_year": 2022,
    "authors": "Weidong Chen; Guorong Li; Xinfeng Zhang; Shuhui Wang; Liang Li; Qingming Huang",
    "corresponding_authors": "",
    "abstract": "In real-world scenarios, it is common that a video contains multiple actors and their activities. Selectively localizing one specific actor and its action spatially and temporally via a language query becomes a vital and challenging task. Existing fully supervised methods require extensive elaborately annotated data and are sensitive to the class labels, which cannot satisfy real-world applications’ needs. Thus, we introduce the task of weakly supervised actor-action video segmentation from a sentence query (AAVSS) in this work, where only the video-sentence pairs are provided. To the best of our knowledge, our work is the first to perform AAVSS under weakly supervised situations. However, this task is extremely challenging not only because the task aims to learn the complex interactions between two heterogeneous modalities but also because the task needs to learn fine-grained analysis of video content without pixel-level annotations. To overcome the challenges, we propose a two-stage network. The network first follows the sentence guidance to localize the candidate region and then performs segmentation to achieve selective segmentation. Specifically, a novel tracker-based clip-level multiple instance learning paradigm is proposed in this article to learn the matches between regions and sentences, which makes our two-stage network robust to the region proposal network. Furthermore, two intrinsic characteristics of the video, temporal consistency and motion information, are utilized in companion with the weak supervision to facilitate the region-query matching. Through extensive experiments, the proposed method achieves comparable performance to state-of-the-art fully supervised approaches on two large-scale benchmarks, including A2D Sentences and J-HMDB Sentences.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4285729260",
    "type": "article"
  },
  {
    "title": "Semantic Embedding Guided Attention with Explicit Visual Feature Fusion for Video Captioning",
    "doi": "https://doi.org/10.1145/3550276",
    "publication_date": "2022-07-22",
    "publication_year": 2022,
    "authors": "Shan-Shan Dong; Tian-Zi Niu; Xin Luo; Wu Liu; Xin-Shun Xu",
    "corresponding_authors": "",
    "abstract": "Video captioning, which bridges vision and language, is a fundamental yet challenging task in computer vision. To generate accurate and comprehensive sentences, both visual and semantic information is quite important. However, most existing methods simply concatenate different types of features and ignore the interactions between them. In addition, there is a large semantic gap between visual feature space and semantic embedding space, making the task very challenging. To address these issues, we propose a framework named semantic embedding guided attention with Explicit visual Feature Fusion for vidEo CapTioning, EFFECT for short, in which we design an explicit visual-feature fusion (EVF) scheme to capture the pairwise interactions between multiple visual modalities and fuse multimodal visual features of videos in an explicit way. Furthermore, we propose a novel attention mechanism called semantic embedding guided attention (SEGA ), which cooperates with the temporal attention to generate a joint attention map. Specifically, in SEGA, the semantic word embedding information is leveraged to guide the model to pay more attention to the most correlated visual features at each decoding stage. In this way, the semantic gap between visual and semantic space is alleviated to some extent. To evaluate the proposed model, we conduct extensive experiments on two widely used datasets, i.e., MSVD and MSR-VTT. The experimental results demonstrate that our approach achieves state-of-the-art results in terms of four evaluation metrics.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4286488082",
    "type": "article"
  },
  {
    "title": "Efficient Light Field Image Compression with Enhanced Random Access",
    "doi": "https://doi.org/10.1145/3471905",
    "publication_date": "2022-03-23",
    "publication_year": 2022,
    "authors": "Hadi Amirpour; António Pinheiro; Manuela Pereira; Fernando Lopes; M. Ghanbari",
    "corresponding_authors": "",
    "abstract": "In light field image compression, facilitating random access to individual views plays a significant role in decoding views quickly, reducing memory footprint, and decreasing the bandwidth requirement for transmission. Highly efficient light field image compression methods mainly use inter view prediction. Therefore, they typically do not provide random access to individual views. On the other hand, methods that provide full random access usually reduce compression efficiency. To address this trade-off, a light field image encoding method that favors random access is proposed in this paper. Light field image views are grouped into independent (3× 3) views, which are called Macro View Images (MVIs) . To encode MVIs, the central view is used as a reference to compress its adjacent neighboring views using a hierarchical reference structure. To encode the central view of each MVI, the most central view along with the center of a maximum of three MVIs, are used as reference images for the disparity estimation. In addition, the proposed method allows the use of parallel processing to reduce the maximum encoding/decoding time-complexity in multi-core processors. Tile partitioning can also be used to randomly access different regions of the light field images. The simulation results show that the proposed method outperforms other state-of-the-art methods in terms of compression efficiency while providing random access to both views and regions of interest.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4293180450",
    "type": "article"
  },
  {
    "title": "Cross-scale Graph Interaction Network for Semantic Segmentation of Remote Sensing Images",
    "doi": "https://doi.org/10.1145/3558770",
    "publication_date": "2022-08-29",
    "publication_year": 2022,
    "authors": "Jie Nie; Lei Huang; Chengyu Zheng; Xiaowei Lv; Rui Wang",
    "corresponding_authors": "",
    "abstract": "Semantic segmentation of remote sensing (RS) images plays a vital role in a variety of fields, including urban planning, natural disaster monitoring, and land resource management. Due to the complexity and low resolution of RS images, many approaches have been proposed to handle the related task. However, these previously developed approaches dedicate to contextual interaction but ignore the cross-scale semantic correlation and multi-scale boundary information. Therefore, we propose a Cross-scale Graph Interaction Network (CGIN) to address semantic segmentation problems of RS images, which consists of a semantic branch and a boundary branch. In the semantic branch, we first apply atrous convolution to extract multi-scale semantic features of RS images. Particularly, based on the multi-scale semantic features, a Cross-scale Graph Interaction (CGI) module is introduced, which establishes cross-scale graph structures and performs adaptive graph reasoning to capture the cross-scale semantic correlation of RS objects. In the boundary branch, we propose a Multi-scale Boundary Feature Extraction (MBFE) module that utilizes atrous convolutions with different dilation rates to extract multi-scale boundary features. Finally, to address the problem of sparse boundary pixels in the fusion process of the two branches, we propose a Multi-scale Similarity-guided Aggregation (MSA) module by calculating the similarity of semantic features and boundary features at the corresponding scale, which can emphasize the boundary information in semantic features. Our proposed CGIN outperforms state-of-the-art approaches in numerical experiments conducted on two benchmark remote sensing datasets.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4293452808",
    "type": "article"
  },
  {
    "title": "A Closer Look at Debiased Temporal Sentence Grounding in Videos: Dataset, Metric, and Approach",
    "doi": "https://doi.org/10.1145/3565573",
    "publication_date": "2022-10-07",
    "publication_year": 2022,
    "authors": "Xiaohan Lan; Yitian Yuan; Xin Wang; Long Chen; Zhi Wang; Lin Ma; Wenwu Zhu",
    "corresponding_authors": "",
    "abstract": "Temporal Sentence Grounding in Videos (TSGV) , which aims to ground a natural language sentence that indicates complex human activities in an untrimmed video, has drawn widespread attention over the past few years. However, recent studies have found that current benchmark datasets may have obvious moment annotation biases, enabling several simple baselines even without training to achieve state-of-the-art (SOTA) performance. In this paper, we take a closer look at existing evaluation protocols for TSGV, and find that both the prevailing dataset splits and evaluation metrics are the devils that lead to untrustworthy benchmarking. Therefore, we propose to re-organize the two widely-used datasets, making the ground-truth moment distributions different in the training and test splits, i.e., out-of-distribution (OOD) test. Meanwhile, we introduce a new evaluation metric “dR@ n ,IoU= m ” that discounts the basic recall scores especially with small IoU thresholds, so as to alleviate the inflating evaluation caused by biased datasets with a large proportion of long ground-truth moments. New benchmarking results indicate that our proposed evaluation protocols can better monitor the research progress in TSGV. Furthermore, we propose a novel causality-based Multi-branch Deconfounding Debiasing (MDD) framework for unbiased moment prediction. Specifically, we design a multi-branch deconfounder to eliminate the effects caused by multiple confounders with causal intervention. In order to help the model better align the semantics between sentence queries and video moments, we enhance the representations during feature encoding. Specifically, for textual information, the query is parsed into several verb-centered phrases to obtain a more fine-grained textual feature. For visual information, the positional information has been decomposed from the moment features to enhance the representations of moments with diverse locations. Extensive experiments demonstrate that our proposed approach can achieve competitive results among existing SOTA approaches and outperform the base model with great gains.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4303427395",
    "type": "article"
  },
  {
    "title": "PEDM: A Multi-task Learning Model for Persona-aware Emoji-embedded Dialogue Generation",
    "doi": "https://doi.org/10.1145/3571819",
    "publication_date": "2022-11-22",
    "publication_year": 2022,
    "authors": "Sirui Zhao; Hongyu Jiang; Hanqing Tao; Rui Zha; Kun Zhang; Tong Xu; Enhong Chen",
    "corresponding_authors": "",
    "abstract": "As a vivid and linguistic symbol, Emojis have become a prevailing medium interspersed in text-based communication (e.g., social media and chit-chat) to express emotions, attitudes, and situations. Generally speaking, a social-oriented chatbot that can generate appropriate Emoji-embedded responses would be much more competitive, making communications more fun, engaging, and human-like. However, the current Emoji-related research is still in its infancy, leading to an awkward situation of data deficiency. How to develop an Emoji-embedded dialogue system while addressing the lack of data will be interesting and meaningful for the application of future AI. To bridge this gap, we propose a multi-task learning method for persona-aware Emoji-embedded dialogue generation in this article. Specifically, as the benchmark of model training and evaluation, which includes 1.2 million Emoji-embedded tweets and 1.1 million post-response pairs, we first construct a dataset named EmojiTweet to handle the data deficiency problem. Then, a Seq2Seq-based model with multi-task learning is designed to simultaneously learn response generation and Emoji embedding from the constructed non-Emoji dialogue and Emoji-embedded monologue data. Afterward, we incorporate persona factors into our model by adopting persona fusion and personalized bias methods to deliver personalized dialogues with more accurately selected Emojis. Finally, we conduct extensive experiments, where the experimental results and evaluations demonstrate that our model has three key benefits: improved dialogue quality, higher user engagement, and not relying on large-scale Emoji-embedded dialogue data representing specific personas. EmojiTweet will be published publicly via https://mea-lab-421.github.io/EmojiTweet/ .",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4309695664",
    "type": "article"
  },
  {
    "title": "High-Fidelity Face Reenactment Via Identity-Matched Correspondence Learning",
    "doi": "https://doi.org/10.1145/3571857",
    "publication_date": "2022-11-23",
    "publication_year": 2022,
    "authors": "Han Xue; Jun Ling; Anni Tang; Li Song; Rong Xie; Wenjun Zhang",
    "corresponding_authors": "",
    "abstract": "Face reenactment aims to generate an animation of a source face using the poses and expressions from a target face. Although recent methods have made remarkable progress by exploiting generative adversarial networks, they are limited in generating high-fidelity and identity-preserving results due to the inappropriate driving information and insufficiently effective animating strategies. In this work, we propose a novel face reenactment framework that achieves both high-fidelity generation and identity preservation. Instead of sparse face representations (e.g., facial landmarks and keypoints), we utilize the Projected Normalized Coordinate Code (PNCC) to better preserve facial details. We propose to reconstruct the PNCC with the source identity parameters and the target pose and expression parameters estimated by 3D face reconstruction to factor out the target identity. By adopting the reconstructed representation as the driving information, we address the problem of identity mismatch. To effectively utilize the driving information, we establish the correspondence between the reconstructed representation and the source representation based on the features extracted by an encoder network. This identity-matched correspondence is then utilized to animate the source face using a novel feature transformation strategy. The generator network is further enhanced by the proposed geometry-aware skip connection. Once trained, our model can be applied to previously unseen faces without further training or fine-tuning. Through extensive experiments, we demonstrate the effectiveness of our method in face reenactment and show that our model outperforms state-of-the-art approaches both qualitatively and quantitatively. Additionally, the proposed PNCC reconstruction module can be easily inserted into other methods and improve their performance in cross-identity face reenactment.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4309764400",
    "type": "article"
  },
  {
    "title": "Self-supervised Image-based 3D Model Retrieval",
    "doi": "https://doi.org/10.1145/3548690",
    "publication_date": "2023-01-18",
    "publication_year": 2023,
    "authors": "Dan Song; Chumeng Zhang; Xiaoqian Zhao; Teng Wang; Weizhi Nie; Xuanya Li; An-An Liu",
    "corresponding_authors": "",
    "abstract": "Image-based 3D model retrieval aims at organizing unlabeled 3D models according to the relevance to the labeled 2D images. With easy accessibility of 2D images and wide applications of 3D models, image-based 3D model retrieval attracts more and more attentions. However, it is still a challenging problem due to the modality gap between 2D images and 3D models. In spite of the remarkable progress brought by domain adaptation techniques for this research topic, which usually propose to align the global distribution statistics of two domains, these methods are limited in learning discriminative features for target samples due to the lack of label information in target domain. In this article, besides utilizing the label information of 2D image domain and the adversarial domain alignment, we additionally incorporate self-supervision to address cross-domain 3D model retrieval problem. Specifically, we simultaneously optimize the adversarial adaptation for both domains based on visual features and the contrastive learning for unlabeled 3D model domain to help the feature extractor to learn discriminative feature representations. The contrastive learning is used to map view representations of the identical model nearby while view representations of different models far apart. To guarantee adequate and high-quality negative samples for contrastive learning, we design a memory bank to store and update representative view for each 3D model based on entropy minimization principle. Comprehensive experimental results on the public image-based 3D model retrieval datasets, i.e., MI3DOR and MI3DOR-2, have demonstrated the effectiveness of the proposed method.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4317209717",
    "type": "article"
  },
  {
    "title": "Design Principles for Content Creation in Location-Based Games",
    "doi": "https://doi.org/10.1145/3583689",
    "publication_date": "2023-02-13",
    "publication_year": 2023,
    "authors": "Pasi Fränti; Nancy Fazal",
    "corresponding_authors": "",
    "abstract": "Location-based games have been around since 2000 across various fields, including education, health, and entertainment. The main challenge facing such games is content generation. In contrast to normal games, content in location-based games is inherently dependent on location. The biggest challenge is the availability of the content globally. Other challenges include player engagement, enjoyable interactions with the real-world environment, safety, and customizability based on player performance and preference. While crowdsourcing has often been adopted as a tool for content creation, this approach requires quality control. Designing high-quality content requires detailed guidelines. In this paper, we introduce design principles for the creation of high-quality content that can survive for long periods of time. These principles are derived from ten years of experience running our in-house orienteering treasure-hunt game called O-Mopsi , which represents a case study in this paper. O-Mopsi allows players to visit pre-defined locations. The design principles are expected to be generalizable to other location-based games as well as to the creation of sightseeing tours more generally.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4320490874",
    "type": "article"
  },
  {
    "title": "Learning Disentangled Features for Person Re-identification under Clothes Changing",
    "doi": "https://doi.org/10.1145/3584359",
    "publication_date": "2023-02-16",
    "publication_year": 2023,
    "authors": "Patrick P. K. Chan; Xiaoman Hu; Haorui Song; Peng Peng; Keke Chen",
    "corresponding_authors": "",
    "abstract": "Clothes changing is one of the challenges in person re-identification (ReID), since clothes provide remarkable and reliable information for decision, especially when the resolution of an image is low. Variation of clothes significantly downgrades standard ReID models, since the clothes information dominates the decisions. The performance of the existing methods considering clothes changing is still not satisfying, since they fail to extract sufficient identity information that excludes clothes information. This study aims to disentangle identity, clothes, and unrelated features with a Generative Adversarial Network (GAN). A GAN model with three encoders, one generator, and three discriminators, and its training procedure are proposed to learn these kinds of features separately and exclusively. Experimental results indicate that our model generally achieves the best performance among state-of-the-art methods in both ReID tasks with and without clothes changing, which confirms that the identity, clothes, and unrelated features are extracted by our model more precisely and effectively.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4321089363",
    "type": "article"
  },
  {
    "title": "Meta-learning Advisor Networks for Long-tail and Noisy Labels in Social Image Classification",
    "doi": "https://doi.org/10.1145/3584360",
    "publication_date": "2023-02-18",
    "publication_year": 2023,
    "authors": "Simone Ricci; Tiberio Uricchio; Alberto Del Bimbo",
    "corresponding_authors": "",
    "abstract": "Deep neural networks (DNNs) for social image classification are prone to performance reduction and overfitting when trained on datasets plagued by noisy or imbalanced labels. Weight loss methods tend to ignore the influence of noisy or frequent category examples during the training, resulting in a reduction of final accuracy and, in the presence of extreme noise, even a failure of the learning process. A new advisor network is introduced to address both imbalance and noise problems, and is able to pilot learning of a main network by adjusting the visual features and the gradient with a meta-learning strategy. In a curriculum learning fashion, the impact of redundant data is reduced while recognizable noisy label images are downplayed or redirected. Meta Feature Re-Weighting (MFRW) and Meta Equalization Softmax (MES) methods are introduced to let the main network focus only on the information in an image deemed relevant by the advisor network and to adjust the training gradient to reduce the adverse effects of frequent or noisy categories. The proposed method is first tested on synthetic versions of CIFAR10 and CIFAR100, and then on the more realistic ImageNet-LT, Places-LT, and Clothing1M datasets, reporting state-of-the-art results.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4321329572",
    "type": "article"
  },
  {
    "title": "AMC: Adaptive Multi-expert Collaborative Network for Text-guided Image Retrieval",
    "doi": "https://doi.org/10.1145/3584703",
    "publication_date": "2023-02-20",
    "publication_year": 2023,
    "authors": "Hongguang Zhu; Yunchao Wei; Yao Zhao; Chunjie Zhang; Shujuan Huang",
    "corresponding_authors": "",
    "abstract": "Text-guided image retrieval integrates reference image and text feedback as a multimodal query to search the image corresponding to user intention. Recent approaches employ multi-level matching, multiple accesses, or multiple subnetworks for better performance regardless of the heavy burden of storage and computation in the deployment. Additionally, these models not only rely on expert knowledge to handcraft image-text composing modules but also do inference by the static computational graph. It limits the representation capability and generalization ability of networks in the face of challenges from complex and varied combinations of reference image and text feedback. To break the shackles of the static network concept, we introduce the dynamic router mechanism to achieve data-dependent expert activation and flexible collaboration of multiple experts to explore more implicit multimodal fusion patterns. Specifically, we construct AMC, our A daptive M ulti-expert C ollaborative network, by using the proposed router to activate the different experts with different levels of image-text interaction. Since routers can dynamically adjust the activation of experts for the current samples, AMC can achieve the adaptive fusion mode for the different reference image and text combinations and generate dynamic computational graphs according to varied multimodal queries. Extensive experiments on two benchmark datasets demonstrate that due to benefits from the image-text composing representation produced by an adaptive multi-expert collaboration mechanism, AMC has better retrieval performance and zero-shot generalization ability than the state-of-the-art method while keeping the lightweight model and fast retrieval speed. Moreover, we analyze the visualization of path activation, attention map, and retrieval results to further understand the routing decisions and semantic localization ability of AMC. The codes and pretrained models are available at https://github.com/KevinLight831/AMC .",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4321372711",
    "type": "article"
  },
  {
    "title": "TEVL: Trilinear Encoder for Video-language Representation Learning",
    "doi": "https://doi.org/10.1145/3585388",
    "publication_date": "2023-02-24",
    "publication_year": 2023,
    "authors": "Xin Man; Jie Shao; Feiyu Chen; Mingxing Zhang; Heng Tao Shen",
    "corresponding_authors": "",
    "abstract": "Pre-training model on large-scale unlabeled web videos followed by task-specific fine-tuning is a canonical approach to learning video and language representations. However, the accompanying Automatic Speech Recognition (ASR) transcripts in these videos are directly transcribed from audio, which may be inconsistent with visual information and would impair the language modeling ability of the model. Meanwhile, previous V-L models fuse visual and language modality features using single- or dual-stream architectures, which are not suitable for the current situation. Besides, traditional V-L research focuses mainly on the interaction between vision and language modalities and leaves the modeling of relationships within modalities untouched. To address these issues and maintain a small manual labor cost, we add automatically extracted dense captions as a supplementary text and propose a new trilinear video-language interaction framework TEVL (Trilinear Encoder for Video-Language representation learning). TEVL contains three unimodal encoders, a TRIlinear encOder (TRIO) block, and a temporal Transformer. TRIO is specially designed to support effective text-vision-text interaction, which encourages inter-modal cooperation while maintaining intra-modal dependencies. We pre-train TEVL on the HowTo100M and TV datasets with four task objectives. Experimental results demonstrate that TEVL can learn powerful video-text representation and achieve competitive performance on three downstream tasks, including multimodal video captioning, video Question Answering (QA), as well as video and language inference. Implementation code is available at https://github.com/Gufrannn/TEVL .",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4321786075",
    "type": "article"
  },
  {
    "title": "3D Object Watermarking from Data Hiding in the Homomorphic Encrypted Domain",
    "doi": "https://doi.org/10.1145/3588573",
    "publication_date": "2023-03-21",
    "publication_year": 2023,
    "authors": "Bianca Jansen van Rensburg; Pauline Puteaux; William Puech; Jean-Pierre Pedeboy",
    "corresponding_authors": "",
    "abstract": "For over a decade, 3D objects are an increasingly popular form of media. It has become necessary and urgent to secure them during their transmission or archiving. In this article, we propose a new method to obtain a watermarked 3D object from high-capacity data hiding in the encrypted domain. Based on the homomorphic properties of the Paillier cryptosystem, our proposed method allows us to embed several secret messages in the encrypted domain with a high-capacity. These messages can be extracted in the plain-text domain after the 3D object decryption. To the best of our knowledge, we are the first to propose a data hiding method in the encrypted domain where the high-capacity watermark is conserved in the plain-text domain after the 3D object is decrypted. The encryption and the data hiding in the encrypted domain are format compliant and without size expansion, despite the use of the Paillier cryptosystem. Each time a new message is embedded in the encrypted domain, flags are added in order to indicate which blocks are still available for the embedding of additional messages. After the decryption of a watermarked encrypted 3D object, our method produces a watermarked 3D object which is visually very similar to the original 3D object. From the decrypted watermarked 3D object, we can then extract all the embedded messages directly in the plain-text domain, without the need for an auxiliary file. Moreover, large keys are used, rending our method secure for real-life applications.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4328127670",
    "type": "article"
  },
  {
    "title": "Text Image Super-Resolution Guided by Text Structure and Embedding Priors",
    "doi": "https://doi.org/10.1145/3595924",
    "publication_date": "2023-05-05",
    "publication_year": 2023,
    "authors": "Cong Huang; Xiulian Peng; Dong Liu; Yan Lu",
    "corresponding_authors": "",
    "abstract": "We aim to super-resolve text images from unrecognizable low-resolution inputs. Existing super-resolution methods mainly learn a direct mapping from low-resolution to high-resolution images by exploring low-level features, which usually generate blurry outputs and suffer from severe structure distortion for text parts, especially when the resolution is quite low. Both the visual quality and the readability will suffer. To tackle these issues, we propose a new text super-resolution paradigm by recovering with understanding. Specifically, we extract a text-embedding prior and a text-structure prior from the upsampled image by learning to understand the text. The two priors with rich structure information and text-embedding information are then used as auxiliary information to recover the clear text structure. In addition, we introduce a text-feature loss to guide the training for better text recognizability. Extensive evaluations on both screen and scene text image datasets show that our method largely outperforms the state-of-the-art in both visual quality and recognition accuracy.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4372346670",
    "type": "article"
  },
  {
    "title": "A Universal Optimization Framework for Learning-based Image Codec",
    "doi": "https://doi.org/10.1145/3580499",
    "publication_date": "2023-07-03",
    "publication_year": 2023,
    "authors": "Jing Zhao; Bin Li; Jiahao Li; Ruiqin Xiong; Yan Lu",
    "corresponding_authors": "",
    "abstract": "Recently, machine learning-based image compression has attracted increasing interests and is approaching the state-of-the-art compression ratio. But unlike traditional codec, it lacks a universal optimization method to seek efficient representation for different images. In this paper, we develop a plug-and-play optimization framework for seeking higher compression ratio, which can be flexibly applied to existing and potential future compression networks. To make the latent representation more efficient, we propose a novel latent optimization algorithm to adaptively remove the redundancy for each image. Additionally, inspired by the potential of side information for traditional codecs, we introduce side information into our framework, and integrate side information optimization with latent optimization to further enhance the compression ratio. In particular, with the joint side information and latent optimization, we can achieve fine rate control using only single model instead of training different models for different rate-distortion trade-offs, which significantly reduces the training and storage cost to support multiple bit rates. Experimental results demonstrate that our proposed framework can remarkably boost the machine learning-based compression ratio, achieving more than 10% additional bit rate saving on three different representative network structures. With the proposed optimization framework, we can achieve 7.6% bit rate saving against the latest traditional coding standard VVC on Kodak dataset, yielding the state-of-the-art compression ratio.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4382938565",
    "type": "article"
  },
  {
    "title": "Syncretic Space Learning Network for NIR-VIS Face Recognition",
    "doi": "https://doi.org/10.1145/3607143",
    "publication_date": "2023-07-04",
    "publication_year": 2023,
    "authors": "Yiming Yang; Weipeng Hu; Haifeng Hu",
    "corresponding_authors": "",
    "abstract": "To overcome the technical bottleneck of face recognition in low-light scenarios, Near-InfraRed and VISible (NIR-VIS) heterogeneous face recognition is proposed for matching well-lit VIS faces with poorly lit NIR faces. Current cross-modal synthesis methods visually convert the NIR modality to the VIS modality and then perform face matching in the VIS modality. However, using a heavyweight GAN network on unpaired NIR-VIS faces may lead to high synthesis difficulty, low inference efficiency, and other problems. To alleviate the above problems, we simultaneously synthesize NIR and VIS images into modality-independent syncretic images and propose a novel syncretic space learning (SSL) model to eliminate the modal gap. First, Syncretic Modality Generator (SMG) synthesizes NIR and VIS images into syncretic images using channel-level convolution with a shallow CNN. In particular, the discriminative structural information is well preserved and the face quality can be further improved with small modal variations in a self-supervised learning manner. Second, Modality-adversarial Syncretic space Learning (MSL) projects NIR and VIS images into the syncretic space by a syncretic-modality adversarial learning strategy with syncretic pattern guided objective, so the modal gap of NIR-VIS faces can be effectively reduced. Finally, the Syncretic Distribution Consistency (SDC) constructed by NIR-syncretic, syncretic-syncretic, and VIS-syncretic consistency can enhance the intra-class compactness and learn discriminative representations. Extensive experiments on three challenging datasets demonstrate the effectiveness of the SSL method.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4383069317",
    "type": "article"
  },
  {
    "title": "Boosting Few-shot Object Detection with Discriminative Representation and Class Margin",
    "doi": "https://doi.org/10.1145/3608478",
    "publication_date": "2023-07-12",
    "publication_year": 2023,
    "authors": "Yanyan Shi; Shaowu Yang; Wenjing Yang; Dianxi Shi; Xuehui Li",
    "corresponding_authors": "",
    "abstract": "Classifying and accurately locating a visual category with few annotated training samples in computer vision has motivated the few-shot object detection technique, which exploits transfering the source-domain detection model to the target domain. Under this paradigm, however, such transferred source-domain detection model usually encounters difficulty in the classification of the target domain because of the low data diversity of novel training samples. To combat this, we present a simple yet effective few-shot detector, Transferable RCNN. To transfer general knowledge learned from data-abundant base classes to data-scarce novel classes, we propose a weight transfer strategy to promote model transferability and an attention-based feature enhancement mechanism to learn more robust object proposal feature representations. Further, we ensure strong discrimination by optimizing the contrastive objectives of feature maps via a supervised spatial contrastive loss. Meanwhile, we introduce an angle-guided additive margin classifier to augment instance-level inter-class difference and intra-class compactness, which is beneficial for improving the discriminative power of the few-shot classification head under a few supervisions. Our proposed framework outperforms the current works in various settings of PASCAL VOC and MSCOCO datasets; this demonstrates the effectiveness and generalization ability.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4384030053",
    "type": "article"
  },
  {
    "title": "Collocated Clothing Synthesis with GANs Aided by Textual Information: A Multi-Modal Framework",
    "doi": "https://doi.org/10.1145/3614097",
    "publication_date": "2023-08-14",
    "publication_year": 2023,
    "authors": "Linlin Liu; Haijun Zhang; Qun Li; Jianghong Ma; Zhao Zhang",
    "corresponding_authors": "",
    "abstract": "Synthesizing realistic images of fashion items which are compatible with given clothing images, as well as conditioning on multiple modalities, brings novel and exciting applications together with enormous economic potential. In this work, we propose a multi-modal collocation framework based on generative adversarial network (GAN) for synthesizing compatible clothing images. Given an input clothing item that consists of an image and a text description, our model works on synthesizing a clothing image which is compatible with the input clothing, as well as being guided by a given text description from the target domain. Specifically, a generator aims to synthesize realistic and collocated clothing images relying on image- and text-based latent representations learned from the source domain. An auxiliary text representation from the target domain is added for supervising the generation results. In addition, a multi-discriminator framework is carried out to determine compatibility between the generated clothing images and the input clothing images, as well as visual-semantic matching between the generated clothing images and the targeted textual information. Extensive quantitative and qualitative results demonstrate that our model substantially outperforms state-of-the-art methods in terms of authenticity, diversity, and visual-semantic similarity between image and text.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4385810246",
    "type": "article"
  },
  {
    "title": "Cross-modality Multiple Relations Learning for Knowledge-based Visual Question Answering",
    "doi": "https://doi.org/10.1145/3618301",
    "publication_date": "2023-09-02",
    "publication_year": 2023,
    "authors": "Yan Wang; Peize Li; Qingyi Si; Hanwen Zhang; Wenyu Zang; Zheng Lin; Peng Fu",
    "corresponding_authors": "",
    "abstract": "Knowledge-based visual question answering not only needs to answer the questions based on images but also incorporates external knowledge to study reasoning in the joint space of vision and language. To bridge the gap between visual content and semantic cues, it is important to capture the question-related and semantics-rich vision-language connections. Most existing solutions model simple intra-modality relation or represent cross-modality relation using a single vector, which makes it difficult to effectively model complex connections between visual features and question features. Thus, we propose a cross-modality multiple relations learning model, aiming to better enrich cross-modality representations and construct advanced multi-modality knowledge triplets. First, we design a simple yet effective method to generate multiple relations that represent the rich cross-modality relations. The various cross-modality relations link the textual question to the related visual objects. These multi-modality triplets efficiently align the visual objects and corresponding textual answers. Second, to encourage multiple relations to better align with different semantic relations, we further formulate a novel global-local loss. The global loss enables the visual objects and corresponding textual answers close to each other through cross-modality relations in the vision-language space, and the local loss better preserves semantic diversity among multiple relations. Experimental results on the Outside Knowledge VQA and Knowledge-Routed Visual Question Reasoning datasets demonstrate that our model outperforms the state-of-the-art methods.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4386390772",
    "type": "article"
  },
  {
    "title": "Underwater Image Quality Assessment from Synthetic to Real-world: Dataset and Objective Method",
    "doi": "https://doi.org/10.1145/3624983",
    "publication_date": "2023-09-20",
    "publication_year": 2023,
    "authors": "Xinyue Li; Haiyong Xu; Gangyi Jiang; Mei Yu; Ting Luo; Xuebo Zhang; Hongwei Ying",
    "corresponding_authors": "",
    "abstract": "The complicated underwater environment and lighting conditions lead to severe influence on the quality of underwater imaging, which tends to impair underwater exploration and research. To effectively evaluate the quality of underwater images, an underwater image quality assessment dataset is constructed from synthetic to real-world, and then a new objective underwater image assessment method based on the characteristics of the underwater imaging is proposed (UICQA). Specifically, to address the lack of a publicly available datasets and more accurately quantify the quality of underwater images, a subjective underwater image quality assessment dataset from synthetic to real-world underwater images, named USRD, is constructed. Considering that the transmission map can effectively reflect the characteristics of the underwater imaging, statistical features are effectively extracted from the transmission map for distinguishing underwater images of different quality. Further, considering that the transmission map negatively correlates with scene depth, a local-to-global transmission map weighted contrast feature is constructed. Additionally, the color features of human perception and texture features based on fractal dimensions are proposed. Finally, the experimental results show that the proposed UICQA method exhibits the highest correlation with ground truth scores compared to state-of-the-art UIQA methods.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4386890104",
    "type": "article"
  },
  {
    "title": "Geometric and Learning-Based Mesh Denoising: A Comprehensive Survey",
    "doi": "https://doi.org/10.1145/3625098",
    "publication_date": "2023-09-27",
    "publication_year": 2023,
    "authors": "Honghua Chen; Zhiqi Li; Mingqing Wei; Jun Wang",
    "corresponding_authors": "",
    "abstract": "Mesh denoising is a fundamental problem in digital geometry processing. It seeks to remove surface noise while preserving surface intrinsic signals as accurately as possible. While traditional wisdom has been built upon specialized priors to smooth surfaces, learning-based approaches are making their debut with great success in generalization and automation. In this work, we provide a comprehensive review of the advances in mesh denoising, containing both traditional geometric approaches and recent learning-based methods. First, to familiarize readers with the denoising tasks, we summarize four common issues in mesh denoising. We then provide two categorizations of the existing denoising methods. Furthermore, three important categories, including optimization-, filter-, and data-driven-based techniques, are introduced and analyzed in detail, respectively. Both qualitative and quantitative comparisons are illustrated, to demonstrate the effectiveness of the state-of-the-art denoising methods. Finally, potential directions of future work are pointed out to solve the common problems of these approaches. A mesh denoising benchmark is also built in this work, and future researchers will easily and conveniently evaluate their methods with state-of-the-art approaches. To aid reproducibility, we release our datasets and used results at https://github.com/chenhonghua/Mesh-Denoiser.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4387105976",
    "type": "article"
  },
  {
    "title": "Rethinking Person Re-Identification via Semantic-based Pretraining",
    "doi": "https://doi.org/10.1145/3628452",
    "publication_date": "2023-10-17",
    "publication_year": 2023,
    "authors": "Suncheng Xiang; Dahong Qian; Jingsheng Gao; Zirui Zhang; Ting Liu; Yuzhuo Fu",
    "corresponding_authors": "",
    "abstract": "Pretraining is a dominant paradigm in computer vision. Generally, supervised ImageNet pretraining is commonly used to initialize the backbones of person re-identification (Re-ID) models. However, recent works show a surprising result that CNN-based pretraining on ImageNet has limited impacts on Re-ID system due to the large domain gap between ImageNet and person Re-ID data. To seek an alternative to traditional pretraining, here we investigate semantic-based pretraining as another method to utilize additional textual data against ImageNet pretraining. Specifically, we manually construct a diversified FineGPR-C caption dataset for the first time on person Re-ID events. Based on it, a pure semantic-based pretraining approach named VTBR is proposed to adopt dense captions to learn visual representations with fewer images. We train convolutional neural networks from scratch on the captions of FineGPR-C dataset, and then transfer them to downstream Re-ID tasks. Comprehensive experiments conducted on benchmark datasets show that our VTBR can achieve competitive performance compared with ImageNet pretraining—despite using up to 1.4× fewer images, revealing its potential in Re-ID pretraining. Our source code is also publicly available at https://github.com/JeremyXSC/VTBR .",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4387698360",
    "type": "article"
  },
  {
    "title": "Nonlocal Hybrid Network for Long-tailed Image Classification",
    "doi": "https://doi.org/10.1145/3630256",
    "publication_date": "2023-11-02",
    "publication_year": 2023,
    "authors": "Rongjiao Liang; Shichao Zhang; Wenzhen Zhang; Guixian Zhang; Jinyun Tang",
    "corresponding_authors": "",
    "abstract": "It is a significant issue to deal with long-tailed data when classifying images. A nonlocal hybrid network (NHN) that takes into account both feature learning and classifier learning is proposed. The NHN can capture the existence of dependencies between two locations that are far away from each other as well as alleviate the impact of long-tailed data on the model to some extent. The dependency relationship between distant pixels is obtained first through a nonlocal module to extract richer feature representations. Then, a learnable soft class center is proposed to balance the supervised contrastive loss and reduce the impact of long-tailed data on feature learning. For efficiency, a logit adjustment strategy is adopted to correct the bias caused by the different label distributions between the training and test sets and obtain a classifier that is more suitable for long-tailed data. Finally, extensive experiments are conducted on two benchmark datasets, the long-tailed CIFAR and the large-scale real-world iNaturalist 2018, both of which have imbalanced label distributions. The experimental results show that the proposed NHN model is efficient and promising.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4388210396",
    "type": "article"
  },
  {
    "title": "Cross-modal Semantically Augmented Network for Image-text Matching",
    "doi": "https://doi.org/10.1145/3631356",
    "publication_date": "2023-11-04",
    "publication_year": 2023,
    "authors": "Tao Yao; Yiru Li; Ying Li; Yingying Zhu; Gang Wang; Jun Yue",
    "corresponding_authors": "",
    "abstract": "Image-text matching plays an important role in solving the problem of cross-modal information processing. Since there are nonnegligible semantic differences between heterogenous pairwise data, a crucial challenge is how to learn a unified representation. Existing methods mainly rely on the alignment between regional image features and corresponding entity words. However, the regional features in the image are often more concerned with the foreground entity information, and the attribute information of the entities and the relational information are ignored. How to effectively integrate entity-attribute alignment and relationship alignment has not been fully studied. Therefore, we propose a Cross-Modal Semantically Augmented Network for Image-Text Matching (CMSAN), which combines the relationships between entities in the image with the semantics of relational words in the text. CMSAN (1) proposes an adaptive word-type prediction model that classifies the words into four types, i.e., entity word, attribute word, relation word, and unnecessary word. It can align different image features at multiple levels. CMSAN (2) designs a sophisticated relationship alignment module and an entity-attribute alignment module that maximizes the exploitation of the semantic information, which enables the model to have more discriminative power and further improves the matching accuracy.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4388332081",
    "type": "article"
  },
  {
    "title": "Viewpoint Disentangling and Generation for Unsupervised Object Re-ID",
    "doi": "https://doi.org/10.1145/3632959",
    "publication_date": "2023-11-14",
    "publication_year": 2023,
    "authors": "Zongyi Li; Yuxuan Shi; Hefei Ling; Jiazhong Chen; Boyuan Liu; Runsheng Wang; Chengxin Zhao",
    "corresponding_authors": "",
    "abstract": "Unsupervised object Re-ID aims to learn discriminative identity features from a fully unlabeled dataset to solve the open-class re-identification problem. Satisfying results have been achieved in existing unsupervised Re-ID methods, primarily trained with pseudo-labels created by feature clustering. However, the viewpoint variation of objects is the key challenge, introducing noisy labels in the clustering process. To address this problem, a novel viewpoint disentangling and generation framework (VDG) is proposed to learn viewpoint-invariant ID features, including a disentangling and generation module, as well as a contrastive learning module. First, we design an ID encoder to map the viewpoint and identity features into the latent space. Second, a generator is used to disentangle view features and synthesize images with different orientations. Especially, the well-trained encoder serves as a pre-trained feature extractor in the contrastive learning module. Third, a viewpoint-aware loss and a class-level loss are integrated to facilitate contrastive learning between original and novel views. The generation of novel view images and the application of viewpoint-aware contrastive loss mutually assist model learning viewpoint-invariant ID features. Extensive experiments on Market-1501, DukeMTMC, MSMT17, and VeRi-776 demonstrate the effectiveness of the proposed VDG framework, as well as its superiority over the existing state-of-the-art approaches. The VDG model also demonstrates high quality in the image generation tasks.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4388657562",
    "type": "article"
  },
  {
    "title": "DPDFormer: A Coarse-to-Fine Model for Monocular Depth Estimation",
    "doi": "https://doi.org/10.1145/3638559",
    "publication_date": "2023-12-25",
    "publication_year": 2023,
    "authors": "Chunpu Liu; Guanglei Yang; Wangmeng Zuo; Tianyi Zang",
    "corresponding_authors": "",
    "abstract": "Monocular depth estimation attracts great attention from computer vision researchers for its convenience in acquiring environment depth information. Recently classification-based MDE methods show its promising performance and begin to act as an essential role in many multi-view applications such as reconstruction and 3D object detection. However, existed classification-based MDE models usually apply fixed depth range discretization strategy across a whole scene. This fixed depth range discretization leads to the imbalance of discretization scale among different depth ranges, resulting in the inexact depth range localization. In this article, to alleviate the imbalanced depth range discretization problem in classification-based monocular depth estimation (MDE) method we follow the coarse-to-fine principle and propose a novel depth range discretization method called depth post-discretization (DPD). Based on a coarse depth anchor roughly indicating the depth range, the DPD generates the depth range discretization adaptively for every position. The depth range discretization with DPD is more fine-grained around the actual depth, which is beneficial for locating the depth range more precisely for each scene position. Besides, to better manage the prediction of the coarse depth anchor and depth probability distribution for calculating the final depth, we design a dual-decoder transformer-based network, i.e., DPDFormer, which is more compatible with our proposed DPD method. We evaluate DPDFormer on popular depth datasets NYU Depth V2 and KITTI. The experimental results prove the superior performance of our proposed method.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4390195732",
    "type": "article"
  },
  {
    "title": "RAST: Restorable Arbitrary Style Transfer",
    "doi": "https://doi.org/10.1145/3638770",
    "publication_date": "2023-12-30",
    "publication_year": 2023,
    "authors": "Yingnan Ma; Chenqiu Zhao; Bingran Huang; Xudong Li; Anup Basu",
    "corresponding_authors": "",
    "abstract": "The objective of arbitrary style transfer is to apply a given artistic or photo-realistic style to a target image. Although current methods have shown some success in transferring style, arbitrary style transfer still has several issues, including content leakage. Embedding an artistic style can result in unintended changes to the image content. This paper proposes an iterative framework called Restorable Arbitrary Style Transfer (RAST) to effectively ensure content preservation and mitigate potential alterations to the content information. RAST can transmit both content and style information through multi-restorations and balance the content-style trade-off in stylized images using the image restoration accuracy. To ensure RAST’s effectiveness, we introduce two novel loss functions: multi-restoration loss and style difference loss. We also propose a new quantitative evaluation method to assess content preservation and style embedding performance. Experimental results show that RAST outperforms state-of-the-art methods in generating stylized images that preserve content and embed style accurately.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4390450990",
    "type": "article"
  },
  {
    "title": "ReFID: Reciprocal Frequency-aware Generalizable Person Re-identification via Decomposition and Filtering",
    "doi": "https://doi.org/10.1145/3643684",
    "publication_date": "2024-02-16",
    "publication_year": 2024,
    "authors": "Jinjia Peng; Song Pengpeng; Hui Li; Huibing Wang",
    "corresponding_authors": "",
    "abstract": "Domain generalization of person re-identification aims to conduct testing across domains that have not been previously encountered, without utilizing target domain data during the training stage. As the number of source domains increases, the relationships between training samples become more complex. This can lead to domain-invariant features that include certain instance-level spurious correlations, which can impact the model’s ability to generalize further. To overcome this limitation, the Reciprocal Frequency-aware Generalizable Person Re-identification method is proposed in this article, which aims to utilize spectral feature correlation learning to transmit frequency component information and generate more discriminative hybrid features. A module called Bilateral Frequency Component-guided Attention is developed to help the network understand high-level semantic and texture information from various frequency features. Furthermore, to reduce the impact of noise from the frequency domain, this article proposes an innovative module called Fourier Noise Masquerade Filtering. This module enhances the portability of frequency domain components while simultaneously suppressing elements that do not contribute to generalization. Extensive experimental results on various datasets demonstrate that our method is effective and superior to the state-of-the-art methods.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4391884336",
    "type": "article"
  },
  {
    "title": "Learning Nighttime Semantic Segmentation the Hard Way",
    "doi": "https://doi.org/10.1145/3650032",
    "publication_date": "2024-03-04",
    "publication_year": 2024,
    "authors": "Wenxi Liu; Jiaxin Cai; Qi Li; Chenyang Liao; Jingjing Cao; Shengfeng He; Yuanlong Yu",
    "corresponding_authors": "",
    "abstract": "Nighttime semantic segmentation is an important but challenging research problem for autonomous driving. The major challenges lie in the small objects or regions from the under-/over-exposed areas or suffer from motion blur caused by the camera deployed on moving vehicles. To resolve this, we propose a novel hard-class-aware module that bridges the main network for full-class segmentation and the hard-class network for segmenting aforementioned hard-class objects. In specific, it exploits the shared focus of hard-class objects from the dual-stream network, enabling the contextual information flow to guide the model to concentrate on the pixels that are hard to classify. In the end, the estimated hard-class segmentation results will be utilized to infer the final results via an adaptive probabilistic fusion refinement scheme. Moreover, to overcome over-smoothing and noise caused by extreme exposures, our model is modulated by a carefully crafted pretext task of constructing an exposure-aware semantic gradient map, which guides the model to faithfully perceive the structural and semantic information of hard-class objects while mitigating the negative impact of noises and uneven exposures. In experiments, we demonstrate that our unique network design leads to superior segmentation performance over existing methods, featuring the strong ability of perceiving hard-class objects under adverse conditions.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4392376875",
    "type": "article"
  },
  {
    "title": "Discriminative Segment Focus Network for Fine-grained Video Action Recognition",
    "doi": "https://doi.org/10.1145/3654671",
    "publication_date": "2024-03-26",
    "publication_year": 2024,
    "authors": "Baoli Sun; Xinchen Ye; Tiantian Yan; Zhihui Wang; Haojie Li; Zhiyong Wang",
    "corresponding_authors": "",
    "abstract": "Fine-grained video action recognition aims at identifying minor and discriminative variations among fine categories of actions. While many recent action recognition methods have been proposed to better model spatio-temporal representations, how to model the interactions among discriminative atomic actions to effectively characterize inter-class and intra-class variations has been neglected, which is vital for understanding fine-grained actions. In this work, we devise a Discriminative Segment Focus Network (DSFNet) to mine the discriminability of segment correlations and localize discriminative action-relevant segments for fine-grained video action recognition. Firstly, we propose a hierarchic correlation reasoning (HCR) module which explicitly establishes correlations between different segments at multiple temporal scales and enhances each segment by exploiting the correlations with other segments. Secondly, a discriminative segment focus (DSF) module is devised to localize the most action-relevant segments from the enhanced representations of HCR by enforcing the consistency between the discriminability and the classification confidence of a given segment with a consistency constraint. Finally, these localized segment representations are combined with the global action representation of the whole video for boosting final recognition. Extensive experimental results on two fine-grained action recognition datasets, i.e., FineGym and Diving48, and two action recognition datasets, i.e., Kinetics400 and Something-Something, demonstrate the effectiveness of our approach compared with the state-of-the-art methods.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4393191810",
    "type": "article"
  },
  {
    "title": "Dual Dynamic Threshold Adjustment Strategy",
    "doi": "https://doi.org/10.1145/3656047",
    "publication_date": "2024-04-03",
    "publication_year": 2024,
    "authors": "Xiruo Jiang; Yazhou Yao; Sheng Liu; Fumin Shen; Liqiang Nie; Xian‐Sheng Hua",
    "corresponding_authors": "",
    "abstract": "Loss functions and sample mining strategies are essential components in deep metric learning algorithms. However, the existing loss function or mining strategy often necessitates the incorporation of additional hyperparameters, notably the threshold, which defines whether the sample pair is informative. The threshold provides a stable numerical standard for determining whether to retain the pairs. It is a vital parameter to reduce the redundant sample pairs participating in training. Nonetheless, finding the optimal threshold can be a time-consuming endeavor, often requiring extensive grid searches. Because the threshold cannot be dynamically adjusted in the training stage, we should conduct plenty of repeated experiments to determine the threshold. Therefore, we introduce a novel approach for adjusting the thresholds associated with both the loss function and the sample mining strategy. We design a static Asymmetric Sample Mining Strategy (ASMS) and its dynamic version, the Adaptive Tolerance ASMS (AT-ASMS), tailored for sample mining methods. ASMS utilizes differentiated thresholds to address the problems (too few positive pairs and too many redundant negative pairs) caused by only applying a single threshold to filter samples. The AT-ASMS can adaptively regulate the ratio of positive and negative pairs during training according to the ratio of the currently mined positive and negative pairs. This meta-learning-based threshold generation algorithm utilizes a single-step gradient descent to obtain new thresholds. We combine these two threshold adjustment algorithms to form the Dual Dynamic Threshold Adjustment Strategy (DDTAS). Experimental results show that our algorithm achieves competitive performance on the CUB200, Cars196, and SOP datasets. Our codes are available at https://github.com/NUST-Machine-Intelligence-Laboratory/DDTAS .",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4393871823",
    "type": "article"
  },
  {
    "title": "Multi-grained Representation Aggregating Transformer with Gating Cycle for Change Captioning",
    "doi": "https://doi.org/10.1145/3660346",
    "publication_date": "2024-04-22",
    "publication_year": 2024,
    "authors": "Shengbin Yue; Yunbin Tu; Liang Li; Shengxiang Gao; Zhengtao Yu",
    "corresponding_authors": "",
    "abstract": "Change captioning aims to describe the difference within an image pair in natural language, which combines visual comprehension and language generation. Although significant progress has been achieved, it remains a key challenge of perceiving the object change from different perspectives, especially the severe situation with drastic viewpoint change. In this article, we propose a novel full-attentive network, namely Multi-grained Representation Aggregating Transformer (MURAT), to distinguish the actual change from viewpoint change. Specifically, the Pair Encoder first captures similar semantics between pairwise objects in a multi-level manner, which are regarded as the semantic cues of distinguishing the irrelevant change. Next, a novel Multi-grained Representation Aggregator (MRA) is designed to construct the reliable difference representation by employing both coarse- and fine-grained semantic cues. Finally, the language decoder generates a description of the change based on the output of MRA. Besides, the Gating Cycle Mechanism is introduced to facilitate the semantic consistency between difference representation learning and language generation with a reverse manipulation, so as to bridge the semantic gap between change features and text features. Extensive experiments demonstrate that the proposed MURAT can greatly improve the ability to describe the actual change in the distraction of irrelevant change and achieves state-of-the-art performance on three benchmarks, CLEVR-Change, CLEVR-DC, and Spot-the-Diff.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4395001637",
    "type": "article"
  },
  {
    "title": "P2FTrack: Multi-object tracking with motion Prior and Feature Posterior",
    "doi": "https://doi.org/10.1145/3700443",
    "publication_date": "2024-10-14",
    "publication_year": 2024,
    "authors": "Hong Zhang; Jiaxu Wan; Jing Zhang; Ding Yuan; Xuliang Li; Y. F. Yang",
    "corresponding_authors": "",
    "abstract": "Multiple object tracking (MOT) has emerged as a crucial component of the rapidly developing computer vision. However, existing multi-object tracking methods often overlook the relationship between features and motion, hindering the ability to strike a performance balance between coupled motion and complex scenes. In this work, we propose a novel end-to-end multi-object tracking method that integrates motion and feature information. To achieve this, we introduce a motion prior generator that transforms motion information into attention masks. Additionally, we leverage prior-posterior fusion multi-head attention to combine the motion-derived priors and attention-based posteriors. Our proposed method is extensively evaluated on MOT17 and DanceTrack datasets through comprehensive experiments and ablation studies, demonstrating state-of-the-art performance in the feature-based method with reasonable speed.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4403381123",
    "type": "article"
  },
  {
    "title": "IMCE",
    "doi": "https://doi.org/10.1145/1083314.1083315",
    "publication_date": "2005-08-01",
    "publication_year": 2005,
    "authors": "Brett Adams; Svetha Venkatesh; Ramesh Jain",
    "corresponding_authors": "",
    "abstract": "We discuss the design goals for an integrated media creation environment (IMCE) aimed at enabling the average user to create media artifacts with professional qualities. The resulting requirements are implemented and we demonstrate the efficacy of the resulting system with the generation of two simple home movies. The significance for the average user seeking to create home movies lies in the flexible and automatic application of film principles to the task, removal of tedious low-level editing by means of well-formed media transformations in terms of high-level film constructs (e.g., tempo), and content repurposing powered by those same transformations added to the rich semantic information maintained at each phase of the process.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2041719916",
    "type": "article"
  },
  {
    "title": "Salient stills",
    "doi": "https://doi.org/10.1145/1047936.1047940",
    "publication_date": "2005-02-01",
    "publication_year": 2005,
    "authors": "Laura Teodosio; Walter Bender",
    "corresponding_authors": "",
    "abstract": "Salient Stills are a class of images that reflect the aggregation of the temporal changes that occur in a moving-image sequence with the salient features of individual frames preserved. They convey the intended expression of an entire series of moving frames---a visual summary of camera and object movements. The original frames, which may include variations in focal length or field of view, or moving objects, are combined to create a single still image. The still image may have multiresolution patches, a larger field of view, or higher overall resolution than any individual frame in the original image sequence. Salient Stills may also contain selected significant objects from individual or multiple video frames. Salient Stills attempt to retain much of the original content (detail) and spatial and temporal extent (context) of the original video or film sequence.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2295024634",
    "type": "article"
  },
  {
    "title": "Coordinated enroute multimedia object caching in transcoding proxies for tree networks",
    "doi": "https://doi.org/10.1145/1083314.1083318",
    "publication_date": "2005-08-01",
    "publication_year": 2005,
    "authors": "Keqiu Li; Hong Shen",
    "corresponding_authors": "",
    "abstract": "Transcoding is a promising technology that allows systems to effect a quality-versus-size tradeoff on multimedia objects. As audio and video applications have proliferated on the Internet, caching in transcoding proxies has become an important technique for improving network performance, especially in mobile networks. This article addresses the problem of coordinated enroute multimedia object caching in transcoding proxies for tree networks. We formulate this problem as an optimization problem based on our proposed model, in which multimedia object caching decisions are made on all enroute caches along the routing path by integrating both object placement and replacement policies and cache status information along the routing path of a request is used to determine the optimal locations for caching multiple versions of the same multimedia object. We propose an optimal solution using dynamic programming to compute the optimal locations. We also extend this solution to solve the same problem for several constrained cases, including constraints on the cost gain per node and on the number of versions to be placed. Our model is evaluated on different performance metrics through extensive simulation experiments. The implementation results show that our model significantly outperforms existing models that consider Web caching in transcoding proxies either on a single path or at individual nodes.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2064316430",
    "type": "article"
  },
  {
    "title": "Rate-distortion optimized streaming of fine-grained scalable video sequences",
    "doi": "https://doi.org/10.1145/1324287.1324289",
    "publication_date": "2008-01-01",
    "publication_year": 2008,
    "authors": "Mohamed Hefeeda; Cheng-Hsin Hsu",
    "corresponding_authors": "",
    "abstract": "We present optimal schemes for allocating bits of fine-grained scalable video sequences among multiple senders streaming to a single receiver. This allocation problem is critical in optimizing the perceived quality in peer-to-peer and distributed multi-server streaming environments. Senders in such environments are heterogeneous in their outgoing bandwidth and they hold different portions of the video stream. We first formulate and optimally solve the problem for individual frames, then we generalize to the multiple frame case. Specifically, we formulate the allocation problem as an optimization problem, which is nonlinear in general. We use rate-distortion models in the formulation to achieve the minimum distortion in the rendered video, constrained by the outgoing bandwidth of senders, availability of video data at senders, and incoming bandwidth of receiver. We show how the adopted rate-distortion models transform the nonlinear problem to an integer linear programming (ILP) problem. We then design a simple rounding scheme that transforms the ILP problem to a linear programming (LP) one, which can be solved efficiently using common optimization techniques such as the Simplex method. We prove that our rounding scheme always produces a feasible solution, and the solution is within a negligible margin from the optimal solution. We also propose a new algorithm (FGSAssign) for the single-frame allocation problem that runs in O ( n log n ) steps, where n is the number of senders. We prove that FGSAssign is optimal. Furthermore, we propose a heuristic algorithm (mFGSAssign) that produces near-optimal solutions for the multiple-frame case, and runs an order of magnitude faster than the optimal one. Because of its short running time, mFGSAssign can be used in real time. Our experimental study validates our analytical analysis and shows the effectiveness of our allocation algorithms in improving the video quality.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2077871141",
    "type": "article"
  },
  {
    "title": "On the accuracy and complexity of rate-distortion models for fine-grained scalable video sequences",
    "doi": "https://doi.org/10.1145/1352012.1352019",
    "publication_date": "2008-05-01",
    "publication_year": 2008,
    "authors": "Cheng-Hsin Hsu; Mohamed Hefeeda",
    "corresponding_authors": "",
    "abstract": "Rate-distortion (R-D) models are functions that describe the relationship between the bitrate and expected level of distortion in the reconstructed video stream. R-D models enable optimization of the received video quality in different network conditions. Several R-D models have been proposed for the increasingly popular fine-grained scalable video sequences. However, the models' relative performance has not been thoroughly analyzed. Moreover, the time complexity of each model is not known, nor is the range of bitrates in which the model produces valid results. This lack of quantitative performance analysis makes it difficult to select the model that best suits a target streaming system. In this article, we classify, analyze, and rigorously evaluate all R-D models proposed for FGS coders in the literature. We classify R-D models into three categories: analytic, empirical, and semi-analytic. We describe the characteristics of each category. We analyze the R-D models by following their mathematical derivations, scrutinizing the assumptions made, and explaining when the assumptions fail and why. In addition, we implement all R-D models, a total of eight, and evaluate them using a diverse set of video sequences. In our evaluation, we consider various source characteristics, diverse channel conditions, different encoding/decoding parameters, different frame types, and several performance metrics including accuracy, range of applicability, and time complexity of each model. We also present clear systematic ways (pseudo codes) for constructing various R-D models from a given video sequence. Based on our experimental results, we present a justified list of recommendations on selecting the best R-D models for video-on-demand, video conferencing, real-time, and peer-to-peer streaming systems.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2090251087",
    "type": "article"
  },
  {
    "title": "Looking at near-duplicate videos from a human-centric perspective",
    "doi": "https://doi.org/10.1145/1823746.1823749",
    "publication_date": "2010-08-01",
    "publication_year": 2010,
    "authors": "Rodrigo Cardoso de Oliveira; Mauro Cherubini; Nuria Oliver",
    "corresponding_authors": "",
    "abstract": "Popular content in video sharing websites (e.g., YouTube) is usually replicated via identical copies or near-duplicates. These duplicates are usually studied because they pose a threat to site owners in terms of wasted disk space, or privacy infringements. Furthermore, this content might potentially hinder the users' experience in these websites. The research presented in this article focuses around the central argument that there is no agreement on the technical definition of what these near-duplicates are, and, more importantly, there is no strong evidence that users of video sharing websites would like this content to be removed. Most scholars define near-duplicate video clips (NDVC) by means of non-semantic features (e.g., different image/audio quality), while a few also include semantic features (i.e., different videos of similar content). However, it is unclear what features contribute to the human perception of near-duplicate videos. The findings of four large scale online surveys that were carried out in the context of our research confirm the relevance of both types of features. Some of our findings confirm the adopted definitions of NDVC whereas other findings are surprising: Near-duplicate videos with different image quality, audio quality, or with/without overlays were perceived as NDVC. However, the same could not be verified when videos differed by more than one of these features at the same time. With respect to semantics, it is yet unclear the exact role that it plays in relation to the features that make videos alike. From a user's perspective, participants preferred in most cases to see only one of the NDVC in the search results of a video search query and they were more tolerant to changes in the audio than in the video tracks. Based on all these findings, we propose a new user-centric NDVC definition and present implications for how duplicate content should be dealt with by video sharing Web sites.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W1985455454",
    "type": "article"
  },
  {
    "title": "Music-driven character animation",
    "doi": "https://doi.org/10.1145/1596990.1596991",
    "publication_date": "2009-10-01",
    "publication_year": 2009,
    "authors": "Danielle Sauer; Yee‐Hong Yang",
    "corresponding_authors": "",
    "abstract": "Music-driven character animation extracts musical features from a song and uses them to create an animation. This article presents a system that builds a new animation directly from musical attributes, rather than simply synchronizing it to the music like similar systems. Using a simple script that identifies the movements involved in the performance and their timing, the user can easily control the animation of characters. Another unique feature of the system is its ability to incorporate multiple characters into the same animation, both with synchronized and unsynchronized movements. A system that integrates Celtic dance movements is developed in this article. An evaluation of the results shows that the majority of animations are found to be appealing to viewers and that altering the music can change the attractiveness of the final result.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2166754568",
    "type": "article"
  },
  {
    "title": "A robust high-capacity affine-transformation-invariant scheme for watermarking 3D geometric models",
    "doi": "https://doi.org/10.1145/2344436.2344440",
    "publication_date": "2012-09-01",
    "publication_year": 2012,
    "authors": "Xifeng Gao; Caiming Zhang; Yan Huang; Zhigang Deng",
    "corresponding_authors": "",
    "abstract": "In this article we propose a novel, robust, and high-capacity watermarking method for 3D meshes with arbitrary connectivities in the spatial domain based on affine invariants. Given a 3D mesh model, a watermark is embedded as affine-invariant length ratios of one diagonal segment to the residing diagonal intersected by the other one in a coplanar convex quadrilateral. In the extraction process, a watermark is recovered by combining all the watermark pieces embedded in length ratios through majority voting. Extensive experimental results demonstrate the robustness, high computational efficiency, high capacity, and affine-transformation-invariant characteristics of the proposed approach.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2058582017",
    "type": "article"
  },
  {
    "title": "Online Estimation of Evolving Human Visual Interest",
    "doi": "https://doi.org/10.1145/2632284",
    "publication_date": "2014-08-01",
    "publication_year": 2014,
    "authors": "Harish Katti; Anoop Kolar Rajagopal; Mohan Kankanhalli; Ramakrishnan Kalpathi",
    "corresponding_authors": "",
    "abstract": "Regions in video streams attracting human interest contribute significantly to human understanding of the video. Being able to predict salient and informative Regions of Interest (ROIs) through a sequence of eye movements is a challenging problem. Applications such as content-aware retargeting of videos to different aspect ratios while preserving informative regions and smart insertion of dialog (closed-caption text) 1 into the video stream can significantly be improved using the predicted ROIs. We propose an interactive human-in-the-loop framework to model eye movements and predict visual saliency into yet-unseen frames. Eye tracking and video content are used to model visual attention in a manner that accounts for important eye-gaze characteristics such as temporal discontinuities due to sudden eye movements, noise, and behavioral artifacts. A novel statistical- and algorithm-based method gaze buffering is proposed for eye-gaze analysis and its fusion with content-based features. Our robust saliency prediction is instantiated for two challenging and exciting applications. The first application alters video aspect ratios on-the-fly using content-aware video retargeting, thus making them suitable for a variety of display sizes. The second application dynamically localizes active speakers and places dialog captions on-the-fly in the video stream. Our method ensures that dialogs are faithful to active speaker locations and do not interfere with salient content in the video stream. Our framework naturally accommodates personalisation of the application to suit biases and preferences of individual users.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2001342242",
    "type": "article"
  },
  {
    "title": "A control theoretic scheme for efficient video transmission over IEEE 802.11e EDCA WLANs",
    "doi": "https://doi.org/10.1145/2240136.2240142",
    "publication_date": "2012-07-01",
    "publication_year": 2012,
    "authors": "Paul Patras; Albert Banchs; Pablo Serrano",
    "corresponding_authors": "",
    "abstract": "The EDCA mechanism of the IEEE 802.11 standard has been designed to support, among others, video traffic. This mechanism relies on a number of parameters whose configuration is left open by the standard. Although there are some recommended values for these parameters, they are fixed independent of the WLAN conditions, which results in suboptimal performance. Following this observation, a number of approaches in the literature have been devised to set the EDCA parameters based on an estimation of the WLAN conditions. However, these previous approaches are based on heuristics and hence do not guarantee optimized performance. In this article we propose a novel algorithm to adjust the EDCA parameters to carry video traffic which, in contrast to previous approaches, is sustained on mathematical foundations that guarantee optimal performance. In particular, our approach builds upon (i) an analytical model of the WLAN performance under video traffic, used to derive the optimal point of operation of EDCA, and (ii) a control theoretic designed mechanism which drives the WLAN to this point of operation. Via extensive simulations, we show that the proposed approach performs optimally and substantially outperforms the standard recommended configuration as well as previous adaptive proposals.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2009880377",
    "type": "article"
  },
  {
    "title": "Image hatching for visual cryptography",
    "doi": "https://doi.org/10.1145/2344436.2344438",
    "publication_date": "2012-09-01",
    "publication_year": 2012,
    "authors": "Jonathan Weir; WeiQi Yan; Mohan Kankanhalli",
    "corresponding_authors": "",
    "abstract": "Image hatching (or nonphotorealistic line-art) is a technique widely used in the printing or engraving of currency. Diverse styles of brush strokes have previously been adopted for different areas of an image to create aesthetically pleasing textures and shading. Because there is no continuous tone within these types of images, a multilevel scheme is proposed, which uses different textures based on a threshold level. These textures are then applied to the different levels and are then combined to build up the final hatched image. The proposed technique allows a secret to be hidden using Visual Cryptography (VC) within the hatched images. Visual cryptography provides a very powerful means by which one secret can be distributed into two or more pieces known as shares. When the shares are superimposed exactly together, the original secret can be recovered without computation. Also provided is a comparison between the original grayscale images and the resulting hatched images that are generated by the proposed algorithm. This reinforces that the overall quality of the hatched scheme is sufficient. The Structural SIMilarity index (SSIM) is used to perform this comparison.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2138392313",
    "type": "article"
  },
  {
    "title": "A Temporal Order Modeling Approach to Human Action Recognition from Multimodal Sensor Data",
    "doi": "https://doi.org/10.1145/3038917",
    "publication_date": "2017-03-06",
    "publication_year": 2017,
    "authors": "Jun Ye; Hao Hu; Guo-Jun Qi; Kien A. Hua",
    "corresponding_authors": "",
    "abstract": "From wearable devices to depth cameras, researchers have exploited various multimodal data to recognize human actions for applications, such as video gaming, education, and healthcare. Although there many successful techniques have been presented in the literature, most current approaches have focused on statistical or local spatiotemporal features and do not explicitly explore the temporal dynamics of the sensor data. However, human action data contain rich temporal structure information that can characterize the unique underlying patterns of different action categories. From this perspective, we propose a novel temporal order modeling approach to human action recognition. Specifically, we explore subspace projections to extract the latent temporal patterns from different human action sequences. The temporal order between these patterns are compared, and the index of the pattern that appears first is used to encode the entire sequence. This process is repeated multiple times and produces a compact feature vector representing the temporal dynamics of the sequence. Human action recognition can then be efficiently solved by the nearest neighbor search based on the Hamming distance between these compact feature vectors. We further introduce a sequential optimization algorithm to learn the optimized projections that preserve the pairwise label similarity of the action sequences. Experimental results on two public human action datasets demonstrate the superior performance of the proposed technique in both accuracy and efficiency.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2594683362",
    "type": "article"
  },
  {
    "title": "V-JAUNE",
    "doi": "https://doi.org/10.1145/3063532",
    "publication_date": "2017-04-26",
    "publication_year": 2017,
    "authors": "Fairouz Hussein; Massimo Piccardi",
    "corresponding_authors": "",
    "abstract": "Video summarization and action recognition are two important areas of multimedia video analysis. While these two areas have been tackled separately to date, in this article, we present a latent structural SVM framework to recognize the action and derive the summary of a video in a joint, simultaneous fashion. Efficient inference is provided by a submodular score function that accounts for the action and summary jointly. In this article, we also define a novel measure to evaluate the quality of a predicted video summary against the annotations of multiple annotators. Quantitative and qualitative results over two challenging action datasets—the ACE and MSR DailyActivity3D datasets—show that the proposed joint approach leads to higher action recognition accuracy and equivalent or better summary quality than comparable approaches that perform these tasks separately.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2608530812",
    "type": "article"
  },
  {
    "title": "Can You See What I See? Quality-of-Experience Measurements of Mobile Live Video Broadcasting",
    "doi": "https://doi.org/10.1145/3165279",
    "publication_date": "2018-04-25",
    "publication_year": 2018,
    "authors": "Matti Siekkinen; Teemu Kämäräinen; Leonardo Favario; Enrico Masala",
    "corresponding_authors": "",
    "abstract": "Broadcasting live video directly from mobile devices is rapidly gaining popularity with applications like Periscope and Facebook Live. The quality of experience (QoE) provided by these services comprises many factors, such as quality of transmitted video, video playback stalling, end-to-end latency, and impact on battery life, and they are not yet well understood. In this article, we examine mainly the Periscope service through a comprehensive measurement study and compare it in some aspects to Facebook Live. We shed light on the usage of Periscope through analysis of crawled data and then investigate the aforementioned QoE factors through statistical analyses as well as controlled small-scale measurements using a couple of different smartphones and both versions, Android and iOS, of the two applications. We report a number of findings including the discrepancy in latency between the two most commonly used protocols, RTMP and HLS, surprising surges in bandwidth demand caused by the Periscope app’s chat feature, substantial variations in video quality, poor adaptation of video bitrate to available upstream bandwidth at the video broadcaster side, and significant power consumption caused by the applications.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2791416928",
    "type": "article"
  },
  {
    "title": "SKEPRID",
    "doi": "https://doi.org/10.1145/3243217",
    "publication_date": "2018-10-10",
    "publication_year": 2018,
    "authors": "Tuo Yu; Haiming Jin; Wai-Tian Tan; Klara Nahrstedt",
    "corresponding_authors": "",
    "abstract": "Currently, the surveillance camera-based person re-identification is still challenging because of diverse factors such as people’s changing poses and various illumination. The various poses make it hard to conduct feature matching across images, and the illumination changes make color-based features unreliable. In this article, we present SKEPRID, 1 a skeleton-based person re-identification method that handles strong pose and illumination changes jointly. To reduce the impacts of pose changes on re-identification, we estimate the joints’ positions of a person based on the deep learning technique and thus make it possible to extract features on specific body parts with high accuracy. Based on the skeleton information, we design a set of local color comparison-based cloth-type features, which are resistant to various lighting conditions. Moreover, to better evaluate SKEPRID, we build the PO8LI 2 dataset, which has large pose and illumination diversity. Our experimental results show that SKEPRID outperforms state-of-the-art approaches in the case of strong pose and illumination variation.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2897266728",
    "type": "article"
  },
  {
    "title": "Alone versus In-a-group",
    "doi": "https://doi.org/10.1145/3321509",
    "publication_date": "2019-05-31",
    "publication_year": 2019,
    "authors": "Wenxuan Mou; Hatice Güneş; Ioannis Patras",
    "corresponding_authors": "",
    "abstract": "Recognition and analysis of human affect has been researched extensively within the field of computer science in the past two decades. However, most of the past research in automatic analysis of human affect has focused on the recognition of affect displayed by people in individual settings and little attention has been paid to the analysis of the affect expressed in group settings. In this article, we first analyze the affect expressed by each individual in terms of arousal and valence dimensions in both individual and group videos and then propose methods to recognize the contextual information, i.e., whether a person is alone or in-a-group by analyzing their face and body behavioral cues. For affect analysis, we first devise affect recognition models separately in individual and group videos and then introduce a cross-condition affect recognition model that is trained by combining the two different types of data. We conduct a set of experiments on two datasets that contain both individual and group videos. Our experiments show that (1) the proposed Volume Quantized Local Zernike Moments Fisher Vector outperforms other unimodal features in affect analysis; (2) the temporal learning model, Long-Short Term Memory Networks, works better than the static learning model, Support Vector Machine; (3) decision fusion helps to improve affect recognition, indicating that body behaviors carry emotional information that is complementary rather than redundant to the emotion content in facial behaviors; and (4) it is possible to predict the context, i.e., whether a person is alone or in-a-group, using their non-verbal behavioral cues.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2951556150",
    "type": "article"
  },
  {
    "title": "Physiological Signals-based Emotion Recognition via High-order Correlation Learning",
    "doi": "https://doi.org/10.1145/3332374",
    "publication_date": "2019-11-30",
    "publication_year": 2019,
    "authors": "Junjie Zhu; Yuxuan Wei; Yifan Feng; Xibin Zhao; Yue Gao",
    "corresponding_authors": "",
    "abstract": "Emotion recognition by physiological signals is an effective way to discern the inner state of human beings and therefore has been widely adopted in many user-centered applications. The majority of current state-of-the-art methods focus on exploring relationship among emotion and physiological signals. Given some particular features of the natural process of emotional expression, it is still a challenging and urgent issue to efficiently combine such high-order correlations among multimodal physiological signals and subjects. To tackle the problem, a novel multi-hypergraph neural networks is proposed, in which one hypergraph is established with one type of physiological signals to formulate inter-subject correlations. Each one of the vertices in a hypergraph stands for one subject with a description of its related stimuli, and the complex correlations among the vertices can be formulated through hyperedges. With the multi-hypergraph structure of the subjects, emotion recognition is translated into classification of vertices in the multi-hypergraph structure. Experimental results with the DEAP dataset and ASCERTAIN dataset demonstrate that the proposed method outperforms the current state-of-the-art methods.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W3027131706",
    "type": "article"
  },
  {
    "title": "Protecting the Content Integrity of Digital Imagery with Fidelity Preservation",
    "doi": "https://doi.org/10.1145/2568224",
    "publication_date": "2014-04-01",
    "publication_year": 2014,
    "authors": "Marco Botta; Davide Cavagnino; Victor Pomponiu",
    "corresponding_authors": "",
    "abstract": "Fragile watermarking has attracted a lot of attention in the last decade. An interesting approach, presented in 2011 by Lin et al., results in very high quality of the watermarked images. However, after a thorough examination of the paper, a few improvements are proposed in our revised version of the algorithm in order to overcome some shortcomings. In particular, changes to the pseudocode and modifications to deal with pixel saturation are suggested, along with a way to improve the scheme security. Finally, a deeper analysis of the security is presented.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W1995708218",
    "type": "article"
  },
  {
    "title": "Socially-aware multimedia authoring",
    "doi": "https://doi.org/10.1145/2491893",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Dick C. A. Bulterman; Pablo César; Rodrigo Laiola Guimarães",
    "corresponding_authors": "",
    "abstract": "Creating compelling multimedia productions is a nontrivial task. This is as true for creating professional content as it is for nonprofessional editors. During the past 20 years, authoring networked content has been a part of the research agenda of the multimedia community. Unfortunately, authoring has been seen as an initial enterprise that occurs before ‘real’ content processing takes place. This limits the options open to authors and to viewers of rich multimedia content for creating and receiving focused, highly personal media presentations. This article reflects on the history of multimedia authoring. We focus on the particular task of supporting socially-aware multimedia , in which the relationships within particular social groups among authors and viewers can be exploited to create highly personal media experiences. We provide an overview of the requirements and characteristics of socially-aware multimedia authoring within the context of exploiting community content. We continue with a short historical perspective on authoring support for these types of situations. We then present an overview of a current system for supporting socially-aware multimedia authoring within the community content. We conclude with a discussion of the issues that we feel can provide a fruitful basis for future multimedia authoring support. We argue that providing support for socially-aware multimedia authoring can have a profound impact on the nature and architecture of the entire multimedia information processing pipeline.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2054535005",
    "type": "article"
  },
  {
    "title": "Designing Vibrotactile Codes to Communicate Verb Phrases",
    "doi": "https://doi.org/10.1145/2637289",
    "publication_date": "2014-10-01",
    "publication_year": 2014,
    "authors": "Manoj Prasad; Murat I. Russell; Tracy Hammond",
    "corresponding_authors": "",
    "abstract": "Soldiers, to guard themselves from enemy assault, have to maintain visual and auditory awareness of their environment. Their visual and auditory senses are thus saturated. This makes these channels less usable for communication. The tactile medium of communication with users is appropriate for displaying information in such situations. Research in interpersonal communication among soldiers shows that the most common form of communication between soldiers involves the use of verb phrases. In this article, we have developed a three-by-three tactile display and proposed a method for mapping the components of a verb phrase to two dimensions of tactile codes—shape and waveform. Perception of tactile codes by users depends on the ability of users to distinguish shape and waveform of the code. We have proposed a measure to rate the distinguish-ability of any two shapes and created a graph-based user-centric model using this measure to select distinguishable shapes from a set of all presentable shapes. We conducted two user studies to evaluate the ability of users to perceive tactile information. The results from our first study showed users' ability to perceive tactile shapes, tactile waveforms, and form verb phrases from tactile codes. The recognition accuracy and time taken to distinguish were better when the shapes were selected from the graph model than when shapes were chosen based on intuition. The second user study was conducted to test the performance of users while performing a primary visual task simultaneously with a secondary audio or haptic task. Users were more familiar with perceiving information from an auditory medium than from a haptic medium, which was reflected in their performance. Thus the performance of users in the primary visual task was better while using an audio medium of communication than while using a haptic medium of communication.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2076442894",
    "type": "article"
  },
  {
    "title": "Interactive Film Recombination",
    "doi": "https://doi.org/10.1145/3103241",
    "publication_date": "2017-08-12",
    "publication_year": 2017,
    "authors": "Fabrizio Guerrini; Nicola Adami; Sergio Benini; Alberto Piacenza; Julie Porteous; Marc Cavazza; Riccardo Leonardi",
    "corresponding_authors": "",
    "abstract": "In this article, we discuss an innovative media entertainment application called Interactive Movietelling. As an offspring of Interactive Storytelling applied to movies, we propose to integrate narrative generation through artificial intelligence (AI) planning with video processing and modeling to construct filmic variants starting from the baseline content. The integration is possible thanks to content description using semantic attributes pertaining to intermediate-level concepts shared between video processing and planning levels. The output is a recombination of segments taken from the input movie performed so as to convey an alternative plot. User tests on the prototype proved how promising Interactive Movietelling might be, even if it was designed at a proof of concept level. Possible improvements that are suggested here lead to many challenging research issues.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2725020571",
    "type": "article"
  },
  {
    "title": "Multimodal Multiplatform Social Media Event Summarization",
    "doi": "https://doi.org/10.1145/3115433",
    "publication_date": "2018-04-25",
    "publication_year": 2018,
    "authors": "Akanksha Tiwari; Christian von der Weth; Mohan Kankanhalli",
    "corresponding_authors": "",
    "abstract": "Social media platforms are turning into important news sources since they provide real-time information from different perspectives. However, high volume, dynamism, noise, and redundancy exhibited by social media data make it difficult to comprehend the entire content. Recent works emphasize on summarizing the content of either a single social media platform or of a single modality (either textual or visual). However, each platform has its own unique characteristics and user base, which brings to light different aspects of real-world events. This makes it critical as well as challenging to combine textual and visual data from different platforms. In this article, we propose summarization of real-world events with data stemming from different platforms and multiple modalities. We present the use of a Markov Random Fields based similarity measure to link content across multiple platforms. This measure also enables the linking of content across time, which is useful for tracking the evolution of long-running events. For the final content selection, summarization is modeled as a subset selection problem. To handle the complexity of the optimal subset selection, we propose the use of submodular objectives. Facets such as coverage, novelty, and significance are modeled as submodular objectives in a multimodal social media setting. We conduct a series of quantitative and qualitative experiments to illustrate the effectiveness of our approach compared to alternative methods.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2802396771",
    "type": "article"
  },
  {
    "title": "Efficient Video Encoding for Automatic Video Analysis in Distributed Wireless Surveillance Systems",
    "doi": "https://doi.org/10.1145/3226036",
    "publication_date": "2018-07-24",
    "publication_year": 2018,
    "authors": "Lingchao Kong; Rui Dai",
    "corresponding_authors": "",
    "abstract": "In many distributed wireless surveillance applications, compressed videos are used for performing automatic video analysis tasks. The accuracy of object detection, which is essential for various video analysis tasks, can be reduced due to video quality degradation caused by lossy compression. This article introduces a video encoding framework with the objective of boosting the accuracy of object detection for wireless surveillance applications. The proposed video encoding framework is based on systematic investigation of the effects of lossy compression on object detection. It has been found that current standardized video encoding schemes cause temporal domain fluctuation for encoded blocks in stable background areas and spatial texture degradation for encoded blocks in dynamic foreground areas of a raw video, both of which degrade the accuracy of object detection. Two measures, the sum-of-absolute frame difference (SFD) and the degradation of texture in 2D transform domain (TXD), are introduced to depict the temporal domain fluctuation and the spatial texture degradation in an encoded video, respectively. The proposed encoding framework is designed to suppress unnecessary temporal fluctuation in stable background areas and preserve spatial texture in dynamic foreground areas based on the two measures, and it introduces new mode decision strategies for both intra- and interframes to improve the accuracy of object detection while maintaining an acceptable rate distortion performance. Experimental results show that, compared with traditional encoding schemes, the proposed scheme improves the performance of object detection and results in lower bit rates and significantly reduced complexity with comparable quality in terms of PSNR and SSIM.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2884051051",
    "type": "article"
  },
  {
    "title": "ORL-SDN",
    "doi": "https://doi.org/10.1145/3219752",
    "publication_date": "2018-08-09",
    "publication_year": 2018,
    "authors": "Abdelhak Bentaleb; Ali C. Begen; Roger Zimmermann",
    "corresponding_authors": "",
    "abstract": "In designing an HTTP adaptive streaming (HAS) system, the bitrate adaptation scheme in the player is a key component to ensure a good quality of experience (QoE) for viewers. We propose a new online reinforcement learning optimization framework, called ORL-SDN, targeting HAS players running in a software-defined networking (SDN) environment. We leverage SDN to facilitate the orchestration of the adaptation schemes for a set of HAS players. To reach a good level of QoE fairness in a large population of players, we cluster them based on a perceptual quality index. We formulate the adaptation process as a Partially Observable Markov Decision Process and solve the per-cluster optimization problem using an online Q-learning technique that leverages model predictive control and parallelism via aggregation to avoid a per-cluster suboptimal selection and to accelerate the convergence to an optimum. This framework achieves maximum long-term revenue by selecting the optimal representation for each cluster under time-varying network conditions. The results show that ORL-SDN delivers substantial improvements in viewer QoE, presentation quality stability, fairness, and bandwidth utilization over well-known adaptation schemes.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2886723870",
    "type": "article"
  },
  {
    "title": "Joint Head Attribute Classifier and Domain-Specific Refinement Networks for Face Alignment",
    "doi": "https://doi.org/10.1145/3241059",
    "publication_date": "2018-10-10",
    "publication_year": 2018,
    "authors": "Junfeng Zhang; Haifeng Hu",
    "corresponding_authors": "",
    "abstract": "In this article, a two-stage refinement network is proposed for facial landmarks detection on unconstrained conditions. Our model can be divided into two modules, namely the Head Attribude Classifier (HAC) module and the Domain-Specific Refinement (DSR) module. Given an input facial image, HAC adopts multi-task learning mechanism to detect the head pose and obtain an initial shape. Based on the obtained head pose, DSR designs three different CNN-based refinement networks trained by specific domain, respectively, and automatically selects the most approximate network for the landmarks refinement. Different from existing two-stage models, HAC combines head pose prediction with facial landmarks estimation to improve the accuracy of head pose prediction, as well as obtaining a robust initial shape. Moreover, an adaptive sub-network training strategy applied in the DSR module can effectively solve the issue of traditional multi-view methods that an improperly selected sub-network may result in alignment failure. The extensive experimental results on two public datasets, AFLW and 300W, confirm the validity of our model.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2896912191",
    "type": "article"
  },
  {
    "title": "Robust Electric Network Frequency Estimation with Rank Reduction and Linear Prediction",
    "doi": "https://doi.org/10.1145/3241058",
    "publication_date": "2018-10-23",
    "publication_year": 2018,
    "authors": "Xiaodan Lin; Xiangui Kang",
    "corresponding_authors": "",
    "abstract": "This article deals with the problem of Electric Network Frequency (ENF) estimation where Signal to Noise Ratio (SNR) is an essential challenge. By exploiting the low-rank structure of the ENF signal from the audio spectrogram, we propose an approach based on robust principle component analysis to get rid of the interference from speech contents and some of the background noise, which in our case can be regarded as sparse in nature. Weighted linear prediction is enforced on the low-rank signal subspace to gain accurate ENF estimation. The performance of the proposed scheme is analyzed and evaluated as a function of SNR, and the Cramér-Rao Lower Bound (CRLB) is approached at an SNR level above -10 dB. Experiments on real datasets have demonstrated the advantages of the proposed method over state-of-the-art work in terms of estimation accuracy. Specifically, the proposed scheme can effectively capture the ENF fluctuations along the time axis using small numbers of signal observations while preserving sufficient frequency precision.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2897553423",
    "type": "article"
  },
  {
    "title": "Orchestrating Caching, Transcoding and Request Routing for Adaptive Video Streaming Over ICN",
    "doi": "https://doi.org/10.1145/3289184",
    "publication_date": "2019-01-23",
    "publication_year": 2019,
    "authors": "Han Hu; Yichao Jin; Yonggang Wen; Cédric Westphal",
    "corresponding_authors": "",
    "abstract": "Information-centric networking (ICN) has been touted as a revolutionary solution for the future of the Internet, which will be dominated by video traffic. This work investigates the challenge of distributing video content of adaptive bitrate (ABR) over ICN. In particular, we use the in-network caching capability of ICN routers to serve users; in addition, with the help of named function, we enable ICN routers to transcode videos to lower-bitrate versions to improve the cache hit ratio. Mathematically, we formulate this design challenge into a constrained optimization problem, which aims to maximize the cache hit ratio for service providers and minimize the service delay for endusers. We design a two-step iterative algorithm to find the optimum. First, given a content management scheme, we minimize the service delay via optimally configuring the routing scheme. Second, we maximize the cache hits for a given routing policy. Finally, we rigorously prove its convergence. Through extensive simulations, we verify the convergence and the performance gains over other algorithms. We also find that more resources should be allocated to ICN routers with a heavier request rate, and the routing scheme favors the shortest path to schedule more traffic.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2912480658",
    "type": "article"
  },
  {
    "title": "Design, Large-Scale Usage Testing, and Important Metrics for Augmented Reality Gaming Applications",
    "doi": "https://doi.org/10.1145/3311748",
    "publication_date": "2019-05-31",
    "publication_year": 2019,
    "authors": "Roberto Pierdicca; Emanuele Frontoni; Primo Zingaretti; Adriano Mancini; Jelena Loncarski; Marina Paolanti",
    "corresponding_authors": "",
    "abstract": "Augmented Reality (AR) offers the possibility to enrich the real world with digital mediated content, increasing in this way the quality of many everyday experiences. While in some research areas such as cultural heritage, tourism, or medicine there is a strong technological investment, AR for game purposes struggles to become a widespread commercial application. In this article, a novel framework for AR kid games is proposed, already developed by the authors for other AR applications such as Cultural Heritage and Arts. In particular, the framework includes different layers such as the development of a series of AR kid puzzle games in an intermediate structure which can be used as a standard for different applications development, the development of a smart configuration tool, together with general guidelines and long-life usage tests and metrics. The proposed application is designed for augmenting the puzzle experience, but can be easily extended to other AR gaming applications. Once the user has assembled the real puzzle, AR functionality within the mobile application can be unlocked, bringing to life puzzle characters, creating a seamless game that merges AR interactions with the puzzle reality. The main goals and benefits of this framework can be seen in the development of a novel set of AR tests and metrics in the pre-release phase (in order to help the commercial launch and developers), and in the release phase by introducing the measures for long-life app optimization, usage tests and hint on final users together with a measure to design policy, providing a method for automatic testing of quality and popularity improvements. Moreover, smart configuration tools, as part of the general framework, enabling multi-app and eventually also multi-user development, have been proposed, facilitating the serialization of the applications. Results were obtained from a large-scale user test with about 4 million users on a set of eight gaming applications, providing the scientific community a workflow for implicit quantitative analysis in AR gaming. Different data analytics developed on the data collected by the framework prove that the proposed approach is affordable and reliable for long-life testing and optimization.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2950030403",
    "type": "article"
  },
  {
    "title": "QoE for Mobile Clients with Segment-aware Rate Adaptation Algorithm (SARA) for DASH Video Streaming",
    "doi": "https://doi.org/10.1145/3311749",
    "publication_date": "2019-05-31",
    "publication_year": 2019,
    "authors": "Hema Kumar Yarnagula; Parikshit Juluri; Sheyda Kiani Mehr; Venkatesh Tamarapalli; Deep Medhi",
    "corresponding_authors": "",
    "abstract": "Dynamic adaptive streaming over HTTP (DASH) is widely used for video streaming on mobile devices. Ensuring a good quality of experience (QoE) for mobile video streaming is essential, as it severely impacts both the network and content providers’ revenue. Thus, a good rate adaptation algorithm at the client end that provides high QoE is critically important. Recently, a segment size-aware rate adaptation (SARA) algorithm was proposed for DASH clients. However, its performance on mobile clients has not been investigated so far. The main contributions of this article are twofold: (1) We discuss SARA’s implementation for mobile clients to improve the QoE in mobile video streaming, one that accurately predicts the download time for the next segment and makes an informed bitrate selection, and (2) we developed a new parametric QoE model to compute a cumulative score that helps in fair comparison of different adaptation algorithms. Based on our subjective and objective evaluation, we observed that SARA for mobile clients outperforms others by 17% on average, in terms of the Mean Opinion Score, while achieving, on average, a 76% improvement in terms of the interruption ratio. The score obtained from our new parametric QoE model also demonstrates that the SARA algorithm for mobile clients gives a better QoE among all the algorithms.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2952356233",
    "type": "article"
  },
  {
    "title": "Egocentric Hand Detection Via Dynamic Region Growing",
    "doi": "https://doi.org/10.1145/3152129",
    "publication_date": "2017-12-13",
    "publication_year": 2017,
    "authors": "Shao Huang; Weiqiang Wang; Shengfeng He; Rynson W. H. Lau",
    "corresponding_authors": "",
    "abstract": "Egocentric videos, which mainly record the activities carried out by the users of wearable cameras, have drawn much research attention in recent years. Due to its lengthy content, a large number of ego-related applications have been developed to abstract the captured videos. As the users are accustomed to interacting with the target objects using their own hands, while their hands usually appear within their visual fields during the interaction, an egocentric hand detection step is involved in tasks like gesture recognition, action recognition, and social interaction understanding. In this work, we propose a dynamic region-growing approach for hand region detection in egocentric videos, by jointly considering hand-related motion and egocentric cues. We first determine seed regions that most likely belong to the hand, by analyzing the motion patterns across successive frames. The hand regions can then be located by extending from the seed regions, according to the scores computed for the adjacent superpixels. These scores are derived from four egocentric cues: contrast, location, position consistency, and appearance continuity. We discuss how to apply the proposed method in real-life scenarios, where multiple hands irregularly appear and disappear from the videos. Experimental results on public datasets show that the proposed method achieves superior performance compared with the state-of-the-art methods, especially in complicated scenarios.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2963592825",
    "type": "article"
  },
  {
    "title": "Stochastic Optimization for Green Multimedia Services in Dense 5G Networks",
    "doi": "https://doi.org/10.1145/3328996",
    "publication_date": "2019-08-31",
    "publication_year": 2019,
    "authors": "Tengfei Cao; Changqiao Xu; Mu Wang; Zhongbai Jiang; Xingyan Chen; Lujie Zhong; Luigi Alfredo Grieco",
    "corresponding_authors": "",
    "abstract": "The manyfold capacity magnification promised by dense 5G networks will make possible the provisioning of broadband multimedia services, including virtual reality, augmented reality, and mobile immersive video, to name a few. These new applications will coexist with classic ones and contribute to the exponential growth of multimedia services in mobile networks. At the same time, the different requirements of past and old services pose new challenges to the effective usage of 5G resources. In response to these challenges, a novel Stochastic Optimization framework for Green Multimedia Services named SOGMS is proposed herein that targets the maximization of system throughput and the minimization of energy consumption in data delivery. In particular, Lyapunov optimization is leveraged to face this optimization objective, which is formulated and decomposed into three tractable subproblems. For each subproblem, a distinct algorithm is conceived, namely quality of experience--based admission control, cooperative resource allocation, and multimedia services scheduling. Finally, extensive simulations are carried out to evaluate the proposed method against state-of-art solutions in dense 5G networks.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2972437493",
    "type": "article"
  },
  {
    "title": "Embedding Distortion Analysis in Wavelet-domain Watermarking",
    "doi": "https://doi.org/10.1145/3357333",
    "publication_date": "2019-11-30",
    "publication_year": 2019,
    "authors": "Deepayan Bhowmik; Charith Abhayaratne",
    "corresponding_authors": "",
    "abstract": "Imperceptibility and robustness are two complementary fundamental requirements of any watermarking algorithm. Low-strength watermarking yields high imperceptibility, but exhibits poor robustness. High-strength watermarking schemes achieve good robustness but often infuse distortions resulting in poor visual quality in host images. This article analyses the embedding distortion for wavelet-based watermarking schemes. We derive the relationship between distortion, measured in mean square error (MSE), and the watermark embedding modification and propose the linear proportionality between MSE and the sum of energy of the selected wavelet coefficients for watermark embedding modification. The initial proposition assumes the orthonormality of discrete wavelet transform. It is further extended for non-orthonormal wavelet kernels using a weighting parameter that follows the energy conservation theorems in wavelet frames. The proposed analysis is verified by experimental results for both non-blind and blind watermarking schemes. Such a model is useful to find the optimum input parameters, including the wavelet kernel, coefficient selection, and subband choices for wavelet domain image watermarking.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2972560931",
    "type": "article"
  },
  {
    "title": "Image/Video Restoration via Multiplanar Autoregressive Model and Low-Rank Optimization",
    "doi": "https://doi.org/10.1145/3341728",
    "publication_date": "2019-11-30",
    "publication_year": 2019,
    "authors": "Mading Li; Jiaying Liu; Xiaoyan Sun; Zhiwei Xiong",
    "corresponding_authors": "",
    "abstract": "In this article, we introduce an image/video restoration approach by utilizing the high-dimensional similarity in images/videos. After grouping similar patches from neighboring frames, we propose to build a multiplanar autoregressive (AR) model to exploit the correlation in cross-dimensional planes of the patch group, which has long been neglected by previous AR models. To further utilize the nonlocal self-similarity in images/videos, a joint multiplanar AR and low-rank based approach is proposed (MARLow) to reconstruct patch groups more effectively. Moreover, for video restoration, the temporal smoothness of the restored video is constrained by the Markov random field (MRF), where MRF encodes a priori knowledge about consistency of patches from neighboring frames. Specifically, we treat different restoration results (from different patch groups) of a certain patch as labels of an MRF, and temporal consistency among these restored patches is imposed. The proposed method is also suitable for other restoration applications such as interpolation and text removal. Extensive experimental results demonstrate that the proposed approach obtains encouraging performance comparing with state-of-the-art methods.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2995368614",
    "type": "article"
  },
  {
    "title": "Adaptive Chunklets and AQM for Higher-Performance Content Streaming",
    "doi": "https://doi.org/10.1145/3344381",
    "publication_date": "2019-11-30",
    "publication_year": 2019,
    "authors": "Jonathan Kua; Grenville Armitage; Philip Branch; Jason But",
    "corresponding_authors": "",
    "abstract": "Commercial streaming services such as Netflix and YouTube use proprietary HTTP-based adaptive streaming (HAS) techniques to deliver content to consumers worldwide. MPEG recently developed Dynamic Adaptive Streaming over HTTP (DASH) as a unifying standard for HAS-based streaming. In DASH systems, streaming clients employ adaptive bitrate (ABR) algorithms to maximise user Quality of Experience (QoE) under variable network conditions. In a typical Internet-enabled home, video streams have to compete with diverse application flows for the last-mile Internet Service Provider (ISP) bottleneck capacity. Under such circumstances, ABR algorithms will only act upon the fraction of the network capacity that is available, leading to possible QoE degradation. We have previously explored chunklets as an approach orthogonal to ABR algorithms, which uses parallel connections for intra-video chunk retrieval. Chunklets effectively make more bandwidth available for ABR algorithms in the presence of cross-traffic, especially in environments where Active Queue Management (AQM) schemes such as Proportional Integral controller Enhanced (PIE) and FlowQueue-Controlled Delay (FQ-CoDel) are deployed. However, chunklets consume valuable server/middlebox resources which typically handle hundreds of thousands of requests/connections per second. In this article, we propose ‘adaptive chunklets’ -- a novel chunklet enhancement that dynamically tunes the number of concurrent connections. We demonstrate that the combination of adaptive chunklets and FQ-CoDel is the most effective strategy. Our experiments show that adaptive chunklets can reduce the number of connections by almost 30% and consume almost 8% less bandwidth than fixed chunklets while providing the same QoE.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2995824708",
    "type": "article"
  },
  {
    "title": "Cell Nuclei Classification in Histopathological Images using Hybrid O <sub>L</sub> ConvNet",
    "doi": "https://doi.org/10.1145/3345318",
    "publication_date": "2020-01-31",
    "publication_year": 2020,
    "authors": "Suvidha Tripathi; Satish Kumar Singh",
    "corresponding_authors": "",
    "abstract": "Computer-aided histopathological image analysis for cancer detection is a major research challenge in the medical domain. Automatic detection and classification of nuclei for cancer diagnosis impose a lot of challenges in developing state-of-the-art algorithms due to the heterogeneity of cell nuclei and dataset variability. Recently, a multitude of classification algorithms have used complex deep learning models for their dataset. However, most of these methods are rigid, and their architectural arrangement suffers from inflexibility and non-interpretability. In this research article, we have proposed a hybrid and flexible deep learning architecture O L ConvNet that integrates the interpretability of traditional object-level features and generalization of deep learning features by using a shallower Convolutional Neural Network (CNN) named as CNN 3L . CNN 3L reduces the training time by training fewer parameters and hence eliminating space constraints imposed by deeper algorithms. We used F1-score and multiclass Area Under the Curve (AUC) performance parameters to compare the results. To further strengthen the viability of our architectural approach, we tested our proposed methodology with state-of-the-art deep learning architectures AlexNet, VGG16, VGG19, ResNet50, InceptionV3, and DenseNet121 as backbone networks. After a comprehensive analysis of classification results from all four architectures, we observed that our proposed model works well and performs better than contemporary complex algorithms.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W3012315701",
    "type": "article"
  },
  {
    "title": "Pulmonary Nodule Detection Based on ISODATA-Improved Faster RCNN and 3D-CNN with Focal Loss",
    "doi": "https://doi.org/10.1145/3365445",
    "publication_date": "2020-01-31",
    "publication_year": 2020,
    "authors": "Chao Tong; Baoyu Liang; Mengze Zhang; Rongshan Chen; Arun Kumar Sangaiah; Zhigao Zheng; Tao Wan; Chenyang Yue; Xinyi Yang",
    "corresponding_authors": "",
    "abstract": "The early diagnosis of pulmonary cancer can significantly improve the survival rate of patients, where pulmonary nodules detection in computed tomography images plays an important role. In this article, we propose a novel pulmonary nodule detection system based on convolutional neural networks (CNN). Our system consists of two stages, pulmonary nodule candidate detection and false positive reduction. For candidate detection, we introduce Iterative Self-Organizing Data Analysis Techniques Algorithm (ISODATA) to Faster Region-based Convolutional Neural Network (Faster R-CNN) model. For false positive reduction, a three-dimensional convolutional neural network (3D-CNN) is employed to completely utilize the three-dimensional nature of CT images. In this network, Focal Loss is used to solve the class imbalance problem in this task. Experiments were conducted on LUNA16 dataset. The results show the preferable performance of the proposed system and the effectiveness of using ISODATA and Focal loss in pulmonary nodule detection is proved.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W3016536844",
    "type": "article"
  },
  {
    "title": "Intelligent Classification and Analysis of Essential Genes Using Quantitative Methods",
    "doi": "https://doi.org/10.1145/3343856",
    "publication_date": "2020-01-31",
    "publication_year": 2020,
    "authors": "Ranjeet Kumar Rout; Sk. Sarif Hassan; Sanchit Sindhwani; Hari Mohan Pandey; Saiyed Umer",
    "corresponding_authors": "",
    "abstract": "Essential genes are considered to be the genes required to sustain life of different organisms. These genes encode proteins that maintain central metabolism, DNA replications, translation of genes, and basic cellular structure, and mediate the transport process within and out of the cell. The identification of essential genes is one of the essential problems in computational genomics. In this present study, to discriminate essential genes from other genes from a non-biologists perspective, the purine and pyrimidine distribution over the essential genes of four exemplary species, namely Homo sapiens , Arabidopsis thaliana , Drosophila melanogaster , and Danio rerio are thoroughly experimented using some quantitative methods. Moreover, the Indigent classification method has also been deployed for classification on the essential genes of the said species. Based on Shannon entropy, fractal dimension, Hurst exponent, and purine and pyrimidine bases distribution, 10 different clusters have been generated for the essential genes of the four species. Some proximity results are also reported herewith for the clusters of the essential genes.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W3016692474",
    "type": "article"
  },
  {
    "title": "Listen, Look, and Find the One",
    "doi": "https://doi.org/10.1145/3380549",
    "publication_date": "2020-05-22",
    "publication_year": 2020,
    "authors": "Xiao Wang; Wu Liu; Jun Chen; Xiaobo Wang; Chenggang Yan; Tao Mei",
    "corresponding_authors": "",
    "abstract": "Person search with one portrait, which attempts to search the targets in arbitrary scenes using one portrait image at a time, is an essential yet unexplored problem in the multimedia field. Existing approaches, which predominantly depend on the visual information of persons, cannot solve problems when there are variations in the person’s appearance caused by complex environments and changes in pose, makeup, and clothing. In contrast to existing methods, in this article, we propose an associative multimodality index for person search with face, body, and voice information. In the offline stage, an associative network is proposed to learn the relationships among face, body, and voice information. It can adaptively estimate the weights of each embedding to construct an appropriate representation. The multimodality index can be built by using these representations, which exploit the face and voice as long-term keys and the body appearance as a short-term connection. In the online stage, through the multimodality association in the index, we can retrieve all targets depending only on the facial features of the query portrait. Furthermore, to evaluate our multimodality search framework and facilitate related research, we construct the Cast Search in Movies with Voice (CSM-V) dataset, a large-scale benchmark that contains 127K annotated voices corresponding to tracklets from 192 movies. According to extensive experiments on the CSM-V dataset, the proposed multimodality person search framework outperforms the state-of-the-art methods.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W3049356352",
    "type": "article"
  },
  {
    "title": "Action Recognition Using Form and Motion Modalities",
    "doi": "https://doi.org/10.1145/3350840",
    "publication_date": "2020-01-31",
    "publication_year": 2020,
    "authors": "Quanling Meng; Heyan Zhu; Weigang Zhang; Xuefeng Piao; Aijie Zhang",
    "corresponding_authors": "",
    "abstract": "Action recognition has attracted increasing interest in computer vision due to its potential applications in many vision systems. One of the main challenges in action recognition is to extract powerful features from videos. Most existing approaches exploit either hand-crafted techniques or learning-based methods to extract features from videos. However, these methods mainly focus on extracting the dynamic motion features, which ignore the static form features. Therefore, these methods cannot fully capture the underlying information in videos accurately. In this article, we propose a novel feature representation method for action recognition, which exploits hierarchical sparse coding to learn the underlying features from videos. The learned features characterize the form and motion simultaneously and therefore provide more accurate and complete feature representation. The learned form and motion features are considered as two modalities, which are used to represent both the static and motion features. These modalities are further encoded into a global representation via a pairwise dictionary learning and then fed to an SVM classifier for action classification. Experimental results on several challenging datasets validate that the proposed method is superior to several state-of-the-art methods.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W3090628358",
    "type": "article"
  },
  {
    "title": "MMFN",
    "doi": "https://doi.org/10.1145/3410439",
    "publication_date": "2020-11-30",
    "publication_year": 2020,
    "authors": "Weizhi Nie; Qi Liang; Yixin Wang; Xing Wei; Yuting Su",
    "corresponding_authors": "",
    "abstract": "In recent years, research into 3D shape recognition in the field of multimedia and computer vision has attracted wide attention. With the rapid development of deep learning, various deep models have achieved state-of-the-art performance based on different representations. There are many modalities for representing a 3D model, such as point cloud, multiview, and panorama view. Deep learning models based on these different modalities have different concerns, and all of them have achieved high performance for 3D shape recognition. However, all of these methods ignore the multimodality information in conditions where the same 3D model is represented by different modalities. Thus, we can obtain a better descriptor by guiding the training to consider these multiple representations. In this article, we propose MMFN, a novel multimodal fusion network for 3D shape recognition that employs correlations between the different modalities to generate a fused descriptor, which is more robust. In particular, we design two novel loss functions to help the model learn the correlation information during training. The first is correlation loss, which focuses on the correlations among different descriptors generated from different structures. This approach reduces the training time and improves the robustness of the fused descriptor of the 3D model. The second is instance loss, which preserves the independence of each modality and utilizes feature differentiation to guide model learning during the training process. More specifically, we use the weighted fusion method, which applies statistical methods to obtain robust descriptors that maximize the advantages of the information from the different modalities. We evaluated the proposed method on the ModelNet40 and ShapeNetCore55 datasets for 3D shape classification and retrieval tasks. The experimental results and comparisons with state-of-the-art methods demonstrate the superiority of our approach.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W3111354616",
    "type": "article"
  },
  {
    "title": "Learning a Deep Agent to Predict Head Movement in 360-Degree Images",
    "doi": "https://doi.org/10.1145/3410455",
    "publication_date": "2020-11-30",
    "publication_year": 2020,
    "authors": "Yucheng Zhu; Guangtao Zhai; Xiongkuo Min; Jiantao Zhou",
    "corresponding_authors": "",
    "abstract": "Virtual reality adequately stimulates senses to trick users into accepting the virtual environment. To create a sense of immersion, high-resolution images are required to satisfy human visual system, and low latency is essential for smooth operations, which put great demands on data processing and transmission. Actually, when exploring in the virtual environment, viewers only perceive the content in the current field of view. Therefore, if we can predict the head movements that are important behaviors of viewers, more processing resources can be allocated to the active field of view. In this article, we propose a model to predict the trajectory of head movement. Deep reinforcement learning is employed to mimic the decision making. In our framework, to characterize each state, features for viewport images are extracted by convolutional neural networks. In addition, the spherical coordinate maps and visited maps are generated for each viewport image, which facilitate the multiple dimensions of the state information by considering the impact of historical head movement and position information. To ensure the accurate simulation of visual behaviors during the watching of panoramas, we stipulate that the model imitates the behaviors of human demonstrators. To allow the model to generalize to more conditions, the intrinsic motivation is employed to guide the agent’s action toward reducing uncertainty, which can enhance robustness during the exploration. The experimental results demonstrate the effectiveness of the proposed stepwise head movement predictor.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W3112280458",
    "type": "article"
  },
  {
    "title": "MV2Flow",
    "doi": "https://doi.org/10.1145/3422360",
    "publication_date": "2020-10-31",
    "publication_year": 2020,
    "authors": "Hezhen Hu; Wengang Zhou; Xingze Li; Ning Yan; Houqiang Li",
    "corresponding_authors": "",
    "abstract": "In video action recognition, motion is a very crucial clue, which is usually represented by optical flow. However, optical flow is computationally expensive to obtain, which becomes the bottleneck for the efficiency of traditional action recognition algorithms. In this article, we propose a network called MV2Flow to learn motion representation efficiently from the signals in the compressed domain. To learn the network, three losses are defined. First, we select the classical TV-L1 flow as proxy ground truth to guide the learning. Besides, an unsupervised image reconstruction loss is proposed to further refine it. Moreover, toward the task of action recognition, the above two losses are combined with a motion content loss. To evaluate our approach, extensive experiments on two benchmark datasets UCF-101 and HMDB-51 are conducted. The motion representation generated with our MV2Flow has shown comparable classification performance on action recognition with TV-L1 flow, while operating at an over 200× faster speed. Based on our MV2Flow and 2D-CNN-based network, we have achieved state-of-the-art performance in the compressed domain. With 3D-CNN-based network, we also achieve comparable accuracy with higher inference speed than methods in the decoded domain setting.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W3115205610",
    "type": "article"
  },
  {
    "title": "QoE-Based Cross-Layer Optimization for Uplink Video Transmission",
    "doi": "https://doi.org/10.1145/2801124",
    "publication_date": "2015-08-24",
    "publication_year": 2015,
    "authors": "Ali El Essaili; Zibin Wang; Eckehard Steinbach; Liang Zhou",
    "corresponding_authors": "",
    "abstract": "We study the problem of resource-efficient uplink distribution of user-generated video content over fourth-generation mobile networks. This is challenged by (1) the capacity-limited and time-variant uplink channel, (2) the resource-hungry upstreamed videos and their dynamically changing complexity, and (3) the different playout times of the video consumers. To address these issues, we propose a systematic approach for quality-of-experience (QoE)-based resource optimization and uplink transmission of multiuser generated video content. More specifically, we present an analytical model for distributed scalable video transmission at the mobile producers which considers these constraints. This is complemented by a multiuser cross-layer optimizer in the mobile network which determines the transmission capacity for each mobile terminal under current cell load and radio conditions. Both optimal and low-complexity solutions are presented. Simulation results for LTE uplink transmission show that significant gains in perceived video quality can be achieved by our cross-layer resource optimization scheme. In addition, the distributed optimization at the mobile producers can further improve the user experience across the different types of video consumers.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W324615742",
    "type": "article"
  },
  {
    "title": "QoE-Based SVC Layer Dropping in LTE Networks Using Content-Aware Layer Priorities",
    "doi": "https://doi.org/10.1145/2754167",
    "publication_date": "2015-08-24",
    "publication_year": 2015,
    "authors": "Bo Fu; Dirk Staehle; Gerald Kunzmann; Eckehard Steinbach; Wolfgang Kellerer",
    "corresponding_authors": "",
    "abstract": "The increasing popularity of mobile video streaming applications has led to a high volume of video traffic in mobile networks. As the base station, for instance, the eNB in LTE networks, has limited physical resources, it can be overloaded by this traffic. This problem can be addressed by using Scalable Video Coding (SVC), which allows the eNB to drop layers of the video streams to dynamically adapt the bitrate. The impact of bitrate adaptation on the Quality of Experience (QoE) for the users depends on the content characteristics of videos. As the current mobile network architectures do not support the eNB in obtaining video content information, QoE optimization schemes with explicit signaling of content information have been proposed. These schemes, however, require the eNB or a specific optimization module to process the video content on the fly in order to extract the required information. This increases the computation and signaling overhead significantly, raising the OPEX for mobile operators. To address this issue, in this article, a content-aware (CA) priority marking and layer dropping scheme is proposed. The CA priority indicates a transmission order for the layers of all transmitted videos across all users, resulting from a comparison of their utility versus rate characteristics. The CA priority values can be determined at the P-GW on the fly, allowing mobile operators to control the priority marking process. Alternatively, they can be determined offline at the video servers, avoiding real-time computation in the core network. The eNB can perform content-aware SVC layer dropping using only the priority values. No additional content processing is required. The proposed scheme is lightweight both in terms of architecture and computation. The improvement in QoE is substantial and very close to the performance obtained with the computation and signaling-intensive QoE optimization schemes.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W1148380748",
    "type": "article"
  },
  {
    "title": "User Preferences Modeling and Learning for Pleasing Photo Collage Generation",
    "doi": "https://doi.org/10.1145/2801126",
    "publication_date": "2015-08-24",
    "publication_year": 2015,
    "authors": "Simone Bianco; Gianluigi Ciocca",
    "corresponding_authors": "",
    "abstract": "In this article, we consider how to automatically create pleasing photo collages created by placing a set of images on a limited canvas area. The task is formulated as an optimization problem. Differently from existing state-of-the-art approaches, we here exploit subjective experiments to model and learn pleasantness from user preferences. To this end, we design an experimental framework for the identification of the criteria that need to be taken into account to generate a pleasing photo collage. Five different thematic photo datasets are used to create collages using state-of-the-art criteria. A first subjective experiment where several subjects evaluated the collages, emphasizes that different criteria are involved in the subjective definition of pleasantness. We then identify new global and local criteria and design algorithms to quantify them. The relative importance of these criteria are automatically learned by exploiting the user preferences, and new collages are generated. To validate our framework, we performed several psycho-visual experiments involving different users. The results shows that the proposed framework allows to learn a novel computational model which effectively encodes an inter-user definition of pleasantness. The learned definition of pleasantness generalizes well to new photo datasets of different themes and sizes not used in the learning. Moreover, compared with two state-of-the-art approaches, the collages created using our framework are preferred by the majority of the users.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W1895638139",
    "type": "article"
  },
  {
    "title": "Fast Near-Duplicate Image Detection Using Uniform Randomized Trees",
    "doi": "https://doi.org/10.1145/2602186",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "Yanqiang Lei; Guoping Qiu; Ligang Zheng; Jiwu Huang",
    "corresponding_authors": "",
    "abstract": "Indexing structure plays an important role in the application of fast near-duplicate image detection, since it can narrow down the search space. In this article, we develop a cluster of uniform randomized trees (URTs) as an efficient indexing structure to perform fast near-duplicate image detection. The main contribution in this article is that we introduce “uniformity” and “randomness” into the indexing construction. The uniformity requires classifying the object images into the same scale subsets. Such a decision makes good use of the two facts in near-duplicate image detection, namely: (1) the number of categories is huge; (2) a single category usually contains only a small number of images. Therefore, the uniform distribution is very beneficial to narrow down the search space and does not significantly degrade the detection accuracy. The randomness is embedded into the generation of feature subspace and projection direction, improveing the flexibility of indexing construction. The experimental results show that the proposed method is more efficient than the popular locality-sensitive hashing and more stable and flexible than the traditional KD-tree.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W1963629073",
    "type": "article"
  },
  {
    "title": "Integration of Multisensorial Stimuli and Multimodal Interaction in a Hybrid 3DTV System",
    "doi": "https://doi.org/10.1145/2617992",
    "publication_date": "2014-10-01",
    "publication_year": 2014,
    "authors": "Francisco Luque; Iris Galloso; Claudio Feijóo; Carlos A. Martín; Guillermo Cisneros",
    "corresponding_authors": "",
    "abstract": "This article proposes the integration of multisensorial stimuli and multimodal interaction components into a sports multimedia asset under two dimensions: immersion and interaction . The first dimension comprises a binaural audio system and a set of sensory effects synchronized with the audiovisual content, whereas the second explores interaction through the insertion of interactive 3D objects into the main screen and on-demand presentation of additional information in a second touchscreen. We present an end-to-end solution integrating these components into a hybrid (internet-broadcast) television system using current 3DTV standards. Results from an experimental study analyzing the perceived quality of these stimuli and their influence on the Quality of Experience are presented.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W1991318935",
    "type": "article"
  },
  {
    "title": "PSNController",
    "doi": "https://doi.org/10.1145/2808206",
    "publication_date": "2015-10-21",
    "publication_year": 2015,
    "authors": "Sixuan Ma; Zheng Yan",
    "corresponding_authors": "",
    "abstract": "Pervasive social networking (PSN) supports online and instant social activities and communications in a universal and pervasive manner on the basis of heterogeneous networks. However, at the same time, when mobile users expect useful and valuable contents via PSN, they may also receive unwanted, unexpected, or even malicious contents. These contents may intrude user devices, occupy device memories, and irritate mobile users. Unwanted content control in PSN has become a crucial issue that impacts the success of PSN usage. Nowadays, the literature still lacks a robust and generic unwanted content control system that can be practically applied. In this article, we present the design and implementation of PSNController, an unwanted content control system in PSN based on trust management. We evaluate the system performance under a variety of intrusions and attacks. The result shows the system is effective with regard to accuracy, efficiency, and robustness. It can control unwanted contents in PSN according to trust evaluation. We further study user acceptance on PSNController prototype system based on a small-scale user study. We receive sound user feedback on PSNController with regard to perceived ease of use, perceived usefulness, interface design, playfulness, and acceptance attitude.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2060453042",
    "type": "article"
  },
  {
    "title": "High performance many-to-many intranet screen sharing with DisplayCast",
    "doi": "https://doi.org/10.1145/2534328",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "Surendar Chandra; John Boreczky; Lawrence A. Rowe",
    "corresponding_authors": "",
    "abstract": "DisplayCast is a many to many Intranet screen sharing system. Its screen capture mechanism creates a sequence of pixmap images of the screen updates. Prior systems that used a similar approach were designed to operate over constrained wide-area networks and did not exploit the Intranet network conditions to achieve high capture rates. First we empirically analyzed the screen contents for a variety of scenarios. We showed that screen updates were sporadic with long periods of inactivity. When active, screens were updated at far higher rates than was supported by earlier systems. The mismatch was pronounced for interactive scenarios. Even during active screen updates, the number of updated pixels were frequently small. We showed that crucial information can be lost if individual updates were merged. When the available system resources could not support high capture rates, we showed ways in which updates can be effectively collapsed. Next, we investigate compression mechanisms for streaming these updates. Even while using a hardware encoder, lossy compressors such as H.264 were unable to sustain high frame rates. Though Zlib lossless compression operated within the latency and compression rate requirements, the compression efficiency was poor. By analyzing the screen pixels, we developed a practical transformation that significantly improved compression rates. DisplayCast incorporates these observations. It shares the processor and network resources required for screen capture, compression and transmission with host applications whose output needs to be shared. DisplayCast is agile and uses faster processing capability to achieve even higher performance. Our system components operate natively in Windows 7, Mac OS X and iOS and is deployed in a production setting. DisplayCast is released under a New BSD License.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2095514649",
    "type": "article"
  },
  {
    "title": "Enhanced Reweighted MRFs for Efficient Fashion Image Parsing",
    "doi": "https://doi.org/10.1145/2890104",
    "publication_date": "2016-03-08",
    "publication_year": 2016,
    "authors": "Qiong Wu; Pierre Boulanger",
    "corresponding_authors": "",
    "abstract": "Previous image parsing methods usually model the problem in a conditional random field which describes a statistical model learned from a training dataset and then processes a query image using the conditional probability. However, for clothing images, fashion items have a large variety of layering and configuration, and it is hard to learn a certain statistical model of features that apply to general cases. In this article, we take fashion images as an example to show how Markov Random Fields (MRFs) can outperform Conditional Random Fields when the application does not follow a certain statistical model learned from the training data set. We propose a new method for automatically parsing fashion images in high processing efficiency with significantly less training time by applying a modification of MRFs, named reweighted MRF (RW-MRF), which resolves the problem of over smoothing infrequent labels. We further enhance RW-MRF with occlusion prior and background prior to resolve two other common problems in clothing parsing, occlusion, and background spill. Our experimental results indicate that our proposed clothing parsing method significantly improves processing time and training time over state-of-the-art methods, while ensuring comparable parsing accuracy and improving label recall rate.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2296973526",
    "type": "article"
  },
  {
    "title": "Delving Deeper in Drone-Based Person Re-Id by Employing Deep Decision Forest and Attributes Fusion",
    "doi": "https://doi.org/10.1145/3360050",
    "publication_date": "2020-01-31",
    "publication_year": 2020,
    "authors": "Aleksei Grigorev; Shaohui Liu; Zhihong Tian; Jianxin Xiong; Seungmin Rho; Feng Jiang",
    "corresponding_authors": "",
    "abstract": "Deep learning has revolutionized the field of computer vision and image processing. Its ability to extract the compact image representation has taken the person re-identification (re-id) problem to a new level. However, in most cases, researchers are focused on developing new approaches to extract more fruitful image representation and use it in the re-id task. The extra information about images is rarely taken into account because the traditional person re-id datasets usually do not have it. Nevertheless, the research in multimodal machine learning has demonstrated that the utilization of the information from different sources leads to better performance. In this work, we demonstrate how a person re-id problem can benefit from the utilization of multimodal data. We have used the UAV drone to collect and label the new person re-id dataset, which is composed of pedestrian images and its attributes. We have manually annotated this dataset with attributes, and in contrast to the recent research, we do not use the deep network to classify them. Instead, we employ the continuous bag-of-words model to extract the word embeddings from text descriptions and fuse it with features extracted from images. Then the deep neural decision forest is used for pedestrians classification. The extensive experiments on the collected dataset demonstrate the effectiveness of the proposed model.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W3022764761",
    "type": "article"
  },
  {
    "title": "Socializing the Videos: A Multimodal Approach for Social Relation Recognition",
    "doi": "https://doi.org/10.1145/3416493",
    "publication_date": "2021-02-28",
    "publication_year": 2021,
    "authors": "Tong Xu; Peilun Zhou; Linkang Hu; Xiangnan He; Yao Hu; Enhong Chen",
    "corresponding_authors": "",
    "abstract": "As a crucial task for video analysis, social relation recognition for characters not only provides semantically rich description of video content but also supports intelligent applications, e.g., video retrieval and visual question answering. Unfortunately, due to the semantic gap between visual and semantic features, traditional solutions may fail to reveal the accurate relations among characters. At the same time, the development of social media platforms has now promoted the emergence of crowdsourced comments, which may enhance the recognition task with semantic and descriptive cues. To that end, in this article, we propose a novel multimodal-based solution to deal with the character relation recognition task. Specifically, we capture the target character pairs via a search module and then design a multistream architecture for jointly embedding the visual and textual information, in which feature fusion and attention mechanism are adapted for better integrating the multimodal inputs. Finally, supervised learning is applied to classify character relations. Experiments on real-world data sets validate that our solution outperforms several competitive baselines.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W3155322285",
    "type": "article"
  },
  {
    "title": "A Security and Privacy Validation Methodology for e-Health Systems",
    "doi": "https://doi.org/10.1145/3412373",
    "publication_date": "2021-05-18",
    "publication_year": 2021,
    "authors": "Flora Amato; Valentina Casola; Giovanni Cozzolino; Alessandra De Benedictis; Nicola Mazzocca; Francesco Moscato",
    "corresponding_authors": "",
    "abstract": "e-Health applications enable one to acquire, process, and share patient medical data to improve diagnosis, treatment, and patient monitoring. Despite the undeniable benefits brought by the digitization of health systems, the transmission of and access to medical information raises critical issues, mainly related to security and privacy. While several security mechanisms exist that can be applied in an e-Health system, they may not be adequate due to the complexity of involved workflows, and to the possible inherent correlation among health-related concepts that may be exploited by unauthorized subjects. In this article, we propose a novel methodology for the validation of security and privacy policies in a complex e-Health system, that leverages a formal description of clinical workflows and a semantically enriched definition of the data model used by the workflows, in order to build a comprehensive model of the system that can be analyzed with automated model checking and ontology-based reasoning techniques. To validate the proposed methodology, we applied it to two case studies, subjected to the directives of the EU GDPR regulation for the protection of health data, and demonstrated its ability to correctly verify the fulfillment of desired policies in different scenarios.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W3162307706",
    "type": "article"
  },
  {
    "title": "An Explainable Framework for Diagnosis of COVID-19 Pneumonia via Transfer Learning and Discriminant Correlation Analysis",
    "doi": "https://doi.org/10.1145/3449785",
    "publication_date": "2021-10-26",
    "publication_year": 2021,
    "authors": "Siyuan Lu; Di Wu; Zheng Zhang; Shuihua Wang",
    "corresponding_authors": "",
    "abstract": "The new coronavirus COVID-19 has been spreading all over the world in the last six months, and the death toll is still rising. The accurate diagnosis of COVID-19 is an emergent task as to stop the spreading of the virus. In this paper, we proposed to leverage image feature fusion for the diagnosis of COVID-19 in lung window computed tomography (CT). Initially, ResNet-18 and ResNet-50 were selected as the backbone deep networks to generate corresponding image representations from the CT images. Second, the representative information extracted from the two networks was fused by discriminant correlation analysis to obtain refined image features. Third, three randomized neural networks (RNNs): extreme learning machine, Schmidt neural network and random vector functional-link net, were trained using the refined features, and the predictions of the three RNNs were ensembled to get a more robust classification performance. Experiment results based on five-fold cross validation suggested that our method outperformed state-of-the-art algorithms in the diagnosis of COVID-19.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W3209574471",
    "type": "article"
  },
  {
    "title": "Will You Ever Become Popular? Learning to Predict Virality of Dance Clips",
    "doi": "https://doi.org/10.1145/3477533",
    "publication_date": "2022-02-16",
    "publication_year": 2022,
    "authors": "Jiahao Wang; Yunhong Wang; Nina Weng; Tianrui Chai; Annan Li; Faxi Zhang; Sansi Yu",
    "corresponding_authors": "",
    "abstract": "Dance challenges are going viral in video communities like TikTok nowadays. Once a challenge becomes popular, thousands of short-form videos will be uploaded within a couple of days. Therefore, virality prediction from dance challenges is of great commercial value and has a wide range of applications, such as smart recommendation and popularity promotion. In this article, a novel multi-modal framework that integrates skeletal, holistic appearance, facial and scenic cues is proposed for comprehensive dance virality prediction. To model body movements, we propose a pyramidal skeleton graph convolutional network (PSGCN) that hierarchically refines spatio-temporal skeleton graphs. Meanwhile, we introduce a relational temporal convolutional network (RTCN) to exploit appearance dynamics with non-local temporal relations. An attentive fusion approach is finally proposed to adaptively aggregate predictions from different modalities. To validate our method, we introduce a large-scale viral dance video (VDV) dataset, which contains over 4,000 dance clips of eight viral dance challenges. Extensive experiments on the VDV dataset well demonstrate the effectiveness of our approach. Furthermore, we show that short video applications such as multi-dimensional recommendation and action feedback can be derived from our model.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3214109797",
    "type": "article"
  },
  {
    "title": "BiRe-ID: Binary Neural Network for Efficient Person Re-ID",
    "doi": "https://doi.org/10.1145/3473340",
    "publication_date": "2022-02-08",
    "publication_year": 2022,
    "authors": "Sheng Xu; Chang Liu; Baochang Zhang; Jinhu Lü; Guodong Guo; David Doermann",
    "corresponding_authors": "",
    "abstract": "Person re-identification (Re-ID) has been promoted by the significant success of convolutional neural networks (CNNs). However, the application of such CNN-based Re-ID methods depends on the tremendous consumption of computation and memory resources, which affects its development on resource-limited devices such as next generation AI chips. As a result, CNN binarization has attracted increasing attention, which leads to binary neural networks (BNNs). In this article, we propose a new BNN-based framework for efficient person Re-ID (BiRe-ID). In this work, we discover that the significant performance drop of binarized models for Re-ID task is caused by the degraded representation capacity of kernels and features. To address the issues, we propose the kernel and feature refinement based on generative adversarial learning (KR-GAL and FR-GAL) to enhance the representation capacity of BNNs. We first introduce an adversarial attention mechanism to refine the binarized kernels based on their real-valued counterparts. Specifically, we introduce a scale factor to restore the scale of 1-bit convolution. And we employ an effective generative adversarial learning method to train the attention-aware scale factor. Furthermore, we introduce a self-supervised generative adversarial network to refine the low-level features using the corresponding high-level semantic information. Extensive experiments demonstrate that our BiRe-ID can be effectively implemented on various mainstream backbones for the Re-ID task. In terms of the performance, our BiRe-ID surpasses existing binarization methods by significant margins, at the level even comparable with the real-valued counterparts. For example, on Market-1501, BiRe-ID achieves 64.0% mAP on ResNet-18 backbone, with an impressive 12.51× speedup in theory and 11.75× storage saving. In particular, the KR-GAL and FR-GAL methods show strong generalization on multiple tasks such as Re-ID, image classification, object detection, and 3D point cloud processing.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4210933607",
    "type": "article"
  },
  {
    "title": "NR-CNN: Nested-Residual Guided CNN In-loop Filtering for Video Coding",
    "doi": "https://doi.org/10.1145/3502723",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Kai Lin; Chuanmin Jia; Xinfeng Zhang; Shanshe Wang; Siwei Ma; Wen Gao",
    "corresponding_authors": "",
    "abstract": "Recently, deep learning for video coding, such as deep predictive coding, deep transform coding, and deep in-loop filtering, has been an emerging research area. The coding gain of hybrid coding framework could be extensively promoted by the data-driven models. However, previous deep coding tools especially deep in-loop filtering mainly consider the performance improvement while pay less attention to the reliability, usability, and adaptivity of the networks. In this article, a nested-residual guided convolutional neural network (NR-CNN) structure with cascaded global shortcut and configurable residual blocks is proposed for in-loop filtering. By taking advantage of the correlation between different color components, we further extend the NR-CNN by utilizing luminance as textural and structural guidance for chrominance filtering, which significantly improves the filtering performance. To fully exploit the proposed network into codec integration, we subsequently introduce an efficient and adaptive framework consisting of an adaptive granularity optimization and a parallel inference pipeline for deep learning based filtering. The former contributes to the coding performance improvement through an adaptive decision-making based on rate-distortion analysis at various granularities. The latter reduces the running time of network inference. The extensive experimental results show the superiority of the proposed method, achieving 8.2%, 14.9%, and 13.2% BD-rate savings on average under random access (RA) configuration. Meanwhile, the proposed method also obtains better subjective quality.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4214890647",
    "type": "article"
  },
  {
    "title": "Millimeter Wave and Free-space-optics for Future Dual-connectivity 6DOF Mobile Multi-user VR Streaming",
    "doi": "https://doi.org/10.1145/3544494",
    "publication_date": "2022-06-16",
    "publication_year": 2022,
    "authors": "Jacob Chakareski; Mahmudur Khan; Tanguy Ropitault; Steve Blandino",
    "corresponding_authors": "",
    "abstract": "Dual-connectivity streaming is a key enabler of next-generation six Degrees Of Freedom (6DOF) Virtual Reality (VR) scene immersion. Indeed, using conventional sub-6 GHz WiFi only allows to reliably stream a low-quality baseline representation of the VR content, while emerging high-frequency communication technologies allow to stream in parallel a high-quality user viewport-specific enhancement representation that synergistically integrates with the baseline representation to deliver high-quality VR immersion. We investigate holistically as part of an entire future VR streaming system two such candidate emerging technologies, Free Space Optics (FSO) and millimeter-Wave (mmWave), that benefit from a large available spectrum to deliver unprecedented data rates. We analytically characterize the key components of the envisioned dual-connectivity 6DOF VR streaming system that integrates in addition edge computing and scalable 360° video tiling, and we formulate an optimization problem to maximize the immersion fidelity delivered by the system, given the WiFi and mmWave/FSO link rates, and the computing capabilities of the edge server and the users’ VR headsets. This optimization problem is mixed integer programming of high complexity and we formulate a geometric programming framework to compute the optimal solution at low complexity. We carry out simulation experiments to assess the performance of the proposed system using actual 6DOF navigation traces from multiple mobile VR users that we collected. Our results demonstrate that our system considerably advances the traditional state of the art and enables streaming of 8K-120 frames-per-second (fps) 6DOF content at high fidelity.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4282962200",
    "type": "article"
  },
  {
    "title": "Cyclic Self-attention for Point Cloud Recognition",
    "doi": "https://doi.org/10.1145/3538648",
    "publication_date": "2022-06-24",
    "publication_year": 2022,
    "authors": "Guanyu Zhu; Yong Zhou; Rui Yao; Hancheng Zhu; Jiaqi Zhao",
    "corresponding_authors": "",
    "abstract": "Point clouds provide a flexible geometric representation for computer vision research. However, the harsh demands for the number of input points and computer hardware are still significant challenges, which hinder their deployment in real applications. To address these challenges, we design a simple and effective module named cyclic self-attention module (CSAM). Specifically, three attention maps of the same input are obtained by cyclically pairing the feature maps, thus exploring the features sufficiently of the attention space of the original input. CSAM can adequately explore the correlation between points to obtain sufficient feature information despite the multiplicative decrease in inputs. Meanwhile, it can direct the computational power to the more essential features, relieving the burden on the computer hardware. We build a point cloud classification network by simply stacking CSAM called cyclic self-attention network (CSAN). We also propose a novel framework for point cloud semantic segmentation called full cyclic self-attention network (FCSAN). By adaptively fusing the original mapping features and the CSAM extracted features, it can better capture the context information of point clouds. Extensive experiments on several benchmark datasets show that our methods can achieve competitive performance in classification and segmentation tasks.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4283374589",
    "type": "article"
  },
  {
    "title": "Context Sensing Attention Network for Video-based Person Re-identification",
    "doi": "https://doi.org/10.1145/3573203",
    "publication_date": "2022-12-01",
    "publication_year": 2022,
    "authors": "Kan Wang; Changxing Ding; Jianxin Pang; Xiangmin Xu",
    "corresponding_authors": "",
    "abstract": "Video-based person re-identification (ReID) is challenging due to the presence of various interferences in video frames. Recent approaches handle this problem using temporal aggregation strategies. In this work, we propose a novel Context Sensing Attention Network (CSA-Net), which improves both the frame feature extraction and temporal aggregation steps. First, we introduce the Context Sensing Channel Attention (CSCA) module, which emphasizes responses from informative channels for each frame. These informative channels are identified with reference not only to each individual frame, but also to the content of the entire sequence. Therefore, CSCA explores both the individuality of each frame and the global context of the sequence. Second, we propose the Contrastive Feature Aggregation (CFA) module, which predicts frame weights for temporal aggregation. Here, the weight for each frame is determined in a contrastive manner: i.e., not only by the quality of each individual frame, but also by the average quality of the other frames in a sequence. Therefore, it effectively promotes the contribution of relatively good frames. Extensive experimental results on four datasets show that CSA-Net consistently achieves state-of-the-art performance.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4284887198",
    "type": "article"
  },
  {
    "title": "Dual Projective Zero-Shot Learning Using Text Descriptions",
    "doi": "https://doi.org/10.1145/3514247",
    "publication_date": "2022-07-29",
    "publication_year": 2022,
    "authors": "Yunbo Rao; Ziqiang Yang; Shaoning Zeng; Qifeng Wang; Jiansu Pu",
    "corresponding_authors": "",
    "abstract": "Zero-shot learning (ZSL) aims to recognize image instances of unseen classes solely based on the semantic descriptions of the unseen classes. In this field, Generalized Zero-Shot Learning (GZSL) is a challenging problem in which the images of both seen and unseen classes are mixed in the testing phase of learning. Existing methods formulate GZSL as a semantic-visual correspondence problem and apply generative models such as Generative Adversarial Networks and Variational Autoencoders to solve the problem. However, these methods suffer from the bias problem since the images of unseen classes are often misclassified into seen classes. In this work, a novel model named the Dual Projective model for Zero-Shot Learning (DPZSL) is proposed using text descriptions. In order to alleviate the bias problem, we leverage two autoencoders to project the visual and semantic features into a latent space and evaluate the embeddings by a visual-semantic correspondence loss function. An additional novel classifier is also introduced to ensure the discriminability of the embedded features. Our method focuses on a more challenging inductive ZSL setting in which only the labeled data from seen classes are used in the training phase. The experimental results, obtained from two popular datasets—Caltech-UCSD Birds-200-2011 (CUB) and North America Birds (NAB)—show that the proposed DPZSL model significantly outperforms both the inductive ZSL and GZSL settings. Particularly in the GZSL setting, our model yields an improvement up to 15.2% in comparison with state-of-the-art CANZSL on datasets CUB and NAB with two splittings.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4288707709",
    "type": "article"
  },
  {
    "title": "Scalable Color Quantization for Task-centric Image Compression",
    "doi": "https://doi.org/10.1145/3551389",
    "publication_date": "2022-08-01",
    "publication_year": 2022,
    "authors": "Jae Hyun Park; Sanghoon Kim; Joo Chan Lee; Jong Hwan Ko",
    "corresponding_authors": "",
    "abstract": "Conventional image compression techniques targeted for the perceptual quality are not generally optimized for classification tasks using deep neural networks (DNNs). To compress images for DNN inference tasks, recent studies have proposed task-centric image compression methods with quantization techniques optimized for DNN inference. Among them, color quantization was proposed to reduce the amount of data per pixel by limiting the number of distinct colors (color space) in an image. However, quantizing images into various color space sizes requires training and inference of multiple DNNs, each of which is dedicated to each color space. To overcome this limitation, we propose a scalable color quantization method, where images with variable color space sizes can be extracted from a master image generated by a single DNN model. This scalability is enabled by weighted color grouping that constructs a color palette using critical color components for the classification task. We also propose an adaptive training method that can jointly optimize images with various color-space sizes. The results show that the proposed method supports dynamic changes of the color space size between 1–6 bit color space per pixel, while even increasing the inference accuracy at a low bit precision up to 20.2% and 46.6% compared to other task- and human-centric color quantizations, respectively.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4289313617",
    "type": "article"
  },
  {
    "title": "ML-CookGAN: Multi-Label Generative Adversarial Network for Food Image Generation",
    "doi": "https://doi.org/10.1145/3554738",
    "publication_date": "2022-08-13",
    "publication_year": 2022,
    "authors": "Zhiming Liu; Kai Niu; Zhiqiang He",
    "corresponding_authors": "",
    "abstract": "Generating food images from recipe and ingredient information can be applied to many tasks such as food recommendation, recipe development, and health management. For the characteristics of food images, this paper proposes ML-CookGAN, a novel CGAN. This network enables the generation of food images based on recipe and ingredient labels. The generator of ML-CookGAN, Multi-Label Fusion Generator, converts recipe and ingredient labels into different granularity features and generates corresponding food images. The discriminator of ML-CookGAN, Multi-Branch Discriminator, implements discrimination and classification with a multi-branch structure. In addition, we propose two training strategies, Region-Wise Pooling and Image Style Distillation, to better the network performance. Region-Wise Pooling handles region-wise features with the discriminator. Image Style Distillation aims at extracting image latent features to assist image generation by an unsupervised method. The experiments conducted on VIREO Food-172 databases validate the proposed method to generate high-quality Chinese food images. And Region-Wise Pooling and Image Style Distillation are proven to enhance the diversity and realism of generated food images.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4291433229",
    "type": "article"
  },
  {
    "title": "Attention-Augmented Memory Network for Image Multi-Label Classification",
    "doi": "https://doi.org/10.1145/3570166",
    "publication_date": "2022-11-03",
    "publication_year": 2022,
    "authors": "Wei Zhou; Yanke Hou; Dihu Chen; Haifeng Hu; Tao Su",
    "corresponding_authors": "",
    "abstract": "The purpose of image multi-label classification is to predict all the object categories presented in an image. Some recent works exploit graph convolution network to capture the correlation between labels. Although promising results have been reported, these methods cannot learn salient object features in the images and ignore the correlation between channel feature maps. In addition, the current researches only learn the feature information within individual input image, but fail to mine the contextual information of various categories from the dataset to enhance the input feature representation. To address these issues, we propose an A ttention- A ugmented M emory N etwork ( AAMN ) model for the image multi-label classification task. Specifically, we first propose a novel categorical memory module to excavate the contextual information of various categories from the dataset to augment the current input feature. Secondly, we design a new channel-relation exploration module to capture the inter-channel relationship of features, so as to enhance the correlation between objects in the images. Thirdly, we develop a spatial-relation enhancement module to model second-order statistics of features and capture long-range dependencies between pixels in feature maps, so as to learn salient object features. Experimental results on standard benchmarks, including MS-COCO 2014, PASCAL VOC 2007, and VG-500, demonstrate the effectiveness and superiority of AAMN model, which outperforms current state-of-the-art methods.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4308104970",
    "type": "article"
  },
  {
    "title": "Temporal Dropout for Weakly Supervised Action Localization",
    "doi": "https://doi.org/10.1145/3567827",
    "publication_date": "2022-11-07",
    "publication_year": 2022,
    "authors": "Chi Xie; Zikun Zhuang; Shengjie Zhao; Shuang Liang",
    "corresponding_authors": "",
    "abstract": "Weakly supervised action localization is a challenging problem in video understanding and action recognition. Existing models usually formulate the training process as direct classification using video-level supervision. They tend to only locate the most discriminative parts of action instances and produce temporally incomplete detection results. A natural solution for this problem, the adversarial erasing strategy, is to remove such parts from training so that models can attend to complementary parts. Previous works do it in an offline and heuristic way. They adopt a multi-stage pipeline, where discriminative regions are determined and erased under the guidance of detection results from last stage. Such a pipeline can be both ineffective and inefficient, possibly hindering the overall performance. On the contrary, we combine adversarial erasing with dropout mechanism and propose a Temporal Dropout Module that learns where to remove in a data-driven and online manner. This plug-and-play module is trained without iterative stages, which not only simplifies the pipeline but also makes the regularization during training easier and more adaptive. Experiments show that the proposed method outperforms previous erasing-based methods by a large margin. More importantly, it achieves universal improvement when plugged into various direct classification methods and obtains state-of-the-art performance.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4308391676",
    "type": "article"
  },
  {
    "title": "Exploiting Residual and Illumination with GANs for Shadow Detection and Shadow Removal",
    "doi": "https://doi.org/10.1145/3571745",
    "publication_date": "2022-11-17",
    "publication_year": 2022,
    "authors": "Ling Zhang; Chengjiang Long; Xiaolong Zhang; Chunxia Xiao",
    "corresponding_authors": "",
    "abstract": "Residual image and illumination estimation have been proven to be helpful for image enhancement. In this article, we propose a general framework, called RI-GAN, that exploits residual and illumination using generative adversarial networks (GANs). The proposed framework detects and removes shadows in a coarse-to-fine fashion. At the coarse stage, we employ three generators to produce a coarse shadow-removal result, a residual image, and an inverse illumination map. We also incorporate two indirect shadow-removal images via the residual image and the inverse illumination map. With the residual image, the illumination map, and the two indirect shadow-removal images as auxiliary information, the refinement stage estimates a shadow mask to identify shadow regions in the image, and then refines the coarse shadow-removal result to the fine shadow-free image. We introduce a cross-encoding module to the refinement generator, in which the use of feature-crossing can provide additional details to promote the shadow mask and the high-quality shadow-removal result. In addition, we apply data augmentation to the discriminator to reduce the dependence between representations of the discriminator and the quality of the predicted image. Experiments for shadow detection and shadow removal demonstrate that our method outperforms state-of-the-art methods. Furthermore, RI-GAN exhibits good performance in terms of image dehazing, rain removal, and highlight removal, demonstrating the effectiveness and flexibility of the proposed framework.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4309616870",
    "type": "article"
  },
  {
    "title": "TP-FER: An Effective Three-phase Noise-tolerant Recognizer for Facial Expression Recognition",
    "doi": "https://doi.org/10.1145/3570329",
    "publication_date": "2022-11-17",
    "publication_year": 2022,
    "authors": "Junjie Li; Jin Yuan; Zhiyong Li",
    "corresponding_authors": "",
    "abstract": "Single-label facial expression recognition (FER), which aims to classify single expression for facial images, usually suffers from the label noisy and incomplete problem, where manual annotations for partial training images exist wrong or incomplete labels, resulting in performance decline. Although prior work has attempted to leverage external sources or manual annotations to handle this problem, it usually requires extra costs. This article explores a simple yet effective three-phase paradigm (“warm-up,” “selection,” and “relabeling”) for FER task. First, the warm-up phase attempts to build an initial recognition network based on noisy samples for discriminative feature extractions and facial expression predictions. Then, the second selection phase defines several rules to choose high confident samples according to prediction scores, and the third relabeling phase assigns two potential labels to those samples for network updating according to a composite two-label loss. Compared with the previous studies, the three-phase learning could effectively correct noisy labels in the ground truth without extra information and automatically assign two potential labels to single-label samples without manual annotations. As a result, the label information is purified and supplemented with few cost, yielding significant performance improvement. Extensive experiments are conducted on three datasets, and the experimental results demonstrate that our approach is robust to noisy training samples and outperforms several state-of-the-art methods.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4309617229",
    "type": "article"
  },
  {
    "title": "Exploring the Effect of High-frequency Components in GANs Training",
    "doi": "https://doi.org/10.1145/3578585",
    "publication_date": "2022-09-30",
    "publication_year": 2022,
    "authors": "Ziqiang Li; Pengfei Xia; Rui Xue; Bin Li",
    "corresponding_authors": "",
    "abstract": "Generative Adversarial Networks (GANs) have the ability to generate images that are visually indistinguishable from real images. However, recent studies have revealed that generated and real images share significant differences in the frequency domain. In this article, we argue that the frequency gap is caused by the high-frequency sensitivity of the discriminator. According to our observation, during the training of most GANs, severe high-frequency differences make the discriminator focus on high-frequency components excessively, which hinders the generator from fitting the low-frequency components that are important for learning images’ content. Then, we propose two simple yet effective image pre-processing operations in the frequency domain for eliminating the side effects caused by high-frequency differences in GANs training: High-frequency Confusion (HFC) and High-frequency Filter (HFF). The proposed operations are general and can be applied to most existing GANs at a fraction of the cost. The advanced performance of the proposed operations is verified on multiple loss functions, network architectures, and datasets. Specifically, the proposed HFF achieves significant improvements of 42.5% FID on CelebA (128*128) unconditional generation based on SNGAN, 30.2% FID on CelebA unconditional generation based on SSGAN, and 69.3% FID on CelebA unconditional generation based on InfoMAXGAN. Furthermore, we also adopt HFF as the first attempt at data augmentation in the frequency domain for contrastive learning, achieving state-of-the-art performance on unconditional generation. Code is available at https://github.com/iceli1007/HFC-and-HFF .",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4313331847",
    "type": "article"
  },
  {
    "title": "Recurrent Multi-scale Approximation-Guided Network for Single Image Super-Resolution",
    "doi": "https://doi.org/10.1145/3592613",
    "publication_date": "2023-04-14",
    "publication_year": 2023,
    "authors": "Wei‐Yen Hsu; Pei-Wen Jian",
    "corresponding_authors": "",
    "abstract": "Single-image super-resolution (SISR) is an essential topic in computer vision applications. However, most CNN-based SISR approaches directly learn the relationship between low- and high-resolution images while ignoring the contextual texture and detail fidelity to explore super-resolution; thus, they hinder the representational power of CNNs and lead to the unrealistic, distorted reconstruction of edges and textures in the images. In this study, we propose a novel recurrent structure preservation mechanism with the integration and innovative use of multi-scale wavelet transform, Recurrent Multiscale Approximation-guided Network (RMANet) , to recursively process the low-frequency and high-frequency sub-networks at each level separately. Unlike traditional wavelet transform, we propose a novel Approximation Level Preservation (ALP) architecture to import and learn the low-frequency sub-networks at each level. Through proposed Approximation level fusion (ALF) and inverse wavelet transform, rich image structures of low frequency at each level can be recursively restored and greatly preserved with the combination of ALP at each level. In addition, a novel low-frequency to high-frequency detail enhancement (DE) mechanism is also proposed to solve the problem of detail distortion in high-frequency networks by transmitting low-frequency information to the high-frequency network. Finally, a joint loss function is used to balance low-frequency and high-frequency information with different degrees of fusion. In addition to correct restoration, image details are further enhanced by tuning different hyperparameters during training. Compared with the state-of-the-art approaches, the experimental results on synthetic and real datasets demonstrate that the proposed RMANet achieves better performance in visual presentation, especially in image edges and texture details.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4365509791",
    "type": "article"
  },
  {
    "title": "Language-guided Residual Graph Attention Network and Data Augmentation for Visual Grounding",
    "doi": "https://doi.org/10.1145/3604557",
    "publication_date": "2023-06-14",
    "publication_year": 2023,
    "authors": "Jia Wang; Hong-Han Shuai; Yung‐Hui Li; Wen-Huang Cheng",
    "corresponding_authors": "",
    "abstract": "Visual grounding is an essential task in understanding the semantic relationship between the given text description and the target object in an image. Due to the innate complexity of language and the rich semantic context of the image, it is still a challenging problem to infer the underlying relationship and to perform reasoning between the objects in an image and the given expression. Although existing visual grounding methods have achieved promising progress, cross-modal mapping across different domains for the task is still not well handled, especially when the expressions are complex and long. To address the issue, we propose a language-guided residual graph attention network for visual grounding (LRGAT-VG), which enables us to apply deeper graph convolution layers with the assistance of residual connections between them. This allows us to better handle long and complex expressions than other graph-based methods. Furthermore, we perform a Language-guided Data Augmentation (LGDA), which is based on copy-paste operations on pairs of source and target images to increase the diversity of training data while maintaining the relationship between the objects in the image and the expression. With extensive experiments on three visual grounding benchmarks, including RefCOCO, RefCOCO+, and RefCOCOg, LRGAT-VG with LGDA achieves competitive performance with other state-of-the-art graph network-based referring expression approaches and demonstrates its effectiveness.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4380609723",
    "type": "article"
  },
  {
    "title": "Double-Layer Search and Adaptive Pooling Fusion for Reference-Based Image Super-Resolution",
    "doi": "https://doi.org/10.1145/3604937",
    "publication_date": "2023-06-22",
    "publication_year": 2023,
    "authors": "Kehua Guo; Liang Chen; Xiangyuan Zhu; Xiaoyan Kui; Jian Zhang; Heyuan Shi",
    "corresponding_authors": "",
    "abstract": "Reference-based image super-resolution (RefSR) aims to reconstruct high-resolution (HR) images from low-resolution (LR) images by introducing HR reference images. The key step of RefSR is to transfer reference features to LR features. However, existing methods still lack an efficient transfer mechanism, resulting in blurry details in the generated image. In this article, we propose a double-layer search module and an adaptive pooling fusion module group for reference-based image super-resolution, called DLASR. Based on the re-search strategy, the double-layer search module can produce an accurate index map and score map. These two maps are used to filter out accurate reference features, which greatly increases the efficiency of feature transfer in the later stage. Through two continuous feature-enhancement steps, the adaptive pooling fusion module group can transfer more valuable reference features to the corresponding LR features. In addition, a structure reconstruction module is proposed to recover the geometric information of the images, which further improves the visual quality of the generated image. We conduct comparative experiments on a variety of datasets, and the results prove that DLASR achieves significant improvements over other state-of-the-art methods, in terms of quantitative accuracy and qualitative visual effect. The code is available at https://github.com/clttyou/DLASR.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4381611109",
    "type": "article"
  },
  {
    "title": "BiC-Net: Learning Efficient Spatio-temporal Relation for Text-Video Retrieval",
    "doi": "https://doi.org/10.1145/3627103",
    "publication_date": "2023-10-13",
    "publication_year": 2023,
    "authors": "Ning Han; Yawen Zeng; Chuhao Shi; Guangyi Xiao; Hao Chen; Jingjing Chen",
    "corresponding_authors": "",
    "abstract": "The task of text-video retrieval aims to understand the correspondence between language and vision and has gained increasing attention in recent years. Recent works have demonstrated the superiority of local spatio-temporal relation learning with graph-based models. However, most existing graph-based models are handcrafted and depend heavily on expert knowledge and empirical feedback, which may be unable to mine the high-level fine-grained visual relations effectively. These limitations result in their inability to distinguish videos with the same visual components but different relations. To solve this problem, we propose a novel cross-modal retrieval framework, Bi-Branch Complementary Network (BiC-Net), which modifies Transformer architecture to effectively bridge text-video modalities in a complementary manner via combining local spatio-temporal relation and global temporal information. Specifically, local video representations are encoded using multiple Transformer blocks and additional residual blocks to learn fine-grained spatio-temporal relations and long-term temporal dependency, calling the module a Fine-grained Spatio-temporal Transformer (FST). Global video representations are encoded using a multi-layer Transformer block to learn global temporal features. Finally, we align the spatio-temporal relation and global temporal features with the text feature on two embedding spaces for cross-modal text-video retrieval. Extensive experiments are conducted on MSR-VTT, MSVD, and YouCook2 datasets. The results demonstrate the effectiveness of our proposed model. Our code is public at https://github.com/lionel-hing/BiC-Net .",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4387614569",
    "type": "article"
  },
  {
    "title": "Weighted Guided Optional Fusion Network for RGB-T Salient Object Detection",
    "doi": "https://doi.org/10.1145/3624984",
    "publication_date": "2023-10-13",
    "publication_year": 2023,
    "authors": "Jie Wang; Guoqiang Li; Jie Shi; Jinwen Xi",
    "corresponding_authors": "",
    "abstract": "There is no doubt that the rational and effective use of visible and thermal infrared image data information to achieve cross-modal complementary fusion is the key to improving the performance of RGB-T salient object detection (SOD). A meticulous analysis of the RGB-T SOD data reveals that it mainly consists of three scenarios in which both modalities (RGB and T) have a significant foreground and only a single modality (RGB or T) is disturbed. However, existing methods are obsessed with pursuing more effective cross-modal fusion based on treating both modalities equally. Obviously, the subjective use of equivalence has two significant limitations. Firstly, it does not allow for practical discrimination of which modality makes the dominant contribution to performance. While both modalities may have visually significant foregrounds, differences in their imaging properties will result in distinct performance contributions. Secondly, in a specific acquisition scenario, a pair of images with two modalities will contribute differently to the final detection performance due to their varying sensitivity to the same background interference. Intelligibly, for the RGB-T saliency detection task, it would be more reasonable to generate exclusive weights for the two modalities and select specific fusion mechanisms based on different weight configurations to perform cross-modal complementary integration. Consequently, we propose a weighted guided optional fusion network (WGOFNet) for RGB-T SOD. Specifically, a feature refinement module is first used to perform an initial refinement of the extracted multilevel features. Subsequently, a weight generation module (WGM) will generate exclusive network performance contribution weights for each of the two modalities, and an optional fusion module (OFM) will rely on this weight to perform particular integration of cross-modal information. Simple cross-level fusion is finally utilized to obtain the final saliency prediction map. Comprehensive experiments on three publicly available benchmark datasets demonstrate the proposed WGOFNet achieves superior performance compared with the state-of-the-art RGB-T SOD methods. The source code is available at: https://github.com/WJ-CV/WGOFNet .",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4387614573",
    "type": "article"
  },
  {
    "title": "2BiVQA: Double Bi-LSTM-based Video Quality Assessment of UGC Videos",
    "doi": "https://doi.org/10.1145/3632178",
    "publication_date": "2023-11-08",
    "publication_year": 2023,
    "authors": "Ahmed Telili; Sid Ahmed Fezza; Wassim Hamidouche; Hanene F. Z. Brachemi Meftah",
    "corresponding_authors": "",
    "abstract": "Recently, with the growing popularity of mobile devices as well as video sharing platforms (e.g., YouTube, Facebook, TikTok, and Twitch), User-Generated Content (UGC) videos have become increasingly common and now account for a large portion of multimedia traffic on the internet. Unlike professionally generated videos produced by filmmakers and videographers, typically, UGC videos contain multiple authentic distortions, generally introduced during capture and processing by naive users. Quality prediction of UGC videos is of paramount importance to optimize and monitor their processing in hosting platforms, such as their coding, transcoding, and streaming. However, blind quality prediction of UGC is quite challenging, because the degradations of UGC videos are unknown and very diverse, in addition to the unavailability of pristine reference. Therefore, in this article, we propose an accurate and efficient Blind Video Quality Assessment (BVQA) model for UGC videos, which we name 2BiVQA for double Bi-LSTM Video Quality Assessment. 2BiVQA metric consists of three main blocks, including a pre-trained Convolutional Neural Network to extract discriminative features from image patches, which are then fed into two Recurrent Neural Networks for spatial and temporal pooling. Specifically, we use two Bi-directional Long Short-term Memory networks, the first is used to capture short-range dependencies between image patches, while the second allows capturing long-range dependencies between frames to account for the temporal memory effect. Experimental results on recent large-scale UGC VQA datasets show that 2BiVQA achieves high performance at lower computational cost than most state-of-the-art VQA models. The source code of our 2BiVQA metric is made publicly available at https://github.com/atelili/2BiVQA .",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4388499609",
    "type": "article"
  },
  {
    "title": "Multi-Object Tracking with Spatial-Temporal Tracklet Association",
    "doi": "https://doi.org/10.1145/3635155",
    "publication_date": "2023-11-30",
    "publication_year": 2023,
    "authors": "Sisi You; Hantao Yao; Bing‐Kun Bao; Changsheng Xu",
    "corresponding_authors": "",
    "abstract": "Recently, the tracking-by-detection methods have achieved excellent performance in Multi-Object Tracking (MOT), which focuses on obtaining a robust feature for each object and generating tracklets based on feature similarity. However, they are confronted with two issues: 1) unstable features in short-term occlusion; 2) insufficient matching in long-term occlusion. Specifically, the unstable feature is caused by the appearance variation under occlusion, and the association with the current unstable feature will lead to insufficient matching in long-term occlusion. To address the above issues, we propose a two-stage tracklet-level association method, Spatial-Temporal Tracklet Association (STTA), to effectively combine spatial-temporal context between feature extraction and data association. In the first stage, we propose the Tracklet-guided Spatial-Temporal Attention network (TSTA) to generate robust and stable features. Specifically, TSTA captures spatial-temporal context to obtain the most salient regions between the current and previous clips. In the second stage, we design the Bi-Tracklet Spatial-Temporal association (BTST) module to fully exploit the spatial-temporal context in data association. Specifically, we leverage BTST to merge different tracklets into long-term trajectories by jointly learning visual feature and spatial-temporal context, and designing a bidirectional interpolation to recover the missed objects between matched tracklets. Extensive experiments of public and private detections on four benchmarks demonstrate the robustness of STTA. Furthermore, the proposed method is a model-agnostic method, which can be plugged and played with existing methods to boost their performance, e.g. , obtain 11.0%, 10.1%, 2.9%, 3.2%, 7.8% improvement on IDF1 in MOT16 validation dataset for Tracktor, CenterTrack, Deepsort, JDE, and CTracker, respectively.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4389195961",
    "type": "article"
  },
  {
    "title": "GAN-assisted Road Segmentation from Satellite Imagery",
    "doi": "https://doi.org/10.1145/3635153",
    "publication_date": "2023-11-30",
    "publication_year": 2023,
    "authors": "Wenmiao Hu; Yifang Yin; Ying Kiat Tan; An Tran; Hannes Kruppa; Roger Zimmermann",
    "corresponding_authors": "",
    "abstract": "Geo-information extraction from satellite imagery has become crucial to carry out large-scale ground surveys in a short amount of time. With the increasing number of commercial satellites launched into orbit in recent years, high-resolution RGB color remote sensing imagery has attracted a lot of attention. However, because of the high cost of image acquisition and even more complicated annotation procedures, there are limited high-resolution satellite datasets available. Compared to close-range imagery datasets, existing satellite datasets have a much lower number of images and cover only a few scenarios (cities, background environments, etc. ). They may not be sufficient for training robust learning models that fit all environmental conditions or be representative enough for training regional models that optimize for local scenarios. Instead of collecting and annotating more data, using synthetic images could be another solution to boost the performance of a model. This study proposes a GAN-assisted training scheme for road segmentation from high-resolution RGB color satellite images, which includes three critical components: a) synthetic training sample generation, b) synthetic training sample selection, and c) assisted training strategy. Apart from the GeoPalette and cSinGAN image generators introduced in our prior work, this paper in detail explains how to generate new training pairs using OpenStreetMap (OSM) and introduces a new set of evaluation metrics for selecting synthetic training pairs from a pool of generated samples. We conduct extensive quantitative and qualitative experiments to compare different image generators and training strategies. Our experiments on the downstream road segmentation task show that 1) our proposed metrics are more aligned with the trained model performance compared to commonly used GAN evaluation metrics such as the Fréchet inception distance (FID); and 2) by using synthetic data with the best training strategy, the model performance, mean Intersection over Union (mean IoU), is improved from 60.92% to 64.44%, when 1,000 real training pairs are available for learning, which reaches a similar level of performance as a model that is standard-trained with 4,000 real images (64.59%), i.e. , enabling a 4-fold reduction in real dataset size.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4389196019",
    "type": "article"
  },
  {
    "title": "Trustworthy and Efficient Digital Twins in Post-Quantum Era with Hybrid Hardware-Assisted Signatures",
    "doi": "https://doi.org/10.1145/3638250",
    "publication_date": "2023-12-21",
    "publication_year": 2023,
    "authors": "Saif E. Nouma; Attila A. Yavuz",
    "corresponding_authors": "",
    "abstract": "Digital Twins (DT) virtually model cyber-physical objects via sensory inputs by simulating or monitoring their behavior. Therefore, DTs usually harbor vast quantities of Internet of Things (IoT) components (e.g., sensors) that gather, process, and offload sensitive information (e.g., healthcare) to the cloud. It is imperative to ensure the trustworthiness of such sensitive information with long-term and compromise-resilient security guarantees. Digital signatures provide scalable authentication and integrity with non-repudiation and are vital tools for DTs. Post-quantum cryptography (PQC) and forward-secure signatures are two fundamental tools to offer long-term security and breach resiliency. However, NIST-PQC signature standards are exorbitantly costly for embedded DT components and are infeasible when forward-security is also considered. Moreover, NIST-PQC signatures do not admit aggregation, which is a highly desirable feature to mitigate the heavy storage and transmission burden in DTs. Finally, NIST recommends hybrid PQ solutions to enable cryptographic agility and transitional security. Yet, there is a significant gap in the state of the art in the achievement of all these advanced features simultaneously. Therefore, there is a significant need for lightweight digital signatures that offer compromise resiliency and compactness while permitting transitional security into the PQ era for DTs. We create a series of highly lightweight digital signatures called Hardware-ASisted Efficient Signature ( HASES ) that meets the above requirements. The core of HASES is a hardware-assisted cryptographic commitment construct oracle ( CCO ) that permits verifiers to obtain expensive commitments without signer interaction. We created three HASES schemes: PQ-HASES is a forward-secure PQ signature, LA-HASES is an efficient aggregate Elliptic-Curve signature, and HY-HASES is a novel hybrid scheme that combines PQ-HASES and LA-HASES with novel strong nesting and sequential aggregation. HASES does not require a secure-hardware on the signer. We prove that HASES schemes are secure and implemented them on commodity hardware and and 8-bit AVR ATmega2560. Our experiments confirm that PQ-HASES and LA-HASES are two magnitudes of times more signer efficient than their PQ and conventional-secure counterparts, respectively. HY-HASES outperforms NIST PQC and conventional signature combinations, offering a standard-compliant transitional solution for emerging DTs. We open-source HASES schemes for public-testing and adaptation.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4390051482",
    "type": "article"
  },
  {
    "title": "Dynamic Weighted Adversarial Learning for Semi-Supervised Classification under Intersectional Class Mismatch",
    "doi": "https://doi.org/10.1145/3635310",
    "publication_date": "2024-01-11",
    "publication_year": 2024,
    "authors": "Mingyu Li; Tao Zhou; Zhuo Huang; Jian Yang; Jie Yang; Chen Gong",
    "corresponding_authors": "",
    "abstract": "Nowadays, class-mismatch problem has drawn intensive attention in Semi-Supervised Learning (SSL), where the classes of labeled data are assumed to be only a subset of the classes of unlabeled data. However, in a more realistic scenario, the labeled data and unlabeled data often share some common classes while they also have their individual classes, which leads to an “intersectional class-mismatch” problem. As a result, existing SSL methods are often confused by these individual classes and suffer from performance degradation. To address this problem, we propose a novel Dynamic Weighted Adversarial Learning (DWAL) framework to properly utilize unlabeled data for boosting the SSL performance. Specifically, to handle the influence of the individual classes in unlabeled data (i.e., Out-Of-Distribution classes), we propose an enhanced adversarial domain adaptation to dynamically assign weight for each unlabeled example from the perspectives of domain adaptation and a class-wise weighting mechanism, which consists of transferability score and prediction confidence value. Besides, to handle the influence of the individual classes in labeled data (i.e., private classes), we propose a dissimilarity maximization strategy to suppress the inaccurate correlations caused by the examples of individual classes within labeled data. Therefore, our DWAL can properly make use of unlabeled data to acquire an accurate SSL classifier under intersectional class-mismatch setting, and extensive experimental results on five public datasets demonstrate the effectiveness of the proposed model over other state-of-the-art SSL methods.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4389269168",
    "type": "article"
  },
  {
    "title": "NSDIE: Noise Suppressing Dark Image Enhancement Using Multiscale Retinex and Low-Rank Minimization",
    "doi": "https://doi.org/10.1145/3638772",
    "publication_date": "2024-01-03",
    "publication_year": 2024,
    "authors": "Manvi Jha; Ashish Kumar Bhandari",
    "corresponding_authors": "",
    "abstract": "It is inevitable for dark images to have crucial information obscured by low-light conditions, which are worsened by the presence of noise in these images. This work introduces a groundbreaking solution, Noise-Suppressing Dark Image Enhancement for Web Apps (NSDIE), to address the challenging task of enhancing low-light images marred by noise. The proposed work utilizes a low-rank model with simultaneous enhancement of reflectance and illumination components to improve the nighttime scenes while also eradicating the present noise of the image. The reflectance component is further processed using a multiscale retinex model to compensate for the possible color distortions while the illumination component is enhanced using the camera response model to ensure the genuineness of the scene. The proposed work is also tested for a standalone application and is presented to the user through a web portal to aid the concerns of dark image enhancement in the daily life of the user. Rigorous quantitative and qualitative analyses assert NSDIE's superiority over existing techniques, establishing its pivotal role in addressing the critical concern of dark image enhancement.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4390532869",
    "type": "article"
  },
  {
    "title": "Two-stream Multi-level Dynamic Point Transformer for Two-person Interaction Recognition",
    "doi": "https://doi.org/10.1145/3639470",
    "publication_date": "2024-01-05",
    "publication_year": 2024,
    "authors": "Yao Liu; Gangfeng Cui; Jiahui Luo; Xiaojun Chang; Lina Yao",
    "corresponding_authors": "",
    "abstract": "As a fundamental aspect of human life, two-person interactions contain meaningful information about people’s activities, relationships, and social settings. Human action recognition serves as the foundation for many smart applications, with a strong focus on personal privacy. However, recognizing two-person interactions poses more challenges due to increased body occlusion and overlap compared to single-person actions. In this article, we propose a point cloud-based network named Two-stream Multi-level Dynamic Point Transformer for two-person interaction recognition. Our model addresses the challenge of recognizing two-person interactions by incorporating local-region spatial information, appearance information, and motion information. To achieve this, we introduce a designed frame selection method named Interval Frame Sampling (IFS), which efficiently samples frames from videos, capturing more discriminative information in a relatively short processing time. Subsequently, a frame features learning module and a two-stream multi-level feature aggregation module extract global and partial features from the sampled frames, effectively representing the local-region spatial information, appearance information, and motion information related to the interactions. Finally, we apply a transformer to perform self-attention on the learned features for the final classification. Extensive experiments are conducted on two large-scale datasets, the interaction subsets of NTU RGB+D 60 and NTU RGB+D 120. The results show that our network outperforms state-of-the-art approaches in most standard evaluation settings.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4390604645",
    "type": "article"
  },
  {
    "title": "Meetor: A Human-Centered Automatic Video Editing System for Meeting Recordings",
    "doi": "https://doi.org/10.1145/3648681",
    "publication_date": "2024-08-16",
    "publication_year": 2024,
    "authors": "Haihan Duan; Junhua Liao; Lehao Lin; Abdulmotaleb El Saddik; Wei Cai",
    "corresponding_authors": "",
    "abstract": "Widely adopted digital cameras and smartphones have generated a large number of videos, which have brought a tremendous workload to video editors. Recently, a variety of automatic/semi-automatic video editing methods have been proposed to tackle these issues in some specific areas. However, for the production of meeting recordings, the existing studies highly depend on extra equipment in the conference venues, such as the infrared camera or special microphone, which are not practical. In this article, we design and implement Meetor, a human-centered automatic video editing system for meeting recordings. The Meetor mainly contains three parts: an audio-based video synchronization algorithm, human-centered video content flaw detection algorithms, and an automatic video editing algorithm. Two main experiments are conducted from both objective and subjective aspects to evaluate the performance of the Meetor. The experimental results on a testbed illustrate that the proposed algorithms could achieve state-of-the-art (SOTA) performance in video content flaw detection. However, the conducted user study demonstrates that Meetor could generate meeting recordings with a satisfactory quality compared with professional video editors. Moreover, we also present a practical application of the Meetor in a university campus prototype, in which the Meetor is applied in the automatic editing of lecture recordings. All in all, the proposed Meetor can be utilized in practical applications to release the workload of professional video editors.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4391932663",
    "type": "article"
  },
  {
    "title": "VertexShuffle-Based Spherical Super-Resolution for 360-Degree Videos",
    "doi": "https://doi.org/10.1145/3649315",
    "publication_date": "2024-08-16",
    "publication_year": 2024,
    "authors": "Na Li; Yao Liu",
    "corresponding_authors": "",
    "abstract": "360-degree video is an emerging form of media that encodes information about all directions surrounding a camera, offering an immersive experience to the users. Unlike traditional 2D videos, visual information in 360-degree videos can be naturally represented as pixels on a sphere. Inspired by state-of-the-art deep-learning-based 2D image super-resolution models and spherical CNNs, in this article, we design a novel spherical super-resolution (SSR) approach for 360-degree videos. To support viewport-adaptive and bandwidth-efficient transmission/streaming of 360-degree video data and save computation, we propose the Focused Icosahedral Mesh to represent a small area on the sphere. We further construct matrices to rotate spherical content over the entire sphere to the focused mesh area, allowing us to use the focused mesh to represent any area on the sphere. Motivated by the PixelShuffle operation for 2D super-resolution, we also propose a novel VertexShuffle operation on the mesh and an improved version VertexShuffle_V2. We compare our SSR approach with state-of-the-art 2D super-resolution models and show that SSR has the potential to achieve significant benefits when applied to spherical signals.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4392121238",
    "type": "article"
  },
  {
    "title": "Deep Network for Image Compressed Sensing Coding Using Local Structural Sampling",
    "doi": "https://doi.org/10.1145/3649441",
    "publication_date": "2024-02-26",
    "publication_year": 2024,
    "authors": "Wenxue Cui; Xingtao Wang; Xiaopeng Fan; Shaohui Liu; Xinwei Gao; Debin Zhao",
    "corresponding_authors": "",
    "abstract": "Existing image compressed sensing (CS) coding frameworks usually solve an inverse problem based on measurement coding and optimization-based image reconstruction, which still exist the following two challenges: (1) the widely used random sampling matrix, such as the Gaussian Random Matrix (GRM), usually leads to low measurement coding efficiency, and (2) the optimization-based reconstruction methods generally maintain a much higher computational complexity. In this article, we propose a new convolutional neural network based image CS coding framework using local structural sampling (dubbed CSCNet) that includes three functional modules: local structural sampling, measurement coding, and Laplacian pyramid reconstruction. In the proposed framework, instead of GRM, a new local structural sampling matrix is first developed, which is able to enhance the correlation between the measurements through a local perceptual sampling strategy. Besides, the designed local structural sampling matrix can be jointly optimized with the other functional modules during the training process. After sampling, the measurements with high correlations are produced, which are then coded into final bitstreams by the third-party image codec. Last, a Laplacian pyramid reconstruction network is proposed to efficiently recover the target image from the measurement domain to the image domain. Extensive experimental results demonstrate that the proposed scheme outperforms the existing state-of-the-art CS coding methods while maintaining fast computational speed.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4392168377",
    "type": "article"
  },
  {
    "title": "Instance-level Adversarial Source-free Domain Adaptive Person Re-identification",
    "doi": "https://doi.org/10.1145/3649900",
    "publication_date": "2024-02-27",
    "publication_year": 2024,
    "authors": "Xiaofeng Qu; Li Liu; Lei Zhu; Liqiang Nie; Huaxiang Zhang",
    "corresponding_authors": "",
    "abstract": "Domain adaption (DA) for person re-identification (ReID) has attained considerable progress by transferring knowledge from a source domain with labels to a target domain without labels. Nonetheless, most of the existing methods require access to source data, which raises privacy concerns. Source-free DA has recently emerged as a response to these privacy challenges, yet its direct application to open-set pedestrian re-identification tasks is hindered by the reliance on a shared category space in existing methods. Current source-free DA approaches for person ReID still encounter several obstacles, particularly the divergence-agnostic problem and the notable domain divergence due to the absent source data. In this article, we introduce an Instance-level Adversarial Mutual Teaching (IAMT) framework, which utilizes adversarial views to tackle the challenges mentioned above. Technically, we first elaborately develop a variance-based division (VBD) module to segregate the target data into instance-level subsets based on their similarity and dissimilarity to the source using the source-trained model, implicitly tackling the divergence-agnostic problem. To mitigate domain divergence, we additionally introduce a dynamic adversarial alignment (DAA) strategy, aiming to enhance the consistence of feature distribution across domains by employing adversarial instances from the target data to confuse the discriminators. Experiments reveal the superiority of the IAMT over state-of-the-art methods for DA person ReID tasks, while preserving the privacy of the source data.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4392191951",
    "type": "article"
  },
  {
    "title": "Joint Distortion Restoration and Quality Feature Learning for No-reference Image Quality Assessment",
    "doi": "https://doi.org/10.1145/3649899",
    "publication_date": "2024-02-28",
    "publication_year": 2024,
    "authors": "Jifan Yang; Zhongyuan Wang; Baojin Huang; Jiaxin Ai; Yuhong Yang; Zixiang Xiong",
    "corresponding_authors": "",
    "abstract": "No-reference image quality assessment (NR-IQA) methods, inspired by the free energy principle, improve the accuracy of image quality prediction by simulating the human brain’s repair process for distorted images. However, existing methods use separate optimization schemes for distortion restoration and quality prediction, which undermines the accurate mapping of feature representations to quality scores. To address this issue, we propose a joint restoration and quality feature learning NR-IQA (RQFL-IQA) method to jointly tackle distortion image restoration and quality prediction within a unified framework. To accurately establish the quality reconstruction relationship between distorted and restored images, a hybrid loss function based on pixel-wise and structure-wise representations is used to improve the restoration capability of the image restoration network. The proposed RQFL-IQA exploits rich labels, including restored images and quality scores, to enable the model to learn more discriminative features and establish a more accurate mapping from feature representation to quality scores. In addition, to avoid the impact of poor restoration on quality prediction, we propose a module with a cleaning function to reweight the fusion of restored and primitive features to achieve more perceptual consistency in feature fusion. Experimental results on public IQA datasets show that the proposed RQFL-IQA is superior over existing methods.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4392240210",
    "type": "article"
  },
  {
    "title": "Learning Scene Representations for Human-assistive Displays Using Self-attention Networks",
    "doi": "https://doi.org/10.1145/3650111",
    "publication_date": "2024-03-02",
    "publication_year": 2024,
    "authors": "Jaime Ruiz-Serra; Jack White; Stephen Petrie; Tatiana Kameneva; Chris McCarthy",
    "corresponding_authors": "",
    "abstract": "Video-see-through (VST) augmented reality (AR) is widely used to present novel augmentative visual experiences by processing video frames for viewers. Among VST AR systems, assistive vision displays aim to compensate for low vision or blindness, presenting enhanced visual information to support activities of daily living for the vision impaired/deprived. Despite progress, current assistive displays suffer from a visual information bottleneck, limiting their functional outcomes compared to healthy vision. This motivates the exploration of methods to selectively enhance and augment salient visual information. Traditionally, vision processing pipelines for assistive displays rely on hand-crafted, single-modality filters, lacking adaptability to time-varying and environment-dependent needs. This article proposes the use of Deep Reinforcement Learning (DRL) and Self-attention (SA) networks as a means to learn vision processing pipelines for assistive displays. SA networks selectively attend to task-relevant features, offering a more parameter—and compute-efficient approach to RL-based task learning. We assess the feasibility of using SA networks in a simulation-trained model to generate relevant representations of real-world states for navigation with prosthetic vision displays. We explore two prosthetic vision applications, vision-to-auditory encoding, and retinal prostheses, using simulated phosphene visualisations. This article introduces SA-px, a general-purpose vision processing pipeline using self-attention networks, and SA-phos, a display-specific formulation targeting low-resolution assistive displays. We present novel scene visualisations derived from SA image patches importance rankings to support mobility with prosthetic vision devices. To the best of our knowledge, this is the first application of self-attention networks to the task of learning vision processing pipelines for prosthetic vision or assistive displays.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4392347629",
    "type": "article"
  },
  {
    "title": "Robust Image Hashing via CP Decomposition and DCT for Copy Detection",
    "doi": "https://doi.org/10.1145/3650112",
    "publication_date": "2024-03-01",
    "publication_year": 2024,
    "authors": "Xiaoping Liang; Wanting Liu; Xianquan Zhang; Zhenjun Tang",
    "corresponding_authors": "",
    "abstract": "Copy detection is a key task of image copyright protection. This article proposes a robust image hashing algorithm by CP decomposition and discrete cosine transform (DCT) for copy detection. The first contribution is the third-order tensor construction with low-frequency coefficients in the DCT domain. Since the low-frequency DCT coefficients contain most of the image energy, they can reflect the basic visual content of the image and are less disturbed by noise. Hence, the third-order tensor construction with the low-frequency DCT coefficients can ensure robustness of our algorithm. Another contribution is the application of the CP decomposition to the third-order tensor for learning a short binary hash. As the factor matrices learned from the CP decomposition can preserve the topology of the original tensor, the binary hash derived from the factor matrices can reach good discrimination. Lots of experiments and comparisons are done to validate effectiveness and advantage of our algorithm. The results demonstrate that our algorithm has superior classification and copy detection performances than several baseline algorithms. In addition, our algorithm is also better than some baseline algorithms with regard to hash length and computational time.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4392356417",
    "type": "article"
  },
  {
    "title": "Recoverable Privacy-Preserving Image Classification through Noise-like Adversarial Examples",
    "doi": "https://doi.org/10.1145/3653676",
    "publication_date": "2024-03-21",
    "publication_year": 2024,
    "authors": "Jun Liu; Jiantao Zhou; Jinyu Tian; Weiwei Sun",
    "corresponding_authors": "",
    "abstract": "With the increasing prevalence of cloud computing platforms, ensuring data privacy during the cloud-based image-related services such as classification has become crucial. In this study, we propose a novel privacy-preserving image classification scheme that enables the direct application of classifiers trained in the plaintext domain to classify encrypted images without the need of retraining a dedicated classifier. Moreover, encrypted images can be decrypted back into their original form with high fidelity (recoverable) using a secret key. Specifically, our proposed scheme involves utilizing a feature extractor and an encoder to mask the plaintext image through a newly designed Noise-like Adversarial Example (NAE). Such an NAE not only introduces a noise-like visual appearance to the encrypted image but also compels the target classifier to predict the ciphertext as the same label as the original plaintext image. At the decoding phase, we adopt a Symmetric Residual Learning (SRL) framework for restoring the plaintext image with minimal degradation. Extensive experiments demonstrate that (1) the classification accuracy of the classifier trained in the plaintext domain remains the same in both the ciphertext and plaintext domains; (2) the encrypted images can be recovered into their original form with an average PSNR of up to 51+ dB for the SVHN dataset and 48+ dB for the VGGFace2 dataset; (3) our system exhibits satisfactory generalization capability on the encryption, decryption, and classification tasks across datasets that are different from the training one; and (4) a high-level of security is achieved against three potential threat models. The code is available at https://github.com/csjunjun/RIC.git .",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4393042474",
    "type": "article"
  },
  {
    "title": "Multimodal Score Fusion with Sparse Low-rank Bilinear Pooling for Egocentric Hand Action Recognition",
    "doi": "https://doi.org/10.1145/3656044",
    "publication_date": "2024-04-02",
    "publication_year": 2024,
    "authors": "Kankana Roy",
    "corresponding_authors": "Kankana Roy",
    "abstract": "With the advent of egocentric cameras, there are new challenges where traditional computer vision is not sufficient to handle this kind of video. Moreover, egocentric cameras often offer multiple modalities that need to be modeled jointly to exploit complimentary information. In this article, we propose a sparse low-rank bilinear score pooling approach for egocentric hand action recognition from RGB-D videos. It consists of five blocks: a baseline CNN to encode RGB and depth information for producing classification probabilities; a novel bilinear score pooling block to generate a score matrix; a sparse low-rank matrix recovery block to reduce redundant features, which is common in bilinear pooling; a one-layer CNN for frame-level classification; and an RNN for video-level classification. We proposed to fuse classification probabilities instead of traditional CNN features from RGB and depth modality, involving an effective yet simple sparse low-rank bilinear score pooling to produce a fused RGB-D score matrix. To demonstrate the efficacy of our method, we perform extensive experiments over two large-scale hand action datasets, namely, THU-READ and FPHA, and two smaller datasets, GUN-71 and HAD. We observe that the proposed method outperforms state-of-the-art methods and achieves accuracies of 78.55% and 96.87% over the THU-READ dataset in cross-subject and cross-group settings, respectively. Further, we achieved accuracies of 91.59% and 43.87% over the FPHA and Gun-71 datasets, respectively.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4393519251",
    "type": "article"
  },
  {
    "title": "Facial Soft-biometrics Obfuscation through Adversarial Attacks",
    "doi": "https://doi.org/10.1145/3656474",
    "publication_date": "2024-04-06",
    "publication_year": 2024,
    "authors": "Vincenzo Carletti; Pasquale Foggia; Antonio Greco; Alessia Saggese; Mario Vento",
    "corresponding_authors": "",
    "abstract": "Sharing facial pictures through online services, especially on social networks, has become a common habit for thousands of users. This practice hides a possible threat to privacy: the owners of such services, as well as malicious users, could automatically extract information from faces using modern and effective neural networks. In this article, we propose the harmless use of adversarial attacks, i.e., variations of images that are almost imperceptible to the human eye and that are typically generated with the malicious purpose to mislead Convolutional Neural Networks (CNNs). Such attacks have been instead adopted to (1) obfuscate soft biometrics (gender, age, ethnicity) but (2) without degrading the quality of the face images posted online. We achieve the above-mentioned two conflicting goals by modifying the implementations of four of the most popular adversarial attacks, namely FGSM, PGD, DeepFool, and C&amp;W, in order to constrain the average amount of noise they generate on the image and the maximum perturbation they add on the single pixel. We demonstrate, in an experimental framework including three popular CNNs, namely VGG16, SENet, and MobileNetV3, that the considered obfuscation method, which requires at most 4 seconds for each image, is effective not only when we have a complete knowledge of the neural network that extracts the soft biometrics (white box attacks) but also when the adversarial attacks are generated in a more realistic black box scenario. Finally, we prove that an opponent can implement defense techniques to partially reduce the effect of the obfuscation, but substantially paying in terms of accuracy over clean images; this result, confirmed by the experiments carried out with three popular defense methods, namely adversarial training, denoising autoencoder, and Kullback-Leibler autoencoder, shows that it is not convenient for the opponent to defend himself and that the proposed approach is robust to defenses.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4394013274",
    "type": "article"
  },
  {
    "title": "Leveraging Frame- and Feature-Level Progressive Augmentation for Semi-supervised Action Recognition",
    "doi": "https://doi.org/10.1145/3655025",
    "publication_date": "2024-04-11",
    "publication_year": 2024,
    "authors": "Zhewei Tu; Xiangbo Shu; Peng Huang; Rui Yan; Zhenxing Liu; Jiachao Zhang",
    "corresponding_authors": "",
    "abstract": "Semi-supervised action recognition is a challenging yet prospective task due to its low reliance on costly labeled videos. One high-profile solution is to explore frame-level weak/strong augmentations for learning abundant representations, inspired by the FixMatch framework dominating the semi-supervised image classification task. However, such a solution mainly brings perturbations in terms of texture and scale, leading to the limitation in learning action representations in videos with spatiotemporal redundancy and complexity. Therefore, we revisit the creative trick of weak/strong augmentations in FixMatch, and then propose a novel Frame- and Feature-level augmentation FixMatch (dubbed as F 2 -FixMatch) framework to learn more abundant action representations for being robust to complex and dynamic video scenarios. Specifically, we design a new Progressive Augmentation (P-Aug) mechanism that implements the weak/strong augmentations first at the frame level, and further implements the perturbation at the feature level, to obtain abundant four types of augmented features in broader perturbation spaces. Moreover, we present an evolved Multihead Pseudo-Labeling (MPL) scheme to promote the consistency of features across different augmented versions based on the pseudo labels. We conduct extensive experiments on several public datasets to demonstrate that our F 2 -FixMatch achieves the performance gain compared with current state-of-the-art methods. The source codes of F 2 -FixMatch are publicly available at https://github.com/zwtu/F2FixMatch.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4394717651",
    "type": "article"
  },
  {
    "title": "UniQRNet: Unifying Referring Expression Grounding and Segmentation with QRNet",
    "doi": "https://doi.org/10.1145/3660638",
    "publication_date": "2024-04-25",
    "publication_year": 2024,
    "authors": "Jiabo Ye; Junfeng Tian; Ming Yan; Haiyang Xu; Qinghao Ye; Yaya Shi; Xiaoshan Yang; Xuwu Wang; Ji Zhang; Liang He; Xin Lin",
    "corresponding_authors": "",
    "abstract": "Referring expression comprehension aims to align natural language queries with visual scenes, which requires establishing fine-grained correspondence between vision and language. This has important applications in multi-modal reasoning systems. Existing methods typically use text-agnostic visual backbones to extract features independently without considering the specific text input. However, we argue that the extracted visual features can be inconsistent with the referring expression, which hurts multi-modal understanding. To address this, we first propose Query-modulated Refinement Network (QRNet) that leverages language guidance to guide visual feature extraction. However, it only focuses on the grounding task that can only provide coarse-grained annotations in the form of bounding box coordinates. The guidance for the visual backbone is indirect, and the inconsistent issue still exists. To this end, we further propose UniQRNet, a multi-task framework over the QRNet to learn referring expression grounding and segmentation jointly. The framework introduces a multi-task head that leverages fine-grained pixel-level supervision from the segmentation task to directly guide the intermediate layers of QRNet to learn text-consistent visual features. Besides, UniQRNet also includes a loss balance strategy that allows two types of supervision signals to cooperate and optimize the model together. We conduct the most comprehensive comparison experiment covering four major datasets, ten evaluation set and three evaluation metrics used in previous work. UniQRNet outperforms previous state-of-the-art methods by a large margin on both referring comprehensive grounding (1.8%~5.09%) and segmentation tasks (0.57%~5.56%). Ablation and analysis reveal that UniQRNet can improve the consistency of visual features with text input and can bring significant performance improvement.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4395464584",
    "type": "article"
  },
  {
    "title": "Efficient Decoding of Affective States from Video-elicited EEG Signals: An Empirical Investigation",
    "doi": "https://doi.org/10.1145/3663669",
    "publication_date": "2024-05-03",
    "publication_year": 2024,
    "authors": "Kayhan Latifzadeh; Nima Gozalpour; V. Javier Traver; Tuukka Ruotsalo; Aleksandra Kawala‐Sterniuk; Luis A. Leiva",
    "corresponding_authors": "",
    "abstract": "Affect decoding through brain-computer interfacing (BCI) holds great potential to capture users’ feelings and emotional responses via non-invasive electroencephalogram (EEG) sensing. Yet, little research has been conducted to understand efficient decoding when users are exposed to dynamic audiovisual contents. In this regard, we study EEG-based affect decoding from videos in arousal and valence classification tasks, considering the impact of signal length, window size for feature extraction, and frequency bands. We train both classic Machine Learning models (SVMs and k -NNs) and modern Deep Learning models (FCNNs and GTNs). Our results show that: (1) affect can be effectively decoded using less than 1 minute of EEG signal; (2) temporal windows of 6 and 10 seconds provide the best classification performance for classic Machine Learning models but Deep Learning models benefit from much shorter windows of 2 seconds; and (3) any model trained on the Beta band alone achieves similar (sometimes better) performance than when trained on all frequency bands. Taken together, our results indicate that affect decoding can work in more realistic conditions than currently assumed, thus becoming a viable technology for creating better interfaces and user models.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4396612962",
    "type": "article"
  },
  {
    "title": "Generating and Evaluating Data of Daily Activities with an Autonomous Agent in a Virtual Smart Home",
    "doi": "https://doi.org/10.1145/3665331",
    "publication_date": "2024-05-22",
    "publication_year": 2024,
    "authors": "Lysa Gramoli; Julien Cumin; Jérémy Lacoche; Anthony Foulonneau; Bruno Arnaldi; Valérie Gouranton",
    "corresponding_authors": "",
    "abstract": "Training machine learning models to identify human behavior is a difficult yet essential task to develop autonomous and adaptive systems such as smart homes. These models require large and diversified amounts of labeled data to be trained effectively. Due to the high variety of home environments and occupant behaviors, collecting datasets that are representative of all possible homes is a major challenge. In addition, privacy and cost are major hurdles to collect real home data. To avoid these difficulties, one solution consists of training these models using purely synthetic data, which can be generated through the simulation of home and their occupants. Two challenges arise from this approach: designing a methodology with a simulation able to generate credible simulated data and evaluating this credibility. In this article, we explain the methodology used to generate diversified synthetic data of daily activities, through the combination of an agent model to simulate an occupant and a simulated 3D house enriched with sensors and effectors to produce such data. We demonstrate the credibility of the generated synthetic data by comparing their efficacy for training human context understanding models against the efficacy generated by real data. To achieve this, we replicate a real dataset collection setting with our smart home simulator. The occupant is replaced by an autonomous agent following the same experimental protocol used for the real dataset collection. This agent is a BDI-based model enhanced with a scheduler designed to offer a balance between control and autonomy. This balance is useful in synthetic data generation since strong constraints can be imposed on the agent to simulate desired situations while allowing autonomous behaviors outside these constraints to generate diversified data. In our case, the constraints are those imposed during the real dataset collection that we want to replicate. The simulated sensors and effectors were configured to react to the agent’s behaviors similarly to the real ones. We experimentally show that data generated from this simulation are valuable for two human context understanding tasks: current human activity recognition and future human activity prediction. In particular, we show that models trained solely with simulated data can give reasonable predictions about real situations occurring in the original dataset. We also report experimental results regarding statistical analysis and C2ST to assess the credibility of generated data. We discuss the generality of our approach for evaluating the credibility of simulated data from their use as training data.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4398197744",
    "type": "article"
  },
  {
    "title": "ASIFusion: An Adaptive Saliency Injection-Based Infrared and Visible Image Fusion Network",
    "doi": "https://doi.org/10.1145/3665893",
    "publication_date": "2024-05-23",
    "publication_year": 2024,
    "authors": "Ziyi Liu; You Yang; Kejun Wu; Qiong Liu; Xinghua Xu; Xiaoxuan Ma; Jiang Tang",
    "corresponding_authors": "",
    "abstract": "The purpose of infrared and visible image fusion (IVIF) is to acquire a more informative fused image by leveraging complementary information, facilitating human perception and machine vision. Among the existing fusion methods, the saliency-based methods conform to human perception characteristics and achieve relatively advantageous fusion performance. However, such methods fail to adaptively maintain the edge and intensity of salient objects, resulting in fixed fusion performance. To address these issue, we present ASIFusion, an adaptive saliency injection-based IVIF network. First, source images are inputted to the feature extraction encoder for fully extracting features. Meanwhile, the proposed adaptive saliency injection module detects salient objects in the infrared image and then learns the fusion weights of each channel, which serve as supplementary information for further fusion. These learned weights are utilized to merge the source images’ extracted features. Finally, the feature reconstruction decoder produces a fused image with injected saliency. The fused image maintains the intensity and edge of the salient objects and fully preserves the complementary information. Extensive experiments demonstrate that our proposed network outperforms state-of-the-art (SOTA) approaches with regard to fusion performance and computational efficiency.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4398236438",
    "type": "article"
  },
  {
    "title": "Unified and Scalable Deep Image Compression Framework for Human and Machine",
    "doi": "https://doi.org/10.1145/3678472",
    "publication_date": "2024-07-17",
    "publication_year": 2024,
    "authors": "Gai Zhang; Xinfeng Zhang; Lv Tang",
    "corresponding_authors": "",
    "abstract": "Image compression aims to minimize the amount of data in image representation while maintaining a certain visual quality for humans, which is an essential technique for storage and transmission. Recently, along with the development of computer vision, machines have become another primary receiver for images and require compressed images at a certain quality level, which may be different from that of human vision. In many scenarios, compressed images should serve both human and machine vision tasks, but few compression methods are designed for both goals simultaneously. In this article, we propose a unified and scalable deep image compression (USDIC) framework that jointly optimizes the image quality according to human and machine vision in an end-to-end style. For the encoder, we propose an information splitting mechanism (ISM) to separate images into semantic and visual features, which mainly aims at machine analysis and human viewing tasks. For the decoder, we design a scalable decoding architecture. The encoded semantic feature is first decoded for machine analysis tasks, and the image is decoded and reconstructed further by leveraging the decoded semantic features. Herein, to further remove the redundancy between the semantic and visual features of images, we propose a scalable entropy model (SEM) with a joint optimization strategy to reconstruct the image using the two kinds of decoded features. Extensive experimental results show that the proposed USDIC achieves much better performance on the image analysis task while maintaining competitive performance on the traditional image reconstruction task compared with popular image compression methods.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4400729529",
    "type": "article"
  },
  {
    "title": "Asymmetric Deformable Spatio-temporal Framework for Infrared Object Tracking",
    "doi": "https://doi.org/10.1145/3678882",
    "publication_date": "2024-07-19",
    "publication_year": 2024,
    "authors": "Jingjing Wu; Xi Yi Zhou; Xiaohong Li; Hao Liu; Meibin Qi; Richang Hong",
    "corresponding_authors": "",
    "abstract": "The Infrared Object Tracking (IOT) task aims to locate objects in infrared sequences. Since color and texture information is unavailable in infrared modality, most existing infrared trackers merely rely on capturing spatial contexts from the image to enhance feature representation, where other complementary information is rarely deployed. To fill this gap, we in this article propose a novel Asymmetric Deformable Spatio-Temporal Framework (ADSF) to fully exploit collaborative shape and temporal clues in terms of the objects. Firstly, an asymmetric deformable cross-attention module is designed to extract shape information, which attends to the deformable correlations between distinct frames in an asymmetric manner. Secondly, a spatio-temporal tracking framework is coined to learn the temporal variance trend of the object during the training process and store the template information closest to the tracking frame when testing. Comprehensive experiments demonstrate that ADSF outperforms state-of-the-art methods on three public datasets. Extensive ablation experiments further confirm the effectiveness of each component in ADSF. Furthermore, we conduct generalization validation to demonstrate that the proposed method also achieves performance gains in RGB-based tracking scenarios.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4400812987",
    "type": "article"
  },
  {
    "title": "SSAT: Active Authorization Control and User’s Fingerprint Tracking Framework for DNN IP Protection",
    "doi": "https://doi.org/10.1145/3679202",
    "publication_date": "2024-07-20",
    "publication_year": 2024,
    "authors": "Mingfu Xue; Yinghao Wu; Leo Yu Zhang; Dujuan Gu; Yushu Zhang; Weiqiang Liu",
    "corresponding_authors": "",
    "abstract": "As training a high-performance deep neural network (DNN) model requires a large amount of data, powerful computing resources and expert knowledge, protecting well-trained DNN models from intellectual property (IP) infringement has raised serious concerns in recent years. Most existing methods using DNN watermarks to verify the ownership of the models after IP infringement occurs, which is reactive in the sense that they cannot prevent unauthorized users from using the model in the first place. Different from these methods, in this article, we propose an active authorization control and user’s fingerprint tracking method for the IP protection of DNN models by utilizing sample-specific backdoor attack. The proposed method inversely and multiplely exploits sample-specific trigger as the key to implement authorization control for DNN model, in which the generated triggers are imperceptible and sample-specific for clean images. Specifically, a U-Net model is used to generate backdoor instances. Then, the target model is trained on the clean images and backdoor instances, which are inversely labeled as wrong classes and correct classes, respectively. Only authorized users can use the target model normally by pre-processing the clean images through the U-Net model. Moreover, the images processed by the U-Net model will contain unique fingerprint that can be extracted to verify and track the corresponding user’s identity. This article is the first work that utilizes the sample-specific backdoor attack to implement active authorization control and user’s fingerprint management for DNN model under black-box scenarios. Extensive experimental results on ImageNet dataset and YouTube Aligned Face dataset demonstrate that the proposed method is effective in protecting the DNN model from unauthorized usage. Specifically, the protected model has a low inference accuracy (1.00%) for unauthorized users, while maintaining a normal inference accuracy (97.67%) for authorized users. Besides, the proposed method can achieve 100% fingerprint tracking success rates on both the ImageNet and YouTube Aligned Face datasets. Moreover, it is demonstrated that the proposed method is robust against fine-tuning attack, pruning attack, pruning attack with retraining, reverse-engineering attack, adaptive attack, and JPEG compression attack. The code is available at https://github.com/nuaaaisec/SSAT .",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4400850895",
    "type": "article"
  },
  {
    "title": "Multi-Scale and Multi-Layer Lattice Transformer for Underwater Image Enhancement",
    "doi": "https://doi.org/10.1145/3688802",
    "publication_date": "2024-08-14",
    "publication_year": 2024,
    "authors": "Wei‐Yen Hsu; Yu-Yu Hsu",
    "corresponding_authors": "",
    "abstract": "Underwater images are often subject to color deviation and a loss of detail due to the absorption and scattering of light. The challenge of enhancing underwater images is compounded by variations in wavelength and distance attenuation, as well as color deviation that exist across different scales and layers, resulting in different degrees of color deviation, attenuation, and blurring. To address these issues, we propose a novel multi-scale and multi-layer lattice transformer (MMLattFormer) to effectively eliminate artifacts and color deviation, prevent over-enhancement, and preserve details across various scales and layers, thereby achieving more accurate and natural results in underwater image enhancement. The proposed MMLattFormer model integrates the advantage of LattFormer to enhance global perception with the advantage of “multi-scale and multi-layer” configuration to leverages the differences and complementarities between features of various scales and layers to boost local perception. The proposed MMLattFormer model is comprised of multi-scale and multi-layer LattFormers. Each LattFormer primarily encompasses two modules: Multi-head Transposed-attention Residual Network (MTRN) and Gated-attention Residual Network (GRN). The MTRN module enables cross-pixel interaction and pixel-level aggregation in an efficient manner to extract more significant and distinguishable features, whereas the GRN module can effectively suppress under-informed or redundant features and retain only useful information, enabling excellent image restoration exploiting the local and global structures of the images. Moreover, to better capture local details, we introduce depthwise convolution in these two modules before generating global attention maps and decomposing images into different features to better capture the local context in image features. The qualitative and quantitative results indicate that the proposed method outperforms state-of-the-art approaches in delivering more natural results. This is evident in its superior detail preservation, effective prevention of over-enhancement, and successful removal of artifacts and color deviation on several public datasets.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4401581339",
    "type": "article"
  },
  {
    "title": "Efficient and Privacy-Enhanced Asynchronous Federated Learning for Multimedia Data in Edge-based IoT",
    "doi": "https://doi.org/10.1145/3688002",
    "publication_date": "2024-08-16",
    "publication_year": 2024,
    "authors": "Hu Xiong; Hang Yan; Mohammad S. Obaidat; Jingxue Chen; Mingsheng Cao; Sachin Kumar; Kadambri Agarwal; Saru Kumari",
    "corresponding_authors": "",
    "abstract": "With the rapid development of smart device technology, the current version of the Internet of Things (IoT) is moving towards a multimedia IoT because of multimedia data. This innovative concept seamlessly integrates multimedia data with the IoT-Edge Continuum. Recently, a distributed learning framework shows promise in revolutionizing various industries, including smart cities, healthcare, etc. However, these applications may face challenges such as the presence of malicious devices that invade the privacy of other devices or corrupt uploaded model parameters. Additionally, the existing synchronous federated learning (FL) methods face challenges in effectively training models on local datasets due to the diversity of IoT devices. To tackle these concerns, we propose an efficient and privacy-enhanced asynchronous federated learning approach for multimedia data in edge-based IoT. In contrast to traditional FL methods, our approach combines revocable attribute-based encryption (RABE) and differential privacy (DP). This guarantees the privacy of the entire process while allowing seamless collaboration between multiple devices and the aggregation server during model training. Also, this combination brings a dynamic nature to the system. Furthermore, we utilize an asynchronous weight-based aggregation algorithm to improve the efficiency of training and the quality of the final returned model. Our proposed scheme is confirmed by theoretical safety proofs and experimental results with multimedia data. Performance evaluation shows that our framework reduces the cryptography runtime by 63.3% and the global model aggregation time by 61.9% compared to cutting-edge schemes. Moreover, our accuracy is comparable to the most primitive FL schemes, maintaining 86.7%, 70.8%, and 86.1% on MNIST, CIFAR-10, and Fashion-MNIST, respectively. The experimental results highlight the remarkable practicality, resilience and effectiveness of the proposed scheme.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4401634624",
    "type": "article"
  },
  {
    "title": "A Deep Retinex-Based Low-Light Enhancement Network Fusing Rich Intrinsic Prior Information",
    "doi": "https://doi.org/10.1145/3689642",
    "publication_date": "2024-08-23",
    "publication_year": 2024,
    "authors": "Yujie Li; Xuekai Wei; Xiaofeng Liao; You Zhao; Fan Jia; Xu Zhuang; Mingliang Zhou",
    "corresponding_authors": "",
    "abstract": "Images captured under low-light conditions are characterized by lower visual quality and perception levels than images obtained in better lighting scenarios. Studies focused on low-light enhancement techniques seek to address this dilemma. However, simple image brightening results in significant noise, blurring, and color distortion. In this paper, we present a low-light enhancement (LLE) solution that effectively synergizes Retinex theory with deep learning. Specifically, we construct an efficient image gradient map estimation module based on convolutional networks that can efficiently generate noise-free image gradient maps to assist with denoising. Second, to improve upon the traditional optimization model, we design a matrix-preserving optimization method (MPOM) coupled with deep learning modules, and it exhibits high speed and low memory consumption. Third, we incorporate image structure, image texture, and implicit prior information to optimize the enhancement process for low-light conditions and overcome prevailing limitations, such as oversmoothing, significant noise, and so forth. Through extensive experiments, we show that our approach has notable advantages over the existing methods and demonstrate superiority and effectiveness, surpassing the state-of-the-art methods by an average of 1.23 dB in PSNR for the LOL and VE-LOL datasets. The code for the proposed method is available in a public repository for open-source use: https://github.com/luxunL/DRNet .",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4401815601",
    "type": "article"
  },
  {
    "title": "Real-world Scene Image Enhancement with Contrastive Domain Adaptation Learning",
    "doi": "https://doi.org/10.1145/3694973",
    "publication_date": "2024-09-06",
    "publication_year": 2024,
    "authors": "Yongheng Zhang; Yuanqiang Cai; Danfeng Yan; Rongheng Lin",
    "corresponding_authors": "",
    "abstract": "Image enhancement methods leveraging learning-based approaches have demonstrated impressive results when trained on synthetic degraded-clear image pairs. However, when deployed in real-world scenarios, such models often suffer significant performance degradation due to the inherent domain gap between synthetic and real degradations. To bridge this gap, we propose a novel Two-stage Contrastive Domain Adaptation image Enhancement (TCDAE) framework consisting of two key strategies: (1) Synthetic-to-Real Domain Transfer Learning (S2R-DTL) that effectively translates images from the synthetic degraded domain to the real degraded domain, aligning the domains at the pixel level, and (2) Degraded-to-Clear Domain Transfer Learning (D2C-DTL) that further adapts the enhancement model from the synthetic to the real domain by translating images from the real degraded domain to the real clean domain in both supervised and unsupervised branches. A unique aspect of our approach is the integration of a Domain Noise Contrastive Estimation (DoNCE) loss in both learning strategies. This specialized loss formulation enables TCDAE to robustly translate images across domains, even in scenarios lacking strong positive examples. Consequently, our framework can generate enhanced images with natural, realistic appearances akin to real clear images. Comprehensive experiments on real-world degraded scenes across diverse tasks, including dehazing, deraining, and deblurring, demonstrate the superiority of TCDAE over state-of-the-art methods, achieving improved visual quality, quantitative metrics, and downstream task performance.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4402308682",
    "type": "article"
  },
  {
    "title": "ProtoRefine: Enhancing Prototypes with Similar Structure in Few-Shot Learning",
    "doi": "https://doi.org/10.1145/3694686",
    "publication_date": "2024-09-20",
    "publication_year": 2024,
    "authors": "Zhenyu Zhou; Qing Liao; Lei Luo; Xinwang Liu; En Zhu",
    "corresponding_authors": "",
    "abstract": "Few-shot learning presents a substantial challenge in developing robust models due to the inherent scarcity of samples within each category. To overcome this challenge, metric-based methods have been introduced, classifying images based on the relationships among samples within a given embedding space. While these methods are effective, the limited samples often result in an incomplete representation of the category’s feature space, leading to sub-optimal prototypes for classification. Recognizing this shortcoming, we identified that categories in new tasks often exhibit structural similarities with those in the relative base domain. Driven by this observation, we introduce ProtoRefine. Our approach employs the structural information of categories within the base domain that bear relevance to the new tasks, generating additional sample embeddings. This strategy refines the prototype representation, thus providing a more accurate prototype for category classification. We performed extensive experiments on popular few-shot learning benchmarks, with the results highlighting the effectiveness of ProtoRefine, especially within the 5-way 1-shot settings. Matching the competitive results of state-of-the-art methods, our work underlines the significant advantage of enhancing prototypes with structurally similar information from the base domain in the context of few-shot learning.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4402678991",
    "type": "article"
  },
  {
    "title": "TSFormer: Tracking Structure Transformer for Image Inpainting",
    "doi": "https://doi.org/10.1145/3696452",
    "publication_date": "2024-09-20",
    "publication_year": 2024,
    "authors": "Jiayu Lin; Yuan‐Gen Wang",
    "corresponding_authors": "",
    "abstract": "Recent studies have shown that image structure can significantly facilitate image inpainting. However, current approaches mostly explore structure prior without considering its guidance to texture reconstruction, leading to performance degradation. To solve this issue, we propose a two-stream Tracking Structure Transformer (TSFormer), including structure target stream and image completion stream, to capture the synchronous and dynamic interplay between structure and texture. Specifically, we first design a structure enhancement module to restore the Histograms of Oriented Gradient (HOG) and the edge of an input image in a sketch space, which forms the input of the structure target stream. Meanwhile, in the image completion stream, we design a channel-space parallel-attention component to facilitate the efficient co-learning of channel and spatial visual cues. To build a bridge between the two streams, we further develop a structure-texture cross-attention module, wherein both structure and texture are synchronously extracted through self-attention, and texture extraction is implemented by dynamically tracking the structure in a cross-attention fashion, enabling the capture of the intricate interaction between structure and texture. Extensive experiments evaluated on three benchmark datasets, including CelebA, Places2, and Paris StreetView, demonstrate that the proposed TSFormer achieves state-of-the-art performance compared to its competitors. The code is available at https://github.com/GZHU-DVL/TSFormer .",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4402679002",
    "type": "article"
  },
  {
    "title": "Improving Reference-based Distinctive Image Captioning with Contrastive Rewards",
    "doi": "https://doi.org/10.1145/3694683",
    "publication_date": "2024-09-24",
    "publication_year": 2024,
    "authors": "Y. Mao; Jun Xiao; Dong Zhang; Meng Cao; Jian Shao; Yueting Zhuang; Long Chen",
    "corresponding_authors": "",
    "abstract": "Distinctive Image Captioning (DIC) — generating distinctive captions that describe the unique details of a target image — has received considerable attention over the last few years. A recent DIC method proposes to generate distinctive captions by comparing the target image with a set of semantic-similar reference images, i . e ., reference-based DIC (Ref-DIC). It aims to force the generated captions to distinguish between the target image and the reference image. Unfortunately, reference images used by existing Ref-DIC works are easy to distinguish: these reference images only resemble the target image at scene-level and have few common objects, such that a Ref-DIC model can trivially generate distinctive captions even without considering the reference images. For example, if the target image contains objects “ towel ” and “ toilet ” while all reference images are without them, then a simple caption “ A bathroom with a towel and a toilet ” is distinctive enough to tell apart target and reference images. To ensure Ref-DIC models really perceive the unique objects (or attributes) in target images, we first propose two new Ref-DIC benchmarks. Specifically, we design a two-stage matching mechanism, which strictly controls the similarity between the target and reference images at the object-/attribute- level (v.s. scene-level). Secondly, to generate distinctive captions, we develop a Transformer-based Ref-DIC baseline TransDIC . It not only extracts visual features from the target image, but also encodes the differences between objects in the target and reference images. Taking one step further, we propose a stronger TransDIC++ , which consists of an extra contrastive learning module to make full use of the reference images. This new module is model-agnostic, which can be easily incorporated into various Ref-DIC architectures. Finally, for more trustworthy benchmarking, we propose a new evaluation metric named DisCIDEr for Ref-DIC, which evaluates both the accuracy and distinctiveness of the generated captions. Experimental results demonstrate that our TransDIC++ can generate distinctive captions. Besides, it outperforms several state-of-the-art models on the two new benchmarks over different metrics.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4402760797",
    "type": "article"
  },
  {
    "title": "Establishing Trust and Security in Decentralized Metaverse: A Web 3.0 Approach",
    "doi": "https://doi.org/10.1145/3696454",
    "publication_date": "2024-09-24",
    "publication_year": 2024,
    "authors": "Daniel Mebrahtom; Siem Hadish; Aron Sbhatu; Moayad Aloqaily; Mohsen Guizani",
    "corresponding_authors": "",
    "abstract": "The integration of blockchain and Web 3.0 technologies offers significant advancements in identity management and trust within decentralized Metaverse environments. Despite the rapid development of numerous Metaverse platforms, these environments often operate in isolation, lacking interoperability and cohesive security measures. This paper proposes a novel architecture leveraging Self-Sovereign Identity (SSI) principles and blockchain technology to achieve secure and interoperable interactions across different Metaverse platforms. Our framework encompasses a digital wallet application, an interactive virtual environment, and a secure blockchain-based backend infrastructure. We detail the system architecture, operational specifics, and demonstrate significant improvements in scalability, efficiency, and security through comprehensive performance evaluations. Additionally, we address the challenges of real-world implementation and propose viable solutions. By enhancing user control over personal data and enabling secure cross-platform interactions, our approach aims to foster a more secure, interoperable, and user-centric decentralized Metaverse.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4402760993",
    "type": "article"
  },
  {
    "title": "Occupancy Map Guided Attributes Artifacts Removal for Video-based Point Cloud Compression",
    "doi": "https://doi.org/10.1145/3697351",
    "publication_date": "2024-09-25",
    "publication_year": 2024,
    "authors": "Peilin Chen; Shiqi Wang; Zhu Li",
    "corresponding_authors": "",
    "abstract": "Point clouds offer realistic three-dimensional (3-D) representations of objects and scenes at the expense of large data volumes. To represent such data compactly in real-world applications, Video-based Point Cloud Compression (V-PCC) converts their texture into two-dimensional (2-D) attributes and occupancy maps before applying lossy video compression. Unfortunately, the coding artifacts introduced in the decoded attribute maps eventually degrade the quality of the reconstructed point cloud, thereby influencing its immersive experience. This paper proposes a deep learning-based attribute map enhancement method that fully leverages the occupancy map's guidance. The design philosophy is that the cross-modality guidance from occupancy can be leveraged as critical information to enhance the attribute. Therefore, instead of treating attribute and occupancy as two separate sources of signals, occupancy serves as an indispensable auxiliary, such that the proposed framework explicitly provides the model with abundant clues by conducting local feature modification and global dependencies aggregation. In particular, the proposed framework is compatible with existing V-PCC bitstreams and can be feasibly incorporated into the standardized decoder pipeline. Extensive evaluations show the effectiveness of the proposed framework in attribute enhancement, with equivalently 6.0% Bjontegaard Delta-rate (BD-rate) savings obtained.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4402826826",
    "type": "article"
  },
  {
    "title": "Enhanced Multi-Object Tracking: Inferring Motion States of Tracked Objects",
    "doi": "https://doi.org/10.1145/3699960",
    "publication_date": "2024-10-11",
    "publication_year": 2024,
    "authors": "Pan Liao; Feng Yang; Di Wu; Bo Liu; Xingle Zhang; Shanding Zhou",
    "corresponding_authors": "",
    "abstract": "Multi-Object Tracking (MOT) is a critical problem in computer vision, yet current research on tracking the motion state of objects relative to the ground remains limited. The extant literature exhibits a notable dearth in the exploration of this aspect. Deep learning methodologies encounter challenges in accurately discerning object motion states, while conventional approaches reliant on comprehensive mathematical modeling may yield suboptimal tracking accuracy. To address these challenges, we introduce a Model-Data-Driven Motion State Judgment Object Tracking Method (MoD2T). This innovative architecture adeptly amalgamates classical mathematical modeling with deep learning-based multi-object tracking frameworks. The integration of mathematical modeling and deep learning within MoD2T enhances the precision of object motion state determination, thereby elevating tracking accuracy. Our empirical investigations comprehensively validate the efficacy of MoD2T across varied scenarios, encompassing unmanned aerial vehicle surveillance and street-level tracking. Furthermore, to gauge the method's adeptness in discerning object motion states, we introduce the Motion State Validation F1 (MVF1) metric.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4403319716",
    "type": "article"
  },
  {
    "title": "Towards Complex-query Referring Image Segmentation: A Novel Benchmark",
    "doi": "https://doi.org/10.1145/3701733",
    "publication_date": "2024-11-04",
    "publication_year": 2024,
    "authors": "Wei Ji; Li Li; Hao Fei; X. H. Liu; Xun Yang; Juncheng Li; Roger Zimmermann",
    "corresponding_authors": "",
    "abstract": "Referring Image Segmentation (RIS) has been extensively studied over the past decade, leading to the development of advanced algorithms. However, there has been a lack of research investigating how existing algorithms should be benchmarked with complex language queries, which include more informative descriptions of surrounding objects and backgrounds ( e.g. “the black car.” vs. “the black car is parking on the road and beside the bus.” ). Given the significant improvement in the semantic understanding capability of large pre-trained models, it is crucial to take a step further in RIS by incorporating complex language that resembles real-world applications. To close this gap, building upon the existing RefCOCO and Visual Genome datasets, we propose a new RIS benchmark with complex queries, namely RIS-CQ . The RIS-CQ dataset is of high quality and large scale, which challenges the existing RIS with enriched, specific and informative queries, and enables a more realistic scenario of RIS research. Besides, we present a nichetargeting method to better task the RIS-CQ, called dual-modality graph alignment model ( DuMoGa ), which outperforms a series of RIS methods. To provide a valuable foundation for future advancements in the field of RIS with complex queries, we release the datasets, preprocessing and synthetic scripts, and the algorithm implementations at https://github.com/lili0415/DuMoGa .",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4404020436",
    "type": "article"
  },
  {
    "title": "Meta-Review of Wearable Devices for Healthcare in the Metaverse",
    "doi": "https://doi.org/10.1145/3705320",
    "publication_date": "2024-11-24",
    "publication_year": 2024,
    "authors": "Monireh Vahdati; Fedwa Laamarti; Abdulmotaleb El Saddik",
    "corresponding_authors": "",
    "abstract": "In recent years, there has been a growing interest in leveraging the metaverse to enhance community engagement and healthcare. This paper provides a comprehensive examination of wearable devices and sensors utilized within immersive environments to improve well-being and healthcare outcomes. We categorize the healthcare application domains that employ wearable devices and identify commonly used devices and sensors based on a thorough review of the literature. Our study offers a detailed summary of these applications, highlighting their potential to enhance overall quality of life through remote monitoring, rehabilitation, and chronic disease management. Furthermore, we address existing research gaps and challenges in this field, offering insights for future research directions. This meta-review emphasizes the need for further exploration in the rapidly evolving domain of wearable healthcare technologies within the metaverse, presenting an overview of the current state of wearable devices in healthcare and underscoring their significance in advancing healthcare delivery and outcomes.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4404658067",
    "type": "article"
  },
  {
    "title": "Semantics and feature discovery via confidence-based ensemble",
    "doi": "https://doi.org/10.1145/1062253.1062257",
    "publication_date": "2005-05-01",
    "publication_year": 2005,
    "authors": "King-Shy Goh; Beitao Li; Edward Yi Chang",
    "corresponding_authors": "",
    "abstract": "Providing accurate and scalable solutions to map low-level perceptual features to high-level semantics is essential for multimedia information organization and retrieval. In this paper, we propose a confidence-based dynamic ensemble (CDE) to overcome the shortcomings of the traditional static classifiers. In contrast to the traditional models, CDE can make dynamic adjustments to accommodate new semantics, to assist the discovery of useful low-level features, and to improve class-prediction accuracy. We depict two key components of CDE: a multi-level function that asserts class-prediction confidence, and the dynamic ensemble method based upon the confidence function. Through theoretical analysis and empirical study, we demonstrate that CDE is effective in annotating large-scale, real-world image datasets.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2006706581",
    "type": "article"
  },
  {
    "title": "Smooth and efficient real-time video transport in the presence of wireless errors",
    "doi": "https://doi.org/10.1145/1142020.1142022",
    "publication_date": "2006-05-01",
    "publication_year": 2006,
    "authors": "Guang Yang; Tony Sun; Mário Gerla; M.Y. Sanadidi; Ling‐Jyh Chen",
    "corresponding_authors": "",
    "abstract": "In this article we study a smooth and efficient transport protocol for real-time video over wireless networks. The proposed scheme, named the video transport protocol (VTP), has a new and unique end-to-end rate control mechanism that aims to avoid drastic rate fluctuations while maintaining friendliness to legacy protocols. VTP is also equipped with an achieved rate estimation scheme and a loss discrimination algorithm, both end-to-end, to cope with random errors in wireless networks efficiently. We show by analysis that VTP preserves most of the convergence properties of AIMD and converges to its fair share fast. VTP is compared to two recent TCP friendly rate control (TFRC) extensions, namely TFRC Wireless and MULTFRC, in wired-cum-wireless scenarios in Ns-2. Results show that VTP excels in all tested scenarios in terms of smoothness, fairness, and opportunistic friendliness. VTP is also implemented to work with a video camera and an H.263 video codec as part of our hybrid testbed, where its good performance as a transport layer protocol is confirmed by measurement results.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2124762593",
    "type": "article"
  },
  {
    "title": "Automatic summarization of music videos",
    "doi": "https://doi.org/10.1145/1142020.1142023",
    "publication_date": "2006-05-01",
    "publication_year": 2006,
    "authors": "Xi Shao; Changsheng Xu; Namunu C. Maddage; Qi Tian; Mohan Kankanhalli; Jesse S. Jin",
    "corresponding_authors": "",
    "abstract": "In this article, we propose a novel approach for automatic music video summarization. The proposed summarization scheme is different from the current methods used for video summarization. The music video is separated into the music track and video track. For the music track, a music summary is created by analyzing the music content using music features, an adaptive clustering algorithm, and music domain knowledge. Then, shots in the video track are detected and clustered. Finally, the music video summary is created by aligning the music summary and clustered video shots. Subjective studies by experienced users have been conducted to evaluate the quality of music summaries and effectiveness of the proposed summarization approach. Experiments are performed on different genres of music videos and comparisons are made with the summaries generated based on music track, video track, and manually. The evaluation results indicate that summaries generated using the proposed method are effective in helping realize users' expectations.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W1989605374",
    "type": "article"
  },
  {
    "title": "Hierarchical video patching with optimal server bandwidth",
    "doi": "https://doi.org/10.1145/1324287.1324295",
    "publication_date": "2008-01-01",
    "publication_year": 2008,
    "authors": "Helmut Hlavacs; Shelley Buchinger",
    "corresponding_authors": "",
    "abstract": "Video patching is a way for transporting true video-on-demand, that is, instantaneous without any delay, from a video server to several clients. Instead of sending a unique stream to each newly arriving client, clients share as many multicast transmissions as possible, and are serviced only those parts of the video that they have missed. We present a novel video patching scheme using hierarchies of patches. Our scheme minimizes the bandwidth needed by the video server, and may result in the fact that clients receive several streams in parallel. We show analytically that for Poisson arrival our algorithm achieves the optimal possible server bandwidth for all schemes where clients share multicast transmissions. We also show, how our approach can be combined with batching. This combination requires less server bandwidth than all fixed start point periodic broadcast algorithms.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W1964928026",
    "type": "article"
  },
  {
    "title": "Critical causal order of events in distributed virtual environments",
    "doi": "https://doi.org/10.1145/1236471.1236474",
    "publication_date": "2007-08-01",
    "publication_year": 2007,
    "authors": "Suiping Zhou; Wentong Cai; Stephen John Turner; Bu‐Sung Lee; Junhu Wei",
    "corresponding_authors": "",
    "abstract": "We investigate the causal order of events in distributed virtual environments (DVEs). We first define the critical causal order relation among the events. Then, we propose some mechanisms to enhance the prevalent RO (receive order delivery) mechanism in DVEs so that the real-time property of DVEs is preserved while the critical causal order violations are reduced. These mechanisms are implemented as a middleware. Experimental results show that the middleware performs well in reducing the critical causality violations in simulation and incurs little processing overhead.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W1999679789",
    "type": "article"
  },
  {
    "title": "Nuisance level of a voice call",
    "doi": "https://doi.org/10.1145/1404880.1404886",
    "publication_date": "2008-10-01",
    "publication_year": 2008,
    "authors": "Prakash Kolan; Ram Dantu; João W. Cangussu",
    "corresponding_authors": "",
    "abstract": "In our everyday life, we communicate with many people such as family, friends, neighbors, and colleagues. We communicate with them using different communication media such as email, telephone calls, and face-to-face interactions. While email is not real-time and face-to-face communications require geographic proximity, voice and video communications are preferred over other modes of communication. However, real-time voice/video calls may create nuisance to the receiver. In this article, we describe a mathematical model for computing nuisance level of incoming voice/video calls. We computed the closeness and nuisance level using the calling patterns between the caller and the callee. To validate the nuisance model, we collected cell phone call records of real-life people at our university and computed the nuisance value for all voice calls. We validated the nuisance levels using the feedback from those real-life people. Such a nuisance model is useful for predicting unwanted voice and video sessions in an IP communication network.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2053075734",
    "type": "article"
  },
  {
    "title": "Introduction to special issue",
    "doi": "https://doi.org/10.1145/1412196.1412197",
    "publication_date": "2008-10-01",
    "publication_year": 2008,
    "authors": "Pablo César; Dick C. A. Bulterman; Luiz Fernando Gomes Soares",
    "corresponding_authors": "",
    "abstract": "The research area of interactive digital TV is in the midst of a significant revival. Unlike the first generation of digital TV, which focused on producer concerns that effectively limited (re)distribution, the current generation of research is closely linked to the role of the user in selecting, producing, and distributing content. The research field of interactive digital television is being transformed into a study of human-centered television. Our guest editorial reviews relevant aspects of this transformation in the three main stages of the content lifecycle: content production, content delivery, and content consumption. While past research on content production tools focused on full-fledged authoring tools for professional editors, current research studies lightweight, often informal end-user authoring systems. In terms of content delivery, user-oriented infrastructures such as peer-to-peer are being seen as alternatives to more traditional broadcast solutions. Moreover, end-user interaction is no longer limited to content selection, but now facilitates nonlinear participatory television productions. Finally, user-to-user communication technologies have allowed television to become a central component of an interconnected social experience. The background context given in this article provides a framework for appreciating the significance of four detailed contributions that highlight important directions in transforming interactive television research.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2075418162",
    "type": "article"
  },
  {
    "title": "Design of multimedia surveillance systems",
    "doi": "https://doi.org/10.1145/1556134.1556140",
    "publication_date": "2009-08-01",
    "publication_year": 2009,
    "authors": "G. S. V. S. Sivaram; Mohan Kankanhalli; K.R. Ramakrishnan",
    "corresponding_authors": "",
    "abstract": "This article addresses the problem of how to select the optimal combination of sensors and how to determine their optimal placement in a surveillance region in order to meet the given performance requirements at a minimal cost for a multimedia surveillance system. We propose to solve this problem by obtaining a performance vector, with its elements representing the performances of subtasks, for a given input combination of sensors and their placement. Then we show that the optimal sensor selection problem can be converted into the form of Integer Linear Programming problem (ILP) by using a linear model for computing the optimal performance vector corresponding to a sensor combination. Optimal performance vector corresponding to a sensor combination refers to the performance vector corresponding to the optimal placement of a sensor combination. To demonstrate the utility of our technique, we design and build a surveillance system consisting of PTZ (Pan-Tilt-Zoom) cameras and active motion sensors for capturing faces. Finally, we show experimentally that optimal placement of sensors based on the design maximizes the system performance.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2123784270",
    "type": "article"
  },
  {
    "title": "Towards attention-centered interfaces",
    "doi": "https://doi.org/10.1145/1386109.1386111",
    "publication_date": "2008-08-01",
    "publication_year": 2008,
    "authors": "Ingmar S. Franke; Sebastian Pannasch; Jens R. Helmert; Robert Rieger; Rainer Groh; Boris M. Velichkovsky",
    "corresponding_authors": "",
    "abstract": "The established method of representing three-dimensional space on a two-dimensional surface involves camera based, point of regard systems, comparable in design to the early “camera obscura”. However, geometrical limitations of such models lead to distortions of perspective when projected. This research investigated the influence of single- versus multi-perspectives on aesthetic choices within one image. A clear perceptual bias towards multi-perspective images was found, additionally supported by an eye tracking study. We propose that human users are more attracted by multi-perspective images, which emphasise the “semantic foci” of the scene, than by those being synthesized statically with only one geometrical prospect.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2152873115",
    "type": "article"
  },
  {
    "title": "Image label completion by pursuing contextual decomposability",
    "doi": "https://doi.org/10.1145/2168996.2169001",
    "publication_date": "2012-05-01",
    "publication_year": 2012,
    "authors": "Xiaobai Liu; Shuicheng Yan; Tat‐Seng Chua; Hai Jin",
    "corresponding_authors": "",
    "abstract": "This article investigates how to automatically complete the missing labels for the partially annotated images, without image segmentation. The label completion procedure is formulated as a nonnegative data factorization problem, to decompose the global image representations that are used for describing the entire images, for instance, various image feature descriptors, into their corresponding label representations, that are used for describing the local semantic regions within images. The solution provided in this work is motivated by following observations. First, label representations of the regions with the same label often share certain commonness, yet may be essentially different due to the large intraclass variations. Thus, each label or concept should be represented by using a subspace spanned by an ensemble of basis, instead of a single one, to characterize the intralabel diversities. Second, the subspaces for different labels are different from each other. Third, while two images are similar with each other, the corresponding label representations should be similar. We formulate this cross-image context as well as the given partial label annotations in the framework of nonnegative data factorization and then propose an efficient multiplicative nonnegative update rules to alternately optimize the subspaces and the reconstruction coefficients. We also provide the theoretic proof of algorithmic convergence and correctness. Extensive experiments over several challenging image datasets clearly demonstrate the effectiveness of our proposed solution in boosting the quality of image label completion and image annotation accuracy. Based on the same formulation, we further develop a label ranking algorithms, to refine the noised image labels without any manual supervision. We compare the proposed label ranking algorithm with the state-of-the-arts over the popular evaluation databases and achieve encouragingly improvements.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2062709888",
    "type": "article"
  },
  {
    "title": "Modeling progressive mesh streaming",
    "doi": "https://doi.org/10.1145/1925101.1925105",
    "publication_date": "2011-02-01",
    "publication_year": 2011,
    "authors": "Wei Cheng; Wei Tsang Ooi; Sébastien Mondet; Romulus Grigoraş; Géraldine Morin",
    "corresponding_authors": "",
    "abstract": "3D triangular meshes are becoming an increasingly prevalent data type in networked applications such as digital museums, online games, and virtual worlds. In these applications, a 3D mesh is typically coded progressively, yielding a multiresolution representation suitable for streaming. While such progressive coding allows incremental rendering for users while data is being transmitted, it introduces dependencies between data, causing delay in rendering when packets are lost. This article quantitatively analyzes the effects of such dependency by modeling the distribution of decoding time as a function of mesh properties and network parameters. We apply our model to study two extreme cases of dependency in progressive meshes and show that the effect of dependencies on decoded mesh quality diminishes with time. Our model provides the expected decoded mesh quality at the receiver at a given time. Based on this expected value, we propose a packetization strategy that improves the decoded mesh quality during the initial stage of streaming. We validate the accuracy of our model under a variety of network conditions, including bursty losses, fluctuating RTT, and varying sending rate. The values predicted from our model match the measured value reasonably well in all cases except when losses are too bursty.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2065157358",
    "type": "article"
  },
  {
    "title": "Fusing multiple video sensors for surveillance",
    "doi": "https://doi.org/10.1145/2071396.2071403",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Lauro Snidaro; Ingrid Visentini; Gian Luca Foresti",
    "corresponding_authors": "",
    "abstract": "Real-time detection, tracking, recognition, and activity understanding of moving objects from multiple sensors represent fundamental issues to be solved in order to develop surveillance systems that are able to autonomously monitor wide and complex environments. The algorithms that are needed span therefore from image processing to event detection and behaviour understanding, and each of them requires dedicated study and research. In this context, sensor fusion plays a pivotal role in managing the information and improving system performance. Here we present a novel fusion framework for combining the data coming from multiple and possibly heterogeneous sensors observing a surveillance area.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W1991365832",
    "type": "article"
  },
  {
    "title": "Near-duplicate keyframe retrieval by semi-supervised learning and nonrigid image matching",
    "doi": "https://doi.org/10.1145/1870121.1870125",
    "publication_date": "2011-01-01",
    "publication_year": 2011,
    "authors": "Jianke Zhu; Steven C. H. Hoi; Michael R. Lyu; Shuicheng Yan",
    "corresponding_authors": "",
    "abstract": "Near-duplicate keyframe (NDK) retrieval techniques are critical to many real-world multimedia applications. Over the last few years, we have witnessed a surge of attention on studying near-duplicate image/keyframe retrieval in the multimedia community. To facilitate an effective approach to NDK retrieval on large-scale data, we suggest an effective Multi-Level Ranking (MLR) scheme that effectively retrieves NDKs in a coarse-to-fine manner. One key stage of the MLR ranking scheme is how to learn an effective ranking function with extremely small training examples in a near-duplicate detection task. To attack this challenge, we employ a semi-supervised learning method, semi-supervised support vector machines, which is able to significantly improve the retrieval performance by exploiting unlabeled data. Another key stage of the MLR scheme is to perform a fine matching among a subset of keyframe candidates retrieved from the previous coarse ranking stage. In contrast to previous approaches based on either simple heuristics or rigid matching models, we propose a novel Nonrigid Image Matching (NIM) approach to tackle near-duplicate keyframe retrieval from real-world video corpora in order to conduct an effective fine matching. Compared with the conventional methods, the proposed NIM approach can recover explicit mapping between two near-duplicate images with a few deformation parameters and find out the correct correspondences from noisy data simultaneously. To evaluate the effectiveness of our proposed approach, we performed extensive experiments on two benchmark testbeds extracted from the TRECVID2003 and TRECVID2004 corpora. The promising results indicate that our proposed method is more effective than other state-of-the-art approaches for near-duplicate keyframe retrieval.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2023693944",
    "type": "article"
  },
  {
    "title": "Object-based image retrieval with kernel on adjacency matrix and local combined features",
    "doi": "https://doi.org/10.1145/2379790.2379796",
    "publication_date": "2012-11-01",
    "publication_year": 2012,
    "authors": "Heng Qi; Keqiu Li; Yanming Shen; Wenyu Qu",
    "corresponding_authors": "",
    "abstract": "In object-based image retrieval, there are two important issues: an effective image representation method for representing image content and an effective image classification method for processing user feedback to find more images containing the user-desired object categories. In the image representation method, the local-based representation is the best selection for object-based image retrieval. As a kernel-based classification method, Support Vector Machine (SVM) has shown impressive performance on image classification. But SVM cannot work on the local-based representation unless there is an appropriate kernel. To address this problem, some representative kernels are proposed in literatures. However, these kernels cannot work effectively in object-based image retrieval due to ignoring the spatial context and the combination of local features. In this article, we present Adjacent Matrix (AM) and the Local Combined Features (LCF) to incorporate the spatial context and the combination of local features into the kernel. We propose the AM-LCF feature vector to represent image content and the AM-LCF kernel to measure the similarities between AM-LCF feature vectors. According to the detailed analysis, we show that the proposed kernel can overcome the deficiencies of existing kernels. Moreover, we evaluate the proposed kernel through experiments of object-based image retrieval on two public image sets. The experimental results show that the performance of object-based image retrieval can be improved by the proposed kernel.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2039235422",
    "type": "article"
  },
  {
    "title": "On the Effectiveness of Offset Projections for 360-Degree Video Streaming",
    "doi": "https://doi.org/10.1145/3209660",
    "publication_date": "2018-06-15",
    "publication_year": 2018,
    "authors": "Chao Zhou; Zhenhua Li; Joe Osgood; Yao Liu",
    "corresponding_authors": "",
    "abstract": "A new generation of video streaming technology, 360-degree video, promises greater immersiveness than standard video streams. This level of immersiveness is similar to that produced by virtual reality devices—users can control the field of view using head movements rather than needing to manipulate external devices. Although 360-degree video could revolutionize the streaming experience, its large-scale adoption is hindered by a number of factors: 360-degree video streams have larger bandwidth requirements and require faster responsiveness to user inputs, and users may be more sensitive to lower quality streams. In this article, we review standard approaches toward 360-degree video encoding and compare these to families of approaches that distort the spherical surface to allow oriented concentrations of the 360-degree view. We refer to these distorted projections as offset projections. Our measurement studies show that most types of offset projections produce rendered views with better quality than their nonoffset equivalents when view orientations are within 40 or 50 degrees of the offset orientation. Offset projections complicate adaptive 360-degree video streaming because they require a combination of bitrate and view orientation adaptations. We estimate that this combination of streaming adaptation in two dimensions can cause over 57% extra segments to be downloaded compared to an ideal downloading strategy, wasting 20% of the total downloading bandwidth.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2809041286",
    "type": "article"
  },
  {
    "title": "Symmetrical Residual Connections for Single Image Super-Resolution",
    "doi": "https://doi.org/10.1145/3282445",
    "publication_date": "2019-02-25",
    "publication_year": 2019,
    "authors": "Xianguo Li; Yemei Sun; Yanli Yang; Changyun Miao",
    "corresponding_authors": "",
    "abstract": "Single-image super-resolution (SISR) methods based on convolutional neural networks (CNN) have shown great potential in the literature. However, most deep CNN models don’t have direct access to subsequent layers, seriously hindering the information flow. Furthermore, they fail to make full use of the hierarchical features from different low-level layers, thereby resulting in relatively low accuracy. In this article, we present a new SISR CNN, called SymSR, which incorporates symmetrical nested residual connections to improve both the accuracy and the execution speed. SymSR takes a larger image region for contextual spreading. It symmetrically combines multiple short paths for the forward propagation to improve the accuracy and for the backward propagation of gradient flow to accelerate the convergence speed. Extensive experiments based on open challenge datasets show the effectiveness of symmetrical residual connections. Compared with four other state-of-the-art super-resolution CNN methods, SymSR is superior in both accuracy and runtime.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2917823630",
    "type": "article"
  },
  {
    "title": "Collaborations on YouTube",
    "doi": "https://doi.org/10.1145/3241054",
    "publication_date": "2018-10-10",
    "publication_year": 2018,
    "authors": "Christian Koch; Moritz Lode; Denny Stohr; Amr Rizk; Ralf Steinmetz",
    "corresponding_authors": "",
    "abstract": "YouTube is the most popular platform for streaming of user-generated videos. Nowadays, professional YouTubers are organized in so-called multichannel networks (MCNs). These networks offer services such as brand deals, equipment, and strategic advice in exchange for a share of the YouTubers’ revenues. A dominant strategy to gain more subscribers and, hence, revenue is collaborating with other YouTubers. Yet, collaborations on YouTube have not been studied in a detailed quantitative manner. To close this gap, first, we collect a YouTube dataset covering video statistics over 3 months for 7,942 channels. Second, we design a framework for collaboration detection given a previously unknown number of persons featured in YouTube videos. We denote this framework, for the detection and analysis of collaborations in YouTube videos using a Deep Neural Network (DNN)-based approach, as CATANA. Third, we analyze about 2.4 years of video content and use CATANA to answer research questions guiding YouTubers and MCNs for efficient collaboration strategies. Thereby, we focus on (1) collaboration frequency and partner selectivity, (2) the influence of MCNs on channel collaborations, (3) collaborating channel types, and (4) the impact of collaborations on video and channel popularity. Our results show that collaborations are in many cases significantly beneficial regarding viewers and newly attracted subscribers for both collaborating channels, often showing more than 100% popularity growth compared with noncollaboration videos.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2963891776",
    "type": "article"
  },
  {
    "title": "An End-to-End Attention-Based Neural Model for Complementary Clothing Matching",
    "doi": "https://doi.org/10.1145/3368071",
    "publication_date": "2019-11-30",
    "publication_year": 2019,
    "authors": "Jinhuan Liu; Xuemeng Song; Liqiang Nie; Tian Gan; Jun Ma",
    "corresponding_authors": "",
    "abstract": "In modern society, people tend to prefer fashionable and decent outfits that can meet more than basic physiological needs. In fact, a proper outfit usually relies on good matching among complementary fashion items (e.g., the top, bottom, and shoes) that compose it, which thus propels us to investigate the automatic complementary clothing matching scheme. However, this is non-trivial due to the following challenges. First, the main challenge lies in how to accurately model the compatibility between complementary fashion items (e.g., the top and bottom) that come from the heterogeneous spaces with multi-modalities (e.g., the visual modality and textual modality). Second, since different features (e.g., the color, style, and pattern) of fashion items may contribute differently to compatibility modeling, how to encode the confidence of different pairwise features presents a tough challenge. Third, how to jointly learn the latent representation of multi-modal data and the compatibility between complementary fashion items contributes to the last challenge. Toward this end, in this work, we present an end-to-end attention-based neural framework for the compatibility modeling, where we introduce a feature-level attention model to adaptively learn the confidence for different pairwise features. Extensive experiments on a public available real-world dataset show the superiority of our model over state-of-the-art methods.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W3007947854",
    "type": "article"
  },
  {
    "title": "Affective Content-aware Adaptation Scheme on QoE Optimization of Adaptive Streaming over HTTP",
    "doi": "https://doi.org/10.1145/3328997",
    "publication_date": "2019-11-30",
    "publication_year": 2019,
    "authors": "Shenghong Hu; Min Xu; Haimin Zhang; Chunxia Xiao; Chao Gui",
    "corresponding_authors": "",
    "abstract": "The article presents a novel affective content-aware adaptation scheme (ACAA) to optimize Quality of Experience (QoE) for dynamic adaptive video streaming over HTTP (DASH). Most of the existing DASH adaptation schemes conduct video bit-rate adaptation based on an estimation of available network resources, which ignore user preference on affective content (AC) embedded in video data streaming over the network. Since the personal demands to AC is very different among all viewers, to satisfy individual affective demand is critical to improve the QoE in commercial video services. However, the results of video affective analysis cannot be applied into a current adaptive streaming scheme directly. Correlating the AC distributions in user's viewing history to each being streamed segment, the affective relevancy can be inferred as an affective metric for the AC related segment. Further, we have proposed an ACAA scheme to optimize QoE for user desired affective content while taking into account both network status and affective relevancy. We have implemented the ACAA scheme over a realistic trace-based evaluation and compared its performance in terms of network performance, QoE with that of Probe and Adaptation (PANDA), buffer-based adaptation (BBA), and Model Predictive Control (MPC). Experimental results show that ACAA can preserve available buffer time for future being delivered affective content preferred by viewer's individual preference to achieve better QoE in affective contents than those normal contents while remain the overall QoE to be satisfactory.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W3017129869",
    "type": "article"
  },
  {
    "title": "Image retrieval with query-adaptive hashing",
    "doi": "https://doi.org/10.1145/2422956.2422958",
    "publication_date": "2013-02-01",
    "publication_year": 2013,
    "authors": "Dong Liu; Shuicheng Yan; Rongrong Ji; Xian‐Sheng Hua; Hao Zhang",
    "corresponding_authors": "",
    "abstract": "Hashing-based approximate nearest-neighbor search may well realize scalable content-based image retrieval. The existing semantic-preserving hashing methods leverage the labeled data to learn a fixed set of semantic-aware hash functions. However, a fixed hash function set is unable to well encode all semantic information simultaneously, and ignores the specific user's search intention conveyed by the query. In this article, we propose a query-adaptive hashing method which is able to generate the most appropriate binary codes for different queries. Specifically, a set of semantic-biased discriminant projection matrices are first learnt for each of the semantic concepts, through which a semantic-adaptable hash function set is learnt via a joint sparsity variable selection model. At query time, we further use the sparsity representation procedure to select the most appropriate hash function subset that is informative to the semantic information conveyed by the query. Extensive experiments over three benchmark image datasets well demonstrate the superiority of our proposed query-adaptive hashing method over the state-of-the-art ones in terms of retrieval accuracy.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2030861432",
    "type": "article"
  },
  {
    "title": "Annotation propagation in image databases using similarity graphs",
    "doi": "https://doi.org/10.1145/2487736",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Michael E. Houle; Vincent Oria; Shin’ichi Satoh; Jichao Sun",
    "corresponding_authors": "",
    "abstract": "The practicality of large-scale image indexing and querying methods depends crucially upon the availability of semantic information. The manual tagging of images with semantic information is in general very labor intensive, and existing methods for automated image annotation may not always yield accurate results. The aim of this paper is to reduce to a minimum the amount of human intervention required in the semantic annotation of images, while preserving a high degree of accuracy. Ideally, only one copy of each object of interest would be labeled manually, and the labels would then be propagated automatically to all other occurrences of the objects in the database. To this end, we propose an influence propagation strategy, SW-KProp , that requires no human intervention beyond the initial labeling of a subset of the images. SW-KProp distributes semantic information within a similarity graph defined on all images in the database: each image iteratively transmits its current label information to its neighbors, and then readjusts its own label according to the combined influences of its neighbors. SW-KProp influence propagation can be efficiently performed by means of matrix computations, provided that pairwise similarities of images are available. We also propose a variant of SW-KProp which enhances the quality of the similarity graph by selecting a reduced feature set for each prelabeled image and rebuilding its neighborhood. The performances of the SW-KProp method and its variant were evaluated against several competing methods on classification tasks for three image datasets: a handwritten digit dataset, a face dataset and a web image dataset. For the digit images, SW-KProp and its variant performed consistently better than the other methods tested. For the face and web images, SW-KProp outperformed its competitors for the case when the number of prelabeled images was relatively small. The performance was seen to improve significantly when the feature selection strategy was applied.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2095409239",
    "type": "article"
  },
  {
    "title": "Audio Musical Dice Game",
    "doi": "https://doi.org/10.1145/2710015",
    "publication_date": "2015-06-02",
    "publication_year": 2015,
    "authors": "Yin-Tzu Lin; I-Ting Liu; Jyh‐Shing Roger Jang; Ja‐Ling Wu",
    "corresponding_authors": "",
    "abstract": "This article proposes a framework for creating user-preference-aware music medleys from users' music collections. We treat the medley generation process as an audio version of a musical dice game. Once the user's collection has been analyzed, the system is able to generate various pleasing medleys. This flexibility allows users to create medleys according to the specified conditions, such as the medley structure or the must-use clips. Even users without musical knowledge can compose medley songs from their favorite tracks. The effectiveness of the system has been evaluated through both objective and subjective experiments on individual components in the system.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2200211686",
    "type": "article"
  },
  {
    "title": "The Cameraman Operating My Virtual Camera is Artificial",
    "doi": "https://doi.org/10.1145/2744411",
    "publication_date": "2015-06-02",
    "publication_year": 2015,
    "authors": "Vamsidhar Reddy Gaddam; Ragnhild Eg; Ragnar Langseth; Carsten Griwodz; Pål Halvorsen",
    "corresponding_authors": "",
    "abstract": "In this article, we argue that the energy spent in designing autonomous camera control systems is not spent in vain. We present a real-time virtual camera system that can create smooth camera motion. Similar systems are frequently benchmarked with the human operator as the best possible reference; however, we avoid a priori assumptions in our evaluations. Our main question is simply whether we can design algorithms to steer a virtual camera that can compete with the user experience for recordings from an expert operator with several years of experience? In this respect, we present two low-complexity servoing methods that are explored in two user studies. The results from the user studies give a promising answer to the question pursued. Furthermore, all components of the system meet the real-time requirements on commodity hardware. The growing capabilities of both hardware and network in mobile devices give us hope that this system can be deployed to mobile users in the near future. Moreover, the design of the presented system takes into account that services to concurrent users must be supported.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2271886155",
    "type": "article"
  },
  {
    "title": "Design and Analysis of QoE-Aware Quality Adaptation for DASH",
    "doi": "https://doi.org/10.1145/3092839",
    "publication_date": "2017-07-14",
    "publication_year": 2017,
    "authors": "Cong Wang; Divyashri Bhat; Amr Rizk; Michael Zink",
    "corresponding_authors": "",
    "abstract": "The dynamics of the application-layer-based control loop of dynamic adaptive streaming over HTTP (DASH) make video bitrate selection for DASH a difficult problem. In this work, we provide a DASH quality adaptation algorithm, named SQUAD, that is specifically tailored to provide a high quality of experience (QoE). We review and provide new insights into the challenges for DASH rate estimation. We found that in addition to the ON-OFF behavior of DASH clients, there exists a discrepancy in the timescales that form the basis of the rate estimates across (i) different video segments and (ii) the rate control loops of DASH and Transmission Control Protocol (TCP). With these observations in mind, we design SQUAD aiming to maximize the average quality bitrate while minimizing the quality variations. We test our implementation of SQUAD together with a number of different quality adaptation algorithms under various conditions in the Global Environment for Networking Innovation testbed, as well as, in a series of measurements over the public Internet. Through a measurement study, we show that by sacrificing little to nothing in average quality bitrate, SQUAD can provide significantlygt ; better QoE in terms of quality switching and magnitude. In addition, we show that retransmission of higher-quality segments that were originally received in low-quality is feasible and improves the QoE.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2726908435",
    "type": "article"
  },
  {
    "title": "Cloud-Assisted Crowdsourced Livecast",
    "doi": "https://doi.org/10.1145/3095755",
    "publication_date": "2017-07-14",
    "publication_year": 2017,
    "authors": "Cong Zhang; Jiangchuan Liu; Haiyang Wang",
    "corresponding_authors": "",
    "abstract": "The past two years have witnessed an explosion of a new generation of livecast services, represented by Twitch.tv , GamingLive , and Dailymotion , to name but a few. With such a livecast service, geo-distributed Internet users can broadcast any event in real-time, for example, game, cooking, drawing, and so on, to viewers of interest. Its crowdsourced nature enables rich interactions among broadcasters and viewers but also introduces great challenges to accommodate their great scales and dynamics. To fulfill the demands from a large number of heterogeneous broadcasters and geo-distributed viewers, expensive server clusters have been deployed to ingest and transcode live streams. Yet our Twitch-based measurement shows that a significant portion of the unpopular and dynamic broadcasters are consuming considerable system resources; in particular, 25% of bandwidth resources and 30% of computational capacity are used by the broadcasters who do not have any viewers at all. In this article, through the real-world measurement and data analysis, we show that the public cloud has great potentials to address these scalability challenges. We accordingly present the design of Cloud-assisted Crowdsourced Livecast (CACL) and propose a comprehensive set of solutions for broadcaster partitioning. Our trace-driven evaluations show that our CACL design can smartly assign ingesting and transcoding tasks to the elastic cloud virtual machines, providing flexible and cost-effective system deployment.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2735606267",
    "type": "article"
  },
  {
    "title": "Look at Me! Correcting Eye Gaze in Live Video Communication",
    "doi": "https://doi.org/10.1145/3311784",
    "publication_date": "2019-05-31",
    "publication_year": 2019,
    "authors": "Chih-Fan Hsu; Yu-Shuen Wang; Chin‐Laung Lei; Kuan-Ta Chen",
    "corresponding_authors": "",
    "abstract": "Although live video communication is widely used, it is generally less engaging than face-to-face communication because of limitations on social, emotional, and haptic feedback. Missing eye contact is one such problem caused by the physical deviation between the screen and camera on a device. Manipulating video frames to correct eye gaze is a solution to this problem. In this article, we introduce a system to rotate the eyeball of a local participant before the video frame is sent to the remote side. It adopts a warping-based convolutional neural network to relocate pixels in eye regions. To improve visual quality, we minimize the L2 distance between the ground truths and warped eyes. We also present several newly designed loss functions to help network training. These new loss functions are designed to preserve the shape of eye structures and minimize color changes around the periphery of eye regions. To evaluate the presented network and loss functions, we objectively and subjectively compared results generated by our system and the state-of-the-art, DeepWarp, in relation to two datasets. The experimental results demonstrated the effectiveness of our system. In addition, we showed that our system can perform eye-gaze correction in real time on a consumer-level laptop. Because of the quality and efficiency of the system, gaze correction by postprocessing through this system is a feasible solution to the problem of missing eye contact in video communication.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2950995827",
    "type": "article"
  },
  {
    "title": "Image Captioning by Asking Questions",
    "doi": "https://doi.org/10.1145/3313873",
    "publication_date": "2019-04-30",
    "publication_year": 2019,
    "authors": "Xiaoshan Yang; Changsheng Xu",
    "corresponding_authors": "",
    "abstract": "Image captioning and visual question answering are typical tasks that connect computer vision and natural language processing. Both of them need to effectively represent the visual content using computer vision methods and smoothly process the text sentence using natural language processing skills. The key problem of these two tasks is to infer the target result based on the interactive understanding of the word sequence and the image. Though they practically use similar algorithms, they are studied independently in the past few years. In this article, we attempt to exploit the mutual correlation between these two tasks. We propose the first VQA-improved image-captioning method that transfers the knowledge learned from the VQA corpora to the image-captioning task. A VQA model is first pretrained on image--question--answer instances. Then, the pretrained VQA model is used to extract VQA-grounded semantic representations according to selected free-form open-ended visual question--answer pairs. The VQA-grounded features are complementary to the visual features, because they interpret images from a different perspective. We incorporate the VQA model into the image-captioning model by adaptively fusing the VQA-grounded feature and the attended visual feature. We show that such simple VQA-improved image-captioning (VQA-IIC) models perform better than conventional image-captioning methods on large-scale public datasets.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2963355794",
    "type": "article"
  },
  {
    "title": "A Simplistic Global Median Filtering Forensics Based on Frequency Domain Analysis of Image Residuals",
    "doi": "https://doi.org/10.1145/3321508",
    "publication_date": "2019-08-20",
    "publication_year": 2019,
    "authors": "Abhinav Gupta; Divya Singhal",
    "corresponding_authors": "",
    "abstract": "Sophisticated image forgeries introduce digital image forensics as an active area of research. In this area, many researchers have addressed the problem of median filtering forensics. Existing median filtering detectors are adequate to classify median filtered images in uncompressed mode and in compressed mode at high-quality factors. Despite that, the field is lacking a robust method to detect median filtering in low-resolution images compressed with low-quality factors. In this article, a novel feature set (four feature dimensions), based on first-order statistics of frequency contents of median filtered residuals (MFRs) of original and median filtered images, has been proposed. The proposed feature set outperforms handcrafted features-based state-of-the-art detectors in terms of feature set dimensions and detection results obtained for low-resolution images at all quality factors. Also, results reveal the efficacy of proposed method over deep-learning-based median filtering detector. Comprehensive results expose the efficacy of the proposed detector to detect median filtering against other similar manipulations. Additionally, generalization ability test on cross-database images support the cross-validation results on four different databases. Thus, our proposed detector meets the current challenges in the field, to a great extent.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W3003260905",
    "type": "article"
  },
  {
    "title": "Enforcing Affinity Feature Learning through Self-attention for Person Re-identification",
    "doi": "https://doi.org/10.1145/3377352",
    "publication_date": "2020-02-29",
    "publication_year": 2020,
    "authors": "Jean-Paul Ainam; Ke Qin; Guisong Liu; Guangchun Luo; Brighter Agyemang",
    "corresponding_authors": "",
    "abstract": "Person re-identification is the task of recognizing an individual across heterogeneous non-overlapping camera views. It has become a crucial capability needed by many applications in public space video surveillance. However, it remains a challenging task due to the subtle inter-class similarity and large intra-class variation found in person images. Current CNN-based approaches have focused and investigated traditional identification or verification frameworks. Such approaches typically use the whole input image including the background and fail to pay attention to specific body parts, deviating the feature representation learning from informative parts. In this article, we introduce a self-attention mechanism coupled with cross-resolution to improve the feature representation learning of person re-identification task. The proposed self-attention module reinforces the most informative parts from a high-resolution image using its internal representation at the low-resolution. In particular, the model is fed with a pair of images on a different scale and consists of two branches. The upper branch processes the high-resolution image and learns high dimensional feature representation while the lower branch processes the low-resolution image and learns a filtering attention heatmap. The feature maps on the lower branch are subsequently weighted to reflect the importance of each patch of the input image using a softmax operation; whereas, on the upper branch, we apply a max pooling operation to downsample the high-resolution feature map before element-wise multiplied with the attention heatmap. Our attention module helps the network learn the most discriminative visual features of multiple regions of the image and is specifically optimized to attend and enforce feature representation at different scales. Extensive experiments on three large-scale datasets show that network architectures augmented with our self-attention module systematically improve their accuracy and outperform various state-of-the-art models by a large margin.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W3033047436",
    "type": "article"
  },
  {
    "title": "Smart Diagnosis",
    "doi": "https://doi.org/10.1145/3340240",
    "publication_date": "2020-04-30",
    "publication_year": 2020,
    "authors": "Yizhang Jiang; Xiaoqing Gu; Dingcheng Ji; Pengjiang Qian; Jing Xue; Yuanpeng Zhang; Jiaqi Zhu; Kaijian Xia; Shitong Wang",
    "corresponding_authors": "",
    "abstract": "To effectively identify electroencephalogram (EEG) signals in multiple-source domains, a multiple-source transfer learning-based Takagi–Sugeno–Kang (TSK) fuzzy system (FS), called MST-TSK, is proposed, which combines multiple-source transfer learning and manifold regularization (MR) learning mechanisms together into the TSK-FS framework. Specifically, the advantages of MST-TSK include the following: (1) by evaluating the significance of each source domain (SD), a flexible domain entropy-weighting index is presented; (2) using the theory of sample transfer learning, a reweighting strategy is presented to weigh the prediction of unknown samples in the target domain (TD) and the output of the source prediction functions; (3) by taking into account the MR term, the manifold structure of the TD is effectively maintained in the proposed system; and (4) by inheriting the interpretability of TSK-FS, MST-TSK displays good interpretability in identifying EEG signals that are understandable by humans (domain experts). The effectiveness of the proposed FS is demonstrated in several EEG multiple-source transfer learning tasks.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W3036317186",
    "type": "article"
  },
  {
    "title": "Area of Simulation",
    "doi": "https://doi.org/10.1145/2764463",
    "publication_date": "2015-08-24",
    "publication_year": 2015,
    "authors": "Siqi Shen; Shun-Yun Hu; Alexandru Iosup; Dick Epema",
    "corresponding_authors": "",
    "abstract": "Although Multi-Avatar Distributed Virtual Environments (MAVEs) such as Real-Time Strategy (RTS) games entertain daily hundreds of millions of online players, their current designs do not scale. For example, even popular RTS games such as the StarCraft series support in a single game instance only up to 16 players and only a few hundreds of avatars loosely controlled by these players, which is a consequence of the Event-Based Lockstep Simulation (EBLS) scalability mechanism they employ. Through empirical analysis, we show that a single Area of Interest (AoI), which is a scalability mechanism that is sufficient for single-avatar virtual environments (such as Role-Playing Games), also cannot meet the scalability demands of MAVEs. To enable scalable MAVEs, in this work we propose Area of Simulation (AoS), a new scalability mechanism, which combines and extends the mechanisms of AoI and EBLS. Unlike traditional AoI approaches, which employ only update-based operational models, our AoS mechanism uses both event-based and update-based operational models to manage not single, but multiple areas of interest. Unlike EBLS, which is traditionally used to synchronize the entire virtual world, our AoS mechanism synchronizes only selected areas of the virtual world. We further design an AoS-based architecture, which is able to use both our AoS and traditional AoI mechanisms simultaneously, dynamically trading-off consistency guarantees for scalability. We implement and deploy this architecture and we demonstrate that it can operate with an order of magnitude more avatars and a larger virtual world without exceeding the resource capacity of players' computers.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W1892109541",
    "type": "article"
  },
  {
    "title": "Enabling Context-Aware Indoor Augmented Reality via Smartphone Sensing and Vision Tracking",
    "doi": "https://doi.org/10.1145/2808208",
    "publication_date": "2015-10-21",
    "publication_year": 2015,
    "authors": "Kaikai Liu; Xiaolin Li",
    "corresponding_authors": "",
    "abstract": "Augmented reality (AR) aims to render the world that users see and overlay information that reflects the real physical dynamics. The digital view could be potentially projected near the Point-of-Interest (POI) in a way that makes the virtual view attached to the POI even when the camera moves. Achieving smooth support for movements is a subject of extensive studies. One of the key problems is where the augmented information should be added to the field of vision in real time. Existing solutions either leverage GPS location for rendering outdoor AR views (hundreds of kilometers away) or rely on image markers for small-scale presentation (only for the marker region). To realize AR applications under various scales and dynamics, we propose a suite of algorithms for fine-grained AR view tracking to improve the accuracy of attitude and displacement estimation, reduce the drift, eliminate the marker, and lower the computation cost. Instead of requiring extremely high, accurate, absolute locations, we propose multimodal solutions according to mobility levels without additional hardware requirement. Experimental results demonstrate significantly less error in projecting and tracking the AR view. These results are expected to make users excited to explore their surroundings with enriched content.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W1972743196",
    "type": "article"
  },
  {
    "title": "A Sketch-Based Approach for Interactive Organization of Video Clips",
    "doi": "https://doi.org/10.1145/2645643",
    "publication_date": "2014-08-01",
    "publication_year": 2014,
    "authors": "Yong‐Jin Liu; Cuixia Ma; Qiufang Fu; Xiaolan Fu; Shengfeng Qin; Lexing Xie",
    "corresponding_authors": "",
    "abstract": "With the rapid growth of video resources, techniques for efficient organization of video clips are becoming appealing in the multimedia domain. In this article, a sketch-based approach is proposed to intuitively organize video clips by: (1) enhancing their narrations using sketch annotations and (2) structurizing the organization process by gesture-based free-form sketching on touch devices. There are two main contributions of this work. The first is a sketch graph, a novel representation for the narrative structure of video clips to facilitate content organization. The second is a method to perform context-aware sketch recommendation scalable to large video collections, enabling common users to easily organize sketch annotations. A prototype system integrating the proposed approach was evaluated on the basis of five different aspects concerning its performance and usability. Two sketch searching experiments showed that the proposed context-aware sketch recommendation outperforms, in terms of accuracy and scalability, two state-of-the-art sketch searching methods. Moreover, a user study showed that the sketch graph is consistently preferred over traditional representations such as keywords and keyframes. The second user study showed that the proposed approach is applicable in those scenarios where the video annotator and organizer were the same person. The third user study showed that, for video content organization, using sketch graph users took on average 1/3 less time than using a mass-market tool Movie Maker and took on average 1/4 less time than using a state-of-the-art sketch alternative. These results demonstrated that the proposed sketch graph approach is a promising video organization tool.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W1997557743",
    "type": "article"
  },
  {
    "title": "Parallel Massive Clustering of Discrete Distributions",
    "doi": "https://doi.org/10.1145/2700293",
    "publication_date": "2015-06-02",
    "publication_year": 2015,
    "authors": "Yu Zhang; James Z. Wang; Jia Li",
    "corresponding_authors": "",
    "abstract": "The trend of analyzing big data in artificial intelligence demands highly-scalable machine learning algorithms, among which clustering is a fundamental and arguably the most widely applied method. To extend the applications of regular vector-based clustering algorithms, the Discrete Distribution (D2) clustering algorithm has been developed, aiming at clustering data represented by bags of weighted vectors which are well adopted data signatures in many emerging information retrieval and multimedia learning applications. However, the high computational complexity of D2-clustering limits its impact in solving massive learning problems. Here we present the parallel D2-clustering (PD2-clustering) algorithm with substantially improved scalability. We developed a hierarchical multipass algorithm structure for parallel computing in order to achieve a balance between the individual-node computation and the integration process of the algorithm. Experiments and extensive comparisons between PD2-clustering and other clustering algorithms are conducted on synthetic datasets. The results show that the proposed parallel algorithm achieves significant speed-up with minor accuracy loss. We apply PD2-clustering to image concept learning. In addition, by extending D2-clustering to symbolic data, we apply PD2-clustering to protein sequence clustering. For both applications, we demonstrate the high competitiveness of our new algorithm in comparison with other state-of-the-art methods.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2241871771",
    "type": "article"
  },
  {
    "title": "Forensic Analysis of Linear and Nonlinear Image Filtering Using Quantization Noise",
    "doi": "https://doi.org/10.1145/2857069",
    "publication_date": "2016-03-08",
    "publication_year": 2016,
    "authors": "Hareesh Ravi; A V Subramanyam; Sabu Emmanuel",
    "corresponding_authors": "",
    "abstract": "The availability of intelligent image editing techniques and antiforensic algorithms, make it convenient to manipulate an image and to hide the artifacts that it might have produced in the process. Real world forgeries are generally followed by the application of enhancement techniques such as filtering and/or conversion of the image format to suppress the forgery artifacts. Though several techniques evolved in the direction of detecting some of these manipulations, additional operations like recompression, nonlinear filtering, and other antiforensic methods during forgery are not deeply investigated. Toward this, we propose a robust method to detect whether a given image has undergone filtering (linear or nonlinear) based enhancement, possibly followed by format conversion after forgery. In the proposed method, JPEG quantization noise is obtained using natural image prior and quantization noise models. Transition probability features extracted from the quantization noise are used for machine learning based detection and classification. We test the effectiveness of the algorithm in classifying the class of the filter applied and the efficacy in detecting filtering in low resolution images. Experiments are performed to compare the performance of the proposed technique with state-of-the-art forensic filtering detection algorithms. It is found that the proposed technique is superior in most of the cases. Also, experiments against popular antiforensic algorithms show the counter antiforensic robustness of the proposed technique.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2305607063",
    "type": "article"
  },
  {
    "title": "Finding Social Points of Interest from Georeferenced and Oriented Online Photographs",
    "doi": "https://doi.org/10.1145/2854004",
    "publication_date": "2016-01-28",
    "publication_year": 2016,
    "authors": "Bart Thomée; Ioannis Arapakis; David A. Shamma",
    "corresponding_authors": "",
    "abstract": "Points of interest are an important requirement for location-based services, yet they are editorially curated and maintained, either professionally or through community. Beyond the laborious manual annotation task, further complications arise as points of interest may appear, relocate, or disappear over time, and may be relevant only to specific communities. To assist, complement, or even replace manual annotation, we propose a novel method for the automatic localization of points of interest depicted in photos taken by people across the world. Our technique exploits the geographic coordinates and the compass direction supplied by modern cameras, while accounting for possible measurement errors due to the variability in accuracy of the sensors that produced them. We statistically demonstrate that our method significantly outperforms techniques from the research literature on the task of estimating the geographic coordinates and geographic footprints of points of interest in various cities, even when photos are involved in the estimation process that do not show the point of interest at all.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2320453482",
    "type": "article"
  },
  {
    "title": "Editorials from the Past and Current Editors-in-Chief",
    "doi": "https://doi.org/10.1145/2903774",
    "publication_date": "2016-06-15",
    "publication_year": 2016,
    "authors": "Ralf Steinmetz",
    "corresponding_authors": "Ralf Steinmetz",
    "abstract": "editorial Free Access Share on Editorials from the Past and Current Editors-in-Chief Author: Ralf Steinmetz Technical University of Darmstadt Technical University of DarmstadtView Profile , Editor: Alberto del Bimbo Università di Firenze Università di FirenzeView Profile Authors Info & Claims ACM Transactions on Multimedia Computing, Communications, and ApplicationsVolume 12Issue 3June 2016 Article No.: 37epp 1–3https://doi.org/10.1145/2903774Published:15 June 2016Publication History 0citation185DownloadsMetricsTotal Citations0Total Downloads185Last 12 Months12Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2528814121",
    "type": "article"
  },
  {
    "title": "SPGAN: Face Forgery Using Spoofing Generative Adversarial Networks",
    "doi": "https://doi.org/10.1145/3432817",
    "publication_date": "2021-01-31",
    "publication_year": 2021,
    "authors": "Yidong Li; Wenhua Liu; Yi Jin; Yuanzhouhan Cao",
    "corresponding_authors": "",
    "abstract": "Current face spoof detection schemes mainly rely on physiological cues such as eye blinking, mouth movements, and micro-expression changes, or textural attributes of the face images [9]. But none of these methods represent a viable mechanism for makeup-induced spoofing, especially since makeup has been widely used. Compared with face alteration techniques such as plastic surgery, makeup is non-permanent and cost efficient, which makes makeup-induced spoofing become a realistic threat to the integrity of a face recognition system. To solve this problem, we propose a generative model to construct spoofing face images (confusing face images) for improving the accuracy and robustness of automatic face recognition. Our network structure is composed of two separate parts, with one using inter-attention mechanism to obtain interested face region, and another using intra-attention to translate imitation style with preserving imitation style-excluding details. These two attention mechanisms can precisely learn imitation style, where inter-attention pays more attention to imitation regions of image and intra-attention learns face attributes with long distance in image. To effectively discriminate generated images, we introduce an imitation style discriminator. Our model (SPGAN) generates face images that transfer the imitation style from target to subject image and preserve the imitation-excluding features. Experimental results demonstrate the performance of our model in improving quality of imitated face images.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W3140587737",
    "type": "article"
  },
  {
    "title": "Urban Perception: Sensing Cities via a Deep Interactive Multi-task Learning Framework",
    "doi": "https://doi.org/10.1145/3424115",
    "publication_date": "2021-01-31",
    "publication_year": 2021,
    "authors": "Weili Guan; Zhaozheng Chen; Fuli Feng; Weifeng Liu; Liqiang Nie",
    "corresponding_authors": "",
    "abstract": "Social scientists have shown evidence that visual perceptions of urban attributes, such as safe, wealthy, and beautiful perspectives of the given cities, are highly correlated to the residents’ behaviors and quality of life. Despite their significance, measuring visual perceptions of urban attributes is challenging due to the following facts: (1) Visual perceptions are subjectively contradistinctive rather than absolute. (2) Perception comparisons between image pairs are usually conducted region by region, and highly related to the specific urban attributes. And (3) the urban attributes have both the shared and specific information. To address these problems, in this article, we present a Deep inteRActive Multi-task leArning scheme, DRAMA for short. DRAMA comparatively quantifies the perceptions of urban attributes by jointly integrating the pairwise comparisons, regional interactions, and urban attribute correlations within a unified deep scheme. In DRAMA, each urban attribute is treated as a task, whereby the task-sharing and the task-specific information is fully explored. By conducting extensive experiments over a public large-scale benchmark dataset, it is demonstrated that our proposed DRAMA scheme outperforms several state-of-the-art baselines. Meanwhile, we applied the pairwise comparisons of our DRAMA model to further quantify the urban attributes and hence rank cities with respect to the given urban attributes. As a byproduct, we have released the codes and parameter settings to facilitate other researches.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W3151585875",
    "type": "article"
  },
  {
    "title": "Multi-task Learning-based All-in-one Collaboration Framework for Degraded Image Super-resolution",
    "doi": "https://doi.org/10.1145/3417333",
    "publication_date": "2021-02-28",
    "publication_year": 2021,
    "authors": "Xin Jin; Jianfeng Xu; Kazuyuki Tasaka; Zhibo Chen",
    "corresponding_authors": "",
    "abstract": "In this article, we address the degraded image super-resolution problem in a multi-task learning (MTL) manner. To better share representations between multiple tasks, we propose an all-in-one collaboration framework (ACF) with a learnable “junction” unit to handle two major problems that exist in MTL—“How to share” and “How much to share.” Specifically, ACF consists of a sharing phase and a reconstruction phase. Considering the intrinsic characteristic of multiple image degradations, we propose to first deal with the compression artifact, motion blur, and spatial structure information of the input image in parallel under a three-branch architecture in the sharing phase. Subsequently, in the reconstruction phase, we up-sample the previous features for high-resolution image reconstruction with a channel-wise and spatial attention mechanism. To coordinate two phases, we introduce a learnable “junction” unit with a dual-voting mechanism to selectively filter or preserve shared feature representations that come from sharing phase, learning an optimal combination for the following reconstruction phase. Finally, a curriculum learning-based training scheme is further proposed to improve the convergence of the whole framework. Extensive experimental results on synthetic and real-world low-resolution images show that the proposed all-in-one collaboration framework not only produces favorable high-resolution results while removing serious degradation, but also has high computational efficiency, outperforming state-of-the-art methods. We also have applied ACF to some image-quality sensitive practical task, such as pose estimation, to improve estimation accuracy of low-resolution images.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W3153045660",
    "type": "article"
  },
  {
    "title": "Hypomimia Recognition in Parkinson’s Disease With Semantic Features",
    "doi": "https://doi.org/10.1145/3476778",
    "publication_date": "2021-10-26",
    "publication_year": 2021,
    "authors": "Ge Su; Bo Lin; Wei Luo; Jianwei Yin; Shuiguang Deng; Honghao Gao; Renjun Xu",
    "corresponding_authors": "",
    "abstract": "Parkinson’s disease is the second most common neurodegenerative disorder, commonly affecting elderly people over the age of 65. As the cardinal manifestation, hypomimia, referred to as impairments in normal facial expressions, stays covert. Even some experienced doctors may miss these subtle changes, especially in a mild stage of this disease. The existing methods for hypomimia recognition are mainly dominated by statistical variable-based methods with the help of traditional machine learning algorithms. Despite the success of recognizing hypomimia, they show a limited accuracy and lack the capability of performing semantic analysis. Therefore, developing a computer-aided diagnostic method for semantically recognizing hypomimia is appealing. In this article, we propose a Semantic Feature based Hypomimia Recognition network , named SFHR-NET , to recognize hypomimia based on facial videos. First, a Semantic Feature Classifier (SF-C) is proposed to adaptively adjust feature maps salient to hypomimia, which leads the encoder and classifier to focus more on areas of hypomimia-interest. In SF-C, the progressive confidence strategy (PCS) ensures more reliable semantic features. Then, a two-stream framework is introduced to fuse the spatial data stream and temporal optical stream, which allows the encoder to semantically and progressively characterize the rigid process of hypomimia. Finally, to improve the interpretability of the model, Gradient-weighted Class Activation Mapping (Grad-CAM) is integrated to generate attention maps that cast our engineered features into hypomimia-interest regions. These highlighted regions provide visual explanations for decisions of our network. Experimental results based on real-world data demonstrate the effectiveness of our method in detecting hypomimia.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W3210112524",
    "type": "article"
  },
  {
    "title": "The Impact of Artificial Intelligence on the Creativity of Videos",
    "doi": "https://doi.org/10.1145/3462634",
    "publication_date": "2022-01-27",
    "publication_year": 2022,
    "authors": "Ana Daniela Peres Rebelo Verboom; Guedes De Oliveira Inês; D. E. Verboom Damion",
    "corresponding_authors": "",
    "abstract": "This study explored the impact Artificial Intelligence (AI) has on the evaluation of creative elements in artistic videos. The aim was to verify to what extent the use of an AI algorithm (Style Transfer) contributes to changes in the perceived creativity of the videos. Creativity was evaluated in six quantitative items (Likert-type scale) and one qualitative question (qualitative description of the creativity expressed in the video by two words or expressions). Six videos were shown to both control (N = 49) and experimental group (N = 52) aiming at determining possible differences in creativity assessment criteria. Furthermore, both groups contained experts (Experimental, N = 27; Control, N = 25) and non-experts (Experimental, N = 25; Control, N = 24). The first round of videos composed of six videos that were the same for both the experimental and control condition (used to check for bias). No significant differences were found. In a second round, six videos were shown with AI transformation (experimental condition) and without that transformation (control group). Results showed that in two cases the perceived creativity increased in experimental condition, in one case a decrease occurred. In most evaluations no differences were observed. Qualitative evaluations reinforce the absence of a general pattern of improvements in AI transformations. Altogether, the results emphasize the importance of human mediation in the application of AI in creative production: a hybrid approach, or rather, Hybrid Intelligence.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W3153027298",
    "type": "article"
  },
  {
    "title": "A Multi-feature and Time-aware-based Stress Evaluation Mechanism for Mental Status Adjustment",
    "doi": "https://doi.org/10.1145/3462763",
    "publication_date": "2022-01-25",
    "publication_year": 2022,
    "authors": "Min Chen; Wenjing Xiao; Miao Li; Yixue Hao; Long Hu; Guangming Tao",
    "corresponding_authors": "",
    "abstract": "With the rapid economic development, the prominent social competition has led to increasing psychological pressure of people felt from each aspect of life. Driven by the Internet of Things and artificial intelligence, intelligent psychological pressure detection systems based on deep learning and wearable devices have acquired some good results in practical application. However, existing studies argue that the psychological stress state is influenced by the current environment. They put much attention on the momentary features but ignore the dynamic change process of mental status in the time dimension. Besides, the lack of research in the general laws of psychological stress makes it difficult to quantitatively evaluate the stress status, resulting in the inability to perceive the stress state of users effectively. Thus, this article proposes an evaluation mechanism of psychological stress for adjusting the mental status of users. Specifically, we design a multi-dimensional feature space and a time-aware feature encoder, which integrate various stress features and capture time characteristics of stress state change. Moreover, a novel mental state model is proposed, which uses the pressure features with time characteristics to evaluate the pressure stress level. This model also quantifies the internal relationship between pressure features. Last, we establish a practicable testbed to demonstrate how to evaluate and adjust mental state of users by the proposed evaluation mechanism of psychological stress.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4206931784",
    "type": "article"
  },
  {
    "title": "Disentangle Saliency Detection into Cascaded Detail Modeling and Body Filling",
    "doi": "https://doi.org/10.1145/3513134",
    "publication_date": "2022-03-03",
    "publication_year": 2022,
    "authors": "Yue Song; Hao Tang; Nicu Sebe; Wei Wang",
    "corresponding_authors": "",
    "abstract": "Salient object detection has been long studied to identify the most visually attractive objects in images/videos. Recently, a growing amount of approaches have been proposed, all of which rely on the contour/edge information to improve detection performance. The edge labels are either put into the loss directly or used as extra supervision. The edge and body can also be learned separately and then fused afterward. Both methods either lead to high prediction errors near the edge or cannot be trained in an end-to-end manner. Another problem is that existing methods may fail to detect objects of various sizes due to the lack of efficient and effective feature fusion mechanisms. In this work, we propose to decompose the saliency detection task into two cascaded sub-tasks, i.e., detail modeling and body filling. Specifically, detail modeling focuses on capturing the object edges by supervision of explicitly decomposed detail label that consists of the pixels that are nested on the edge and near the edge. Then the body filling learns the body part that will be filled into the detail map to generate more accurate saliency map. To effectively fuse the features and handle objects at different scales, we have also proposed two novel multi-scale detail attention and body attention blocks for precise detail and body modeling. Experimental results show that our method achieves state-of-the-art performances on six public datasets.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4214882588",
    "type": "article"
  },
  {
    "title": "Towards Corruption-Agnostic Robust Domain Adaptation",
    "doi": "https://doi.org/10.1145/3501800",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Yifan Xu; Kekai Sheng; Weiming Dong; Baoyuan Wu; Changsheng Xu; Bao-Gang Hu",
    "corresponding_authors": "",
    "abstract": "Great progress has been achieved in domain adaptation in decades. Existing works are always based on an ideal assumption that testing target domains are independent and identically distributed with training target domains. However, due to unpredictable corruptions (e.g., noise and blur) in real data, such as web images and real-world object detection, domain adaptation methods are increasingly required to be corruption robust on target domains. We investigate a new task, corruption-agnostic robust domain adaptation (CRDA), to be accurate on original data and robust against unavailable-for-training corruptions on target domains. This task is non-trivial due to the large domain discrepancy and unsupervised target domains. We observe that simple combinations of popular methods of domain adaptation and corruption robustness have suboptimal CRDA results. We propose a new approach based on two technical insights into CRDA, as follows: (1) an easy-to-plug module called domain discrepancy generator (DDG) that generates samples that enlarge domain discrepancy to mimic unpredictable corruptions; (2) a simple but effective teacher-student scheme with contrastive loss to enhance the constraints on target domains. Experiments verify that DDG maintains or even improves its performance on original data and achieves better corruption robustness than baselines. Our code is available at: https://github.com/YifanXu74/CRDA .",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4214915138",
    "type": "article"
  },
  {
    "title": "Interactive Re-ranking via Object Entropy-Guided Question Answering for Cross-Modal Image Retrieval",
    "doi": "https://doi.org/10.1145/3485042",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Rintaro Yanagi; Ren Togo; Takahiro Ogawa; Miki Haseyama",
    "corresponding_authors": "",
    "abstract": "Cross-modal image-retrieval methods retrieve desired images from a query text by learning relationships between texts and images. Such a retrieval approach is one of the most effective ways of achieving the easiness of query preparation. Recent cross-modal image-retrieval methods are convenient and accurate when users input a query text that can be used to uniquely identify the desired image. However, in reality, users frequently input ambiguous query texts, and these ambiguous queries make it difficult to obtain desired images. To overcome these difficulties, in this study, we propose a novel interactive cross-modal image-retrieval method based on question answering. The proposed method analyzes candidate images and asks users questions to obtain information that can narrow down retrieval candidates. By only answering questions generated by the proposed method, users can reach their desired images, even when using an ambiguous query text. Experimental results show the proposed method’s effectiveness.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4214946871",
    "type": "article"
  },
  {
    "title": "Spatio-Temporal Context Based Adaptive Camcorder Recording Watermarking",
    "doi": "https://doi.org/10.1145/3503160",
    "publication_date": "2022-03-12",
    "publication_year": 2022,
    "authors": "Hui Chen; Shaohui Liu; Wuzhen Shi; Feng Jiang; Debin Zhao",
    "corresponding_authors": "",
    "abstract": "Video watermarking technology has attracted increasing attention in the past few years, and a great deal of traditional and deep learning-based methods have been proposed. However, these existing methods usually suffer from the following two challenges: First, most algorithms cannot resist camcorder recording attack, which limits their practical application. Second, watermark embedding may cause substantial degradation of video quality. Through analyzing the unique distortions presented in the camcorder recording process, including geometric distortion, temporal sampling distortion, sensor distortion and processing distortion, this paper proposes a novel spatio-temporal context based adaptive camcorder recording watermarking scheme STACR. In STACR, considering the geometric distortion and video visual quality, we embed the watermark by constructing a spatio-temporal histogram and incorporate a content features based adaptive locating algorithm to select embedding blocks and embedding strengths. As for the temporal sampling attack, we put forward a watermark correlation-based synchronization algorithm and combine it with cross-validation. Moreover, to resist the sensor distortion, we design a local matching-based algorithm to improve the extraction accuracy. In addition, grouped and repeated embedding strategies are combined to cope with the processing distortion. Experimental results compared with the state-of-the-art show that the proposed scheme achieves high video quality and is robust to geometric attacks, compression, scaling, transcoding, recoding, frame rate changes and especially for camcorder recording.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4220744054",
    "type": "article"
  },
  {
    "title": "Learning Streamed Attention Network from Descriptor Images for Cross-Resolution 3D Face Recognition",
    "doi": "https://doi.org/10.1145/3527158",
    "publication_date": "2022-03-25",
    "publication_year": 2022,
    "authors": "João Baptista Cardia Neto; Claudio Ferrari; Aparecido Nilceu Marana; Stefano Berretti; Alberto Del Bimbo",
    "corresponding_authors": "",
    "abstract": "In this article, we propose a hybrid framework for cross-resolution 3D face recognition which utilizes a Streamed Attention Network (SAN) that combines handcrafted features with Convolutional Neural Networks (CNNs). It consists of two main stages: first, we process the depth images to extract low-level surface descriptors and derive the corresponding Descriptor Images (DIs), represented as four-channel images. To build the DIs, we propose a variation of the 3D Local Binary Pattern (3DLBP) operator that encodes depth differences using a sigmoid function. Then, we design a CNN that learns from these DIs. The peculiarity of our solution consists in processing each channel of the input image separately, and fusing the contribution of each channel by means of both self- and cross-attention mechanisms. This strategy showed two main advantages over the direct application of Deep-CNN to depth images of the face; on the one hand, the DIs can reduce the diversity between high- and low-resolution data by encoding surface properties that are robust to resolution differences. On the other, it allows a better exploitation of the richer information provided by low-level features, resulting in improved recognition. We evaluated the proposed architecture in a challenging cross-dataset, cross-resolution scenario. To this aim, we first train the network on scanner-resolution 3D data. Next, we utilize the pre-trained network as feature extractor on low-resolution data, where the output of the last fully connected layer is used as face descriptor. Other than standard benchmarks, we also perform experiments on a newly collected dataset of paired high- and low-resolution 3D faces. We use the high-resolution data as gallery, while low-resolution faces are used as probe, allowing us to assess the real gap existing between these two types of data. Extensive experiments on low-resolution 3D face benchmarks show promising results with respect to state-of-the-art methods.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4220774951",
    "type": "article"
  },
  {
    "title": "Balanced and Accurate Pseudo-Labels for Semi-Supervised Image Classification",
    "doi": "https://doi.org/10.1145/3506711",
    "publication_date": "2022-05-02",
    "publication_year": 2022,
    "authors": "Jian Zhao; Xianhui Liu; Weidong Zhao",
    "corresponding_authors": "",
    "abstract": "Image classification by semi-supervised learning has recently become a hot spot, and the Co-Training framework is an important method of semi-supervised image classification. In the traditional Co-Training structure, the sub-networks will generate pseudo-labels for each other, and these pseudo-labels will further be used as a supervisory signal for model training. However, the pseudo-labels will hurt classification performance because of their low accuracy and unbalanced distribution. In this article, we are trying to solve the preceding two problems by designing the Balanced Module (BM) and Gaussian Mixture Module (GMM), and propose BAPS (the B alanced and A ccurate P seudo-labels for S emi-supervised image classification). In BM, the two sub-networks jointly predict the unlabeled images, then select the pseudo-labels with a high-confidence threshold to perform the balancing operation to obtain the initial samples with balanced distribution of each category. In GMM, referring to the common practice of the Learning from Noise Labels task, we use GMM to fit the loss distribution of images with pseudo-labels output by BM, then clean samples and noise samples are divided based on the observation that the loss of correctly labeled images is generally smaller than that of wrongly labeled ones. Through BM and GMM, pseudo-labels with balanced distribution and high accuracy are obtained for the subsequent model training process. Our model has achieved better classification accuracy than most state-of-the-art semi-supervised image classification algorithms on the CIFAR-10/100 and SVHN datasets, and further ablation experiments demonstrate the effectiveness of our BAPS. The source code of BAPS will be available at https://github.com/zhaojianaaa .",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4225274797",
    "type": "article"
  },
  {
    "title": "Sequential Hierarchical Learning with Distribution Transformation for Image Super-Resolution",
    "doi": "https://doi.org/10.1145/3532864",
    "publication_date": "2022-05-06",
    "publication_year": 2022,
    "authors": "Yuqing Liu; Xinfeng Zhang; Shanshe Wang; Siwei Ma; Wen Gao",
    "corresponding_authors": "",
    "abstract": "Multi-scale design has been considered in recent image super-resolution (SR) works to explore the hierarchical feature information. Existing multi-scale networks aim at building elaborate blocks or progressive architecture for restoration. In general, larger scale features concentrate more on structural and high-level information, while smaller scale features contain plentiful details and textured information. In this point of view, information from larger scale features can be derived from smaller ones. Based on the observation, in this article, we build a sequential hierarchical learning super-resolution network (SHSR) for effective image SR. Specially, we consider the inter-scale correlations of features, and devise a sequential multi-scale block (SMB) to progressively explore the hierarchical information. SMB is designed in a recursive way based on the linearity of convolution with restricted parameters. Besides the sequential hierarchical learning, we also investigate the correlations among the feature maps and devise a distribution transformation block (DTB). Different from attention-based methods, DTB regards the transformation in a normalization manner, and jointly considers the spatial and channel-wise correlations with scaling and bias factors. Experiment results show SHSR achieves superior quantitative performance and visual quality to state-of-the-art methods with near 34% parameters and 50% MACs off when scaling factor is × 4. To boost the performance without further training, the extension model SHSR + with self-ensemble achieves competitive performance than larger networks with near 92% parameters and 42% MACs off with scaling factor ×4.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4229049756",
    "type": "article"
  },
  {
    "title": "A Bayesian Quality-of-Experience Model for Adaptive Streaming Videos",
    "doi": "https://doi.org/10.1145/3491432",
    "publication_date": "2022-07-14",
    "publication_year": 2022,
    "authors": "Zhengfang Duanmu; Wentao Liu; Diqi Chen; Zhuoran Li; Zhou Wang; Yizhou Wang; Wen Gao",
    "corresponding_authors": "",
    "abstract": "The fundamental conflict between the enormous space of adaptive streaming videos and the limited capacity for subjective experiment casts significant challenges to objective Quality-of-Experience (QoE) prediction. Existing objective QoE models either employ pre-defined parametrization or exhibit complex functional form, achieving limited generalization capability in diverse streaming environments. In this study, we propose an objective QoE model, namely, the Bayesian streaming quality index (BSQI), to integrate prior knowledge on the human visual system and human annotated data in a principled way. By analyzing the subjective characteristics towards streaming videos from a corpus of subjective studies, we show that a family of QoE functions lies in a convex set. Using a variant of projected gradient descent, we optimize the objective QoE model over a database of training videos. The proposed BSQI demonstrates strong prediction accuracy in a broad range of streaming conditions, evident by state-of-the-art performance on four publicly available benchmark datasets and a novel analysis-by-synthesis visual experiment.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4285389610",
    "type": "article"
  },
  {
    "title": "Progressive Transformer Machine for Natural Character Reenactment",
    "doi": "https://doi.org/10.1145/3559107",
    "publication_date": "2022-08-25",
    "publication_year": 2022,
    "authors": "Yongzong Xu; Zhijing Yang; Tianshui Chen; Kai Li; Chunmei Qing",
    "corresponding_authors": "",
    "abstract": "Character reenactment aims to control a target person’s full-head movement by a driving monocular sequence that is made up of the driving character video. Current algorithms utilize convolution neural networks in generative adversarial networks, which extract historical and geometric information to iteratively generate video frames. However, convolution neural networks can merely capture local information with limited receptive fields and ignore global dependencies that play a crucial role in face synthesis, leading to generating unnatural video frames. In this work, we design a progressive transformer module that introduces multi-head self-attention with convolution refinement to simultaneously capture global-local dependencies. Specifically, we utilize the non-lapping window-based multi-head self-attention mechanism with hierarchical architecture to obtain the larger receptive fields at low-resolution feature map and thus extract global information. To better model local dependencies, we introduce the convolution operation to further refine the attentional weight in the multi-head self-attention mechanism. Finally, we use several stacked progressive transformer modules with the down-sampling operation to encode information of appearance information of previously generated frames and parameterized 3D face information of the current frame. Similarly, we use several stacked progressive transformer modules with the up-sampling operation to iteratively generate video frames. In this way, it can capture global-local information to facilitate generating video frames that are globally natural while preserving sharp outlines and rich detail information. Extensive experiments on several standard benchmarks suggest that the proposed method outperforms current leading algorithms.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4293090828",
    "type": "article"
  },
  {
    "title": "Beyond the Parts: Learning Coarse-to-Fine Adaptive Alignment Representation for Person Search",
    "doi": "https://doi.org/10.1145/3565886",
    "publication_date": "2022-10-08",
    "publication_year": 2022,
    "authors": "Wenxin Huang; Xuemei Jia; Xian Zhong; Xiao Wang; Kui Jiang; Zheng Wang",
    "corresponding_authors": "",
    "abstract": "Person search is a time-consuming computer vision task that entails locating and recognizing query people in scenic pictures. Body components are commonly mismatched during matching due to position variation, occlusions, and partially absent body parts, resulting in unsatisfactory person search results. Existing approaches for extracting local characteristics of the human body using keypoint information are unable to handle the search job when distinct body parts are misaligned, ignoring to exploit multiple granularities, which is crucial in the person search process. Moreover, the alignment learning methods learn body part features with fixed and equal weights, ignoring the beneficial contextual information, e.g., the umbrella carried by the pedestrian, which supplements compelling clues for identifying the person. In this paper, we propose a Coarse-to-Fine Adaptive Alignment Representation (CFA 2 R) network for learning multiple granular features in misaligned person search in the coarse-to-fine perspective. To exploit more beneficial body parts and related context of the cropped pedestrians, we design a Part-Attentional Progressive Module (PAPM) to guide the network to focus on informative body parts and positive accessorial regions. Besides, we propose a Re-weighting Alignment Module (RAM) shedding light on more contributive parts instead of treating them equally. Specifically, adaptive re-weighted but not fixed part features are reconstructed by Re-weighting Reconstruction module, considering that different parts serve unequally during image matching. Extensive experiments conducted on CUHK-SYSU and PRW datasets demonstrate competitive performance of our proposed method.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4303647578",
    "type": "article"
  },
  {
    "title": "Multimodal Cascaded Framework with Multimodal Latent Loss Functions Robust to Missing Modalities",
    "doi": "https://doi.org/10.1145/3711860",
    "publication_date": "2025-01-09",
    "publication_year": 2025,
    "authors": "Vijay John; Yasutomo Kawanishi",
    "corresponding_authors": "",
    "abstract": "Despite interest in multimodal classification, few studies have addressed the missing modality problem in which an incomplete multimodal input with one or more missing modalities is classified as the target class. The missing modality problem is shown to reduce the classification accuracy as the discriminative power of the obtained feature space is reduced. In this study, we address the missing modality problem in multimodal classification using a novel cascaded framework. The proposed framework is formulated in the feature space to address the missing modality problem by generating complete multimodal data from incomplete multimodal data. Subsequently, an optimal multimodal data is obtained by feature selection of the generated and original data. The proposed cascaded framework consists of three steps: feature extraction, feature generation, and classification. The framework is formulated to handle both complete and incomplete multimodal data simultaneously. The cascaded framework is trained using novel latent loss functions: missing modality joint loss, centroid joint loss, and latent prior loss. These loss functions, based on metric learning, are designed to ensure that data from the same class remain proximate in the latent space irrespective of the presence or absence of modality data. The cascaded framework is validated on bimodal audio-visible RAVDESS and trimodal audio-visible-thermal Speaking Faces datasets. The experimental results show that the cascaded framework improves classification accuracy even with incomplete multimodal data.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406219286",
    "type": "article"
  },
  {
    "title": "A Multimodal Hierarchical Attentional Ordering Network",
    "doi": "https://doi.org/10.1145/3711864",
    "publication_date": "2025-01-09",
    "publication_year": 2025,
    "authors": "Yiping Yang; Baiyun Cui; Yingming Li",
    "corresponding_authors": "",
    "abstract": "Sequence coherence modeling refers to how to coherently organize a given set of elements within a sequence, which is a fundamental aspect in comprehension, generation, and reasoning tasks. Although making great progress, existing deep neural network-based ordering approaches suffer from critical limitations: 1) Most of the previous coherence modeling approaches focus on the ordering of pure text sequences, while real-world data is often expressed in multimodal form; 2) The pairwise ranking method, widely used for ordering single/multimodal sequences, faces challenges in effectively modeling overall contextual relationships; 3) The redundancy problem would exist in multimodal ordering and can influence the encoding performance, which has been ignored when performing ordering. In this paper, we propose a novel Multimodal Hierarchical Attentional Ordering Network (referred to as MHAONet) for multimodal sequence ordering, which effectively captures the required global dependencies in multimodal sequences to enhance the coherence modeling. In particular, a multimodal encoder is introduced to encode the whole sequence for fully exploiting global contexts, where a multimodal selectively visible mask module is elaborately designed for controlled information transmission between different modalities. This enables the model to capture multimodal semantic and contextual information across the sequence. Further, the token-level and paragraph-level hierarchical attention layers are employed to encourage the model to concentrate on features of different granularities. In addition, a transformer-based pointer network is used for performing order prediction. Comprehensive experiments and analysis on benchmark datasets demonstrate the effectiveness and superiority of our proposed model.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406219363",
    "type": "article"
  },
  {
    "title": "Multi-Grained Contrastive Learning for Text-supervised Open-vocabulary Semantic Segmentation",
    "doi": "https://doi.org/10.1145/3711868",
    "publication_date": "2025-01-10",
    "publication_year": 2025,
    "authors": "Yajie Liu; Pu Ge; Guodong Wang; Qingjie Liu; Di Huang",
    "corresponding_authors": "",
    "abstract": "Learning open-vocabulary semantic segmentation (OVSS) from text supervision has recently received increasing attention for its promising potential in real-world applications. However, only with image-level supervision, it struggles to achieve dense and robust cross-modal alignment and thus limits pixel-level predictions. In this paper, we present a novel approach to this task with M ulti- G rained C ross-modal C ontrastive L earning, named MGCCL. Specifically, unlike current solutions restricted by coarse image/object-text alignment, MGCCL constructs pseudo multi-granular semantic correspondences at the object-, part-, and pixel-level and collaborates with hard sampling strategies to conduct cross-modal contrastive learning, significantly facilitating fine-grained alignment. Further, we develop an adaptive semantic unit which flexibly harnesses the learned multi-grained cross-modal alignment capabilities to effectively mitigate the under- and over-segmentation issues arising from the per-group and per-pixel units. Extensive experiments over a broad suite of 8 segmentation benchmarks show that our approach delivers significant advancements over state-of-the-art counterparts, demonstrating its effectiveness.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406233682",
    "type": "article"
  },
  {
    "title": "EVASR: Edge-Based Salience-Aware Super-Resolution for Enhanced Video Quality and Power Efficiency",
    "doi": "https://doi.org/10.1145/3711928",
    "publication_date": "2025-01-10",
    "publication_year": 2025,
    "authors": "Na Li; Zichen Zhu; Sheng Wei; Yao Liu",
    "corresponding_authors": "",
    "abstract": "With the rapid growth of video content consumption, it is important to deliver high-quality streaming videos to users even under limited available network bandwidth. In this article, we propose EVASR, a system that performs edge-based video delivery to clients with salience-aware super-resolution. We select patches with higher saliency score to perform super-resolution while applying the simple yet efficient bicubic interpolation for the remaining patches in the same video frame. To efficiently use the computation resources available at the edge server, we introduce a new metric called “saliency visual quality” (SVQ) and formulate patch selection as an optimization problem to achieve the best performance when an edge server is serving multiple users. We implement EVASR based on the FFmpeg framework and deploy it on three different platforms including desktop/laptop computers, mobile phones, and single board computers (SBCs). We conduct extensive experiments for evaluating the visual quality, super-resolution speed, and power savings that can be achieved by EVASR. Results show that EVASR outperforms baseline approaches in both resource efficiency and visual quality metrics including PSNR, SVQ, and VMAF. EVASR can also achieve substantial energy savings compared to baseline approaches MobileSR and JetsonSR on mobile devices.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406233683",
    "type": "article"
  },
  {
    "title": "Modelling Multi-modal Cross-interaction for Multi-label Few-shot Image Classification Based on Local Feature Selection",
    "doi": "https://doi.org/10.1145/3711867",
    "publication_date": "2025-01-10",
    "publication_year": 2025,
    "authors": "Kun Yan; Zied Bouraoui; Fangyun Wei; Chang Xu; Ping Wang; Shoaib Jameel; Steven Schockaert",
    "corresponding_authors": "",
    "abstract": "The aim of multi-label few-shot image classification (ML-FSIC) is to assign semantic labels to images, in settings where only a small number of training examples are available for each label. A key feature of the multi-label setting is that an image often has several labels, which typically refer to objects appearing in different regions of the image. When estimating label prototypes, in a metric-based setting, it is thus important to determine which regions are relevant for which labels, but the limited amount of training data and the noisy nature of local features make this highly challenging. As a solution, we propose a strategy in which label prototypes are gradually refined. First, we initialize the prototypes using word embeddings, which allows us to leverage prior knowledge about the meaning of the labels. Second, taking advantage of these initial prototypes, we then use a Loss Change Measurement (LCM) strategy to select the local features from the training images (i.e. the support set) that are most likely to be representative of a given label. Third, we construct the final prototype of the label by aggregating these representative local features using a multi-modal cross-interaction mechanism, which again relies on the initial word embedding-based prototypes. Experiments on COCO, PASCAL VOC, NUS-WIDE, and iMaterialist show that our model substantially improves the current state-of-the-art.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406233699",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on Deep Learning for Robust Human Body Language Understanding",
    "doi": "https://doi.org/10.1145/3712012",
    "publication_date": "2025-01-13",
    "publication_year": 2025,
    "authors": "Dan Guo; Troy McDaniel; Shuhui Wang; Meng Wang",
    "corresponding_authors": "",
    "abstract": "This editorial introduces the Special Issue on Deep Learning for Robust Human Body Language Understanding , hosted by the ACM Transactions on Multimedia Computing, Communications, and Applications in 2024. Human body language understanding has emerged as a critical research area, addressing challenges in analyzing, recognizing, and synthesizing multimodal human behavioral data such as gestures, poses, facial expressions . This Special Issue highlights recent advancements in deep learning techniques that enhance the robustness, scalability, and applicability of human body language understanding in diverse scenarios, including healthcare, education, and barrier-free human-computer interaction systems. The issue features a total of eight research articles focusing on key aspects of human body language understanding. These contributions are categorized into major research areas: gesture and sign language understanding , pose and action recognition , and facial expression and emotion analysis . Each article provides novel insights into challenges such as data scarcity, multimodal integration, adversarial robustness, and cross-modal generative modeling . We summarize the main contributions of the included works and emphasize their role in advancing the field of human body language understanding. Finally, we discuss ongoing challenges and future opportunities in this rapidly evolving domain, particularly in the context of integrating human-centric AI systems into real-world applications.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406325045",
    "type": "article"
  },
  {
    "title": "Generating High-Fidelity Clothed Human Dynamics with Temporal Diffusion",
    "doi": "https://doi.org/10.1145/3712011",
    "publication_date": "2025-01-13",
    "publication_year": 2025,
    "authors": "Shihao Zou; Yuanlu Xu; Nikolaos Sarafianos; Federica Bogo; Tony Tung; Weixin Si; Li Cheng",
    "corresponding_authors": "",
    "abstract": "Clothed human modeling plays a crucial role in multimedia research, with applications spanning virtual reality, gaming, and fashion design. The goal is to learn clothed human dynamics from observations and then generate humans with high-fidelity clothing details for motion animation. Despite tremendous advancements in clothing shape analysis by existing approaches, the community still faces challenges in generating convincing visual effects of cloth dynamics, maintaining temporally smooth clothing details, and handling diverse clothing patterns. To address these challenges, we introduce ClothDiffuse, a temporal diffusion model that seamlessly integrates three key components into this task: temporal dynamics modeling, iterative refinement, and diversified generation. Our approach begins by using an encoder to extract high-level temporal features from input human body motions. These features are combined with a learnable pixel-aligned garment feature, serving as prior conditions for the shape decoder. The decoder then iteratively denoise Gaussian noise to produce clothing deformations over time on the input unclothed human bodies. To ensure that the results align with observations and adhere to physical plausibility for clothing shape inference, we propose two physics-inspired loss functions that preserve the intra-frame distances and inter-frame forces of clothing points. Additionally, the stochastic nature of the denoising process allows for the generation of diverse and plausible clothing shapes. Experiments show that our approach outperforms state-of-the-art methods in chamfer distance and visual effects, particularly for loose clothing such as dresses and skirts. Furthermore, our approach effectively adapts to out-of-domain clothing types and generate realistic clothes dynamics.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406325331",
    "type": "article"
  },
  {
    "title": "Deformation Field Fusion for Medical Image Registration",
    "doi": "https://doi.org/10.1145/3707462",
    "publication_date": "2025-01-17",
    "publication_year": 2025,
    "authors": "Haifeng Zhao; Chi Zhang; Deyin Liu; Lin Wu",
    "corresponding_authors": "",
    "abstract": "Deformable medical image registration is to find a series of non-linear spatial transformations to align a pair of fixed and moving voxel images. Deep learning based registration models are effective in learning differences between such image pair to obtain the deformation field which is specialized in describing non-rigid deformations in the 3D voxel context. However, existing models tend to learn either one single deformation field only or multi-stage(multi-level) deformation fields progressively arriving at a final optimal field. Actually, deformation fields resulting from different architectures or losses are capable of capturing diverse types of deformations, complementing to each other. In this paper, we propose a novel framework of fusing different deformation fields to acquire an overall field to describe all-round deformations, in which multiple complementary cues regarding deformable 3D voxels can be strategically leveraged to improve the alignment of the given image pair. The key to the effect of deformation field fusion for registration lies in two aspects: the fusion network architecture and the loss function. Thus, we develop a well-designed fusion block using ingenious operations based on different types of pooling, convolution, and concatenation. Moreover, since calculating the deformation field using a conventional similarity loss cannot describe the contextual variations which are inter-dependent in each pair of fixed and moving images, we propose a novel Contrast-Structural loss to enhance the motion displacement between the image pair by calculating the similarity of pixels in density values, while being ranged in their spatial proximity. Extensive experimental results demonstrate that our proposed method achieves state-of-the-art performance on currently mainstream benchmark datasets.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406526480",
    "type": "article"
  },
  {
    "title": "RenderGAN: enhancing real-time rendering efficiency with Deep Learning",
    "doi": "https://doi.org/10.1145/3712263",
    "publication_date": "2025-01-17",
    "publication_year": 2025,
    "authors": "Marco Mameli; Marina Paolanti; Adriano Mancini; Primo Zingaretti; Roberto Pierdicca",
    "corresponding_authors": "",
    "abstract": "In the domain of computer graphics, achieving high visual quality in real-time rendering remains a formidable challenge due to the inherent time-quality trade-off. Conventional real-time rendering engines sacrifice visual fidelity for interactive performance, while image generation using path-tracing techniques can be exceedingly time-consuming. In this paper, we introduce RenderGAN, a deep learning-based solution designed to address this critical challenge in real-time rendering. RenderGAN uses G-Buffers and information from a real-time rendering engine as inputs to produce output images with exceptional visual fidelity. Its encoder-decoder architecture, trained using the Generative Adversarial Network (GAN) framework with perceptual loss, enhances image realism. To evaluate RenderGAN's effectiveness, we quantitatively compare the generated images with those of a path-tracing engine, obtaining a remarkable Universal Image Quality Index (UIQI) value of 0.898. RenderGAN's open-source nature fosters collaboration, driving advancements in real-time computer graphics and rendering techniques. By bridging the gap between real-time and path-tracing rendering, RenderGAN opens new horizons for accelerated image generation, inspiring innovation and unlocking the full potential of real-time visual experiences. Project page: https://github.com/marcomameli1992/RenderNet",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406526513",
    "type": "article"
  },
  {
    "title": "Contextual Interaction via Primitive-based Adversarial Training for Compositional Zero-shot Learning",
    "doi": "https://doi.org/10.1145/3712596",
    "publication_date": "2025-01-17",
    "publication_year": 2025,
    "authors": "Suyi Li; Chenyi Jiang; Shidong Wang; Yang Long; Zheng Zhang; Haofeng Zhang",
    "corresponding_authors": "",
    "abstract": "Compositional Zero-shot Learning (CZSL) aims to identify novel compositions via known attribute-object pairs. The primary challenge in CZSL tasks lies in the significant discrepancies introduced by the complex interaction between the visual primitives of attribute and object, consequently decreasing the classification performance towards novel compositions. Previous remarkable works primarily addressed this issue by focusing on disentangling strategy or utilizing object-based conditional probabilities to constrain the selection space of attributes. Unfortunately, few studies have explored the problem from the perspective of modeling the mechanism of visual primitive interactions. Inspired by the success of vanilla adversarial learning in Cross-Domain Few-Shot Learning, we take a step further and devise a model-agnostic and P rimitive- B ased Adv ersarial training (PBadv) method to deal with this problem. Besides, the latest studies highlight the weakness of the perception of hard compositions even under data-balanced conditions. To this end, we propose a novel over-sampling strategy with object-similarity guidance to augment target compositional training data. We performed detailed quantitative analysis and retrieval experiments on well-established datasets, such as UT-Zappos50K, MIT-States, and C-GQA, to validate the effectiveness of our proposed method, and the state-of-the-art (SOTA) performance demonstrates the superiority of our approach. The code is available at https://github.com/lisuyi/PBadv_czsl .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406526548",
    "type": "article"
  },
  {
    "title": "Mixed Attention and Channel Shift Transformer for Efficient Action Recognition",
    "doi": "https://doi.org/10.1145/3712594",
    "publication_date": "2025-01-17",
    "publication_year": 2025,
    "authors": "Xiusheng Lu; Yanbin Hao; Lechao Cheng; Sicheng Zhao; Yutao Liu; Mingli Song",
    "corresponding_authors": "",
    "abstract": "The practical use of the Transformer-based methods for processing videos is constrained by the high computing complexity. Although previous approaches adopt the spatiotemporal decomposition of 3D attention to mitigate the issue, they suffer from the drawback of neglecting the majority of visual tokens. This paper presents a novel mixed attention operation that subtly fuses the random, spatial, and temporal attention mechanisms. The proposed random attention stochastically samples video tokens in a simple yet effective way, complementing other attention methods. Furthermore, since the attention operation concentrates on learning long-distance relationships, we employ the channel shift operation to encode short-term temporal characteristics. Our model can provide more comprehensive motion representations thanks to the amalgamation of these techniques. Experimental results show that the proposed method produces competitive action recognition results with low computational overhead on both large-scale and small-scale public video datasets.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406526575",
    "type": "article"
  },
  {
    "title": "New Framework of Robust Image Encryption",
    "doi": "https://doi.org/10.1145/3712601",
    "publication_date": "2025-01-17",
    "publication_year": 2025,
    "authors": "Lin Huang; Chuan Qin; Guorui Feng; Xiangyang Luo; Xinpeng Zhang",
    "corresponding_authors": "",
    "abstract": "Designing an end-to-end encryption method for images using the nonlinear properties of deep neural networks (DNNs) has gradually attracted the attention of researchers. In this paper, we introduce a new framework for DNN-based image encryption that embeds a plaintext image as a secret message into a random noise to obtain a ciphertext image. Based on this, we propose an end-to-end robust image encryption method based on the invertible neural network (INN), which can realize secure encryption and resistance to common image processing attacks. Specifically, the INN is exploited as the shared-parameter encoder and decoder to achieve end-to-end encryption and decryption. The ciphertext image can be obtained through the forward process of the INN by inputting the plaintext image and the key, while the decrypted image can be obtained through the backward process of the INN by inputting the ciphertext image and the key. To enhance the security of our method, we design an information reinforcement module to guarantee the encryption effect and the sensitivity of the key. In addition, to improve the robustness of our method, an attack layer is employed for noise simulation training. Experimental results show that our method not only can realize secure encryption but also can achieve the robustness such as resisting JPEG compression, Gaussian noise, scaling, mean filtering, and Gaussian blurring effectively.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406549433",
    "type": "article"
  },
  {
    "title": "CoDE-GAN: Content Decoupled and Enhanced GAN for Sketch-guided Flexible Fashion Editing",
    "doi": "https://doi.org/10.1145/3712063",
    "publication_date": "2025-01-20",
    "publication_year": 2025,
    "authors": "Zhengwentai Sun; Yanghong Zhou; P.Y. Mok",
    "corresponding_authors": "",
    "abstract": "Rapid advancements in generative models, including generative adversarial networks (GANs) and diffusion models, have made possible of automated image editing through the use of text descriptions, semantic segmentation, and/or reference style images. Nevertheless, in terms of fashion image editing, it often requires more flexible, and typically iterative, modifications to the image content that existing methods struggle to achieve. This paper proposes a new model called Content Decoupled and Enhanced GAN (CoDE-GAN), which is formulated and trained for the task of image editing, drawing on methods from image reconstruction, more specifically, image inpainting with sketch-guidance. Through this proxy task, the trained model can be used for flexible image editing, generating new images with consistent colours and required textures based on sketch inputs. In this new model, a content decoupling block is introduced including specially designed dual encoders, which pre-process inputs and transform into separated structure and texture representations. Moreover, a content enhancing module is designed and applied to the decoder, improving the colour consistency and refining the texture of the generated images. The proposed CoDE-GAN can achieve coarse-to-fine results in one single stage. Extensive experiments on three datasets, covering human, garment-only and scene images, show that CoDE-GAN outperforms other state-of-the-art methods in terms of both generated image quality and editing flexibility. The code and dataset are available at: https://github.com/Taited/CoDE-GAN.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406615697",
    "type": "article"
  },
  {
    "title": "Maximizing Long-Term Task Completion Ratio of UAV-Enabled Wirelessly-Powered MEC Systems",
    "doi": "https://doi.org/10.1145/3712599",
    "publication_date": "2025-01-20",
    "publication_year": 2025,
    "authors": "Shaojun Zhu; Bincheng Zhu; Kaikai Chi; Jiefan Qiu; Hailong Shi; Xingyu Gao",
    "corresponding_authors": "",
    "abstract": "Unmanned aerial vehicles (UAV) enabled wirelessly-powered mobile edge computing (MEC) is emerging as a powerful technology for boosting computational capability and energy supplementation in Internet of Things (IoT). This work addresses the long-term task completion ratio maximization problem in UAV-enabled wirelessly-powered MEC systems. Besides the large number of optimization parameters, the environment can only be partially observed as the UAVs cannot cover the whole network area. Then it is very challenging to obtain good solutions due to the lack of global information. We introduce a novel distributed multi-agent deep reinforcement learning (MADRL) framework for optimizing UAVs’ actions and resource allocation, considering the constraints of tasks that vary in size, arrival times, and required computation completion time. To decouple the complicated parameters, we divide the problem into two manageable subproblems: UAVs’ action decision and resource allocation under a given UAV’s action. We employ a distributed DRL scheme for the former subproblem to cope with the partially observable nature. By revealing some important properties of the later subproblem, we design an efficient two-stage optimal algorithm to minimize the total consumed energy of nodes while maximizing the task-completing number. Extensive simulations validate the effectiveness of the proposed framework, achieving over a 50% improvement in task completion ratio compared to baseline schemes in some scenarios.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406616833",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on AI Empowered Edge Computing for Multimedia Applications",
    "doi": "https://doi.org/10.1145/3714421",
    "publication_date": "2025-01-23",
    "publication_year": 2025,
    "authors": "Moncef Gabbouj; Jin Li; Haibo Hu; Yang Xiang",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406763962",
    "type": "article"
  },
  {
    "title": "UVC: An Unified Deep Video Compression Framework",
    "doi": "https://doi.org/10.1145/3715144",
    "publication_date": "2025-01-24",
    "publication_year": 2025,
    "authors": "Lv Tang; Xinfeng Zhang; Li Zhang",
    "corresponding_authors": "",
    "abstract": "Recently, many works have applied deep learning techniques to video compression tasks, achieving promising results and advancing the field of deep learning video compression (DLVC). However, the architecture design of existing DLVC is rigid and limited in terms of flexibility. Specifically, different networks must be designed for different scenarios, such as delay-constrained or non-delay-constrained scenarios. Frequent switching between networks would reduce the speed of modern deep learning platforms and increase maintenance costs. To address this problem, we propose a unified deep video compression (UVC) framework that can be freely switched to different application scenarios without changing the network architecture. Our proposed UVC framework is based on the explicit-compression and implicit-generation perspective, which contains two sub-networks: the explicit reference frame compression network (ERFCN) and the implicit reference frame generation network (IRFGN). The aim of ERFCN is to compress the current frame with the help of the reference frame. To improve the performance of ERFCN, we first introduce the Transformer in this network, which can fully remove the spatial redundancy of the input image and is beneficial for the following inter-prediction process. We also develop a novel long-range motion estimation module for inter-prediction to generate motion vectors based on global motion information between two frames, which can handle long-range complex motion relations. The aim of IRFGN is to capture the temporal relationship between forward and backward reconstructed frames and synthesize a high-quality implicit reference frame for the current frame. To achieve this, we design the split spatial-temporal attention and multi-scale prediction module. We conduct extensive experiments on three widely used video compression databases (HEVC, UVG, and MCL-JVC), and the results demonstrate the superiority of our approach over other related DLVC methods.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406797316",
    "type": "article"
  },
  {
    "title": "MM-PCQA+: Advancing Multi-Modal Learning for Point Cloud Quality Assessment",
    "doi": "https://doi.org/10.1145/3715134",
    "publication_date": "2025-01-24",
    "publication_year": 2025,
    "authors": "Zicheng Zhang; Yingjie Zhou; Chunyi Li; Wei Sun; Xiongkuo Min; Xiaohong Liu; Guangtao Zhai",
    "corresponding_authors": "",
    "abstract": "The importance of visual quality in point clouds has been significantly underlined due to the rapid rise in 3D vision applications which aim to deliver affordable and superior user experiences. Reviewing the evolution of point cloud quality assessment (PCQA), it’s observed that visual quality evaluation typically employs single-modal data, either sourced from 2D projections or the 3D point clouds. The 2D projections possess abundant texture and semantic information while they are heavily reliant on viewpoints. In contrast, 3D point clouds are more reactive to geometric distortions and viewpoint-invariant. Consequently, to maximize the benefits of both point cloud and image modalities, we present an advanced no-reference Multi-Modal Point Cloud Quality Assessment (MM-PCQA+) metric. Specifically, we divide the point clouds into sub-models to reflect local geometric distortions such as point shifting and down-sampling. Afterwards, we render the point clouds using a cube-like projection setup and sample the projections of interest using a point-visible-ratio for image feature extraction. In order to fulfill these objectives, the sub-models and projected images are encoded using point-based and image-based neural networks. Lastly, we implement symmetric cross-modal attention to amalgamate multi-modal quality-aware features. Experimental results demonstrate that our metric surpasses all state-of-the-art methods and significantly advances beyond previous no-reference PCQA methods.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406797550",
    "type": "article"
  },
  {
    "title": "DISA: Disentangled Dual-Branch Framework for Affordance-Aware Human Insertion",
    "doi": "https://doi.org/10.1145/3715140",
    "publication_date": "2025-01-27",
    "publication_year": 2025,
    "authors": "Xiaojun Cao; Wengang Zhou; Qi Sun; Weilun Wang; Li Li; Houqiang Li",
    "corresponding_authors": "",
    "abstract": "Affordance-aware human insertion is a controllable human synthesis task aimed at seamlessly integrating a person into a scene while aligning human pose with contextual scene affordance and preserving human visual identity. Previous methods, typically reliant on a general framework of inpainting that injects all conditional information into a single branch, often struggle with the complexities of real-world contexts and the nuanced attributes of human figures. To this end, we present a novel DIS entangled dual-branch framework for A ffordance-aware human insertion task, termed as DISA, which focuses on both scene context comprehension and precise person attribute extraction. Specifically, our dual-branch design facilitates diffusion models to ensure disentangled and precise manipulations: one branch utilizes an additional network for deep scene context comprehension and control, while the other branch employs a parallel encoder to extract the feature of the reference person and injects this information through cross-attention mechanism. Furthermore, to comprehensively evaluate affordance-aware human insertion task, we introduce a new metric to assess the preservation of visual identity. We conduct a broad variety of evaluation experiments and validate the diversity and robustness of our method in different settings and downstream applications. Both qualitative and quantitative experimental analysis demonstrates that our approach outperforms previous methods in terms of image quality, pose accuracy and visual identity preservation.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406875166",
    "type": "article"
  },
  {
    "title": "<i>RA-MOSAIC</i> : Resource Adaptive Edge AI Optimization over Spatially Multiplexed Video Streams",
    "doi": "https://doi.org/10.1145/3715133",
    "publication_date": "2025-01-27",
    "publication_year": 2025,
    "authors": "Ila Gokarn; Yigong Hu; Tarek Abdelzaher; Archan Misra",
    "corresponding_authors": "",
    "abstract": "Sustaining real-time, high fidelity AI-based vision perception on edge devices is challenging due to both the high computational overhead of increasingly “deeper” Deep Neural Networks (DNNs) and the increasing resolution/quality of camera sensors. Such high-throughput vision perception is even more challenging in multi-tenancy systems, where video streams from multiple such high-quality cameras need to share the same GPU resource on a single edge device. Criticality-aware canvas-based processing is a promising paradigm that decomposes multiple concurrent video streams into Regions of Interest (RoI) and spatially channels the limited computational resources to selected RoI with higher “resolution”, thereby moderating the trade-off between computational load, task fidelity, and processing throughput. RA-MOSAIC (Resource Adaptive MOSAIC) employs such canvas-based processing, while further tuning the incoming video streams and available resources on-demand to allow the system to adapt to dynamic changes in workload (often arising from variations in the number or size of relevant objects observed by individual cameras). RA-MOSAIC utilizes two distinct and synergistic concepts. First, at the camera sensor, a bandwidth-adaptive and lightweight Bandwidth Aware Camera Transmission (BACT) method applies differential down-sampling to create mixed-resolution individual frames that preferentially preserve resolution for critical ROIs, before being transmitted to the edge node. Second, at the edge, BACT video streams received from multiple cameras are decomposed into multi-scale RoI tiles and spatially packed using a novel workload-adaptive bin-packing strategy into a single ‘canvas frame’. Notably, the canvas frame itself is dynamically sized such that the edge device can opportunistically provide higher processing throughput for selected high-priority tiles during periods of lower aggregate workloads. To demonstrate RA-MOSAIC’s gains in processing throughput and perception fidelity, we evaluate RA-MOSAIC on a single NVIDIA Jetson TX2 edge device for two benchmark tasks: Drone-based Pedestrian Detection and Automatic License Plate Recognition. In a bandwidth-constrained wireless environment, RA-MOSAIC employs a batch size of 1 to pack up to 6 concurrent video streams on a dynamically sized canvas frame to provide (i) 14.3% gain in object detection accuracy and (ii) 11.11% gain in throughput on average (up to 20 FPS per camera, cumulatively 120 FPS), over our previous work MOSAIC, a naïve canvas-based baseline. Compared to prior state of the art baselines such as batched inference over extracted RoI, RA-MOSAIC provides a very-significant, 29.6% gain in accuracy for a comparable throughput. Similarly, RA-MOSAIC dramatically outperforms bandwidth adaptive baselines, such as FCFS ( \\(\\leq 1\\%\\) accuracy gain but \\(5.6\\) x or 566.67% throughput gain) and uniform grid packing (17% accuracy improvement and 5% throughput gain).",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406875182",
    "type": "article"
  },
  {
    "title": "Spatio-Temporal Attention for Text-Video Retrieval",
    "doi": "https://doi.org/10.1145/3715137",
    "publication_date": "2025-01-28",
    "publication_year": 2025,
    "authors": "Leqi Shen; Sicheng Zhao; Yifeng Zhang; Pengzhang Liu; Yongjun Bao; Guiguang Ding",
    "corresponding_authors": "",
    "abstract": "Text-video retrieval, a fundamental task for associating textual descriptions with video content, has become increasingly important in the video domain. Most existing methods focus on the single-modality features only considering the knowledge within individual video or text modalities, often neglecting cross-modal interactions. However, a text description corresponds to a specific spatio-temporal content within a video, involving a certain segment of a frame sequence and distinct sub-regions within these frames. Therefore, we focus on the text-conditioned video features to bridge the modality gap. In this paper, we propose Spatio-Temporal Attention for video-text retrieval, termed STAttn, which utilizes textual information to focus on the spatio-temporal video content. Our final text-conditioned video features are generated from the text-related video frames and the text-related regions within these frames. First, we propose the Spatial Text-Attention Module (STAM) to learn the spatial information within video frames. STAM introduces the text-related salient patches to capture more fine-grained details. Second, we propose the Temporal Text-Attention Module (TTAM) to learn the temporal relationships between video frames. Temporal Triplet loss is proposed in TTAM to enhance the attention towards the text-related frames. Thus, the two modules learn the text-related spatio-temporal content from both intra-frame and inter-frame aspects. Extensive experiments on three benchmark datasets, MSRVTT, ActivityNet, and DiDeMo, demonstrate that our STAttn outperforms state-of-the-art methods.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406903228",
    "type": "article"
  },
  {
    "title": "SRF: SpectrumRecombineFormer for Hyperspectral Image Classification",
    "doi": "https://doi.org/10.1145/3715698",
    "publication_date": "2025-01-29",
    "publication_year": 2025,
    "authors": "Weipeng Jing; Peilun Kang; Donglin Di; Juntao Gu; Linhui Li; Mahmoud Emam; Linda Mohaisen; Xun Yang; Chao Li",
    "corresponding_authors": "",
    "abstract": "Hyperspectral (HS) imaging is a valuable technique for accurately classifying materials because of the abundance of spectral information and high resolution it provides. However, the characteristics of Hyperspectral images (HSI), such as high-dimensional features and information redundancy, pose significant challenges to data processing. Traditional dimensionality reduction methods often have information loss, high computational complexity, and easy to ignore the strong correlation between HSI spectral bands when dealing with HSI data. Although other methods can achieve satisfactory classification performance, they do not consider the dimensionality reduction of HSI, and they focus on the model performance, which limits further improvement in classification performance. This paper proposes a transformer-based framework called “SpectrumRecombineFormer” (SRF), which is composed of two key modules, namely “Spatial Spectral ReCombination” (SSRC) and “Cross-layer Fusion” (CF). The SSRC is capable of utilizing both adjacent and non-adjacent spectrums to generate the spatial-sequential perceptive representations, which alleviates the effect of the strong correlation between HSI spectral bands. The CF can avoid the loss of information during the feed-forward procedure among layers. Extensive experiments on five existing datasets (widely-adopted Indian Pines, Houston2013, Pavia University, Salinas and KSC) demonstrate the capability of our proposed method to address the above mentioned challenges. Both quantitative and qualitative experimental ablation studies, including visualization results, reveal that the proposed SRF method can successfully and efficiently classify hyperspectral images and surpass the other state-of-the-art methods. For access to the source code, please visit https://github.com/kangpeilun/SRF-HSI-Classification-master .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406956375",
    "type": "article"
  },
  {
    "title": "Style-FG: a style-based framework for film grain analysis and synthesis",
    "doi": "https://doi.org/10.1145/3712592",
    "publication_date": "2025-01-30",
    "publication_year": 2025,
    "authors": "Zoubida Ameur; Claire-Hélène Demarty; Olivier Le Meur; Daniel Ménard",
    "corresponding_authors": "",
    "abstract": "Film grain which used to be a by-product of the chemical processing in the analog film stock, is a desirable feature in the era of digital cameras. Besides participating to the artistic intent during content creation, film grain has also interesting properties in the video compression chain such as its ability to mask compression artifacts. In this paper, we use a deep learning-based framework for film grain analysis, generation and synthesis. Our framework Style-FG consists of three modules: a style encoder performing film grain style analysis, a mapping network responsible for film grain style generation, and a synthesis network that generates and blends a specific grain style to a given content in a content-adaptive manner. All modules are trained jointly, thanks to dedicated loss functions, on a new large and diverse dataset of pairs of grain-free and grainy images that we made publicly available to the community 1 . Quantitative and qualitative evaluations show that fidelity to the reference grain, diversity of grain styles as well as a perceptually pleasant grain synthesis are achieved, demonstrating that each module outperforms the state-of-the-art in the task it was designed for. To contribute further to the sustainability necessary effort of the digital information and communication field, a light-weight version of Style-FG is also proposed, which demonstrates similar quantitative and qualitative performances, while reducing the number of network parameters by a factor of 92%.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406987220",
    "type": "article"
  },
  {
    "title": "Texture and Structure-Guided Dual-Attention Mechanism for Image Inpainting",
    "doi": "https://doi.org/10.1145/3715962",
    "publication_date": "2025-01-31",
    "publication_year": 2025,
    "authors": "R. G. Li; Jiangyan Dai; Qibing Qin; Chengduan Wang; Huihui Zhang; Yugen Yi",
    "corresponding_authors": "",
    "abstract": "Deep learning exhibits powerful capability in image inpainting task, particularly in generating pixel-level details closely with the human visual perception. However, the complex background or larger missing regions make it still encounters the artifacts. Many researchers have investigated that prior information is crucial for guiding the image inpainting. In this paper, we introduce the dual-attention mechanism, including lightweight spatial attention and linearized attention, to construct an end-to-end texture structure-guided image inpainting method. In the first stage, we build the detail inpainting network with the lightweight spatial attention. In this model, the extracted texture and structural features are fused with multi-layers and then the fused detail image is considered as the prior to guide the detail repair of corrupted images. In the second stage, we construct the content completing network by the repaired detail and the linearized Transformer module. This module not only overcomes the limitation of the receptive field size of convolutional kernels that can improve the long-range modelling of features, but also can significantly reduce the computational complexity of the original Transformer. To demonstrate the superior effectiveness of the proposed method, we perform extensive experiments with advanced models on three datasets: CelebA-HQ, Places2, and Paris Street Views. Comparative results manifest that our method achieves excellent image inpainting results that are conform to the human visual system.The code is available at https://github.com/QinLab-WFU/TSGDAM",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407030046",
    "type": "article"
  },
  {
    "title": "Dual-Domain Triple Contrast for Cross-Dataset Skeleton-based Action Recognition",
    "doi": "https://doi.org/10.1145/3715917",
    "publication_date": "2025-01-31",
    "publication_year": 2025,
    "authors": "Kun Wang; Jiuxin Cao; Jiawei Ge; Chang Liu; Bo Liu",
    "corresponding_authors": "",
    "abstract": "Skeleton-based Action Recognition (SAR) is widely recognized for its robustness and efficiency in human action analysis, but its performance in cross-dataset tasks has been limited due to domain shifts between different datasets. To address this challenge, current methods typically approach cross-dataset SAR as an Unsupervised Domain Adaptation (UDA) task, which is tackled using domain adaptation or self-supervised learning strategies. In this paper, we propose a Dual-Domain Triple Contrast (D2TC) framework for cross-dataset SAR under the UDA setting. Unlike existing UDA methods that either focus on a single strategy or superficially combine strategies, our D2TC leverages contrastive learning to integrate both strategies into a unified framework. It performs three types of contrastive learning: Self-Supervised Contrastive Learning (SS-CL), Supervised Contrastive Learning (Sup-CL), and Unsupervised Domain Adaptation with Contrastive Learning (UDA-CL), across both source and target domains. The triple contrasts go beyond mere summation, effectively bridging the domain gap and enhancing the model’s representational capacity. Additionally, we introduce multi-modal ensemble contrast and extreme skeleton augmentation methods to further enhance the skeleton-based representation learning. Extensive experiments on six cross-dataset settings validate the superiority of our D2TC framework over state-of-the-art methods, demonstrating its effectiveness in reducing domain discrepancies and improving cross-dataset SAR performance. The codes are available on https://github.com/KennCoder7/DualDomainTripleContrast .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407031981",
    "type": "article"
  },
  {
    "title": "LayoutEnc: Leveraging Enhanced Layout Representations for Transformer-based Complex Scene Synthesis",
    "doi": "https://doi.org/10.1145/3716389",
    "publication_date": "2025-02-07",
    "publication_year": 2025,
    "authors": "Xiao Cui; Qi Sun; Min Wang; Li Li; Wengang Zhou; Houqiang Li",
    "corresponding_authors": "",
    "abstract": "In complex scene synthesis, the effective representation of layouts is paramount. This paper introduces LayoutEnc, an advanced approach specifically designed to enhance layout representation by improving interpretability, robustness, and expressiveness, thereby facilitating more efficient image transformation. Distinct from conventional approaches that homogenize layout and image data, LayoutEnc distinctively processes various data modalities, enhancing the fidelity and interpretability of the layout representation. We apply stochastic noise injection to image tokens to align training and inference conditions, thereby fortifying the robustness of the layout representation. Additionally, LayoutEnc employs a two-stage multi-scale guidance learning strategy, to meticulously extract and refine semantic and textural features from training images. This enriched layout representation is then adeptly integrated into a transformer-based image generation framework, facilitating controlled and nuanced scene synthesis. Experimental results on the COCO-stuff and Visual Genome datasets demonstrate that LayoutEnc outperforms prior works in metrics such as FID and Scene-FID scores. The code and demo are available on https://github.com/qsun1/LayoutEnc .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407252190",
    "type": "article"
  },
  {
    "title": "Multi-Grained Alignment with Knowledge Distillation for Partially Relevant Video Retrieval",
    "doi": "https://doi.org/10.1145/3716388",
    "publication_date": "2025-02-07",
    "publication_year": 2025,
    "authors": "Qun Zhang; Chao Yang; Bin Jiang; Bolin Zhang",
    "corresponding_authors": "",
    "abstract": "Partially Relevant Video Retrieval (PRVR) aims to accurately retrieve the most relevant video in response to a query from untrimmed videos. The analysis of video content can be done at three different granularities: frame-level, clip-level, and video-level. Previous methods have focused on one or two of these levels for alignment, limiting the exploration of the video semantics. Moreover, some methods use video-level alignment and apply a self-attention mechanism to generate video-level features, but this may not be ideal as the entire video may not be relevant to the query. We propose a M ulti- G rained A lignment framework with K nowledge D istillation (MGAKD), which purifies the cross-modal alignment knowledge from the Contrastive Language-Image Pre-training (CLIP) model and achieves multi-grained alignment. It extracts cross-modal alignment knowledge from CLIP and imparts this knowledge to the designed student model. For the student model, two branches are designed: an inheritance branch and an exploration branch. The inheritance branch absorbs the knowledge of cross-modal alignment from the CLIP. The exploration branch explores visual features at three granularities: frame-level, clip-level, and video-level. Specifically, we directly align the extracted frame features of the video with the query features to achieve frame-level alignment. In clip-level alignment, the use of Gaussian masks allows for the representation of the beginning, climax, and end of an event. By employing Gaussian masks, we are able to implicitly model clip-level features, resulting in clip features that contain a richer set of contextual information. To further enhance video-level feature exploration, we apply clip-guided attention to generate diverse video-level features based on different queries. This strategy effectively prevents irrelevant video moments from affecting the alignment of videos and queries. We conduct extensive experiments on two publicly available datasets, and the experimental results have surpassed those of the state-of-the-art method, showcasing the superior performance of the proposed method.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407252565",
    "type": "article"
  },
  {
    "title": "Multi-Anchor Offset Representation based Coarse-to-Fine Diffusion Model for Human Pose Estimation",
    "doi": "https://doi.org/10.1145/3716387",
    "publication_date": "2025-02-07",
    "publication_year": 2025,
    "authors": "Qianxing Li; Dehui Kong; Jinghua Li; Dongpan Chen; Baocai Yin",
    "corresponding_authors": "",
    "abstract": "3D human pose estimation (3DHPE) in images aims at estimating 3D joint positions from images. The existing 3DHPE methods usually define the loss function as the error measured by Euclidean distance between the locations of the predicted joints and the ground truth of joints, which confuses two different kinds of errors with obviously different characteristics and should not be processed equally: the error caused by different pose structures and the others. However, The existing human pose representations are not suitable to distinguish these two kinds of errors. In order to tackle this problem, we propose a novel Multi-Anchor Offset human Representation (MAOR) for human pose, which locates the position of each joint using its offsets from a group of selected high-precision joints named as Multi-Anchor. Making use of MAOR, the pose error related to the distortion of spatial structure can be measured independently from other errors, which is helpful to promote the accuracy of pose estimation. We then propose a novel MAOR based coarse-to-fine diffusion model (MAOR-DiffPose) for pose estimation, which optimizes different types of errors of poses step by step. Firstly, a MAOR-based Denoising Process (MDP) is devised to explicitly optimize spatial structures of 3D poses by using MAOR to describe poses and improves the inductive learning ability of MAOR-DiffPose by extracting view-independent features. Secondly, a Joint Coordinate based Denoising Process assisted by MAOR (JCDPaM) is devised to expand the input features meaningfully by combining MAOR with the pose representation based on joint coordinate and optimize the joint coordinates of 3D poses with the assistance of MAOR. MAOR-DiffPose realizes accurate 3DHPE by iterating MDP and JCDPaM modules. Comprehensive experimental results on widely used 3DHPE benchmarks Human3.6M and MPI-INF-3DHP show that the proposed method achieves the competitive performance compared with the state-of-the-art methods.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407253690",
    "type": "article"
  },
  {
    "title": "Improving Domain Generalization for Image Captioning with Unsupervised Prompt Learning",
    "doi": "https://doi.org/10.1145/3715136",
    "publication_date": "2025-02-11",
    "publication_year": 2025,
    "authors": "Hongchen Wei; Zhenzhong Chen",
    "corresponding_authors": "",
    "abstract": "Pretrained Visual Language Models (PVLMs) have demonstrated impressive zero-shot abilities in image captioning when accompanied by prompt prefixes. However, PVLMs using fixed prompt prefixes may suffer from mode collapse when dealing with multiple target domains. Some studies have attempted to mitigate this issue by constructing instruction datasets and injecting domain prior knowledge through hand-crafted prompts. However, this approach inevitably incurs high manual and time costs. In this paper, we propose an unsupervised prompt learning method to improve domain Generalization for Image Captioning (GeneIC), which learns a domain-specific prompt vector for the target domain without requiring annotated data. GeneIC aligns visual and language modalities with a pre-trained CLIP model, thus optimizing the domain-specific prompt vector from two aspects: attribute and semantic consistency. Specifically, GeneIC first generates attribute-transferred images with differing attributes while retaining semantic similarity with the original images. Then, GeneIC uses CLIP to measure the similarity between the images and the generated sentences. By exploring the variable and invariant features in the original and attribute-transferred images, attribute consistency constrains the attribute change direction of both images and sentences to learn domain-specific knowledge. The semantic consistency directly measures the similarity between the generated sentences and images to ensure the accuracy and comprehensiveness of the generated sentences. Consequently, GeneIC only optimizes the prompt vectors, which effectively retains the knowledge in the large model and introduces domain-specific knowledge. Experiments show that GeneIC exhibits superior generalization performance compared to state-of-the-art methods on multiple target domain datasets.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407373298",
    "type": "article"
  },
  {
    "title": "Attention-based Fusion for Stroke Lesion Segmentation on Computed Tomography Perfusion Data",
    "doi": "https://doi.org/10.1145/3716632",
    "publication_date": "2025-02-11",
    "publication_year": 2025,
    "authors": "Chintha Sri Pothu Raju; Rabul Hussain Laskar; Zulfiqar Ali; Ghulam Muhammad",
    "corresponding_authors": "",
    "abstract": "In recent times, stroke has emerged as a significant threat to humans, transforming affected brain tissue into core and penumbra regions. As the penumbra becomes irreversible over time, early core region segmentation is crucial. Automatic segmentation systems offer an efficient alternative to manual segmentation and aid radiologists in stroke lesion segmentation using Computed Tomography and Computed Tomography Perfusion (CTP) maps that comprise four parameter maps. This automatic segmentation is increasingly used in interactive, multimedia-based systems for diagnostic tools and AI-driven health applications. Top-performing models that follow the patch processing approach suffer from high inference times. To incorporate effective feature extraction at image-level inferences, which reduces the inference time, we present a hybrid fusion technique that combines early and bottleneck fusion, leveraging two separate encoders for effective feature extraction. Moreover, fusing the information from various fusion methods arbitrarily may not yield optimal results. Consequently, we have introduced two attention modules, i.e., cross-modal attention and cross-fusion attention modules, designed for the effective integration of features derived from diverse modalities and multiple fusion strategies, respectively. The findings highlight a considerable reduction in computational time alongside achieving a comparable dice score. Additionally, the incorporation of hybrid fusion and attention modules in the baseline notably increased the dice score from 0.482 to 0.521 in the validation dataset and achieved 0.48 in the test dataset of ISLES 2018. It also demonstrates competitive performance compared to existing models while maintaining efficient prediction times.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407374023",
    "type": "article"
  },
  {
    "title": "GANK: Dynamic Geometric and Appearance Features for Efficient and Robust Detection of Face Forgery",
    "doi": "https://doi.org/10.1145/3716827",
    "publication_date": "2025-02-11",
    "publication_year": 2025,
    "authors": "Zekun Sun; Na Ruan",
    "corresponding_authors": "",
    "abstract": "Deepfakes refers to various deep-learning-based techniques that manipulate the face in videos. Maliciously manufactured face forgeries could result in serious problems such as portrait infringement, information confusion, or even public panic. Previous countermeasures focused mainly on promoting detection accuracy while relatively overlooking robustness and computational overhead. In this work, we propose an efficient and robust framework named GANK , which discriminates Deepfake videos through temporal modeling on decoupled geometric and appearance features. A temporal denoising technique featuring landmark tracking and Kalman filtering is introduced to optimize the feature sequences, and multi-stream Recurrent Neural Networks (RNN) are constructed for sufficient exploitation of dynamic features. Besides, we introduce two optimizations to alleviate overfitting and enhance the utilization of temporal information, including channel-wise dropout and temporal random cropping. Our framework achieves outstanding robustness using very lightweight network backbones, reaching the state-of-the-art performance on multiple benchmarks.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407374040",
    "type": "article"
  },
  {
    "title": "LFIZW-GRHFMR: Robust Zero-Watermarking with GRHFMR for Light Field Image",
    "doi": "https://doi.org/10.1145/3717066",
    "publication_date": "2025-02-13",
    "publication_year": 2025,
    "authors": "Wenying Wen; Yu Ye; Ziye Yuan; Baolin Qiu; Dingli Hua",
    "corresponding_authors": "",
    "abstract": "Light field (LF) image data potentially involves a lot of sensitive information about users. Its transmission channel breaches could compromise user privacy and implicate illegal activities. Therefore, the confidentiality and integrity of LF image data transmission are crucial. However, most of the existing watermarking techniques may not adequately consider the confidentiality and integrity authentication of LF image data transmission. To address this problem, this paper employs Generic Radial Harmonic Fourier Moments in Radon space (GRHFMR) to propose a robust zero-watermarking scheme of the LF image, called LFIZW-GRHFMR. Specifically, the radial harmonic Fourier coefficients of sub-aperture images (SAIs) of LF image are calculated by taking into full consideration the characteristics of the GRHFMR variable weight basis function, and then the corresponding watermarks are combined to generate zero-watermarks with stronger robustness. For yielding more watermarking information and enhancing security authentication, the 32 key SAIs of LF image at a sampling rate of 0.5 are selected to act with GRHFMR. Meanwhile, the key SAIs are converted as video streaming with high efficiency video coding for transmission to improve the data transmission efficiency. Extensive experimental results demonstrate that the proposed LFIZW-GRHFMR outperforms the latest watermarking methods.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407506038",
    "type": "article"
  },
  {
    "title": "Image Cropping with Content and Composition Attribute-aware Global Relation Reasoning",
    "doi": "https://doi.org/10.1145/3719012",
    "publication_date": "2025-02-21",
    "publication_year": 2025,
    "authors": "Hancheng Zhu; Yan Li; Yong Zhou; Rui Yao; Zhiwen Shao; Jiaqi Zhao; Leida Li",
    "corresponding_authors": "",
    "abstract": "Image cropping aims to find visually pleasing content in an image, which will enhance its aesthetic quality. Existing image cropping approaches mainly emphasize the geometric properties of images, such as composition and layout, neglecting the rich aesthetic information available from the physical attributes (e.g., content and themes), and background information beyond the foreground in images. Consequently, this paper proposes an image cropping method based on the content and composition attribute-aware global relation reasoning, which aims at guiding the generation of cropped sub-images by exploring critical attributes based on content and composition as well as global object correlations that affect aesthetics in images. Particularly, to comprehensively introduce aesthetic information into image cropping, we capture feature representations reinforced by content and composition attributes simultaneously. The feature representations can strengthen the visual aesthetics of cropped sub-images. To make the cropped sub-images amply contain more global information, we introduce a global relation reasoning branch in the proposed cropping module, which can fully exploit the dependency relationship between the foreground and background in images. Extensive experiments on image cropping benchmarks demonstrate that our approach is superior to state-of-the-art image cropping methods.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407838548",
    "type": "article"
  },
  {
    "title": "Transductive Few-shot Learning via Joint Message Passing and Prototype-based Soft-Label Propagation",
    "doi": "https://doi.org/10.1145/3719204",
    "publication_date": "2025-02-24",
    "publication_year": 2025,
    "authors": "Jiahui Wang; Qin Xu; Bo Jiang; Bin Luo",
    "corresponding_authors": "",
    "abstract": "The transductive few-shot learning (FSL) mostly employs either prototype learning or label propagation methods to generalize to new classes by using the information of all query samples. However, existing methods have several main limitations. First, the prototype methods mainly focus on support samples which fail to fully exploit the relationships of query samples. Second, existing label propagation methods are generally not effective for the class-imbalanced problem. Third, existing works usually optimize the learnable parameters during inference which significantly reduces the efficiency of existing methods. To address these limitations, this paper proposes an efficient and robust method for transductive FSL problem, termed Prototype-based Soft-Label Propagation (PSLP), which combines the prototype learning and label propagation together for FSL problem. In our proposed method, first, the soft-label presentation for each query sample is estimated by leveraging prototypes. Then, the soft-label propagation is conducted on the learned query-support graph and the prototype representation is rectified. Both steps are conducted progressively for boosting the performance. Moreover, to learn effective prototypes for soft-label estimation and the desirable query-support graph for soft-label propagation, we design a new joint message passing scheme to learn the sample presentation and relational graph jointly. The PSLP method is parameter-free and can be implemented very efficiently. The experiments conducted on four popular benchmarks show that our method achieves competitive results on both balanced and imbalanced settings compared to the state-of-the-art methods. The code is released at https://github.com/mobulan/PSLP.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407902644",
    "type": "article"
  },
  {
    "title": "Counterfeiting Attacks on a RDH-EI scheme based on block-permutation and Co-XOR",
    "doi": "https://doi.org/10.1145/3719294",
    "publication_date": "2025-02-25",
    "publication_year": 2025,
    "authors": "Fan Chen; Lingfeng Qu; Hadi Amirpour; Christian Timmerer; Hongjie He",
    "corresponding_authors": "",
    "abstract": "Reversible data hiding in encrypted images (RDH-EI) has gained widespread attention due to its potential applications in secure cloud storage. However, the security challenges of RDH-EI in cloud storage scenarios remain largely unexplored. In this paper, we present a counterfeiting attack on RDH-EI schemes that utilize block-permutation and Co-XOR (BPCX) encryption. We demonstrate that ciphertext images generated by BPCX-based RDH-EI are easily tampered with to produce a counterfeit decrypted image with different contents imperceptible to the human eye. This vulnerability is mainly because the block permutation key information of BPCX is susceptible to known-plaintext attacks (KPAs). Taking ciphertext images in telemedicine scenarios as an example, we describe two potential counterfeiting attacks, namely fixed-area and optimal-area attacks. We show that the quality of forged decrypted images depends on the accuracy of the estimated block-permutation key under KPA conditions. To improve the invisibility of counterfeit decrypted images, we analyze the limitations of existing KPA methods against BPCX encryption for \\(2\\times 2\\) block sizes and propose a novel diagonal inversion rule specifically designed for image blocks. This rule further enhances the accuracy of the estimated block-permutation key. The experiments show that, compared to existing KPA methods, the accuracy of the estimated block-permutation key in the UCID dataset increases by an average of 11.5%. In the counterfeiting attack experiments on Camera's encrypted image, we successfully tampered with over 80% of the pixels in the target area under the fixed-region attack. Additionally, we achieved a tampering success rate exceeding 90% in the optimal-region attack.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407924473",
    "type": "article"
  },
  {
    "title": "FishFormer: Annulus Slicing-based Transformer for Fisheye Rectification",
    "doi": "https://doi.org/10.1145/3719348",
    "publication_date": "2025-02-26",
    "publication_year": 2025,
    "authors": "Shangrong Yang; Chunyu Lin; Kang Liao; Yao Zhao",
    "corresponding_authors": "",
    "abstract": "Numerous significant progress on fisheye image rectification has been achieved through CNN. Nevertheless, constrained by a fixed receptive field, the global distribution and the local symmetry of the distortion have not been fully exploited. To leverage these two characteristics, we introduce FishFormer that processes the fisheye image as a sequence to enhance global and local perception. We tune the Transformer according to the structural properties of fisheye images. First, the uneven distortion distribution in patches generated by the existing square slicing method hinders the understanding of the global structure. Therefore, we propose an annulus slicing method to maintain the consistency of the distortion in each patch, thus, the applicability of Transformer is expanded to perceive the distortion distribution efficiently. Second, the distortion of adjacent patches is progressive. Such explicit correlations in local regions need to be rapidly constructed and maintained, but Transformer has a weakness in local area perception. Hence, a novel layer attention mechanism is introduced to enhance the local perception and feature interaction. Our network simultaneously implements global perception and focused local perception. Extensive experiments demonstrate that our method provides superior performance compared with state-of-the-art methods.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407958138",
    "type": "article"
  },
  {
    "title": "MoDA: Mixture of Domain Adapters for Parameter-efficient Generalizable Person Re-Identification",
    "doi": "https://doi.org/10.1145/3712595",
    "publication_date": "2025-02-27",
    "publication_year": 2025,
    "authors": "Yang Wang; Yixing Zhang; Xudie Ren; Yuxin Deng",
    "corresponding_authors": "",
    "abstract": "The Domain Generalizable Re-identification (DG ReID) task has attracted significant attention in recent years, as a challenging task but closely aligned with practical applications. Mixture-of-experts (MoE) based methods has been studied for DG ReID to exploit the discrepancies and inherent correlations between diverse domains. However, most of DG ReID methods, especially MoE-based methods, have to full fine-tune a large amount of parameters, which is not always practical in real-world scenarios. Considering this problem, we propose a novel MoE-based DG ReID method, named mixture of domain adapters (MoDA), which utilizes many expert adapters and a global adapter to help MoE-based method scale to a much larger model but in a more parameter-efficient way. Furthermore, we conduct our approach with the large-scale vision-language pre-trained model CLIP, which exploits both visual and text encoders, to learn more robust representations based on multimodal information. Extensive experiments verify the effectiveness of our method and show that MoDA achieves competitive with state-of-the-art DG ReID methods with much fewer tunable parameters.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408022430",
    "type": "article"
  },
  {
    "title": "PAPooling: Graph-based Position Adaptive Aggregation of Local Geometry in Point Clouds",
    "doi": "https://doi.org/10.1145/3718742",
    "publication_date": "2025-02-27",
    "publication_year": 2025,
    "authors": "Jie Wang; Tingfa Xu; Liqiang Song; Lihe Ding; Hui Li; Peng Jiang; Yuqi Han; Jianan Li",
    "corresponding_authors": "",
    "abstract": "Fine-grained geometry, obtained through the assimilation of localized point features, is crucial in the realms of object recognition and scene comprehension within point cloud contexts. Traditional point cloud backbones predominantly utilize max pooling for the amalgamation of local features, a process that tends to overlook spatial interrelations among points, consequently leading to the potential loss of fine-grained geometric details. To overcome this limitation, we introduce an innovative operation termed Position Adaptive Pooling (PAPooling), which is designed to amalgamate local features while sensitively considering the spatial positions of points. This is achieved by employing a graph-based representation to explicitly model the spatial relationships of points. PAPooling involves two principal components: firstly, the local graph construction , which establishes a local graph for a set of points by linking a central point with its adjacent points, thereby transforming pairwise relative positions into channel-specific attention weights; secondly, the attentive feature aggregation , which adeptly takes into account the contribution of each node and simulates the inter-node relationships within the local graph, effectively extracting representations of local features through a Graph Convolution Network (GCN). PAPooling’s simplicity and efficacy make it a versatile addition to widely-used point-based backbones such as PointNet++ and DGCNN, offering a plug-and-play solution. Comprehensive experimental analysis demonstrates PAPooling’s enhanced capability in capturing local geometry, contributing significantly across a spectrum of applications including 3D shape classification, part segmentation, scene segmentation, and corruption defense, all with minimal computational increase. Code will be public at https://github.com/Roywangj/PAPooling/ .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408022682",
    "type": "article"
  },
  {
    "title": "Dynamic Point Cloud Denoising via Gradient Fields",
    "doi": "https://doi.org/10.1145/3721431",
    "publication_date": "2025-03-04",
    "publication_year": 2025,
    "authors": "Qianjiang Hu; Wei Hu",
    "corresponding_authors": "",
    "abstract": "3D dynamic point clouds provide a discrete representation of real-world objects or scenes in motion, which have been widely applied in immersive telepresence, autonomous driving, surveillance, etc . However, point clouds acquired from sensors are usually perturbed by noise, which affects downstream tasks such as surface reconstruction and analysis. Although many efforts have been made for static point cloud denoising, dynamic point cloud denoising remains under-explored. In this paper, we propose a novel gradient-field-based dynamic point cloud denoising method, exploiting the temporal correspondence via the estimation of gradient fields—a fundamental problem in dynamic point cloud processing and analysis. The gradient field is the gradient of the log-probability function of the noisy point cloud, based on which we perform gradient ascent so as to converge each point to the underlying clean surface. We estimate the gradient of each surface patch and exploit the temporal correspondence, where the temporally corresponding patches are searched leveraging on rigid motion in classical mechanics. In particular, we treat each patch as a rigid object, which moves in the gradient field of an adjacent frame via force until reaching a balanced state, i.e. , when the sum of gradients over the patch reaches 0. Since the gradient would be smaller when the point is closer to the underlying surface, the balanced patch would fit the underlying surface well, thus leading to the temporal correspondence. Finally, the position of each point in the patch is updated along the direction of the gradient averaged from corresponding patches in adjacent frames. Experimental results demonstrate that the proposed model outperforms state-of-the-art methods under both synthetic noise and simulated real-world noise.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408149566",
    "type": "article"
  },
  {
    "title": "Axial-shunted Spatial-temporal Conversation for Change Detection",
    "doi": "https://doi.org/10.1145/3721135",
    "publication_date": "2025-03-04",
    "publication_year": 2025,
    "authors": "Yuchao Feng; Mengjie Qin; Jiawei Jiang; J. Lai; Jianwei Zheng",
    "corresponding_authors": "",
    "abstract": "Benefitting from the maturing of intelligence techniques and advanced sensors, recent years have witnessed the full flourishing of change detection (CD) on multi-temporal remote sensing images. However, extraneous interference caused by normal tempor evolution and the extreme sparsity of spatial changes still plague the detection accuracy. To counteract this dilemma, a lightweight axial-shunted spatial-temporal conversation network (ASCNet) is proposed, which models the intrinsic representations in dually augmented images with a parallel treatment of convolutions and attentions. Specifically, for the features of weakly-augmented bi-temporal image pairs from siamese CNN, a roundtable attention-based and intra-scale axial-shunted interaction, with linear complexity, is presented. By splitting horizontally or vertically into multiple chunks and then performing axial-squeeze operation, axial-shunted scheme can achieve fine-grained attention while maintaining linear complexity. Moreover, roundtable attention pursues efficient bi-temporal modeling by incorporating both self-attention and cross-attention in a single attentional computation, while imposing change guiding and difference gating for focusing on changes. Simultaneously, a video transformer is introduced for the modeling of strongly-augmented sequences, followed by an inter-scale spatial-temporal alignment to recalibrate the feature responses. ASCNet demonstrates state-of-the-art performance on four publicly available CD datasets while maintaining superior computational efficiency. The source code is available at: https://github.com/fengyuchao97/ASCNet .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408149677",
    "type": "article"
  },
  {
    "title": "MLIC <sup>++</sup> : Linear Complexity Multi-Reference Entropy Modeling for Learned Image Compression",
    "doi": "https://doi.org/10.1145/3719011",
    "publication_date": "2025-03-05",
    "publication_year": 2025,
    "authors": "Wei Jiang; Jiayu Yang; Yongqi Zhai; Feng Gao; Ronggang Wang",
    "corresponding_authors": "",
    "abstract": "The latent representation in learned image compression encompasses channel-wise, local spatial, and global spatial correlations, which are essential for the entropy model to capture for conditional entropy minimization. Efficiently capturing these contexts within a single entropy model, especially in high-resolution image coding, presents a challenge due to the computational complexity of existing global context modules. To address this challenge, we propose the Linear Complexity Multi-Reference Entropy Model (MEM \\({}^{++}\\) ). Specifically, the latent representation is partitioned into multiple slices. For channel-wise contexts, previously compressed slices serve as the context for compressing a particular slice. For local contexts, we introduce a shifted-window-based checkerboard attention module. This module ensures linear complexity without sacrificing performance. For global contexts, we propose a linear complexity attention mechanism. It captures global correlations by decomposing the softmax operation, enabling the implicit computation of attention maps from previously decoded slices. Using MEM++ as the entropy model, we develop the image compression method MLIC \\({}^{++}\\) . Extensive experimental results demonstrate that MLIC \\({}^{++}\\) achieves state-of-the-art performance, reducing BD-rate by \\(13.39\\%\\) on the Kodak dataset compared to VTM-17.0 in Peak Signal-to-Noise Ratio (PSNR). Furthermore, MLIC \\({}^{++}\\) exhibits linear computational complexity and memory consumption with resolution, making it highly suitable for high-resolution image coding. Code and pre-trained models are available at https://github.com/JiangWeibeta/MLIC .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408219838",
    "type": "article"
  },
  {
    "title": "Fine-Grained Alignment Network for Zero-Shot Cross-Modal Retrieval",
    "doi": "https://doi.org/10.1145/3722223",
    "publication_date": "2025-03-10",
    "publication_year": 2025,
    "authors": "Shiping Ge; Zhiwei Jiang; Yafeng Yin; Cong Wang; Zifeng Cheng; Qing Gu",
    "corresponding_authors": "",
    "abstract": "Zero-Shot Cross-Modal Retrieval (ZS-CMR) aims to perform cross-modal retrieval on data of unseen classes, where a key challenge is how to address the modality-gap and domain-shift problems simultaneously. Existing methods tackle this challenge mainly by embracing a sample-label alignment paradigm, which aligns samples of different modalities but of the same class with the word embedding of their class label. However, these methods only focus on the class-level alignment and overlook the alignment of rich fine-grained semantic information in samples, incurring coarse understanding of sample matching and poor generalization on unseen classes. In this paper, we propose a novel Fine-Grained Alignment Network (FGAN), an end-to-end framework that learns representation with two fine-grained alignment strategies, yielding representation space that can be better generalized to unseen classes. Specifically, we extract two kinds of fine-grained representations, region embedding and label distribution, respectively from aspects of both feature and label. To optimize the region embedding, we propose a Fine-Grained Contrastive Learning (FGCL) strategy to simultaneously conduct class-level alignment and model the intra-class discrepancy. To optimize the label distribution, we propose a Fine-Grained Label Alignment (FGLA) strategy to align diverse fine-grained semantic information among samples, rather than merely label information. Finally, both region embedding and label distribution are utilized together to perform ZS-CMR at a finer granularity. Experimental results on three widely-used datasets demonstrate that our method outperforms the state-of-the-art methods by a large margin. Detailed ablation studies have also been carried out, which provably affirm the advantage of each component we propose. Our code will be available at https://github.com/ShipingGe/FGAN .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408287655",
    "type": "article"
  },
  {
    "title": "Convex Hull Prediction Methods for Bitrate Ladder Construction: Design, Evaluation, and Comparison",
    "doi": "https://doi.org/10.1145/3723006",
    "publication_date": "2025-03-12",
    "publication_year": 2025,
    "authors": "Ahmed Telili; Wassim Hamidouche; Hadi Amirpour; Sid Ahmed Fezza; Christian Timmerer; Luce Morin",
    "corresponding_authors": "",
    "abstract": "HTTP adaptive streaming (HAS) has emerged as a prevalent approach for over-the-top (OTT) video streaming services due to its ability to deliver a seamless user experience. A fundamental component of HAS is the bitrate ladder, which comprises a set of encoding parameters (e.g., bitrate-resolution pairs) used to encode the source video into multiple representations. This adaptive bitrate ladder enables the client’s video player to dynamically adjust the quality of the video stream in real-time based on fluctuations in network conditions, ensuring uninterrupted playback by selecting the most suitable representation for the available bandwidth. The most straightforward approach involves using a fixed bitrate ladder for all videos, consisting of pre-determined bitrate-resolution pairs known as one-size-fits-all . Conversely, the most reliable technique relies on intensively encoding all resolutions over a wide range of bitrates to build the convex hull , thereby optimizing the bitrate ladder by selecting the representations from the convex hull for each specific video. Several techniques have been proposed to predict content-based ladders without performing a costly, exhaustive search encoding. This article provides a comprehensive review of various convex hull prediction methods, including both conventional and learning-based approaches. Furthermore, we conduct a benchmark study of several handcrafted- and deep learning (DL)-based approaches for predicting content-optimized convex hulls across multiple codec settings. The considered methods are evaluated on our proposed large-scale dataset, which includes 300 UHD video shots encoded with software and hardware encoders using three state-of-the-art video standards, including AVC/H.264, HEVC/H.265, and VVC/H.266, at various bitrate points. Our analysis provides valuable insights and establishes baseline performance for future research in this field ( Dataset URL : https://nasext-vaader.insa-rennes.fr/ietr-vaader/datasets/br_ladder ).",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408361490",
    "type": "article"
  },
  {
    "title": "Towards Energy-efficient Audio-Visual Classification via Multimodal Interactive Spiking Neural Network",
    "doi": "https://doi.org/10.1145/3721981",
    "publication_date": "2025-03-12",
    "publication_year": 2025,
    "authors": "Xu Liu; Na Xia; Jinxing Zhou; Zheng-Xuan Li; Dan Guo",
    "corresponding_authors": "",
    "abstract": "The Audio-Visual Classification (AVC) task aims to determine video categories by integrating audio and visual signals. Traditional methods for AVC leverage artificial neural networks (ANNs) that operate on floating-point features, affording large parameter counts and consuming extensive energy. Recent research has shifted towards brain-inspired spiking neural networks (SNNs), which transmit audiovisual information through sparser 0/1 spike features allowing for better energy efficiency. However, a byproduct of such sparsity is the increased difficulty in effectively encoding and utilizing these spike features. Moreover, the spike firing characteristics based on neuron membrane potential cause asynchronous spike activations due to the heterogeneous distributions of different modalities in the AVC task, resulting in cross-modal asynchronization. This issue is often overlooked by prior SNN models, resulting in lower classification accuracy compared to traditional ANN models. To address these challenges, we present a new Multimodal Interaction Spiking Network (MISNet), the first to successfully balance both accuracy and efficiency for the AVC task. As the core of MISNet, we propose a Multimodal Leaky Integrate-and-Fire (MLIF) neuron, which coordinates and synchronizes the spike activations of audiovisual signals within a single neuron, distinguishing it from the prior paradigm of SNNs that relies on multiple separate processing neurons. As a result, our MISNet enables to generate audio and visual spiking features with effective cross-modal fusion. Additionally, we propose to add extra loss regularizations before fusing the obtained audio-visual features for final classification, thereby benefiting unimodal spiking learning for multimodal interaction. We evaluate our method on five audio-visual datasets, demonstrating advanced performance in both accuracy and energy consumption.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408361498",
    "type": "article"
  },
  {
    "title": "Interpretable Novel Target Discovery Through Open-Set Domain Adaptation",
    "doi": "https://doi.org/10.1145/3722557",
    "publication_date": "2025-03-17",
    "publication_year": 2025,
    "authors": "Taotao Jing; H. R. Xia; Hongfu Liu; Zhengming Ding",
    "corresponding_authors": "",
    "abstract": "Open-set domain adaptation (OSDA) considers a special domain adaptation problem in which the target domain contains novel categories that never appear in the well-labeled source domain. Unfortunately, prior efforts on OSDA simply detect and recognize all novel categories as one “unknown” group without further exploration. The demand for exploring these novel categories prompts us to consider the underlying multi-class structure and semantic description of those unknown categories in more detail. In this paper, we propose a novel interpretable framework to accurately identify the seen categories in the target domain and effectively recover the semantic knowledge of the unseen categories with attributes and visual interpretations, which is referred to as Semantic Recovery Open-Set Domain Adaptation (SR-OSDA). Specifically, the proposed framework includes an explicit attribute explainable module and an implicit semantic interpretable module, which provide insight into the process of domain adaptation and the discovery of new categories. Furthermore, structure-preserving partial alignment is developed as a method of recognizing and aligning the visible categories across domains with the aid of domain-invariant feature learning. The visual-structural semantic attributes propagation is designed to provide smooth transitions from seen categories to unseen categories via visual-semantic mapping. Three new cross-domain SR-OSDA benchmarks are constructed in order to evaluate the proposed framework in novel and practical challenges. Experimental results and empirical analysis of our proposed solution to open-set recognition and semantic recovery demonstrate its superiority over other state-of-the-art solutions. Our source code is available at https://github.com/scottjingtt/XSROSDA .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408503356",
    "type": "article"
  },
  {
    "title": "Video Frame Interpolation via Fast Bidirectional 3D Correlation Volume",
    "doi": "https://doi.org/10.1145/3724123",
    "publication_date": "2025-03-18",
    "publication_year": 2025,
    "authors": "Dengyong Zhang; Runqi Lou; Jiaxin Chen; Xiangling Ding; Xin Liao; Gaobo Yang",
    "corresponding_authors": "",
    "abstract": "Recently, there has been a growing demand for flow-based video frame interpolation methods, which introduce correlation volumes to supervise the correlation of bidirectional optical flows. However, they often overlook the symmetry of the bidirectional motion field by consuming substantial computational cost, which is reflected in the fact that these methods often require a long runtime. To address these issues, in this paper, we propose a bidirectional 3D correlation volume which is suitable for video frame interpolation. By decomposing the 4D correlation volume into two 3D correlation volumes in the horizontal and vertical directions,we significantly enhance the model's inference speed with a minor sacrifice compared to our baseline. Additionally, when handling 2K video frames, our method achieves several-fold improvement in inference speed compared to other methods which implied correlation volume. The code is available at https://github.com/famt0531",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408564221",
    "type": "article"
  },
  {
    "title": "Fine-grained Semantic Disentanglement Network for Multimodal Sarcasm Analysis",
    "doi": "https://doi.org/10.1145/3722558",
    "publication_date": "2025-03-18",
    "publication_year": 2025,
    "authors": "Jiajia Tang; Binbin Ni; Feiwei Zhou; Dongjun Liu; Yu Ding; Yong Peng; Andrzej Cichocki; Qibin Zhao; Wanzeng Kong",
    "corresponding_authors": "",
    "abstract": "Multimodal sarcasm analysis is one of the most challenging research branch of the sentiment analysis area, due to the presence of cross-modality incongruity. However, existing works mainly attend to the coarse-grained incongruity analysis, and totally ignore the sentiment semantic coupling issue. This indeed limits the discriminate capability and robustness of the sarcasm analysis model. In order to address the above issue, we propose a novel fine-grained semantic disentanglement network (FSDN). Specifically, the intra-modality semantic disentanglement is performed to investigate the more intrinsic semantic cues of the same modality. Additionally, the inter-modality semantic disentanglement is leveraged to simultaneously facilitate the common and intrinsic semantic cues across modalities. Furthermore, the dual-spatial semantic interaction block is presented to explore the long-range cross-spatial semantic context between the obtained verbal and non-verbal semantic space with the global view. The above semantic disentanglement processes with both local and global views significantly unleash much more robustness even for the sarcasm case consists of multiple semantic message. Various experiments indicate that FSDN can receive state-of-the-art or competitive performance.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408564232",
    "type": "article"
  },
  {
    "title": "Cross-Domain Semantic Transfer for Domain Generalization",
    "doi": "https://doi.org/10.1145/3724398",
    "publication_date": "2025-03-19",
    "publication_year": 2025,
    "authors": "Yan Wang; Hong Xie; Jianzhi He; Xiaoyu Shi; Mingsheng Shang",
    "corresponding_authors": "",
    "abstract": "Data augmentation is a kind of mainstream domain generalization method aimed at enhancing the model's ability to learn from out-of-distribution data. Most existing data augmentation methods fail to simultaneously preserve the semantic consistency and ensure the domain diversity in the augmented samples, which hinders further improvements in the generalization capacity of the model. To cope with this issue, we propose a novel Cross-Domain Semantic Transfer-based data augmentation method (CDST), which improves generalizability from a novel perspective of exploring the diversity of semantic directions. Specifically, to ensure semantic consistency, an adjacent domain center interpolation module is proposed to find new generalization centers far from the classification boundary. To ensure data diversity, a semantic sample reproduction module is proposed to synthesize new semantic directions by transferring the cross-domain semantic information, and reproduce new samples along new semantic directions around new feature centers. Furthermore, a category-preserving regularization is introduced to further constrain the category invariance of the augmented samples. Extensive experiments are implemented to verify the superiority, effectiveness and transferability of CDST on the Digit-DG, PACS, VLCS, and OfficeHome datasets.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408589729",
    "type": "article"
  },
  {
    "title": "Dual-Modality-Shared Learning and Label Refinement for Unsupervised Visible-Infrared Person ReID",
    "doi": "https://doi.org/10.1145/3724397",
    "publication_date": "2025-03-19",
    "publication_year": 2025,
    "authors": "Licun Dai; Zhiming Luo; Yongguo Ling; Jiaxing Chai; Shaozi Li",
    "corresponding_authors": "",
    "abstract": "Unsupervised visible-infrared person re-identification (USVI-ReID) aims to match a person across two modalities without annotations. Current research primarily addresses the modality gap by establishing cross-modality correspondences through matching algorithms and utilizing memory banks for contrastive learning. However, the inherent noise in pseudo labels and neglect of hard samples often limit the efficacy of cross-modality learning. In this paper, we propose a Dual-modality-shared Learning and Label Refinement (DLLR) algorithm for USVI-ReID. First, we leverage a Cluster Similarity Matching (CSM) module and a Cluster Relationship-based Label Refinement (CRLR) algorithm to create and refine pseudo labels. Then, we adopt a Weighted Modality-shared Memory (WMM) to construct memory banks by jointly considering sample distribution and feature differences, thereby enhancing the effectiveness of cross-modality learning. Extensive experiments on three publicly available datasets validate the effectiveness of our proposed method, which outperforms state-of-the-art methods. Code is available at https://github.com/CharRic/DLLR .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408614652",
    "type": "article"
  },
  {
    "title": "Prompt based Invertible Mapping Alignment for Unsupervised Domain Adaptation",
    "doi": "https://doi.org/10.1145/3725735",
    "publication_date": "2025-03-21",
    "publication_year": 2025,
    "authors": "Chao Wen; Wei Chen; Yuhua Qian; Xiaodan Song; Xuemei Xie",
    "corresponding_authors": "",
    "abstract": "Large pre-trained vision-language models (VLMs) like CLIP have shown great potential for solving the unsupervised domain adaptation (UDA) problem. Existing prompt learning for UDA based on the unsupervised-trained VLMs requires distribution alignment between source and target domains in the common space for both vision and language branches. However, it is difficult for rough cross-domain alignment to maintain the discriminative semantic structure of both domains. Besides, the coarse features with non-informative noises due to ignoring the pseudo-label noises may cause failures to concentrate on precise semantics alignment. In this work, we propose a Prompt based Invertible Mapping Alignment (PIMA) method to incorporate discriminative domain knowledge into prompt learning, which is featured with refined cross-domain alignment in two separate space with well-kept structure. Specifically, we design an invertible neural network based homeomorphism mapping, and then achieve distribution alignment through such invertible mapping for connecting source and target visual feature space, which can preserve the data semantic structure. For better semantic alignment in vision-language space, we develop cross-modal implicit contrastive learning module to regularize non-informative features, which aims to find the low-rankness of implicit representation space. We conducted extensive experiments on three benchmark datasets to prove the advantages of our proposed PIMA over state-of-the-art methods.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408722897",
    "type": "article"
  },
  {
    "title": "RaT2IGen: Relation-aware Text-to-image Generation via Learnable Prompt",
    "doi": "https://doi.org/10.1145/3726527",
    "publication_date": "2025-03-28",
    "publication_year": 2025,
    "authors": "Zhaoda Ye; Xiangteng He; Yuxin Peng",
    "corresponding_authors": "",
    "abstract": "Text-to-image generation is to generate photo-realistic images according to the given text descriptions by users. Current methods have achieved promising performance. However, those methods still fail to generate the correct relation of the objects in the text descriptions, which cannot correctly reflect the users’ intention and hinders the application of text-to-image generation. This is due to two aspects: Firstly, they focus on the attributes and the concepts of the object, and have difficulty in transferring/mapping object relation knowledge into tasks. Secondly, they lack an effective mechanism to apply the semantic information from the text to guide spatial layout of the object in the generation stage. Thus, this paper proposes RaT2IGen (Relation-aware Text-to-image Generation), which aims to improve the semantic consistency of the generation model of object relation with the proper spatial layout. The main contribution can be summarized as follows: 1) The learnable relation prompt is proposed to capture the semantic information of the relation of the objects, which aims to strengthen the generation model to understand the object relations. 2) A bidirectional condition generation mechanism is proposed to generate the condition vectors of the image patches in both horizontal and vertical flattening, which helps the model effectively control the spatial layout of the objects according to the text description. 3) This paper proposes a new metric to evaluate the consistency of the spatial layout of the generated images with the real images. The experiments conducted on MS-COCO and LN-COCO datasets demonstrate the effectiveness of our approach in achieving the best performance.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408930059",
    "type": "article"
  },
  {
    "title": "Scalable Residual Laplacian Network for HEVC-compressed Video Restoration",
    "doi": "https://doi.org/10.1145/3727147",
    "publication_date": "2025-03-28",
    "publication_year": 2025,
    "authors": "Claudio Rota; Marco Buzzelli; Simone Bianco; Raimondo Schettini",
    "corresponding_authors": "",
    "abstract": "We present a novel Convolutional Neural Network that exploits the Laplacian decomposition technique, which is typically used in traditional image processing, to restore videos compressed with the High-Efficiency Video Coding (HEVC) algorithm. The proposed method decomposes the compressed frames into multi-scale frequency bands using the Laplacian decomposition, it restores each band using the ad-hoc designed Multi-frame Residual Laplacian Network (MRLN), and finally recomposes the restored bands to obtain the restored frames. By leveraging the multi-scale frequency representation of compressed frames provided by the Laplacian decomposition, MRLN can effectively reduce the compression artifacts and restore the image details with a reduced computational cost. In addition, our method can be easily instantiated in various versions to control the trade-off between efficiency and effectiveness, representing a versatile solution for scenarios with constrained computational resources. Experimental results on the MFQEv2 benchmark dataset show that our method achieves state-of-the-art performance in HEVC-compressed video restoration with a lower model complexity and shorter runtime with respect to existing methods. The project page is available at https://github.com/claudiom4sir/LaplacianVCAR .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408930093",
    "type": "article"
  },
  {
    "title": "The Interpretable and Transferable Adversarial Attack Against Synthetic Speech Detectors",
    "doi": "https://doi.org/10.1145/3727341",
    "publication_date": "2025-04-01",
    "publication_year": 2025,
    "authors": "Jiacheng Deng; Dengpan Ye; J. Li; Yunming Zhang; Ziyi Liu; Long Tang",
    "corresponding_authors": "",
    "abstract": "Existing work finds it challenging for adversarial examples to transfer among different synthetic speech detectors because of cross-feature and cross-model. To enhance the transferability of adversarial examples, we propose a spectral saliency analysis method and gain insight into the underlying detection mechanisms of existing detectors for the first time. These insights offer an interpretable basis for why adversarial examples are challenging to transfer between synthetic speech detection models. Then we further propose a two-stage adversarial attack framework. Specifically, the first stage leverages insights into the model detection mechanism to design a random time-frequency masking module, the random offset module, and one-dimensional convolution to generate transferable and robust adversarial examples. In the second stage, to mitigate the problem of obvious noise in the low-energy frames of the carrier in existing adversarial attacks, we perform secondary optimization on frames below the Signal-Noise-Rate threshold to enhance its auditory quality. Extensive experimental results demonstrate that the proposed method significantly enhances the transferability and robustness of adversarial examples, while simultaneously preserving the acoustic quality compared to typical approaches.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409045741",
    "type": "article"
  },
  {
    "title": "AI Visual Art History: An Art Movement With Expanded Artistic Horizon",
    "doi": "https://doi.org/10.1145/3726868",
    "publication_date": "2025-04-01",
    "publication_year": 2025,
    "authors": "Y. Wang; James She; Tianzhao Lin; Kang Zhang",
    "corresponding_authors": "",
    "abstract": "The progression of AI technology has spurred a growing number of artists to engage in AI Art production. This trend has sparked a multifaceted societal discourse. Given the tight integration of AI Art with society, it is essential to explore the potential art innovations and impacts AI Art might introduce. This article explores the relationship between AI visual art and conventional art, tracing the development of AI visual art and its societal reception. Through historical case studies, the article forecasts the future influence of AI visual art in the following aspects: Regarding artistic creation, AI visual art challenges established criteria for artistic evaluation. It presents artists with enhanced learning and creative capacities, paving the way for a new artist archetype characterized by human-computer symbiosis. From a societal standpoint, AI visual art is to advance the democratization of art, developing alongside traditional art forms.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409078647",
    "type": "article"
  },
  {
    "title": "Techniques to Conceal Dark Standby Flying Light Specks",
    "doi": "https://doi.org/10.1145/3724399",
    "publication_date": "2025-04-02",
    "publication_year": 2025,
    "authors": "Hamed Alimohammadzadeh; Shuqin Zhu; Shahram Ghandeharizadeh",
    "corresponding_authors": "",
    "abstract": "A Flying Light Speck, FLS, is a small drone configured with light sources to illuminate different colors and textures. A swarm of FLSs illuminates complex 3D multimedia shapes in a fixed volume, a 3D display. An FLS is a mechanical device. Its failure is the norm rather than an exception, causing a point of an illumination to go dark. In this paper, we use reliability groups with dark standby FLSs to minimize the duration of time a point remains dark. We introduce three techniques to prevent a dark standby FLS from obstructing the user’s field of view, FoV. All three move the FLS out of the user’s FoV. One technique, Suspend:Closest, maximizes the utility of a standby FLS while preventing it from obstructing the user’s FoV.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409107089",
    "type": "article"
  },
  {
    "title": "A Survey of Perceptual Hashing for Multimedia",
    "doi": "https://doi.org/10.1145/3727880",
    "publication_date": "2025-04-03",
    "publication_year": 2025,
    "authors": "Yuanding Zhou; Xinran Li; Cheng Xiong; Heng Yao; Chuan Qin",
    "corresponding_authors": "",
    "abstract": "Perceptual hashing is a cutting-edge technique in the field of digital multimedia security, which maps the perceptual content of multimedia information to a fixed length hash sequence to achieve content authentication. This survey provides a systematic overview of the definition, basic steps, main characters, and application scenarios of perceptual hashing. According to the different authentication objects, representative schemes of perceptual image hashing, perceptual video hashing and perceptual hashing for neural network model are introduced respectively. Both perceptual image hashing and perceptual video hashing can be divided into classical methods-based schemes and learning-based schemes, where learning-based schemes can be subdivided into supervised and unsupervised. Classical methods-based image hashing schemes can be divided into four categories: local feature-based, transformation-based, statistical feature-based, and dimensionality reduction-based. Classical methods-based video hashing schemes are mainly categorized into spatial feature-based schemes and spatial-temporal feature-based schemes. Additionally, we introduce the dataset composition and hash distance metric strategies for perceptual hashing, and analyze the performance of some representative schemes. Finally, we summarize the existing schemes and offer prospects for future research directions and development trends.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409125463",
    "type": "article"
  },
  {
    "title": "Multi-Person Pose Estimation with Feature Enhancement and Decoupling Based on Contrastive Learning",
    "doi": "https://doi.org/10.1145/3727984",
    "publication_date": "2025-04-04",
    "publication_year": 2025,
    "authors": "Zhiyuan Liu; Qi Zou; Xixia Xu; Yanting Pei",
    "corresponding_authors": "",
    "abstract": "Most methods of multi-person pose estimation treat the human detection and keypoint localization separately. They need additional supervision like instance bounding boxes, or complex hand-crafted processes like RoI cropping or grouping. In this paper, we propose a novel one-stage multi-person pose estimation (MPPE) method, named COPE, which unifies human detection and keypoint regression into an end-to-end learnable framework. To handle the challenges plague one-stage MPPE, i.e. instance overlapping and misalignment of local and global context, we design contrastive constraints at two levels of semantic granularity and feature sampling strategies. Based on a whole-process differentiable pipeline, COPE establishes a simple yet effective framework for MPPE without additional instance-level supervision and resource-intensive modules like transformer. Benefit from specially-designed contrastive constraints and sampling strategies, COPE can better handle occluded scenes and correct keypoint localization errors. Extensive experiments demonstrate COPE's superiority. It attains 71.3 AP and 18.0 FPS on COCO val2017, effectively balancing accuracy and speed. Particularly in crowded and occluded scenarios, COPE achieves state-of-the-art performance on CrowdPose and OCHuman, surpassing CID by 0.6 AP and 1.7 AP, respectively. Furthermore, COPE strongly improves generalization performance on the Human-Art benchmark, outperforming EDPose by 6.7 AP and ClickPose by 3.7 AP.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409160016",
    "type": "article"
  },
  {
    "title": "Effects of Human Cognition-Inspired Task Presentation on Interactive Video Retrieval",
    "doi": "https://doi.org/10.1145/3727983",
    "publication_date": "2025-04-04",
    "publication_year": 2025,
    "authors": "Nina Willis; Abraham Bernstein; Luca Rossetto",
    "corresponding_authors": "",
    "abstract": "Interactive video retrieval is a cooperative process between humans and retrieval systems. Large-scale evaluation campaigns, however, often overlook human factors, such as the effects of perception, attention, and memory, when assessing media retrieval systems. Consequently, their setups fall short of emulating realistic retrieval scenarios. In this paper, we study the effect of task target presentation on the rate of retrieval success in a large crowdsourced experiment. To do this, we design novel task presentation modes based on concepts in media memorability, implement the pipelines necessary for processing target video segments, and build a custom experimental platform for the final evaluation. Our findings demonstrate that the way in which the target of a video retrieval task is presented has a substantial influence on the difficulty of the retrieval task and that individuals can successfully retrieve a target video segment despite reducing or even altering the provided hints, opening up a discussion around future evaluation protocols in the domain of interactive media retrieval.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409160219",
    "type": "article"
  },
  {
    "title": "Sparse Reduced-Rank Fully Connected Layers with Its Applications in Detection and Classification",
    "doi": "https://doi.org/10.1145/3727982",
    "publication_date": "2025-04-08",
    "publication_year": 2025,
    "authors": "Linlin Fan; Mingliang Zhou; Xuekai Wei; Yong Feng; Tao Xiang; Bin Fang; Zhaowei Shang; Fan Jia; Xu Zhuang; Huayan Pu; Jun Luo",
    "corresponding_authors": "",
    "abstract": "Fully connected (FC) layers play a significant role in deep neural networks (DNNs) models. Owing to the complexity of its parameters, a FC layer has sufficient capacity to manage high-dimensional tasks, so a large amount of memory and powerful computing capabilities become essential requirements. However, the large number of parameters in a FC layer greatly limits the practical application of this model. To address this problem, we apply matrix optimization to a FC layer. First, an added penalty term properly maintains the sparsity of the imposed weights. Second, a rank constraint is applied to the two components of the factorized weight matrix. Our compression algorithm can effectively reduce the number of required network parameters, which not only reduces the computational complexity of the network but also results in better generalizability on a test dataset. Finally, the effectiveness of the proposed method is verified in two different computer vision task domains. Experiments show that our sparse reduced-rank method achieves a better compression ratio with a lower accuracy loss relative to the competing approaches. The code is available at https://github.com/cheer79/Compress_FC .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409257238",
    "type": "article"
  },
  {
    "title": "RePC: A Novel Neural Video Quality Enhancement System Framework for ABR Streaming of VBR-encoded Videos",
    "doi": "https://doi.org/10.1145/3727879",
    "publication_date": "2025-04-09",
    "publication_year": 2025,
    "authors": "Mengyu Shi; Miao Wang; Yujun Zhang",
    "corresponding_authors": "",
    "abstract": "With the emergence of next-generation video applications and increasing spatial resolutions, delivering high-quality video is still limited by network bandwidth. Adaptive bitrate (ABR) can select the appropriate bitrate for video streaming based on bandwidth, which can mitigate rebuffering caused by insufficient bandwidth. In comparison to Constant Bitrate (CBR), Variable Bitrate’s (VBR) encoding scheme can achieve the same quality with less bandwidth consumption and is gradually being widely used in ABR streaming. However, the quality of the video is still degraded due to a poor network. Recent research utilizes super-resolution (SR) in ABR streaming to construct neural video quality enhancement (VQE) systems, thereby improving the quality of video segments downloaded due to insufficient bandwidth. However, SR cannot participate in the downsampling encoding process of videos, which results in the effectiveness of existing SR-based VQE systems being inherently limited due to unavoidable information loss during downsampling encoding. Concurrently, SR’s high computational cost restricts neural VQE systems’ deployment on clients without GPUs. In contrast to the unidirectional workflow of SR, Rescaling can be integrated into the downsampling encoding process of videos, allowing favorable information to be retained for VQE. To implement high-quality real-time VQE for ABR streaming of VBR-encoded videos on CPUs, we propose RePC, a novel neural VQE system framework for optimizing existing neural VQE systems based on Rescaling (Re) for the first time, and Patch Content-awareness (PC). In detail, RePC uses Rescaling instead of SR to achieve better VQE by participating in the video downsampling. We also propose a Video Single-Image Rescaling model, VSIR, to indicate the effectiveness of RePC in quality enhancement. To speed up VQE, RePC designs a patch content-awareness algorithm to mix interpolation and neural computation based on the practical upsampling ability. Our evaluation results demonstrate quality gains of 0.55dB-2.96dB in PSNR and 1.79-3.18 in VMAF with SR’s half parameters, a speed-up of 15x-286x well up to real-time requirements on CPUs, and quality of experience (QoE) improvements of 16.58-26.65 are also achieved in an ABR system under various networking conditions.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409292485",
    "type": "article"
  },
  {
    "title": "Implicit Representation-based Volumetric Video Streaming for Photorealistic Full-scene Experience",
    "doi": "https://doi.org/10.1145/3728472",
    "publication_date": "2025-04-10",
    "publication_year": 2025,
    "authors": "Jianxin Shi; Miao Zhang; Linfeng Shen; Jiangchuan Liu; Yuan Zhang; Lingjun Pu; Jingdong Xu",
    "corresponding_authors": "",
    "abstract": "The widespread integration of the Internet of Things with sensors like depth-of-field cameras, LiDAR scanners, and eye-tracking infrared sensors, in head-mounted devices, has ushered in a new era of immersive digital experiences. Full-scene volumetric video (VV), a key innovation in this integration, provides a deeply immersive experience by capturing the richness and detail of the 3D world. However, its massive data volume presents significant streaming challenges. While 3D tile-based viewport approaches have been proposed, they struggle to full-scene VV given the small video buffer limitation, high tile segmentation overhead, and lack of full-scene consideration. In this work, inspired by the advancements of implicit neural radiance field (NeRF), we present V \\({{}^{2}}\\) NeRF , a novel full-scene VV streaming system featured by layered representation. It harmonizes the NeRF with explicit point clouds to represent the static background and dynamic foreground, thereby avoiding large data transfers and achieving photorealistic content representation. To tackle the issues of intensive computation requirements and multiscale adaptation scheduling within V \\({{}^{2}}\\) NeRF system, we propose a lightweight non-visible background removal method and a two-stage decoupled architecture. In addition, an efficient buffer-aware simulated annealing algorithm is developed, alongside the utilization of a perceptually-learned metric, to enhance user experience. We further discuss the concerns about practical development and deployment. Extensive prototype evaluations demonstrate V \\({{}^{2}}\\) NeRF ’s superior streaming and viewing performance on a wide variety of networks, viewing motions, and scenes. For instance, compared to state-of-the-art approaches, it achieves a 24% increment in perceptual quality, an 83% reduction in rebuffering time, and a 54% enhancement in user experience on average.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409316248",
    "type": "article"
  },
  {
    "title": "Interventional Feature Generation for Few-shot Learning",
    "doi": "https://doi.org/10.1145/3729171",
    "publication_date": "2025-04-10",
    "publication_year": 2025,
    "authors": "Shuo Wang; Jinda Lu; Huixia Ben; Yanbin Hao; Xingyu Gao; Meng Wang",
    "corresponding_authors": "",
    "abstract": "Few-shot learning (FSL) aims to classify a novel object into a specific category under limited training samples. This is a challenging task since (1) the features expressed by pre-trained knowledge introduce perceived bias and then constrain the classification space, and (2) the use of general hallucination techniques based on global features fails to escape the limited classification space, resulting in suboptimal improvements. To solve these issues, this paper proposes an interventional feature generation (IFG) method. Specifically, we first use the relations of the categories or instances as interventional operations to implicitly constrain the feature representations (pre-trained knowledge) into different classification subsets. Then, we employ a parameter-free feature generation strategy to enrich each subset’s training samples of the support category. In other words, IFG provides a multi-subsets learning strategy to reduce the influence of perceived bias, enrich the diversity of generated features, and improve the robustness of the few-shot classifier. We apply our method to four benchmark datasets and observe state-of-the-art performance across all experiments. Specifically, compared to the baseline on the Mini-ImageNet dataset, our approach yields accuracy improvements of 6.03% and 3.46% for 1 and 5 support training samples, respectively. Furthermore, the proposed interventional feature generation technique can improve classifier performance in other FSL methods, demonstrating its versatility and potential for broader applications. The code is available at https://github.com/ShuoWangCS/IFG-FSL/.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409327268",
    "type": "article"
  },
  {
    "title": "On Temporal Smoothness of Video Reconstruction Quality in the DCVS via Non-Uniform Sampling",
    "doi": "https://doi.org/10.1145/3729270",
    "publication_date": "2025-04-10",
    "publication_year": 2025,
    "authors": "Davoud Fani; Ali Asghar Beheshti Shirazi; M. Ghanbari; Esmatollah Rezaei",
    "corresponding_authors": "",
    "abstract": "To the distributed video coding approach, which focuses on fully or partially shifting the computational complexity from video encoder to the decoder, the simplicity and highly compact sampling in emerging compressive sensing appear to be very efficient tools. So, the distributed compressive video sensing (DCVS) has attracted much attentions in the video coding community by applying constant high measurement rate (MR) for key frames sampling and constant low MR for non-key frames sampling. According to use of constant and different MRs for the key and non-key frames sampling, severe and undesirable fluctuations in quality of reconstructed video frames is a common unresolved shortcoming in the DCVS, which negatively affects the users’ visual experience. To suppress sharp and undesirable quality fluctuations, group of picture (GOP)-level non-uniform MR allocation models are proposed in this paper for the key and non-key frames at the encoder of the DCVS. This enhances visual quality without incurring noticeable computational cost to the encoder. A new multi-step reconstruction scheme is also proposed at the decoder exploiting spatial-temporal information in the reconstruction process with a tolerable computational complexity and remarkable reconstruction performance. It compensates for quality degradations, which may be caused by non-uniform MR allocation, to successive GOPs to reach high average quality and temporal smoothness of quality at the same time. Extensive experiments on different video sequences show that not only desirable high average reconstruction quality is maintained, but severe and undesirable quality fluctuations are also well suppressed. Hence, the users’ perceived quality is highly promoted, while the compression ratio does not exceed a certain target by restricting average MR to reach target MR in the long-term.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409327473",
    "type": "article"
  },
  {
    "title": "Joint Structure-Texture Scan-Order for Point Cloud Attribute Compression Using Affine Transformation",
    "doi": "https://doi.org/10.1145/3729232",
    "publication_date": "2025-04-11",
    "publication_year": 2025,
    "authors": "Qian Yin; Xinfeng Zhang; Ruoke Yan; Yuhuai Zhang; Shanshe Wang; Siwei Ma",
    "corresponding_authors": "",
    "abstract": "Existing geometry-based point cloud compression (PCC) frameworks are typically designed to code the geometric coordinates first, followed by compressing the attributes (e.g., colors, reflectances) according to the order derived from geometric structures, such as the morton codes. Although geometry-based reordering methods can eliminate the redundancy of attributes, the errors caused by dramatic variations of attributes in the non-smooth areas potentially limit the efficiency of the point cloud attribute coding. To tackle this challenge, a novel joint structure-texture scan-order and coding scheme is proposed, which aims to explore a better attribute coding order from the viewpoint of improving the geometry-attribute consistency. Specifically, we formulate the attribute reordering problem as a geometry-attribute alignment task, and utilize the affine-transform model to find the optimal correspondence between geometry and attribute information by minimizing attributed prediction residuals. Then, the morton codes based point cloud reordering is conducted on the transformed point cloud. Note that our residual-based mode decision scheme implicitly embodies that the proposed reordering method further incorporates attribute textures based on the geometric structure. However, the exhaustive search for the optimal transformation space introduces the extremely high complexity to the encoder. Therefore, we also propose a fast pruning algorithm to narrow the search space for the approximate solution as an alternative. Experimental results conducted on the various benchmark datasets have illustrated that our proposed method outperforms the MPEG standard G-PCC with gains of up to 2%, 9% and 7% in luma, chroma and reflectance, respectively.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409359841",
    "type": "article"
  },
  {
    "title": "Elevating Textual Question Answering with On-Demand Visual Augmentation",
    "doi": "https://doi.org/10.1145/3729231",
    "publication_date": "2025-04-15",
    "publication_year": 2025,
    "authors": "Sina Ehsani; Jian Liu",
    "corresponding_authors": "",
    "abstract": "Textual Question Answering (TQA) remains a formidable challenge, despite over a decade of research. The integration of transformer networks and external knowledge via pre-trained models has marked a significant advancement in TQA. Yet, a crucial element often overlooked is the incorporation of external visual understanding. In this study, we introduce an innovative TQA approach that equips machines with the capability for on-demand visual grounding, thereby enriching their comprehension of questions and enhancing the relevance of generated answers. Our methodology utilizes Google's image search to tap into a vast pool of global knowledge and employs a novel technique for determining the most appropriate answer through on-demand visual grounding. We present a variety of multimedia model configurations, showcasing that our proposed method not only surpasses existing systems without necessitating pre-training but also achieves performance comparable to models 30 times its size, a testament to its efficiency. Furthermore, an interpretability analysis reveals the integral role of visual grounding in the model's decision-making process. This research offers a fresh outlook on augmenting TQA performance by harnessing the potential of visual grounding, with broad implications for natural language processing and artificial intelligence.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409478869",
    "type": "article"
  },
  {
    "title": "StyleInject: Parameter Efficient Tuning of Text-to-Image Diffusion Models",
    "doi": "https://doi.org/10.1145/3730403",
    "publication_date": "2025-04-16",
    "publication_year": 2025,
    "authors": "Mohan Zhou; Yalong Bai; Qing Yang; Tiejun Zhao",
    "corresponding_authors": "",
    "abstract": "The ability to fine-tune generative models for text-to-image generation tasks is crucial, particularly when facing the complexity involved in accurately interpreting and visualizing textual inputs. While LoRA is efficient for language model adaptation, it often falls short in text-to-image tasks due to the intricate demands of image generation, such as accommodating a broad spectrum of styles and nuances. To bridge this gap, we introduce StyleInject, a specialized fine-tuning approach tailored for text-to-image models. StyleInject comprises multiple parallel low-rank parameter matrices, maintaining the diversity of visual features. It dynamically adapts to varying styles by adjusting the variance of visual features based on the characteristics of the input signal. This approach significantly minimizes the impact on the original model's text-image alignment capabilities while adeptly adapting to various styles in transfer learning. StyleInject proves particularly effective in learning from and enhancing a range of advanced, community-fine-tuned generative models. Our comprehensive experiments, including both small-sample and large-scale data fine-tuning as well as base model distillation, show that StyleInject surpasses traditional LoRA in both text-image semantic consistency and human preference evaluation, all while ensuring greater parameter efficiency.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409486164",
    "type": "article"
  },
  {
    "title": "An X Language-Driven Framework for Systematic Development of Digital Twin Healthcare Systems",
    "doi": "https://doi.org/10.1145/3729230",
    "publication_date": "2025-04-17",
    "publication_year": 2025,
    "authors": "Kunyu Xie; Zhang Li; Yuan Yang; Xiaohe Li; Ridha Khédri; Zhen Chen; M. Jamal Deen",
    "corresponding_authors": "",
    "abstract": "The rapid advancements in big data and the Internet of Things (IoT) have significantly accelerated the digital transformation of medical institutions, leading to the widespread adoption of Digital Twin Healthcare (DTH). The Cloud DTH Platform (CDTH) serves as a cloud-based framework that integrates DTH models, healthcare resources, patient data, and medical services. By leveraging real-time data from medical devices, the CDTH platform enables intelligent healthcare services such as disease prediction and medical resource optimization. However, the platform functions as a system of systems (SoS), comprising interconnected yet independent healthcare services. This complexity is further compounded by the integration of both black-box AI models and domain-specific mechanistic models, which pose challenges in ensuring the interpretability and trustworthiness of DTH models. To address these challenges, we propose a Model-Based Systems Engineering (MBSE)-driven DTH modeling methodology derived from systematic requirement and functional analyses. To implement this methodology effectively, we introduce a DTH model development approach using the X language, along with a comprehensive toolchain designed to streamline the development process. Together, this methodology and toolchain form a robust framework that enables engineers to efficiently develop interpretable and trustworthy DTH models for the CDTH platform. By integrating domain-specific mechanistic models with AI algorithms, the framework enhances model transparency and reliability. Finally, we validate our approach through a case study involving elderly patient care, demonstrating its effectiveness in supporting the development of DTH models that meet healthcare and interpretability requirements.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409538758",
    "type": "article"
  },
  {
    "title": "Diverse and High-Quality Food Image Generation from Only Food Names",
    "doi": "https://doi.org/10.1145/3730588",
    "publication_date": "2025-04-18",
    "publication_year": 2025,
    "authors": "Dongjian Yu; Weiqing Min; Xin Jin; Qian Jiang; Ying Jin; Shuqiang Jiang",
    "corresponding_authors": "",
    "abstract": "Food image generation holds promising application prospects in food design, advertising, and food education. However, the existing methods rely on information such as recipes, ingredients, or food names, which leads to generated food images with less intra-class diversity. When recipes, ingredients and food names are identical for the same food, the real-world images may vary significantly in appearance. The question of how to simultaneously ensure the quality and diversity of the generated images is a key issue. To this end, we employ pre-trained diffusion model and Transformer to propose a method for generating diverse and high-quality images of both Chinese and Western food, named CW-Food. Different from previous works that utilize an overall food feature to generate new images, CW-Food first decouples the food images to obtain common intra-class features and private instance features. Additionally, we design a Transformer-based feature fusion module to integrate the common and private features, in order to avoid the shortcomings of conventional methods. Moreover, we also utilize a pre-trained diffusion model as our backbone, which is fine-tuned using LoRA with the fused multi-variate features. Extensive experiments on four datasets demonstrate the advantages of our proposed method, producing diverse and high-quality food images encompassing both Chinese and Western cuisines. To the best of our knowledge, our work is the first attempt to generate Chinese food images using only food names.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409572673",
    "type": "article"
  },
  {
    "title": "MAINet: Modality-Aware Interaction Network for Medical Image Fusion",
    "doi": "https://doi.org/10.1145/3731247",
    "publication_date": "2025-04-18",
    "publication_year": 2025,
    "authors": "Lisi Wei; Libo Zhao; Xiaoli Zhang",
    "corresponding_authors": "",
    "abstract": "Due to the limitations of imaging sensors, obtaining a medical image that simultaneously captures both functional metabolic data and structural tissue details remains a significant challenge in clinical diagnosis. To address this, Multimodal Medical Image Fusion (MMIF) has emerged as an effective technique for integrating complementary information from multimodal source images, such as CT, PET, and SPECT, which is critical for providing a comprehensive understanding of both anatomical and functional aspects of the human body. One of the key challenges in MMIF is how to exchange and aggregate this multimodal information. This paper rethinks MMIF by addressing the harmony of modality gaps and proposes a novel Modality-Aware Interaction Network (MAINet), which leverages cross-modal feature interaction and progressively fuses multiple features in graph space. Specifically, we introduce two key modules: the Cascade Modality Interaction (CMI) module and the Dual-Graph Learning (DGL) module. The CMI module, integrated within a multi-scale encoder with triple branches, facilitates complementary multimodal feature learning and provides beneficial feedback to enhance discriminative feature learning across modalities. In the decoding process, the DGL module aggregates hierarchical features in two distinct graph spaces, enabling global feature interactions. Moreover, the DGL module incorporates a bottom-up guidance mechanism, where deeper semantic features guide the learning of shallower detail features, thus improving the fusion process by enhancing both scale diversity and modality awareness for visual fidelity results. Experimental results on medical image datasets demonstrate the superiority of the proposed method over existing fusion approaches in both subjective and objective evaluations. We also validated the performance of the proposed method in applications such as infrared-visible image fusion and medical image segmentation.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409572687",
    "type": "article"
  },
  {
    "title": "EnIter: Enhancing Iterative Multi-View Depth Estimation with Universal Contextual Hints",
    "doi": "https://doi.org/10.1145/3731760",
    "publication_date": "2025-04-24",
    "publication_year": 2025,
    "authors": "Qianqian Du; Hui Yin; Lang Nie; Yanting Liu; Jin Wan",
    "corresponding_authors": "",
    "abstract": "Iterative inference approaches have shown promising success in the task of multi-view depth estimation. However, these methods put excessive emphasis on the universal inter-view correspondences while neglecting the correspondence ambiguity in regions of low texture and depth discontinuous areas. Thus, they are prone to produce inaccurate or even erroneous depth estimations, which is further exacerbated cumulative errors especially in the iterative pipeline, providing unreliable information in many real-world scenarios. In this paper, we revisit this issue from the intra-view Contextual Hints and introduce a novel enhancing iterative approach, named EnIter. Concretely, at the beginning of each iteration, we present a Depth Intercept (DI) modulator to provide more accurate depth by aggregating neighbor uncertainty, correlation volume of reference and normal. This plug and play modulator is effective at intercepting the erroneous depth estimations with implicit guidance from the universal correlation contextual hints, especially for the challenging regions. Furthermore, at the end of each iteration, we refine the depth map with another plug and play modulator termed as Depth Refine (DR). It mines the latent structure knowledge of reference Contextual Hints and establishes one-way dependency using local attention from reference features to depth, yielding delicate depth in details. Extensive experiment demonstrates that our method not only achieves state-of-the-art performance over existing models but also exhibits remarkable universality in popular iterative pipelines, e.g. , CasMVS, UCSNet, TransMVS, UniMVS.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409735558",
    "type": "article"
  },
  {
    "title": "Web3 Multimedia Applications: Under the Impact of Decentralization",
    "doi": "https://doi.org/10.1145/3725851",
    "publication_date": "2025-04-25",
    "publication_year": 2025,
    "authors": "Hao Wu; Maha Abdallah; Yuanfang Chi; Lehao Lin; Wei Cai",
    "corresponding_authors": "",
    "abstract": "In the Web3 ecosystem, multimedia applications exhibit significant potential by leveraging decentralization, regarded as the core spirit of Web3. This survey aims to provide a comprehensive overview of the potential of decentralization in shaping multimedia applications in the Web3 ecosystem. Through a systematic review of the academic research conducted over the past decade on Web3 decentralization, we identify the two key distinctive decentralization characteristics (decentralized assets and decentralized participation). Subsequently, we comprehensively analyze Web3 applications from both technology and application dimensions. Building upon this, we focus on multimedia-related aspects and propose an architecture for Web3 multimedia applications. In contrast to the broader scope of Web3 applications, the unique aspects of Web3 multimedia applications reside in their core application components (non-fungible tokens and smart contract-based rules) and core application domains (art, games, and social media). Based on this architecture, we provide a precise definition of Web3 multimedia applications. Lastly, through the lens of the two identified distinctive decentralization characteristics, we investigate the advantages, development, and limitations of Web3 multimedia applications within the three core application domains, namely crypto art, blockchain games, and blockchain on social media (BOSM). Furthermore, we share our insights into several promising yet challenging directions, covering the interoperability and potential of increasingly valuable multimedia content, as well as the delicate balance between centralization and decentralization.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409798557",
    "type": "article"
  },
  {
    "title": "M <sup>2</sup> ATURE: Mobile Multistage Throughput Prediction for Adaptive Video Streaming in Cellular Networks",
    "doi": "https://doi.org/10.1145/3724400",
    "publication_date": "2025-04-25",
    "publication_year": 2025,
    "authors": "Darijo Raca; Gregory Provan; Ahmed H. Zahran",
    "corresponding_authors": "",
    "abstract": "Accurate Throughput Prediction (TP) represents a real challenge for reliable adaptive streaming in challenging mediums, such as cellular networks. State-of-the-art solutions adopt Deep Learning (DL) models to improve TP accuracy for various multimedia systems. This paper illustrates that designing blackbox TP engines that depend solely on the model’s capacity and power of learning does not achieve consistent accuracy across all throughput ranges. Additionally, we propose MATURE, a novel multi-stage DL-based TP model designed to capture network operating context to improve prediction accuracy. MATURE’s prediction involves characterising the operating context before estimating the network throughput. We show that MATURE delivers consistent, accurate prediction for all throughput ranges in both 4G and 5G networks. We also show that light-weight mature models that use quantized parameters maintain their accuracy while featuring up to 100x faster inference, thus making them suitable for mobile implementation. Our real video streaming experiments further show that MATURE improves the average user Quality of Experience (QoE) by up to 20% when compared to other throughput prediction methods.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409798605",
    "type": "article"
  },
  {
    "title": "DiFace: Cross-Modal Face Recognition through Controlled Diffusion",
    "doi": "https://doi.org/10.1145/3732288",
    "publication_date": "2025-04-29",
    "publication_year": 2025,
    "authors": "Bowen Sun; Guo Lu; Shibao Zheng",
    "corresponding_authors": "",
    "abstract": "Diffusion probabilistic models (DPMs) have exhibited exceptional proficiency in generating visual media of outstanding quality and realism. Nonetheless, their potential in non-generative domains, such as face recognition (FR), has yet to be thoroughly investigated. Meanwhile, despite the extensive development of multi-modal FR methods, their emphasis has predominantly centered on visual modalities. In this context, FR through textual description presents a unique and promising solution that not only transcends the limitations from application scenarios but also expands the potential for research in the field of cross-modal FR. It is regrettable that this avenue remains underutilized, a consequence from the challenges mainly associated with three aspects: 1) the intrinsic imprecision of verbal descriptions; 2) the significant gaps between texts and images; and 3) the immense hurdle posed by insufficient databases. To tackle this problem, we present DiFace, an end-to-end solution that effectively achieves FR via text through a controllable diffusion process, by establishing its theoretical connection with probability transport. Our approach not only unleashes the potential of DPMs across a wide range of tasks but also showcases a remarkable improvement in accuracy for text-image FR, as evidenced by our experiments on verification and identification.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409920318",
    "type": "article"
  },
  {
    "title": "Community Transferrable Representation Learning for Image Style Classification",
    "doi": "https://doi.org/10.1145/3735136",
    "publication_date": "2025-05-12",
    "publication_year": 2025,
    "authors": "Jia Cui; Jiexuan Shen; Jialin Wei; Shiyu Liu; Zhaojia Ye; Shijian Luo; Zhen Qin",
    "corresponding_authors": "",
    "abstract": "Style is prone to experience but hard to formulate, even for design professionals. The current style classification (SC) methods can be categorized into mono-tasking (style) and multi-tasking (style, artists, genre and other additional clues). However, the style strength spread-out phenomenon challenges them by the uncertain noises’ distribution and unstable distinguishable feature representation. The multiple sub-styles make the principal style recognition hard for current SC approaches with lower accuracy. This paper proposes the community transferrable representation learning (CTRL) framework inspired by community detection and style transfer studies. The community feature is the transferrable feature embeddings in latent space through all samples within the same style concept. Therefore, instead of learning the unique style features, we introduce the learnable transferrable embeddings, whose distance will be smaller for the intraclass and larger for the interclass, to represent the principal style concept. There are two generative processes, p-AE and q-AE, as the cooperative operations to learn the community styles by the proposed diamond cells (D-cells) in a stacked way. The proposed community loss includes two terms: the invariance term is used to make the principal style embedding consistent, and the redundancy reduction term can decorate the different vector embeddings of sub-styles. The style classification and style feature learning model are optimized simultaneously in a semi-supervised way to learn the class-level style representation. We conduct experiments on six style datasets (three oil paintings, one architecture, one fashion and one general image). Results show that the proposed framework brings a performance gain of 2%-7% in terms of accuracy compared with the SOTA approaches.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410300657",
    "type": "article"
  },
  {
    "title": "MoHGCN: Momentum Hypergraph Convolution Network for Cross-modal Retrieval",
    "doi": "https://doi.org/10.1145/3735135",
    "publication_date": "2025-05-12",
    "publication_year": 2025,
    "authors": "Ying Li; Yuxiang Ding",
    "corresponding_authors": "",
    "abstract": "Cross-modal retrieval tasks, encompassing the retrieval of image-text, video-audio, and more, are progressively gaining significance in response to the exponential growth of information on the Internet. However, there has always been a cloud hanging over multimodal tasks due to the inherent challenges in aligning different modalities with distinct physical meanings. Most previous works simply rely on a single multimodal encoder or a novel similarity calculation for fusion, which often result in unsatisfactory performance. To tackle this challenge, we introduce a Momentum Hypergraph Convolutional Network (MoHGCN) for multimodal representation learning, which strengthens the alignment of both visual and textual data before the retrieval process. Specifically, MoHGCN utilizes contrastive learning to select the most challenging negative and positive samples to form hyperedges, and completes the modality alignment through two rounds of fusion. Subsequently, the fully integrated node features and global features are fused using a fusion encoder to obtain the final multimodal representation vector for image-text retrieval. Extensive experiments are conducted on two widely-used datasets, namely Flickr30K and MSCOCO, to demonstrate the superiority of the proposed MoHGCN approach in achieving the state-of-the-art performances.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410300693",
    "type": "article"
  },
  {
    "title": "DQFormer: Transformer with Decoupled Query Augmentations for End-to-End Multi-Object Tracking",
    "doi": "https://doi.org/10.1145/3735510",
    "publication_date": "2025-05-12",
    "publication_year": 2025,
    "authors": "Yuanzhou Huang; Songwei Pei; Rui Zeng",
    "corresponding_authors": "",
    "abstract": "Recent online Transformer-based multi-object tracking methods achieve end-to-end optimization by jointly performing detection and association. However, these trackers apply query augmentations uniformly to detect queries and track queries during training, which may limit their ability to fully exploit distinct features. The different augmentations serve distinct purposes and may have conflicting effects on detection and association. Moreover, the detect queries and the track queries with augmentations contain different semantic information. Jointly feeding these queries into self-attention modules for feature interaction may lead to suppression between the two types of queries. In this paper, we present a novel Transformer-based end-to-end model, DQFormer, which mitigates conflicts and effectively learns task-specific features through the proposed Decoupled Query Augmentation (DQA) strategy. DQA categorizes query augmentations into separate detection and association branches, generating more discriminative queries tailored specifically for detection and association. To align with DQA, we decompose the self-attention module into a Dual Cross-Attention module. Furthermore, a Targeted Label Assignment strategy is applied to the augmented queries in each cross-attention module, helping the model to learn distinct features for tracking. DQFormer achieves competitive detection and association results across the MOT17, MOT20, and DanceTrack datasets.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410300694",
    "type": "article"
  },
  {
    "title": "Domain-Aware Semantic Alignment Hashing for Large-Scale Zero-Shot Image Retrieval",
    "doi": "https://doi.org/10.1145/3734871",
    "publication_date": "2025-05-12",
    "publication_year": 2025,
    "authors": "Yongxin Wang; Feng Dong; Zhen-Duo Chen; Xin Luo; Xin-Shun Xu",
    "corresponding_authors": "",
    "abstract": "Hashing has been proven to be effective in the field of large-scale image retrieval. However, traditional hashing is stuck in performance dilemmas under zero-shot scenarios due to the concept shift problem. Although some zero-shot hashing methods exploit category attributes to facilitate knowledge transfer across domains, they usually struggle to generate domain-adaptive hash codes, making it hard to distinguish samples between unknown and known classes. With this motivation, we propose a novel approach called Domain-aware Semantic Alignment Zero-shot Hashing (DSAZH), which reveals three issues that suppress performance: semantic misalignment, biased optimization, and ambiguous Hamming distance. To address these challenges, multiple initiatives are innovatively integrated into a unified framework: First, it generates semantic-aligned hash codes through class-level and instance-level semantic alignment ; then it learns unbiased hash codes and domain-adaptive hash function through unbiased optimization equipped with asymmetric processing and class-prompting regression; finally, it distinguishes seen instances from unseen using domain-aware thresholding . Extensive experiments show that DSAZH achieves up to 15.82% MAP improvement ( e.g. , 69.71% vs. 53.89% on large-scale ImageNet with 256-bit codes) while reducing training time by two orders of magnitude ( e.g. , 3.07s vs. 202.84s), demonstrating its superior accuracy and efficiency compared to state-of-the-art ZSH methods. The source code is available at https://github.com/yxinwang/DSAZH .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410301123",
    "type": "article"
  },
  {
    "title": "Multi-Attribute Feature-aware Network for Facial Expression Recognition",
    "doi": "https://doi.org/10.1145/3735559",
    "publication_date": "2025-05-13",
    "publication_year": 2025,
    "authors": "Wei‐Yen Hsu; Yu-Chieh Chen",
    "corresponding_authors": "",
    "abstract": "Facial expression recognition (FER) has gained popularity as a research topic due to its broad applicability. However, real-world environments present significant challenges to FER, including occlusion, illumination variation, and angle. To address these issues, we propose a novel multi-attribute feature-aware network (MAFaNet) to enhance the performance and accuracy of FER in real-world environments. The proposed MAFaNet enhances the FER performance in real-world environments by effectively utilizing multi-attribute facial features from global, local, and salient subregions and thus fully exploiting the diverse potential information provided by each facial attribute, in line with the human face perception mechanism that extracts both global and regional information. Specifically, the global facial feature (GFF) module focuses on the more important facial features in the overall face by expanding the number of channels to preserve features and assigning weights to different channels. Moreover, the local facial feature (LFF) and salient facial feature (SFF) modules capture regional feature information from local and salient facial features, respectively, focusing on fine-grained regional features and reducing the interference from irrelevant regions in feature extraction. The experimental results indicate that the proposed MAFaNet method achieves the promising FER performance in comparison with the state-of-the-art approaches on several real-world datasets.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410343747",
    "type": "article"
  },
  {
    "title": "GAHE: Geometry-Aware Embedding for Hyper-Relational Knowledge Graph Representation",
    "doi": "https://doi.org/10.1145/3733602",
    "publication_date": "2025-05-13",
    "publication_year": 2025,
    "authors": "Zongsheng Cao; Qianqian Xu; Zhiyong Yang; Yuan He; Xiaochun Cao; Qingming Huang",
    "corresponding_authors": "",
    "abstract": "Knowledge graphs have proven highly effective for learning representations of entities and relations, with hyper-relational knowledge graphs (HKGs) gaining increased attention due to their enhanced representation capabilities. Each fact in an HKG consists of a main triple supplemented by attribute-value qualifiers that provide additional contextual information. Due to the complexity of hyper-relations, HKGs typically contain complex geometric structures, such as hierarchical, ring, and chain structures, often mixed together. However, previous work mainly embeds HKGs into Euclidean space, limiting their ability to capture these complex geometric structures simultaneously. To address this challenge, we propose a novel model called Geometry Aware Hyper-relational Embedding (GAHE). Specifically, GAHE adopts a multi-curvature geometry-aware approach by modeling HKGs in Euclidean space (zero curvature), hyperbolic space (negative curvature), and hyperspherical space (positive curvature) in a unified framework. In this way, it can integrate space-invariant and space-specific features to accurately capture the diverse structures in HKGs. In addition, GAHE introduces a module termed hyper-relational subspace learning, which allocates multiple sub-relations for each hyper-relation. It enables the exploitation of abundant latent semantic interactions and facilitates the exploration of fine-grained semantics between attribute-value pairs and hyper-relations across multiple subspaces. Furthermore, we provide theoretical guarantees that GAHE is fully expressive and capable of modeling a wide range of semantic patterns for hyper-relations. Empirical evaluations demonstrate that GAHE achieves state-of-the-art results on both hyper-relational and binary-relational benchmarks.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410344762",
    "type": "article"
  },
  {
    "title": "Semantic-driven Cross-space Graph Interaction Network for Fine-grained 3D Point Cloud Understanding",
    "doi": "https://doi.org/10.1145/3735560",
    "publication_date": "2025-05-13",
    "publication_year": 2025,
    "authors": "Peng Ren; Yunfeng Bai; Xiaoheng Li; Jinyuan Jia",
    "corresponding_authors": "",
    "abstract": "Since irregular 3D point clouds inherently lack connected relations, most approaches focus less on low-level spherical geometric features and high-level distant semantic feature dependencies and interactions, leading to inadequate feature representations for fine-grained point cloud understanding. To tackle the puzzle, we propose a novel and effective semantic-driven cross-space graph interaction network (CrossGIN) to explore and leverage local spatial features and range-aware semantic features across potential dual semantic spaces. Specifically, a local spatial aggregation is designed to capture position structures of spherical geometries by a spatial position module and enhance low-level spatial features using the semantic-aware module. Moreover, a graph interaction filter is proposed to dynamically aggregate the metric long-range semantic clues and better facilitate the adaptive feature interactions between 3D spatial and deep feature space. Finally, comprehensive experiments are conducted for 3D shape classification and object part segmentation tasks on several benchmark datasets such as ScanobjectNN, ModelNet40, and ShapeNetPart. The quantitative and qualitative results demonstrate that our method achieves competitive performance in comparison to recent approaches and verify the effectiveness of various modules.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410344798",
    "type": "article"
  },
  {
    "title": "TexIm FAST: Text-to-Image Encoding for Semantic Similarity Evaluation of Disproportionate Sequences",
    "doi": "https://doi.org/10.1145/3735974",
    "publication_date": "2025-05-16",
    "publication_year": 2025,
    "authors": "Wazib Ansar; Saptarsi Goswami; Amlan Chakrabarti; Basabi Chakraborty",
    "corresponding_authors": "",
    "abstract": "One of the principal objectives of Natural Language Processing (NLP) is to generate meaningful representations from text. Improving the informativeness of the representations has led to a tremendous rise in the dimensionality and the memory footprint. It leads to a cascading effect amplifying the complexity of the downstream model by increasing its parameters. The available techniques cannot be applied to cross-modal applications such as text-to-image. To ameliorate these issues, a novel Text-to-Image Fixed-dimensional encoding technique through a self-supervised Variational Auto-Encoder (VAE) for semantic evaluation applying transformers (TexIm FAST) has been proposed in this paper. The pictorial representations allow oblivious inference while retaining the linguistic intricacies, and are potent in cross-modal applications. TexIm FAST deals with variable-length sequences and generates uniform-dimensional images with over 75% reduced memory footprint. It enhances the efficiency of the models for downstream tasks by reducing its parameters. The efficacy of TexIm FAST has been extensively analyzed for the task of Semantic Textual Similarity (STS) on a benchmark data-set and two new data-sets put forth containing disproportionate sequences. The results demonstrate its exceptional ability to compare disparate length sequences such as a text with its summary with 3% improvement in accuracy compared to the SOTA despite having 68% less parameters.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410430101",
    "type": "article"
  },
  {
    "title": "Discrete Elective Hashing with Incomplete Labels for Efficient Cross-Modal Retrieval",
    "doi": "https://doi.org/10.1145/3736414",
    "publication_date": "2025-05-20",
    "publication_year": 2025,
    "authors": "Donglin Zhang; Caozhe Li; Mengke Li; Zhikai Hu",
    "corresponding_authors": "",
    "abstract": "Recently, supervised cross-modal hashing methods have gained considerable attention due to their ability to mine credible semantic relationships between multi-modal data. These methods typically rely on labels to explore semantic relationships provided that labels are always reliable, which, however, may not be true from the practical perspective. In fact, labels may be incomplete, i.e., true-label incomplete and fine-grained incomplete, which makes the performance of the existing methods deteriorated. To this end, we propose a method called Discrete Elective Hashing with Incomplete Labels (DEH-IL), which is designed to alleviate the impact of incomplete labels. Specifically, we introduce a relaxed label scheme that allows the algorithm to automatically mine potential missing information from incomplete labels, which is beneficial for exploring intra-class relationships. Moreover, we propose a novel elective loss that aggregates all estimations from incomplete labels to mine inter-class relationships. Since elective loss does not rely on any single estimation, it can effectively mitigate estimation errors arising from incomplete labels. By combining these two components, DEH-IL can effectively explore both intra-class and inter-class relationships through incomplete labels. Experimental results on benchmark datasets demonstrate the effectiveness of the proposed method.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410515637",
    "type": "article"
  },
  {
    "title": "Historical Object-Aware Prompt Learning for Universal Hyperspectral Object Tracking",
    "doi": "https://doi.org/10.1145/3736581",
    "publication_date": "2025-05-21",
    "publication_year": 2025,
    "authors": "Lu Zhang; Rui Yao; Yuhong Zhang; Yong Zhou; Fuyuan Hu; Jiaqi Zhao; Zhiwen Shao",
    "corresponding_authors": "",
    "abstract": "Hyperspectral Object Tracking (HOT), utilizing rich spectral information from hyperspectral video (HSV), holds significant importance for object tracking. We identify that a major obstacle in improving HOT performance lies in effectively leveraging spectral and historical information. Furthermore, due to the mismatch in band dimensions between hyperspectral and RGB images, state-of-the-art RGB-based trackers struggle to adapt to unified HOT tasks. To address this, we propose a Historical Object-Aware Prompt Learning (HOPL) method for universal hyperspectral object tracking. Initially, we transform hyperspectral image ( \\( N \\) bands) into multiple sets of three bands with different combinations and feed them into a backbone network to generate base features. Subsequently, we introduce a historical object-aware prompter, where historical object-aware images are input to generate prompt features that enhance the representation of object information when combined with base features. Additionally, we design a band information fusion module to integrate the multiple sets of base features. By introducing historical object-aware prompts, HOPL significantly enhances tracking performance without retraining the backbone network. Experimental results on the HOT2023 dataset (comprising HSV with 25-band, 16-band, and 15-band wavelength ranges) and HOT2022 dataset validate the superiority of HOPL over state-of-the-art methods. The source code is available at https://github.com/rayyao/HOPL .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410552372",
    "type": "article"
  },
  {
    "title": "Enhancing Video Conference Applications with <scp>VCApather</scp> : A Network as a Service Perspective",
    "doi": "https://doi.org/10.1145/3732780",
    "publication_date": "2025-05-22",
    "publication_year": 2025,
    "authors": "Dongbiao He; Xian Yu; Canshu Lin; Cédric Westphal; Zhongxing Ming; Laizhong Cui; Zhou Xu; J.J. Garcia‐Luna‐Aceves; Yanbiao Li",
    "corresponding_authors": "",
    "abstract": "The provision of performance-aware video conferencing services today relies on approaches that focus on data compression and client-side bitrate adaptation techniques to optimize transmission. However, these methods fail to quickly respond to fluctuations in network conditions, thereby compromising the quality of service for transmissions. For this reason, this paper aims to propose a novel traffic scheduling-based video transmission optimization solution from the perspective of the network service provider. We first investigate the resource requirements of video conferences and present the experiential performance of video conferences under different network conditions and network competition. Based on these results, we design a servicecustomized routing mechanism called VCApather that minimizes network contention. We then provide implementation solutions for the control plane and the data plane of VCApather . We evaluate VCApather using a fully-meshed topology with five nodes and real-world video conference traffic. The results show that VCApather is capable of achieving high link utilization and balance, while also meeting predefined user metrics. Compared to other schemes, VCApather could satisfy 69.8% more QoE requirements and yielded an average bitrate improvement of \\(\\times\\) .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410609909",
    "type": "article"
  },
  {
    "title": "ALOHA: Adapting Local Spatio-Temporal Context to Enhance the Audio-Visual Semantic Segmentation",
    "doi": "https://doi.org/10.1145/3735975",
    "publication_date": "2025-05-22",
    "publication_year": 2025,
    "authors": "Yanghao Zhou; Heyan Huang; Cunhan Guo; Rong-Cheng Tu; Zeyu Xiao; Bo Wang; Xian-Ling Mao",
    "corresponding_authors": "",
    "abstract": "Audio-Visual Semantic Segmentation (AVSS) plays a crucial role in pixel-level multi-modal perception for real-world applications such as robotic navigation and autonomous driving. Existing methods typically rely on global spatio-temporal modules to fuse audio and visual representations, which aids in generating pixel-level semantic masks. However, these approaches often overlook the importance of local spatio-temporal context in understanding semantics, leading to suboptimal performance. This limitation makes it difficult for models to accurately distinguish sound-emitting objects from irrelevant background noise, resulting in erroneous segmentation across the spatio-temporal dimension. To address this issue, we propose the ALOHA framework, which A dapts LO cal spatio-temporal context to en HA nce AVSS. The framework introduces two key components designed to leverage and enhance local spatio-temporal context information: the LOHA adapter and the Selective Context Enhancement (SCE) module. Specifically, the LOHA adapter adaptively captures essential modality information across spatio-temporal dimensions, while implicitly learning fine-grained local context through the local attention mechanism. Furthermore, the SCE module selectively enhances the local context related to the semantics, thereby facilitating the distinction between the sounding object and irrelevant background and improving segmentation accuracy. Moreover, to better adapt to embodied AI systems, our framework utilizes a parameter-shared encoder and applies the adapters in a staged manner. This design significantly reduces the number of trainable parameters, making it more parameter-efficient. Experimental results demonstrate that the proposed framework achieves state-of-the-art performance on the AVSBench-Semantic benchmark dataset and shows competitive results on the AVSBench-Object benchmark, while exhibiting broad adaptability across different visual backbone networks.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410609966",
    "type": "article"
  },
  {
    "title": "LPIC: Learnable Prompts and ID-guided Contrastive Learning for Multimodal Recommendation",
    "doi": "https://doi.org/10.1145/3735561",
    "publication_date": "2025-05-23",
    "publication_year": 2025,
    "authors": "Xin Liu; Qiya Song; Lin Xiao; Chuanbin Wang; Fen Xiao",
    "corresponding_authors": "",
    "abstract": "Multimodal recommendation systems improve the accuracy of recommendations by integrating information from different modalities to obtain potential representations of users and items. However, existing multimodal recommendation methods often use single user embedding to model users’ interests in different modalities, neglecting multimodal information. Furthermore, the semantics expressed by the same items in different modalities may be inconsistent, leading to suboptimal recommendation performance. To alleviate the impact of these issues, we propose a new multimodal recommendation framework called Learnable Prompts and ID-guided Contrastive Learning (LPIC). Specifically, we introduce a continuously learnable prompt embedding method, incorporating multimodal features of items to model users’ interests in specific modalities. Then, we propose an ID-guided contrastive learning component to enhance historical interaction features in textual, visual, and fused modalities, while aligning text, image, and fused modality to enhance semantic consistency between modalities. Finally, we conduct extensive experiments on three publicly available Amazon datasets to demonstrate the effectiveness of the LPIC framework.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410641204",
    "type": "article"
  },
  {
    "title": "Echo Depth Estimation via Attention based Hierarchical Multi-scale Feature Fusion Network",
    "doi": "https://doi.org/10.1145/3736768",
    "publication_date": "2025-05-26",
    "publication_year": 2025,
    "authors": "Wenjie Zhang; Jun Yin; Peng Yu; Yibo Guo; Xiaoheng Jiang; Shaohui Jin; Mingliang Xu",
    "corresponding_authors": "",
    "abstract": "In environments where vision-based depth estimation systems, such as those utilizing infrared or imaging technologies, encounter limitations—particularly in low-light conditions—alternative approaches become essential. Echo depth estimation emerges as a compelling solution by leveraging the time delay of echoes to map the geometric structure of the surrounding environment. This method offers distinct advantages in specific scenarios, providing reliable data for accurate scene understanding and 3D reconstruction. Traditional echo depth estimation techniques primarily depend on spatial information captured by the encoder and depth predictions made by the decoder. However, these methods often fail to fully exploit the rich depth features present at different simultaneous frequencies. To address this challenge, we propose an echo depth estimation method via Attention based Hierarchical Multi-scale Feature Fusion Network (AHMF-Net). This network is designed to extract spatial depth information from echo spectrograms across multiple scales and hierarchical levels, while fusing the most relevant information using an attention mechanism. AHMF-Net introduces two key modules in hierarchical levels: the Intra-layer Multi-scale Attention Feature Fusion (IMAF) module, which functions as the encoder to capture multi-scale features across varying granularities, and the Inter-layer Multi-Scale Detail Feature Fusion (IMDF) module, which integrates features from all encoding layers into the decoder to enable effective inter-layer multi-scale fusion. Additionally, the encoder incorporates an attention mechanism that enhances depth-related features by capturing channel dependencies at multiple scales. We evaluated AHMF-Net on the Replica, Matterport3D, and BatVision datasets, where it consistently outperformed state-of-the-art models in echo-based depth estimation, demonstrating superior accuracy and robustness. The source code is publicly available at https://github.com/wjzhang-ai/AHMF-Net.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410734423",
    "type": "article"
  },
  {
    "title": "Picasso: Analyzing Prompt Design for Text-To-Image Generative Diffusion Models from a Temporal-Spatial Perspective",
    "doi": "https://doi.org/10.1145/3724122",
    "publication_date": "2025-05-27",
    "publication_year": 2025,
    "authors": "Haoyu Cai; Wenqi Lou; Chao Wang; Xuehai Zhou",
    "corresponding_authors": "",
    "abstract": "Visual Concept Implantation (VCI) is essential in text-to-image fields. While VCI methods in diffusion models have matured, Visual Concept Disentanglement (VCD) remains an unexplored area. VCD involves analyzing Prompt Spaces trained in VCI to produce disentangled SubPrompts or SubCones for exploring interpretability. However, challenges arise due to Prompt Space design complexity and feature information extraction. We propose Picasso, a unified framework for VCD in diffusion models (DM). Our contributions include: for performance evaluation: Transforming VCD in DM into a regular clustering task by Visualization based on SubCones (VbSC); for unified framework design: Picasso processes diverse Prompt Spaces using spatially clusterable features; for prompt space exploration: Introducing a temporal-spatial SOTA Prompt Design subset based on temporal features. Our method provides a feasible mechanism for VCD in DM. Through functional and interpretability validation methods, we will comprehensively evaluate the effectiveness of our proposed method in visual concept implantation tasks and verify the correctness of the parameter space design principles. Picasso will be released at https://haoyu-cai.github.io/our_picasso.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410772208",
    "type": "article"
  },
  {
    "title": "Multi-Scale Reconstruction and Relation Decomposition Modeling for Group Activity Recognition",
    "doi": "https://doi.org/10.1145/3737646",
    "publication_date": "2025-05-28",
    "publication_year": 2025,
    "authors": "Longteng Kong; Wanting Zhou; Yongjian Huai; Jie Qin",
    "corresponding_authors": "",
    "abstract": "Group activity recognition (GAR) is a challenging task in computer vision, which needs to comprehensively model the spatiotemporal relations among actors. However, most previous methods tend to only model unitary actor relations, and directly aggregate actor features to form group representation at a single scale. To address these issues, we propose a novel GAR approach termed multi-scale cross-distance transformer (MSCD-Former), capable of capturing diverse actor relation contexts in multiple spatiotemporal scales. A cross-distance attentive block (CDA-Block) is designed to decompose the actor relations into local and distant ones, diversifying the relation features in rearranged groups. The multi-scale group descriptors are then enhanced by deploying stacked CDA-Blocks to cascaded stages and tightening the sampling scales accordingly. Moreover, we introduce a multi-scale reconstructive learning measure (MSR-Learning) between adjacent scales of CDA-Blocks. Via the reconstruction of actor relational features from lower scales to upper scales, MSR-Learning can enforce semantic consistency in multiple spatiotemporal scales. Consequently, our MSCD-Former boosts group activity recognition by fusing such discriminative relation features of different scales. We extensively evaluate the proposed approach on the VolleyTactic, Volleyball, Collective Activity, NBA, and JRDB-PAR datasets, and the experimental results demonstrate its superiority.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410816692",
    "type": "article"
  },
  {
    "title": "Scaled Background Swap: Video Augmentation for Action Quality Assessment with Background Debiasing",
    "doi": "https://doi.org/10.1145/3737461",
    "publication_date": "2025-05-29",
    "publication_year": 2025,
    "authors": "Xin Zhang; Hongzhi Feng; M. Shamim Hossain; Yinzhuo Chen; Hongbo Wang; Yuyu Yin",
    "corresponding_authors": "",
    "abstract": "Action Quality Assessment (AQA) has become crucial in video analysis, finding wide applications in various domains, such as healthcare and sports. A significant challenge faced by AQA is the background bias due to the dominance of the background in videos. Especially, the background bias tends to overshadow subtle foreground differences, which is crucial for precise action evaluation. To address the background bias issue, we propose a novel data augmentation method named Scaled Background Swap. Firstly, the background regions between different video samples are swapped to guide models focus toward the dynamic foreground regions and mitigate its sensitivity to the background during training. Secondly, the video’s foreground region is up-scaled to further enhance models’ attention to the critical foreground action information for AQA tasks. In particular, the proposed Scaled Background Swap method can effectively improve models’ accuracy and generalization by prioritizing foreground motion and swapping backgrounds. It can be flexibly applied with various video analysis models. Extensive experiments on AQA benchmarks demonstrate that Scaled Background Swap method achieves better performance than baselines. Specifically, the Spearman’s rank correlation on datasets AQA-7 and MTL-AQA reaches 0.8870 and 0.9526, respectively. The code is available at: https://github.com/Emy-cv/Scaled-Background Swap.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410856883",
    "type": "article"
  },
  {
    "title": "Spread Spectrum Watermark in DC: A view from the embedding processing",
    "doi": "https://doi.org/10.1145/3743139",
    "publication_date": "2025-06-06",
    "publication_year": 2025,
    "authors": "Huijuan Guo; Baoning Niu; Ying Huang; Xuefei Bai; Fangpeng Lan; Peng Zhao",
    "corresponding_authors": "",
    "abstract": "Spread spectrum watermarking is a robust methodology for image copyright identification. Embedding location and weight are two critical factors that influence watermark performance. As a rule of thumb, direct current (DC) coefficients are generally excluded from embedding the spread spectrum watermark due to the introduction of block artifacts. We investigate into this issue from the view of watermark embedding and reveal the fact that a desired embedding location should have small module and high stability. The DC location, while exhibiting high stability, is constrained by its large module. To address this limitation, we present a method for adjusting the DC coefficients (ADC). First, the discrete cosine transform (DCT) is performed on a matrix comprised of DC coefficients from all blocks of the host image. Second, the coefficients at the end of the high-frequency band, with the smallest module, are selected to form the embedding location. To adapt different real scenes, an attack-based weight method (ABW) is presented. It incorporates prior attack information into the weight determination. Experimental results demonstrate that the performance of the state-of-the-art algorithms is improved using the adjusted embedding location, and the integration of ADC and ABW methods further enhances the performance of the watermark system, which achieves an average PSNR of 48 dB with the bit error ratio (BER) approaching zero.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411087996",
    "type": "article"
  },
  {
    "title": "Efficient Privacy-Preserving Video Analytics via Share Transforming in Distributed Clouds",
    "doi": "https://doi.org/10.1145/3744248",
    "publication_date": "2025-06-10",
    "publication_year": 2025,
    "authors": "Tengfei Zheng; Bo Wang; Gen Li; Yuxing Tang; Qiang Dou",
    "corresponding_authors": "",
    "abstract": "Cloud-based video analytics services have been widely employed to support various real-world surveillance and monitoring applications while bearing the risk of disclosing sensitive visuals. The state-of-the-art (SoTA) solution has explored the feasibility of applying cryptographic techniques for privacy-preserving video analytics, but unfortunately incurs high computation and communication burdens on the end users. In this paper, we propose Pri3D , an efficient privacy-preserving video analytics system performed over two distributed clouds. Pri3D flexibly combines additive and multiplicative secret sharing techniques to free end devices and facilitate on-premise analytics services. Particularly, targeting the mainstream 3D convolutional neural network (CNN) pipeline, Pri3D securely accomplishes the non-linear operations (e.g., ReLU and max pooling) in merely 2 interaction rounds, owing to the novel design of the bi-directional transforming protocols for different modalities of secret sharing. To further optimize the latency and bandwidth confronted with large amounts of video data, \\(\\textsf{AS2MS}^{++}\\) and \\(\\textsf{MS2AS}^{++}\\) are proposed by subtly utilizing randomization factors and pre-encrypted nonce. With the transforming protocols, a series of privacy-preserving layer protocols are devised and tailored to build up the privacy-preserving analytics pipeline. Theoretical analysis shows that Pri3D can effectively fulfill the desired privacy requirements. Extensive evaluations demonstrate that Pri3D provides up to 11.85 \\(\\times\\) speed boost and 14.86 \\(\\times\\) communication reduction compared to the SoTA work, while it is sufficiently efficient for working on resource-constrained devices.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411178071",
    "type": "article"
  },
  {
    "title": "Spotting the Fakes: A Deep Dive into GAN-Generated Face Detection",
    "doi": "https://doi.org/10.1145/3742786",
    "publication_date": "2025-06-12",
    "publication_year": 2025,
    "authors": "Xin Wang; Ting Tsai; Li Lin; Guo Hui; Shu Hu; Ming-Ching Chang; Pradeep K. Atrey; Siwei Lyu",
    "corresponding_authors": "",
    "abstract": "Generative Adversarial Networks (GANs) have enabled the creation of highly authentic facial images, which are increasingly used in deceptive social media profiles and other forms of disinformation, resulting in serious consequences. Significant progress has been made in developing GAN-generated face detection systems to identify these fake images. This study offers a comprehensive review of recent advancements in GAN-generated face detection, focusing on techniques that detect facial images generated by GAN models. We categorize detection methods into three groups: (1) deep learning-based approaches, (2) physics-based methods, and (3) physiology-based methods. We summarize key concepts in each category, connecting them to relevant implementations, datasets, and evaluation metrics. Additionally, we provide a comparative analysis between automated detection and human visual performance to highlight the strengths and weaknesses of both approaches. Furthermore, we review related surveys, including detecting morphed faces, manipulated faces, DeepFake, and faces generated by diffusion models. Finally, we discuss unresolved challenges and suggest potential directions for future research.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411237757",
    "type": "article"
  },
  {
    "title": "Adaptive Multi-Exposure Image Correction via Joint Lightness and Structure Awareness",
    "doi": "https://doi.org/10.1145/3735973",
    "publication_date": "2025-06-20",
    "publication_year": 2025,
    "authors": "Bo Peng; Jia Zhang; Zhe Zhang; Liying Xu; Qingming Huang; Tao Wang; Jianjun Lei",
    "corresponding_authors": "",
    "abstract": "In order to alleviate the impact of ambient light on the quality of captured images, correcting multi-exposure images has become a popular topic. Most existing multi-exposure image correction methods mainly focus on the adjustment of lightness levels, but ignore the significant issue of structural information loss in incorrectly exposed images. Taking into consideration both lightness adjustment and structural reconstruction, this paper proposes an adaptive multi-exposure image correction network by jointly exploring the lightness and structure information, named LSANet. Specifically, the proposed LSANet first extracts lightness and structure representations of the input image in the frequency domain, and then performs exposure level adjustment and structure detail reconstruction based on the lightness and structure representations. In the proposed network, the lightness- and structure-aware adaptive module is designed to achieve adaptive correction by predicting dynamic kernels under the guidance of the lightness and structure representations. Experimental results on the widely-used ME and SICE datasets demonstrate that the proposed LSANet achieves excellent performance and generates images with well-exposed levels and rich structural details.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411488348",
    "type": "article"
  },
  {
    "title": "DIRL: Learning Discriminative Id-related Representations for Video Visible-Infrared Person Re-ID",
    "doi": "https://doi.org/10.1145/3745784",
    "publication_date": "2025-06-24",
    "publication_year": 2025,
    "authors": "Jiahe Wang; Xizhan Gao; Sijie Niu; Hui Zhao; Guang Feng",
    "corresponding_authors": "",
    "abstract": "The core of video-based visible-infrared person re-identification (VVI-ReID) lies in learning modal-sharing features, namely id-related feature representations, that are often mixed within modal-invariant features. Existing methods use weight sharing networks to learn frame-level modal-invariant features, but fail to consider modality invariance when computing sequence-level features, resulting in a significant gap between modalities. Moreover, these methods do not explicitly separate the id-related features and modal-related features, which reduces the model’s discriminative ability and leads to interference in VVI-ReID. In this paper, we propose a Discriminative Id-related Representation Learning network (DIRL) for video-based visible-infrared person re-identification. DIRL network consists of three key components, that is two-stream backbone module (TBM), cross-modality interaction module (CIM) and feature decoupling module (FDM). More specifically, the TBM is first constructed to preliminarily capture frame-level modal-invariant features. Then, the CIM is designed to interact information between modals and aggregate temporal features simultaneously, thereby obtaining sequence-level modal-invariant features. Finally, the FDM is designed to explicitly separate modal-related features from id-related ones within the modal-invariant features, thereby leaving only discriminative id-related representations. Through extensive benchmark experiments, our method demonstrates superior performance over state-of-the-art approaches by significant margins. Our code will be available at https://github.com/JhSearch/DIRL .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411577166",
    "type": "article"
  },
  {
    "title": "Multi-level Inter-Frame Parallelization in an Open Optimized VVC Encoder",
    "doi": "https://doi.org/10.1145/3742869",
    "publication_date": "2025-06-24",
    "publication_year": 2025,
    "authors": "Valeri George; Jens Brandenburg; Gabriel Hege; Tobias Hinz; Adam Wieckowski; Benjamin Bross; Thomas Schierl; Detlev Marpe",
    "corresponding_authors": "",
    "abstract": "This work investigates video encoding parallelization techniques based on the VVC standard, using the open and optimized encoder software implementation VVenC. Modern multi-processor systems offer significant opportunities for accelerating video encoding. By employing a proposed combination of parallelization methods, the VVenC encoder achieves an acceleration factor of up to 22 compared to single-threaded mode on a 32-core system, with potential increases to 27x at higher bitrates. Building upon prior work on inter-frame parallelization (IFP), the study introduces frame region-based synchronization, enabling further acceleration of up to 10%. Beyond that, the study demonstrates extending frame parallelization beyond Group of Pictures (GOP) boundaries, which improves IFP speed up by 37% and 11% at HD and UHD resolutions, respectively. Additional combinations with other VVC parallelization tools, such as tiles and VVC Wavefront Parallel Processing (WPP), are also explored. The paper provides a comprehensive analysis of parallelization challenges and highlights areas for further improvement.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411605028",
    "type": "article"
  },
  {
    "title": "CLIP-GS: CLIP-Informed Gaussian Splatting for View-Consistent 3D Indoor Semantic Understanding",
    "doi": "https://doi.org/10.1145/3746284",
    "publication_date": "2025-06-27",
    "publication_year": 2025,
    "authors": "Guibiao Liao; Jiankun Li; Zhenyu Bao; Xiaoqing Ye; Qing Li; Kanglin Liu",
    "corresponding_authors": "",
    "abstract": "Exploiting 3D Gaussian Splatting (3DGS) with Contrastive Language-Image Pre-Training (CLIP) models for open-vocabulary 3D semantic understanding of indoor scenes has emerged as an attractive research focus. Existing methods typically attach high-dimensional CLIP semantic embeddings to 3D Gaussians and leverage view-inconsistent 2D CLIP semantics as Gaussian supervision, resulting in efficiency bottlenecks and deficient 3D semantic consistency. To address these challenges, we present CLIP-GS, efficiently achieving a coherent semantic understanding of 3D indoor scenes via the proposed Semantic Attribute Compactness (SAC) and 3D Coherent Regularization (3DCR). SAC approach exploits the naturally unified semantics within objects to learn compact, yet effective, semantic Gaussian representations, enabling highly efficient rendering (&gt;100 FPS). 3DCR enforces semantic consistency in 2D and 3D domains: In 2D, 3DCR utilizes refined view-consistent semantic outcomes derived from 3DGS to establish cross-view coherence constraints; in 3D, 3DCR encourages features similar among 3D Gaussian primitives associated with the same object, leading to more precise and coherent segmentation results. Extensive experimental results demonstrate that our method remarkably suppresses existing state-of-the-art approaches, achieving mIoU improvements of 21.20% and 13.05% on ScanNet and Replica datasets, respectively, while maintaining real-time rendering speed. Furthermore, our approach exhibits superior performance even with sparse input data, substantiating its robustness.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411712579",
    "type": "article"
  },
  {
    "title": "Uniform Light Transformer for Person Re-identification under Complex Illumination",
    "doi": "https://doi.org/10.1145/3745786",
    "publication_date": "2025-07-01",
    "publication_year": 2025,
    "authors": "Xiang Guo; Ruimin Hu; Dongliang Zhu; Mei Wang",
    "corresponding_authors": "",
    "abstract": "The quality of pedestrian image retrieval is affected by the difference in illumination between images. Previous studies have used one-to-one lighting transformers to convert images taken under different lighting conditions into the target lighting. However, can we use a single lighting transformer to convert input images with various lighting conditions to the target lighting? This question motivated us to investigate the discrepancy between images generated by a Unified Lighting Transformer and the ground truth images across different illumination scales. We discovered that the modeling capability of the Unified Lighting Transformer for low-frequency information decreases gradually with an increase in the number of illuminant variations. Therefore, based on this insight, we proposed a Discriminative Feature Spectrum Consistency and Low-Frequency Information Constrained method. This method employs two constraints to enhance the Unified Lighting Transformer’s modeling capability for low-frequency information. The first mechanism enforces the constraint at the feature level by comparing the spectrum information between real and fake discriminative features. The second approach constrains the differences in pedestrian recognition features caused by the differences in low-frequency information between real and virtual images composed of low-frequency information from fake images and high-frequency information from authentic images. Our experiments show that our method outperforms other approaches and performs best across all metrics.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411870907",
    "type": "article"
  },
  {
    "title": "Online Hashing With Discriminative Attribute Embedding",
    "doi": "https://doi.org/10.1145/3746641",
    "publication_date": "2025-07-02",
    "publication_year": 2025,
    "authors": "Xingbo Liu; Zhijie Zhao; Xuening Zhang; X. Kang; Xiushan Nie",
    "corresponding_authors": "",
    "abstract": "Online hashing has emerged as a powerful tool for efficiently processing large-scale and streaming data. However, existing approaches often struggle with scalability limitations in similarity relations and inadequate discrimination provided by one-hot labels. To address these challenges, we propose Online Hashing with Discriminative Attribute Embedding (OHDAE). This novel method leverages a triple-matrix decomposition framework to dynamically decompose features into a dictionary, attributes, and category representations, effectively capturing semantic consistency without relying on accumulated data. To enhance the consistency and discriminability of attributes, we introduce an attribute construction strategy that integrates dictionary constraints with an online optimization strategy. Additionally, fine-grained semantic labels are embedded to improve the discriminability of hash codes by incorporating both semantic and similarity relationships. Experiments conducted on three benchmark datasets validate the superior performance, scalability and robustness of OHDAE compared to existing state-of-the-art methods.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411958777",
    "type": "article"
  },
  {
    "title": "Cross-Modality Relation and Uncertainty Exploration for Text-Based Person Search",
    "doi": "https://doi.org/10.1145/3747185",
    "publication_date": "2025-07-03",
    "publication_year": 2025,
    "authors": "Shijuan Huang; Zongyi Li; Hefei Ling; Jianbo Li",
    "corresponding_authors": "",
    "abstract": "Text-based person search aims to retrieve specific individuals from an extensive image gallery using textual queries. Recent approaches have delved into aligning global and part features in both text and image modalities, yielding substantial improvements. However, these methods often overlook intra-modality instance relations and uncertainties inherent in text-based person search. In response to these challenges, we propose the Cross-modality Relation and Uncertainty Exploration (CRUE) method to model the relations and uncertainties in the matching procedure. To alleviate the strict alignment issues arising from hard labels in the original contrastive loss, an Intra-modality Relation Exploration module (IRE) is introduced. This module is specifically designed to smooth hard-matching relations by modeling intra-modality similarity. Additionally, to address uncertain matching problems stemming from many-to-many relations, we propose a novel Uncertainty-guided Modeling module (UGM). This module is specifically designed to handle weak and noise-matched image-text pairs by modeling features as distributions, thereby alleviating instability and noise. Both the IRE and UGM modules effectively consider genuine intra-modality similarities and reduce the negative impact of uncertainties. Experimental results demonstrate significant improvements across three widely-used person search datasets, thereby validating the efficacy of the CRUE method in enhancing text-based person search. Our code will be available on GitHub at https://github.com/ShijuanHuang/CRUE .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412013308",
    "type": "article"
  },
  {
    "title": "Towards Key Point Identification (KPI) for Lecture Videos: Approaches and Performance Evaluation",
    "doi": "https://doi.org/10.1145/3746640",
    "publication_date": "2025-07-03",
    "publication_year": 2025,
    "authors": "Jiaqi Wang; R. Kwok; Edith C.‐H. Ngai",
    "corresponding_authors": "",
    "abstract": "To maximize the utility of lecture videos, in today's fast-paced society with dwindling attention spans, various e-learning technologies are introduced, e.g., non-linear learning, bite-sized learning, personalized lecture video fragment recommendation, etc. In this paper, we conduct a detailed performance study on a key enabler for aforementioned technologies: Lecture Video Fragmentation by Key Point Identification in lecture videos. We begin with a taxonomy of existing methods, which are classified into two categories: boundary-based methods, where the fragmentation is achieved using specific methods depending on the modality, and representation-based methods, where the fragmentation task is formulated as a boundary prediction task based on representations of smaller video chunks. Various configurations of these methods are also examined in detail. To conduct an extensive, comprehensive, and objective comparison study, we address the limitations of existing datasets by introducing a new lecture video fragmentation dataset, MITFLD, without any synthetic videos. We also propose a unified framework kpi , which includes the implementation of datasets, metrics, and compared methods to facilitate the experiments and future research on lecture video fragmentation. The experiments cover different configurations of existing methods on two large datasets (AVLecture and MITFLD). Further experiments are also conducted for ablation studies, such as the effect of feature combinations and the influence of lecture modes. Through the experiments, the representation-based method BiLSTM with self-supervised learning representations is found to exhibit promising performance. Key insights and potential future directions are also discussed.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412013313",
    "type": "article"
  },
  {
    "title": "Deep Chroma Compression of Tone-Mapped Images",
    "doi": "https://doi.org/10.1145/3744925",
    "publication_date": "2025-07-04",
    "publication_year": 2025,
    "authors": "Xenios Milidonis; Alessandro Artusi; Francesco Banterle",
    "corresponding_authors": "",
    "abstract": "Acquisition of high dynamic range (HDR) images is thriving due to the increasing use of smart devices and the demand for high-quality output. Extensive research has focused on developing methods for reducing the luminance range in HDR images using conventional and deep learning-based tone mapping operators to enable accurate reproduction on conventional 8 and 10-bit digital displays. However, these methods often fail to account for pixels that may lie outside the target display's gamut, resulting in visible chromatic distortions or color clipping artifacts. Previous studies suggested that a gamut management step ensures that all pixels remain within the target gamut. However, such approaches are computationally expensive and cannot be deployed on devices with limited computational resources. We propose a generative adversarial network for fast and reliable chroma compression of HDR tone-mapped images. We design a loss function that considers the hue property of generated images to improve color accuracy, and train the model on an extensive image dataset. Quantitative experiments demonstrate that the proposed model outperforms state-of-the-art image generation and enhancement networks in color accuracy, while a subjective study suggests that the generated images are on par or superior to those produced by conventional chroma compression methods in terms of visual quality. Additionally, the model achieves real-time performance, showing promising results for deployment on devices with limited computational resources.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412033435",
    "type": "article"
  },
  {
    "title": "Robust and Secure Hashing Towards Pirated Neural Network Model Detection",
    "doi": "https://doi.org/10.1145/3747297",
    "publication_date": "2025-07-07",
    "publication_year": 2025,
    "authors": "Cheng Xiong; Chuan Qin; Zhenxing Qian; Xiaolong Li; Xinpeng Zhang",
    "corresponding_authors": "",
    "abstract": "With accelerating development of artificial intelligence, neural network models have been applied in many fields. The structure designing and training for models consume vast manpower and computing resources, which are the core interests of related research institutions and enterprises. However, the high-value attributes of neural network models also attract the attention of pirates, who may steal them for illegal profits and also slightly modify model parameters to escape model piracy detection. In order to solve the problem, in this work, we propose a robust and secure model hashing method based on dynamic branch reorganization and multi-feature fusion. Detailedly, we dynamically adjust the branches in our model hashing network to extract the robust features of all kinds of model parameters for generating the hash sequence. Besides, we integrate the encryption and the feature matrix generation to a unified stage in the hash generation for resisting possible encryption escape attack. Thus, the pirated models, even with some modifications, can be correctly detected through calculating hash distances. Experimental results demonstrate the effectiveness and superiority of our model hashing method with respect to pirated model detection, non-pirated model discrimination and security.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412070816",
    "type": "article"
  },
  {
    "title": "Multi-spectral Class Center Network for Face Manipulation Localization",
    "doi": "https://doi.org/10.1145/3747296",
    "publication_date": "2025-07-11",
    "publication_year": 2025,
    "authors": "Changtao Miao; Qi Chu; Zhentao Tan; Zhenchao Jin; Tao Gong; Wanyi Zhuang; Yue Wu; Bin Liu; Hanyang Hu; Nenghai Yu",
    "corresponding_authors": "",
    "abstract": "As deepfake content proliferates online, advancing face manipulation forensics has become crucial. To combat this emerging threat, previous methods mainly focus on studying how to distinguish authentic and manipulated face images. Although impressive, image-level classification lacks explainability and is limited to specific application scenarios, spurring recent research on pixel-level prediction for face manipulation forensics. However, existing forgery localization methods suffer from exploring frequency-based forgery traces in the localization network. In this paper, we observe that multi-frequency spectrum information is effective for identifying tampered regions. To this end, a novel M ulti- S pectral C lass C enter Net work (MSCCNet) is proposed for face manipulation localization. Specifically, we design a M ulti- S pectral C lass C enter (MSCC) module to learn more generalizable and multi-frequency features. Based on the features of different frequency bands, the MSCC module collects multi-spectral class centers and computes pixel-to-class relations. Applying multi-spectral class-level representations suppresses the semantic information of the visual concepts which is insensitive to manipulated regions of forgery images. Furthermore, we propose a M ulti-level F eatures A ggregation (MFA) module to employ more low-level forgery artifacts and structural textures. Meanwhile, we conduct a comprehensive localization benchmark based on pixel-level FF++ and Dolos datasets. Experimental results quantitatively and qualitatively demonstrate the effectiveness and superiority of the proposed MSCCNet. We expect this work to inspire more studies on pixel-level face manipulation localization. The codes are available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412198937",
    "type": "article"
  },
  {
    "title": "Ultra-Fast Intra Screen Content Coding via Accelerated Re-visit CU-coding in AVS3",
    "doi": "https://doi.org/10.1145/3748509",
    "publication_date": "2025-07-14",
    "publication_year": 2025,
    "authors": "Xueyan Cao; Tao Lin; Kailun Zhou; Liping Zhao; Wei Han; Shanshe Wang; Yufen Yang",
    "corresponding_authors": "",
    "abstract": "Screen Content Coding (SCC) is an indispensable tool for enabling distributed collaboration, such as video conferencing. Encoders in the latest video coding standards, particularly for SCC scenarios, employ a wider variety of partitioning tree splitting types, recursively traversing all branches, as well as a larger number of coding modes and submodes, to achieve higher coding efficiency compared to encoders in previous standards. This process leads to very high coding complexity, as each tree leaf node, called a coding unit (CU), for every partitioning size and location in the picture is repeatedly visited and evaluated multiple times during the optimal partitioning search. Additionally, each CU visit involves evaluating a vast number of coding options and their combinations to identify the best one. The complexity is further exacerbated in SCC due to the addition of many new CU coding modes and options. To significantly reduce SCC complexity without coding efficiency loss, this paper proposes a new technique, Accelerated Revisit CU-coding (ARC), along with an SCC search space analysis for in-depth operation-level and run/platform-independent assessment of SCC complexity. ARC exploits the correlation between the first visit and subsequent revisits of a CU with the same location and size. By fully leveraging the correlation and information from the first visit, ARC significantly accelerates revisit CU-coding while maintaining the same high coding efficiency. ARC is implemented in HPM, the AVS3 reference software. Experiments demonstrate that ARC reduces encoding runtime by 29.74%, 47.78%, and 54.25% for 1920x1080 FHD, 4 K UHD, and 8 K UHD test sequences, respectively, in All Intra configuration, without coding efficiency loss. These runtime reductions align with corresponding search space reductions of 30.91%, 49.67%, and 54.41%, as obtained from the search space analysis.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412400241",
    "type": "article"
  },
  {
    "title": "Infrared and Visible Image Fusion via Text-Prior Guided Frequency-Domain Decomposition",
    "doi": "https://doi.org/10.1145/3748510",
    "publication_date": "2025-07-14",
    "publication_year": 2025,
    "authors": "Wenyu Shao; Hongbo Liu",
    "corresponding_authors": "",
    "abstract": "Existing fusion methods usually neglect the effective utilization of frequency-domain information and deeper textual semantic information, which degrades the quality and semantic richness of the fused image. To address these limitations, we propose a novel Text-prior guided Frequency-Domain Decomposition Fusion network ( \\(\\text{TFD}^{2}\\text{Fusion}\\) ). Specifically, \\(\\text{TFD}^{2}\\text{Fusion}\\) features a two-stage encoder designed to enhance the representational capacity of the fused image while enriching its semantic content: (i) In the feature decomposition stage, we design a three-branch feature decomposition and aggregation architecture, which utilizes the frequency-domain feature decomposition module to extract multi-grained low- and high-frequency features in the scene and detail branches, and introduces a frequency-domain fusion module in the aggregation branch to integrate shallow visual features extracted from different convolution blocks in the frequency-domain, thereby enhancing the representation of scene and detail information; (ii) In the prior guidance stage, we propose a text-prior guided semantic amplifier, which leverages a large vision-language model to interactively integrate text-level priors into visual features, thus enriching the semantic information of the fused image. Additionally, to promote the implementation and application of the proposed text-prior guided feature decomposition fusion paradigm, we release benchmark datasets that integrate text-level priors into three mainstream datasets. Extensive experiments demonstrate the superior fusion performance of our \\(\\text{TFD}^{2}\\text{Fusion}\\) . The datasets and code are available at https://github.com/wyshao-01/TFD2Fusion .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412400260",
    "type": "article"
  },
  {
    "title": "Action-Aware Visual-Textual Alignment for Long-Instruction Vision-and-Language Navigation",
    "doi": "https://doi.org/10.1145/3748656",
    "publication_date": "2025-07-15",
    "publication_year": 2025,
    "authors": "Bowen Huang; Yanwei Zheng; Chuanlin Lan; Dongchen Sui; Xinpeng Zhao; Xiao Zhang; Mengbai Xiao; Dongxiao Yu",
    "corresponding_authors": "",
    "abstract": "Traditional vision-and-language navigation (VLN) requires an agent to navigate to a target location solely based on visual observations, guided by natural language instructions. Compared to this task, long-instruction VLN involves longer instructions, extended trajectories, and the need to consider more contextual information for global path planning. As a result, it is more challenging and requires accurately aligning the instructions with the agent's current visual observations, which is accompanied by two significant issues. Firstly, there is a misalignment between actions. The visual observations of the agent at each step lack explicit action-related details, while the instructions contain action-oriented words. Secondly, there is a misalignment between global instructions and local visual observations. The instructions describe the entire navigation trajectory, whereas the agent's visual observations only provide localized information about a specific position along the trajectory. To address these issues, this paper introduces the Action-Perception Alignment Framework (APAF). In this framework, we first design the Action-Contextual Encoding Module (ACEM), which enriches the agent's visual perception by encoding potential actions with relative heading and elevation angles. We then propose the Dynamic Instruction Weighting Module (DIWM), which adjusts the importance of instruction words based on the agent's current visual observations, emphasizing those words most relevant to the agent's visual observations. Our approach significantly outperforms existing methods, achieving state-of-the-art results with improvements of 8.5% and 4.0% in Success Rate (SR) on the long-instruction R4R and RxR datasets, respectively.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412420758",
    "type": "article"
  },
  {
    "title": "Visible-Infrared Person Re-Identification Based on Feature Decoupling and Refinement",
    "doi": "https://doi.org/10.1145/3749843",
    "publication_date": "2025-07-22",
    "publication_year": 2025,
    "authors": "Hao Ding; Jing Sun; Rui Long; Xiaoping Jiang; Hongling Shi; Yuting Qin; Zongze Li; Jian‐Jin Li",
    "corresponding_authors": "",
    "abstract": "The objective of visible-infrared person re-identification is to accurately match pedestrian images captured in different modalities. Since these images are taken from varying viewpoints by different cameras, the cross-modal detection task must address both modality discrepancies and camera variations. Many existing approaches primarily focus on minimizing inter-modality differences to enhance retrieval accuracy, often overlooking the impact of camera viewpoint differences. To tackle these challenges, this paper introduces a hierarchical feature decoupling network. Firstly, the network decouples and extracts camera-related and camera-irrelated features separately to mitigate the effects of camera variations. Secondly, it addresses modality differences by extracting modality-independent features. Additionally, an adversarial decoupling loss is employed to further disentangle identity-irrelevant information from identity-relevant features, thereby boosting the system's accuracy and robustness. Extensive experiments conducted on the SYSU-MM01 and RegDB datasets validate the effectiveness of the proposed method.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412565295",
    "type": "article"
  },
  {
    "title": "Source Information assisted UV-Space Transformation Network for Person Image Generation",
    "doi": "https://doi.org/10.1145/3749375",
    "publication_date": "2025-07-22",
    "publication_year": 2025,
    "authors": "Guiyu Xia; Zhedong Jin; Dongdong Fang; Yubao Sun",
    "corresponding_authors": "",
    "abstract": "Person image generation is widely used in many fields, but it still faces some challenges. Most of current person image generation methods suffer from an intractable problem of handling the spatial deformation caused by the pose change in the generation process, while convolution based generative model is not good at handling the region-unaligned task. Therefore, we propose a novel UV-space transformation network to implement the primary generation of person image in the UV-space. This framework can effectively avoid the spatial deformation problems in the generation process and instead transfer them to the preceding pose estimation stage. Within the framework, we propose the self-reconstruction assisted UV texture transformation blocks which aim to exploit the self-reconstruction of source texture map to guide and assist the generation of target UV texture map. In addition, after obtaining the target person image from the generated UV texture map, we use the correlations between the source and generated images to further improve the details of the generated person images. Superior experiment results compared with other state-of-the-art methods demonstrate the effectiveness of the proposed method.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412565313",
    "type": "article"
  },
  {
    "title": "A Patch Can Disrupt Live Video Streaming: Physical Adversarial Attacks on Deep Learning Compression",
    "doi": "https://doi.org/10.1145/3750047",
    "publication_date": "2025-07-23",
    "publication_year": 2025,
    "authors": "Yuqing Yang; Anh Nguyen; Zhisheng Yan",
    "corresponding_authors": "",
    "abstract": "Deep learning (DL)-based compression has achieved outstanding performance compared to traditional compression. However, due to the vulnerability of adversarial attacks on DL models, understanding the security of DL-based compression is crucial. Previous attacks have demonstrated the feasibility of failing DL-based compression models in the digital domain. However, these attacks rely on perfect digital modification of the whole image and internal access to camera/server files, preventing their usage in the physical world where such assumptions do not hold. In this paper, we unveil the first physical adversarial attack targeting DL-based compression in the context of live video streaming. The proposed attack, namely CamHack , places a small-sized physical patch in the visual scene covered by the live camera to manipulate the bitrate of compressed content and disrupt live streaming. The patch is crafted to address color and geometric transformations in diverse streaming scenes while remaining inconspicuous. Extensive experiments in various streaming scenes and network conditions show that CamHack increases bit consumption by 276.58%, 384.74%, and 942.28% over the clean scene without a patch on three representative victim models. CamHack is also robust under various practical impacts such as patch location, lighting condition, and patch size.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412602605",
    "type": "article"
  },
  {
    "title": "MRFGNet: Multiscale Reference Frame Generation Network for VVC Inter Coding",
    "doi": "https://doi.org/10.1145/3750049",
    "publication_date": "2025-07-23",
    "publication_year": 2025,
    "authors": "Pengyu Li; Cheolkon Jung",
    "corresponding_authors": "",
    "abstract": "Since the quality of the reference frames is critical for VVC inter coding, the neural network (NN)-based reference frame generation aims to generate a better quality reference frame from the two decoded frames in the decoded picture buffer (DPB) and inserts it into the reference picture list (RPL) for inter prediction. However, it is difficult to directly apply video frame interpolation or extrapolation to the reference frame generation because compression artifacts severely degrade the quality of the generated reference frame. In this paper, we propose a multiscale reference frame generation network for VVC inter coding, named MRFGNet. To reduce the compression artifacts, MRFGNet adopts the high-performance operation point (HOP) network, which has been released by JVET, as preprocessing for frame enhancement. Unlike the HOP in-loop filter that takes multiple inputs, MRFGNet only takes the reconstructed frame and QP map as input for the HOP network. Moreover, a frame generation network is proposed to conduct accurate motion estimation based on directional optical flow. Thus, in both RA and LDB configurations, MRFGNet has the same architecture to achieve both bidirectional and unidirectional predictions. MRFGNet estimates optical flow at mulitple scales with the same dimension, thereby leveraging scale independent bidirectional optical flow prediction. An optical flow warping and fusion module is designed to two cascaded U-nets for flow feature maps and get two frames, and finally the two frames are fused to generate a reference frame. Furthermore, a novel training strategy based on QP distance is utilized to optimize MRFGNet by taking compressed data with higher quality as label for training. Experimental results show that MRFGNet achieves average BD rate gains of {-5.10% (Y), -12.14% (U), -11.51% (V)} and {-5.98% (Y), -15.32% (U), -14.22% (V)} over VTM_11.0-NNVC_4.0 anchor under RA and LDB configurations, respectively, as well as outperforms the state-of-the art method for reference frame generation, i.e., JVET-AD0160, by {0.78% (Y), 1.42% (U), 1.44% (V)} and {2.82% (Y), 4.92% (U), 6.62 %(V)} under RA and LDB configurations, respectively.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412602612",
    "type": "article"
  },
  {
    "title": "GD-NeRF: Generative Detail Compensation for One-shot Generalizable Neural Radiance Fields",
    "doi": "https://doi.org/10.1145/3748331",
    "publication_date": "2025-07-23",
    "publication_year": 2025,
    "authors": "Xiao Pan; Zongxin Yang; Shuai Bai; Yi Yang",
    "corresponding_authors": "",
    "abstract": "In this paper, we focus on the one-shot novel view synthesis task which targets synthesizing photo-realistic novel views given only one reference image per scene. Previous One-shot Generalizable Neural Radiance Fields (OG-NeRF) methods solve this task in a finetuning-free manner, yet suffer from the blurry issue due to the encoder-only architecture that highly relies on the limited reference image. On the other hand, recent diffusion-based image-to-3d methods show vivid plausible results via distilling pre-trained 2D diffusion models, yet require tedious per-scene optimization. Targeting these issues, we propose GD-NeRF, a generative detail compensation framework that is both capable of producing vivid plausible details and is finetuning-free. Following a coarse-to-fine strategy, it is mainly composed of a One-stage Parallel Pipeline (OPP) and a Diffusion-based 3D-consistent Enhancer (Diff3DE). At the coarse stage, OPP first efficiently integrates the GAN model into the existing OG-NeRF pipeline for injecting primary in-distribution details. Then, at the fine stage, Diff3DE further leverages the pre-trained diffusion models to complement rich out-distribution details while maintaining decent 3D consistency. Extensive experiments on both the synthetic and real-world datasets show that GD-NeRF noticeably improves the vivid details while eliminating the need for per-scene finetuning.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412602637",
    "type": "article"
  },
  {
    "title": "Syntactic-Conditional Diffusion Networks for Controllable Image Captioning",
    "doi": "https://doi.org/10.1145/3748653",
    "publication_date": "2025-07-24",
    "publication_year": 2025,
    "authors": "Bing Liu; Wenjie Yang; Mingming Liu; Hao Liu; Yong Zhou; Peng Liu",
    "corresponding_authors": "",
    "abstract": "Current diffusion model-based image captioning methods generally focus on generating descriptions in a non-autoregressive manner. Nevertheless, it is not trivial to employ such generative models to control the generation of discrete words while pursuing the balance between diversity and accuracy. Inspired by the success of continuous diffusions in image captioning, we introduce the Part-of-Speech (POS) information and classifier-free guidance into the diffusion model, and propose a novel controllable image captioning model, namely POS-Conditional Diffusion Networks (POSCD-Net), which consists of a Diffusion-based POS Generator (DPG) and a Diffusion-based Caption Generator (DCG). The DPG is built to produce diverse syntactic structures for each input image. The diverse POS sequences are further regarded as the control signals of the DCG, which produces the output sentences in a conditional diffusion process. In the DCG, a syntactic control module (SCM) is designed to strengthen the alignment progressively between words and the corresponding POS tags in a cascaded manner. Furthermore, to improve the controllability of POSCD-Net, the classifier-free guidance with learnable parameters is exploited to jointly optimize both the DPG and DCG in a non-autoregressive manner. Extensive experiments on the MSCOCO dataset demonstrate that our proposed method outperforms the state-of-the-art non-autoregressive counterparts and achieves promising performance compared with the autoregressive models.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412627883",
    "type": "article"
  },
  {
    "title": "MM-MoE: A Disease-oriented Multi-Task and Multi-View Framework for Generalized Cardiac Segmentation",
    "doi": "https://doi.org/10.1145/3757325",
    "publication_date": "2025-07-31",
    "publication_year": 2025,
    "authors": "Zheng Zhang; Yushan Song; Tianzhuzi Tan; Bo Zhang; Wu Liu; Xiuzhuang Zhou; Huadóng Ma",
    "corresponding_authors": "",
    "abstract": "Cardiovascular diseases typically cause specific changes in cardiac structure, but these disease-induced variations are often less pronounced than those introduced by multi-source data acquisition or noise in the images, such as fluctuations in image brightness, contrast, and field of view. This makes it more difficult to extract key details about organs and diseases, posing significant challenges to traditional segmentation and domain generalization methods. To address these challenges, we propose MM-MoE, a disease-oriented multi-task and multi-view framework designed for more accurate generalized cardiac structure segmentation. Specifically, we introduce a Mixture-of-Experts based Multi-Task Joint and Adversarial Learning Strategy (MTMoE-J&amp;A) to enable the model to learn more general and domain-invariant disease-specific features, and employ a Multi-View Feature Fusion Network (MVFusion) to effectively integrate complementary spatial information from different views, leading to improved segmentation accuracy and completeness. Additionally, we develop a parallel Channel-Spatial-Slice Attention Module (CSS-Attn) to mitigate crucial detail loss caused by the anisotropic nature of medical images. Extensive experiments conducted on the M&amp;Ms-2 dataset demonstrate that MM-MoE outperforms state-of-the-art segmentation models and domain generalization methods in terms of segmentation generalization performance.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412792140",
    "type": "article"
  },
  {
    "title": "Trans-Convo-Former Net for hierarchical prediction of household images",
    "doi": "https://doi.org/10.1145/3757323",
    "publication_date": "2025-07-31",
    "publication_year": 2025,
    "authors": "Divya Arora Bhayana; Om Prakash Verma",
    "corresponding_authors": "",
    "abstract": "Image classification has become the backbone of computer vision in recent times. Hierarchical image classification has been a scarcely exploited field, particularly in household images. Although many convolution and transformer learning models have been introduced for image classification, the fusion models exhibit much better performance in image classification. The potential of hierarchical image classification for household robotics has not yet been explored. Therefore, we propose a Trans-convo-former net for the hierarchical prediction of household images. The fine class refers to the class of the object identified, and the coarse class refers to the object's location. This process facilitates the path-planning stage of household robotics. Trans-convo-former net employs self-attention-based encoders with intermittent convolution layers to extract global and local features from the image. The model is observed to outperform the state-of-the-art models applied for hierarchical image classification. Trans-convo-former net is proposed in two versions namely; big and small. The model is compared to another fusion model as well. The performance of the proposed model is found to be the most optimum. An ablation study is also performed with different numbers of transconvoformers, attention layers, and epochs to find the best-performing parameters for the proposed model.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412792246",
    "type": "article"
  },
  {
    "title": "Multiscale Feature Importance-based Bit Allocation for End-to-End Feature Coding for Machines",
    "doi": "https://doi.org/10.1145/3748654",
    "publication_date": "2025-08-01",
    "publication_year": 2025,
    "authors": "Junle Liu; Yun Zhang; Zixi Guo; Xiaoxia Huang; Gangyi Jiang",
    "corresponding_authors": "",
    "abstract": "Feature Coding for Machines (FCM) aims to compress intermediate features effectively for remote intelligent analytics, which is crucial for future intelligent visual applications. In this paper, we propose a Multiscale Feature Importance-based Bit Allocation (MFIBA) for end-to-end FCM. First, we find that the importance of features for machine vision tasks varies with the scales, object size, and image instances. Based on this finding, we propose a Multiscale Feature Importance Prediction (MFIP) module to predict the importance weight for each scale of features. Secondly, we propose a task loss-rate model to establish the relationship between the task accuracy losses of using compressed features and the bitrate of encoding these features. Finally, we develop a MFIBA for end-to-end FCM, which is able to assign coding bits of multiscale features more reasonably based on their importance. Experimental results demonstrate that when combined with a retained Efficient Learned Image Compression (ELIC), the proposed MFIBA achieves an average of 38.202% bitrate savings in object detection compared to the anchor ELIC. Moreover, the proposed MFIBA achieves an average of 17.212% and 36.492% feature bitrate savings for instance segmentation and keypoint detection, respectively. When the proposed MFIBA is applied to the LIC-TCM, it achieves an average of 18.103%, 19.866% and 19.597% bit rate savings on three machine vision tasks, respectively, which validates the proposed MFIBA has good generalizability and adaptability to different machine vision tasks and FCM base codecs.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412827311",
    "type": "article"
  },
  {
    "title": "Is a Pure Transformer Effective for Separated and Online Multi-Object Tracking?",
    "doi": "https://doi.org/10.1145/3749105",
    "publication_date": "2025-08-01",
    "publication_year": 2025,
    "authors": "Chongwei Liu; Haojie Li; Zhihui Wang; Rui Xu",
    "corresponding_authors": "",
    "abstract": "Recent advances in Multi-Object Tracking (MOT) have demonstrated significant success in short-term association within the separated tracking-by-detection online paradigm. However, long-term tracking remains challenging. While graph-based approaches address this by modeling trajectories as global graphs, these methods are unsuitable for real-time applications due to their non-online nature. In this paper, we review the concept of trajectory graphs and propose a novel perspective by representing them as directed acyclic graphs. This representation can be described using frame-ordered object sequences and binary adjacency matrices. We observe that this structure naturally aligns with Transformer attention mechanisms, enabling us to model the association problem using a classic Transformer architecture. Based on this insight, we introduce a concise Pure Transformer (PuTR) to validate the effectiveness of Transformer in unifying short- and long-term tracking for separated online MOT. Extensive experiments on four diverse datasets (SportsMOT, DanceTrack, MOT17, and MOT20) demonstrate that PuTR effectively establishes a solid baseline compared to existing foundational online methods while exhibiting superior domain adaptation capabilities. Furthermore, the separated nature enables efficient training and inference, making it suitable for practical applications. Implementation code and trained models are available at https://github.com/chongweiliu/PuTR .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412827336",
    "type": "article"
  },
  {
    "title": "Language-guided Visual Tracking: Comprehensive and Effective Multimodal Information Fusion",
    "doi": "https://doi.org/10.1145/3757322",
    "publication_date": "2025-08-05",
    "publication_year": 2025,
    "authors": "Jianbo Song; Hong Zhang; Yachun Feng; Hanyang Liu; Y. F. Yang",
    "corresponding_authors": "",
    "abstract": "Current vision-language trackers often struggle to fuse multimodal information comprehensively and effectively, leading to suboptimal performance in multimodal tasks. This study introduces LGTrack, a novel language-guided visual tracking framework designed to achieve a more comprehensive and efficient fusion of vision and language information. In the encoding stage, an Enhanced Multimodal Interaction Module is proposed to achieve full multimodal fusion, and it is used to construct Early Language Multilevel-guided Multimodal Encoding, which leverages deep semantic information for early and multilevel guidance of vision encoding. In the decoding stage, a multimodal decoding based on Joint Query is proposed, utilizing global features from both vision and language modalities, guiding the efficient operation of the decoding layers. These innovations achieve a more comprehensive fusion of multimodal information. Additionally, a contrastive learning strategy is introduced to align vision-language features in the semantic space, further enhancing the fusion effectiveness. Extensive experiments on multiple benchmarks such as LaSOT, \\(\\rm{LaSOT_{ext}}\\) , TNL2K, and OTB99-Lang demonstrate that our approach outperforms existing state-of-the-art trackers.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412991841",
    "type": "article"
  },
  {
    "title": "Transformer-Based and Structure-Aware Dual-Stream Network for Low-Light Image Enhancement",
    "doi": "https://doi.org/10.1145/3758097",
    "publication_date": "2025-08-05",
    "publication_year": 2025,
    "authors": "Mingliang Zhou; Shuqi Han; Jun Luo; Xu Zhuang; Qin Mao; Zhengguo Li",
    "corresponding_authors": "",
    "abstract": "In this article, we propose an end-to-end Transformer-based and structure-aware dual-stream network for low-light image enhancement. First, we divide the dual-stream network into a main stream and a structure stream. The main stream is used to recover an enhanced image and supply structural information to the structure stream, whereas the structure stream is designed to extract structural features from the main stream to provide rich structural information for the enhancement process. Second, we devise a structure-gated Transformer to balance the extraction of global and local features through parallel multihead self-attention with convolution operations following a multilayer perceptron, thus extracting sufficient structural information from the main stream in the encoder part of the dual-stream network. Finally, we develop a cross-attention-based feature fusion module that divides different window sizes into distinct feature fusion stages to achieve multistage, multiscale and multitype feature fusion in the decoder part of the dual-stream network. The experimental results show that our method not only works effectively but also offers favorable computational and storage overheads.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412991871",
    "type": "article"
  },
  {
    "title": "Garment Recycle Training and Conditional Garment-Person Outline Attention-Guided Virtual Tryon",
    "doi": "https://doi.org/10.1145/3758098",
    "publication_date": "2025-08-06",
    "publication_year": 2025,
    "authors": "Sanhita Pathak; Vinay Kaushik; Brejesh Lall",
    "corresponding_authors": "",
    "abstract": "Virtual try-on, a significant application in computer vision, aims to seamlessly simulate the appearance of clothing on a person from a single image. We propose a diffusion-based tryon approach, solving virtual tryon as a problem of conditional image inpainting. Our method introduces GarNet and OutlineNet as two learnable Stable Diffusion ControlNet encoders conditioned on the garment and person outline images, enhancing the controllability and realism of the generated try-on. We propose a two-stage garment diffusion recycling training strategy, utilizing \\(x_{0}\\) -parameterization. We estimate the initial clean image that is conditioned on the maximum noised input and feed the same to the same diffusion model again to estimate total noise. This reduces over-fitting and makes our model more generalized. We also introduce a zero garment-outline conditioning (ZGOC) block along with a Garment-Outline Cross Attention layer to optimize garment draping and ensure global consistency in the try-on results. The ZGOC block provides control and adaptability by prioritizing garment details that are most affected by body shape, ensuring precise garment alignment with the person’s outline. Our comprehensive experiments on the VITON-HD and Dresscode dataset demonstrate that our proposed approach achieves state-of-the-art realism and controllability in VITON, marking a significant advancement in virtual fashion experiences and online shopping applications.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413019910",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on MMSys 2023 and NOSSDAV 2023",
    "doi": "https://doi.org/10.1145/3722560",
    "publication_date": "2025-07-17",
    "publication_year": 2025,
    "authors": "Carsten Griwodz; Mea Wang; Roger Zimmermann",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413078280",
    "type": "article"
  },
  {
    "title": "Intelligent Tumor Synthesis Based on Medical Image Knowledge for Liver Tumor Segmentation",
    "doi": "https://doi.org/10.1145/3749104",
    "publication_date": "2025-07-17",
    "publication_year": 2025,
    "authors": "Hefeng Ji; Jing Xiao; Jia-Wei Lin; Liu Ji-min; Haoyong Yu",
    "corresponding_authors": "",
    "abstract": "Accurate segmentation of liver tumors is crucial for their proper diagnosis and treatment. However, achieving high levels of precision typically depends on meticulous manual annotation, a process that is not only labor-intensive but also constrained by the scarcity of large-scale, real-world datasets. These datasets are indispensable for the training and validation of segmentation algorithms. Furthermore, the liver’s considerable variation in size, shape, and pathology type poses a challenge in collecting a sufficient number of image samples that adequately represent this diversity. To surmount the challenges of the time-consuming manual annotation process and the limitations in data acquisition, there is an urgent need for the development of an efficient and comprehensive tumor generation method. Some existing approaches, such as those utilizing Gaussian blurred ellipses to simulate tumors, fail to accurately reflect the biological complexity and pathological diversity of liver tumors. In this paper, we introduce an innovative tumor synthesis method LDMP (Latent Diffusion Model for Pathology) that leverages medical imaging knowledge to more accurately replicate the intricacies of liver tumor morphology and pathology. This approach aims to enhance the quality and diversity of training data, thereby improving the performance of segmentation algorithms and ultimately contributing to more precise diagnoses and treatments. The method uses deep learning techniques, particularly diffusion models, to simulate real liver CT images and incorporates the biological properties of tumors into the synthesis process to generate realistic tumor images. The quality of the synthetic images is assessed using principal component analysis (PCA) and Kullback-Leibler divergence (KL) to ensure the authenticity of the tumor’s spatial structure. Experimental results show that the proposed method can significantly improve the Dice Similarity Coefficient (DSC) of the tumor segmentation model and enable researchers to freely define the size and blur degree of the tumor, thereby creating medical images with precise annotations. In addition, we introduce a self-checking step before the output of synthetic data, which provides a new paradigm in the field of image synthesis and effectively compensates for potential errors in synthetic data. Our approach not only provides an effective solution for medical image analysis but also provides high-quality synthetic image resources for medical education and clinical practice. Codes are available at: https://github.com/jhf0721/tumor",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413078282",
    "type": "article"
  },
  {
    "title": "Enhancing Lip Dynamic Authenticity: Learning 3D Temporal Representations for Talking Head Synthesis",
    "doi": "https://doi.org/10.1145/3750048",
    "publication_date": "2025-08-08",
    "publication_year": 2025,
    "authors": "Haojie Li; Hao Chen; Yining Huang; Tianshui Chen; Shuangping Huang",
    "corresponding_authors": "",
    "abstract": "Audio-driven talking head synthesis aims to generate lifelike facial animations synchronized with audio. Current approaches primarily focus on lip motion information in 2D visual space for lip-audio synchronization and expressive lip dynamic, often neglecting 3D geometric motion representations of the lips that can more accurately capture lip movements in real-world scenarios. This oversight can result in suboptimal lip dynamic authenticity. In this work, we introduce a novel 3D Temporal Representation Learning (3D-TRL) algorithm that models 3D lip temporal information as latent representations and utilizes these representations as additional supervision to enhance dynamic authenticity. To achieve this, we leverage the geometric mesh constructed from the 3D Morphable Model (3DMM) as our 3D information of the lip and explore two self-supervised strategies to learn temporal representation in 3D geometric space. First, we propose a Reconstruction-oriented 3D-TRL algorithm that reconstructs the input to obtain motion tokens in hidden space, encapsulating content while capturing richer contextual representations of the sequence. Second, we develop a Contrastive-based 3D-TRL algorithm that utilizes contrastive learning to extract hidden 3D motion representations. This algorithm employs data augmentation strategies appropriate specifically for the 3D temporal sequences of the lips. Extensive experiments demonstrate that our approach, as a versatile and adaptable supervisory, can be integrated into various state-of-the-art network frameworks, leading to substantial enhancements in lip dynamic authenticity.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413131608",
    "type": "article"
  },
  {
    "title": "Haptic Network Protocols: A Comprehensive Review and Directions for Next-Gen Metaverse Applications",
    "doi": "https://doi.org/10.1145/3759459",
    "publication_date": "2025-08-11",
    "publication_year": 2025,
    "authors": "Mohd Faisal; Roberto Velazquez; Fedwa Laamarti; Hussein Al Osman; Abdulmotaleb El Saddik",
    "corresponding_authors": "",
    "abstract": "This paper presents a systematic review of haptic network protocols, in the context of the Metaverse. With the increasing integration of haptic technologies into applications like remote collaboration and robotic surgery, the need for reliable, low-latency data transmission has intensified. This work provides a comprehensive analysis of existing haptic protocols and frameworks, focusing on their development, implementation, and the methods employed to optimize Quality of Service (QoS) parameters such as latency, delay, packet loss, jitter, throughput, and bandwidth. By examining the strengths and limitations of these protocols in real-time applications, this paper identifies critical areas for improvement and suggests future directions, including the potential for incorporating machine learning (ML) and artificial intelligence (AI) to enable next-generation haptic communication suited for high-demand environments like the Metaverse.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413214375",
    "type": "review"
  },
  {
    "title": "Dual-Branch Point-Supervised Action Detection Network with High Quality Pseudo-Labels",
    "doi": "https://doi.org/10.1145/3762194",
    "publication_date": "2025-08-19",
    "publication_year": 2025,
    "authors": "Xu Cui; Xinyan Liu; Weigang Zhang; Laiyun Qing",
    "corresponding_authors": "",
    "abstract": "Point-supervised temporal action detection, which relies on sparse timestamp annotations, mitigates the annotation costs required by the fully supervised method. However, there is still a gap between point supervised methods and the fully supervised methods due to i) the sparsity of pseudo-labels and ii) insufficient exploitation of temporal information. We introduce a dual-branch architecture enhanced by high-quality pseudo-labels to narrow the gap. Specifically, this study proposes: 1) an attention-guided label propagation mechanism that supplements pseudo-labels by incorporating Transformer-generated attention map, 2) a Mamba-based pseudo-label generation mechanism which enhances local temporal pattern recognition to improve the reliability of pseudo-labels, and 3) a global completeness constraint to enforce temporal coherence and action integrity. Extensive experiments on three benchmark datasets validate the superiority of our approach, achieving state-of-the-art performance under point supervision settings.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413325515",
    "type": "article"
  },
  {
    "title": "Query-Based Knowledge Sharing for Open-Vocabulary Multi-Label Classification",
    "doi": "https://doi.org/10.1145/3762195",
    "publication_date": "2025-08-19",
    "publication_year": 2025,
    "authors": "Xuelin Zhu; Jian Liu; Dongqi Tang; Jiawei Ge; Weijia Liu; Bo Liu; Jiuxin Cao",
    "corresponding_authors": "",
    "abstract": "Identifying labels that are unseen during training, known as multi-label zero-shot learning, is a non-trivial task in computer vision. Recent studies have increasingly focused on utilizing vision-language pre-training (VLP) models to recognize unseen labels in an open-vocabulary manner. However, these approaches like knowledge distillation have offered only modest performance gains. The challenge of fully harnessing the potential of VLP models for effective multi-label zero-shot learning remains open. In this work, an advanced query-based knowledge sharing framework is proposed to explore the multi-modal knowledge from VLP models for open-vocabulary multi-label classification. Specifically, we introduce a set of label-agnostic query tokens that are designed to capture essential and informative visual knowledge from input images. These tokens are subsequently shared across all labels, allowing them to select pertinent one as visual clues for accurate recognition. Then, by integrating the pre-trained knowledge of VLP models, these query tokens, trained on seen labels, can be efficiently generalized to the recognition of unseen labels. Additionally, we reformulate ranking learning into a form of classification to enable the magnitude of feature vectors for prediction, which significantly benefits label recognition. Experiment results show that our framework outperforms state-of-the-art methods in multi-label zero-shot learning task by a significant margin, reaching 4.2% and 2.4% in mAP on the NUS-WIDE and Open Images datasets, respectively. Code and models are available on https://github.com/jasonseu/QKS.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413325615",
    "type": "article"
  },
  {
    "title": "RDIAS: Robust and Decentralized Image Authentication System",
    "doi": "https://doi.org/10.1145/3760260",
    "publication_date": "2025-08-19",
    "publication_year": 2025,
    "authors": "Ali Ghorbanpour; Mohammad Amin Arab; Mohamed Hefeeda",
    "corresponding_authors": "",
    "abstract": "Recent artificial intelligence (AI) tools can subtly manipulate images, eroding users’ trust in the authenticity of images they see on their displays. Current image authentication methods either detect artifacts that may result from manipulations or attach hashes of images as metadata for users to verify. The efficacy of the first approach is rapidly deteriorating with the continuous improvements in AI tools, leading to missing many serious manipulations. Hashes become invalid once images are subjected to any processing, such as re-sizing and transcoding. This makes the second approach impractical as most platforms, e.g., Facebook and X, perform several legitimate operations on images. Further, most platforms remove the metadata attached to images. We propose RDIAS, a robust and practical image authentication system. RDIAS securely embeds representative fingerprints into images without damaging their visual quality. We design these fingerprints to robustly detect malicious manipulations, e.g., adding/removing objects, while tolerating legitimate operations, e.g., image resizing and transcoding. Rigorous evaluation of RDIAS with diverse image datasets and realistic manipulations conducted by human subjects utilizing AI tools shows its high accuracy and efficiency. For example, RDIAS detects DeepFake manipulations that change facial features/expressions with an accuracy of 99%. The results also show that RDIAS preserves image quality and verifies authenticity in real-time.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413325618",
    "type": "article"
  },
  {
    "title": "Elevating Mesh Saliency in VR: Introducing a Novel Prediction Network and Dataset",
    "doi": "https://doi.org/10.1145/3761816",
    "publication_date": "2025-08-19",
    "publication_year": 2025,
    "authors": "Kaiwei Zhang; M. He; Dandan Zhu; Kun Zhu; Xiongkuo Min; Guangtao Zhai",
    "corresponding_authors": "",
    "abstract": "In computer graphics, polygon meshes stand out as a popular representation providing effective delineation of delicate textures and complex geometries. When dealing with geometric processing tasks for critical regions of the mesh, it is necessary to consider the human visual perception related to saliency. Therefore, we establish a novel mesh saliency dataset, facilitated by a more comprehensive gathering pipeline of eye-tracking from subjects observing mesh models at arbitrary viewpoints in a virtual reality space with six degrees of freedom. Additionally, we propose a mesh saliency prediction model that accurately infers visual attention density maps for complex and irregular mesh surfaces. This model integrates surface curvature and triangular face shape information from multiscale neighboring ranges as local geometric features, while also leveraging surface spatial positioning as a global feature. Our work aims to preserve critical areas and minimize visual loss in saliency-driven tasks such as mesh simplification, rendering, and texturing. We believe that our research can offer valuable insights for human-centered mesh computation applications.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413332228",
    "type": "article"
  },
  {
    "title": "Diffusion-based Perceptual Neural Video Compression with Temporal Diffusion Information Reuse",
    "doi": "https://doi.org/10.1145/3761815",
    "publication_date": "2025-08-19",
    "publication_year": 2025,
    "authors": "Wenzhuo Ma; Zhenzhong Chen",
    "corresponding_authors": "",
    "abstract": "Recently, foundational diffusion models have attracted considerable attention in image compression tasks, whereas their application to video compression remains largely unexplored. In this article, we introduce DiffVC, a diffusion-based perceptual neural video compression framework that effectively integrates foundational diffusion model with the video conditional coding paradigm. This framework uses temporal context from previously decoded frame and the reconstructed latent representation of the current frame to guide the diffusion model in generating high-quality results. To accelerate the iterative inference process of diffusion model, we propose the Temporal Diffusion Information Reuse (TDIR) strategy, which significantly enhances inference efficiency with minimal performance loss by reusing the diffusion information from previous frames. Additionally, to address the challenges posed by distortion differences across various bitrates, we propose the Quantization Parameter-based Prompting (QPP) mechanism, which utilizes quantization parameters as prompts fed into the foundational diffusion model to explicitly modulate intermediate features, thereby enabling a robust variable bitrate diffusion-based neural compression framework. Experimental results demonstrate that our proposed solution delivers excellent performance in both perception metrics and visual quality.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413332241",
    "type": "article"
  },
  {
    "title": "Metapath-enhanced Language Model Pretraining on Text-Attributed Heterogeneous Graphs",
    "doi": "https://doi.org/10.1145/3763241",
    "publication_date": "2025-08-22",
    "publication_year": 2025,
    "authors": "S. B. Chen; Quan Fang; Shengsheng Qian; Changsheng Xu",
    "corresponding_authors": "",
    "abstract": "Text-Attributed Heterogeneous Graphs (TAHGs), which combine text data with various graph relationship information linked to rich semantic entities, are ubiquitous in real-world scenarios. To extract information from TAHGs, a commonly used method is employing Pretrained Language Models (PLMs). However, existing methods are primarily designed for processing text and face challenges when dealing with graph information, leading to two main issues: incomplete context due to graph sampling and weak integration of text and graph information. In this paper, we present a new approach named Metapath-enhanced Language Model Pretraining on Text-Attributed Heterogeneous Graphs (MLMP). The proposed model starts by gathering metapath information through pre-computed neighbor aggregation using a simple mean aggregator. Subsequently, this gathered metapath information, combined with textual data, is input into a GNN-nested PLM. Here, GNN components at each layer are nested alongside the transformer blocks of PLMs during the training process. We have also developed corresponding pretraining strategies for joint pretraining. The experimental results indicate that our model efficiently captures information within TAHGs. Across benchmark datasets, it consistently outperforms current state-of-the-art methods, demonstrating remarkable effectiveness in tasks such as link prediction and node classification. Our code is available at https://github.com/chensh911/MLMP .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413418658",
    "type": "article"
  },
  {
    "title": "Corrigendum: MAINet: Modality-Aware Interaction Network for Medical Image Fusion",
    "doi": "https://doi.org/10.1145/3758084",
    "publication_date": "2025-08-25",
    "publication_year": 2025,
    "authors": "Lisi Wei; Libo Zhao; Xiaoli Zhang",
    "corresponding_authors": "",
    "abstract": "This is a corrigendum for the article “MAINet: Modality-Aware Interaction Network for Medical Image Fusion” published in ACM Trans. Multimedia Comput. Commun. Appl. 21(6): 166:1–166:23 (2025).",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413502742",
    "type": "erratum"
  },
  {
    "title": "Immersive Ink-and-wash Landscape Design in Multimedia for Art Therapy",
    "doi": "https://doi.org/10.1145/3762998",
    "publication_date": "2025-08-25",
    "publication_year": 2025,
    "authors": "Jing Li; Jun Guo; M. Shamim Hossain; Yu Ning",
    "corresponding_authors": "",
    "abstract": "This study integrates traditional Chinese ink-and-wash painting with multimedia technology to create immersive ink-and-wash landscape animations. The study explores the psychological intervention effects of these animations on individuals experiencing anxiety and depression. Based on an interdisciplinary approach, a qualitative research methodology is employed, examining the fundamental theories and key issues of art therapy and patient psychology within the context of art therapy. A mixed-methods design was adopted, utilizing a specialized hospital as the implementation site for case studies. In-depth interviews were conducted with 15 patients, and three-dimensional virtual reality (VR) ink-and-wash animations were developed based on their needs. A survey questionnaire was administered to 50 participants, and the results demonstrate that the application of immersive ink-and-wash animations in art therapy exhibits positive therapeutic effects. Among the 50 participants, anxiety scores decreased from 10.26 to 7.58 (a 26.1% reduction) after the intervention, and depression scores decreased from 11.14 to 8.36 (a 25.0% reduction), significantly improving their emotional states. Additionally, patients generally provided positive feedback on the natural imagery, traditional elements, and soft color tones within the ink-and-wash animations. Notably, middle-aged and elderly groups showed the most significant effects in terms of emotional regulation and psychological comfort. This study demonstrates that immersive art scenes, which merge traditional Chinese ink-and-wash aesthetics with digital multimedia technology, can effectively alleviate anxiety and depression symptoms. These scenes possess high emotional regulation value and cross-population adaptability, offering an innovative pathway for digital art therapy within an Eastern cultural context.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413506625",
    "type": "article"
  },
  {
    "title": "MMHCL: Multi-Modal Hypergraph Contrastive Learning for Recommendation",
    "doi": "https://doi.org/10.1145/3762665",
    "publication_date": "2025-08-25",
    "publication_year": 2025,
    "authors": "Xu Guo; Tong Zhang; Fuyun Wang; Xudong Wang; Xiaoya Zhang; Xin Liu; Zhen Cui",
    "corresponding_authors": "",
    "abstract": "The burgeoning presence of multimodal content-sharing platforms propels the development of personalized recommender systems. Previous works usually suffer from data sparsity and cold-start problems, and may fail to adequately explore semantic user-product associations from multimodal data. To address these issues, we propose a novel Multi-Modal Hypergraph Contrastive Learning (MMHCL) framework for user recommendation. For a comprehensive information exploration from user-product relations, we construct two hypergraphs, i.e. a user-to-user (u2u) hypergraph and an item-to-item (i2i) hypergraph, to mine shared preferences among users and intricate multimodal semantic resemblance among items, respectively. This process yields denser second-order semantics that are fused with first-order user-item interaction as complementary to alleviate the data sparsity issue. Then, we design a contrastive feature enhancement paradigm by applying synergistic contrastive learning. By maximizing/minimizing the mutual information between second-order (e.g. shared preference pattern for users) and first-order (information of selected items for users) embeddings of the same/different users and items, the feature distinguishability can be effectively enhanced. Compared with using sparse primary user-item interaction only, our MMHCL obtains denser second-order hypergraphs and excavates more abundant shared attributes to explore the user-product associations, which to a certain extent alleviates the problems of data sparsity and cold-start. Extensive experiments have comprehensively demonstrated the effectiveness of our method. Our code is publicly available at https://github.com/Xu107/MMHCL .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413506857",
    "type": "article"
  },
  {
    "title": "ADTC: Adaptive Dual-stage Tree Construction for Point-Supervised Video Moment Retrieval",
    "doi": "https://doi.org/10.1145/3744651",
    "publication_date": "2025-08-26",
    "publication_year": 2025,
    "authors": "Dikai Fang; Huahu Xu; Honghao Gao; Yuzhe Huang",
    "corresponding_authors": "",
    "abstract": "Video moment retrieval (VMR) is a key cross-modal task with broad theoretical and practical applications. While fully supervised methods deliver strong performance, they are constrained by the high cost of temporal boundary annotations. Weakly supervised methods mitigate this issue but suffer from limited accuracy due to coarse supervision. Recently, point-supervised approaches that leverage single-frame annotations as a cost-effective alternative have emerged as a promising paradigm. However, these methods often fail to leverage annotated frames for cross-modal semantic alignment. Additionally, they overlook global video structures and hierarchical segment relationships, leading to suboptimal retrieval accuracy under sparse supervision. To address these challenges, we propose the adaptive dual-stage tree construction (ADTC) model, a novel framework designed specifically for point-supervised VMR. First, the model introduces a dual-stage hypothesis tree architecture that seamlessly integrates local and global trees, enabling the effective modeling of semantic relationships across multiple temporal scales. Second, it incorporates frame clustering and scene segmentation to extract the structural characteristics of video content, providing a foundation for comprehensive node relevance evaluation and an adaptive merging control strategy to optimize tree construction. Third, a hierarchical adaptive tree pruning strategy is implemented, combined with a novel proposal selection mechanism for distinguishing between positive and negative samples. These components are jointly optimized through a multilevel loss function, enabling enhanced semantic alignment and retrieval performance. The experimental results demonstrate that ADTC achieves state-of-the-art performance on the Charades-STA and ActivityNet Captions datasets with the point-supervised setting. On Charades-STA, it reaches an R@1 values of 50.28% at IoU=0.5 and 34.79% at IoU=0.7, surpassing those of other point-supervised methods. On ActivityNet Captions, ADTC achieves R@1 values of 65.02% at IoU=0.3 and 46.13% at IoU=0.5, setting new benchmarks. Notably, it outperforms fully supervised methods while significantly reducing annotation costs. Ablation studies confirm the effectiveness of each model component.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413640700",
    "type": "article"
  },
  {
    "title": "Learning Robust Representations via Bidirectional Transition for Visual Reinforcement Learning",
    "doi": "https://doi.org/10.1145/3765517",
    "publication_date": "2025-09-02",
    "publication_year": 2025,
    "authors": "Xiaobo Hu; Youfang Lin; Jian Wang; Yue Liu; Shuo Wang; Hehe Fan; Kai Lv",
    "corresponding_authors": "",
    "abstract": "Visual reinforcement learning has exhibited efficacy in solving control tasks characterized by high-dimensional observations. However, a central challenge persists in deriving dependable and generalizable representations from vision-based observations. Inspired by the human thought process, when the visual representation extracted from the observation can predict the future and trace history, the representation is reliable and accurate in comprehending the environmental state. Based on this concept, we introduce a B idirectional T ransition (BT) framework for representation learning. This framework employs the bidirectional prediction of both forward and backward environmental transitions as auxiliary tasks to extract reliable representations. Additionally, we introduce an inverse dynamic model to predict the actions causing environmental state transitions, thereby learning the task relevance of state representations. Our method demonstrates competitive generalization performance and sample efficiency in two settings in the DeepMind Control suite. Moreover, we utilize the robotic manipulation simulator, autonomous driving simulator CARLA, and visual navigation simulator Habitat to demonstrate the wide applicability of our method. The results indicate that BT offers more stable and reliable representations and exhibits robust generalization performance for visual reinforcement learning tasks.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413926957",
    "type": "article"
  },
  {
    "title": "CSRef: Contrastive Semantic Alignment for Speech Referring Expression Comprehension",
    "doi": "https://doi.org/10.1145/3765520",
    "publication_date": "2025-09-03",
    "publication_year": 2025,
    "authors": "Lihong Huang; Sheng-hua Zhong; Yan Liu",
    "corresponding_authors": "",
    "abstract": "Referring Expression Comprehension (REC) aims to locate a target object in an image based on a natural language description. While existing REC methods primarily rely on textual input, spoken language remains an underexplored modality, despite its inherent naturalness and accessibility. To bridge this gap, we introduce a novel task, Speech Referring Expression Comprehension (SREC), which enables object localization using spoken language as input. To advance this task, we propose a new method, CSRef, alongside datasets and evaluation criteria tailored to SREC. CSRef integrates a global Contrastive Semantic Alignment (CSA) mechanism with the SREC framework, enabling direct extraction of semantic information from speech for visual grounding. This approach streamlines the semantic processing pipeline and reduces complexity compared to conventional methods that depend on automatic speech recognition followed by textual REC. We conduct extensive experiments on three widely used REC datasets extended with synthetic speech, as well as three constructed face-centric SREC datasets. The results demonstrate that CSRef outperforms transcription-based baselines in both efficiency and accuracy. Furthermore, we evaluate CSRef in a downstream application, language-guided face blurring, and compare it with the MLLM-Guided Image Editing (MGIE) approach. CSRef achieves superior performance, both in the precision of region modification and in the overall quality of the face-blurring effect. These findings establish CSRef as an effective and scalable solution for SREC, representing a promising step toward speech-based visual grounding in real-world human-computer interaction scenarios. The code is publicly available at https://github.com/macrorise-lh/CSRef .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413943358",
    "type": "article"
  },
  {
    "title": "Dual-Branch Cross-Layer Information Flow Network for Camouflaged Object Detection in Complex Scenes",
    "doi": "https://doi.org/10.1145/3764866",
    "publication_date": "2025-09-02",
    "publication_year": 2025,
    "authors": "Yuan Cao; Dong Wang",
    "corresponding_authors": "",
    "abstract": "Camouflaged object detection (COD) aims to accurately detect objects that blend in with their backgrounds. While deep learning-based methods have significantly improved detection accuracy, the persistent similarity in chromatic and textural characteristics between camouflaged objects and their environments continues to pose substantial challenges, particularly in complex scenarios. Existing COD methods still encounter two primary challenges: (1) the difficulty in effectively fusing detailed features with semantic representations, leading to suboptimal balance between local and global features in the fused features; (2) progressive information dilution in conventional top-down decoder architectures, resulting in imprecise boundary localization. To address these dual challenges of multi-scale feature enhancement and semantic information dilution, we propose a D ual-Branch C ross-Layer I nformation F low Network (DCIFNet) for camouflaged object detection in complex scenes. Specifically, our framework integrates a hybrid CNN-ViT architecture for parallel extraction of localized patterns and global contextual features. Subsequently, a global local fusion (GLF) module is employed to iteratively and adaptively fuse the global and local features at each stage using gating weights. Additionally, a novel Dense Connection Decoder (DCD) module is designed to mitigate the information dilution problem associated with traditional top-down decoders. Extensive experiments show that DCIFNet outperforms other mainstream COD models in most performance metrics on four widely used benchmark datasets. On the COD10K dataset, compared with the latest SOTA work of PRNet, DCIFNet improves the metrics of \\({S_{\\alpha}}\\) , \\({E_{\\phi}}\\) , and \\(F_{\\beta}^{w}\\) by 0.11 \\(\\%\\) , 0.11 \\(\\%\\) , and 1.38 \\(\\%\\) , respectively, demonstrating a significant performance enhancement. The code is available https://github.com/WateverOk/DCIFNet .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413944980",
    "type": "article"
  },
  {
    "title": "Efficient Client Selection for Asynchronous Federated Learning for Adaptive Bitrate Streaming",
    "doi": "https://doi.org/10.1145/3765759",
    "publication_date": "2025-09-04",
    "publication_year": 2025,
    "authors": "Yi Jie Wong; Mau‐Luen Tham; Ban-Hoe Kwan; Yoong Choon Chang; Anissa Mokraoui; Feng Ke",
    "corresponding_authors": "",
    "abstract": "Recently, deep reinforcement learning (DRL) has been applied to enhance the quality-of-experience (QoE) of adaptive bitrate streaming (ABR) by adjusting the video quality level in real time based on instantaneous network conditions. To build a state-of-the-art DRL-based ABR (DRLABR) algorithm, it must learn from the clients’ actual network and video streaming behavior. However, collecting such data directly from clients introduces several challenges, including privacy concerns, high bandwidth consumption, and the straggler effect—where poor network conditions of certain clients delay the training process, as DRLABR’s performance is highly dependent on network interactions. To overcome these limitations, we propose a decentralized training approach for DRLABR using a Federated Learning (FL) framework. Instead of gathering raw data, clients train their local DRLABR models independently and send only model updates to the central server. To address the straggler issue, we propose to desynchronize the FL update rules, allowing clients to contribute their model updates at their own pace, regardless of varying network conditions. In addition, we design a DRL-based client selection mechanism to prevent oversampling of high-bandwidth clients, which could lead to model divergence, thereby ensuring balanced participation and improving the overall training efficiency. We validate our approach through a comprehensive simulation encompassing diverse video content and real-world network traces, simulating a wide range of streaming activities. Our results show that the proposed framework significantly outperforms conventional FedAvg and FedAsync methods, achieving the highest average QoE score of 2.02 and reducing the total training latency by 21.26%.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413982344",
    "type": "article"
  },
  {
    "title": "Joint Spatiotemporal Adversarial Attacks on Video Transformer Models Through XAI-guided Perturbation",
    "doi": "https://doi.org/10.1145/3766071",
    "publication_date": "2025-09-05",
    "publication_year": 2025,
    "authors": "Zerui Wang; Yan Liu",
    "corresponding_authors": "",
    "abstract": "The widespread deployment of video transformer models in action recognition systems necessitates a comprehensive understanding of their vulnerability to adversarial attacks. Unlike traditional CNN-based video models, transformers process spatiotemporal dependencies through self-attention mechanisms, creating a different vulnerability profile to adversarial attacks. This study presents an investigation of adversarial robustness in video transformers. We develop a novel joint spatiotemporal attack method that precisely targets the attention mechanisms of video transformers. By simultaneously perturbing both spatial and temporal features, our method achieves a 76.30% in ASR on the Kinetics-400 dataset, outperforming frame-wise attacks and state-of-the-art query-based attacks. To interpret the mechanisms underlying these attacks, we introduce quantitative metrics based on Explainable AI (XAI) analysis. Spatial analysis reveals systematic disruption of attention patterns, with adversarial examples showing median SSIM scores of 0.353. Temporal correlation analysis also demonstrates severe degradation in attention coherence across frame sequences. Through experiments comparing previous attack methods, including common corruptions benchmark, frame-wise attacks, sparse attacks, and recent V-BAD attacks, we demonstrate that our proposed method is more effective in transformer-based video models. This study further examines the adversarial training strategy against the selected attacks. To promote reproducibility and facilitate future research, we provide our methods and analysis tools through a public GitHub repository. These findings underscore the effectiveness of jointly considering spatial and temporal dimensions when developing adversarial attack strategies and defense mechanisms for video AI models.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414014113",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on Deep Multimodal Generation and Retrieval",
    "doi": "https://doi.org/10.1145/3762666",
    "publication_date": "2025-09-05",
    "publication_year": 2025,
    "authors": "Hao Fei; Wei Ji; Yinwei Wei; Zhedong Zheng; Jialie Shen; Alan Hanjalić; Roger Zimmermann",
    "corresponding_authors": "",
    "abstract": "This editorial introduces the Special Issue on Deep Multimodal Generation and Retrieval , hosted by the ACM Transactions on Multimedia Computing, Communications, and Applications in 2024. Information generation (IG) and information retrieval (IR) are two key representative approaches of information acquisition, i.e. , producing content either via generation or via retrieval. While traditional IG and IR have achieved great success within the scope of languages, the under-utilization of varied data sources in different modalities ( i.e. , text, images, audio, and video) would hinder IG and IR techniques from giving the full advances and thus limit the applications in the real world. Knowing the fact that our world is replete with multimedia information, this special issue encourages the development of deep multimodal learning for the research of IG and IR. Benefiting from a variety of data types and modalities, some of the latest prevailing techniques are extensively invented to show great facilitation in multimodal IG and IR learning. With this special issue, we encourage explorations in Deep Multimodal Generation and Retrieval, providing a platform for researchers to share insights and advancements in this rapidly evolving domain. Each article provides novel insights into areas and challenges such as Multimodal Semantics Understanding, Generative Models for Vision Synthesis, Multimodal Information Retrieval, Explainable and Reliable Multimodal Learning . We summarize the main contributions of the included works and emphasize their role in advancing the field of multimodal generation or via retrieval. Finally, we discuss ongoing challenges and future opportunities in this rapidly evolving domain, particularly in the context of large foundational models.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414014890",
    "type": "article"
  },
  {
    "title": "CamWave-Emo: Advanced Contactless Emotion Detection from Facial Expression Analysis Using Camera Labeling and mm-wave Sensing",
    "doi": "https://doi.org/10.1145/3767743",
    "publication_date": "2025-09-16",
    "publication_year": 2025,
    "authors": "Naveed Imran; Jian Zhang; Jehad Ali; Sana Hameed; Mohammed J. F. Alenazi; Houbing Song",
    "corresponding_authors": "",
    "abstract": "This study introduces a novel dual-modality emotion recognition system that combines mm-wave radar with camera-based labeling to provide accurate and privacy-preserving emotion detection. The mm-wave radar captures subtle physiological signals through micro-Doppler and time-frequency characteristics, while the camera assists in labeling facial expressions. The radar data is transformed into spectrograms, which are then fused with camera datasets to train deep learning models, employing convolutional layers for feature extraction and recurrent layers for temporal pattern recognition. Performance evaluation, conducted across a wide range of real-world occlusion and interference scenarios, shows that the system achieves 98.5% accuracy, 0.98 F1-score, and 0.98 recall, significantly outperforming traditional systems. Other experiments, including those for multi-person interference, hand-held paper occlusion, and industrial goggles, achieved accuracy rates of 92%, 91%, and 92%, respectively. The system’s latency for real-time processing is 60.5 ms on edge devices like the NVIDIA Jetson, making it suitable for applications requiring low-latency emotion recognition. Additionally, radar parameter optimization, such as adjusting the ADC sample rate and chirp size, has been shown to improve classification accuracy. These findings highlight the system’s robustness and adaptability to varying environmental conditions and its potential use in privacy-sensitive applications, including healthcare, security, and interactive media. Future work will explore radar-only systems, further reducing dependence on visual data, and investigate more advanced deep learning techniques to improve performance, scalability, and real-time deployment.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414241593",
    "type": "article"
  },
  {
    "title": "Early Traffic Accident Anticipation via Feature Consistency Representation and Soft Label Regression",
    "doi": "https://doi.org/10.1145/3767737",
    "publication_date": "2025-09-16",
    "publication_year": 2025,
    "authors": "Yuanhong Zhong; Ge Yan; Ran Zhu; Ping Gan; Xuerui Shen",
    "corresponding_authors": "",
    "abstract": "Early traffic accident prediction using dashcam videos plays a crucial role in enhancing the safety of intelligent vehicles. Accurately predicting accidents in advance can significantly reduce traffic accidents and improve overall road safety. However, despite extensive research efforts to capture more visual information by employing different feature extraction methods within the same frame, the consistency between features within the same frame and the discrepancy between features across different frames have not been sufficiently emphasized. To address this critical issue, we introduce contrastive learning into the field of accident prediction and propose a novel feature fusion module for the deep integration of diverse features. Our method treats features from the same frame as positive pairs, neighboring frames as sub-positive pairs due to their high correlation, and features from temporally distant frames as negative pairs. This approach effectively strengthens the representation capability of the model, thereby improving overall predictive performance. Additionally, we redefine the accident prediction task by converting it into an anomaly score regression problem using soft labels. This redefinition allows the model to better quantify the likelihood of an accident, offering a more nuanced and accurate prediction. We evaluate our method comprehensively on two publicly available Dashcam Accident Dataset (DAD) and Car Crash Dataset(CCD) datasets to assess its performance. The results demonstrate that our method outperforms state-of-the-art accident prediction approaches, highlighting its potential for practical applications. Code will be available at https://github.com/yangugu/TAP_CC.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414241947",
    "type": "article"
  },
  {
    "title": "Dual Interest Learning With Context-Aware Adaptive Interaction for Social Recommendation",
    "doi": "https://doi.org/10.1145/3767747",
    "publication_date": "2025-09-16",
    "publication_year": 2025,
    "authors": "Meng Jian; Ruoxi Li; Xiaoyan Gao; Liqiang Wei; Lifang Wu",
    "corresponding_authors": "",
    "abstract": "Social recommendation utilizes social relations to extract auxiliary collaborative signals, effectively mitigating data sparsity issues. However, existing approaches predominantly focus on static influence from social friends while neglecting two critical aspects: the dynamic contextual patterns in user behaviors and the potential of collaborative users. To address these limitations and further alleviate data sparsity, we propose a context-aware dual graph attention network (CDGA) that simultaneously captures users’ static and dynamic interests through social relations and interaction records. The proposed CDGA model introduces a dynamic activation mechanism to simulate contextual influences, generating dynamic embeddings for users and items. Furthermore, we develop an adaptive fusion mechanism that integrates interaction channels across static and dynamic embeddings for interaction prediction. Extensive experiments on three benchmark datasets demonstrate that CDGA consistently outperforms state-of-the-art social recommendation methods, confirming its effectiveness.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414245739",
    "type": "article"
  },
  {
    "title": "Bi-Level Routing Attention and Enhanced Spatial-Temporal Inconsistency Learning for Deep VFI Video Detection",
    "doi": "https://doi.org/10.1145/3767749",
    "publication_date": "2025-09-16",
    "publication_year": 2025,
    "authors": "Xiangling Ding; Jia Tang; Yunyi Li; Gaobo Yang; Yubo Lang",
    "corresponding_authors": "",
    "abstract": "With the maturation of Deep Learning-based Video Frame Interpolation (Deep VFI), the left spatial-temporal inconsistency in the synthesis process is greatly improved, which poses a challenge to the current VFI detector. This paper presents a dual-stream identification network based on bi-level routing attention and enhanced spatial-temporal inconsistency learning (BRA-ST) to address this challenge. Specifically, the spatial inconsistencies in Deep VFI are mainly reflected in their motion regions and moving object edges; thus, the high-pass filter is introduced to enhance them, facilitating the three-stage pyramid structure of bi-former blocks with bi-level routing attention in the frame-level stream to learn. To fully exploit the temporal inconsistencies in the Deep VFI video, the time-difference module in the time-level stream is superimposed with the ConvGRU to extract the temporally dependent features of continuous multiple frames. Additionally, the middle layer of the two streams interacts and aggregates with the channel attention, and then their last layer adaptively merges from a whole and part perspective for the ultimate frame prediction. Finally, the experimental findings on a constructed dataset by the five most advanced Deep VFI methods indicate that the proposed BRA-ST achieved \\(F_{\\text{ 1Score }}\\) of 99.73%, which is superior to the existing Deep VFI detectors, and further verify that the resolution of BRA-ST for different Deep VFI methods reached 78.55%. Our source codes and dataset are available at https://pan.baidu.com/jiatang625/BRA-ST",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414245741",
    "type": "article"
  },
  {
    "title": "Scalable MDC-Based WebRTC Streaming for One-to-Many Volumetric Video Conferencing",
    "doi": "https://doi.org/10.1145/3768314",
    "publication_date": "2025-09-17",
    "publication_year": 2025,
    "authors": "Matthias De Fré; Jeroen van der Hooft; Tim Wauters; Filip De Turck",
    "corresponding_authors": "",
    "abstract": "Video consumption has become central to modern life, with users seeking more immersive experiences such as virtual conferencing or concerts within virtual reality (VR). While 360° video offers rotational movement, it lacks true positional freedom. Fully immersive formats like light fields and volumetric video enable six degrees-of-freedom (6DoF), allowing both types of freedom. However, their high bandwidth and computational demands make them impractical for low-latency applications. Efforts to address these issues through compression and quality adaptation have improved quality of experience (QoE), but real-time interaction remains limited because of latency. To solve this, we introduce a novel, open-source one-tomany streaming architecture using point cloud-based volumetric video. By compressing point clouds with the Draco codec and transmitting via web real-time communication (WebRTC), we achieve low-latency 6DoF streaming. Content is adapted by employing a multiple description coding (MDC) strategy which combines sampled point cloud descriptions using the estimated bandwidth returned by the Google congestion control (GCC) algorithm. MDC encoding scales more easily to a larger number of users compared to individual encoding. Our proposed solution achieves similar real-time latency for both three and eight clients (163 ms and 166 ms), which is 9% and 19% lower compared to individual encoding. The MDC-based approach, using three workers, achieves similar visual quality compared to a per client encoding solution using five worker threads, and increased quality when the number of clients is greater than 20. Additionally, when compared to an approach with five fixed quality levels, our MDC-based approach scores 13% better in terms of latency, while achieving similar quality.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414277092",
    "type": "article"
  },
  {
    "title": "Artificial Intelligence for Virtual Reality: State of the Art, Challenges, and Future Perspectives",
    "doi": "https://doi.org/10.1145/3769090",
    "publication_date": "2025-09-23",
    "publication_year": 2025,
    "authors": "Ya‐Hui Wang; Mohsen Guizani; M. Shamim Hossain",
    "corresponding_authors": "",
    "abstract": "Artificial Intelligence (AI) has wide applications in virtual reality (VR), especially in analyzing its implementation progress, existing limitations, and future development paths. This study finds that AI's main contributions to VR are in four key areas: creating intelligent virtual characters, advancing education and training, providing medical assistance, and generating dynamic scenes. This interdisciplinary convergence also brings significant opportunities, particularly in sectors such as education, healthcare, gaming, and corporate training. Additionally, the study discusses technical challenges related to computational costs, real-time feedback, user privacy, and algorithmic ethics. Critical challenges persist in data processing bottlenecks, privacy protection issues, and user adaptation. While AI enhances VR's intelligence and interactivity, breakthroughs are still needed in cross-modal integration, privacy, security, and user experience. Future advancements in deep learning and reinforcement learning may unlock unlimited potential for AI-driven VR in personalized adaptation and immersive interaction. Therefore, this study provides a comprehensive analysis of AI–VR integration, providing valuable insights for academic research and technological development.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414425236",
    "type": "article"
  },
  {
    "title": "CtxMIM: Context-Enhanced Masked Image Modeling for Remote Sensing Image Understanding",
    "doi": "https://doi.org/10.1145/3769084",
    "publication_date": "2025-09-23",
    "publication_year": 2025,
    "authors": "Mingming Zhang; Qingjie Liu; Yunhong Wang",
    "corresponding_authors": "",
    "abstract": "Learning representations through self-supervision on unlabeled data has proven highly effective for understanding diverse images. However, remote sensing images often have complex and densely populated scenes with multiple land objects and no clear foreground objects. This intrinsic property generates high object density, resulting in false positive pairs or missing contextual information in self-supervised learning. To address these problems, we propose a context-enhanced masked image modeling method (CtxMIM), a simple yet efficient MIM-based self-supervised learning for remote sensing image understanding. CtxMIM formulates original image patches as a reconstructive template and employs a Siamese framework to operate on two sets of image patches. A context-enhanced generative branch is introduced to provide contextual information through context consistency constraints in the reconstruction. With the simple and elegant design, CtxMIM encourages the pretraining model to learn object-level or pixel-level features on a large-scale dataset without specific temporal or geographical constraints. Finally, extensive experiments show that features learned by CtxMIM outperform fully supervised and state-of-the-art self-supervised learning methods on various downstream tasks, including land cover classification, semantic segmentation, object detection, and instance segmentation. These results demonstrate that CtxMIM learns impressive remote sensing representations with high generalization and transferability.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414425396",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on Explainable AI (XAI) and Digital Twins (DT) for Smart Healthcare",
    "doi": "https://doi.org/10.1145/3767168",
    "publication_date": "2025-09-24",
    "publication_year": 2025,
    "authors": "M. Shamim Hossain; Diana P. Tobón; Md. Abdur Rahman; Abdulsalam Yassine",
    "corresponding_authors": "",
    "abstract": "The special issue on Explainable Artificial Intelligence (XAI) provides a representative snapshot of the state of the art in the 2020-2021 time-frame and highlights future research directions. The scope of the special issue is intentionally broad,...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414452277",
    "type": "article"
  },
  {
    "title": "Arbitrary Large-Scale Scene Reconstruction without Annotated Block Partitions",
    "doi": "https://doi.org/10.1145/3768159",
    "publication_date": "2025-09-24",
    "publication_year": 2025,
    "authors": "X. Wang; Zhiqiang Tian; Lin Bie; Siqi Li; Dadi Guo; Shaoyi Du; Yue Gao",
    "corresponding_authors": "",
    "abstract": "Large-scale scene reconstruction is a challenging problem. As different parts of the scene could be visible from different collected image frames, previous works manually use distance or geography to decompose the scene into parts and reconstruct each part of the scene separately. However, such manual decomposition is a laborious and time-consuming task when applied to large-scale scene reconstruction in real-world applications. To address this, we propose VisibleNeRF automatically reconstructs large-scale scenes by decomposing scenes into parts based on the part visibility. More specifically, we propose a visibility judgment strategy to decompose the scenes into visible and invisible parts. Then we reconstruct the visible part with the corresponding collected images and continue to decompose the rest of the invisible parts with the proposed visibility judgment strategy. New NeRF modules are re-established for the decomposed invisible parts until the entire scene is reconstructed. To the best of our knowledge, we are the first to propose an online reconstruction of large-scale scenes without manual decomposition. Experimental results on three datasets show that our method successfully reconstructs large-scale scenes in a fully automatic manner. Besides, in the widely used Mission Bay dataset, our model outperforms other state-of-the-art methods by a large margin.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414452283",
    "type": "article"
  },
  {
    "title": "Empowering Personalized and Privacy-Preserving for Image Super-Resolution with Decentralized Collaboration",
    "doi": "https://doi.org/10.1145/3769431",
    "publication_date": "2025-09-26",
    "publication_year": 2025,
    "authors": "Yue Yang; Ru Zhang; Fan Ma",
    "corresponding_authors": "",
    "abstract": "Reconstructing a high-resolution (HR) image from its degraded low-resolution (LR) counterpart facilitates humans to explore more details from the high-quality version. Current image super-resolution (SR) approaches mainly focus on mining richer context through carefully-designed network architectures and optimization objectives within a centralized environment. However, these approaches lack effective data protection and generate unsatisfactory results in practical scenario, due to the degradation gap between public training data (e.g., bicubic downsampling) and real-world personal data captured by different imaging devices. To address these issues, we introduce a novel collaborative learning paradigm, SRGuardian, comprising the personalized SR model for encoding individual-specific imaging information and a shared model for learning general image patterns across individuals. Specifically, SRGuardian inherits a distributed training framework that involves two stages: degradation-aware individual training and personalized parameter fusion. During individual training, we employ a customized degradation generator to encode personalized imaging priors for accurate reconstruction, coupled with a perceptual contrastive regularizer that aligns image representations between the current model and its historical counterparts, thereby enhancing semantic consistency. In the model fusion stage, parameters from personalized models are amalgamated via a hybrid strategy to construct a shared model, where layers responsible for general representation are shared among individuals and layers capturing personal details are retained locally. The integration of these two stages empowers our SRGuardian to achieve both privacy preservation and high-fidelity image rendering. Extensive experiments conducted on two real-world image SR datasets (i.e., RealSR and DRealSR) with various backbones demonstrate that our method significantly surpasses existing approaches in terms of image fidelity and quantitative metrics.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414544890",
    "type": "article"
  },
  {
    "title": "Learning Prediction-aware Prior in Transformer Network for Accurate Spatio-Temporal Video Grounding",
    "doi": "https://doi.org/10.1145/3769088",
    "publication_date": "2025-09-29",
    "publication_year": 2025,
    "authors": "Xin Wang; Tong Zhang; Yongshun Gong; Jialin Gao; Yanyu Xu; Jun Qian; Xiushan Nie; Lizhen Cui; Chengqi Zhang",
    "corresponding_authors": "",
    "abstract": "Spatio-temporal video grounding (STVG) aims to precisely locate a spatio-temporal tube in an untrimmed video corresponding to a given language description. Many existing methods decouple spatial and temporal grounding as separate tasks, missing the strong interdependencies between the two, which are crucial for accurately aligning spatial regions (such as objects) with their motion over time. Thus, to enhance spatio-temporal associations, we introduce a new Prior-Driven Transformer Network (PDTNet) with predicted temporal boundaries as priors to guide object bounding boxes for improved spatial grounding over time. Firstly, PDTNet employs a temporal prior, termed reference query, to enhance discriminability between language-related and language-irrelevant visual content, improving temporal boundary localization. Further, the context within predicted temporal boundaries serves as another prior knowledge to modulate spatial features. We also introduce a prediction-aware Gaussian prior to precise object localization. This ensures consistent tube construction and accurate object localization. Extensive experiments on STVG benchmarks validate the effectiveness of PDTNet. Code is available at https://github.com/tongzhang111/PDTNet .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414607018",
    "type": "article"
  },
  {
    "title": "How to Understand Named Entities: Using Commonsense for News Captioning",
    "doi": "https://doi.org/10.1145/3769085",
    "publication_date": "2025-10-03",
    "publication_year": 2025,
    "authors": "Shenyuan Zhang; Ning Xu; Yanhui Wang; Tongle Ma; Wu Liu; Jinlin Guo; Chao Xue; An-An Liu",
    "corresponding_authors": "",
    "abstract": "News captioning aims to describe an image with its news article body as input. It greatly relies on a set of detected named entities, including real-world people, organizations, and places. This paper exploits commonsense knowledge to understand named entities for news captioning. By “understand”, we mean correlating the news content with commonsense in the wild, which helps an agent to 1) distinguish semantically similar named entities and 2) describe named entities using words outside of training corpora. Our approach consists of three modules: (a) Filter Module aims to clarify the commonsense concerning a named entity from two aspects: what does it mean? and what is it related to? , which divide the commonsense into explanatory knowledge and relevant knowledge , respectively. (b) Distinguish Module aggregates explanatory knowledge from node-degree , dependency , and distinguish three aspects to distinguish semantically similar named entities. (c) Enrich Module attaches relevant knowledge to named entities to enrich the entity description by commonsense information (e.g., identity and social position). Finally, all of information is integrated into the large multimodal model to generate the news caption. Extensive experiments on two challenging datasets (i.e., GoodNews and NYTimes) demonstrate the superiority of our method. Ablation studies and visualization further validate its effectiveness in understanding named entities.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414786926",
    "type": "article"
  },
  {
    "title": "Correlation-guided Masked Autoencoder with Multimodal Contrastive Interaction on Point Clouds",
    "doi": "https://doi.org/10.1145/3770579",
    "publication_date": "2025-10-03",
    "publication_year": 2025,
    "authors": "Peng Ren; Xiaoheng Li; Yunfeng Bai; Jinyuan Jia",
    "corresponding_authors": "",
    "abstract": "Self-supervised learning has shown remarkable effectiveness in 3D point cloud understanding. Existing masked autoencoders or contrastive learning paradigms can acquire robust feature representations from unlabeled data. Specifically, masked autoencoders extract features of local patches and directly map them to latent global vectors, suffering from insufficient semantic extraction and latent interaction. Contrastive paradigms capture global correspondence via restricted constraints and are limited by the absence of local detail modeling. This prompts us to integrate the synergistic local and global advantages of two effective components and extend them further for multimodal dependencies. In this paper, we propose a unified correlation-guided masked autoencoder with multimodal contrastive interaction (CorMAC) learning for self-supervised point cloud analysis. We first design the spherical adaptive embedding backbone to learn local underlying semantics and improve the masked mechanism for patch autoencoding and reconstruction. Then, we expand multimodal contrastive correspondence and constraints to leverage the potential alignments across point cloud and auxiliary image modalities. In addition, we devise adaptable loss functions to jointly optimize masked recover and contrast errors, aiming to enhance latent feature learning. Extensive experiments show that our method achieves superior performance than other self-supervised ones on various datasets and exhibits better generalization capability across diverse downstream tasks.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414786940",
    "type": "article"
  },
  {
    "title": "GAT-NeRF: Geometry-Aware-Transformer Enhanced Neural Radiance Fields for High-Fidelity 4D Facial Avatars",
    "doi": "https://doi.org/10.1145/3770086",
    "publication_date": "2025-10-03",
    "publication_year": 2025,
    "authors": "Zhe Chang; Haodong Jin; Yan Song; Ying Sun; Hui Yu",
    "corresponding_authors": "",
    "abstract": "High-fidelity 4D dynamic facial avatar reconstruction from monocular video is a critical yet challenging task, driven by increasing demands for immersive virtual human applications. While Neural Radiance Fields (NeRF) have advanced scene representation, their capacity to capture high-frequency facial details, such as dynamic wrinkles and subtle textures from information-constrained monocular streams, requires significant enhancement. To tackle this challenge, we propose a novel hybrid neural radiance field framework, called Geometry-Aware-Transformer Enhanced NeRF (GAT-NeRF) for high-fidelity and controllable 4D facial avatar reconstruction, which integrates the Transformer mechanism into the NeRF pipeline. GAT-NeRF synergistically combines a coordinate-aligned Multilayer Perceptron (MLP) with a lightweight Transformer module, termed as Geometry-Aware-Transformer (GAT) due to its processing of multi-modal inputs containing explicit geometric priors. The GAT module is enabled by fusing multi-modal input features, including 3D spatial coordinates, 3D Morphable Model (3DMM) expression parameters, and learnable latent codes to effectively learn and enhance feature representations pertinent to fine-grained geometry. The Transformer’s effective feature learning capabilities are leveraged to significantly augment the modeling of complex local facial patterns like dynamic wrinkles and acne scars. Comprehensive experiments unequivocally demonstrate GAT-NeRF’s state-of-the-art performance in visual fidelity and high-frequency detail recovery, forging new pathways for creating realistic dynamic digital humans for multimedia applications.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414786941",
    "type": "article"
  },
  {
    "title": "R-HMF: A Relation-enhanced Hierarchical Multimodal Framework for Few-shot Knowledge Graph Completion",
    "doi": "https://doi.org/10.1145/3769865",
    "publication_date": "2025-10-08",
    "publication_year": 2025,
    "authors": "Chengmei Yang; Qian Li; Zhenyang Li; Chen Ma; Lianghua He",
    "corresponding_authors": "",
    "abstract": "Knowledge graph completion (KGC), which aims at inferring the missing fact triples, has shown an essential role in constructing a complete knowledge graph to enhance downstream applications. However, most KGC techniques require a large number of labeled training instances, and the performance drops dramatically when only a few triples are available. The primary challenge lies in the insufficient information that the few-shot annotated triples provided. Recently, several works have utilized multimodal entity contexts to enrich the entity representation, but their performance remains constrained by 1) overlooking the challenges of modality heterogeneity, 2) introducing the redundant multimodal noise of entities that is irrelevant to the corresponding relation, and 3) the difficulty in learning relation representation with only a few labeled cases. To address the above issues, we propose a novel R elation-enhanced H ierarchical M ultimodal F ramework (R-HMF) for the few-shot knowledge graph completion. Specifically, to take the modality heterogeneity into account, we first conduct the modality-specific few-shot relation learning to capture the correlation between entities and relations within each modality. Subsequently, a multimodal fact assessment module is designed to validate the correctness of given triples by considering the entity contexts in a joint multimodal representation space. Notably, to avoid the involvement of redundant multimodal noise of entities, adaptive entity features are extracted to fit for different relations. In addition, to strengthen the relation representation in the few-shot setting, we also take full advantage of the large language models (LLMs) to generate the corresponding relation names and descriptions, which will be utilized to align different modalities as well. Extensive experimental results on two multimodal knowledge graph datasets, MM-FB15K237 and MM-DBpedia, show that our framework achieves better performance than previous state-of-the-art methods by improving 3.23% Hits@10 score under the 1-shot setting and 6.45% Hits@10 score under the 5-shot setting on average.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414971110",
    "type": "article"
  },
  {
    "title": "ShapeMoiré: Channel-Wise Shape-Guided Network for Image Demoiréing",
    "doi": "https://doi.org/10.1145/3748657",
    "publication_date": "2025-10-10",
    "publication_year": 2025,
    "authors": "Jinming Cao; Sicheng Shen; Qiu Zhou; Yifang Yin; Yangyan Li; Roger Zimmermann",
    "corresponding_authors": "Yifang Yin",
    "abstract": "Photographing optoelectronic displays often introduces unwanted moiré patterns due to analog signal interference between the pixel grids of the display and the camera sensor arrays. This work identifies two problems that are largely ignored by existing image demoiréing approaches: 1) moiré patterns vary across different channels (RGB); 2) repetitive patterns are constantly observed. However, employing conventional convolutional (CNN) layers cannot address these problems. Instead, this paper presents the use of our recently proposed Shape concept. It was originally employed to model consistent features from fragmented regions, particularly when identical or similar objects coexist in an RGB-D image. Interestingly, we find that the Shape information effectively captures the moiré patterns in artifact images. Motivated by this discovery, we propose a new method, ShapeMoiré, for image demoiréing. Beyond modeling shape features at the patch-level, we further extend this to the global image-level and design a novel Shape-Architecture. Consequently, our proposed method, equipped with both ShapeConv and Shape-Architecture, can be seamlessly integrated into existing approaches without introducing any additional parameters or computation overhead during inference. We conduct extensive experiments on four widely used datasets, and the results demonstrate that our ShapeMoiré achieves state-of-the-art performance, particularly in terms of the PSNR metric. We then apply our method across four popular architectures to showcase its generalization capabilities. Moreover, to further validate its generality beyond the demoiréing task, we apply ShapeMoiré to the image deblurring task, where it continues to deliver consistent performance gains. Finally, experiments on real-world images captured by smartphones confirm the robustness and practical applicability of ShapeMoiré in challenging demoiréing scenarios. We open-sourced an implementation of ShapeMoiré in PyTorch at https://github.com/ShapeMoire .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415046923",
    "type": "article"
  },
  {
    "title": "Frequency Restoration and Modality Enforcement towards Resisting-corruption Multimodal Sentiment Analysis",
    "doi": "https://doi.org/10.1145/3767746",
    "publication_date": "2025-10-10",
    "publication_year": 2025,
    "authors": "Weicheng Xie; Hanzhe Liang; Zenghao Niu; Xianxu Hou; Siyang Song; Zitong Yu; Linlin Shen",
    "corresponding_authors": "",
    "abstract": "For Multimodal Sentiment Analysis (MSA), previous methods concentrate on designing sophisticated fusion strategies and performing representation learning across heterogeneous modalities, aiming to leverage multimodal signals to detect human sentiment. However, these approaches fail to address the long-standing issue of corrupted modal details in videos, which maybe caused by the challenge of the excessive loss of emotionally relevant semantics resulted from the degradation of detailed information. In this work, we aim to improve the robustness capacity of resisting corruption in MSA, by introducing a Hierarchical Frequency Restoration and Adaptive Modality Enforcement (HFR-AME) approach. The HFR-AME progressively recovers blurred detailed cues in each modality while enhancing the discriminative power of modal representations. Specifically, to reconstruct distinct frequency band features, we propose to equip the HFR module with a key component called the Frequency Multimodal UNet (FM-UNet), so as to utilize complementary modal features as conditions. This meticulous restoration process, performed from low to high frequency, facilitates the comprehensive recovery of intricate details. Meanwhile, to adaptively integrate these diverse frequency features, we introduce the AME module to enhance the beneficial modal frequencies while suppressing irrelevant ones, with the goal of strengthening the restored modal representations. Extensive experiments show our HFR-AME outperforms state-of-the-art methods on the CMU-MOSI and CMU-MOSEI datasets, improving 7-class accuracy by 0.5% and 0.6%, respectively. Further analysis also confirms its cross-lingual generalization and competitive computational efficiency. Our code is made available at https://github.com/nianhua20/HFR-AME.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415047052",
    "type": "article"
  },
  {
    "title": "Lightweight Depthwise Separable ConvNet with Frequency-Domain Enhancement for Retinal Vessel Segmentation",
    "doi": "https://doi.org/10.1145/3767732",
    "publication_date": "2025-10-13",
    "publication_year": 2025,
    "authors": "Yang Wen; Siu-Tsen Shen; Wuzhen Shi; Wenming Cao; Lei Bi; Xiaokang Yang; Bin Sheng",
    "corresponding_authors": "",
    "abstract": "Automatic retinal vessel segmentation is crucial in the diagnosis and treatment of various cardiovascular and eye diseases. Although current vessel segmentation methods have achieved impressive performance, some challenging issues still need to be addressed. For example, existing methods always cannot segment complex capillaries well because they may be interfered with or covered by other components in the retina, and they need to further improve the continuity and consistency of vessel segmentation results. Moreover, the excellent vessel segmentation methods are usually built on bulky and cumbersome models which greatly limit their application range. In this paper, we propose a novel efficient depthwise separable convolution network with frequency-domain enhancement (dubbed RetiNeXt) for retinal vessel segmentation. Firstly, we design a lightweight vessel enhancement module to extract global fine topological structure features from the frequency domain to enhance the complex capillary vessel details. Secondly, we propose a global feature extraction block to fully capture the large-scale spatial information and global characterizations, which enables the model to maintain vessel structural coherence from a global perspective. Thirdly, we construct a local feature mixing block based on SimAM attention mechanism to highlight the tiny capillary topological structure features and optimize the segmentation of low-contrast blood vessels, thereby improving the integrity and continuity of complex capillaries. Comprehensive comparison experiments on three well-benchmarked retinal vessel segmentation datasets fully verify the effectiveness and superiority of the proposed RetiNeXt. To further demonstrate the universality of RetiNeXt for medical image segmentation, we also conduct sufficient comparative experiments on two classical coronary angiography datasets. Extensive quantitative and qualitative experiments fully show that RetiNeXt outperforms other state-of-the-art methods with only 0.4M of trainable parameters.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415128570",
    "type": "article"
  },
  {
    "title": "CookingDiffusion: Cooking Procedural Image Generation with Stable Diffusion",
    "doi": "https://doi.org/10.1145/3771995",
    "publication_date": "2025-10-17",
    "publication_year": 2025,
    "authors": "Yuan Wang; Bin Zhu; Yanbin Hao; Chong‐Wah Ngo; Yi Tan; Xiang Wang",
    "corresponding_authors": "",
    "abstract": "Recent advancements in text-to-image generation models have excelled in creating diverse and realistic images. This success extends to food imagery, where various conditional inputs like cooking styles, ingredients, and recipes are utilized. However, a yet-unexplored challenge is generating a sequence of procedural images based on cooking steps from a recipe. This could enhance the cooking experience with visual guidance and possibly lead to an intelligent cooking simulation system. To fill this gap, we introduce a novel task called cooking procedural image generation . This task is inherently demanding, as it strives to create photo-realistic images that align with cooking steps while preserving sequential consistency. To collectively tackle these challenges, we present CookingDiffusion , a novel approach that leverages Stable Diffusion and three innovative Memory Nets to model procedural prompts. These prompts encompass text prompts (representing cooking steps), image prompts (corresponding to cooking images), and multi-modal prompts (mixing cooking steps and images), ensuring the consistent generation of cooking procedural images. To validate the effectiveness of our approach, we preprocess the YouCookII dataset, establishing a new benchmark. Our experimental results demonstrate that our model excels at generating high-quality cooking procedural images with remarkable consistency across sequential cooking steps, as measured by both the FID and the proposed Average Procedure Consistency metrics. Furthermore, CookingDiffusion demonstrates the ability to manipulate ingredients and cooking methods in a recipe. We will make our code, models, and dataset publicly accessible.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415281761",
    "type": "article"
  },
  {
    "title": "Unsupervised Visible-Infrared Person ReID via Modality-Camera Balance Label Refinement",
    "doi": "https://doi.org/10.1145/3772086",
    "publication_date": "2025-10-18",
    "publication_year": 2025,
    "authors": "Junbin He; Yiming Yang; Ruixing Wu; Haifeng Hu",
    "corresponding_authors": "",
    "abstract": "Unsupervised visible-infrared person re-identification (USL-VI-ReID) focuses on developing a cross-modality retrieval model without the need for labels, minimizing the dependence on costly manual annotation across modalities. Recently, various approaches focus on reducing the cross-modality discrepancies. However, they ignore that USL-VI-ReID is also a task of solving discrepancies while exploring fine-grained information in hierarchical domains. In this paper, we propose a hierarchical Modality-Camera Balance Label Refinement (MCBL) framework to balance the contributions of each camera-modality. Meanwhile, we explore the fine-grained features and refine the noise labels at each training stages. Specifically, our MCBL naturally combines Modality-Camera Balanced Label Mining (MBLM), Unreliable Pseudo-Label Re-align (UPR), and Hybrid Modality-Camera Contrastive Learning (HMCCL) into a unified framework, which balances the association information for each hierarchical domain through refining noise labels. Technically, MBLM filters cluster-level noise samples utilizing a modality-camera balance strategy, thereby ensuring that reliable samples are stored in memory for effective contrast learning. UPR refines the noise labels through the re-alignment methods at the instance level, thus improving the accuracy of labels and further enhancing the model’s generalization ability. Moreover, the key of HMCCL is optimizing the distribution at both the instance and cluster levels, which forces the sample to be close to its cluster proxy while being far from others in a real-time memory update phase. Extensive experiments have shown that our MCBL addresses the current limitations of camera discrepancy and achieves competitive performance.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415312891",
    "type": "article"
  },
  {
    "title": "Real-time multidepth stream compression",
    "doi": "https://doi.org/10.1145/1062253.1062255",
    "publication_date": "2005-05-01",
    "publication_year": 2005,
    "authors": "Sang-Uok Kum; Ketan Mayer‐Patel",
    "corresponding_authors": "",
    "abstract": "The goal of tele-immersion has long been to enable people at remote locations to share a sense of presence. A tele-immersion system acquires the 3D representation of a collaborator's environment remotely and sends it over the network where it is rendered in the user's environment. Acquisition, reconstruction, transmission, and rendering all have to be done in real-time to create a sense of presence. With added commodity hardware resources, parallelism can increase the acquisition volume and reconstruction data quality while maintaining real-time performance. However, this is not as easy for rendering since all of the data need to be combined into a single display.In this article, we present an algorithm to compress data from such 3D environments in real-time to solve this imbalance. We present a compression algorithm which scales comparably to the acquisition and reconstruction, reduces network transmission bandwidth, and reduces the rendering requirement for real-time performance. This is achieved by exploiting the coherence in the 3D environment data and removing them in real-time. We have tested the algorithm using a static office data set as well as a dynamic scene, the results of which are presented in the article.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W1968694805",
    "type": "article"
  },
  {
    "title": "Automatic temporal layout mechanisms revisited",
    "doi": "https://doi.org/10.1145/1047936.1047942",
    "publication_date": "2005-02-01",
    "publication_year": 2005,
    "authors": "M. Cecelia Buchanan; Polle T. Zellweger",
    "corresponding_authors": "",
    "abstract": "A traditional static document has a spatial layout that specifies where objects in the document appear. Because multimedia documents incorporate time, they also require a temporal layout, or schedule, that specifies when events in the document occur. This article argues that multimedia document systems should provide mechanisms for automatically producing temporal layouts for documents. The major advantage of this approach is that it makes it easier for authors to create and modify multimedia documents.This article revisits our 1993 framework for understanding automatic temporal formatters and explores the basic issues surrounding them. It also describes the Firefly multimedia document system, which was developed in 1992 to test the potential of automatic temporal formatting. Using our original framework, the paper reviews a representative sample of recent automatic document formatters. This analysis validates the basic framework and demonstrates the progress of the field in the intervening decade. A discussion of potential extensions to the framework is included.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W1986236842",
    "type": "article"
  },
  {
    "title": "A narrative-based abstraction framework for story-oriented video",
    "doi": "https://doi.org/10.1145/1230812.1230817",
    "publication_date": "2007-05-01",
    "publication_year": 2007,
    "authors": "Byunghee Jung; Junehwa Song; Yoonjoon Lee",
    "corresponding_authors": "",
    "abstract": "This article proposes a novel video abstraction framework for online review services of story-oriented videos such as dramas. Among the many genres of TV programs, a drama is one of the most popularly watched on the Web. The abstracts generated by the proposed framework not only give a summary of a video but also effectively help viewers understand the overall story. In addition, our method is duration-flexible. We get clues about human understanding of a story from scenario writing rules and editorial techniques that are popularly used in the process of video production to explicitly express a narrative, and propose a new video abstraction model, called a Narrative Abstraction Model . The model effectively captures the narrative structure embedded in a story-oriented video and articulates the progress of the story in a weighted directed graph, called a Narrative Structure Graph (NSG) . The model provides a basis for a flexible framework for abstract generation using the NSG as the intermediary representation of a video. Different abstracts can be appropriately generated based upon different user requirements. To show the effectiveness of the proposed model and method, we developed a video abstraction system realizing the framework, and successfully applied it to large volumes of TV dramas. The evaluation results show that the proposed framework is a feasible solution for online review services.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W1969368842",
    "type": "article"
  },
  {
    "title": "Equipment allocation in video-on-demand network deployments",
    "doi": "https://doi.org/10.1145/1404880.1404885",
    "publication_date": "2008-10-01",
    "publication_year": 2008,
    "authors": "Frederic Thouin; Mark Coates",
    "corresponding_authors": "",
    "abstract": "Video-on-Demand (VoD) services are very user-friendly, but also complex and resource demanding. Deployments involve careful design of many mechanisms where content attributes and usage models should be taken into account. We define, and propose a methodology to solve, the VoD Equipment Allocation Problem of determining the number and type of streaming servers with directly attached storage (VoD servers) to install at each potential location in a metropolitan area network topology such that deployment costs are minimized. We develop a cost model for VoD deployments based on streaming, storage and transport costs and train a parametric function that maps the amount of available storage to a worst-case hit ratio. We observe the impact of having to determine the amount of storage and streaming cojointly, and determine the minimum demand required to deploy replicas as well as the average hit ratio at each location. We observe that common video-on-demand server configurations lead to the installation of excessive storage, because a relatively high hit-ratio can be achieved with small amounts of storage so streaming requirements dominate.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2019395521",
    "type": "article"
  },
  {
    "title": "Towards efficient context-specific video coding based on gaze-tracking analysis",
    "doi": "https://doi.org/10.1145/1314303.1314307",
    "publication_date": "2007-12-01",
    "publication_year": 2007,
    "authors": "Dimitris K. Agrafiotis; S.J.C. Davies; Nishan Canagarajah; David Bull",
    "corresponding_authors": "",
    "abstract": "This article discusses a framework for model-based, context-dependent video coding based on exploitation of characteristics of the human visual system. The system utilizes variable-quality coding based on priority maps which are created using mostly context-dependent rules. The technique is demonstrated through two case studies of specific video context, namely open signed content and football sequences. Eye-tracking analysis is employed for identifying the characteristics of each context, which are subsequently exploited for coding purposes, either directly or through a gaze prediction model. The framework is shown to achieve a considerable improvement in coding efficiency.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2041823005",
    "type": "article"
  },
  {
    "title": "A revenue-rewarding scheme of providing incentive for cooperative proxy caching for media streaming systems",
    "doi": "https://doi.org/10.1145/1324287.1324292",
    "publication_date": "2008-01-01",
    "publication_year": 2008,
    "authors": "Alan T. S. Ip; John C. S. Lui; Jiangchuan Liu",
    "corresponding_authors": "",
    "abstract": "Network entities cooperating together can improve system performance of media streaming. In this paper, we address the “ incentive issue ” of a cooperative proxy caching system and how to motivate each proxy to provide cache space to the system. To encourage proxies to participate, we propose a “ revenue-rewarding scheme ” to credit the cooperative proxies according to the resources they contribute. A game-theoretic model is used to analyze the interactions among proxies under the revenue-rewarding scheme. We propose two cooperative game settings that lead to optimal situations. In particular, (1) We propose a distributed incentive framework for peers to participate in resource contribution for media streaming; (2) Proxies are encouraged to cooperate under the revenue-rewarding scheme; (3) Profit and social welfare are maximized in these cooperative games; and (4) Cost-effective resource allocation is achieved in these cooperative games. Large scale simulation is carried out to validate and verify the merits of our proposed incentive schemes.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2089787080",
    "type": "article"
  },
  {
    "title": "Dialocalization",
    "doi": "https://doi.org/10.1145/1865106.1865111",
    "publication_date": "2010-11-01",
    "publication_year": 2010,
    "authors": "Gerald Friedland; Chuohao Yeo; Hayley Hung",
    "corresponding_authors": "",
    "abstract": "The following article presents a novel audio-visual approach for unsupervised speaker localization in both time and space and systematically analyzes its unique properties. Using recordings from a single, low-resolution room overview camera and a single far-field microphone, a state-of-the-art audio-only speaker diarization system (speaker localization in time) is extended so that both acoustic and visual models are estimated as part of a joint unsupervised optimization problem. The speaker diarization system first automatically determines the speech regions and estimates “who spoke when,” then, in a second step, the visual models are used to infer the location of the speakers in the video. We call this process “dialocalization.” The experiments were performed on real-world meetings using 4.5 hours of the publicly available AMI meeting corpus. The proposed system is able to exploit audio-visual integration to not only improve the accuracy of a state-of-the-art (audio-only) speaker diarization, but also adds visual speaker localization at little incremental engineering and computation costs. The combined algorithm has different properties, such as increased robustness, that cannot be observed in algorithms based on single modalities. The article describes the algorithm, presents benchmarking results, explains its properties, and systematically discusses the contributions of each modality.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2115878730",
    "type": "article"
  },
  {
    "title": "Spatial-geometric approach to physical mobile interaction based on accelerometer and IR sensory data fusion",
    "doi": "https://doi.org/10.1145/1865106.1865112",
    "publication_date": "2010-11-01",
    "publication_year": 2010,
    "authors": "A. Rahman; M. Anwar Hossain; Abdulmotaleb El Saddik",
    "corresponding_authors": "",
    "abstract": "Interaction with the physical environment using mobile phones has become increasingly desirable and feasible. Nowadays mobile phones are being used to control different devices and access information/services related to those devices. To facilitate such interaction, devices are usually marked with RFID tags or visual markers, which are read by a mobile phone equipped with an integrated RFID reader or camera to fetch related information about those objects and initiate further actions. This article contributes in this domain of mobile physical interaction; however, using a spatial-geometric approach for interacting with indoor physical objects and artifacts instead of RFID based solutions. Using this approach, a mobile phone can point from a distance to an annotated object or a spatial subregion of that object for the purpose of interaction. The pointing direction and location is determined based on the fusion of IR camera and accelerometer data, where the IR cameras are used to calculate the 3D position of the mobile phone users and the accelerometer in the phone provides its tilting and orientation information. The annotation of objects and their subregions with which the mobile phone interacts is performed by specifying their geometric coordinates and associating related information or services with them. We perform experiment in a technology-augmented smart space and show the applicability and potential of the proposed approach.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2123680668",
    "type": "article"
  },
  {
    "title": "An eye localization, tracking and blink pattern recognition system",
    "doi": "https://doi.org/10.1145/1671962.1671964",
    "publication_date": "2010-03-01",
    "publication_year": 2010,
    "authors": "Junwen Wu; Mohan M. Trivedi",
    "corresponding_authors": "",
    "abstract": "This study is to investigate the fundamental problems of, (1) facial feature detection and localization, especially eye features; and (2) eye dynamics, including tracking and blink detection. We first describe our contribution to eye localization. Following that, we discuss a simultaneous eye tracking and blink detection system. Facial feature detection is solved in a general object detection framework and its performance for eye localization is presented. A binary tree representation based on feature dependency partitions the object feature space in a coarse to fine manner. In each compact feature subspace, independent component analysis (ICA) is used to get the independent sources, whose probability density functions (PDFs) are modeled by Gaussian mixtures. When applying this representation for the task of eye detection, a subwindow is used to scan the entire image and each obtained image patch is examined using Bayesian criteria to determine the presence of an eye subject. After the eyes are automatically located with binary tree-based probability learning, interactive particle filters are used for simultaneously tracking the eyes and detecting the blinks. The particle filters use classification-based observation models, in which the posterior probabilities are evaluated by logistic regressions in tensor subspaces. Extensive experiments are used to evaluate the performance from two aspects, (1) blink detection rate and the accuracy of blink duration in terms of the frame numbers; (2) eye tracking accuracy. We also present an experimental setup for obtaining the benchmark data in tracking accuracy evaluation. The experimental evaluation demonstrates the capability of this approach.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W1969580032",
    "type": "article"
  },
  {
    "title": "REED",
    "doi": "https://doi.org/10.1145/2168996.2169000",
    "publication_date": "2012-05-01",
    "publication_year": 2012,
    "authors": "Grenville Armitage; Amiel A. Heyde",
    "corresponding_authors": "",
    "abstract": "Online First Person Shooter (FPS) games typically use a client-server communication model, with thousands of enthusiast-hosted game servers active at any time. Traditional FPS server discovery may take minutes, as clients create thousands of short-lived packet flows while probing all available servers to find a selection of game servers with tolerable round trip time (RTT). REED reduces a client's probing time and network traffic to 1% of traditional server discovery. REED game servers participate in a centralized, incremental calculation of their network coordinates, and clients use these coordinates to expedite the discovery of servers with low RTTs.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W1990825535",
    "type": "article"
  },
  {
    "title": "A new methodology to derive objective quality assessment metrics for scalable multiview 3D video coding",
    "doi": "https://doi.org/10.1145/2348816.2348823",
    "publication_date": "2012-09-01",
    "publication_year": 2012,
    "authors": "Hoda Roodaki; Mahmoud Reza Hashemi; Shervin Shirmohammadi",
    "corresponding_authors": "",
    "abstract": "With the growing demand for 3D video, efforts are underway to incorporate it in the next generation of broadcast and streaming applications and standards. 3D video is currently available in games, entertainment, education, security, and surveillance applications. A typical scenario for multiview 3D consists of several 3D video sequences captured simultaneously from the same scene with the help of multiple cameras from different positions and through different angles. Multiview video coding provides a compact representation of these multiple views by exploiting the large amount of inter-view statistical dependencies. One of the major challenges in this field is how to transmit the large amount of data of a multiview sequence over error prone channels to heterogeneous mobile devices with different bandwidth, resolution, and processing/battery power, while maintaining a high visual quality. Scalable Multiview 3D Video Coding (SMVC) is one of the methods to address this challenge; however, the evaluation of the overall visual quality of the resulting scaled-down video requires a new objective perceptual quality measure specifically designed for scalable multiview 3D video. Although several subjective and objective quality assessment methods have been proposed for multiview 3D sequences, no comparable attempt has been made for quality assessment of scalable multiview 3D video. In this article, we propose a new methodology to build suitable objective quality assessment metrics for different scalable modalities in multiview 3D video. Our proposed methodology considers the importance of each layer and its content as a quality of experience factor in the overall quality. Furthermore, in addition to the quality of each layer, the concept of disparity between layers (inter-layer disparity) and disparity between the units of each layer (intra-layer disparity) is considered as an effective feature to evaluate overall perceived quality more accurately. Simulation results indicate that by using this methodology, more efficient objective quality assessment metrics can be introduced for each multiview 3D video scalable modalities.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2146321804",
    "type": "article"
  },
  {
    "title": "SCENT",
    "doi": "https://doi.org/10.1145/2037676.2037686",
    "publication_date": "2011-10-01",
    "publication_year": 2011,
    "authors": "Yu‐Ru Lin; K. Selçcuk Candan; Hari Sundaram; Lexing Xie",
    "corresponding_authors": "",
    "abstract": "We propose SCENT, an innovative, scalable spectral analysis framework for internet scale monitoring of multirelational social media data, encoded in the form of tensor streams. In particular, a significant challenge is to detect key changes in the social media data, which could reflect important events in the real world, sufficiently quickly. Social media data have three challenging characteristics. First, data sizes are enormous; recent technological advances allow hundreds of millions of users to create and share content within online social networks. Second, social data are often multifaceted (i.e., have many dimensions of potential interest, from the textual content to user metadata). Finally, the data is dynamic; structural changes can occur at multiple time scales and be localized to a subset of users. Consequently, a framework for extracting useful information from social media data needs to scale with data volume, and also with the number and diversity of the facets of the data. In SCENT, we focus on the computational cost of structural change detection in tensor streams. We extend compressed sensing (CS) to tensor data. We show that, through the use of randomized tensor ensembles, SCENT is able to encode the observed tensor streams in the form of compact descriptors. We show that the descriptors allow very fast detection of significant spectral changes in the tensor stream, which also reduce data collection, storage, and processing costs. Experiments over synthetic and real data show that SCENT is faster (17.7x--159x for change detection) and more accurate (above 0.9 F-score) than baseline methods.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2146644487",
    "type": "article"
  },
  {
    "title": "Learning a Multi-Concept Video Retrieval Model with Multiple Latent Variables",
    "doi": "https://doi.org/10.1145/3176647",
    "publication_date": "2018-04-25",
    "publication_year": 2018,
    "authors": "Amir Mazaheri; Boqing Gong; Mubarak Shah",
    "corresponding_authors": "",
    "abstract": "Effective and efficient video retrieval has become a pressing need in the “big video” era. The objective of this work is to provide a principled model for computing the ranking scores of a video in response to one or more concepts, where the concepts could be directly supplied by users or inferred by the system from the user queries. Indeed, how to deal with multi-concept queries has become a central component in modern video retrieval systems that accept text queries. However, it has been long overlooked and simply implemented by weighted averaging of the corresponding concept detectors’ scores. Our approach, which can be considered as a latent ranking SVM, integrates the advantages of various recent works in text and image retrieval, such as choosing ranking over structured prediction, modeling inter-dependencies between querying concepts, and so on. Videos consist of shots, and we use latent variables to account for the mutually complementary cues within and across shots. Concept labels of shots are scarce and noisy. We introduce a simple and effective technique to make our model robust to outliers. Our approach gives superior performance when it is tested on not only the queries seen at training but also novel queries, some of which consist of more concepts than the queries used for training.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2802567013",
    "type": "article"
  },
  {
    "title": "Prototyping a Web-Scale Multimedia Retrieval Service Using Spark",
    "doi": "https://doi.org/10.1145/3209662",
    "publication_date": "2018-06-15",
    "publication_year": 2018,
    "authors": "Gylfi Þór Guđmundsson; Björn Þór Jónsson; Laurent Amsaleg; Michael J. Franklin",
    "corresponding_authors": "",
    "abstract": "The world has experienced phenomenal growth in data production and storage in recent years, much of which has taken the form of media files. At the same time, computing power has become abundant with multi-core machines, grids and clouds. Yet it remains a challenge to harness the available power and move towards gracefully searching and retrieving from web-scale media collections. Several researchers have experimented with using automatically distributed computing frameworks, notably Hadoop and Spark, for processing multimedia material, but mostly using small collections on small computing clusters. In this paper, we describe a prototype of a (near) web-scale throughput-oriented MM retrieval service using the Spark framework running on the AWS cloud service. We present retrieval results using up to 43 billion SIFT feature vectors from the public YFCC 100M collection, making this the largest high-dimensional feature vector collection reported in the literature. We also present a publicly available demonstration retrieval system, running on our own servers, where the implementation of the Spark pipelines can be observed in practice using standard image benchmarks, and downloaded for research purposes. Finally, we describe a method to evaluate retrieval quality of the ever-growing high-dimensional index of the prototype, without actually indexing a web-scale media collection.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2808827381",
    "type": "article"
  },
  {
    "title": "Characterizing User Behaviors in Mobile Personal Livecast",
    "doi": "https://doi.org/10.1145/3219751",
    "publication_date": "2018-06-30",
    "publication_year": 2018,
    "authors": "Ming Ma; Lei Zhang; Jiangchuan Liu; Zhi Wang; Haitian Pang; Lifeng Sun; Weihua Li; Guangling Hou; Kaiyan Chu",
    "corresponding_authors": "",
    "abstract": "Mobile personal livecast (MPL) services are emerging and have received great attention recently. In MPL, numerous and geo-distributed ordinary people broadcast their video contents to worldwide viewers. Different from conventional social networking services like Twitter and Facebook, which have a tolerance for interaction delay, the interactions (e.g., chat messages) in a personal livecast must be in real-time with low feedback latency. These unique characteristics inspire us to: (1) investigate how the relationships (e.g., social links and geo-locations) between viewers and broadcasters influence the user behaviors, which has yet to be explored in depth; and (2) explore insights to benefit the improvement of system performance. In this article, we carry out extensive measurements of a representative MPL system, with a large-scale dataset containing 11M users. In the current costly and limited cloud-based MPL system, which is faced with scalability problem, we find: (1) the long content uploading distances between broadcasters and cloud ingesting servers result in an impaired system QoS, including a high broadcast latency and a frequently buffering events; and (2) most of the broadcasters in MPL are geographically locally popular (the majority of the views come from the same region of the broadcaster), which consume vast computation and bandwidth resources of the clouds and Content Delivery Networks. Fortunately, the emergence of edge computing, which provides cloud-computing capabilities at the edge of the mobile network, naturally sheds new light on the MPL system; i.e., localized ingesting, transcoding, and delivering locally popular live content is possible. Based on these critical observations, we propose an edge-assisted MPL system that collaboratively utilizes the core-cloud and abundant edge computing resources to improve the system efficiency and scalability. In our framework, we consider a dynamic broadcaster assignment to minimize the broadcast latency while keeping the resource lease cost low. We formulate the broadcaster scheduling as a stable matching with migration problem to solve it effectively. Compared with the current pure cloud-based system, our edge-assisted delivery approach reduces the broadcast latency by about 35%.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2886152067",
    "type": "article"
  },
  {
    "title": "Probability Model-Based Early Merge Mode Decision for Dependent Views Coding in 3D-HEVC",
    "doi": "https://doi.org/10.1145/3267128",
    "publication_date": "2018-10-01",
    "publication_year": 2018,
    "authors": "Yue Li; Gaobo Yang; Yapei Zhu; Xiangling Ding; Rongrong Gong",
    "corresponding_authors": "",
    "abstract": "As a 3D extension to the High Efficiency Video Coding (HEVC) standard, 3D-HEVC was developed to improve the coding efficiency of multiview videos. It inherits the prediction modes from HEVC, yet both Motion Estimation (ME) and Disparity Estimation (DE) are required for dependent views coding. This improves coding efficiency at the cost of huge computational costs. In this article, an early Merge mode decision approach is proposed for dependent texture views and dependent depth maps coding in 3D-HEVC based on priori and posterior probability models. First, the priori probability model is established by exploiting the hierarchical and interview correlations from those previously encoded blocks. Second, the posterior probability model is built by using the Coded Block Flag (CBF) of the current coding block. Finally, the joint priori and posterior probability model is adopted to early terminate the Merge mode decision for both dependent texture views and dependent depth maps coding. Experimental results show that the proposed approach saves 45.2% and 30.6% encoding time on average for dependent texture views and dependent depth maps coding while maintaining negligible loss of coding efficiency, respectively.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2894865079",
    "type": "article"
  },
  {
    "title": "Unsupervised Similarity Learning through Rank Correlation and kNN Sets",
    "doi": "https://doi.org/10.1145/3241053",
    "publication_date": "2018-10-23",
    "publication_year": 2018,
    "authors": "Lucas Pascotti Valem; Carlos Renan De Oliveira; Daniel Carlos Guimarães Pedronette; Jurandy Almeida",
    "corresponding_authors": "",
    "abstract": "The increasing amount of multimedia data collections available today evinces the pressing need for methods capable of indexing and retrieving this content. Despite the continuous advances in multimedia features and representation models, to establish an effective measure for comparing different multimedia objects still remains a challenging task. While supervised and semi-supervised techniques made relevant advances on similarity learning tasks, scenarios where labeled data are non-existent require different strategies. In such situations, unsupervised learning has been established as a promising solution, capable of considering the contextual information and the dataset structure for computing new similarity/dissimilarity measures. This article extends a recent unsupervised learning algorithm that uses an iterative re-ranking strategy to take advantage of different k -Nearest Neighbors (kNN) sets and rank correlation measures. Two novel approaches are proposed for computing the kNN sets and their corresponding top- k lists. The proposed approaches were validated in conjunction with various rank correlation measures, yielding superior effectiveness results in comparison with previous works. In addition, we also evaluate the ability of the method in considering different multimedia objects, conducting an extensive experimental evaluation on various image and video datasets.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2897076095",
    "type": "article"
  },
  {
    "title": "Towards an automatic music arrangement framework using score reduction",
    "doi": "https://doi.org/10.1145/2071396.2071404",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Jiun‐Long Huang; Shih-Chuan Chiu; Man-Kwan Shan",
    "corresponding_authors": "",
    "abstract": "Score reduction is a process that arranges music for a target instrument by reducing original music. In this study we present a music arrangement framework that uses score reduction to automatically arrange music for a target instrument. The original music is first analyzed to determine the type of arrangement element of each section, then the phrases are identified and each is assigned a utility according to its type of arrangement element. For a set of utility-assigned phrases, we transform the music arrangement into an optimization problem and propose a phrase selection algorithm. The music is arranged by selecting appropriate phrases satisfying the playability constraints of a target instrument. Using the proposed framework, we implement a music arrangement system for the piano. An approach similar to Turing test is used to evaluate the quality of the music arranged by our system. The experiment results show that our system is able to create viable music for the piano.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2123306325",
    "type": "article"
  },
  {
    "title": "Deformation-Based 3D Facial Expression Representation",
    "doi": "https://doi.org/10.1145/3176649",
    "publication_date": "2018-03-10",
    "publication_year": 2018,
    "authors": "Girum Demisse; Djamila Aouada; Björn Ottersten",
    "corresponding_authors": "",
    "abstract": "We propose a deformation-based representation for analyzing expressions from three-dimensional (3D) faces. A point cloud of a 3D face is decomposed into an ordered deformable set of curves that start from a fixed point. Subsequently, a mapping function is defined to identify the set of curves with an element of a high-dimensional matrix Lie group, specifically the direct product of SE(3). Representing 3D faces as an element of a high-dimensional Lie group has two main advantages. First, using the group structure, facial expressions can be decoupled from a neutral face. Second, an underlying non-linear facial expression manifold can be captured with the Lie group and mapped to a linear space, Lie algebra of the group. This opens up the possibility of classifying facial expressions with linear models without compromising the underlying manifold. Alternatively, linear combinations of linearised facial expressions can be mapped back from the Lie algebra to the Lie group. The approach is tested on the Binghamton University 3D Facial Expression (BU-3DFE) and the Bosphorus datasets. The results show that the proposed approach performed comparably, on the BU-3DFE dataset, without using features or extensive landmark points.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2784519437",
    "type": "article"
  },
  {
    "title": "Toward Personalized Activity Level Prediction in Community Question Answering Websites",
    "doi": "https://doi.org/10.1145/3187011",
    "publication_date": "2018-04-25",
    "publication_year": 2018,
    "authors": "Zhenguang Liu; Yingjie Xia; Qi Liu; Qinming He; Chao Zhang; Roger Zimmermann",
    "corresponding_authors": "",
    "abstract": "Community Question Answering (CQA) websites have become valuable knowledge repositories. Millions of internet users resort to CQA websites to seek answers to their encountered questions. CQA websites provide information far beyond a search on a site such as Google due to (1) the plethora of high-quality answers, and (2) the capabilities to post new questions toward the communities of domain experts. While most research efforts have been made to identify experts or to preliminarily detect potential experts of CQA websites, there has been a remarkable shift toward investigating how to keep the engagement of experts. Experts are usually the major contributors of high-quality answers and questions of CQA websites. Consequently, keeping the expert communities active is vital to improving the lifespan of these websites. In this article, we present an algorithm termed PALP to predict the activity level of expert users of CQA websites. To the best of our knowledge, PALP is the first approach to address a personalized activity level prediction model for CQA websites. Furthermore, it takes into consideration user behavior change over time and focuses specifically on expert users. Extensive experiments on the Stack Overflow website demonstrate the competitiveness of PALP over existing methods.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2801761070",
    "type": "article"
  },
  {
    "title": "Analytical Global Median Filtering Forensics Based on Moment Histograms",
    "doi": "https://doi.org/10.1145/3176650",
    "publication_date": "2018-04-25",
    "publication_year": 2018,
    "authors": "Abhinav Gupta; Divya Singhal",
    "corresponding_authors": "",
    "abstract": "Median filtering forensics in images has gained wide attention from researchers in recent years because of its inherent nature of preserving visual traces. Although many forensic methods are developed for median filtering detection, probability of detection reduces under JPEG compression at low-quality factors and for low-resolution images. The feature set reduction is also a challenging issue among existing detectors. In this article, a 19-dimensional feature set is analytically derived from image skewness and kurtosis histograms. This new feature set is exploited for the purpose of global median filtering forensics and verified with exhaustive experimental results. The efficacy of the method is tested on six popular databases (UCID, BOWS2, BOSSBase, NRCS, RAISE, and DID) and found that the new feature set uncovers filtering traces for moderate, low JPEG post-compression and low-resolution operation. Our proposed method yields lowest probability of error and largest area under the ROC curve for most of the test cases in comparison with previous approaches. Some novel test cases are introduced to thoroughly assess the benefits and limitations of the proposed method. The obtained results indicate that the proposed method would provide an important tool to the field of passive image forensics.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2803017374",
    "type": "article"
  },
  {
    "title": "Visual Content Recognition by Exploiting Semantic Feature Map with Attention and Multi-task Learning",
    "doi": "https://doi.org/10.1145/3231739",
    "publication_date": "2019-01-31",
    "publication_year": 2019,
    "authors": "Rui-Wei Zhao; Qi Zhang; Zuxuan Wu; Jianguo Li; Yu‐Gang Jiang",
    "corresponding_authors": "",
    "abstract": "Recent studies have shown that spatial relationships among objects are very important for visual recognition, since they can provide rich clues on object contexts within the images. In this article, we introduce a novel method to learn the Semantic Feature Map (SFM) with attention-based deep neural networks for image and video classification in an end-to-end manner, aiming to explicitly model the spatial object contexts within the images. In particular, we explicitly apply the designed gate units to the extracted object features for important objects selection and noise removal. These selected object features are then organized into the proposed SFM, which is a compact and discriminative representation with the spatial information among objects preserved. Finally, we employ either Fully Convolutional Networks (FCN) or Long-Short Term Memory (LSTM) as the classifiers on top of the SFM for content recognition. A novel multi-task learning framework with image classification loss, object localization loss, and grid labeling loss are also introduced to help better learn the model parameters. We conduct extensive evaluations and comparative studies to verify the effectiveness of the proposed approach on Pascal VOC 2007/2012 and MS-COCO benchmarks for image classification. In addition, the experimental results also show that the SFMs learned from the image domain can be successfully transferred to CCV and FCVID benchmarks for video classification.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2912684514",
    "type": "article"
  },
  {
    "title": "Exploring Disorder-Aware Attention for Clinical Event Extraction",
    "doi": "https://doi.org/10.1145/3372328",
    "publication_date": "2020-01-31",
    "publication_year": 2020,
    "authors": "Shweta Yadav; Pralay Ramteke; Asif Ekbal; Sriparna Saha; Pushpak Bhattacharyya",
    "corresponding_authors": "",
    "abstract": "Event extraction is one of the crucial tasks in biomedical text mining that aims to extract specific information concerning incidents embedded in the texts. In this article, we propose a deep learning framework that aims to identify the attributes (severity, course, temporal expression, and document creation time) associated with the medical concepts extracted from electronic medical records. The bi-directional long short-term memory network assisted by the attention mechanism is utilized to uncover the important aspects of the patient’s medical conditions. The attention mechanism specific to the medical disorder mention can focus on various parts of the sentence when different disorders are considered as input. The proposed methodology is evaluated on benchmark ShARe/CLEF eHealth Evaluation Lab 2014 shared task 2 datasets. In addition to the CLEF dataset, we also used the social media text, especially the medical blog posts. Experimental results of the proposed approach illustrate that our proposed approach achieves significant performance improvements over the state-of-the-art techniques and the highly competitive deep learning--based baseline methods.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W3016973506",
    "type": "article"
  },
  {
    "title": "SAMAF",
    "doi": "https://doi.org/10.1145/3380828",
    "publication_date": "2020-05-22",
    "publication_year": 2020,
    "authors": "Abraham Báez-Suárez; Nolan Shah; Juan A. Nolazco‐Flores; Shou‐Hsuan Stephen Huang; Omprakash Gnawali; Weidong Shi",
    "corresponding_authors": "",
    "abstract": "Audio fingerprinting techniques were developed to index and retrieve audio samples by comparing a content-based compact signature of the audio instead of the entire audio sample, thereby reducing memory and computational expense. Different techniques have been applied to create audio fingerprints; however, with the introduction of deep learning, new data-driven unsupervised approaches are available. This article presents Sequence-to-Sequence Autoencoder Model for Audio Fingerprinting (SAMAF), which improved hash generation through a novel loss function composed of terms: Mean Square Error, minimizing the reconstruction error; Hash Loss, minimizing the distance between similar hashes and encouraging clustering; and Bitwise Entropy Loss, minimizing the variation inside the clusters. The performance of the model was assessed with a subset of VoxCeleb1 dataset, a“speech in-the-wild” dataset. Furthermore, the model was compared against three baselines: Dejavu, a Shazam-like algorithm; Robust Audio Fingerprinting System (RAFS), a Bit Error Rate (BER) methodology robust to time-frequency distortions and coding/decoding transformations; and Panako, a constellation-based algorithm adding time-frequency distortion resilience. Extensive empirical evidence showed that our approach outperformed all the baselines in the audio identification task and other classification tasks related to the attributes of the audio signal with an economical hash size of either 128 or 256 bits for one second of audio.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W3017057111",
    "type": "article"
  },
  {
    "title": "3D Facial Similarity Measurement and Its Application in Facial Organization",
    "doi": "https://doi.org/10.1145/3397765",
    "publication_date": "2020-07-05",
    "publication_year": 2020,
    "authors": "Chenlei Lv; Zhongke Wu; Xingce Wang; Mingquan Zhou",
    "corresponding_authors": "",
    "abstract": "We propose a novel framework for 3D facial similarity measurement and its application in facial organization. The construction of the framework is based on Kendall shape space theory. Kendall shape space is a quotient space that is constructed by shape features. In Kendall shape space, the shape features can be measured and is robust to similarity transformations. In our framework, a 3D face is represented by the facial feature landmarks model (FFLM), which can be regarded as the facial shape features. We utilize the geodesic in Kendall shape space to represent the FFLM similarity measurement, which can be regarded as the 3D facial similarity measurement. The FFLM similarity measurement is robust to facial expressions, head poses, and partial facial data. In our experiments, we compute the distance between different FFLMs in two public facial databases: FRGC2.0 and BosphorusDB. On average, we achieve a rank-one facial recognition rate of 98%. Based on the similarity results, we propose a method to construct the facial organization. The facial organization is a hierarchical structure that is achieved from the facial clustering by FFLM similarity measurement. Based on the facial organization, the performance of face searching in a large facial database can be improved obviously (about 400% improvement in experiments).",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W3040129105",
    "type": "article"
  },
  {
    "title": "Spatio-temporal Segmentation Based Adaptive Compression of Dynamic Mesh Sequences",
    "doi": "https://doi.org/10.1145/3377475",
    "publication_date": "2020-02-29",
    "publication_year": 2020,
    "authors": "Guoliang Luo; Zhigang Deng; Xin Zhao; Xiaogang Jin; Wei Zeng; Wenqiang Xie; Hyewon Seo",
    "corresponding_authors": "",
    "abstract": "With the recent advances in data acquisition techniques, the compression of various dynamic mesh sequence data has become an important topic in the computer graphics community. In this article, we present a new spatio-temporal segmentation-based approach for the adaptive compression of the dynamic mesh sequences. Given an input dynamic mesh sequence, we first compute an initial temporal cut to obtain a small subsequence by detecting the temporal boundary of dynamic behavior. Then, we apply a two-stage vertex clustering on the resulting subsequence to classify the vertices into groups with optimal intra-affinities. After that, we design a temporal segmentation step based on the variations of the principal components within each vertex group prior to performing a PCA-based compression. Furthermore, we apply an extra step on the lossless compression of the PCA bases and coefficients to gain more storage saving. Our approach can adaptively determine the temporal and spatial segmentation boundaries to exploit both temporal and spatial redundancies. We have conducted extensive experiments on different types of 3D mesh animations with various segmentation configurations. Our comparative studies show the advantages of our approach for the compression of 3D mesh animations.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W3042628600",
    "type": "article"
  },
  {
    "title": "Spider",
    "doi": "https://doi.org/10.1145/2422956.2422963",
    "publication_date": "2013-02-01",
    "publication_year": 2013,
    "authors": "Naghmeh Khodabakhshi; Mohamed Hefeeda",
    "corresponding_authors": "",
    "abstract": "This article presents a novel content-based copy detection system for 3D videos. The system creates compact and robust depth and visual signatures from the 3D videos. Then, signature of a query video is compared against an indexed database of reference videos' signatures. The system returns a score, using both spatial and temporal characteristics of videos, indicating whether the query video matches any video in the reference video database, and in case of matching, which portion of the reference video matches the query video. Analysis shows that the system is efficient, both computationally and storage-wise. The system can be used, for example, by video content owners, video hosting sites, and third-party companies to find illegally copied 3D videos. We implemented Spider, a complete realization of the proposed system, and conducted rigorous experiments on it. Our experimental results show that the proposed system can achieve high accuracy in terms of precision and recall even if the 3D videos are subjected to several transformations at the same time. For example, the proposed system yields 100% precision and recall when copied videos are parts of original videos, and more than 90% precision and recall when copied videos are subjected to different individual transformations.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W1993229211",
    "type": "article"
  },
  {
    "title": "Memory recall based video search",
    "doi": "https://doi.org/10.1145/2534409",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "Jin Yuan; Yiliang Zhao; Huanbo Luan; Meng Wang; Tat‐Seng Chua",
    "corresponding_authors": "",
    "abstract": "We often remember images and videos that we have seen or recorded before but cannot quite recall the exact venues or details of the contents. We typically have vague memories of the contents, which can often be expressed as a textual description and/or rough visual descriptions of the scenes. Using these vague memories, we then want to search for the corresponding videos of interest. We call this “Memory Recall based Video Search” (MRVS). To tackle this problem, we propose a video search system that permits a user to input his/her vague and incomplete query as a combination of text query, a sequence of visual queries, and/or concept queries. Here, a visual query is often in the form of a visual sketch depicting the outline of scenes within the desired video, while each corresponding concept query depicts a list of visual concepts that appears in that scene. As the query specified by users is generally approximate or incomplete, we need to develop techniques to handle this inexact and incomplete specification by also leveraging on user feedback to refine the specification. We utilize several innovative approaches to enhance the automatic search. First, we employ a visual query suggestion model to automatically suggest potential visual features to users as better queries. Second, we utilize a color similarity matrix to help compensate for inexact color specification in visual queries. Third, we leverage on the ordering of visual queries and/or concept queries to rerank the results by using a greedy algorithm. Moreover, as the query is inexact and there is likely to be only one or few possible answers, we incorporate an interactive feedback loop to permit the users to label related samples which are visually similar or semantically close to the relevant sample. Based on the labeled samples, we then propose optimization algorithms to update visual queries and concept weights to refine the search results. We conduct experiments on two large-scale video datasets: TRECVID 2010 and YouTube. The experimental results demonstrate that our proposed system is effective for MRVS tasks.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2140222244",
    "type": "article"
  },
  {
    "title": "Image Retrieval for Complex Queries Using Knowledge Embedding",
    "doi": "https://doi.org/10.1145/3375786",
    "publication_date": "2020-02-29",
    "publication_year": 2020,
    "authors": "Chandramani Chaudhary; Poonam Goyal; Navneet Goyal; Yi‐Ping Phoebe Chen",
    "corresponding_authors": "",
    "abstract": "With the increase in popularity of image-based applications, users are retrieving images using more sophisticated and complex queries. We present three types of complex queries, namely, long, ambiguous, and abstract. Each type of query has its own characteristics/complexities and thus leads to imprecise and incomplete image retrieval. Existing methods for image retrieval are unable to deal with the high complexity of such queries. Search engines need to integrate their image retrieval process with knowledge to obtain rich semantics for effective retrieval. We propose a framework, Image Retrieval using Knowledge Embedding (ImReKE), for embedding knowledge with images and queries, allowing retrieval approaches to understand the context of queries and images in a better way. ImReKE (IR_Approach, Knowledge_Base) takes two inputs, namely, an image retrieval approach and a knowledge base. It selects quality concepts (concepts that possess properties such as rarity, newness , etc.) from the knowledge base to provide rich semantic representations for queries and images to be leveraged by the image retrieval approach. For the first time, an effective knowledge base that exploits both the visual and textual information of concepts has been developed. Our extensive experiments demonstrate that the proposed framework improves image retrieval significantly for all types of complex queries. The improvement is remarkable in the case of abstract queries, which have not yet been dealt with explicitly in the existing literature. We also compare the quality of our knowledge base with the existing text-based knowledge bases, such as ConceptNet, ImageNet, and the like.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W3019359265",
    "type": "article"
  },
  {
    "title": "Image to Modern Chinese Poetry Creation via a Constrained Topic-aware Model",
    "doi": "https://doi.org/10.1145/3381858",
    "publication_date": "2020-05-22",
    "publication_year": 2020,
    "authors": "Lingxiang Wu; Min Xu; Shengsheng Qian; Jianwei Cui",
    "corresponding_authors": "",
    "abstract": "Artificial creativity has attracted increasing research attention in the field of multimedia and artificial intelligence. Despite the promising work on poetry/painting/music generation, creating modern Chinese poetry from images, which can significantly enrich the functionality of photo-sharing platforms, has rarely been explored. Moreover, existing generation models cannot tackle three challenges in this task: (1) Maintaining semantic consistency between images and poems; (2) preventing topic drift in the generation; (3) avoidance of certain words appearing frequently. These three points are even common challenges in other sequence generation tasks. In this article, we propose a Constrained Topic-aware Model (CTAM) to create modern Chinese poetries from images regarding the challenges above. Without image-poetry paired dataset, we construct a visual semantic vector to embed visual contents via image captions. For the topic-drift problem, we propose a topic-aware poetry generation model. Additionally, we design an Anti-frequency Decoding (AFD) scheme to constrain high-frequency characters in the generation. Experimental results show that our model achieves promising performance and is effective in poetry’s readability and semantic consistency.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W3029342055",
    "type": "article"
  },
  {
    "title": "PPNet",
    "doi": "https://doi.org/10.1145/3379983",
    "publication_date": "2020-05-22",
    "publication_year": 2020,
    "authors": "Kutalmış Akpınar; Kien A. Hua",
    "corresponding_authors": "",
    "abstract": "Software-defined networking introduces opportunities to optimize the Internet Service Provider’s network and to improve client experience for the Video-on-Demand applications. Recent studies on SDN frameworks show that traffic engineering methods allow a fair share of bandwidth between adaptive video streaming clients. Additionally, ISPs can make better estimations of bandwidth and contribute to the bitrate selection for the clients. This study focuses on another aspect of network assistance in video delivery: CDN server selection. In a typical framework where the ISP contributes to the CDN selection, the video provider and the network provider interfaces are merged together. Clients connect to the ISP to get the best CDN server candidate for a given video. This exposes client requests to the ISP. However, video providers have been investing large resources for encrypted video provisioning to preserve their client’s information from third parties, especially network providers. The typical approach is not practical due to privacy concerns. In this study, we present a framework called PPNet to allow CDN-ISP collaboration while preventing the ISP’s access to the video request and availability information. Our framework introduces an isolation between the video provider’s and the ISP’s web interfaces. Clients connect to both of the interfaces and deliver information on a need-to-know basis. As a second contribution, PPNet introduces a practical optimization method for CDN selection. Real-time data collection capabilities of a typical OpenFlow network is used as the input for optimization. Congestion-awareness has been the priority. To adapt for changing network conditions, capability of utilizing multiple servers simultaneously for a single video is introduced. Instead of directing each video client into a CDN node, the proposed system performs request routing per video segment. Finally, we present a system prototype of PPNet and show that our multiple-host adaptive streaming method introduces a significant improvement in quality of experience when compared to the state of the art.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W3032576743",
    "type": "article"
  },
  {
    "title": "Differentially Private Tensor Train Deep Computation for Internet of Multimedia Things",
    "doi": "https://doi.org/10.1145/3421276",
    "publication_date": "2020-10-31",
    "publication_year": 2020,
    "authors": "Nicholaus J. Gati; Laurence T. Yang; Jun Feng; Yijun Mo; Mamoun Alazab",
    "corresponding_authors": "",
    "abstract": "The significant growth of the Internet of Things (IoT) takes a key and active role in healthcare, smart homes, smart manufacturing, and wearable gadgets. Due to complexness and difficulty in processing multimedia data, the IoT based scheme, namely Internet of Multimedia Things (IoMT) exists that is specialized for services and applications based on multimedia data. However, IoMT generated data are facing major processing and privacy issues. Therefore, tensor-based deep computation models proved a better platform to process IoMT generated data. A differentially private deep computation method working in the tensor space can attest to its efficacy for IoMT. Nevertheless, the deep computation model comprises a multitude of parameters; thus, it requires large units of memory and expensive computing units with higher performance levels, which hinders its performance for IoMT. Motivated by this, therefore, the paper proposes a deep private tensor train autoencoder (dPTTAE) technique to deal with IoMT generated data. Notably, the compression of weight tensors to manageable tensor train format is achieved through Tensor Train (TT) network. Moreover, TT format parameters are trained through higher-order back-propagation and gradient descent. We applied dPTTAE on three representative datasets. Comprehensive experimental evaluations and theoretical analysis show that dPTTAE enhances training time efficiency, and greatly improve memory utilization efficiency, attesting its potential for IoMT.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W3117754898",
    "type": "article"
  },
  {
    "title": "Improved Jitter Buffer Management for WebRTC",
    "doi": "https://doi.org/10.1145/3410449",
    "publication_date": "2021-02-28",
    "publication_year": 2021,
    "authors": "Yusuf Cinar; Peter Počta; Desmond Chambers; Hugh Melvin",
    "corresponding_authors": "",
    "abstract": "This work studies the jitter buffer management algorithm for Voice over IP in WebRTC. In particular, it details the core concepts of WebRTC’s jitter buffer management. Furthermore, it investigates how jitter buffer management algorithm behaves under network conditions with packet bursts. It also proposes an approach, different from the default WebRTC algorithm, to avoid distortions that occur under such network conditions. Under packet bursts, when the packet buffer becomes full, the WebRTC jitter buffer algorithm may discard all the packets in the buffer to make room for incoming packets. The proposed approach offers a novel strategy to minimize the number of packets discarded in the presence of packet bursts. Therefore, voice quality as perceived by the user is improved. ITU-T Rec. P.863, which also confirms the improvement, is employed to objectively evaluate the listening quality.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W3152567458",
    "type": "article"
  },
  {
    "title": "Spatial-temporal Regularized Multi-modality Correlation Filters for Tracking with Re-detection",
    "doi": "https://doi.org/10.1145/3430257",
    "publication_date": "2021-05-29",
    "publication_year": 2021,
    "authors": "Xiangyuan Lan; Zifei Yang; Wei Zhang; Pong C. Yuen",
    "corresponding_authors": "",
    "abstract": "The development of multi-spectrum image sensing technology has brought great interest in exploiting the information of multiple modalities (e.g., RGB and infrared modalities) for solving computer vision problems. In this article, we investigate how to exploit information from RGB and infrared modalities to address two important issues in visual tracking: robustness and object re-detection. Although various algorithms that attempt to exploit multi-modality information in appearance modeling have been developed, they still face challenges that mainly come from the following aspects: (1) the lack of robustness to deal with large appearance changes and dynamic background, (2) failure in re-capturing the object when tracking loss happens, and (3) difficulty in determining the reliability of different modalities. To address these issues and perform effective integration of multiple modalities, we propose a new tracking-by-detection algorithm called Adaptive Spatial-temporal Regulated Multi-Modality Correlation Filter. Particularly, an adaptive spatial-temporal regularization is imposed into the correlation filter framework in which the spatial regularization can help to suppress effect from the cluttered background while the temporal regularization enables the adaptive incorporation of historical appearance cues to deal with appearance changes. In addition, a dynamic modality weight learning algorithm is integrated into the correlation filter training, which ensures that more reliable modalities gain more importance in target tracking. Experimental results demonstrate the effectiveness of the proposed method.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W3166542104",
    "type": "article"
  },
  {
    "title": "A High-Fidelity and Low-Interaction-Delay Screen Sharing System",
    "doi": "https://doi.org/10.1145/2897395",
    "publication_date": "2016-05-20",
    "publication_year": 2016,
    "authors": "Dan Miao; Jingjing Fu; Yan Lu; Shipeng Li; Chang Wen Chen",
    "corresponding_authors": "",
    "abstract": "The pervasive computing environment and wide network bandwidth provide users more opportunities to share screen content among multiple devices. In this article, we introduce a remote display system to enable screen sharing among multiple devices with high fidelity and responsive interaction. In the developed system, the frame-level screen content is compressed and transmitted to the client side for screen sharing, and the instant control inputs are simultaneously transmitted to the server side for interaction. Even if the screen responds immediately to the control messages and updates at a high frame rate on the server side, it is difficult to update the screen content with low delay and high frame rate in the client side due to non-negligible time consumption on the whole screen frame compression, transmission, and display buffer updating. To address this critical problem, we propose a layered structure for screen coding and rendering to deliver diverse screen content to the client side with an adaptive frame rate. More specifically, the interaction content with small region screen update is compressed by a blockwise screen codec and rendered at a high frame rate to achieve smooth interaction, while the natural video screen content is compressed by standard video codec and rendered at a regular frame rate for a smooth video display. Experimental results with real applications demonstrate that the proposed system can successfully reduce transmission bandwidth cost and interaction delay during screen sharing. Especially for user interaction in small regions, the proposed system can achieve a higher frame rate than most previous counterparts.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2397480939",
    "type": "article"
  },
  {
    "title": "MobiCoop",
    "doi": "https://doi.org/10.1145/2957752",
    "publication_date": "2016-08-24",
    "publication_year": 2016,
    "authors": "Bruno M. C. Silva; Joel J. P. C. Rodrigues; Neeraj Kumar; Mário Lemes Proença; Guangjie Han",
    "corresponding_authors": "",
    "abstract": "Network architectures based on mobile devices and wireless communications present several constraints (e.g., processor, energy storage, bandwidth, etc.) that affect the overall network performance. Cooperation strategies have been considered as a solution to address these network limitations. In the presence of unstable network infrastructures, mobile nodes cooperate with each other, forwarding data and performing other specific network functionalities. This article proposes a generalized incentive-based cooperation solution for mobile services and applications called MobiCoop. This reputation-based scheme includes an application framework for mobile applications that uses a Web service to handle all the nodes reputation and network permissions. The main goal of MobiCoop is to provide Internet services to mobile devices without network connectivity through cooperation with neighbor devices. The article includes a performance evaluation study of MobiCoop considering both a real scenario (using a prototype) and a simulation-based study. Results show that the proposed approach provides network connectivity independency to users with mobile apps when Internet connectivity is unavailable. Then, it is concluded that MobiCoop improved significantly the overall system performance and the service provided for a given mobile application.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2514254082",
    "type": "article"
  },
  {
    "title": "Approximate Asymmetric Search for Binary Embedding Codes",
    "doi": "https://doi.org/10.1145/2990504",
    "publication_date": "2016-10-25",
    "publication_year": 2016,
    "authors": "Chih‐Yi Chiu; Yu-Cyuan Liou; Amorntip Prayoonwong",
    "corresponding_authors": "",
    "abstract": "In this article, we propose a method of approximate asymmetric nearest-neighbor search for binary embedding codes. The asymmetric distance takes advantage of less information loss at the query side. However, calculating asymmetric distances through exhaustive search is prohibitive in a large-scale dataset. We present a novel method, called multi-index voting, that integrates the multi-index hashing technique with a voting mechanism to select appropriate candidates and calculate their asymmetric distances. We show that the candidate selection scheme can be formulated as the tail of the binomial distribution function. In addition, a binary feature selection method based on minimal quantization error is proposed to address the memory insufficiency issue and improve the search accuracy. Substantial experimental evaluations were made to demonstrate that the proposed method can yield an approximate accuracy to the exhaustive search method while significantly accelerating the runtime. For example, one result shows that in a dataset of one billion 256-bit binary codes, examining only 0.5% of the dataset, can reach 95--99% close accuracy to the exhaustive search method and accelerate the search by 73--128 times. It also demonstrates an excellent tradeoff between the search accuracy and time efficiency compared to the state-of-the-art nearest-neighbor search methods. Moreover, the proposed feature selection method shows its effectiveness and improves the accuracy up to 8.35% compared with other feature selection methods.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2536218508",
    "type": "article"
  },
  {
    "title": "Design, Implementation, and Measurement of a Crowdsourcing-Based Content Distribution Platform",
    "doi": "https://doi.org/10.1145/2978655",
    "publication_date": "2016-11-08",
    "publication_year": 2016,
    "authors": "Yipeng Zhou; Liang Chen; Jing Mi; Shenglong Zou; T. B. Richard",
    "corresponding_authors": "",
    "abstract": "Content distribution, especially the distribution of video content, unavoidably consumes bandwidth resources heavily. Internet content providers invest heavily in purchasing content distribution network (CDN) services. By deploying tens of thousands of edge servers close to end users, CDN companies are able to distribute content efficiently and effectively, but at considerable cost. Thus, it is of great importance to develop a new system that distributes content at a lower cost but comparable service quality. In lieu of expensive CDN systems, we implement a crowdsourcing-based content distribution system, Thunder Crystal, by renting bandwidth for content upload/download and storage for content cache from agents. This is a large-scale system with tens of thousands of agents, whose resources significantly amplify Thunder Crystal’s content distribution capacity. The involved agents are either from ordinary Internet users or enterprises. Monetary rewards are paid to agents based on their upload traffic so as to motivate them to keep contributing resources. As far as we know, this is a novel system that has not been studied or implemented before. This article introduces the design principles and implementation details before presenting the measurement study. In summary, with the help of agent devices, Thunder Crystal is able to reduce the content distribution cost by one half and amplify the content distribution capacity by 11 to 15 times.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2549486930",
    "type": "article"
  },
  {
    "title": "Mimicking Individual Media Quality Perception with Neural Network based Artificial Observers",
    "doi": "https://doi.org/10.1145/3464393",
    "publication_date": "2022-01-27",
    "publication_year": 2022,
    "authors": "Lohic Fotio Tiotsop; Tomas Mizdos; Marcus Barkowsky; Peter Počta; Antonio Servetti; Enrico Masala",
    "corresponding_authors": "",
    "abstract": "The media quality assessment research community has traditionally been focusing on developing objective algorithms to predict the result of a typical subjective experiment in terms of Mean Opinion Score (MOS) value. However, the MOS, being a single value, is insufficient to model the complexity and diversity of human opinions encountered in an actual subjective experiment. In this work we propose a complementary approach for objective media quality assessment that attempts to more closely model what happens in a subjective experiment in terms of single observers and, at the same time, we perform a qualitative analysis of the proposed approach while highlighting its suitability. More precisely, we propose to model, using neural networks (NNs) , the way single observers perceive media quality. Once trained, these NNs, one for each observer, are expected to mimic the corresponding observer in terms of quality perception. Then, similarly to a subjective experiment, such NNs can be used to simulate the users’ single opinions, which can be later aggregated by means of different statistical indicators such as average, standard deviation, quantiles, etc. Unlike previous approaches that consider subjective experiments as a black box providing reliable ground truth data for training, the proposed approach is able to consider human factors by analyzing and weighting individual observers. Such a model may therefore implicitly account for users’ expectations and tendencies, that have been shown in many studies to significantly correlate with visual quality perception. Furthermore, our proposal also introduces and investigates an index measuring how much inconsistency there would be if an observer was asked to rate many times the same stimulus. Simulation experiments conducted on several datasets demonstrate that the proposed approach can be effectively implemented in practice and thus yielding a more complete objective assessment of end users’ quality of experience.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3175982415",
    "type": "article"
  },
  {
    "title": "Fine-Grained Adversarial Semi-Supervised Learning",
    "doi": "https://doi.org/10.1145/3485473",
    "publication_date": "2022-01-25",
    "publication_year": 2022,
    "authors": "Daniele Mugnai; Federico Pernici; Francesco Turchini; Alberto Del Bimbo",
    "corresponding_authors": "",
    "abstract": "In this article, we exploit Semi-Supervised Learning ( SSL ) to increase the amount of training data to improve the performance of Fine-Grained Visual Categorization ( FGVC ). This problem has not been investigated in the past in spite of prohibitive annotation costs that FGVC requires. Our approach leverages unlabeled data with an adversarial optimization strategy in which the internal features representation is obtained with a second-order pooling model. This combination allows one to back-propagate the information of the parts, represented by second-order pooling, onto unlabeled data in an adversarial training setting. We demonstrate the effectiveness of the combined use by conducting experiments on six state-of-the-art fine-grained datasets, which include Aircrafts, Stanford Cars, CUB-200-2011, Oxford Flowers, Stanford Dogs, and the recent Semi-Supervised iNaturalist-Aves. Experimental results clearly show that our proposed method has better performance than the only previous approach that examined this problem; it also obtained higher classification accuracy with respect to the supervised learning methods with which we compared.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4206897074",
    "type": "article"
  },
  {
    "title": "PAINT: Photo-realistic Fashion Design Synthesis",
    "doi": "https://doi.org/10.1145/3545610",
    "publication_date": "2022-06-30",
    "publication_year": 2022,
    "authors": "Xiaoling Gu; Jie Huang; Yongkang Wong; Jun Yu; Jianping Fan; Pai Peng; Mohan Kankanhalli",
    "corresponding_authors": "",
    "abstract": "In this article, we investigate a new problem of generating a variety of multi-view fashion designs conditioned on a human pose and texture examples of arbitrary sizes, which can replace the repetitive and low-level design work for fashion designers. To solve this challenging multi-modal image translation problem, we propose a novel Photo-reAlistic fashIon desigN synThesis (PAINT) framework, which decomposes the framework into three manageable stages. In the first stage, we employ a Layout Generative Network (LGN) to transform an input human pose into a series of person semantic layouts. In the second stage, we propose a Texture Synthesis Network (TSN) to synthesize textures on all transformed semantic layouts. Specifically, we design a novel attentive texture transfer mechanism for precisely expanding texture patches to the irregular clothing regions of the target fashion designs. In the third stage, we leverage an Appearance Flow Network (AFN) to generate the fashion design images of other viewpoints from a single-view observation by learning 2D multi-scale appearance flow fields. Experimental results demonstrate that our method is capable of generating diverse photo-realistic multi-view fashion design images with fine-grained appearance details conditioned on the provided multiple inputs. The source code and trained models are available at https://github.com/gxl-groups/PAINT .",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4283731241",
    "type": "article"
  },
  {
    "title": "Resolution Identification of Encrypted Video Streaming Based on HTTP/2 Features",
    "doi": "https://doi.org/10.1145/3551891",
    "publication_date": "2022-07-28",
    "publication_year": 2022,
    "authors": "Hua Wu; Xin Li; Gang Wang; Guang Cheng; Xiaoyan Hu",
    "corresponding_authors": "",
    "abstract": "With the inevitable dominance of video traffic on the Internet, Internet service providers (ISP) are striving to deliver video streaming with high quality. Video resolution, as a direct reflection of video quality, is a key factor of the video quality of experience (QoE). Since the displayed information of video cannot be observed by ISPs, ISPs can only measure the video resolution from traffic. However, with HTTP/2 being gradually adopted in video services, the multiplexing feature of HTTP/2 allows audio and video chunks to be mixed during transmission, making existing monitoring approaches unusable. In this article, we propose a method called H2CI to monitor resolution for adaptive encrypted video traffic under HTTP/2. We consider the size of the mixed data for identification. Specifically, H2CI consists of a length restoration method to extract restored fingerprints and a fingerprint-matching method for fine-grained resolution identification. The experimental results show that H2CI can achieve more than 98% accuracy for fine-grained resolution identification. Our method can be effectively applied to infer the adaptation behavior of encrypted video streaming and monitor the QoE of video services under HTTP/2.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4288084520",
    "type": "article"
  },
  {
    "title": "Pose- and Attribute-consistent Person Image Synthesis",
    "doi": "https://doi.org/10.1145/3554739",
    "publication_date": "2022-08-04",
    "publication_year": 2022,
    "authors": "Cheng Xu; Zejun Chen; Jiajie Mai; Xuemiao Xu; Shengfeng He",
    "corresponding_authors": "",
    "abstract": "Person Image Synthesis aims at transferring the appearance of the source person image into a target pose. Existing methods cannot handle large pose variations and therefore suffer from two critical problems: (1) synthesis distortion due to the entanglement of pose and appearance information among different body components and (2) failure in preserving original semantics (e.g., the same outfit). In this article, we explicitly address these two problems by proposing a Pose- and Attribute-consistent Person Image Synthesis Network (PAC-GAN). To reduce pose and appearance matching ambiguity, we propose a component-wise transferring model consisting of two stages. The former stage focuses only on synthesizing target poses, while the latter renders target appearances by explicitly transferring the appearance information from the source image to the target image in a component-wise manner. In this way, source-target matching ambiguity is eliminated due to the component-wise disentanglement of pose and appearance synthesis. Second, to maintain attribute consistency, we represent the input image as an attribute vector and impose a high-level semantic constraint using this vector to regularize the target synthesis. Extensive experimental results on the DeepFashion dataset demonstrate the superiority of our method over the state of the art, especially for maintaining pose and attribute consistencies under large pose variations.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4289766616",
    "type": "article"
  },
  {
    "title": "Full-body Human Motion Reconstruction with Sparse Joint Tracking Using Flexible Sensors",
    "doi": "https://doi.org/10.1145/3564700",
    "publication_date": "2022-09-29",
    "publication_year": 2022,
    "authors": "Xiaowei Chen; Xiao Feng Jiang; Lishuang Zhan; Shihui Guo; Qunsheng Ruan; Guoliang Luo; Minghong Liao; Yipeng Qin",
    "corresponding_authors": "",
    "abstract": "Human motion tracking is a fundamental building block for various applications including computer animation, human-computer interaction, healthcare, and so on. To reduce the burden of wearing multiple sensors, human motion prediction from sparse sensor inputs has become a hot topic in human motion tracking. However, such predictions are non-trivial as (i) the widely adopted data-driven approaches can easily collapse to average poses, and (ii) the predicted motions contain unnatural jitters. In this work, we address the aforementioned issues by proposing a novel framework which can accurately predict the human joint moving angles from the signals of only four flexible sensors, thereby achieving the tracking of human joints in multi-degrees of freedom. Specifically, we mitigate the collapse to average poses by implementing the model with a Bi-LSTM neural network that makes full use of short-time sequence information; we reduce jitters by adding a median pooling layer to the network, which smooths consecutive motions. Although being bio-compatible and ideal for improving the wearing experience, the flexible sensors are prone to aging which increases prediction errors. Observing that the aging of flexible sensors usually results in drifts of their resistance ranges, we further propose a novel dynamic calibration technique to rescale sensor ranges, which further improves the prediction accuracy. Experimental results show that our method achieves a low and stable tracking error of 4.51 degrees across different motion types with only four sensors.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4297999581",
    "type": "article"
  },
  {
    "title": "A Differentiable Parallel Sampler for Efficient Video Classification",
    "doi": "https://doi.org/10.1145/3569584",
    "publication_date": "2022-10-27",
    "publication_year": 2022,
    "authors": "Xiaohan Wang; Linchao Zhu; Fei Wu; Yi Yang",
    "corresponding_authors": "",
    "abstract": "It is crucial to sample a small portion of relevant frames for efficient video classification. The existing methods mainly develop hand-designed sampling strategies or learn sequential selection policies. However, there are two challenges to be solved. First, hand-designed sampling strategies are intrinsically non-adaptive to different video backbones. Second, sequential frame selection policies ignore temporal relations among all video frames. The sequential selection process also hinders the application of these video samplers in speed-critical systems. In this article, we propose a differentiable parallel video sampling network (PSN) to tackle the aforementioned challenges, First, we optimize the video sampler with a differentiable surrogate loss, allowing to dynamically learn the sampler with the cooperation from the video classification model. Our sampler considers the feedback from all frames jointly, eliminating the learning difficulties of sequential decision making. The learning process is fully gradient-based, making the sampler be learned efficiently. Our video sampler can assess a set of frames swiftly and determine the importance of each frame in parallel. Second, we propose to model the inter-relation among contextual frames, which encourages the sampler to select frames based on a comprehensive inspection of the entire video. We observe that a simple context relation mining instantiation would significantly improve the classification performance. The experimental results on three standard video recognition benchmarks demonstrate the efficacy and efficiency of our framework.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4307639905",
    "type": "article"
  },
  {
    "title": "A Novel Lightweight Audio-visual Saliency Model for Videos",
    "doi": "https://doi.org/10.1145/3576857",
    "publication_date": "2022-12-16",
    "publication_year": 2022,
    "authors": "Dandan Zhu; Xuan Shao; Qiangqiang Zhou; Xiongkuo Min; Guangtao Zhai; Xiaokang Yang",
    "corresponding_authors": "",
    "abstract": "Audio information has not been considered an important factor in visual attention models regardless of many psychological studies that have shown the importance of audio information in the human visual perception system. Since existing visual attention models only utilize visual information, their performance is limited but also requires high-computational complexity due to the limited information available. To overcome these problems, we propose a lightweight audio-visual saliency (LAVS) model for video sequences. To the best of our knowledge, this article is the first trial to utilize audio cues for an efficient deep-learning model for the video saliency estimation. First, spatial-temporal visual features are extracted by the lightweight receptive field block (RFB) with the bidirectional ConvLSTM units. Then, audio features are extracted by using an improved lightweight environment sound classification model. Subsequently, deep canonical correlation analysis (DCCA) aims at capturing the correspondence between audio and spatial-temporal visual features, thus obtaining a spatial-temporal auditory saliency. Lastly, the spatial-temporal visual and auditory saliency are fused to obtain the audio-visual saliency map. Extensive comparative experiments and ablation studies validate the performance of the LAVS model in terms of effectiveness and complexity.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4313476633",
    "type": "article"
  },
  {
    "title": "Feedforward and Feedback Modulations Based Foveated JND Estimation for Images",
    "doi": "https://doi.org/10.1145/3579094",
    "publication_date": "2022-09-30",
    "publication_year": 2022,
    "authors": "Haibing Yin; Hongkui Wang; Li Yu; Junhui Liang; Guangtao Zhai",
    "corresponding_authors": "",
    "abstract": "The just noticeable difference (JND) reveals the key characteristic of visual perception, which has been widely used in many perception-based image and video applications. Nevertheless, the modulatory mechanism of the human visual system (HVS) has not been fully exploited in JND threshold estimation, which results in the existing JND models not being accurate enough. In this article, by analyzing the feedforward and feedback modulatory behaviors in the HVS, an enhanced foveated JND (FJND) estimation model is proposed considering modulatory effects and masking effects in visual perception. The contributions of this article are mainly twofold. On the one hand, by analyzing the modulatory behaviors in the HVS, the modulatory mechanism is incorporated into JND estimation and a hierarchical modulation-based JND estimation framework is proposed for the first time. On the other hand, according to the response characteristics of visual neurons, modulatory effects on visual sensitivity are formulated as several modulatory factors to modulate the estimated JND threshold properly. Compared with existing models, the proposed model is developed in view of not only the masking effects but also the modulatory effects, which makes our model more consistent with the HVS. For different complex input images, experimental results show that the proposed FJND model tolerates more distortion at the same perceptual quality in comparison with other existing models.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4313590607",
    "type": "article"
  },
  {
    "title": "Bottom-up and Top-down Object Inference Networks for Image Captioning",
    "doi": "https://doi.org/10.1145/3580366",
    "publication_date": "2022-09-30",
    "publication_year": 2022,
    "authors": "Yingwei Pan; Yehao Li; Ting Yao; Tao Mei",
    "corresponding_authors": "",
    "abstract": "A bottom-up and top-down attention mechanism has led to the revolutionizing of image captioning techniques, which enables object-level attention for multi-step reasoning over all the detected objects. However, when humans describe an image, they often apply their own subjective experience to focus on only a few salient objects that are worthy of mention, rather than all objects in this image. The focused objects are further allocated in linguistic order, yielding the “object sequence of interest” to compose an enriched description. In this work, we present the Bottom-up and Top-down Object inference Network (BTO-Net), which novelly exploits the object sequence of interest as top-down signals to guide image captioning. Technically, conditioned on the bottom-up signals (all detected objects), an LSTM-based object inference module is first learned to produce the object sequence of interest, which acts as the top-down prior to mimic the subjective experience of humans. Next, both of the bottom-up and top-down signals are dynamically integrated via an attention mechanism for sentence generation. Furthermore, to prevent the cacophony of intermixed cross-modal signals, a contrastive learning-based objective is involved to restrict the interaction between bottom-up and top-down signals, and thus leads to reliable and explainable cross-modal reasoning. Our BTO-Net obtains competitive performances on the COCO benchmark, in particular, 134.1% CIDEr on the COCO Karpathy test split. Source code is available at https://github.com/YehLi/BTO-Net .",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4317435799",
    "type": "article"
  },
  {
    "title": "CD <sup>2</sup> : Fine-grained 3D Mesh Reconstruction with Twice Chamfer Distance",
    "doi": "https://doi.org/10.1145/3582694",
    "publication_date": "2023-02-02",
    "publication_year": 2023,
    "authors": "Rongfei Zeng; Mai Su; Ruiyun Yu; Xingwei Wang",
    "corresponding_authors": "",
    "abstract": "Monocular 3D reconstruction is to reconstruct the shape of object and its other information from a single RGB image. In 3D reconstruction, polygon mesh, with detailed surface information and low computational cost, is the most prevalent expression form obtained from deep learning models. However, the state-of-the-art schemes fail to directly generate well-structured meshes, and we identify that most meshes have severe Vertices Clustering (VC) and Illegal Twist (IT) problems. By analyzing the mesh deformation process, we pinpoint that the inappropriate usage of Chamfer Distance (CD) loss is a root cause of VC and IT problems in deep learning model. In this article, we initially demonstrate these two problems induced by CD loss with visual examples and quantitative analyses. Then, we propose a fine-grained reconstruction method CD 2 by employing Chamfer distance twice to perform a plausible and adaptive deformation. Extensive experiments on two 3D datasets and comparisons with five latest schemes demonstrate that our CD 2 directly generates a well-structured mesh and outperforms others in terms of several quantitative metrics.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4319034720",
    "type": "article"
  },
  {
    "title": "D <sup>3</sup> T-GAN: Data-Dependent Domain Transfer GANs for Image Generation with Limited Data",
    "doi": "https://doi.org/10.1145/3576858",
    "publication_date": "2023-02-06",
    "publication_year": 2023,
    "authors": "Xintian Wu; Huanyu Wang; Yiming Wu; Xi Li",
    "corresponding_authors": "",
    "abstract": "As an important and challenging problem, image generation with limited data aims at generating realistic images through training a GAN model given few samples. A typical solution is to transfer a well-trained GAN model from a data-rich source domain to the data-deficient target domain. In this paper, we propose a novel self-supervised transfer scheme termed D 3 T-GAN, addressing the cross-domain GANs transfer in limited image generation. Specifically, we design two individual strategies to transfer knowledge between generators and discriminators, respectively. To transfer knowledge between generators, we conduct a data-dependent transformation, which projects target samples into the latent space of source generator and reconstructs them back. Then, we perform knowledge transfer from transformed samples to generated samples. To transfer knowledge between discriminators, we design a multi-level discriminant knowledge distillation from the source discriminator to the target discriminator on both the real and fake samples. Extensive experiments show that our method improves the quality of generated images and achieves the state-of-the-art FID scores on commonly used datasets.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4319316769",
    "type": "article"
  },
  {
    "title": "Complementary Feature Pyramid Network for Object Detection",
    "doi": "https://doi.org/10.1145/3584362",
    "publication_date": "2023-02-15",
    "publication_year": 2023,
    "authors": "Jin Xie; Yanwei Pang; Jing Pan; Jing Nie; Jiale Cao; Jungong Han",
    "corresponding_authors": "",
    "abstract": "The way of constructing a robust feature pyramid is crucial for object detection. However, existing feature pyramid methods, which aggregate multi-level features by using element-wise sum or concatenation, are inefficient to construct a robust feature pyramid. The reason is that these methods cannot be effective in discriminating the relevant semantics of objects. In this article, we propose a Complementary Feature Pyramid Network (CFPN) to aggregate multi-level features selectively and efficiently by exploring complementary information between multi-level features. Specifically, a Spatial Complementary Module (SCM) and a Channel Complementary Module (CCM) are designed and embedded in CFPN to enhance useful information and suppress irrelevant information during feature fusions along spatial and channel dimensions, respectively. CFPN is a generic feature extractor, as evidenced by its seamless integration into single-stage, two-stage, and end-to-end object detectors. Experiments conducted on the COCO and Pascal VOC datasets demonstrate that integrating our CFPN into RetinaNet, Faster RCNN, Cascade RCNN, and Sparse RCNN obtains consistent performance improvements with negligible overheads. Code and models are available at: https://github.com/VIPLab-CQU/CFPN .",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4320892151",
    "type": "article"
  },
  {
    "title": "Video Captioning by Learning from Global Sentence and Looking Ahead",
    "doi": "https://doi.org/10.1145/3587252",
    "publication_date": "2023-03-09",
    "publication_year": 2023,
    "authors": "Tian-Zi Niu; Zhen-Duo Chen; Xin Luo; Peng-Fei Zhang; Zi Huang; Xin-Shun Xu",
    "corresponding_authors": "",
    "abstract": "Video captioning aims to automatically generate natural language sentences describing the content of a video. Although encoder-decoder-based models have achieved promising progress, it is still very challenging to effectively model the linguistic behavior of humans in generating video captions. In this paper, we propose a novel video captioning model by learning from gLobal sEntence and looking AheaD, LEAD for short. Specifically, LEAD consists of two modules: a Vision Module (VM) and a Language Module (LM) . Thereinto, VM is a novel attention network, which can map visual features to high-level language space and model entire sentences explicitly. LM can not only effectively make use of the information of the previous sequence when generating the current word, but also have a look at the future word. Therefore, based on VM and LM, LEAD can obtain global sentence information and future word information to make video captioning more like a fill-in-the-blank task than a word-by-word sentence generation. In addition, we also propose an autonomous strategy and a multi-stage training scheme to optimize the model, which can mitigate the problem of information leakage. Extensive experiments show that LEAD outperforms some state-of-the-art methods on MSR-VTT, MSVD, and VATEX, demonstrating the effectiveness of the proposed approach in video captioning. In addition, we release the code of our proposed model to be publicly available. 1",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4323662761",
    "type": "article"
  },
  {
    "title": "Weakly Supervised Hashing with Reconstructive Cross-modal Attention",
    "doi": "https://doi.org/10.1145/3589185",
    "publication_date": "2023-04-08",
    "publication_year": 2023,
    "authors": "Yongchao Du; Min Wang; Zhenbo Lu; Wengang Zhou; Houqiang Li",
    "corresponding_authors": "",
    "abstract": "On many popular social websites, images are usually associated with some meta-data such as textual tags, which involve semantic information relevant to the image and can be used to supervise the representation learning for image retrieval. However, these user-provided tags are usually polluted by noise, therefore the main challenge lies in mining the potential useful information from those noisy tags. Many previous works simply treat different tags equally to generate supervision, which will inevitably distract the network learning. To this end, we propose a new framework, termed as Weakly Supervised Hashing with Reconstructive Cross-modal Attention (WSHRCA), to learn compact visual-semantic representation with more reliable supervision for retrieval task. Specifically, for each image-tag pair, the weak supervision from tags is refined by cross-modal attention, which takes image feature as query to aggregate the most content-relevant tags. Therefore, tags with relevant content will be more prominent while noisy tags will be suppressed, which provides more accurate supervisory information. To improve the effectiveness of hash learning, the image embedding in WSHRCA is reconstructed from hash code, which is further optimized by cross-modal constraint and explicitly improves hash learning. The experiments on two widely-used datasets demonstrate the effectiveness of our proposed method for weakly-supervised image retrieval. The code is available at https://github.com/duyc168/weakly-supervised-hashing .",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4362722892",
    "type": "article"
  },
  {
    "title": "3V3D: Three-View Contextual Cross-slice Difference Three-dimensional Medical Image Segmentation Adversarial Network",
    "doi": "https://doi.org/10.1145/3592614",
    "publication_date": "2023-04-13",
    "publication_year": 2023,
    "authors": "Xianhua Zeng; Saiyuan Chen; Yicai Xie; Tianxing Liao",
    "corresponding_authors": "",
    "abstract": "In three-dimensional (3D) medical image segmentation, it is still a great challenge to obtain the multidimensional feature information contained in voxel images using a single view for smaller segmentation targets, and the robustness of models obtained by relying solely on segmentation networks needs to be enhanced. In this article, we propose a three-view contextual cross-slice difference 3D segmentation adversarial network, in which three-view contextual cross-slice difference decoding blocks are introduced to improve the segmentation decoder’s ability to perceive edge feature information. Meanwhile, dense skip connections are used to alleviate the problem that a large amount of shallow feature information is lost in encoding and insufficient information provided by a single long skip connection during image reconstruction. The adversarial network improves the performance of the segmentation network by distinguishing true or false for each patch of the predicted image. Further, the robustness of the segmentation model is improved in the form of adversarial training. We evaluate our model on the publicly available brain tumor BraTS2019 dataset as well as the ADNI1 dataset and achieve optimal results compared to recent excellent models.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4365447174",
    "type": "article"
  },
  {
    "title": "PANet: An End-to-end Network Based on Relative Motion for Online Multi-object Tracking",
    "doi": "https://doi.org/10.1145/3595379",
    "publication_date": "2023-05-05",
    "publication_year": 2023,
    "authors": "Rui Li; Baopeng Zhang; Wei Liu; Teng Zhu; Jianping Fan",
    "corresponding_authors": "",
    "abstract": "The popular tracking-by-detection paradigm of multi-object tracking (MOT) takes detections of each frame as the input and associates detections from one frame to another. Existing association methods based on the relative motion have attracted attention, because they restrain the effect of noisy detections and improve the performance of MOT. However, these methods depend only on the immediately previous frame, which may easily lead to inaccurate matches and even large accumulated errors. Furthermore, multiple objects involved in occlusions are not fully exploited in these existing methods, which leads to the aggravation of inaccurate matches. Motivated by these issues, we design the pivot to represent each object and propose a novel pivot association network (PANet) for the MOT task. Specifically, pivots are learned from spatial semantic and historical contextual clues, which alleviates the dependency on the immediately previous frame. Our online tracker PANet employs pivots and a lightweight associator to localize tracklets of objects, which can inhibit noise detections and improve the accuracy of tracklet prediction by learning the correlation responses between pivots and spatial search areas. Extensive experiments conducted on two-dimensional MOT15, MOT16, MOT17, and MOT20 demonstrate the effectiveness of the proposed method against numerous state-of-the-art MOT trackers.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4372342213",
    "type": "article"
  },
  {
    "title": "Forgery Detection by Weighted Complementarity between Significant Invariance and Detail Enhancement",
    "doi": "https://doi.org/10.1145/3605893",
    "publication_date": "2023-06-27",
    "publication_year": 2023,
    "authors": "Shuai Xiao; Zhuo Zhang; Jiachen Yang; Jiabao Wen; Yang Li",
    "corresponding_authors": "",
    "abstract": "Generative adversarial networks have shown impressive results in the modeling of movies and games, but what if such powerful image generation capability is used to harm the Multimedia? The face replacement methods represented by Deepfakes are becoming a threat to everyone, so the development of image authenticity detection methods has become a top priority. For achieving accurate detection resistant to compression effects, we propose a weighted complementary dual-stream detection method. First, to alleviate the influence of image compression on manipulation detection, we propose the concept of pixel-wise saliency invariance. We map fake images onto saliency maps via Quaternary Fourier Transform, which discovers the invariant properties of image phase spectra on different compressions. Meanwhile, to capture boundary traces more easily, we propose the concept of pixel-wise detail enhancement. We apply Bilateral Filtering to preserve the texture edges of fake images and amplify the fake boundaries. Finally, to take full advantage of the two proposed concepts, a weighted complementary dual-stream network is designed as a classifier to fuse features and identify real and fake. On different benchmarks like FaceForensics++ (FF++), Celeb-DF, and DFDC, the experimental results show that the proposed method has the average best detection accuracy compared to existing methods.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4382246587",
    "type": "article"
  },
  {
    "title": "Dynamic Weighted Gradient Reversal Network for Visible-infrared Person Re-identification",
    "doi": "https://doi.org/10.1145/3607535",
    "publication_date": "2023-07-07",
    "publication_year": 2023,
    "authors": "Chenghua Li; Zongze Li; Jing Sun; Yun Zhang; Xiaoping Jiang; Fan Zhang",
    "corresponding_authors": "",
    "abstract": "Due to intra-modality variations and cross-modality discrepancy, visible-infrared person re-identification (VI Re-ID) is an important and challenging task in intelligent video surveillance. The cross-modality discrepancy is mainly caused by the differences between visible images and infrared images, the inherent essence of which is heterogeneous. To alleviate this discrepancy, we propose a Dynamic Weighted Gradient Reversal Network (DGRNet) to enhance the learning of discriminative common representations by confusing the modality discrimination. In the proposed DGRNet, we design the gradient reversal model guiding adversarial training between identity classifier and modality discriminator to reduce the modality discrepancy of the same person in different modalities. Furthermore, we propose an optimization training method, that is, designing dynamic weight of gradient reversal to achieve optimal adversarial training, and dynamic weight has the ability to dynamically and adaptively evaluate the significance of target loss term, without involving hyper-parameter tuning. Extensive experiments were conducted on two public VI Re-ID datasets, SYSU-MM01 and RegDB. The experimental results show that the proposed DGRNet outperforms state-of-the-art methods and demonstrate the effectiveness of the DGRNet to learn more discriminative common representations for VI Re-ID.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4383556116",
    "type": "article"
  },
  {
    "title": "Improved Security for Multimedia Data Visualization Using Hierarchical Clustering Algorithm",
    "doi": "https://doi.org/10.1145/3610296",
    "publication_date": "2023-07-21",
    "publication_year": 2023,
    "authors": "Shitharth Selvarajan; Hariprasath Manoharan; Alaa O. Khadidos; Achyut Shankar; Carsten Maple; Adil O. Khadidos; Shahid Mumtaz",
    "corresponding_authors": "",
    "abstract": "In this paper, a realization technique is designed with a unique analytical model for transmitting multimedia data to appropriate end users. Transmission of multimedia data to all end users through a variety of visualization methods is the foundation of future computer systems. Yet, highly limited system resources prevent the updating of the methods used to manage multimedia data. Hence, a high-end visualization technique where uncertainties are eliminated is required for the visualization process with a multimedia system. As a result, the suggested system incorporates a clustering technique utilizing an analytical framework to ensure a high degree of transmission for all multimedia data. The technical contribution of the proposed method depends on a multimedia visualization process that takes place with high security features by including necessary parametric relationships such as occurrence of jitter, data density points, time period, multimedia storage, data smoothness and distance. For the established parametric relationship the validation methodology is integrated with a hierarchical clustering algorithm, thereby transmitting every clustered data with high security feature, thereby the examined outcomes under five scenarios proves that data security which is represented by simulation outcomes is improved to 88% as compared to the existing approach.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4385068798",
    "type": "article"
  },
  {
    "title": "Contrastive Attention-guided Multi-level Feature Registration for Reference-based Super-resolution",
    "doi": "https://doi.org/10.1145/3616495",
    "publication_date": "2023-08-21",
    "publication_year": 2023,
    "authors": "Jianwei Zheng; Yu Liu; Yuchao Feng; Honghui Xu; Meiyu Zhang",
    "corresponding_authors": "",
    "abstract": "Given low-quality input and assisted by referential images, reference-based super-resolution (RefSR) strives to enlarge the spatial size with the guarantee of realistic textures, for which sophisticated feature-matching strategies are naturally demanded. However, the miserable transformation gap between inputs and references, e.g., texture rotation and scaling within patches, often yields distorted textures and terrible ghosting artifacts, which seriously hampers the visual senses and their further investigation. To circumvent this challenge, we propose a contrastive attention-guided multi-level feature registration for RefSR, explicitly tapping the potential of interacting between inputs and references. Specifically, we develop a multi-level feature warping scheme, involving patch-level coarse feature swapping and pixel-level deformable alignment, to model generalized spatial transformation correspondences steered by contrastive attention. Notably, a spatial registration module is embedded for further calibration against the potential misalignment issue and inter-feature distribution difference. In addition, aiming at suppressing the impacts of irrelevant or superfluous information on cross-scale features, we incorporate a multi-residual feature fusion module to strive for visually plausible textures. Experimental results on four publicly available datasets demonstrate that our method outperforms most state-of-the-art approaches in terms of both efficiency and perceptual effectiveness.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4386031559",
    "type": "article"
  },
  {
    "title": "Relation with Free Objects for Action Recognition",
    "doi": "https://doi.org/10.1145/3617596",
    "publication_date": "2023-08-26",
    "publication_year": 2023,
    "authors": "Shuang Liang; Wentao Ma; Chi Xie",
    "corresponding_authors": "",
    "abstract": "Relevant objects are widely used for aiding human action recognition in still images. Such objects are founded by a dedicated and pre-trained object detector in all previous methods. Such methods have two drawbacks. First, training an object detector requires intensive data annotation. This is costly and sometimes unaffordable in practice. Second, the relation between objects and humans are not fully taken into account in training. This work proposes a systematic approach to address the two problems. We propose two novel network modules. The first is an object extraction module that automatically finds relevant objects for action recognition, without requiring annotations. Thus, it is free . The second is a human-object relation module that models the pairwise relation between humans and objects, and enhances their features. Both modules are trained in the action recognition network, end-to-end. Comprehensive experiments and ablation studies on three datasets for action recognition in still images demonstrate the effectiveness of the proposed approach. Our method yields state-of-the-art results. Specifically, on the HICO dataset, it achieves 44.9% mAP, which is 12% relative improvement over the previous best result. In addition, this work makes an observational contribution that it is no longer necessary to rely on a pre-trained object detector for this task. Relevant objects can be found via end-to-end learning with only action labels. This is encouraging for action recognition in the wild. Models and code will be released.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4386195714",
    "type": "article"
  },
  {
    "title": "Image Defogging Based on Regional Gradient Constrained Prior",
    "doi": "https://doi.org/10.1145/3617834",
    "publication_date": "2023-08-29",
    "publication_year": 2023,
    "authors": "Qiang Guo; Zhi Zhang; Mingliang Zhou; Hong Yue; Huayan Pu; Jun Luo",
    "corresponding_authors": "",
    "abstract": "Foggy days limit the functionality of outdoor surveillance systems. However, it is still a challenge for existing methods to maintain the uniformity of defogging between image regions with a similar depth of field and large differences in appearance. To address above problem, this article proposes a regional gradient constrained prior (RGCP) for defogging that uses the piecewise smoothing characteristic of the scene structure to achieve accurate estimation and reliable constraint of the transmission. RGCP first derives that when adjacent similar pixels in the fog image are aggregated and spatially divided into regions, clusters of region pixels in RGB space conform to a chi-square distribution. The offset of the confidence boundary of the clusters can be regarded as the initial transmission of each region. RGCP further uses a gradient distribution to distinguish different regional appearances and formulate an interregional constraint function to constrain the overestimation of the transmission in the flat region, thereby maintaining the consistency between the estimated transmission map and the depth map. The experimental results demonstrate that the proposed method can achieve natural defogging performance in terms of various foggy conditions.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4386251391",
    "type": "article"
  },
  {
    "title": "HARR: Learning Discriminative and High-quality Hash Codes for Image Retrieval",
    "doi": "https://doi.org/10.1145/3627162",
    "publication_date": "2023-10-12",
    "publication_year": 2023,
    "authors": "Zeyu Ma; Siwei Wang; Xiao Luo; Zhonghui Gu; Chong Chen; Jinxing Li; Xian‐Sheng Hua; Guangming Lu",
    "corresponding_authors": "",
    "abstract": "This article studies deep unsupervised hashing, which has attracted increasing attention in large-scale image retrieval. The majority of recent approaches usually reconstruct semantic similarity information, which then guides the hash code learning. However, they still fail to achieve satisfactory performance in reality for two reasons. On the one hand, without accurate supervised information, these methods usually fail to produce independent and robust hash codes with semantics information well preserved, which may hinder effective image retrieval. On the other hand, due to discrete constraints, how to effectively optimize the hashing network in an end-to-end manner with small quantization errors remains a problem. To address these difficulties, we propose a novel unsupervised hashing method called HARR to learn discriminative and high-quality hash codes. To comprehensively explore semantic similarity structure, HARR adopts the Winner-Take-All hash to model the similarity structure. Then similarity-preserving hash codes are learned under the reliable guidance of the reconstructed similarity structure. Additionally, we improve the quality of hash codes by a bit correlation reduction module, which forces the cross-correlation matrix between a batch of hash codes under different augmentations to approach the identity matrix. In this way, the generated hash bits are expected to be invariant to disturbances with minimal redundancy, which can be further interpreted as an instantiation of the information bottleneck principle. Finally, for effective hashing network training, we minimize the cosine distances between real-value network outputs and their binary codes for small quantization errors. Extensive experiments demonstrate the effectiveness of our proposed HARR.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4387579400",
    "type": "article"
  },
  {
    "title": "Characters Link Shots: Character Attention Network for Movie Scene Segmentation",
    "doi": "https://doi.org/10.1145/3630257",
    "publication_date": "2023-10-26",
    "publication_year": 2023,
    "authors": "Jiawei Tan; Hongxing Wang; Junsong Yuan",
    "corresponding_authors": "",
    "abstract": "Movie scene segmentation aims to automatically segment a movie into multiple story units, i.e., scenes, each of which is a series of semantically coherent and time-continual shots. Previous methods have continued efforts on shot semantic association, but few take into account the impact of different semantics on foreground characters and background scenes in movie shots. In particular, the background scene in the shot can adversely affect scene boundary classification. Motivated by the fact that it is the characters who drive the plot development of a movie scene, we build a Character Attention Network (CANet) to detect movie scene boundaries in a character-centric fashion. To eliminate the background clutter, we extract multi-view character semantics for each shot in terms of human bodies and faces. Furthermore, we equip our CANet with two stages of character attention. The first is Masked Shot Attention (MSA) through selective self-attention over similar temporal contexts from multi-view character semantics to yield an enhanced omni-view shot representation, by which the CANet can better handle the variations of characters in pose and appearance. The second is Key Character Attention (KCA) through temporal-aware attention on character reappearances for Bidirectional Long Short-Term Memory (Bi-LSTM) feature association so that linking shots can be focused on those with recurring key characters. We encourage the proposed CANet in learning boundary-discriminative shot features. Specifically, we formulate a Boundary-Aware circle Loss (BAL) to push far apart CANet-features between adjacent scenes, which is also coupled with the cross-entropy loss to drive CANet-features sensitive to scene boundaries. Experimental results on the MovieNet-SSeg and OVSD datasets show that our method achieves superior performance in temporal scene segmentation compared with state-of-the-art methods.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4388196066",
    "type": "article"
  },
  {
    "title": "Sentiment-Oriented Transformer-Based Variational Autoencoder Network for Live Video Commenting",
    "doi": "https://doi.org/10.1145/3633334",
    "publication_date": "2023-11-18",
    "publication_year": 2023,
    "authors": "Fengyi Fu; Shancheng Fang; Weidong Chen; Zhendong Mao",
    "corresponding_authors": "",
    "abstract": "Automatic live video commenting is with increasing attention due to its significance in narration generation, topic explanation, etc. However, the diverse sentiment consideration of the generated comments is missing from the current methods. Sentimental factors are critical in interactive commenting, and lack of research so far. Thus, in this paper, we propose a Sentiment-oriented Transformer-based Variational Autoencoder (So-TVAE) network which consists of a sentiment-oriented diversity encoder module and a batch attention module, to achieve diverse video commenting with multiple sentiments and multiple semantics. Specifically, our sentiment-oriented diversity encoder elegantly combines VAE and random mask mechanism to achieve semantic diversity under sentiment guidance, which is then fused with cross-modal features to generate live video comments. Furthermore, a batch attention module is also proposed in this paper to alleviate the problem of missing sentimental samples, caused by the data imbalance, which is common in live videos as the popularity of videos varies. Extensive experiments on Livebot and VideoIC datasets demonstrate that the proposed So-TVAE outperforms the state-of-the-art methods in terms of the quality and diversity of generated comments. Related code is available at https://github.com/fufy1024/So-TVAE.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4388796930",
    "type": "article"
  },
  {
    "title": "Visual-Linguistic-Stylistic Triple Reward for Cross-Lingual Image Captioning",
    "doi": "https://doi.org/10.1145/3634917",
    "publication_date": "2023-11-28",
    "publication_year": 2023,
    "authors": "Jing Zhang; Dan Guo; Xun Yang; Peipei Song; Meng Wang",
    "corresponding_authors": "",
    "abstract": "Generating image captions in different languages is worth exploring and essential for non-native speakers. Nevertheless, collecting paired annotation for every language is time-consuming and impractical, particularly for minor languages. To this end, the cross-lingual image captioning task is proposed, which leverages existing image-source caption annotation data and wild unrelated target corpus to generate satisfactory caption in the target language. Current methods perform a two-step translation process of image-to-pivot (source) and pivot-to-target. The distinct two-step process comes with certain caption issues, such as the weak semantic alignment between the image and the generated caption and the generated caption’s non-target language style. To address these issues, we propose an end-to-end reinforce learning framework with Visual-linguistic-stylistic Triple Reward named TriR. In TriR, we jointly consider the visual, linguistic, and stylistic alignments to generate factual, fluent, and natural caption in the target language. To be specific, the image-source caption annotation provides factual semantic guidance, whereas the unrelated target corpus guides the language style of generated caption. To achieve this, we construct a visual reward module to measure the cross-modal semantic embedding of image and target caption, a linguistic reward module to measure the cross-linguistic embedding of source and target captions, and a stylistic reward module to imitate the presentation style of target corpus. The TriR can be implemented with either classical CNN-LSTM or prevalent Transformer architecture. Extensive experiments are conducted with four cross-lingual settings, i.e., Chinese-to-English, English-to-Chinese, English-to-German, and English-to-French. Experimental results demonstrate the remarkable superiority of our method, and sufficient ablation experiments validate the beneficial impact of every reward.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4389098529",
    "type": "article"
  },
  {
    "title": "Color Transfer for Images: A Survey",
    "doi": "https://doi.org/10.1145/3635152",
    "publication_date": "2023-11-30",
    "publication_year": 2023,
    "authors": "Chenlei Lv; Dan Zhang; Shengling Geng; Zhongke Wu; Hui Huang",
    "corresponding_authors": "",
    "abstract": "High-quality image generation is an important topic in digital visualization. As a sub-topic of the research, color transfer is to produce a high-quality image with ideal color scheme learned from the reference one. In this article, we investigate the mainstream methods of color transfer to provide a survey that introduces the related theories and frameworks. Such methods can be divided into three categories: statistical color transfer, semantic-based color transfer, and color transfer for special target. For these mainstream technical routes, we discuss the related research background, technical details, and representative methods. We also exhibit some new trends of the topic according to recent progress. Based on the comparisons, we discuss the unsolved issues of color transfer and potential solutions in future work.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4389195917",
    "type": "article"
  },
  {
    "title": "Multiple Pseudo-Siamese Network with Supervised Contrast Learning for Medical Multi-modal Retrieval",
    "doi": "https://doi.org/10.1145/3637441",
    "publication_date": "2023-12-13",
    "publication_year": 2023,
    "authors": "Xianhua Zeng; Xinyu Wang; Yicai Xie",
    "corresponding_authors": "",
    "abstract": "Medical multi-modal retrieval aims to provide doctors with similar medical images from different modalities, which can greatly promote the efficiency and accuracy of clinical diagnosis. However, most existing medical retrieval methods hardly support the retrieval of multi-modal medical images, i.e., the number of modalities is greater than 2, and just convert retrieval to classification or clustering. It futilely breaks the gap between the visual information and the semantic information in different medical image modalities. To solve the problem, a S upervised C ontrast L earning method based on a M ultiple P seudo- S iamese network (SCL-MPS) is proposed for multi-modal medical image retrieval. In order to make the samples with semantic similarity close neighbors on Riemann manifold, the multiple constraints based on semantic consistency and modal invariance are designed in different forward stages of SCL-MPS. We theoretically demonstrate the feasibility of the designed constraints. Finally, experiments on four benchmark datasets (ADNI1, ADNI2, ADNI3, and OASIS3) show that SCL-MPS achieves state-of-the-art performance compared to 15 retrieval methods. Especially, SCL-MPS achieves a 100% mAP score in medical cross-modal retrieval on ADNI1.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4389675191",
    "type": "article"
  },
  {
    "title": "Multimodal Emotion-Cause Pair Extraction with Holistic Interaction and Label Constraint",
    "doi": "https://doi.org/10.1145/3689646",
    "publication_date": "2024-08-23",
    "publication_year": 2024,
    "authors": "Bobo Li; Hao Fei; Fei Li; Tat‐Seng Chua; Donghong Ji",
    "corresponding_authors": "",
    "abstract": "The multimodal emotion-cause pair extraction (MECPE) task aims to detect the emotions, causes, and emotion-cause pairs from multimodal conversations. Existing methods for this task typically concatenate representations of each utterance from distinct modalities and then predict emotion-cause pairs directly. This approach struggles to effectively integrate multimodal features and capture the subtleties of emotion transitions, which are crucial for accurately identifying causes—thereby limiting overall performance. To address these challenges, we propose a novel model that captures holistic interaction and label constraint (HiLo) features for the MECPE task. HiLo facilitates cross-modality and cross-utterance feature interaction with various attention mechanisms, establishing a robust foundation for precise cause extraction. Notably, our model innovatively leverages emotion transition features as pivotal cues to enhance causal inference within conversations. The experimental results demonstrate the superior performance of HiLo, evidenced by an increase of more than 2% in the F1 score compared to existing benchmarks. Further analysis reveals that our approach adeptly utilizes multimodal and dialogue features, making a significant contribution to the field of emotion-cause analysis. Our code is publicly available at https://is.gd/MVdYmx .",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4401818579",
    "type": "article"
  },
  {
    "title": "Cross-modal Contrastive Learning with a Style-mixed Bridge for Single Image 3D Shape Retrieval",
    "doi": "https://doi.org/10.1145/3689645",
    "publication_date": "2024-08-30",
    "publication_year": 2024,
    "authors": "Dan Song; Shumeng Huo; Xinwei Fu; Chumeng Zhang; Wenhui Li; An-An Liu",
    "corresponding_authors": "",
    "abstract": "Image-based 3D shape retrieval (IBSR) is a cross-modal matching task, which searches similar shapes from a 3D repository using a natural image. Continuous attentions have been payed to this topic, such as joint embedding, adversarial learning and contrastive learning. Modality gap and diversity of instance similarities are two obstacles for accurate and fine-grained cross-modal matching. To overcome the two obstacles, we propose a style-mixed contrastive learning method (SC-IBSR). On one hand, we propose a style transition module to mix the styles of images and rendered shape views to form an intermediate style, and inject it to image contents. The obtained style-mixed image features serve as a bridge for later contrastive learning in order to alleviate the modality gap. On the other hand, the proposed strategy of fine-grained consistency constraint aims at cross-domain contrast and considers different importance of negative (positive) samples. Extensive experiments demonstrate the superiority of the style-mixed cross-modal contrastive learning on both the instance-level retrieval benchmark (i.e., Pix3D, Stanford Cars and Comp Cars that annotate shapes to images), and the unsupervised category-level retrieval benchmark (i.e., MI3DOR-1 and MI3DOR-2 with unlabeled 3D shapes). Moreover, experiments are conducted on Office-31 dataset to validate the generalization capability of our method. Code and pretrained models will be available at https://github.com/honoria0204/SC-IBSR .",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4402043891",
    "type": "article"
  },
  {
    "title": "Skeleton-Boundary Guided Network for Camouflaged Object Detection",
    "doi": "https://doi.org/10.1145/3711869",
    "publication_date": "2025-01-09",
    "publication_year": 2025,
    "authors": "Yuzhen Niu; Yuqiu Xu; Yuezhou Li; Jianghe Zhang; Yuzhong Chen",
    "corresponding_authors": "",
    "abstract": "Camouflaged object detection (COD) aims to resolve the tough issue of accurately segmenting objects hidden in the surroundings. However, the existing methods suffer from two major problems: the incomplete interior and the inaccurate boundary of the object. To address these difficulties, we propose a three-stage skeleton-boundary guided network (SBGNet) for the COD task. Specifically, we design a novel skeleton-boundary label to be a complementary to the typical pixel-wise mask annotation, emphasizing the interior skeleton and the boundary of the camouflaged object. Furthermore, the proposed feature guidance module (FGM) leverages the skeleton-boundary feature to guide the model to focus on both the interior and boundary of the camouflaged object. Besides, we design a bidirectional feature flow path with the information interaction module (IIM) to propagate and integrate the semantic and texture information. Finally, we propose the dual feature distillation module (DFDM) to progressively refine the segmentation results in a fine-grained manner. Comprehensive experiments demonstrate that our SBGNet outperforms 20 state-of-the-art methods on three benchmarks in both qualitative and quantitative comparisons.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406219356",
    "type": "article"
  },
  {
    "title": "Learning from Orthogonal Space with Multimodal Large Models for Generalized Few-shot Segmentation",
    "doi": "https://doi.org/10.1145/3712597",
    "publication_date": "2025-01-20",
    "publication_year": 2025,
    "authors": "Xiaojie Zhou; Hang Yu; Shengjie Yang; Jing Huo; Pinzhuo Tian",
    "corresponding_authors": "",
    "abstract": "Generalized Few-Shot Segmentation (GFSS) aims to segment both base and novel classes in a query image, conditioning on richly annotated data of base classes and limited exemplars from novel classes. The learning of novel classes undoubtedly faces a disadvantage in this competition due to the highly unbalanced data, which skews the learned feature space towards the base classes. In this paper, we present an innovative idea termed as “learning from orthogonal space” to avoid the conflict in the process of learning novel classes. Specifically, we first utilize textual modal information from labels to provide more distinguishable initial prototypes for different categories, ensuring that the prototypes for base and novel classes have distinct initial separations. Then, a simple but effective Feature Separating Module (FSM) is introduced to enhance the model’s ability to differentiate between base and novel classes through learning the novel features from orthogonal space. In addition, we propose a Trigger-Promoting Framework (TPF) during the testing stage to further boost performance. The prediction results from the FSM serve as a multimodal prompt to leverage information residing in large models, such as CLIP and SAM, to enhance performance. Comprehensive experiments on two benchmarks demonstrate that our method achieves superior performance on novel classes without sacrificing accuracy on base classes. Notably, our Feature Separating with Trigger-Promopting Network (FS-TPNet) outperforms the current state-of-the-art method by \\(12.8\\%\\) overall IoU on novel classes on PASCAL- \\(5^{i}\\) under the 1-shot scenario. Our codes will be available at https://github.com/returnZXJ/FS-TPNet .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406628766",
    "type": "article"
  },
  {
    "title": "Similarity Regulation and Calibration Alignment for Weakly Supervised Text-Based Person Re-Identification",
    "doi": "https://doi.org/10.1145/3711861",
    "publication_date": "2025-01-25",
    "publication_year": 2025,
    "authors": "Fu Ao; Jiaqi Zhao; Yong Zhou; Wenliang Du; Rui Yao; Abdulmotaleb El Saddik",
    "corresponding_authors": "",
    "abstract": "Traditional text-based person re-identification relies on identity labels. However, it is impossible to annotate large datasets, since identity annotation is expensive and time-consuming. Weakly supervised text-based person re-identification, where only text-image pairs are available without annotation of identities, is very practical in real life. While dealing with the weakly supervised person re-identification, two issues should be strengthed, i.e., alignment caused by different modal, and cross-modal matching ambiguity caused by the lack of identity labels. In this paper, we propose a Similarity Regulation and Calibration Alignment (SRCA) framework, which consists of two unimodal encoders for images and text respectively and a multi-modal encoder for the masked language modelling task. Firstly, a Similarity Regulation (SR) strategy is proposed to relax the strict one-to-one constraints for the local similarities between different pairs by introducing a novel soft objective. The soft objective can adjust hard objectives to achieve soft cross-modal alignment by establishing a many-to-many relationship between two modalities. Secondly, the Calibration Alignment (CA) module is proposed to improve intra-class compactness by modelling pseudo-label assignment as optimal transport. The ambiguity of cross-modal matching can be reduced by aligning features and pseudo-labels of different modalities and gradually calibrating the distribution of pseudo-labels. Experimental results show that our method has achieved obvious advantages compared with existing methods, and also demonstrated competitive performance compared with fully supervised methods.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406825864",
    "type": "article"
  },
  {
    "title": "GFPNet: Generalizable Face Privacy Network with Dynamic Defense Training",
    "doi": "https://doi.org/10.1145/3715132",
    "publication_date": "2025-01-28",
    "publication_year": 2025,
    "authors": "Yucheng Li; Siwang Zhou; Deyan Tang; Liubo Ouyang; Jia Liu",
    "corresponding_authors": "",
    "abstract": "In certain specific scenarios, there is a risk of privacy leakage in terms of the soft biometric attributes on a person’s face. However, existing face privacy-enhancing techniques suffer from limited generalizability, meaning that they can only induce misclassification in a specific classifier but fail to generalize this effect well to arbitrary attribute classifiers. Moreover, existing methods reverse attributes to improve face privacy, but this may result in privacy recovery. To address those problems, we propose GFPNet, a novel privacy-enhancing model that can provide generalizable and reliable privacy to face images. The key factor for improving generalizability is that GFPNet uses defense training, which is an effective way to improve model robustness, to dynamically strengthen the mediocre auxiliary attribute classifier during iterative training. Specifically, the generalizability of GFPNet is enhanced in the game between attack and defense, where the generator attempts to deceive the auxiliary attribute classifier and the classifier defends against the generator’s attack by defense training. Furthermore, instead of reversing attributes, skewing attributes to one side is used to avoid attribute recovery. GFPNet also integrates a face matcher, multi-scale discriminator, and Demiguise Attack to improve face matching and image quality. Extensive experiments demonstrate that GFPNet has excellent generalizability to arbitrary attribute classiﬁers and satisfactory face-matching utility.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406903604",
    "type": "article"
  },
  {
    "title": "GP-HSI: Human-Scene Interaction with Geometric and Physical Constraints",
    "doi": "https://doi.org/10.1145/3716137",
    "publication_date": "2025-02-03",
    "publication_year": 2025,
    "authors": "Nianzi Li; Guijuan Zhang; Ping Du; Dianjie Lu",
    "corresponding_authors": "",
    "abstract": "With the rapid development of AR/VR technologies, achieving natural and seamless human-scene interactions has emerged as a critical challenge in computer vision. Existing methods suffer from low model placement accuracy and unnatural scene interactions. Therefore, we propose a framework called human-scene interaction with geometric and physical constraints (GP-HSI), which places a given pose of a 3D human model in an appropriate position within a 3D scene by establishing geometric and physical constraints, while ensuring interactive fidelity between the human and the scene. Specifically, first, we propose a pose-guided human contact semantic generation method, which generates human semantic labels by classifying the given human poses. Second, we propose a geometrically and semantically constrainted human model placement method, which determines the optimal position of the human model in the scene by constraining the geometric proximity and semantic consistency between models. Third, we propose an inverse kinematics based pose adjustment method, which finds the target human-scene interaction points by constructing a heterogeneous kinematic tree and solves the rotation matrix of human joints to obtain a physically plausible optimal human pose. At last, we develop an interactive system to visualize the generated human-scene interaction. The results of qualitative and quantitative experiments show that our approach is able to place human models at appropriate locations in the scene and generate plausible interactions.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407102659",
    "type": "article"
  },
  {
    "title": "Context-Assisted Active Learning for Weakly Supervised Person Search",
    "doi": "https://doi.org/10.1145/3714413",
    "publication_date": "2025-02-10",
    "publication_year": 2025,
    "authors": "Rinyoichi Takezoe; Hao Chen; Gang Shen; Xuefei Lv; Yaowei Wang; Shiliang Zhang; Xiaoyu Wang",
    "corresponding_authors": "",
    "abstract": "Person search is a challenging task that aims to jointly detect and identify a target person from a large-scale scene image dataset. Fully supervised person search requires both bounding boxes and person identity annotations, making it hard to deploy in real-world applications. Although recent weakly supervised person search methods can alleviate annotation workloads, they often result in severe performance degradation when compared to supervised methods. To pursue better performance with a lower annotation budget, we propose to integrate active learning into weakly supervised person search, where a small number of pairwise identity annotations are actively acquired from oracles. Specifically, we propose a context-assisted active learning framework that selects informative instance pairs for labeling and refines pseudo labels for representation learning. The proposed framework consists of a split module and a merge module, which leverage two types of contextual cues for label refinement. Besides, a pairwise relationship predictor is introduced to estimate relations between instances so that annotation cost can be further reduced. Extensive experiments demonstrate that the proposed method could achieve comparable or even better performance than recent fully supervised methods at a much lower annotation cost. Notably, our method achieves 61.4% mAP on PRW dataset, which outperforms recent fully supervised methods at a much lower annotation cost.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407337549",
    "type": "article"
  },
  {
    "title": "Tropical Cyclone Image Super-Resolution via Multimodality Fusion",
    "doi": "https://doi.org/10.1145/3714471",
    "publication_date": "2025-02-21",
    "publication_year": 2025,
    "authors": "Tao Song; Kunlin Yang; Fan Meng; Xin Li; Handan Sun; Chenglizhao Chen",
    "corresponding_authors": "",
    "abstract": "The traditional super-resolution dataset construction using artificial downsampling techniques can result in information loss, insufficient diversity, and non-uniqueness. Furthermore, existing methods for image super-resolution are limited to single-modal images and cannot accommodate the complexities of multimodal images. This is problematic because diverse modal data requires individualized model design and training, which can hinder the exploitation of complementary relationships among multimodal data. In this paper, we have addressed these issues by undertaking a two-step solution approach. In the first step, we constructed a super-resolution dataset that utilized remote sensing images of tropical cyclones in “real cases”. This dataset comprises HR-LR image pairs originating from multiple sensors of varying satellite sources, resulting in multimodal data. However, the HR-LR image pairs suffer from an additional misalignment issue. Thus, in the second step, we designed a super-resolution network based on MAT to address the misalignment problem in multimodal environment. After numerous ablation experiments and comparison experiments, we have shown that our model is effective, with an improvement of 50% over the original baseline model, and an increase varying between 20% and 50% compared to other common super-resolution models. We have made our source code and data publicly available online at https://github.com/kleenY/MMTCSR .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407838571",
    "type": "article"
  },
  {
    "title": "Mathematics-Inspired Models: A Green and Interpretable Learning Paradigm for Multimedia Computing",
    "doi": "https://doi.org/10.1145/3721136",
    "publication_date": "2025-03-03",
    "publication_year": 2025,
    "authors": "Lei Gao; Kai Liu; Zheng Guo; Ling Guan",
    "corresponding_authors": "",
    "abstract": "The advances of machine learning (ML), and AI in general, have attracted unprecedented attention in intelligent multimedia computing and many other fields. However, due to the concern for sustainability and black-box nature of ML models, especially deep neural networks (DNNs), green and interpretable learnings have been extensively studied in recent years, despite suspicions on effectiveness, subjectivity of interpretability, and complexity. To address these concerns and suspicions, this article starts with a survey on recent discoveries in green learning and interpretable learning and then presents mathematics-inspired (M-I) learning models. We will demonstrate that the M-I models are green in nature with numerous interpretable properties. Finally, we present several examples in multi-view information computing on both static image-based and dynamic video-based tasks to demonstrate that the M-I methodology promises a plausible and sustainable path for natural evolution of ML, which is worth further investment in.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408112773",
    "type": "article"
  },
  {
    "title": "Deep Differential Lifelong Cross-modal Hashing for Stream Medical Data Retrieval",
    "doi": "https://doi.org/10.1145/3721432",
    "publication_date": "2025-03-04",
    "publication_year": 2025,
    "authors": "Liming Xu; Dengping Zhao; Hanqi Li; Xianhua Zeng; Bochuan Zheng",
    "corresponding_authors": "",
    "abstract": "With the explosive growth of stream medical multi-modal data, it is significant to develop an efficient cross-modal retrieval algorithm to achieve effective medical data search. Within it, deep cross-modal hashing which maps cross-modal data into low-dimensional Hamming space where similarity in high-dimension space is preserved has made much progress. However, most of deep cross-modal hashing algorithms are usually facing disability of adapting to dynamic stream medical data, non-differentiable optimization and unaligned semantic across modalities. To address these, we, in this paper, propose a novel deep differential lifelong cross-modal hashing method for large-scale stream medical data retrieval. Specifically, we first design lifelong learning module to keep the learned hash code of base data unchanged and directly learn hash code of incremental data with new categories to achieve continuous retrieval of stream medical data, which effectively mitigates catastrophic forgetting, as well as significantly reduces training time and computation resource. Then, we introduce differential cross-modal hashing module to generate discriminative binary hash codes, which yields continuous and differentiable optimization and improves accuracy. Besides, we design semantic alignment module which embeds intra-modal and inter-modal losses to maintain the semantic similarity and dis-similarity among stream medical data across modalities. Extensive experiments on benchmark medical data sets show that our proposed method can retrieve dynamic stream medical cross-modal data effectively and obtain higher retrieval performance comparing with recent state-of-the-art approaches.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408150011",
    "type": "article"
  },
  {
    "title": "Computational Analysis of Degradation Modeling in Blind Panoramic Image Quality Assessment",
    "doi": "https://doi.org/10.1145/3720547",
    "publication_date": "2025-03-06",
    "publication_year": 2025,
    "authors": "Jiebin Yan; Ziwen Tan; Jiale Rao; Lei Wu; Yifan Zuo; Yuming Fang",
    "corresponding_authors": "",
    "abstract": "Blind panoramic image quality assessment (BPIQA) has recently brought new challenge to the visual quality community, due to the complex interaction between immersive content and human behavior. Although many efforts have been made to advance BPIQA from both conducting psychophysical experiments and designing performance-driven objective algorithms, limited content and few samples in those closed sets inevitably would result in shaky conclusions, thereby hindering the development of BPIQA, we refer to it as the easy-database issue. In this paper, we present a sufficient computational analysis of degradation modeling in BPIQA to thoroughly explore the easy-database issue , where we carefully design three types of experiments via investigating the gap between BPIQA and blind image quality assessment (BIQA), the necessity of specific design in BPIQA models, and the generalization ability of BPIQA models. From extensive experiments, we find that easy databases narrow the gap between the performance of BPIQA and BIQA models, which is unconducive to the development of BPIQA. And the easy databases make the BPIQA models be closed to saturation, therefore the effectiveness of the associated specific designs can not be well verified. Besides, the BPIQA models trained on our recently proposed databases with complicated degradation show better generalization ability. Thus, we believe that much more efforts are highly desired to put into BPIQA from both subjective viewpoint and objective viewpoint.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408186605",
    "type": "article"
  },
  {
    "title": "Viewport-Unaware Blind Omnidirectional Image Quality Assessment: A Flexible and Effective Paradigm",
    "doi": "https://doi.org/10.1145/3723165",
    "publication_date": "2025-03-13",
    "publication_year": 2025,
    "authors": "Jiebin Yan; Kaize Wu; Junjie Chen; Ziwen Tan; Yuming Fang; Weide Liu",
    "corresponding_authors": "",
    "abstract": "Most of existing blind omnidirectional image quality assessment (BOIQA) models rely on viewport generation by modeling user viewing behavior or transforming omnidirectional images (OIs) into varying formats; however, these methods are either computationally expensive or less scalable. To solve these issues, in this paper, we present a flexible and effective paradigm, which is viewport-unaware and can be easily adapted to 2D plane image quality assessment (2D-IQA). Specifically, the proposed BOIQA model includes an adaptive prior-equator sampling module for extracting a patch sequence from the equirectangular projection (ERP) image in a resolution-agnostic manner, a progressive deformation-unaware feature fusion module which is able to capture patch-wise quality degradation in a deformation-immune way, and a local-to-global quality aggregation module to adaptively map local perception to global quality. Extensive experiments across four OIQA databases (including uniformly distorted OIs and non-uniformly distorted OIs) demonstrate that the proposed model achieves competitive performance with low complexity against other state-of-the-art models, and we also verify its adaptive capacity to 2D-IQA. The source code is available at https://github.com/KangchengWu/OIQA .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408405721",
    "type": "article"
  },
  {
    "title": "<i>ReAL</i> : Improving Image-Text Retrieval with Authentic Negative Repository Learning",
    "doi": "https://doi.org/10.1145/3729172",
    "publication_date": "2025-04-12",
    "publication_year": 2025,
    "authors": "Renjie Pan; Hua Yang; Xiangyu Zhao",
    "corresponding_authors": "",
    "abstract": "Current methods for image-text retrieval commonly propose various fusion modules to achieve robust visual-textual alignment, primarily relying on in-batch learning to guide the matching process. Some follow-up methods seek to enlarge the number of negative samples to boost image-text contrastive learning. However, these methods often face challenges posed by semantic-consistent negatives, i.e., negatives samples that share correspondence with the ground truth, leading to confusion in learning cross-modal semantics. To address this issue, we propose a novel Re trieve with A uthentic negative repository L earning ( ReAL ) method, which constructs a specific Authentic Negative Repository filled with valuable negative sample pairs. By introducing a Unique Negative Filter with a Discriminative Triplet Ranking Loss, ReAL effectively filters out the semantic-consistent negatives through similarity distribution analysis and threshold learning. Moreover, existing fusion paradigms suffer from intricate use of fine-grained representations from word- and region-level instances to progressively refine the fused embedding. In this paper, we propose a lightweight Cluster Refinement Module to exploit cross-modal semantics in a 1-way-1-out paradigm. Each visual-textual alignment can spontaneously uncover correlations with adjacent alignments through aggregation and re-allocation, without the need for a redundant and cost-inefficient refinement stage. Furthermore, ReAL employs dual momentum encoders with two memory banks, expanding the selection range of the Authentic Negative Repository to include a broader set of negatives. Extensive experiments conducted on Flickr30K, MS-COCO, and the augmented Flickr30K (with more hard negatives) demonstrate the superiority and robustness of ReAL, while also showcasing its significantly reduced inference time compared to other competitive baselines.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409390835",
    "type": "article"
  },
  {
    "title": "VPFormer: Leveraging Transformer with Voxel Integration for Viewport Prediction in Volumetric Video",
    "doi": "https://doi.org/10.1145/3730402",
    "publication_date": "2025-04-16",
    "publication_year": 2025,
    "authors": "Jie Li; Zhang-Rui Zhao; Qiyue Li; Zhixin Li; Pengyuan Zhou; Zhi Liu; Hao Zhou; Zhu Li",
    "corresponding_authors": "",
    "abstract": "With the continuous advancement of computer vision, image processing technologies, volumetric video, represented by point cloud videos, holds the potential for extensive applications in areas such as Virtual Reality (VR) and Augmented Reality (AR). Viewport prediction, also referred to as Field of View (FoV) prediction, is a crucial component in emerging VR and AR applications, playing a vital role in the transmission of point cloud videos. Currently, models for viewpoint prediction that integrate feature extraction and FoV information heavily rely on the spatial-temporal features extracted by convolutional neural networks. However, the drawback of 3D convolution lies in its inability to effectively capture long-term spatial-temporal dependencies within videos. Moreover, the temporal contrast layer used for time feature extraction only compares features within each block, leading to matching errors and inaccurate temporal feature extraction, consequently diminishing predictive performance. To address these limitations, we propose a Transformer-based Volumetric Point Cloud Video Viewport Prediction Network (VPFormer) that can efficiently extract spatial-temporal features from point cloud videos. VPFormer constitutes a viewport prediction framework that combines the spatial-temporal features of point cloud videos with user trajectory information. Specifically, we introduce a novel sampling method that effectively preserves spatial-temporal information while reducing computational complexity. Additionally, we incorporate context-aware dynamic positional encoding to capture inter-frame spatial-temporal context information. Subsequently, we introduce a voxel-based temporal contrast layer and partition the point cloud into smaller voxel blocks during feature matching, significantly reducing matching errors and enhancing the analysis and extraction of temporal features. Finally, by combining the spatial-temporal features of point cloud videos with user head trajectory information, we successfully predict future user viewpoints. Experimental results demonstrate that this approach outperforms other solutions in terms of performance.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409498455",
    "type": "article"
  },
  {
    "title": "Composing Error Concealment Pipelines for Dynamic 3D Point Cloud Streaming",
    "doi": "https://doi.org/10.1145/3731561",
    "publication_date": "2025-04-23",
    "publication_year": 2025,
    "authors": "I-Chun Huang; Yuang Shi; Yuan-Chun Sun; Wei Tsang Ooi; Chun‐Ying Huang; Cheng-Hsin Hsu",
    "corresponding_authors": "",
    "abstract": "Dynamic 3D point clouds enable the immersive user experience and thus have become increasingly more popular in volumetric video streaming applications. When being streamed over best-effort networks, point cloud frames may suffer from lost or late packets, leading to non-trivial quality degradation. To solve this problem, we proposed the very first error concealment pipeline framework, which comprises five stages: pre-processing, matching, motion estimation, prediction, and post-processing. Alternative algorithms can be developed for each stage, while algorithms of different stages could be mixed and matched into pipelines for end-to-end performance evaluations. We discussed the design goal and proposed multiple algorithms for each stage. These algorithms were then quantitatively compared using dynamic 3D point cloud sequences with diverse characteristics. Based on the comparison results, we proposed four representative pipelines for: (i) diverse degrees of motion variance, i.e., minor versus significant, and (ii) different application requirements, i.e., high quality versus low overhead. Extensive end-to-end evaluations of our proposed pipelines demonstrated their superior concealed quality over the 3D frame-copy method in both: (i) 3D metrics, by up to 5.32 dB in GPSNR and 1.7 dB in CPSNR; as well as (ii) 2D metrics, by up to 2.22 dB in PSNR, 0.06 in SSIM, and 11.67 in VMAF. Adding to that, a user study with 15 subjects indicated that our best-performing pipeline achieved 100% preference winning rate over the state-of-the-art learning-based interpolation algorithms while consuming merely up to 8.55% of running time.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409730599",
    "type": "article"
  },
  {
    "title": "Subjective and Objective Quality Assessment for Dynamic Point Cloud with Visual Attention in 6 DoF",
    "doi": "https://doi.org/10.1145/3731759",
    "publication_date": "2025-04-24",
    "publication_year": 2025,
    "authors": "Xuemei Zhou; Irene Viola; Evangelos Alexiou; Jack Jansen; Pablo César",
    "corresponding_authors": "",
    "abstract": "Perceptual quality assessment of Dynamic Point Cloud (DPC) contents plays an important role in various Virtual Reality (VR) applications that involve human beings as the end user. Understanding and modeling perceptual quality assessment is greatly enriched by insights from visual attention. However, incorporating aspects of visual attention in DPC quality models is largely unexplored, as ground-truth visual attention data is scarcely available. Besides, testing methods and procedures for collecting visual attention data are still to be agreed on. This paper presents a dataset containing subjective opinion scores and visual attention maps of DPCs, collected in a VR environment using eye-tracking technology. Both the quality score and eye-tracking data were collected during a subjective quality assessment experiment, in which subjects were instructed to watch and rate DPCs at various degradation levels under 6 Degrees-of-Freedom (DoF) inspection, using a head-mounted display. Qualitative interview analysis was also conducted after the experiment. The dataset consists of 50 DPCs, including 5 reference DPCs, with each reference encoded at 3 distortion levels using 3 different codecs (namely G-PCC, V-PCC, CWI-PCL), amounting to a total of 9 degraded version per reference. Additionally, it incorporates 1,000 gaze trials from 40 participants, yielding a total of 15,000 visual attention maps across all the DPCs. We additionally benchmark objective quality metrics originally designed for static point clouds, evaluating their performance in our dataset using two temporal pooling strategies. Furthermore, we employ the visual attention data that are retrieved during our experiment to evaluate whether the performance of widely used objective quality metrics is improved by considering subjective measurements of visual attention. This dataset establishes a link between quality assessment and visual attention within the context of DPC. Moreover, thematic analysis of the interviews helps uncover user behavior and factors impacting perceptual quality for DPC in 6 DoF. This work deepens our understanding of DPC quality assessment and visual attention, driving progress in the realm of VR experiences and perception.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409735861",
    "type": "article"
  },
  {
    "title": "Advanced Neural Network-based Video Coding Technologies for Intra Prediction and In-Loop Filtering",
    "doi": "https://doi.org/10.1145/3733108",
    "publication_date": "2025-05-01",
    "publication_year": 2025,
    "authors": "Yue Li; Junru Li; Chaoyi Lin; Kai Zhang; Li Zhang; Franck Galpin; Thierry Dumas; Hongtao Wang; Muhammed Coban; Jacob Ström; Liu Du; Kenneth Andersson",
    "corresponding_authors": "",
    "abstract": "The past decade has witnessed the huge success of deep learning in well-known artificial intelligence applications such as face recognition, autonomous driving, and large language model like ChatGPT. Recently, the application of deep learning has been extended to a much wider range, with Neural Network-Based Video Coding (NNVC) being one of them. NNVC can be performed at two different levels: embedding neural network-based (NN-based) coding tools into a classical video compression framework or building the entire compression framework upon neural networks. This article elaborates our studies in response to the recent exploration efforts in JVET (Joint Video Experts Team of ITU-T SG 16 WP 3 and ISO/IEC JTC 1/SC29) in the name of NNVC, falling in the former category. Specifically, in this article, we propose two advanced NN-based video coding technologies, i.e., NN-based intra prediction and NN-based in-loop filtering, which have been investigated for several meeting cycles in JVET and then adopted into the reference software, i.e., NNVC. In addition, we further propose a Small Ad-hoc Deep-Learning Library (SADL), which provides integer-based inference capabilities for neural networks to ensure interoperability across different systems. SADL has been adopted as the inference platform of all neural networks in NNVC. Extensive experiments on top of the NNVC have been conducted to evaluate the effectiveness of the proposed techniques. Compared with VTM-11.0_nnvc, the proposed two NN-based coding tools jointly achieve {11.94%, 21.86%, 22.59%}, {9.18%, 19.76%, 20.92%}, and {10.63%, 21.56%, 23.02%} BD-rate reductions on average for {Y, Cb, Cr} under random-access, low-delay, and all-intra configurations, respectively.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410008091",
    "type": "article"
  },
  {
    "title": "SCAG: Semantic Co-occurring Attention Guided Alignment for Knowledge-based Visual Question Answering",
    "doi": "https://doi.org/10.1145/3734220",
    "publication_date": "2025-05-06",
    "publication_year": 2025,
    "authors": "Zheng Liu; Kunyu Yang; Yu Weng; Zheng He; Xuan Liu; Honghao Gao",
    "corresponding_authors": "",
    "abstract": "In the realm of Knowledge-based Visual Question Answering (KB-VQA), the intricacy of the task lies in adeptly retrieving pertinent information from external sources and seamlessly aligning and amalgamating multimodal features. While numerous studies have effectively leveraged external knowledge to enrich factual connections among entities, there exists a tendency to overlook the significant reservoir of implicit information inherent in the visual-textual dimension. This oversight often results in suboptimal alignment and an undue reliance on the knowledge base. To address these challenges, this article introduces a novel strategy called SCAG. This approach aggregates the semantic co-occurring attention from diverse regions within images and various tokens within textual inputs using guidance weights to construct joint probabilistic representations grounded in the visual and textual dimensions, respectively. By employing this alignment strategy, the goal is to substantially mitigate information loss, reinforce inter-feature constraints within the model, reduce reliance on external knowledge sources, and enhance self-reasoning capabilities. The efficacy of our proposed model is comprehensively evaluated on the VQAv2 and OK-VQA datasets, with comparative analyses against multiple models conducted on the Ambiguous Knowledge (AK) dataset. Notably, our model exhibits a noteworthy 4.62% improvement over the state-of-the-art in addressing the knowledge dependency problem.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410121147",
    "type": "article"
  },
  {
    "title": "Hybrid Feature Integrated Transformer for 3D Hand Reconstruction from a Single RGB Image",
    "doi": "https://doi.org/10.1145/3734873",
    "publication_date": "2025-05-08",
    "publication_year": 2025,
    "authors": "Bing Yang; Xueqin Xiang; Wanzeng Kong; Jianhai Zhang; Jinliang Yao",
    "corresponding_authors": "",
    "abstract": "Reconstructing a 3D hand from a single RGB image is a very challenging task. Most of the existing Transformer-based 3D hand reconstructing methods do not fully consider the local spatial information from low-level image features, which would be crucial for capturing fine details and accurate shapes of the hand. Consequently, this oversight often leads to reconstructed hands that lack the precision and realism necessary for many applications, such as augmented reality, and hand gesture recognition. To address this limitation, in this paper, we propose a novel and efficient method named HybridMETRO to both utilize low-level and high-level image features for accurate reconstructing 3D hand pose and mesh vertices from a single RGB image. Specifically, we introduce the deformable attention into the encoder of Transformer, making it no longer limited by the length of the image feature sequence. Based on the above mechanism, we further propose an interleaved updating multi-scale feature encoder to fuse low-level and high-level features. Moreover, we incorporate the Graph Convolutional Residual (GCR) module to build a novel decoder to capture explicit semantic connections between grid vertices and thus improve spatial locality of extracted features. Experimental results demonstrate that, when compared with state-of-the-art methods, our proposed HybridMETRO could achieve better performance with significantly smaller model parameters that are about half of METRO’s and a quarter of HandOccNet’s.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410192141",
    "type": "article"
  },
  {
    "title": "Multi-view Panoramic Image Style Transfer with Multi-scale Attention and Global Sharing",
    "doi": "https://doi.org/10.1145/3735137",
    "publication_date": "2025-05-09",
    "publication_year": 2025,
    "authors": "Weiyu Wang; Chunmei Qing; Junpeng Tan; Xiangmin Xu",
    "corresponding_authors": "",
    "abstract": "Style transfer for panoramic images is a challenging task, due to the problems associated with its unique structure, including edge discontinuities, pole distortion, fuzzy details, and memory limitation. In this article, we propose a novel Multi-view Transformation network for Panorama Style Transfer (MuTPST). First, this architecture has a multi-view panoramic transformation mechanism, which includes a multi-view cubic projection and a multi-view equirectangular re-projection of panoramic images. This can address pole distortion and edge discontinuity by skillfully applying multiple types of projections and transformations. To capture different levels of context and structure in the stylization stage, we carefully design a multi-scale attention content encoder, which can coordinate the distribution of visual attention across space and channels. Besides, by the sharing of global style features in thumbnails and patches, MuTPST can process ultra-high-resolution panoramic images (e.g., 10,000 \\(\\times\\) 5,000 pixels) with limited GPU memory. Extensive experiments illustrate that the proposed method outperforms the state-of-the-art with a discernible improvement in panoramic image style transfer. More results and interactive features can be found on https://weiyang001.github.io/MuTPST/ .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410233026",
    "type": "article"
  },
  {
    "title": "3D Facial Shape Similarity with Deep Perceptual Representations",
    "doi": "https://doi.org/10.1145/3734874",
    "publication_date": "2025-05-14",
    "publication_year": 2025,
    "authors": "Seongmin Lee; Jiwoo Kang; Sanghoon Lee",
    "corresponding_authors": "",
    "abstract": "Comparing different 3D shapes is challenging due to their irregularities. Motivated by the human visual system mechanism, where the entire 3D geometry is clearly perceived as a series of multiple projections, we propose a novel facial shape similarity measurement using multiview deep perceptual representations. We introduce a multiview disentangling scheme that accurately represents a facial mesh in multiple coordinates and the training strategy with view specificity and regional consistency to reliably train the network with multiple projections. View specificity pertains to the human visual perception to better recognize facial similarity. Regional consistency mitigates regional redundancy among views. Hence, robust perceptual features with respect to views are embedded and accurate similarity can be measured. Consequently, the view-specific integration scheme incorporates the similarities of all views, allowing for highly consistent measurement. The experiments demonstrate that the proposed similarity outperforms state-of-the-arts and significantly improves the details in terms of geometry and human perception.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410361500",
    "type": "article"
  },
  {
    "title": "T2C: Text-guided 4D Cloth Generation",
    "doi": "https://doi.org/10.1145/3735642",
    "publication_date": "2025-05-14",
    "publication_year": 2025,
    "authors": "Zhipeng Yu; Zimeng Zhao; Y. F. Du; Y. Zheng; Binghui Zuo; Yangang Wang",
    "corresponding_authors": "",
    "abstract": "In the age of AIGC, the creation process is increasingly automated. Generating vivid characters with clothing and motions according to scripts or novels is no exception. Unfortunately, the diversity of fabric topologies, the complexity of fabric layering, and the flexibility of fabric motion make most approaches only applicable to motion generation for characters in undressing or tight-fitting clothing. This article introduces a novel approach named T2C , which employs a multi-layered clothing representation and a physics-based clothing animation paradigm to generate text-controlled Clothed 4D Humans, expanding the boundaries of the aforementioned issues. The hierarchical representation of clothing utilizes Fourier spherical mapping to define the geometric information of garments within a standard pose space, mapping it onto several 2D frequency domain subspaces. The motion of clothing in tandem with the human body is realized through a hybrid forward dynamic solution, where the internal virtual mechanic’s parameters driving the clothing are learned from text features. A series of qualitative and quantitative experiments reveal that T2C can generate dynamic clothing with a sense of layering, realistic details, and rich textures. The code will be publicly available at https://zhipengyu28.github.io/t2c/ .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410361704",
    "type": "article"
  },
  {
    "title": "HTTP Adaptive Streaming: A Review on Current Advances and Future Challenges",
    "doi": "https://doi.org/10.1145/3736306",
    "publication_date": "2025-05-19",
    "publication_year": 2025,
    "authors": "Christian Timmerer; Hadi Amirpour; Farzad Tashtarian; Samira Afzal; Amr Rizk; Michael Zink; Hermann Hellwagner",
    "corresponding_authors": "",
    "abstract": "Video streaming has evolved from push-based, broad-/multicasting approaches with dedicated hard-/software infrastructures to pull-based unicast schemes utilizing existing Web-based infrastructure to allow for better scalability. In this article, we provide an overview of the foundational principles of HTTP Adaptive Streaming (HAS), from video encoding to end user consumption, while focusing on the key advancements in adaptive bitrate algorithms, Quality of Experience (QoE), and energy efficiency. Furthermore, the article highlights the ongoing challenges of optimizing network infrastructure, minimizing latency, and managing the environmental impact of video streaming. Finally, future directions for HAS, including immersive media streaming and neural network-based video codecs, are discussed, positioning HAS at the forefront of next-generation video delivery technologies.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410498109",
    "type": "review"
  },
  {
    "title": "RESIST: Rationale-Enhanced and Reward Model-Based End-to-End Social Influence Dialogue System",
    "doi": "https://doi.org/10.1145/3736580",
    "publication_date": "2025-05-21",
    "publication_year": 2025,
    "authors": "Tong Wu; Jinhua Zhu; Wengang Zhou; Houqiang Li",
    "corresponding_authors": "",
    "abstract": "Developing proactive social influence dialogue systems presents a significant challenge, particularly in non-cooperative scenarios where the system's goals may conflict with those of the user. Traditional methods often focus on training models to plan dialogue strategies, but since human strategies are often sub-optimal, relying solely on manually collected data can be problematic. While large language models (LLMs) facilitate the generation of high-quality synthetic dialogues, their effectiveness in strategic dialogue under zero-shot or few-shot conditions is inconsistent. To address these issues, we propose a training framework applicable to multiple social influence dialogue tasks, named R ationale-Enhanced and R eward Model-Based E nd-to-End S ocial I nfluence Dialogue S ys t em (RESIST). To streamline the dialogue system development, we first use existing datasets to prompt a teacher LLM for generating “chain-of-thought” rationales, which are then used to enrich the data and enable supervised fine-tuning (SFT) of the model. Next, we train a reward model by ranking the fine-tuned model's outputs, thereby deriving task-specific preferences without manually constructing scalar rewards. Finally, we apply reinforcement learning to further refine the system, optimizing dialogue strategies and responses according to specific tasks and conversational contexts. Experimental results on three social influence tasks demonstrate the effectiveness and adaptability of our training approach. In terms of task goal completion , RESIST outperforms baseline models and even exceeds the performance of ChatGPT-driven prompt-based policy planning methods in both efficiency and effectiveness. Additionally, we introduce strategic proactivity as a novel evaluation metric, enabling us to analyze how RESIST training influences the proactive traits of dialogue agents, with a particular focus on the personality tendencies of smaller-scale language models during task execution. Experimental findings indicate that RESIST enhances the strategic proactivity of language models, aligning them more closely with task requirements. The source code will be made publicly available upon publication.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410552403",
    "type": "article"
  },
  {
    "title": "ML-based Load Value Approximator for Efficient Multimedia Processing",
    "doi": "https://doi.org/10.1145/3736582",
    "publication_date": "2025-05-21",
    "publication_year": 2025,
    "authors": "Alain Aoun; Mahmoud Masadeh; Sofiène Tahar",
    "corresponding_authors": "",
    "abstract": "Approximate computing (AC) has gained traction as an alternative computing method for energy-efficient processing. This article proposes the exploitation of AC to address the memory wall. The proposed model predicts the memory load value using machine learning (ML). Subsequently, the ML model is a load value approximator (LVA) where the generated value is accepted as-is. The proposed LVA was tested under various approximate conditions, where 50% to 95% of the load instructions were approximated using a set of multimedia applications. The memory access operation using the proposed LVA was more than \\(6\\times\\) faster in multiple cases. Additionally, the applications tested ran on average \\(1.83\\times\\) faster. The peak signal-to-noise ratio (PSNR) exceeded 37 dB in several scenarios. The average normalized mean absolute error (NMAE) was 4.54%.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410552530",
    "type": "article"
  },
  {
    "title": "DTSD: A Dual Teacher-Student-Based Discrimination Model for Anomaly Detection",
    "doi": "https://doi.org/10.1145/3736725",
    "publication_date": "2025-05-22",
    "publication_year": 2025,
    "authors": "Weizhi Xian; Junyi Wang; Xuekai Wei; Jielu Yan; Yueting Huang; Kunyin Guo; Weijia Jia; Mingliang Zhou",
    "corresponding_authors": "",
    "abstract": "The rapid development of computer vision technology for detecting anomalies in industrial products has received unprecedented attention. In this article, we propose a dual teacher–student-based discrimination (DTSD) model for anomaly detection, which combines the advantages of both embedding-based and reconstruction-based methods. First, the DTSD builds a dual teacher‒student architecture consisting of a pretrained teacher encoder with frozen parameters, a student encoder, and a student decoder. By distillation of knowledge from the teacher encoder, the two teacher‒student modules acquire the ability to capture both local and global anomaly patterns. Second, to address the issue of poor reconstruction quality faced by previous reconstruction-based approaches in some challenging cases, the model employs a feature bank that stores encoded features of normal samples. By incorporating template features from the feature bank, the student decoder receives explicit guidance to enhance the quality of reconstruction. Finally, a segmentation network is utilized to adaptively integrate multiscale anomaly information from the two teacher–student modules, thereby improving segmentation accuracy. Extensive experiments demonstrate that our method outperforms existing state-of-the-art approaches. The code of DTSD is publicly available at https://github.com/Math-Computer/DTSD .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410609964",
    "type": "article"
  },
  {
    "title": "Complementarily Learning Decoupled Category-Region-aware Prototype for Few-shot Classification",
    "doi": "https://doi.org/10.1145/3737645",
    "publication_date": "2025-05-28",
    "publication_year": 2025,
    "authors": "Jian Fang; Mengjuan Jiang; Jiaqing Fan; Bangjun Wang; Fanzhang Li",
    "corresponding_authors": "",
    "abstract": "Open-world few-shot classification is restricted by inadequate image-level content representation capabilities when the training and testing sets have significant differences in categories. Recently, many studies show the effectiveness of deep local descriptor-based methods, which attempt to select out dominating contents and discard noisy ones. However, aforementioned methods focus more on external relevance of support and query sets to filter features and ignore internal relevance among support sets, leading to unsatisfying classification performance. To relieve the issue, in this article, we propose the complementary learning Decoupling Category-Region-Aware Network (DCRNet) to simultaneously learn the correlation between internal members and then interact with the external sets. Specifically, we first propose an effective learnable Category Prototype-generated Feature Decoupling Module (CPFDM) to mine co-existing representations and generate comprehensive global class prototype. Then, to adaptively filter out discriminative local descriptors, we present a Category-Aware Selection Module (CASM) and introduce the Category-Aware Contrastive Loss (CACL) to highlight local information that is highly relative to the current category. In addition, the Region-Aware Contrastive Loss (RACL) is designed to encourage the model to concentrate on local regions, yielding powerful ability to distinguish foreground regions from between various categories. Finally, we leverage the filtered support descriptors to adaptively refine query descriptors through the descriptor selection strategy. Extensive experiments demonstrate that the proposed solution outperforms state-of-the-arts on five mainstream general and fine-grained few-shot classification datasets. We have released the training and testing code on https://github.com/jjfang007/DCRNet .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410819475",
    "type": "article"
  },
  {
    "title": "Towards a Robust Visual-Inertial-Surround-view SLAM System for Autonomous Indoor Parking",
    "doi": "https://doi.org/10.1145/3742433",
    "publication_date": "2025-06-02",
    "publication_year": 2025,
    "authors": "Xuan Shao; Lin Zhang; Tianjun Zhang; Shengjie Zhao",
    "corresponding_authors": "",
    "abstract": "An autonomous parking system is a low-speed unmanned driving system applied in indoor parking environments. Real-time and high-precision vehicle localization and map construction of the environment are two core functional modules of the system. Camera and IMU (Inertial Measurement Unit) sensors provide complementary data to create a Visual-Inertial Simultaneous Localization and Mapping (VI-SLAM) system. However, existing SLAM systems face challenges in complex parking environments. Moreover, limitations inherent in VI-SLAM systems further compromise their perception accuracy, affecting both localization and optimization. This article addresses the shortcomings of current VI-SLAM systems by proposing the RVIS SLAM system. This robust semantic SLAM system integrates data from three sensors: a front-view camera, an IMU, and a surround-view system. To ensure localization accuracy, the system utilizes metric information from common semantic objects on the ground. These objects include parking-slots, speed bumps, and parking-slot numbers captured in surround-view images to build scale-aware constraints. These constraints refine the initial scale of the SLAM system, which is often compromised under low IMU excitation conditions. Additionally, in optimization, SLAM systems ideally assume that the front-end produces optimization graphs without data association outliers. However, in real-world indoor parking environments, sensor noise and vehicle vibrations make this assumption unrealistic. To mitigate the adverse effects of outliers in SLAM systems, this article proposes a robust surround-view semantic data association strategy. This strategy quantifies the uncertainty of surround-view semantic landmarks for the first time, ensuring reliable localization and mapping in challenging environments. Extensive experiments in typical indoor parking environments validate the effectiveness and efficiency of the proposed RVIS SLAM system.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410953849",
    "type": "article"
  },
  {
    "title": "GAMA-Pose: Graph-Aware Multi-representation Aggregation for 3D Human Pose Estimation",
    "doi": "https://doi.org/10.1145/3737647",
    "publication_date": "2025-06-03",
    "publication_year": 2025,
    "authors": "Songran Zhou; Tao Wu; Xuewei Li; Xiubo Liang; Naye Ji; Xi Li",
    "corresponding_authors": "",
    "abstract": "Monocular 3D human pose estimation presents a considerable challenge owing to the intrinsic depth ambiguity associated with single-camera observations. Existing methods primarily rely on mean per joint position error (MPJPE) loss to train models for the conversion from 2D to 3D coordinates. However, empirical analysis reveals that models trained solely with point-based supervision may produce biomechanically implausible poses or exhibit significant depth ambiguity, even when achieving low MPJPE. This limitation arises from the fact that point-based loss only considers individual joint locations without accounting for inter-joint relationships. Fortunately, edges of human pose encode critical prior knowledge, including skeleton connectivity and biomechanical distributions. Explicitly modeling edge representations enables the model to overcome the constraints associated with point-only approaches, reducing the uncertainty in the optimization process of the 2D-3D inverse mapping and directly constraining depth ambiguity. Therefore, we propose the G raph- A ware M ulti-representation A ggregation (GAMA-Pose) framework that jointly predicts points and edges, with their fusion serving as the final output. To ensure the accuracy of edge predictions and mitigate depth ambiguity, A nti- D epth- A mbiguity Loss (ADA-Loss) is introduced to supervise the properties of edges and give direct supervision on depth ambiguity. Correspondingly, edge-based metrics are proposed to quantify the error of predicted edges. Experiments conducted on Human3.6M and MPI-INF-3DHP datasets demonstrate that GAMA-Pose effectively addresses the limitations of models relying solely on point constraints, mitigates depth ambiguity, enhances the accuracy of both point and edge predictions, and achieves state-of-the-art (SOTA) performance on both datasets.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410993923",
    "type": "article"
  },
  {
    "title": "Tensor-empowered Incomplete Multimodal Learning with Modality Reconstruction for Edge Intelligence",
    "doi": "https://doi.org/10.1145/3712593",
    "publication_date": "2025-06-05",
    "publication_year": 2025,
    "authors": "Xin Nie; Laurence T. Yang; Zhe Li; Fulan Fan; Zecan Yang",
    "corresponding_authors": "",
    "abstract": "The distributed computing paradigm of edge computing effectively addresses the challenges of data transmission delay and data privacy security. With the increasing popularity of IoT devices and 5G networks, edge computing has a broader range of applications. The advancement in artificial intelligence (AI) technology enables the realization of edge intelligence, which conducts data processing and analysis on edge devices to avoid excessive data transmission to the cloud, enhance system response speed, and protect user data privacy. In various edge intelligent systems like smart homes and autonomous driving, multimodal data plays a crucial role. However, missing modalities in such systems may lead to model failure in real-world environments. To tackle this issue, we propose a tensor-empowered modality reconstruction network (TMRN) that utilizes an end-to-end variational autoencoder for reconstructing missing modal data. This approach effectively enhances model robustness while reducing model size and training complexity. Furthermore, we introduce a supervised method for feature reconstruction to better align with the true distribution of missing modal data by leveraging tensor feature fusion and label supervision techniques. Additionally, we design a task information disentanglement module to make multimodal representations more relevant to specific tasks by effectively separating task-relevant from task-irrelevant information. Extensive experiments demonstrate that TMRN achieves competitive performance compared to existing state-of-the-art methods.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411059592",
    "type": "article"
  },
  {
    "title": "Accurate Hand Modeling in Whole-Body Mesh Reconstruction Using Joint-Level Features and Kinematic-Aware Topology",
    "doi": "https://doi.org/10.1145/3743138",
    "publication_date": "2025-06-06",
    "publication_year": 2025,
    "authors": "Fu Guo; Qi Wang; Qingshan Wang; Sheng Chen",
    "corresponding_authors": "",
    "abstract": "Whole-body mesh reconstruction utilizes neural networks to reconstruct the 3D human body, face, and hands, forming a fundamental task in computer vision. It is used to model human action in many practical applications that prioritize upper body action, particularly the hands. However, accurately estimating the 3D mesh parameters of hands in practical applications remains challenging due to severe self-occlusion and high self-similarity in hand action. To address these challenges, the Accurate Hand Modeling in Whole-Body Mesh Reconstruction (AHM-WBMR) is proposed in this article. It mainly consists of two innovative components: the joint-level features progressive matching and refinement and the kinematic features propagation. In the joint-level features progressive matching and refinement, the 3D deformable cross attention and the 3D deformable transformer decoder are proposed to assist in refining hand joint-level features. Further, in the kinematic features propagation, the kinematic-aware topology network is proposed to distinguish and relate different hand joint-level features using three types of kinematic topology structures. We evaluated AHM-WBMR on the UBody and FreiHAND datasets, both of which contain rich hand movements. Compared with the state-of-the-art methods, we achieved improvements of at least 10.2% and 11.1% in hand-related metrics on the two datasets, respectively.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411087795",
    "type": "article"
  },
  {
    "title": "Generation and Editing of Mandrill Faces: Application to Sex Editing and Assessment",
    "doi": "https://doi.org/10.1145/3744249",
    "publication_date": "2025-06-10",
    "publication_year": 2025,
    "authors": "Nicolas Dibot; Julien P. Renoult; William Puech",
    "corresponding_authors": "",
    "abstract": "Generative AI has seen major developments in recent years, enhancing the realism of synthetic images, also known as computer-generated images. In addition, generative AI has also made it possible to modify specific image characteristics through image editing. Previous work has developed methods based on generative adversarial networks (GAN) for generating realistic images, in particular faces, but also to modify specific features. However, this work has never been applied to specific animal species. Moreover, the assessment of the results has been generally done subjectively, rather than quantitatively. In this paper, we propose an approach based on methods for generating images of faces of male or female mandrills, a non-human primate. The main novelty of proposed method is the ability to edit their sex by identifying a sex axis in the latent space of a specific GAN. In addition, we have developed an assessment of the sex levels based on statistical features extracted from real image distributions. The experimental results we obtained from a specific database are not only realistic, but also accurate, meeting a need for future work in behavioral experiments with wild mandrills.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411176430",
    "type": "article"
  },
  {
    "title": "CCFL: Customized Client Federated Learning for Unsupervised Person Re-identification",
    "doi": "https://doi.org/10.1145/3735134",
    "publication_date": "2025-06-10",
    "publication_year": 2025,
    "authors": "Zheng Yi; Yong Zhou; Fayao Liu; Jiaqi Zhao; Hancheng Zhu; Wenliang Du",
    "corresponding_authors": "",
    "abstract": "Federated learning-based person re-identification (Re-ID) aims to address the issue of data silos in surveillance systems caused by increasingly stringent regulations on sensitive data. However, due to differences in data collection locations, times, and scales, severe non-independent and identically distributed (non-IID) characteristics exist across different Re-ID datasets. Existing federated learning-based Re-ID methods often adopt a unified model structure, which prevents the model from adapting well to diverse data environments, thereby significantly degrading the overall Re-ID performance. To address the challenges of training neural networks on non-IID data across different datasets, we propose a customizable federated learning framework. First, customizable clients allow each organization to freely select suitable neural network training methods and model architectures based on local data scales and prior knowledge, thus improving training outcomes. Second, since traditional federated learning frameworks cannot achieve knowledge fusion through parameter exchange between models with different architectures, we introduce an independent model, referred to as the interaction model, specifically designed for knowledge exchange among clients. The interaction model learns parameters (knowledge) from local models on each client through distillation learning. Subsequently, the interaction model is uploaded to the server, where it undergoes parameter fusion (knowledge exchange) with interaction models from other clients. Finally, the interaction model, enriched with knowledge from other clients, guides local model training through knowledge distillation. It is worth noting that selecting a lightweight interaction model, while potentially impacting Re-ID performance, can significantly reduce communication costs between the server and clients.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411178115",
    "type": "article"
  },
  {
    "title": "Integrity Protection of Generative Adversarial Networks Using Fragile Watermarking",
    "doi": "https://doi.org/10.1145/3744566",
    "publication_date": "2025-06-12",
    "publication_year": 2025,
    "authors": "Zihan Yuan; Li Li; Zichi Wang; Xinpeng Zhang",
    "corresponding_authors": "",
    "abstract": "Deep learning has made remarkable achievements in the field of artificial intelligence. However, a well-trained deep neural network is at risk of being tampered with. Although some model watermarking schemes have been proposed to solve this problem, most of them are only oriented to discriminant models, and the integrity authentication schemes for generative models are urgently lacking. Especially, the integrity authentication problem of Generative Adversarial Networks(GANs) that plays an important role in computer vision has not been properly solved. To address this problem, we propose a fragile model watermarking framework for GANs. Specifically, we use a secret key to generate specific information as the label, and combine it with the watermark to form a trigger set. Then, we use the trigger set to train the GAN, the training process does not damage the model performance. We can achieve integrity authentication of the GAN using the output of the GAN for the specific label. A large number of experiments show that our proposed method has excellent performance, which can realize the integrity authentication of GANs. What’s more, the proposed method has good generalization and can be easily applied to different GAN architectures.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411237339",
    "type": "article"
  },
  {
    "title": "Joint-Dataset Learning and Cross-Consistent Regularization for Text-to-Motion Retrieval",
    "doi": "https://doi.org/10.1145/3744565",
    "publication_date": "2025-06-12",
    "publication_year": 2025,
    "authors": "Nicola Messina; Jan Sedmidubský; Fabrizio Falchi; Tomáš Rebok",
    "corresponding_authors": "",
    "abstract": "Pose-estimation methods enable extracting human motion from common videos in the structured form of 3D skeleton sequences. Despite great application opportunities, effective content-based access to such spatio-temporal motion data is a challenging problem. In this paper, we focus on the recently introduced text-motion retrieval tasks, which aim to search for database motions that are the most relevant to a specified natural-language textual description ( text-to-motion ) and vice-versa ( motion-to-text ). Despite recent efforts to explore these promising avenues, a primary challenge remains the insufficient data available to train robust text-motion models effectively. To address this issue, we propose to investigate joint-dataset learning – where we train on multiple text-motion datasets simultaneously – together with the introduction of a Cross-Consistent Contrastive Loss function (CCCL), which regularizes the learned text-motion common space by imposing uni-modal constraints that augment the representation ability of the trained network. To learn a proper motion representation, we also introduce a transformer-based motion encoder, called MoT++, which employs spatio-temporal attention to process skeleton data sequences. We demonstrate the benefits of the proposed approaches on the widely-used KIT Motion-Language and HumanML3D datasets, including also some results on the recent Motion-X dataset. We perform detailed experimentation on joint-dataset learning and cross-dataset scenarios, showing the effectiveness of each introduced module in a carefully conducted ablation study and, in turn, pointing out the limitations of state-of-the-art methods. The code for reproducing our results is available here: https://github.com/mesnico/MOTpp .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411237466",
    "type": "article"
  },
  {
    "title": "Adaptive Alignment Contrastive Learning of Degradation Prediction for Blind Image Super-Resolution",
    "doi": "https://doi.org/10.1145/3744650",
    "publication_date": "2025-06-13",
    "publication_year": 2025,
    "authors": "Xianglan Wen; Sheng Ren; Bin Hu; Xiangyuan Zhu; Tianyu Chen; Kehua Guo",
    "corresponding_authors": "",
    "abstract": "Blind super-resolution (BSR) is entering a new era focused on diverse and complex applications, where the trade-off between generalization and performance prevents models from performing as they should. Model performance decreases when trained on multiple degraded images due to the inter-class and intra-class imbalances in degradation prediction, which consists of degradation sampling and estimation. The inter-class imbalance in degradation estimation causes inaccurate estimates, leading to severe artifacts in images. The intra-class imbalance in degradation sampling causes a long-tail problem, leading to model collapse and satisfactory results only in specific applications. To tackle these challenges, we propose adaptive alignment contrastive learning (AACL), which includes adaptive degradation sampling (ADS) and \\(\\sigma\\) -alignment. ADS utilizes nonlinear sampling by weighting the parameters of the degradation process for training uniformly degraded images, avoiding the long-tail problem. \\(\\sigma\\) -alignment controls the standard deviation among positive samples, we identify a subset with small degraded distance, which aids contrastive learning in extracting representations more effectively. We extend AACL to several CNN-based and Transformer-based methods by coming up with a 6 \\(\\times\\) 6 fair architecture with degradation representation fusion block (DRFB) and degradation representation fusion group (DRFG). DRFB and DRFG are designed for degradation representation fusion and image reconstruction, respectively. We evaluate on 6 types of degradation, and the improvement experiments on synthesized images show that our method balances performance and generalization, and is applicable to networks with different architectures. The comparison experiments show that our improved methods achieve promising results compared to SOTA methods. Code is available at: https://github.com/para999/AACL.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411274703",
    "type": "article"
  },
  {
    "title": "Multi-modal 3D Object Detector with Object-Guided Fusion and Hierarchical Sample Selection",
    "doi": "https://doi.org/10.1145/3744247",
    "publication_date": "2025-06-13",
    "publication_year": 2025,
    "authors": "Jianping Zhong; Zhaobo Qi; Kaiwen Duan; Yuanrong Xu; Weigang Zhang; Qingming Huang",
    "corresponding_authors": "",
    "abstract": "Accurately detecting objects in 3D scenes is crucial for autonomous driving. Although existing voxel-based methods have achieved remarkable progress, their performance on tail objects remains unsatisfactory. We identify two core issues contributing to this phenomenon: the detectors frequently misidentify some background elements as foreground objects, and there is a misalignment between the classification score and detection quality. To tackle these challenges, we introduce an object-level guided multi-modal 3D object detector with an Object-Guided Feature Fusion (OFF) module and a Hierarchical Sample Selection (HSS) strategy, named OGMMDet. Specifically, OFF introduces rich image features to enhance the representation of objects while using an object distribution heatmap to suppress the background. This approach provides geometry clues for tail objects while providing category priors to filter out the background. HSS uses a local-to-global ranking approach to calculate the relative classification loss weights of all proposals. It assigns higher weights to proposals with higher IoU when optimizing classification branches. This ensures that the model focuses its optimization on these higher-quality proposals. Consequently, there is a positive correlation between the classification score and IoU. This method alleviates the misalignment between the classification score and detection quality. Extensive experiments on the KITTI and nuScenes benchmarks demonstrate the effectiveness of our OGMMDet, which achieves 45.61% and 68.96% mean Average Precision (mAP) on pedestrians and cyclists on the KITTI benchmark, respectively. Code is available at https://github.com/ZhongJianPing1/ogmmdet.git .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411276791",
    "type": "article"
  },
  {
    "title": "Hybrid Unicast-Broadcast Video Delivery for Scalable Low-Latency Live Streaming",
    "doi": "https://doi.org/10.1145/3742868",
    "publication_date": "2025-06-17",
    "publication_year": 2025,
    "authors": "Casper Haems; Jeroen van der Hooft; Hannes Mareen; Peter Steenkiste; Glenn Van Wallendael; Tim Wauters; Filip De Turck",
    "corresponding_authors": "",
    "abstract": "The demand for high-quality, low-latency video streaming is placing strain on conventional internet infrastructures. This paper proposes a hybrid unicast-broadcast video delivery framework designed to address this challenge by integrating advanced 5G broadcast technologies with traditional unicast methods. By offloading popular content to a broadcast network, the approach aims to alleviate congestion and enhance overall streaming efficiency. To ensure reliable video segment delivery over the broadcast network, regardless of the physical layer, we incorporate packet recovery (PR) and Forward Error Correction (FEC) mechanisms. Additionally, Temporal Layer Injection (TLI) is employed to further improve video quality while maintaining reduced bandwidth requirements compared to traditional unicast-only approaches. This innovative framework leverages 5G terrestrial broadcasting within over-the-top (OTT) streaming environments, enabling seamless delivery of adaptive video content with sub-1-second live latency. Comprehensive experimentation and evaluation through large-scale emulation demonstrate the efficacy of this hybrid approach in meeting the evolving demands of modern multimedia delivery systems. Notably, when broadcasting the top 3 most commonly watched video streams, 63% of viewers no longer need to request video segments via unicast, as they are efficiently delivered over broadcast channels. This hybrid model offers significant scalability, cost reduction for an internet service provider (ISP), and efficiently delivers content directly to user devices without additional intermediaries, improving viewer experience through low-latency, high-quality streaming.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411371457",
    "type": "article"
  },
  {
    "title": "Bool Prompt with Decomposition and Enhancement: Zero-Shot VQA Based on PVLMs",
    "doi": "https://doi.org/10.1145/3744343",
    "publication_date": "2025-06-17",
    "publication_year": 2025,
    "authors": "Liyong Xu; Yifan Jiao; Bing‐Kun Bao",
    "corresponding_authors": "",
    "abstract": "Zero-Shot Visual Question Answering (ZSVQA) aims to answer questions about images without prior training on explicit image question pairs. Most existing methods usually apply Pre-trained Visual and Language Models (PVLMs) by designing prompts to convert questions into predefined input templates, which 1) ignores the text details and associations of the question when the question is complex, hindering comprehensive understanding, and 2) doesn’t pay attention to local information of image, resulting in overlooking some details of the image that are important to the question when the image content is particularly complex or requires detailed observation. To address these challenges, we propose the Bool Prompt with Decomposition and Enhancement (BPDE) framework for Zero-Shot Visual Question Answering. Specifically, we propose the Bool Sub-Questions Generating module to extract keywords from the original question and generate captions from the image, then use these keywords and captions to guide the transformation of original questions into simpler bool sub-questions, which focus on a specific logical point or piece of information, and guided from the captions can provide the model with local visual information, thereby enhancing the model's understanding of complex questions and attention to local visual information. Additionally, an Adaptive Sub-Questions Selecting mechanism is designed to ensure non-redundant selection and that the meanings of the sub-questions can cover the original question. Extensive experiments on VQAv2 and AOKVQA demonstrate that the proposed approach performs favourably against the state-of-the-art methods.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411371543",
    "type": "article"
  },
  {
    "title": "Offloading-based Power Efficient Mobile VTuber Live Streaming",
    "doi": "https://doi.org/10.1145/3742787",
    "publication_date": "2025-06-17",
    "publication_year": 2025,
    "authors": "Zichen Zhu; Stefano Petrangeli; Viswanathan Swaminathan; Sheng Wei",
    "corresponding_authors": "",
    "abstract": "Virtual YouTuber (VTuber) live streaming, which renders and streams a virtual avatar of the actual streamer on top of the live camera view, has gained significant popularity recently. Despite the engaging user experience, the intensive and power-consuming computations required by VTuber applications, such as facial feature extraction and avatar rendering, pose significant challenges to the constrained battery life of mobile devices. We develop a power efficient VTuber live streaming system by offloading the camera view and the computation-intensive operations from the mobile device to an edge server. Our approach not only reduces the power consumption of the mobile device but also enables larger-scale rendering of multiple avatars, which is infeasible in existing mobile VTuber systems. Furthermore, to reduce the bandwidth overhead caused by the camera view offloading, we develop an adaptive framerate control mechanism to dynamically adjust the framerate of the offloaded camera view based on the variations of inter-frame luminance, as well as resolution control to dynamically adjust the resolution of the offloaded camera view based on the number and size of the faces. Our evaluations on the end-to-end VTuber live streaming system demonstrate 26%-30% power savings with limited bandwidth, latency, and quality overhead.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411371841",
    "type": "article"
  },
  {
    "title": "Action Selection Learning for Weakly Labeled Multi-modal Multi-view Action Recognition",
    "doi": "https://doi.org/10.1145/3744742",
    "publication_date": "2025-06-17",
    "publication_year": 2025,
    "authors": "Trung Thành Nguyễn; Yasutomo Kawanishi; Vijay John; Takahiro Komamizu; Ichiro Ide",
    "corresponding_authors": "",
    "abstract": "Multi-view action recognition is a critical task in computer vision, with broad applications in surveillance, robotics, and video-content analysis. Traditional single-view action recognition approaches suffer from a limited field of view and occlusion, leading to incomplete understanding of actions and a higher likelihood of misclassification. Moreover, most existing methods rely on constrained environments with strong label annotations, where the onset and offset times of each action are meticulously labeled at the frame-level. However, annotating strong labels for multi-view video sequences in real-world scenarios is time consuming and labor intensive. In many cases, only a weak video-level (sequence-level) label is available, where only the action class label for the entire video sequence is provided. This limits the performance of accurate action recognition. To overcome this limitation, we propose M ulti-modal M ulti-view A ction S election L earning (MMASL), which integrates audio and video data to perform frame-level action recognition in large-area environments using sequence-level weak labels. The key components of MMASL include modality-specific Shared Audio Encoder and Shared Video Encoder, and an Action Selection Learning (ASL) mechanism. The encoder processes input data from multiple views by extracting and unifying features from audio and video modalities. Meanwhile, ASL dynamically selects relevant frames across views and filters out irrelevant information while focusing on critical action segments to enhance action recognition accuracy. By incorporating audio data with video data, MMASL improves recognition accuracy for visually ambiguous actions that are distinguishable through sound. Experiments in a real-world office environment using the MM-Office dataset demonstrate that MMASL outperforms state-of-the-art methods, achieving up to 8.81% improvement in mAP C (Class-wise mean Average Precision) and 8.43% in mAP S (Sample-wise mean Average Precision), highlighting the significance of multi-modal multi-view action recognition with ASL in real-world scenarios.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411372114",
    "type": "article"
  },
  {
    "title": "A Unified Generative Hashing for Cross-Modal Retrieval",
    "doi": "https://doi.org/10.1145/3744567",
    "publication_date": "2025-06-18",
    "publication_year": 2025,
    "authors": "Junfeng Tu; Xueliang Liu; Yanbin Hao; Richang Hong",
    "corresponding_authors": "",
    "abstract": "Cross-modal hashing is a highly effective and efficient method for information retrieval, enabling the search for correlated data across different modality databases using compact hash codes. Conventional cross-modal hashing typically uses separate model structures for each modality and aligns approximate continuous representations of the final hash codes. These approaches not only require specialized models for each modality but also introduce a gap between the discrete hash codes and their continuous features, yielding only approximate alignment. To address these issues, we propose a unified generative cross-modal hashing method that leverages a single Uniform Mixture-of-Expert Decoder (UMoED) for both image and text modalities. UMoED streamlines cross-modal hash learning by integrating two key design elements: (1) a cross-modal representation unification module that employs unified queries to consolidate modality-specific features into a common space, and (2) an adaptive expert enhancement module that adaptively enhances feature modeling based on the input modality. Furthermore, our decoder-based hashing method outputs hash codes in a generative manner, producing precise representations of discrete codes to bridge the gap between the discrete and continuous space, thus ensuring precise alignment during similarity learning. Extensive experiments on three benchmark datasets demonstrate that the proposed method achieves state-of-the-art performance in cross-modal hashing retrieval.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411403397",
    "type": "article"
  },
  {
    "title": "Interactive Image Retrieval Meets Query Rewriting with Large Language and Vision Language Models",
    "doi": "https://doi.org/10.1145/3744910",
    "publication_date": "2025-06-18",
    "publication_year": 2025,
    "authors": "Hongyi Zhu; Jia-Hong Huang; Yixian Shen; Stevan Rudinac; Evangelos Kanoulas",
    "corresponding_authors": "",
    "abstract": "Image search stands as a pivotal task in multimedia and computer vision, finding applications across diverse domains, ranging from internet search to medical diagnostics. Conventional image search systems operate by accepting textual or visual queries, retrieving the top-relevant candidate results from the database. However, prevalent methods often rely on single-turn procedures, introducing potential inaccuracies and limited recall. These methods also face the challenges, such as vocabulary mismatch and the semantic gap, constraining their overall effectiveness. To address these issues, we propose an interactive image retrieval system capable of refining queries based on user relevance feedback in a multi-turn setting. This system incorporates an image captioner based on vision language model (VLM) to enhance the quality of text-based queries, resulting in more informative queries with each iteration. Moreover, we introduce a denoiser based on large language model (LLM) to refine text-based query expansions, mitigating inaccuracies in image descriptions generated by captioning models. To evaluate our system, we curate a new dataset by adapting the MSR-VTT and MSVD video retrieval datasets to the image retrieval task, offering multiple relevant ground truth images for each query. Through comprehensive experiments, we validate the effectiveness of our proposed system against baseline methods, achieving state-of-the-art performance with a notable 10% improvement in terms of recall. Our contributions encompass the development of an innovative interactive image retrieval system, the integration of an LLM-based denoiser, the curation of a meticulously designed evaluation dataset, and thorough experimental validation.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411403432",
    "type": "article"
  },
  {
    "title": "<i>CLOUD-CODEC</i> : A New Way of Storing Traffic Cameras Footage at Scale",
    "doi": "https://doi.org/10.1145/3744649",
    "publication_date": "2025-06-19",
    "publication_year": 2025,
    "authors": "Hoyoung Kim; Azimbek Khudoyberdiev; Shubhangi S. R. Garnaik; Arani Bhattacharya; Jihoon Ryoo",
    "corresponding_authors": "",
    "abstract": "Storing large volumes of traffic video content in cloud storage is an expensive undertaking, given the limited capacity of cloud storage and its inability to store data beyond a few weeks. To address this issue, this paper introduces CLOUD-CODEC , a novel video encoding approach tailored specifically for traffic monitoring video. CLOUD-CODEC offers three key advantages: (i) real-time encoding without any delay, (ii) near-perfect video quality upon decoding, and (iii) one-fifth the storage size of traditional encoding methods. CLOUD-CODEC is generally applicable to traffic cameras under various weather and lighting conditions. The encoding algorithm is a lightweight DNN-based object detection and box shaped segmentation approach. The method can uniquely detect and segment cars, pedestrians, and moving objects with the marginal box shaped contours. Periodic object detection makes it possible for CLOUD-CODEC to operate in real-time and estimate the movement of objects between predictions. Proof-of-concept evaluations using a massive dataset indicate that CLOUD-CODEC reduces video size by 80%—surpassing AV1 (34.9%), CloudSeg (58.4%), Detection (76.9%), Segmentation (73.1%), and Segm&amp;Sort (69.5%). It achieves a frame rate of \\(95.8\\) when encoding, and a VMAF score of \\(72.54\\) after decoding, with a storage size that is one-fifth of traditional methods. Field-testing of CLOUD-CODEC on metropolitan traffic cameras demonstrates its ability to extend storage time by \\(74.92\\) percent.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411448917",
    "type": "article"
  },
  {
    "title": "A Real-Time Medical Image Encryption Algorithm Leveraging a Novel Hypersensitive Chaotic Map",
    "doi": "https://doi.org/10.1145/3745785",
    "publication_date": "2025-06-24",
    "publication_year": 2025,
    "authors": "Mohamed Zakariya Talhaoui; Zhelong Wang; Mohamed Amine Midoun; Messaouda Trid; Meryem Hamidaoui; Abdelkarim Smaili; Djamel Eddine Mekkaoui; Mourad Lablack",
    "corresponding_authors": "",
    "abstract": "Chaos-based encryption has emerged as a promising approach for securing digital images, particularly in sensitive domains such as medical imaging. However, existing chaos-based encryption algorithms often suffer from design flaws and the use of weak chaotic maps that exhibit limited chaotic ranges and vulnerability to signal estimation. These limitations result in insufficient key space, reduced resistance to statistical and differential attacks, and inefficiency in real-time applications. To address these challenges, this study introduces a novel One-Dimensional Cosine-Exponential (1DCE) map, a hypersensitive chaotic map characterized by enhanced chaotic behavior and robustness. Leveraging the 1DCE map, we propose a real-time image encryption algorithm, termed DCEIES, which incorporates a cross-row-column encryption strategy to significantly improve both encryption speed and security. The DCEIES algorithm also integrates a novel image sensitivity function that detects any alterations in the plain image, rendering the cryptosystem resistant to differential attacks. While the DCEIES algorithm is versatile and applicable to various types of images, its design is particularly optimized for medical imaging, where high pixel correlations and sensitivity to input changes are critical. Extensive simulations and security analyses demonstrate that the DCEIES algorithm outperforms state-of-the-art encryption algorithms in terms of high security and computational efficiency. The results highlight the potential of the DCEIES algorithm for real-time medical image encryption.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411577180",
    "type": "article"
  },
  {
    "title": "Understanding and Taming the Inflated Latency in Mobile Cloud Rendering",
    "doi": "https://doi.org/10.1145/3746283",
    "publication_date": "2025-07-01",
    "publication_year": 2025,
    "authors": "Yuankang Zhao; Qinghua Wu; Gerui Lv; Furong Yang; Jiuhai Zhang; Feng Peng; Yunwei Liu; Zhenyu Li; Hongyu Guo; Ying Chen; Gaogang Xie",
    "corresponding_authors": "",
    "abstract": "Low-latency cloud rendering enables mobile users to experience high-quality, real-time 3D graphics but achieving low motion-to-photon (MTP) latency while maintaining smooth playback is a significant challenge. Our real-world measurement study identifies receive-to-composition (R2C) latency, caused by ineffective jitter buffer management, as the primary factor contributing to increased MTP latency. To address this, we introduce JitBright, a client-side optimization strategy that dynamically reduces MTP latency through adaptive jitter buffer management. By adjusting buffer levels based on smoothing playback probability and implementing proactive keyframe requests to mitigate frame dependency, JitBright minimizes both active and passive waiting times. Our large-scale evaluation, conducted over 591,000 sessions across diverse network conditions (WiFi, 4G, 5G) and device types, demonstrates significant improvements in user experience. JitBright reduces median R2C latency by up to 87.5%, increases the proportion of sessions meeting strict MTP latency requirements by 6%-27%, and decreases the video freeze rate from 2.4%-2.8% to 0.4%-1.0%.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411870882",
    "type": "article"
  },
  {
    "title": "Visuo-Tactile Class-Incremental Learning",
    "doi": "https://doi.org/10.1145/3754452",
    "publication_date": "2025-07-25",
    "publication_year": 2025,
    "authors": "Hao Fu; Binglun Wang; Wei Ji; Fengyu Yang; Hanbin Zhao; Chao Zhang; Roger Zimmermann; Hui Qian",
    "corresponding_authors": "",
    "abstract": "The ability to associate sight with touch is essential for human and robot agents to understand material properties and to interact with the physical world. In the real-world scenarios, the robot agents often operate in a dynamically-changing environment where new classes of objects are continually collected by visual and tactile sensors. In this paper, we define this scenario as V isuo- T actile C lass- I ncremental L earning (VT-CIL). In practical VT-CIL, the robot needs to adapt to a new environment with constrained storage and computing resources, and suffers from the severe forgetting of vision and touch knowledge about old environments. To alleviate this problem, we consider visuo-tactile correlations in VT-CIL and propose a novel framework. It efficiently incorporates the Visuo-Tactile Cross-Modal Pseudo-Label Consistent (VT-CMPLC) constraint, Dual-Visuo-Tactile Exemplars (DVT-E) and a Dual-Visuo-Tactile-Compatible (DVT-C) constraint. The old visual-tactile classes are preserved by the VT-CMPLC constraint and DVT-E, while the visuo-tactile correlations and the VT-CMPLC and DVT-E capabilities are enhanced by the DVT-C constraint. We built two benchmarks, the Touch-and-Go Class-Incremental (TaG-CI) benchmark and the ObjectFolder-Real Class-Incremental (OFR-CI) benchmark. Experimental results on TaG-CI and OFR-CI benchmarks demonstrate the effectiveness of our method against previous state-of-the-art class-incremental learning methods in VT-CIL.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412910063",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on Text-Multimedia Retrieval: Retrieving Multimedia Data by Means of Natural Language",
    "doi": "https://doi.org/10.1145/3750451",
    "publication_date": "2025-07-25",
    "publication_year": 2025,
    "authors": "Alex Falcon; Giuseppe Serra; Sérgio Escalera; Michael Wray",
    "corresponding_authors": "",
    "abstract": "This paper firstly studies the features of content-based multimedia information retrieval, then analyzes the system structure of content-based multimedia information retrieval and retrieval procedures, lastly the author discusses the exist limitations ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412910168",
    "type": "article"
  },
  {
    "title": "KANformer: Dual-Priors-Guided Low-Light Enhancement via KAN and Transformer",
    "doi": "https://doi.org/10.1145/3750732",
    "publication_date": "2025-07-25",
    "publication_year": 2025,
    "authors": "Chenyang Lu; Zhou Wei; Huapeng Wu; Le Sun; Tianming Zhan",
    "corresponding_authors": "",
    "abstract": "Images captured under low-light conditions suffer from poor visibility and clarity due to insufficient light. The emergence of deep learning has greatly boosted the development of low-light enhancement techniques and achieved promising results. However, while these low-light enhancement methods have enhanced the perceptual effects of human vision, their results in high-level visual tasks (e.g., object detection and semantic segmentation) are still unstable and even sometimes bring negative effects. Therefore, in this work, we propose a new model, KANformer, which uses a semantic-gradient prior as a guide to recover pixels relevant to the image subject from both high-frequency and low-frequency perspectives. Specifically, our model consists of three key components: Low-Frequency Enhancement (LFE) module, which aims to enhance the restoration of the image subject via the semantic prior obtained from SAM; Low-Frequency-Based High-Frequency Enhancement (LFHE) module, which utilizes the KAN module to obtain information from the low-frequency features conducive to the enhancement of high-frequency features; and Gradient-Based High-Frequency Enhancement (GHE) module, which aims to utilize the original gradient as prior to further enhance the structural information of the image and reduce the effect of noise. In addition, we introduce the discrete wavelet transform as down-sampling method while transforming the spatial domain features to the frequency domain for processing. Experiments on multiple paired and unpaired datasets show that our method achieves better visualization and image fidelity compared to other state-of-the-art methods. In addition, experiments on object detection and segmentation show that our method provides better enhancement in improving low-light high-level vision tasks.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412910176",
    "type": "article"
  },
  {
    "title": "Unbiased Embodied Visual Representation Learning with Causal Inference and Cross-Modality Alignment",
    "doi": "https://doi.org/10.1145/3760261",
    "publication_date": "2025-08-12",
    "publication_year": 2025,
    "authors": "Jiaxu Kang; Bolei Chen; Ping Zhong; Y.W. Wang; H.R. Yang; Yu Sheng",
    "corresponding_authors": "",
    "abstract": "Object Goal Navigation (ObjectNav) in novel environments relies on comprehensive scene understanding, including precise visual perception and accurate modeling of spatial-semantic regularities. However, excessive attention to the hand-crafted scene representation in prevailing approaches leads to the neglect of the negative influence of the perception bias hidden in the visual observations. The hand-crafted semantic distribution in domestic environments causes the spurious association bias, while the semantic conflict bias arises due to the dynamic perspective changes. Biased visual perception significantly limits the generalization of the navigation strategy. In this paper, we propose the U nbiased E mbodied V isual R epresentation( UEVR ), which overcomes the perception biases using causal inference and cross-modality alignment. Specifically, we establish reasonable assumptions about confounders for multi-object features through our proposed Unbiased Causal R-CNN framework and eliminate the spurious associations bias through B ack-door I ntervention C ausal A djustment( BICA ) module during navigation. To overcome the dynamic-view bias hidden in 2D image features, we propose to employ the cross-modality alignment mechanism with the Geo metric Con straints( GeoCon ) to encode 3D geometry prior into the 2D representations. Finally, we design a modular ObjectNav framework integrated with UEVR named Causal-ObjectNav , which consists of the corner-based scene exploration module and target object discrimination module. Extensive experiments on the MP3D and HM3D datasets demonstrate the superiority of the unbiased navigation model over existing ObjectNav methods.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413105302",
    "type": "article"
  },
  {
    "title": "DADA++: Dual Alignment Domain Adaptation for Unsupervised Video-Text Retrieval",
    "doi": "https://doi.org/10.1145/3759252",
    "publication_date": "2025-08-08",
    "publication_year": 2025,
    "authors": "Xiaoshuai Hao; Haimei Zhao; Yunfeng Diao; Rong Yin; Guangyin Jin; Jing Zhang; Wanqian Zhang; Wei Zhou",
    "corresponding_authors": "",
    "abstract": "Video-text retrieval aims at returning the most semantically relevant videos given a textual query, which is a thriving topic in both computer vision and natural language processing communities. This paper focuses on a more challenging task, i.e., Unsupervised Domain Adaptation Video-text Retrieval (UDAVR), wherein training and testing data come from different distributions. Previous approaches are mostly derived from classification-based domain adaptation methods, which are neither multi-modal nor suitable for retrieval tasks. They merely alleviate the domain shift while overlooking the pairwise misalignment issue in the target domain, i.e., there exist no semantic relationships between target videos and texts. While Foundation Models like CLIP perform well in in-domain video-text retrieval, their effectiveness significantly drops during domain shifts due to this lack of alignment. To tackle this, we propose a novel method named D ual A lignment D omain A daptation ( DADA++ ). Specifically, we first introduce cross-modal semantic embedding to generate discriminative source features in a joint embedding space. Besides, we utilize cross-modal domain adaptations to balance the minimization of domain shift in a smooth manner. Furthermore, we empirically identify the pairwise misalignment in the target domain, and thus propose the i ntegrated D ual A lignment C onsistency (iDAC). The proposed iDAC adaptively aligns the video-text pairs, which are more likely to be relevant in the target domain, by verifying their cross-modal semantic proximity reciprocally in both hard and soft manners. This enables positive pairs to increase progressively while potentially aligning noisy pairs throughout the training procedure. We also provide insights into the functionality of DADA++ through the lens of Foundation Models, explaining its superiority in a theoretical way. Compared with state-of-the-art methods, DADA++ achieves 9.4% and 8.5% relative improvements on R@1 under the settings of TGIF \\(\\rightarrow\\) MSR-VTT and TGIF \\(\\rightarrow\\) MSVD respectively, demonstrating its superior performance.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413243613",
    "type": "article"
  },
  {
    "title": "Progressive Generative Steganography via High-Resolution Image Generation for Covert Communication",
    "doi": "https://doi.org/10.1145/3760531",
    "publication_date": "2025-08-18",
    "publication_year": 2025,
    "authors": "Zhili Zhou; Wensheng Zhang; Zhengdao Li; Huilin Ge; Bin Qiu; Fengjun Xiao; Yongfeng Huang",
    "corresponding_authors": "",
    "abstract": "Recently, as one of the most popular covert communication technologies, generative steganography has received ever-increasing attention due to its promising performance against sophisticated steganalysis tools. However, it is quite difficult for the existing generative steganographic approaches to find a good trade-off between hiding capacity and extraction accuracy, mainly due to the small capacity of their hiding spaces. To overcome this shortcoming, a Progressive Generative Steganographic (PGS) network architecture is proposed to hide a secret message during the progressive image generation process to realize secure covert communication. Specifically, we first propose a robust Secret-to-Noise (S2N) mapping method to encode the secret message as a set of noise maps. Then, guided by these noise maps, a set of corresponding images ranging from low resolution to high resolution are progressively generated by the single generative adversarial networks (SINGAN). Consequently, a large-sized secret message can be hidden in the finally generated high-resolution image, since a set of high-capacity hiding spaces can be provided by the process of progressive image generation. Moreover, to improve the quality of image generation and the accuracy of secret message extraction, a Dense Secret-Feature Connection (DSFC) strategy is designed and integrated into the proposed PGS network architecture. Extensive experiments demonstrate that the proposed PGS outperforms the existing approaches in the aspects of both hiding capacity and message extraction, while maintaining promising anti-detectability and imperceptibility for covert communication.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413279388",
    "type": "article"
  },
  {
    "title": "VideoGNN: Video Representation Learning via Dynamic Graph Modelling",
    "doi": "https://doi.org/10.1145/3760532",
    "publication_date": "2025-08-18",
    "publication_year": 2025,
    "authors": "Zijie Zhou; Mingliang Zhou; Jun Luo; Huayan Pu; Leong Hou U; Xuekai Wei; Weijia Jia",
    "corresponding_authors": "",
    "abstract": "Graphs offer a flexible structure for vision tasks, with CNNs and Transformers conditioned as two specific cases of graph structures. In CNNs, the input images are treated as graphs where only neighboring patches are connected, whereas Transformers view images as fully connected graphs. To leverage the potential of graphs in video representation learning, effective graph generation and training methods are crucial. To this end, we propose VideoGNN, which represents the video as a discrete time dynamic graph and learns the dynamic graph efficiently. Given the multitude of frames in videos, we introduce an efficient graph generation module characterized by low complexity and high quality, facilitating the transformation of videos into dynamic graphs. Additionally, we introduce a dual-view graph neural network to capture spatial and temporal information from the generated dynamic graphs. Then, a sequential model is applied to capture the long-term temporal information and generate the final frame embeddings. Experiments demonstrate that VideoGNN can achieve competitive results in terms of graph quality assessment and video downstream tasks. The codes are available at https://github.com/Dodo-D-Caster/VideoGNN .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413279398",
    "type": "article"
  },
  {
    "title": "Multi-scale Historical Trajectory Decomposition for Viewport Prediction in 360-degree Videos",
    "doi": "https://doi.org/10.1145/3760533",
    "publication_date": "2025-08-18",
    "publication_year": 2025,
    "authors": "Huiyi Zhou; Feng Zhao; Chunhai Li",
    "corresponding_authors": "",
    "abstract": "360-degree panoramic video provides users with an unprecedented immersive experience and is rapidly evolving with the support of virtualized devices. Effective viewport prediction is crucial for alleviating high-speed bandwidth constraints and enhancing user quality of service. However, most existing research relies on saliency maps derived from multi-user viewport trajectories, often neglecting the rich multi-scale information inherent in individual user viewport trajectory. Inspired by signal decomposition theory, we propose an Empirical Mode Decomposition-based LSTM-MLP (EMD-ML) model. The EMD-ML extracts robust spatiotemporal representations from user viewport trajectory at multiple scales. Leveraging multiscale representations and a hybrid learning framework, the model achieves accurate long-term viewport prediction from limited short-term viewport trajectory. The EMD-ML model achieves nearly a 50% reduction in orthodromic distance compared to state-of-the-art methods across four publicly available datasets. Additionally, experiments on different video types and training datasets show that our method generalizes well and can be applied in real-world scenarios.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413279426",
    "type": "article"
  },
  {
    "title": "Cross-Modal Tri-Semantic Correlation-CLIP for Short Video Homogenization Recognition",
    "doi": "https://doi.org/10.1145/3762193",
    "publication_date": "2025-08-19",
    "publication_year": 2025,
    "authors": "Jiacheng Yao; Jing Zhang; Shuying Zhang; Zhuo Li",
    "corresponding_authors": "",
    "abstract": "Short videos are one of the most popular social media in the world, triggering a proliferation of copycat creations leading to homogenized video content, with visual and textual homogenization being the most prevalent. Unlike near-duplicate video retrieval, which relies on visual appearance similarity, homogenization recognition emphasizes identifying videos with similar semantic units. Short videos exhibit multimodal features, in which there is a many-to-many mapping relationship between visual and text elements, and the two modalities are relatively independent and semantically correlated. Therefore, cross-modal semantic correlation needs to be explored and established to achieve homogenization recognition of short videos. Based on the idea of divide-and-conquer and joint processing, we propose a cross-modal tri-semantic correlation-CLIP (CS 3 C-CLIP) for short video homogenization recognition. First, visual and text features in the shared subspace are extracted using the contrastive language-image pre-training visual-text dual encoder. Then, features at the patch, frame, and video levels are generated using the patch selection module and the temporal encoder, while the word-level and sentence-level features are respectively derived from text features and [EOS] token. After establishing cross-modal tri-semantic correlations by constructing a triple semantic (i.e., video-sentence, frame-sentence and patch-word) correlation, homogenized short videos are recognized by measuring the aggregated cross-modal similarity between pairs of short videos. Experimental results on three publicly available datasets demonstrate that our CS 3 C-CLIP outperforms state-of-the-art methods, achieving 85.7% R@1 and 94.4% R@5 on self-built BJUT-HCD, 49.4% R@1 and 74.6% R@5 on MSR-VTT, and 49.8% R@1 and 78.1% R@5 on MSVD, respectively.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413328360",
    "type": "article"
  },
  {
    "title": "EIN: Exposure-Induced Network for Single-Image HDR Reconstruction",
    "doi": "https://doi.org/10.1145/3763240",
    "publication_date": "2025-08-25",
    "publication_year": 2025,
    "authors": "Yue Liu; Zhangkai Ni; Peilin Chen; Shiqi Wang; Xinfeng Zhang; Hanli Wang; Sam Kwong",
    "corresponding_authors": "",
    "abstract": "Reconstructing high dynamic range (HDR) images from standard dynamic range (SDR) ones has received growing attention in recent years. A predominant problem of this task lies in the absence of texture and structural information in under/over-exposed regions. In this paper, we propose an efficient and stable single-image HDR reconstruction method, namely exposure-induced network (EIN). More specifically, a dynamic range expansion branch (DB) is designed to expand the global dynamic range of the input SDR image. Moreover, two exposure-gated detail recovering branches for local over- (OB) and under- (UB) exposed regions are proposed to interact with the DB to progressively infer the texture and structural details with the learned confidence maps to resolve challenging ambiguities in such regions. The features from these three interactional branches are adaptively fused in the joint global-local decoder to reconstruct the final HDR image. The proposed network is trained based upon a large-scale dataset constructed with diverse content. Extensive experimental results demonstrate that the proposed model achieves consistent visual quality improvement for input SDR images with different exposures compared with state-of-the-art methods. The source code is available at: https://github.com/Yliu724/EIN",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413506901",
    "type": "article"
  },
  {
    "title": "PrivaMod: Uncertainty-Aware Multimedia Fusion with Privacy Guarantees for NFT Visual and Transaction Analysis",
    "doi": "https://doi.org/10.1145/3762999",
    "publication_date": "2025-08-25",
    "publication_year": 2025,
    "authors": "Kombou Victor; Qi Xia; Hu Xia; Jianbin Gao; Wei Zhang; Eyezo’o Benjamin Fabien; Stephane Richard Befoum; Anto Leoba Jonathan; Kuiche Sop Brinda Leaticia",
    "corresponding_authors": "",
    "abstract": "Non-fungible token (NFT) markets present a dual analytical challenge: integrating heterogeneous data modalities (high-dimensional visual features and discrete transaction sequences) while preserving privacy for sensitive wallet addresses and trading strategies. Current approaches analyze visual attributes or transaction patterns in isolation, missing critical value drivers from cross-modal interactions. Meanwhile, existing multimodal techniques lack formal privacy guarantees, exposing participants to inference attacks. This paper introduces PrivaMod, a privacy-preserving Bayesian framework that addresses these limitations through uncertainty-aware multimodal fusion. Our approach implements precision-weighted Bayesian fusion that dynamically adjusts modality contributions based on quantified uncertainty levels, while integrating Rényi Differential Privacy throughout the pipeline via calibrated noise injection and adaptive gradient clipping. Evaluated on 167,492 CryptoPunk transactions, PrivaMod achieves a market efficiency score of 0.874 and R 2 of 0.912, outperforming existing methods by 13.4% through superior cross-modal integration while maintaining strong privacy guarantees ( \\(\\varepsilon\\) = 0.08, \\(\\delta\\) = 1e-5) with membership inference attack success rates near random guessing (53.4%). The system demonstrates that privacy-preserving techniques can enhance rather than compromise analytical performance, establishing a foundation for responsible market analysis. To ensure reproducibility, we release our code, preprocessed datasets, and model checkpoints 1 with detailed documentation and scripts to replicate all experiments.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413507117",
    "type": "article"
  },
  {
    "title": "MambaVesselNet++: A Hybrid CNN-Mamba Architecture for Medical Image Segmentation",
    "doi": "https://doi.org/10.1145/3757324",
    "publication_date": "2025-08-26",
    "publication_year": 2025,
    "authors": "Qing Xu; Yanming Chen; Yue Li; Ziyu Liu; Zhenye Lou; Yixuan Zhang; Huizhong Zheng; Xiangjian He",
    "corresponding_authors": "",
    "abstract": "Medical image segmentation plays an important role in computer-aided diagnosis. Traditional convolution-based U-shape segmentation architectures are usually limited by the local receptive field. Existing vision transformers have been widely applied to diverse medical segmentation frameworks due to their superior capabilities of capturing global contexts. Despite the advantage, the real-world application of vision transformers is challenged by their non-linear self-attention mechanism, requiring huge computational costs. To address this issue, the selective state space model (SSM) Mamba has gained recognition for its adeptness in modeling long-range dependencies in sequential data, particularly noted for its efficient memory costs. In this paper, we propose MambaVesselNet++, a Hybrid CNN-Mamba framework for medical image segmentation. Our MambaVesselNet++ is comprised of a hybrid image encoder (Hi-Encoder) and a bifocal fusion decoder (BF-Decoder). In Hi-Encoder, we first devise the texture-aware layer to capture low-level semantic features by leveraging convolutions. Then, we utilize Mamba to effectively model long-range dependencies with linear complexity. The Bi-Decoder adopts skip connections to combine local and global information of the Hi-Encoder for the accurate generation of segmentation masks. Extensive experiments demonstrate that MambaVesselNet++ outperforms current convolution-based, transformer-based, and Mamba-based state-of-the-arts across diverse medical 2D, 3D, and instance segmentation tasks. The code is available at https://github.com/CC0117/MambaVesselNet .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413682217",
    "type": "article"
  },
  {
    "title": "Boosting Transferability of Adversarial Examples with Spatio-Temporal Context",
    "doi": "https://doi.org/10.1145/3766545",
    "publication_date": "2025-09-10",
    "publication_year": 2025,
    "authors": "J. Christina Wang; Xiaolong Li; Bin Ma; Yao Zhao",
    "corresponding_authors": "",
    "abstract": "Transferable adversarial examples have received increasing attention for their utility in spoofing multiple models, but existing attacks still perform poorly in terms of transferability. In light of this, a novel attack method called Spatio-Temporal Context-based Enhanced Momentum Iteration (STCEMI) is proposed for transferability enhancement. First, two spatially and temporally oriented context exploitation strategies are devised, respectively. On the one hand, the blended image is obtained by summing a randomly scrambled version of the original image with itself, and the correction of spatial context momentum to the gradient of the current position is achieved by utilizing the blended image to optimize the perturbation. On the other hand, with the short-time context obtained from single-step iteration along the backward and forward gradient directions, the gradient of the current iteration can be corrected by the temporal context momentum. Secondly, considering the complementarity of spatial and temporal contexts, two strategies are naturally integrated to construct the spatio-temporal context-based attack, STCEMI, with the objective of achieving stronger transferability. The results of extensive experiments demonstrate that the adversarial images generated by STCEMI achieve the highest cross-model attack success rate across multiple mainstream normally trained and adversarially trained models.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414091831",
    "type": "article"
  },
  {
    "title": "Micro-Expression Recognition via Fine-Grained Dynamic Perception",
    "doi": "https://doi.org/10.1145/3765901",
    "publication_date": "2025-09-09",
    "publication_year": 2025,
    "authors": "Zhiwen Shao; Yingfan Cheng; Fan Zhang; Xuehuai Shi; Canlin Li; Lizhuang Ma; Dit‐Yan Yeung",
    "corresponding_authors": "",
    "abstract": "Facial micro-expression recognition (MER) is a challenging task, due to the transience, subtlety, and dynamics of micro-expressions (MEs). Most existing methods resort to hand-crafted features or deep networks, in which the former often additionally requires key frames, and the latter suffers from small-scale and low-diversity training data. In this paper, we develop a novel fine-grained dynamic perception (FDP) framework for MER. We propose to rank frame-level features of a sequence of raw frames in chronological order, in which the rank process encodes the dynamic information of both ME appearances and motions. Specifically, a novel local-global feature-aware transformer is proposed for frame representation learning. A rank scorer is further adopted to calculate rank scores of each frame-level feature. Afterwards, the rank features from rank scorer are pooled in temporal dimension to capture dynamic representation. Finally, the dynamic representation is shared by a MER module and a dynamic image construction module, in which the former predicts the ME category, and the latter uses an encoder-decoder structure to construct the dynamic image. The design of dynamic image construction task is beneficial for capturing facial subtle actions associated with MEs and alleviating the data scarcity issue. Extensive experiments show that our method (i) significantly outperforms the state-of-the-art MER methods, and (ii) works well for dynamic image construction. Particularly, our FDP improves by 4.05%, 2.50%, 7.71%, and 2.11% over the previous best results in terms of F1-score on the CASME II, SAMM, CAS(ME)^2, and CAS(ME)^3 datasets, respectively. The code is available at https://github.com/CYF-cuber/FDP.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414210614",
    "type": "article"
  },
  {
    "title": "Boosting Foreground-Background Disentanglement for Camouflaged Object Detection",
    "doi": "https://doi.org/10.1145/3768584",
    "publication_date": "2025-09-18",
    "publication_year": 2025,
    "authors": "Jiesheng Wu; Fangwei Hao; Jing Xu",
    "corresponding_authors": "",
    "abstract": "In nature, certain objects exhibit patterns that closely resemble their backgrounds, a phenomenon commonly referred to as Camouflaged Object Detection (COD). We argue that existing COD approaches often suffer from insufficient discriminability for these objects, which we attribute to a lack of effective disentangling of foreground and background representations. To address this, we propose a novel Foreground-Background Disentanglement Network (FBD-Net) that enhances foreground-background disentanglement learning to improve discriminability. Specifically, we design an Edge-guided Foreground-Background Decoupling (EFBD) module, which facilitates the separated learning of foreground and background representations. Additionally, we introduce the Foreground-Background Representation Disentangling Head (DisHead) to further boost the discriminative power of the model. The DisHead consists of two objectives: the Edge Objective and the FoBa Objective. Furthermore, we propose three complementary modules: the Context Aggregation Module (CAM) for initial coarse object detection, the Scale-Interaction Enhanced Pyramid (SIEP) for multi-scale information extraction, and the Cross-Stage Adaptive Fusion (CSAF) module for subtle clue accumulation. Extensive experiments demonstrate that both our CNN-based and Transformer-based FBD-Nets outperform 26 state-of-the-art COD methods across four public datasets. Codes will be released on https://github.com/TomorrowJW/FBD-Net-COD .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414321224",
    "type": "article"
  },
  {
    "title": "Health-oriented Multimodal Food Question Answering with Implicit and Explicit Knowledge",
    "doi": "https://doi.org/10.1145/3766065",
    "publication_date": "2025-09-12",
    "publication_year": 2025,
    "authors": "Menghao Hu; Yaguang Song; Xiaoshan Yang; Yaowei Wang; Changsheng Xu",
    "corresponding_authors": "",
    "abstract": "Health-oriented food analysis has become a research hotspot in recent years because it can help people keep away from unhealthy diets. Remarkable advancements have been made in recipe retrieval, food recommendation, nutrition analysis and calorie estimation. However, existing works still cannot well balance the individual preference and the health. Multimodal food question and answering (MFQA) presents substantial promise for practical applications, yet it remains underexplored. In this paper, we introduce a health-oriented MFQA dataset with 9,000 Chinese question-answer pairs based on a multimodal food knowledge graph (MFKG) collected from a food-sharing website. Additionally, we propose a novel framework for MFQA in the health domain that leverages implicit general knowledge and explicit domain-specific knowledge. The framework comprises four key components: implicit general knowledge injection module (IGKIM), explicit domain-specific knowledge retrieval module (EDKRM), ranking module and answer module. The IGKIM facilitates knowledge acquisition at both the feature and text levels. The EDKRM retrieves the most relevant candidate knowledge from the knowledge graph based on the given question. The ranking module sorts the results retrieved by EDKRM and further retrieve candidate knowledge relevant to the problem. Subsequently, the answer module thoroughly analyzes the multimodal information in the query along with the retrieved relevant knowledge to predict accurate answers. Extensive experimental results on the MFQA dataset demonstrate the effectiveness of our proposed method. The code and dataset are available at https://github.com/Wjianghai/HMFQA .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414596664",
    "type": "article"
  },
  {
    "title": "Fine-grained Stroke Recognition in Broadcast Table Tennis Videos with ATDT",
    "doi": "https://doi.org/10.1145/3769299",
    "publication_date": "2025-09-30",
    "publication_year": 2025,
    "authors": "Tang-Chen Chang; Duen-Chian Jheng; Huang Liang; Bill Louis Harchan; P.C. Ching; Tsung-Hsun Tsai; C.S. Chang; Te-Cheng Wu; Yung‐Hui Li; Tse–Yu Pan; Hung‐Kuo Chu; Min‐Chun Hu",
    "corresponding_authors": "",
    "abstract": "This study introduces an automated system for fine-grained stroke recognition in broadcast table tennis videos, designed to address challenges in manual annotation and tactical analysis during international competitions. The proposed framework integrates an Adaptive Temporal Difference Model with a Transformer Encoder (ATDT), leveraging a combination of Temporal Difference Networks (TDN) and Temporal Adaptive Modules (TAM) to enhance spatial and temporal feature extraction. To enhance feature discriminability, we employ supervised contrastive learning, which promotes better representation learning for fine-grained action recognition. The system is divided into two primary modules: the Action Segmentation Module (ASM) and the Action Recognition Module (ARM). ASM precisely identifies the start and end times of each stroke action by incorporating ball trajectory analysis to identify precise hit timings and placements. The precise segmentation facilitates the subsequent ARM to implement a three-stage recognition process: forehand and backhand classification, group-based classification, and intra-group action classification. This hierarchical approach improves the system’s ability to differentiate between subtle stroke variations, even under the constraints of low-resolution broadcast footage. To validate the framework, the MISTT dataset was collected, comprising 3,618 stroke action clips from 18 international matches, with professional player annotations. The proposed ATDT model outperformed existing methods, achieving a top-1 accuracy improvement of 18% for forehand strokes and 25.58% for backhand strokes compared to baseline models. Moreover, our automatic annotation system takes only 1/30 of the time compared to the manual annotation process, demonstrating its efficiency.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414629128",
    "type": "article"
  },
  {
    "title": "HIN: Hierarchical Interaction Network for Image Captioning",
    "doi": "https://doi.org/10.1145/3769866",
    "publication_date": "2025-09-30",
    "publication_year": 2025,
    "authors": "Chuanle Song; Wei Zhou; Han Yi Jiao; Wenjin Huang; Junfeng Li; Yihua Huang",
    "corresponding_authors": "",
    "abstract": "The purpose of the image captioning task is to understand the content of an image and generate corresponding descriptive text. Traditional approaches to image captioning typically generate descriptive text by extracting different types of visual features from an image and performing feature interactions. However, these methods often fail to fully exploit the interactions between different types of visual features, leading to suboptimal feature integration. To address this limitation, we propose a novel H ierarchical I nteraction N etwork (HIN), designed to continuously extract and interact with different types of visual features to perform more effective multilevel feature interactions. Our HIN consists of three key modules: firstly, we design the C ross- T ype F eature A lignment (CTFA) encoder, which aligns different types of visual features by three global features, so that the subsequent modules can effectively carry out the hierarchical interaction; secondly, the H ierarchical I nteraction (HI) module, which utilizes different types of multilevel features output from the encoder to carry out feature interactions and information mining, so as to generate fully mined multilevel features. The B ottom-up G ated A ttention F usion (BGAF) decoder is finally designed to perform the multilevel decoding of the features mined by our HI module, further enhancing the feature interaction capabilities of our HIN. Moreover, additional experiments on the MS-COCO dataset show that our model achieves new state-of-the-art performance. All codes are available at https://github.com/songchuanle-1/HIN .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414629165",
    "type": "article"
  },
  {
    "title": "MAFF-Net: A Multi-level Acoustic Feature Fusion Network For Synthetic Audio Detection",
    "doi": "https://doi.org/10.1145/3767331",
    "publication_date": "2025-09-30",
    "publication_year": 2025,
    "authors": "Dong Chen; Fan Huang; Chengxin Chen; Zhengxuan Song; Ge Lin; Kun Zeng",
    "corresponding_authors": "",
    "abstract": "Voice spoofing attacks have become a significant challenge in today’s security domain. Although progress has been made in synthetic speech detection technology, existing detection methods still struggle to effectively identify unknown attack strategies. To address these challenges, we propose a novel multi-level acoustic feature fusion framework, MAFF-Net, which comprises three main components: multi-level acoustic feature extraction, cross-attention feature fusion and graph-aggregated detection module. The multi-level acoustic feature extraction module involves two complementary processes: multi-spectrogram feature extraction, which captures low-level physical characteristics of the audio signal, and Wav2vec2 feature extraction, which focuses on high-level speech representations. These multi-level features are subsequently integrated through cross-attention, enhancing the discriminative power of the model. To better evaluate the generalization capability of the proposed model, we introduce Chinese Advanced Synthetic Speech Dataset (CASSD), a new dataset that incorporates speech generated using 11 state-of-the-art synthesis techniques. Extensive experiments conducted across four different datasets demonstrate that our approach consistently outperforms existing single-model methods, highlighting the superior performance of MAFF-Net in synthetic speech detection.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414629169",
    "type": "article"
  },
  {
    "title": "Multi-space Representation Fusion Enhanced Monocular Depth Estimation via Virtual Point Cloud",
    "doi": "https://doi.org/10.1145/3770076",
    "publication_date": "2025-10-01",
    "publication_year": 2025,
    "authors": "Lin Bie; Siqi Li; Xiaopin Zhong; Zongze Wu; Yue Gao",
    "corresponding_authors": "",
    "abstract": "Monocular depth estimation (MDE) is a fundamental problem in computer vision with broad applications in various downstream tasks. While recent studies focus on designing increasingly complex and powerful deep learning methods to regress depth maps directly, we propose a novel approach by introducing the virtual point cloud (VPC) as an intermediate representation to provide the approximate geometric prior for the MDE task. In this paper, we design a multi-scale multi-space representation fusion-enhanced monocular depth estimation framework to address the challenges of MDE. Specifically, to resolve the issue of scale ambiguity, we design a VPC feature extraction module to learn multi-scale 3D geometric information for the depth prior. Then, we explicitly introduce geometric constraints for global depth prediction by incorporating a multi-space representation fusion from both the texture features in 2D space and the geometric features in 3D space. To mitigate errors at object boundaries, we introduce a confidence map generated based on the quality of the VPC to refine the predicted depth map. Specifically, we construct convolution receptive fields based on 3D spatial distances in spherical coordinates, ensuring that the confidence map provides reliable geometric guidance at object boundaries. Furthermore, we propose an independent confidence geometric consistency loss to supervise the refinement process. Experimental results demonstrate that our method significantly outperforms state-of-the-art approaches across all evaluation metrics on the KITTI and NYU-Depth-v2 datasets, achieving RMSE improvements of 9.2% and 2.8%, respectively. Moreover, zero-shot evaluations on the nuScenes and SUN-RGBD datasets further validate the generalizability of our approach.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414688435",
    "type": "article"
  },
  {
    "title": "<i>CCM-Net:</i> <i>C</i> ontrastive and <i>C</i> onsistent <i>M</i> ulti- <i>T</i> ask Network for Artifact Segmentation and Quality Classification of OCTA Images",
    "doi": "https://doi.org/10.1145/3765745",
    "publication_date": "2025-10-07",
    "publication_year": 2025,
    "authors": "Yang Wen; Xiangning Wang; Jixue Tang; Ping Li; Lei Zhu; Jing Qin; Xiaokang Yang; Bin Sheng",
    "corresponding_authors": "",
    "abstract": "Artifacts are prevalent in Optical Coherence Tomography Angiography (OCTA) images which probably interfere doctor's diagnosis and greatly limit its utility. Therefore, it is desirable to segment artifacts and assess quality when using them for diagnosis. In this paper, we propose an end-to-end network (named CCM-Net : C ontrastive and C onsistent M ulti-task network) to jointly address artifact segmentation and quality classification of OCTA images. We first devise multiple Task-Specific Attention Blocks (TAB) to integrate deep features at different CNN layers for segmenting artifacts and classifying the quality of the input OCTA image. In this way, the weights of different deep features can be automatically learned and are not the same for the two tasks. Moreover, we devise a contrastive loss and a consistency loss to leverage sample relations for further enhancing prediction accuracy. Specifically, given an input OCTA image, we first augment it with a color jitter and select another OCTA image with the same quality classification label. We then design a contrastive loss so that the segmentation results of the input OCTA image are similar to its enhanced OCTA image, while the segmentation results of the two selected OCTA images are not similar. Besides, we devise a consistency loss on the classification results of the three images, because we can find that these images have the same quality classification labels. Experiments on an in-house OCTA dataset (Multi-OCTA) demonstrate that the proposed CCM-Net outperforms state-of-the-art (SOTA) methods.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414914262",
    "type": "article"
  },
  {
    "title": "A piano duo support system for parents to lead children to practice musical performances",
    "doi": "https://doi.org/10.1145/1230812.1230815",
    "publication_date": "2007-05-01",
    "publication_year": 2007,
    "authors": "Chika Oshima; Kazushi Nishimoto; Norihiro Hagita",
    "corresponding_authors": "",
    "abstract": "In this article, we propose “Family Ensemble,” a piano duo support system for a musically inept parent and his/her child who is a beginner at playing the piano. The system makes it easier for parents to correctly reproduce a given sequence of pitches along with the child's performance by using score tracking and note-replacement functions. The experiments with this support system showed that the parents can immediately participate in the piano duo. Furthermore, we found that during joint practices using Family Ensemble some subjects discussed musical ideas that they would not have talked about without using the system.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W1974959169",
    "type": "article"
  },
  {
    "title": "An open architecture for transport-level protocol coordination in distributed multimedia applications",
    "doi": "https://doi.org/10.1145/1236471.1236476",
    "publication_date": "2007-08-01",
    "publication_year": 2007,
    "authors": "David Ott; Ketan Mayer‐Patel",
    "corresponding_authors": "",
    "abstract": "We consider the problem of flow coordination in distributed multimedia applications. Most transport-level protocols are designed to operate independently and lack mechanisms for sharing information with other flows and coordinating data transport in various ways. This limitation becomes problematic in distributed applications that employ numerous flows between two computing clusters sharing the same intermediary forwarding path across the Internet. In this article, we propose an open architecture that supports the sharing of network state information, peer flow information, and application-specific information. Called simply the coordination protocol (CP) , the scheme facilitates coordination of network resource usage across flows belonging to the same application, as well as aiding other types of coordination. The effectiveness of our approach is illustrated in the context of multistreaming in 3D tele-immersion where consistency of network information across flows both greatly improves frame transport synchrony and minimizes buffering delay.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W1994359704",
    "type": "article"
  },
  {
    "title": "Online audio background determination for complex audio environments",
    "doi": "https://doi.org/10.1145/1230812.1230814",
    "publication_date": "2007-05-01",
    "publication_year": 2007,
    "authors": "Simon Moncrieff; Svetha Venkatesh; Geoff West",
    "corresponding_authors": "",
    "abstract": "We present a method for foreground/background separation of audio using a background modelling technique. The technique models the background in an online, unsupervised, and adaptive fashion, and is designed for application to long term surveillance and monitoring problems. The background is determined using a statistical method to model the states of the audio over time. In addition, three methods are used to increase the accuracy of background modelling in complex audio environments. Such environments can cause the failure of the statistical model to accurately capture the background states. An entropy-based approach is used to unify background representations fragmented over multiple states of the statistical model. The approach successfully unifies such background states, resulting in a more robust background model. We adaptively adjust the number of states considered background according to background complexity, resulting in the more accurate classification of background models. Finally, we use an auxiliary model cache to retain potential background states in the system. This prevents the deletion of such states due to a rapid influx of observed states that can occur for highly dynamic sections of the audio signal. The separation algorithm was successfully applied to a number of audio environments representing monitoring applications.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2057746219",
    "type": "article"
  },
  {
    "title": "ScaleFFS",
    "doi": "https://doi.org/10.1145/1404880.1404889",
    "publication_date": "2008-10-01",
    "publication_year": 2008,
    "authors": "Dawoon Jung; Jaegeuk Kim; Jin‐Soo Kim; Joonwon Lee",
    "corresponding_authors": "",
    "abstract": "NAND flash memory has become one of the most popular storage media for mobile multimedia systems. A key issue in designing storage systems for mobile multimedia systems is handling large-capacity storage media and numerous large files with limited resources such as memory. However, existing flash file systems, including JFFS2 and YAFFS in particular, exhibit many limitations in addressing the storage capacity of mobile multimedia systems. In this article, we design and implement a scalable flash file system, called ScaleFFS, for mobile multimedia systems. ScaleFFS is designed to require only a small fixed amount of memory space and to provide fast mount time, even if the file system size grows to more than tens of gigabytes. The measurement results show that ScaleFFS can be instantly mounted regardless of the file system size, while achieving the same write bandwidth and up to 22% higher read bandwidth compared to JFFS2.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2084115908",
    "type": "article"
  },
  {
    "title": "<i>SEVA</i>",
    "doi": "https://doi.org/10.1145/1556134.1556141",
    "publication_date": "2009-08-01",
    "publication_year": 2009,
    "authors": "Xiaotao Liu; Mark Corner; Prashant Shenoy",
    "corresponding_authors": "",
    "abstract": "In this article, we study how a sensor-rich world can be exploited by digital recording devices such as cameras and camcorders to improve a user's ability to search through a large repository of image and video files. We design and implement a digital recording system that records identities and locations of objects (as advertised by their sensors) along with visual images (as recorded by a camera). The process, which we refer to as Sensor-Enhanced Video Annotation (SEVA) , combines a series of correlation, interpolation, and extrapolation techniques. It produces a tagged stream that later can be used to efficiently search for videos or frames containing particular objects or people. We present detailed experiments with a prototype of our system using both stationary and mobile objects as well as GPS and ultrasound. Our experiments show that: (i) SEVA has zero error rates for static objects, except very close to the boundary of the viewable area; (ii) for moving objects or a moving camera, SEVA only misses objects leaving or entering the viewable area by 1--2 frames; (iii) SEVA can scale to 10 fast-moving objects using current sensor technology; and (iv) SEVA runs online using relatively inexpensive hardware.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2061965506",
    "type": "article"
  },
  {
    "title": "Scalable on-demand media streaming for heterogeneous clients",
    "doi": "https://doi.org/10.1145/1404880.1404888",
    "publication_date": "2008-10-01",
    "publication_year": 2008,
    "authors": "Phillipa Gill; Liqi Shi; Anirban Mahanti; Zongpeng Li; Derek L. Eager",
    "corresponding_authors": "",
    "abstract": "Periodic broadcast protocols enable efficient streaming of highly popular media files to large numbers of concurrent clients. Most previous periodic broadcast protocols, however, assume that all clients can receive at the same rate, and also assume that reception bandwidth is not time-varying. In this article, we first develop a new periodic broadcast protocol, Optimized Heterogeneous Periodic Broadcast (OHPB), that can be optimized for a given population of clients with heterogeneous reception bandwidths and quality-of-service requirements. The OHPB protocol utilizes an optimized segment size progression determined by solving a linear optimization model that takes as input the client population characteristics and an objective function such as mean client startup delay. We then develop a generalization of the OHPB linear optimization model that allows optimal server bandwidth allocation among multiple concurrent OHPB broadcasts, wherein each media file and its clients may have different characteristics. Finally, we propose complementary client protocols employing work-ahead buffering of data during playback, so as to enable more uniform playback quality when the reception bandwidth is time-varying.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2149913911",
    "type": "article"
  },
  {
    "title": "Playout buffer and rate optimization for streaming over IEEE 802.11 wireless networks",
    "doi": "https://doi.org/10.1145/1556134.1556143",
    "publication_date": "2009-08-01",
    "publication_year": 2009,
    "authors": "Mingzhe Li; Mark Claypool; Robert Kinicki",
    "corresponding_authors": "",
    "abstract": "Most streaming rate selection and buffer optimization algorithms are developed for wired networks and can perform poorly over wireless networks. Wireless MAC layer behavior, such as rate adaptation, retransmissions, and medium sharing, can significantly degrade the effectiveness of current streaming algorithms. This article presents the Buffer and Rate Optimization for Streaming (BROS) algorithm to improve streaming performance. BROS uses a bandwidth estimation tool designed specifically for wireless networks and models the relationship between buffer size, streaming data rate, and available bandwidth distribution. BROS optimizes the streaming data rate and initial buffer size, resulting in a high data rate but with few frame losses and buffer underflow events, while still keeping a small initial buffer delay. BROS is implemented in the Emulated Streaming (EmuS) client-server system and evaluated on an IEEE 802.11 wireless testbed with various wireless conditions. The evaluation shows that BROS can effectively optimize the streaming rate and initial buffer size based on wireless network bandwidth conditions, thus achieving better performance than static rate or buffer selection and jitter removal buffers.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2161265889",
    "type": "article"
  },
  {
    "title": "Source traffic analysis",
    "doi": "https://doi.org/10.1145/1823746.1823755",
    "publication_date": "2010-08-01",
    "publication_year": 2010,
    "authors": "João V. Gomes; Pedro R. M. Inácio; Branka Lakic; Mário M. Freire; Henrique Silva; Paulo P. Monteiro",
    "corresponding_authors": "",
    "abstract": "Traffic modeling and simulation plays an important role in the area of Network Monitoring and Analysis, for it provides practitioners with efficient tools to evaluate the performance of networks and of their elements. This article focus on the traffic generated by a single source, providing an overview of what was done in the field and studying the statistical properties of the traffic produced by a personal computer, including analysis of the autocorrelation structure. Different distributions were fitted to the interarrival times, packet sizes, and byte count processes with the goal of singling out the ones most suitable for traffic generation.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W1997272817",
    "type": "article"
  },
  {
    "title": "Reference index-based H.264 video watermarking scheme",
    "doi": "https://doi.org/10.1145/2344436.2344439",
    "publication_date": "2012-09-01",
    "publication_year": 2012,
    "authors": "Jian Li; Hongmei Liu; Jiwu Huang; Yun Q. Shi",
    "corresponding_authors": "",
    "abstract": "Video watermarking has received much attention over the past years as a promising solution to copy protection. Watermark robustness is still a key issue of research, especially when a watermark is embedded in the compressed video domain. In this article, a robust watermarking scheme for H.264 video is proposed. During video encoding, the watermark is embedded in the index of the reference frame, referred to as reference index, a bitstream syntax element newly proposed in the H.264 standard. Furthermore, the video content (current coded blocks) is modified based on an optimization model, aiming at improving watermark robustness without unacceptably degrading the video's visual quality or increasing the video's bit rate. Compared with the existing schemes, our method has the following three advantages: (1) The bit rate of the watermarked video is adjustable; (2) the robustness against common video operations can be achieved; (3) the watermark embedding and extraction are simple. Extensive experiments have verified the good performance of the proposed watermarking scheme.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2035509563",
    "type": "article"
  },
  {
    "title": "Pithos",
    "doi": "https://doi.org/10.1145/3105577",
    "publication_date": "2017-07-12",
    "publication_year": 2017,
    "authors": "Herman A. Engelbrecht; John S. Gilmore",
    "corresponding_authors": "",
    "abstract": "There has been significant research effort into peer-to-peer (P2P) massively multi-user virtual environments (MMVEs). A number of architectures have been proposed to implement the P2P approach; however, the development of fully distributed MMVEs has met with a number of challenges. In this work, we address one of the key remaining challenges of state consistency and persistency in P2P MMVEs. Having reviewed state management and persistency architectures currently receiving research attention, we have identified deficiencies such as lack of load balancing, responsiveness, and scalability. To address these deficiencies, we present Pithos—a reliable, responsive, secure, load-balanced, and scalable distributed storage system, suited to P2P MMVEs. Pithos is designed specifically for P2P MMVEs, and we show that it improves the reliability and responsiveness of storage architectures as compared to existing P2P state persistency architectures. Pithos is implemented as an OverSim simulation running on the OMNeT++ network simulation framework. It is evaluated using up to 10,400 peers, with realistic latency profiles, with up to 15.8 million storage and retrieval requests that are generated to store a total of 2.4 million objects. Each peer in Pithos uses a maximum of 1,950Bps bandwidth to achieve 99.98% storage reliability, while the most reliable overlay storage configuration tested only achieved 93.65% reliability, using 2,182Bps bandwidth. Pithos is also more responsive than overlay storage, with an average responsiveness of 0.192s, compared with the average overlay responsiveness of 1.4s when retrieving objects from storage.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2735921256",
    "type": "article"
  },
  {
    "title": "Modeling and Analysis of Power Consumption in Live Video Streaming Systems",
    "doi": "https://doi.org/10.1145/3115505",
    "publication_date": "2017-09-18",
    "publication_year": 2017,
    "authors": "Yousef Sharrab; Nabil J. Sarhan",
    "corresponding_authors": "",
    "abstract": "This article develops an aggregate power consumption model for live video streaming systems, including many-to-many systems. In many-to-one streaming systems, multiple video sources (i.e., cameras and/or sensors) stream videos to a monitoring station. We model the power consumed by the video sources in the capturing, encoding, and transmission phases and then provide an overall model in terms of the main capturing and encoding parameters, including resolution, frame rate, number of reference frames, motion estimation range, and quantization. We also analyze the power consumed by the monitoring station due to receiving, decoding, and upscaling the received video streams. In addition to modeling the power consumption, we model the achieved bitrate of video encoding. We validate the developed models through extensive experiments using two types of systems and different video contents. Furthermore, we analyze many-to-one systems in terms of bitrate, video quality, and the power consumed by the sources, as well as that by the monitoring station, considering the impacts of multiple parameters simultaneously.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2756177808",
    "type": "article"
  },
  {
    "title": "Visual Background Recommendation for Dance Performances Using Deep Matrix Factorization",
    "doi": "https://doi.org/10.1145/3152463",
    "publication_date": "2018-01-16",
    "publication_year": 2018,
    "authors": "Jiqing Wen; James She; Xiaopeng Li; Hui Mao",
    "corresponding_authors": "",
    "abstract": "The stage background is one of the most important features for a dance performance, as it helps to create the scene and atmosphere. In conventional dance performances, the background images are usually selected or designed by professional stage designers according to the theme and the style of the dance. In new media dance performances, the stage effects are usually generated by media editing software. Selecting or producing a dance background is quite challenging and is generally carried out by skilled technicians. The goal of the research reported in this article is to ease this process. Instead of searching for background images from the sea of available resources, dancers are recommended images that they are more likely to use. This work proposes the idea of a novel system to recommend images based on content-based social computing. The core part of the system is a probabilistic prediction model to predict a dancer’s interests in candidate images through social platforms. Different from traditional collaborative filtering or content-based models, the model proposed here effectively combines a dancer’s social behaviors (rating action, click action, etc.) with the visual content of images shared by the dancer using deep matrix factorization (DMF). With the help of such a system, dancers can select from the recommended images and set them as the backgrounds of their dance performances through a media editor. According to the experiment results, the proposed DMF model outperforms the previous methods, and when the dataset is very sparse, the proposed DMF model shows more significant results.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2785570910",
    "type": "article"
  },
  {
    "title": "Combining Facial Parts For Learning Gender, Ethnicity, and Emotional State Based on RGB-D Information",
    "doi": "https://doi.org/10.1145/3152125",
    "publication_date": "2018-03-06",
    "publication_year": 2018,
    "authors": "Safaa Azzakhnini; Lahoucine Ballihi; Driss Aboutajdine",
    "corresponding_authors": "",
    "abstract": "With the success of emerging RGB-D cameras such as the Kinect sensor, combining the shape (depth) and texture information to improve the quality of recognition became a trend among computer vision researchers. In this work, we address the problem of face classification in the context of RGB images and depth data. Inspired by the psychological results for human face perception, this article focuses on (i) finding out which facial parts are most effective at making the difference for some social aspects of face perception (gender, ethnicity, and emotional state), (ii) determining the optimal decision by combining the decision rendered by the individual parts, and (iii) extracting the promising features from RGB-D faces to exploit all the potential that this data provide. Experimental results on EurecomKinect Face and CurtinFaces databases show that the proposed approach improves the recognition quality in many use cases.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2790930009",
    "type": "article"
  },
  {
    "title": "Virtual Portraitist",
    "doi": "https://doi.org/10.1145/3288760",
    "publication_date": "2019-01-24",
    "publication_year": 2019,
    "authors": "Chuan-Shen Hu; Yi-Tsung Hsieh; Hsiao-Wei Lin; Mei-Chen Yeh",
    "corresponding_authors": "",
    "abstract": "Smart photography carries the promise of quality improvement and functionality extension in making aesthetically appealing pictures. In this article, we focus on self-portrait photographs and introduce new methods that guide a user in how to best pose while taking a selfie. While most of the current solutions use a post processing procedure to beautify a picture, the developed tool enables a novel function of recommending a good look before the photo is captured. Given an input face image, the tool automatically estimates the pose-based aesthetic score, finds the most attractive angle of the face, and suggests how the pose should be adjusted. The recommendation results are determined adaptively to the appearance and initial pose of the input face. We apply a data mining approach to find distinctive, frequent itemsets and association rules from online profile pictures, upon which the aesthetic estimation and pose recommendation methods are developed. A simulated and a real image set are used for experimental evaluation. The results show the proposed aesthetic estimation method can effectively select user-favorable photos. Moreover, the recommendation performance for the vertical adjustment is moderately related to the degree of conformity among the professional photographers’ recommendations. This study echoes the trend of instant photo sharing, in which a user takes a picture and then immediately shares it on a social network without engaging in tedious editing.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2915000894",
    "type": "article"
  },
  {
    "title": "Multi-level Similarity Perception Network for Person Re-identification",
    "doi": "https://doi.org/10.1145/3309881",
    "publication_date": "2019-05-31",
    "publication_year": 2019,
    "authors": "Chen Shen; Zhongming Jin; Wenqing Chu; Rongxin Jiang; Yaowu Chen; Guo-Jun Qi; Xian‐Sheng Hua",
    "corresponding_authors": "",
    "abstract": "In this article, we propose a novel deep Siamese architecture based on a convolutional neural network (CNN) and multi-level similarity perception for the person re-identification (re-ID) problem. According to the distinct characteristics of diverse feature maps, we effectively apply different similarity constraints to both low-level and high-level feature maps during training stage. Due to the introduction of appropriate similarity comparison mechanisms at different levels, the proposed approach can adaptively learn discriminative local and global feature representations, respectively, while the former is more sensitive in localizing part-level prominent patterns relevant to re-identifying people across cameras. Meanwhile, a novel strong activation pooling strategy is utilized on the last convolutional layer for abstract local-feature aggregation to pursue more representative feature representations. Based on this, we propose final feature embedding by simultaneously encoding original global features and discriminative local features. In addition, our framework has two other benefits: First, classification constraints can be easily incorporated into the framework, forming a unified multi-task network with similarity constraints. Second, as similarity-comparable information has been encoded in the network’s learning parameters via back-propagation, pairwise input is not necessary at test time. That means we can extract features of each gallery image and build an index in an off-line manner, which is essential for large-scale real-world applications. Experimental results on multiple challenging benchmarks demonstrate that our method achieves splendid performance compared with the current state-of-the-art approaches.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2949256473",
    "type": "article"
  },
  {
    "title": "Show, Reward, and Tell",
    "doi": "https://doi.org/10.1145/3291925",
    "publication_date": "2019-04-30",
    "publication_year": 2019,
    "authors": "Jinhui Tang; Jing Wang; Zechao Li; Jianlong Fu; Tao Mei",
    "corresponding_authors": "",
    "abstract": "Despite the promising progress made in visual captioning and paragraphing, visual storytelling is still largely unexplored. This task is more challenging due to the difficulty in modeling an ordered photo sequence and in generating a relevant paragraph with expressive language style for storytelling. To deal with these challenges, we propose an Attribute-based Hierarchical Generative model with Reinforcement Learning and adversarial training (AHGRL). First, to model the ordered photo sequence and the complex story structure, we propose an attribute-based hierarchical generator. The generator incorporates semantic attributes to create more accurate and relevant descriptions. The hierarchical framework enables the generator to learn from the complex paragraph structure. Second, to generate story-style paragraphs, we design a language-style discriminator, which provides word-level rewards to optimize the generator by policy gradient. Third, we further consider the story generator and the reward critic as adversaries. The generator aims to create indistinguishable paragraphs to human-level stories, whereas the critic aims at distinguishing them and further improving the generator. Extensive experiments on the widely used dataset well demonstrate the advantages of the proposed method over state-of-the-art methods.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2965353350",
    "type": "article"
  },
  {
    "title": "Expression Robust 3D Facial Landmarking via Progressive Coarse-to-Fine Tuning",
    "doi": "https://doi.org/10.1145/3282833",
    "publication_date": "2019-02-13",
    "publication_year": 2019,
    "authors": "Jia Sun; Di Huang; Yunhong Wang; Liming Chen",
    "corresponding_authors": "",
    "abstract": "Facial landmarking is a fundamental task in automatic machine-based face analysis. The majority of existing techniques for such a problem are based on 2D images; however, they suffer from illumination and pose variations that may largely degrade landmarking performance. The emergence of 3D data theoretically provides an alternative to overcome these weaknesses in the 2D domain. This article proposes a novel approach to 3D facial landmarking, which combines both the advantages of feature-based methods as well as model-based ones in a progressive three-stage coarse-to-fine manner (initial, intermediate, and fine stages). For the initial stage, a few fiducial landmarks (i.e., the nose tip and two inner eye corners) are robustly detected through curvature analysis, and these points are further exploited to initialize the subsequent stage. For the intermediate stage, a statistical model is learned in the feature space of three normal components of the facial point-cloud rather than the smooth original coordinates, namely Active Normal Model (ANM). For the fine stage, cascaded regression is employed to locally refine the landmarks according to their geometry attributes. The proposed approach can accurately localize dozens of fiducial points on each 3D face scan, greatly surpassing the feature-based ones, and it also improves the state of the art of the model-based ones in two aspects: sensitivity to initialization and deficiency in discrimination. The proposed method is evaluated on the BU-3DFE, Bosphorus, and BU-4DFE databases, and competitive results are achieved in comparison with counterparts in the literature, clearly demonstrating its effectiveness.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2998066354",
    "type": "article"
  },
  {
    "title": "Learning Discriminative Sentiment Representation from Strongly- and Weakly Supervised CNNs",
    "doi": "https://doi.org/10.1145/3326335",
    "publication_date": "2019-11-30",
    "publication_year": 2019,
    "authors": "Dongyu She; Ming Sun; Jufeng Yang",
    "corresponding_authors": "",
    "abstract": "Visual sentiment analysis is attracting increasing attention with the rapidly growing amount of images uploaded to social networks. Learning rich visual representations often requires training deep convolutional neural networks (CNNs) on massive manually labeled data, which is expensive or scarce especially for a subjective task like visual sentiment analysis. Meanwhile, a large quantity of social images is quite available yet noisy by querying social networks using the sentiment categories as keywords, where various types of images related to the specific sentiment can be easily collected. In this article, we propose a multiple kernel network for visual sentiment recognition, which learns representation from strongly- and weakly supervised CNNs. Specifically, the weakly supervised deep model is trained using the large-scale data from social images, whereas the strongly supervised deep model is fine tuned on the affecitve datasets with manual annotation. We employ the multiple kernel scheme on the multiple layers of CNNs, which can automatically select the discriminative representation by learning a linear combination from a set of pre-defined kernels. In addition, we introduce a large-scale dataset collected from popular comics of various countries, such as America, Japan, China, and France, which consists of 11,821 images with various artistic styles. Experimental results show that the multiple kernel network achieves consistent improvements over the state-of-the-art methods on the public affective datasets, as well as the newly established Comics dataset. The Comics dataset can be found at http://cv.nankai.edu.cn/projects/Comic.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W3080873008",
    "type": "article"
  },
  {
    "title": "Wireless Multicast for Zoomable Video Streaming",
    "doi": "https://doi.org/10.1145/2801123",
    "publication_date": "2015-08-24",
    "publication_year": 2015,
    "authors": "Hui Wang; Mun Choon Chan; Wei Tsang Ooi",
    "corresponding_authors": "",
    "abstract": "Zoomable video streaming refers to a new class of interactive video applications, where users can zoom into a video stream to view a selected region of interest in higher resolutions and pan around to move the region of interest. The zoom and pan effects are typically achieved by breaking the source video into a grid of independently decodable tiles. Streaming the tiles to a set of heterogeneous users using broadcast is challenging, as users have different link rates and different regions of interest at different resolution levels. In this article, we consider the following problem: Given the subset of tiles that each user requested, the link rate of each user, and the available time slots, at which resolution should each tile be sent, to maximize the overall video quality received by all users. We design an efficient algorithm to solve this problem and evaluate the solution on a testbed using 10 mobile devices. Our method is able to achieve up to 12dB improvements over other heuristic methods.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W1849856486",
    "type": "article"
  },
  {
    "title": "Circle &amp; Search",
    "doi": "https://doi.org/10.1145/2632165",
    "publication_date": "2014-08-01",
    "publication_year": 2014,
    "authors": "Junshi Huang; Si Liu; Junliang Xing; Tao Mei; Shuicheng Yan",
    "corresponding_authors": "",
    "abstract": "Taking the shoe as a concrete example, we present an innovative product retrieval system that leverages object detection and retrieval techniques to support a brand-new online shopping experience in this article. The system, called Circle &amp; Search, enables users to naturally indicate any preferred product by simply circling the product in images as the visual query, and then returns visually and semantically similar products to the users. The system is characterized by introducing attributes in both the detection and retrieval of the shoe. Specifically, we first develop an attribute-aware part-based shoe detection model. By maintaining the consistency between shoe parts and attributes, this shoe detector has the ability to model high-order relations between parts and thus the detection performance can be enhanced. Meanwhile, the attributes of this detected shoe can also be predicted as the semantic relations between parts. Based on the result of shoe detection, the system ranks all the shoes in the repository using an attribute refinement retrieval model that takes advantage of query-specific information and attribute correlation to provide an accurate and robust shoe retrieval. To evaluate this retrieval system, we build a large dataset with 17,151 shoe images, in which each shoe is annotated with 10 shoe attributes e.g., heel height, heel shape, sole shape, etc.). According to the experimental result and the user study, our Circle &amp; Search system achieves promising shoe retrieval performance and thus significantly improves the users' online shopping experience.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2007085438",
    "type": "article"
  },
  {
    "title": "Robust and accurate mobile visual localization and its applications",
    "doi": "https://doi.org/10.1145/2491735",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Heng Liu; Tao Mei; Houqiang Li; Jiebo Luo; Shipeng Li",
    "corresponding_authors": "",
    "abstract": "Mobile applications are becoming increasingly popular. More and more people are using their phones to enjoy ubiquitous location-based services (LBS). The increasing popularity of LBS creates a fundamental problem: mobile localization. Besides traditional localization methods that use GPS or wireless signals, using phone-captured images for localization has drawn significant interest from researchers. Photos contain more scene context information than the embedded sensors, leading to a more precise location description. With the goal being to accurately sense real geographic scene contexts, this article presents a novel approach to mobile visual localization according to a given image (typically associated with a rough GPS position). The proposed approach is capable of providing a complete set of more accurate parameters about the scene geo-context including the real locations of both the mobile user and perhaps more importantly the captured scene, as well as the viewing direction. To figure out how to make image localization quick and accurate, we investigate various techniques for large-scale image retrieval and 2D-to-3D matching. Specifically, we first generate scene clusters using joint geo-visual clustering, with each scene being represented by a reconstructed 3D model from a set of images. The 3D models are then indexed using a visual vocabulary tree structure. Taking geo-tags of the database image as prior knowledge, a novel location-based codebook weighting scheme proposed to embed this additional information into the codebook. The discriminative power of the codebook is enhanced, thus leading to better image retrieval performance. The query image is aligned with the models obtained from the image retrieval results, and eventually registered to a real-world map. We evaluate the effectiveness of our approach using several large-scale datasets and achieving estimation accuracy of a user's location within 13 meters, viewing direction within 12 degrees, and viewing distance within 26 meters. Of particular note is our showcase of three novel applications based on localization results: (1) an on-the-spot tour guide, (2) collaborative routing, and (3) a sight-seeing guide. The evaluations through user studies demonstrate that these applications are effective in facilitating the ideal rendezvous for mobile users.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2052597854",
    "type": "article"
  },
  {
    "title": "Knowledge discovery from 3D human motion streams through semantic dimensional reduction",
    "doi": "https://doi.org/10.1145/1925101.1925104",
    "publication_date": "2011-02-01",
    "publication_year": 2011,
    "authors": "Yohan Jin; Balakrishnan Prabhakaran",
    "corresponding_authors": "",
    "abstract": "3D human motion capture is a form of multimedia data that is widely used in entertainment as well as medical fields (such as orthopedics, physical medicine, and rehabilitation where gait analysis is needed). These applications typically create large repositories of motion capture data and need efficient and accurate content-based retrieval techniques. 3D motion capture data is in the form of multidimensional time-series data. To reduce the dimensions of human motion data while maintaining semantically important features, we quantize human motion data by extracting spatio-temporal features through SVD and translate them onto a symbolic sequential representation through our proposed sGMMEM (semantic Gaussian Mixture Modeling with EM). In order to handle variations in motion capture data due to human body characteristics and speed of motion, we transform the semantically quantized values into a histogram representation. This representation is used as a signature for classification and similarity-based retrieval. We achieved good classification accuracies for “coarse” human motion categories (such as walking 92.85%, run 91.42%, and jump 94.11%) and even for subtle categories (such as dance 89.47%, laugh 83.33%, basketball signal 85.71%, golf putting 80.00%). Experiments also demonstrated that the proposed approach outperforms earlier techniques such as the wMSV (weighted Motion Singular Vector) approach and LB_Keogh method.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2070624965",
    "type": "article"
  },
  {
    "title": "Interactive films and coconstruction",
    "doi": "https://doi.org/10.1145/2043612.2043617",
    "publication_date": "2011-11-01",
    "publication_year": 2011,
    "authors": "Renato Verdugo; Miguél Nussbaum; Pablo Corro; Paulo Palma; Paula Navarrete",
    "corresponding_authors": "",
    "abstract": "Interactive Filmmaking is both an aesthetic and technological challenge. Steerable plots, where audiences are not passive viewers but active participants of the narrative experience, require an engaging narrative model as well as a technologically feasible structure. This article discusses the connection between aesthetics, cinema, and interactivity and presents a model for interactive narration that is based on the audience's ability to read and interpret footage differently according to its context. Through a detour narrative model it is possible to engage audiences in a coconstructive hypermedia experience while at the same time minimizing the amount of footage required. An interface model that allows seamless hypervideo navigation through graphic interaction is also discussed, and the interactive short film The Crime or Revenge of Fernando Moreno is presented, along with user experience and usability studies that experimentally prove our hypothesis.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2092268598",
    "type": "article"
  },
  {
    "title": "Advances in immersive communication",
    "doi": "https://doi.org/10.1145/2492704",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Philip A. Chou",
    "corresponding_authors": "Philip A. Chou",
    "abstract": "The last great advances in immersive communication were the invention of the telephone over 137 years ago and the invention of the video telephone (né television) over 86 years ago. However, a perfect storm is brewing for the next advance in immersive communication, thanks to the convergence of massive amounts of computation, bandwidth, resolution, new sensors, and new displays. It could well be the Multimedia community that turns this brew into the next great advance in immersive communication, something akin to teleportation.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2105225006",
    "type": "article"
  },
  {
    "title": "Discovering Geo-Informative Attributes for Location Recognition and Exploration",
    "doi": "https://doi.org/10.1145/2648581",
    "publication_date": "2014-10-01",
    "publication_year": 2014,
    "authors": "Quan Fang; Jitao Sang; Changsheng Xu",
    "corresponding_authors": "",
    "abstract": "This article considers the problem of automatically discovering geo-informative attributes for location recognition and exploration. The attributes are expected to be both discriminative and representative, which correspond to certain distinctive visual patterns and associate with semantic interpretations. For our solution, we analyze the attribute at the region level. Each segmented region in the training set is assigned a binary latent variable indicating its discriminative capability. A latent learning framework is proposed for discriminative region detection and geo-informative attribute discovery. Moreover, we use user-generated content to obtain the semantic interpretation for the discovered visual attributes. Discriminative and search-based attribute annotation methods are developed for geo-informative attribute interpretation. The proposed approach is evaluated on one challenging dataset including GoogleStreetView and Flickr photos. Experimental results show that (1) geo-informative attributes are discriminative and useful for location recognition; (2) the discovered semantic interpretation is meaningful and can be exploited for further location exploration.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2127585704",
    "type": "article"
  },
  {
    "title": "Improving Concept-Based Image Retrieval with Training Weights Computed from Tags",
    "doi": "https://doi.org/10.1145/2790230",
    "publication_date": "2015-11-02",
    "publication_year": 2015,
    "authors": "Vasileios Papapanagiotou; Christos Diou; Anastasios Delopoulos",
    "corresponding_authors": "",
    "abstract": "This article presents a novel approach to training classifiers for concept detection using tags and a variant of Support Vector Machine that enables the usage of training weights per sample. Combined with an appropriate tag weighting mechanism, more relevant samples play a more important role in the calibration of the final concept-detector model. We propose a complete, automated framework that (i) calculates relevance scores for each image-concept pair based on image tags, (ii) transforms the scores into relevance probabilities and automatically annotates each image according to this probability, (iii) transforms either the relevance scores or the probabilities into appropriate training weights and finally, (iv) incorporates the training weights and the visual features into a Fuzzy Support Vector Machine classifier to build the concept-detector model. The framework can be applied to online public collections, by gathering a large pool of diverse images, and using the calculated probability to select a training set and the associated training weights. To evaluate our argument, we experiment on two large annotated datasets. Experiments highlight the retrieval effectiveness of the proposed approach. Furthermore, experiments with various levels of annotation error show that using weights derived from tags significantly increases the robustness of the resulting concept detectors.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2239739895",
    "type": "article"
  },
  {
    "title": "An SDN Architecture for Privacy-Friendly Network-Assisted DASH",
    "doi": "https://doi.org/10.1145/3092838",
    "publication_date": "2017-06-28",
    "publication_year": 2017,
    "authors": "Jan Willem Kleinrouweler; Sergio Cabrero; Pablo César",
    "corresponding_authors": "",
    "abstract": "Dynamic Adaptive Streaming over HTTP (DASH) is the premier technology for Internet video streaming. DASH efficiently uses existing HTTP-based delivery infrastructures implementing adaptive streaming. However, DASH traffic is bursty in nature. This causes performance problems when DASH players share a network connection or in networks with heavy background traffic. The result is unstable and lower quality video. In this article, we present the design and implementation of a so-called DASH Assisting Network Element (DANE). Our system provides target bitrate signaling and dynamic traffic control. These two mechanisms realize proper bandwidth sharing among clients. Our system is privacy friendly and fully supports encrypted video streams. Trying to improve the streaming experience for users who share a network connection, our system increases the video bitrate and reduces the number of quality switches. We show this through evaluations in our Wi-Fi testbed.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2731928663",
    "type": "article"
  },
  {
    "title": "A Multi-sensor Framework for Personal Presentation Analytics",
    "doi": "https://doi.org/10.1145/3300941",
    "publication_date": "2019-05-31",
    "publication_year": 2019,
    "authors": "Tian Gan; Junnan Li; Yongkang Wong; Mohan Kankanhalli",
    "corresponding_authors": "",
    "abstract": "Presentation has been an effective method for delivering information to an audience for many years. Over the past few decades, technological advancements have revolutionized the way humans deliver presentation. Conventionally, the quality of a presentation is usually evaluated through painstaking manual analysis with experts. Although the expert feedback is effective in assisting users to improve their presentation skills, manual evaluation suffers from high cost and is often not available to most individuals. In this work, we propose a novel multi-sensor self-quantification system for presentations, which is designed based on a new proposed assessment rubric. We present our analytics model with conventional ambient sensors (i.e., static cameras and Kinect sensor) and the emerging wearable egocentric sensors (i.e., Google Glass). In addition, we performed a cross-correlation analysis of speaker’s vocal behavior and body language. The proposed framework is evaluated on a new presentation dataset, namely, NUS Multi-Sensor Presentation dataset, which consists of 51 presentations covering a diverse range of topics. To validate the efficacy of the proposed system, we have conducted a series of user studies with the speakers and an interview with an English communication expert, which reveals positive and promising feedback.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2949135277",
    "type": "article"
  },
  {
    "title": "Joint Stacked Hourglass Network and Salient Region Attention Refinement for Robust Face Alignment",
    "doi": "https://doi.org/10.1145/3374760",
    "publication_date": "2020-02-17",
    "publication_year": 2020,
    "authors": "Junfeng Zhang; Haifeng Hu; Guobin Shen",
    "corresponding_authors": "",
    "abstract": "Facial landmark detection aims to locate keypoints for facial images, which typically suffer from variations caused by arbitrary pose, diverse facial expressions, and partial occlusion. In this article, we propose a coarse-to-fine framework that joins a stacked hourglass network and salient region attention refinement for robust face alignment. To achieve this goal, we first present a multi-scale region learning module to analyze the structure information at a different facial region and extract a strong discriminative deep feature. Then we employ a stacked hourglass network for heatmap regression and initial facial landmarks prediction. Specifically, the stacked hourglass network introduces an improved Inception-ResNet unit as a basic building block, which can effectively improve the receptive field and learn contextual feature representations. Meanwhile, a novel loss function takes into account global weights and local weights to make the heatmap regression more accurate. Different from existing heatmap regression models, we present a salient region attention refinement module to extract a precise feature based on the heatmap regression, and utilize the filtered feature for landmarks refinement to achieve accurate prediction. Extensive experimental results of several challenging datasets (including 300 Faces in the Wild, Caltech Occluded Faces in the Wild, and Annotated Facial Landmarks Faces in the Wild) confirm that our approach can achieve more competitive performance than the most advanced algorithms.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3005538018",
    "type": "article"
  },
  {
    "title": "Spatio-Temporal Deep Residual Network with Hierarchical Attentions for Video Event Recognition",
    "doi": "https://doi.org/10.1145/3378026",
    "publication_date": "2020-04-30",
    "publication_year": 2020,
    "authors": "Yonggang Li; Chunping Liu; Yi Ji; Shengrong Gong; Haibao Xu",
    "corresponding_authors": "",
    "abstract": "Event recognition in surveillance video has gained extensive attention from the computer vision community. This process still faces enormous challenges due to the tiny inter-class variations that are caused by various facets, such as severe occlusion, cluttered backgrounds, and so forth. To address these issues, we propose a spatio-temporal deep residual network with hierarchical attentions (STDRN-HA) for video event recognition. In the first attention layer, the ResNet fully connected feature guides the Faster R-CNN feature to generate object-based attention (O-attention) for target objects. In the second attention layer, the O-attention further guides the ResNet convolutional feature to yield the holistic attention (H-attention) in order to perceive more details of the occluded objects and the global background. In the third attention layer, the attention maps use the deep features to obtain the attention-enhanced features. Then, the attention-enhanced features are input into a deep residual recurrent network, which is used to mine more event clues from videos. Furthermore, an optimized loss function named softmax-RC is designed, which embeds the residual block regularization and center loss to solve the vanishing gradient in a deep network and enlarge the distance between inter-classes. We also build a temporal branch to exploit the long- and short-term motion information. The final results are obtained by fusing the outputs of the spatial and temporal streams. Experiments on the four realistic video datasets, CCV, VIRAT 1.0, VIRAT 2.0, and HMDB51, demonstrate that the proposed method has good performance and achieves state-of-the-art results.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3036280857",
    "type": "article"
  },
  {
    "title": "Posed and Spontaneous Expression Distinction Using Latent Regression Bayesian Networks",
    "doi": "https://doi.org/10.1145/3391290",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "Shangfei Wang; Longfei Hao; Qiang Ji",
    "corresponding_authors": "",
    "abstract": "Facial spatial patterns can help distinguish between posed and spontaneous expressions, but this information has not been thoroughly leveraged by current studies. We present several latent regression Bayesian networks (LRBNs) to capture the patterns existing in facial landmark points and to use those points to differentiate posed from spontaneous expressions. The visible nodes of the LRBN represent facial landmark points. Through learning, the LRBN captures the probabilistic dependencies among landmark points as well as latent variables given observations, successfully modeling the spatial patterns inherent in expressions. Current methods tend to ignore gender and expression categories, although these factors can influence spatial patterns. Therefore, we propose to incorporate this as a kind of privileged information. We construct several LRBNs to capture spatial patterns from spontaneous and posed facial expressions given expression-related factors. Facial landmark points are used during testing to classify samples as either posed or spontaneous, depending on which LRBN has the largest likelihood. We conduct experiments to showcase the superiority of the proposed approach in both modeling spatial patterns and classifying expressions as either posed or spontaneous.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3040747093",
    "type": "article"
  },
  {
    "title": "An Image Privacy Protection Algorithm Based on Adversarial Perturbation Generative Networks",
    "doi": "https://doi.org/10.1145/3381088",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "Chao Tong; Mengze Zhang; Chao Lang; Zhigao Zheng",
    "corresponding_authors": "",
    "abstract": "Today, users of social platforms upload a large number of photos. These photos contain personal private information, including user identity information, which is easily gleaned by intelligent detection algorithms. To thwart this, in this work, we propose an intelligent algorithm to prevent deep neural network (DNN) detectors from detecting private information, especially human faces, while minimizing the impact on the visual quality of the image. More specifically, we design an image privacy protection algorithm by training and generating a corresponding adversarial sample for each image to defend DNN detectors. In addition, we propose an improved model based on the previous model by training an adversarial perturbation generative network to generate perturbation instead of training for each image. We evaluate and compare our proposed algorithm with other methods on wider face dataset and others by three indicators: Mean average precision, Averaged distortion, and Time spent. The results show that our method significantly interferes with DNN detectors while causing weak impact to the visual quality of images, and our improved model does speed up the generation of adversarial perturbations.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3041804732",
    "type": "article"
  },
  {
    "title": "The Impact of Motion and Delay on Selecting Game Targets with a Mouse",
    "doi": "https://doi.org/10.1145/3390464",
    "publication_date": "2020-04-30",
    "publication_year": 2020,
    "authors": "Mark Claypool; Andy Cockburn; Carl Gutwin",
    "corresponding_authors": "",
    "abstract": "All real-time computer games, particularly networked computer games, have a delay from when a player starts an action (e.g., clicking the mouse) until the game renders the result (e.g., firing a projectile). This delay can degrade both player performance (e.g., reduced game score) and quality of experience (e.g., the game is less fun). While previous work has studied the effects of delay on commercial games and individual game actions, a more detailed understanding is needed of the effects of delay on moving target selection with realistic target motion, a common scenario in many games. This paper presents an in-depth study of the effects of delay on the fundamental game action of selecting a moving target with a mouse while varying two parameters for the target motion – turn frequency and turn angle. We design and implement a custom game where players select moving targets using a mouse, while the game controls both the target motion and input delay. Analysis of data gathered in a 56-person user study shows both target selection time and accuracy degrade with delay. However, both selection time and accuracy increase with the frequency and angle of changes in the target’s movement, because turning slows targets down even while making them less predictable. We set these results in the context of other studies of delay and target selection by comparing our findings to those in seven other previously published papers that investigated the effects of delay on target selection.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3062590910",
    "type": "article"
  },
  {
    "title": "A Unified Tensor Framework for Clustering and Simultaneous Reconstruction of Incomplete Imaging Data",
    "doi": "https://doi.org/10.1145/3399806",
    "publication_date": "2020-08-25",
    "publication_year": 2020,
    "authors": "J. E. Francis; M Baburaj; Sudhish N. George",
    "corresponding_authors": "",
    "abstract": "Incomplete observations in the data are always troublesome to data clustering algorithms. In fact, most of the well-received techniques are not designed to encounter such imperative scenarios. Hence, clustering of images under incomplete samples is an inquisitive yet unaddressed area of research. Therefore, the aim of this article is to design a single-stage optimization procedure for clustering as well as simultaneous reconstruction of images without breaking the intrinsic spatial structure. The method employs the self-expressiveness property of submodules, and images are stacked as the lateral slices of a three-dimensional tensor. The proposed optimization method is designed to extract a sparse t -linear combination tensor with low multirank constraint, consisting of a unique set of linear coefficients in the form of mode-3 fibers and the spectral clustering is performed on these fibers. Simultaneously, the recovery of lost samples is accomplished by twisting the entire lateral slices of the data tensor and applying a low-rank approximation on each slice. The prominence of the proposed method lies in the simultaneous execution of data clustering and reconstruction of incomplete observations in a single step. Experimental results reveal the excellence of the proposed method over state-of-the-art clustering algorithms in the context of incomplete imaging data.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3080801374",
    "type": "article"
  },
  {
    "title": "360-Degree VR Video Watermarking Based on Spherical Wavelet Transform",
    "doi": "https://doi.org/10.1145/3425605",
    "publication_date": "2021-02-28",
    "publication_year": 2021,
    "authors": "Yanwei Liu; Jinxia Liu; Antonios Argyriou; Siwei Ma; Liming Wang; Zhen Xu",
    "corresponding_authors": "",
    "abstract": "Similar to conventional video, the increasingly popular 360 <?TeX $^{\\circ }$?> virtual reality (VR) video requires copyright protection mechanisms. The classic approach for copyright protection is the introduction of a digital watermark into the video sequence. Due to the nature of spherical panorama, traditional watermarking schemes that are dedicated to planar media cannot work efficiently for 360 <?TeX $^{\\circ }$?> VR video. In this article, we propose a spherical wavelet watermarking scheme to accommodate 360 <?TeX $^{\\circ }$?> VR video. With our scheme, the watermark is first embedded into the spherical wavelet transform domain of the 360 <?TeX $^{\\circ }$?> VR video. The spherical geometry of the 360 <?TeX $^{\\circ }$?> VR video is used as the host space for the watermark so that the proposed watermarking scheme is compatible with the multiple projection formats of 360 <?TeX $^{\\circ }$?> VR video. Second, the just noticeable difference model, suitable for head-mounted displays (HMDs), is used to control the imperceptibility of the watermark on the viewport. Third, besides detecting the watermark from the spherical projection, the proposed watermarking scheme also supports detecting watermarks robustly from the viewport projection. The watermark in the spherical domain can protect not only the 360 <?TeX $^{\\circ }$?> VR video but also its corresponding viewports. The experimental results show that the embedded watermarks are reliably extracted both from the spherical and the viewport projections of the 360 <?TeX $^{\\circ }$?> VR video, and the robustness of the proposed scheme to various copyright attacks is significantly better than that of the competing planar-domain approaches when detecting the watermark from viewport projection.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3153115310",
    "type": "article"
  },
  {
    "title": "Interactive Search vs. Automatic Search",
    "doi": "https://doi.org/10.1145/3429457",
    "publication_date": "2021-05-11",
    "publication_year": 2021,
    "authors": "Phuong Anh Nguyen; Chong‐Wah Ngo",
    "corresponding_authors": "",
    "abstract": "This article conducts user evaluation to study the performance difference between interactive and automatic search. Particularly, the study aims to provide empirical insights of how the performance landscape of video search changes, with tens of thousands of concept detectors freely available to exploit for query formulation. We compare three types of search modes: free-to-play (i.e., search from scratch), non-free-to-play (i.e., search by inspecting results provided by automatic search), and automatic search including concept-free and concept-based retrieval paradigms. The study involves a total of 40 participants; each performs interactive search over 15 queries of various difficulty levels using two search modes on the IACC.3 dataset provided by TRECVid organizers. The study suggests that the performance of automatic search is still far behind interactive search. Furthermore, providing users with the result of automatic search for exploration does not show obvious advantage over asking users to search from scratch. The study also analyzes user behavior to reveal insights of how users compose queries, browse results, and discover new query terms for search, which can serve as guideline for future research of both interactive and automatic search.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3160105412",
    "type": "article"
  },
  {
    "title": "An Augmented Reality Online Assistance Platform for Repair Tasks",
    "doi": "https://doi.org/10.1145/3429285",
    "publication_date": "2021-05-11",
    "publication_year": 2021,
    "authors": "Lu Sun; Hussein Al Osman; Jochen Lang",
    "corresponding_authors": "",
    "abstract": "Our augmented reality online assistance platform enables an expert to specify 6DoF movements of a component and apply the geometrical and physical constraints in real-time. We track the real components on the expert’s side to monitor the operations of an expert. We leverage a remote rendering technique that we proposed previously to relieve the rendering burden of the augmented reality end devices. By conducting a user study, we show that the proposed method outperforms conventional instructional videos and sketches. The answers to the questionnaires show that the proposed method receives higher recommendation than sketching, and, compared to conventional instructional videos, is outstanding in terms of instruction clarity, preference, recommendation, and confidence of task completion. Moreover, as to the overall user experience, the proposed method has an advantage over the video method.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3161144800",
    "type": "article"
  },
  {
    "title": "-Score-Based Secure Biomedical Model for Effective Skin Lesion Segmentation Over eHealth Cloud",
    "doi": "https://doi.org/10.1145/3430806",
    "publication_date": "2021-06-14",
    "publication_year": 2021,
    "authors": "Amitesh Singh Rajput; Vishesh Kumar Tanwar; Balasubramanian Raman",
    "corresponding_authors": "",
    "abstract": "This study aims to process the private medical data over eHealth cloud platform. The current pandemic situation, caused by Covid19 has made us to realize the importance of automatic remotely operated independent services, such as cloud. However, the cloud servers are developed and maintained by third parties, and may access user’s data for certain benefits. Considering these problems, we propose a specialized method such that the patient’s rights and changes in medical treatment can be preserved. The problem arising due to Melanoma skin cancer is carefully considered and a privacy-preserving cloud-based approach is proposed to achieve effective skin lesion segmentation. The work is accomplished by the development of a Z -score-based local color correction method to differentiate image pixels from ambiguity, resulting the segmentation quality to be highly improved. On the other hand, the privacy is assured by partially order homomorphic Permutation Ordered Binary (POB) number system and image permutation. Experiments are performed over publicly available images from the ISIC 2016 and 2017 challenges, as well as PH <?TeX $^2$?> dataset, where the proposed approach is found to achieve significant results over the encrypted images (known as encrypted domain), as compared to the existing schemes in the plain domain (unencrypted images). We also compare the results with the winners of the ISBI 2016 and 2017 challenges, and show that the proposed approach achieves a very close result with them, even after processing test images in the encrypted domain. Security of the proposed approach is analyzed using a challenge-response game model.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3166718044",
    "type": "article"
  },
  {
    "title": "Equivariant Adversarial Network for Image-to-image Translation",
    "doi": "https://doi.org/10.1145/3458280",
    "publication_date": "2021-06-14",
    "publication_year": 2021,
    "authors": "Masoumeh Zareapoor; Jie Yang",
    "corresponding_authors": "",
    "abstract": "Image-to-Image translation aims to learn an image from a source domain to a target domain. However, there are three main challenges, such as lack of paired datasets, multimodality, and diversity, that are associated with these problems and need to be dealt with. Convolutional neural networks (CNNs), despite of having great performance in many computer vision tasks, they fail to detect the hierarchy of spatial relationships between different parts of an object and thus do not form the ideal representative model we look for. This article presents a new variation of generative models that aims to remedy this problem. We use a trainable transformer, which explicitly allows the spatial manipulation of data within training. This differentiable module can be augmented into the convolutional layers in the generative model, and it allows to freely alter the generated distributions for image-to-image translation. To reap the benefits of proposed module into generative model, our architecture incorporates a new loss function to facilitate an effective end-to-end generative learning for image-to-image translation. The proposed model is evaluated through comprehensive experiments on image synthesizing and image-to-image translation, along with comparisons with several state-of-the-art algorithms.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3167154867",
    "type": "article"
  },
  {
    "title": "Neural-Network-Based Cross-Channel Intra Prediction",
    "doi": "https://doi.org/10.1145/3434250",
    "publication_date": "2021-07-22",
    "publication_year": 2021,
    "authors": "Yue Li; Yan Yi; Dong Liu; Li Li; Zhu Li; Houqiang Li",
    "corresponding_authors": "",
    "abstract": "To reduce the redundancy among different color channels, e.g., YUV, previous methods usually adopt a linear model that tends to be oversimple for complex image content. We propose a neural-network-based method for cross-channel prediction in intra frame coding. The proposed network utilizes twofold cues, i.e., the neighboring reconstructed samples with all channels, and the co-located reconstructed samples with partial channels. Specifically, for YUV video coding, the neighboring samples with YUV are processed by several fully connected layers; the co-located samples with Y are processed by convolutional layers; and the proposed network fuses the twofold cues. We observe that the integration of twofold information is crucial to the performance of intra prediction of the chroma components. We have designed the network architecture to achieve a good balance between compression performance and computational efficiency. Moreover, we propose a transform domain loss for the training of the network. The transform domain loss helps obtain more compact representations of residues in the transform domain, leading to higher compression efficiency. The proposed method is plugged into HEVC and VVC test models to evaluate its effectiveness. Experimental results show that our method provides more accurate cross-channel intra prediction compared with previous methods. On top of HEVC, our method achieves on average 1.3%, 5.4%, and 3.8% BD-rate reductions for Y, Cb, and Cr on common test sequences, and on average 3.8%, 11.3%, and 9.0% BD-rate reductions for Y, Cb, and Cr on ultra-high-definition test sequences. On top of VVC, our method achieves on average 0.5%, 1.7%, and 1.3% BD-rate reductions for Y, Cb, and Cr on common test sequences.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3184371267",
    "type": "article"
  },
  {
    "title": "Adaptive Compression for Online Computer Vision: An Edge Reinforcement Learning Approach",
    "doi": "https://doi.org/10.1145/3447878",
    "publication_date": "2021-11-12",
    "publication_year": 2021,
    "authors": "Zhaoliang He; Hongshan Li; Zhi Wang; Shu‐Tao Xia; Wenwu Zhu",
    "corresponding_authors": "",
    "abstract": "With the growth of computer vision-based applications, an explosive amount of images have been uploaded to cloud servers that host such online computer vision algorithms, usually in the form of deep learning models. JPEG has been used as the de facto compression and encapsulation method for images. However, standard JPEG configuration does not always perform well for compressing images that are to be processed by a deep learning model—for example, the standard quality level of JPEG leads to 50% of size overhead (compared with the best quality level selection) on ImageNet under the same inference accuracy in popular computer vision models (e.g., InceptionNet and ResNet). Knowing this, designing a better JPEG configuration for online computer vision-based services is still extremely challenging. First, cloud-based computer vision models are usually a black box to end-users; thus, it is challenging to design JPEG configuration without knowing their model structures. Second, the “optimal” JPEG configuration is not fixed; instead, it is determined by confounding factors, including the characteristics of the input images and the model, the expected accuracy and image size, and so forth. In this article, we propose a reinforcement learning (RL)-based adaptive JPEG configuration framework, AdaCompress. In particular, we design an edge (i.e., user-side) RL agent that learns the optimal compression quality level to achieve an expected inference accuracy and upload image size, only from the online inference results, without knowing details of the model structures. Furthermore, we design an explore-exploit mechanism to let the framework fast switch an agent when it detects a performance degradation, mainly due to the input change (e.g., images captured across daytime and night). Our evaluation experiments using real-world online computer vision-based APIs from Amazon Rekognition, Face++, and Baidu Vision show that our approach outperforms existing baselines by reducing the size of images by one-half to one-third while the overall classification accuracy only decreases slightly. Meanwhile, AdaCompress adaptively re-trains or re-loads the RL agent promptly to maintain the performance.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3212245772",
    "type": "article"
  },
  {
    "title": "A Generic Utility Model Representing the Quality of Sensory Experience",
    "doi": "https://doi.org/10.1145/2648429",
    "publication_date": "2014-10-01",
    "publication_year": 2014,
    "authors": "Benjamin Rainer; Christian Timmerer",
    "corresponding_authors": "",
    "abstract": "Current QoE research is mainly focusing on single modalities (audio, visual) or combinations thereof. In our research, we propose annotating traditional multimedia content with additional sensory effects, such as ambient light, vibration, wind, and olfaction, which could potentially stimulate all human senses. Investigating the influence of individual sensory effects and combinations thereof is important in order to understand how these individual sensory effects influence the Quality of Experience (QoE) as a whole. In this article, we describe the results of such a subjective quality assessment of audio-visual sequences which are annotated with additional sensory effects such as ambient light, wind, and vibration using the MPEG-V standard. The results of this assessment allow us to derive a utility model representing the Quality of Sensory Experience (QuaSE) complementary to existing QoE models described in terms of Quality of Service (QoS) parameters. For validating our proposed utility model, we provide an example instantiation and validate it against results of subjective quality assessments.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2103818973",
    "type": "article"
  },
  {
    "title": "Atom Decomposition with Adaptive Basis Selection Strategy for Matrix Completion",
    "doi": "https://doi.org/10.1145/2903716",
    "publication_date": "2016-06-15",
    "publication_year": 2016,
    "authors": "Yao Hu; Chen Zhao; Deng Cai; Xiaofei He; Xuelong Li",
    "corresponding_authors": "",
    "abstract": "Estimating missing entries in matrices has attracted much attention due to its wide range of applications like image inpainting and video denoising, which are usually considered as low-rank matrix completion problems theoretically. It is common to consider nuclear norm as a surrogate of the rank operator since it is the tightest convex lower bound of the rank operator under certain conditions. However, most approaches based on nuclear norm minimization involve a number of singular value decomposition (SVD) operations. Given a matrix X ∈ R m × n , the time complexity of the SVD operation is O ( mn 2 ), which brings prohibitive computational burden on large-scale matrices, limiting the further usage of these methods in real applications. Motivated by this observation, a series of atom-decomposition-based matrix completion methods have been studied. The key to these methods is to reconstruct the target matrix by pursuit methods in a greedy way, which only involves the computation of the top SVD and has great advantages in efficiency compared with the SVD-based matrix completion methods. However, due to gradually serious accumulation errors, atom-decomposition-based methods usually result in unsatisfactory reconstruction accuracy. In this article, we propose a new efficient and scalable atom decomposition algorithm for matrix completion called Adaptive Basis Selection Strategy ( ABSS ). Different from traditional greedy atom decomposition methods, a two-phase strategy is conducted to generate the basis separately via different strategies according to their different nature. At first, we globally prune the basis space to eliminate the unimportant basis as much as possible and locate the probable subspace containing the most informative basis. Then, another group of basis spaces are learned to improve the recovery accuracy based on local information. In this way, our proposed algorithm breaks through the accuracy bottleneck of traditional atom-decomposition-based matrix completion methods; meanwhile, it reserves the innate efficiency advantages over SVD-based matrix completion methods. We empirically evaluate the proposed algorithm ABSS on real visual image data and large-scale recommendation datasets. Results have shown that ABSS has much better reconstruction accuracy with comparable cost to atom-decomposition-based methods. At the same time, it outperforms the state-of-the-art SVD-based matrix completion algorithms by similar or better reconstruction accuracy with enormous advantages on efficiency.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2435970246",
    "type": "article"
  },
  {
    "title": "Prediction of Virality Timing Using Cascades in Social Media",
    "doi": "https://doi.org/10.1145/2978771",
    "publication_date": "2016-12-09",
    "publication_year": 2016,
    "authors": "Ming Cheung; James She; Alvin Junus; Lei Cao",
    "corresponding_authors": "",
    "abstract": "Predicting content going viral in social networks is attractive for viral marketing, advertisement, entertainment, and other applications, but it remains a challenge in the big data era today. Previous works mainly focus on predicting the possible popularity of content rather than the timing of reaching such popularity. This work proposes a novel yet practical iterative algorithm to predict virality timing, in which the correlation between the timing and growth of content popularity is captured by using its own big data naturally generated from users’ sharing. Such data is not only able to correlate the dynamics and associated timings in social cascades of viral content but also can be useful to self-correct the predicted timing against the actual timing of the virality in each iterative prediction. The proposed prediction algorithm is verified by datasets from two popular social networks—Twitter and Digg—as well as two synthesized datasets with extreme network densities and infection rates. With about 50% of the required content virality data available (i.e., halfway before reaching its actual virality timing), the error of the predicted timing is proven to be bounded within a 40% deviation from the actual timing. To the best of our knowledge, this is the first work that predicts content virality timing iteratively by capturing social cascades dynamics.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2560439977",
    "type": "article"
  },
  {
    "title": "A Tensor-Based Framework for Software-Defined Cloud Data Center",
    "doi": "https://doi.org/10.1145/2983640",
    "publication_date": "2016-12-12",
    "publication_year": 2016,
    "authors": "Liwei Kuang; Laurence T. Yang; Seungmin Rho; Zheng Yan; Kai Qiu",
    "corresponding_authors": "",
    "abstract": "Multimedia has been exponentially increasing as the biggest big data, which consist of video clips, images, and audio files. Processing and analyzing them on a cloud data center have become a preferred solution that can utilize the large pool of cloud resources to address the problems caused by the tremendous amount of unstructured multimedia data. However, there exist many challenges in processing multimedia big data on a cloud data center, such as multimedia data representation approach, an efficient networking model, and an estimation method for traffic patterns. The primary purpose of this article is to develop a novel tensor-based software-defined networking model on a cloud data center for multimedia big-data computation and communication. First, an overview of the proposed framework is provided, in which the functions of the representative modules are briefly illustrated. Then, three models,—forwarding tensor, control tensor, and transition tensor—are proposed for management of networking devices and prediction of network traffic patterns. Finally, two algorithms about single-mode and multimode tensor eigen-decomposition are developed, and the incremental method is employed for efficiently updating the generated eigen-vector and eigen-tensor. Experimental results reveal that the proposed framework is feasible and efficient to handle multimedia big data on a cloud data center.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2564479228",
    "type": "article"
  },
  {
    "title": "(Compress and Restore) <sup>N</sup> : A Robust Defense Against Adversarial Attacks on Image Classification",
    "doi": "https://doi.org/10.1145/3524619",
    "publication_date": "2022-03-17",
    "publication_year": 2022,
    "authors": "Claudio Ferrari; Federico Becattini; Leonardo Galteri; Alberto Del Bimbo",
    "corresponding_authors": "",
    "abstract": "Modern image classification approaches often rely on deep neural networks, which have shown pronounced weakness to adversarial examples: images corrupted with specifically designed yet imperceptible noise that causes the network to misclassify. In this article, we propose a conceptually simple yet robust solution to tackle adversarial attacks on image classification. Our defense works by first applying a JPEG compression with a random quality factor; compression artifacts are subsequently removed by means of a generative model Artifact Restoration GAN. The process can be iterated ensuring the image is not degraded and hence the classification not compromised. We train different AR-GANs for different compression factors, so that we can change its parameters dynamically at each iteration depending on the current compression, making the gradient approximation difficult. We experiment with our defense against three white-box and two black-box attacks, with a particular focus on the state-of-the-art BPDA attack. Our method does not require any adversarial training, and is independent of both the classifier and the attack. Experiments demonstrate that dynamically changing the AR-GAN parameters is of fundamental importance to obtain significant robustness.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4220981308",
    "type": "article"
  },
  {
    "title": "Optimized Deep-Neural Network for Content-based Medical Image Retrieval in a Brownfield IoMT Network",
    "doi": "https://doi.org/10.1145/3546194",
    "publication_date": "2022-06-30",
    "publication_year": 2022,
    "authors": "Arti Tiwari; Millie Pant",
    "corresponding_authors": "",
    "abstract": "In this paper, a brownfield Internet of Medical Things network is introduced for imaging data that can be easily scaled out depending on the objectives, functional requirements, and the number of facilities and devices connected to it. This is further used to develop a novel Content-based Medical Image Retrieval framework. The developed framework uses DenseNet-201 architecture for generating the image descriptors. Then for classification, the optimized Deep Neural Network model has been configured through a population-based metaheuristic Differential Evolution. Differential Evolution iteratively performs the joint optimization of hyperparameters and architecture of Deep Neural Networks. The competence of the proposed model is validated on three publicly available datasets: Brain Tumor MRI dataset, Covid-19 Radiography database, and Breast Cancer MRI dataset, and by comparing it with selected models over different aspects of performance evaluation. Results show that the convergence rate of the proposed framework is very fast, and it achieves at least 97.28% accuracy across all the models.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4283725562",
    "type": "article"
  },
  {
    "title": "Omniscient Video Super-Resolution with Explicit-Implicit Alignment",
    "doi": "https://doi.org/10.1145/3640346",
    "publication_date": "2024-01-11",
    "publication_year": 2024,
    "authors": "Peng Yi; Zhongyuan Wang; Laigan Luo; Kui Jiang; Zheng He; Junjun Jiang; Tao Lü; Jiayi Ma",
    "corresponding_authors": "",
    "abstract": "When considering the temporal relationships, most previous video super-resolution (VSR) methods follow the iterative or recurrent framework. The iterative framework adopts neighboring low-resolution (LR) frames from a sliding window, while the recurrent framework utilizes the output generated in the previous SR procedure. The hybrid framework combines them but still cannot fully leverage the temporal relationships. Meanwhile, the existing methods are limited in the receptive field of the optical flow or lack semantic constrains on motion information. In this work, we propose an omniscient framework to fully explore the temporal relationships in the video, which encompasses both LR frames and SR outputs from the past, present, and future. The omniscient framework is more generic because the iterative, recurrent, and hybrid frameworks can be regarded as its special cases. Besides, when addressing the motion information, most previous VSR methods adopt the explicit motion estimation and compensation, while many recent methods turn to implicit alignment. In implicit alignment methods, because basic non-local means suffers from heavy computational costs, we improve it by capturing the non-local correlations in a relatively local manner to reduce the complexity. Moreover, we integrate the explicit and implicit methods into an explicit-implicit alignment module to better utilize motion information. We have conducted extensive experiments on public datasets, which show that our method is superior over the state-of-the-art methods in objective metrics, subjective visual quality, and complexity. In particular, on datasets of Vid4 and UDM10, our method improves PSNR by 0.19 dB, 0.49 dB against the most advanced method BasicVSR++, respectively.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4390752025",
    "type": "article"
  },
  {
    "title": "PADVG: A Simple Baseline of Active Protection for Audio-Driven Video Generation",
    "doi": "https://doi.org/10.1145/3638556",
    "publication_date": "2024-01-16",
    "publication_year": 2024,
    "authors": "Huan Liu; Xiaolong Liu; Zichang Tan; Xiaolong Li; Yao Zhao",
    "corresponding_authors": "",
    "abstract": "Over the past few years, deep generative models have significantly evolved, enabling the synthesis of realistic content and also bringing security concerns of illegal misuse. Therefore, active protection for generative models has been proposed recently, aiming to generate samples with hidden messages for future identification while preserving the original generating performance. However, existing active protection methods are specifically designed for generative adversarial networks (GANs), restricted to handling unconditional image generation. We observe that they get limited identification performance and visual quality when handling audio-driven video generation conditioned on target audio and source input to drive video generation with consistent context, e.g., identity and movement, between frame sequences. To address this issue, we introduce a simple yet effective active P rotection framework for A udio- D riven V ideo G eneration, named PADVG. To be specific, we present a novel frame-shared embedding module in which messages to hide are first transformed into frame-shared message coefficients. Then, these coefficients are assembled with the intermediate feature maps of video generators at multiple feature levels to generate the embedded video frames. Besides, PADVG further considers two visual consistent losses: (i) intra-frame loss is utilized to keep the visual consistency with different hidden messages; (ii) inter-frame loss is used to preserve the visual consistency across different video frames. Moreover, we also propose an auxiliary denoising training strategy through perturbing the assembled features by learnable pixel-level noise to improve identification performance, while enhancing robustness against real-world disturbances. Extensive experiments demonstrate that our proposed PADVG for audio-driven video generation can effectively identify the generated videos and achieve high visual quality.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4390918602",
    "type": "article"
  },
  {
    "title": "Discriminative Action Snippet Propagation Network for Weakly Supervised Temporal Action Localization",
    "doi": "https://doi.org/10.1145/3643815",
    "publication_date": "2024-01-31",
    "publication_year": 2024,
    "authors": "Yuanjie Dang; Huang Chun-xia; Peng Chen; Dongdong Zhao; Nan Gao; Ronghua Liang; Ruohong Huan",
    "corresponding_authors": "",
    "abstract": "Weakly supervised temporal action localization (WTAL) aims to classify and localize actions in untrimmed videos with only video-level labels. Recent studies have attempted to obtain more accurate temporal boundaries by exploiting latent action instances in ambiguous snippets or propagating representative action features. However, empirically handcrafted ambiguous snippet extraction and the imprecise alignment of representative snippet propagation lead to challenges in modeling the completeness of actions for these methods. In this article, we propose a Discriminative Action Snippet Propagation Network (DASP-Net) to accurately discover ambiguous snippets in videos and propagate discriminative instance-level features throughout the video for improving action completeness. Specifically, we introduce a novel discriminative feature propagation module for capturing the global contextual attention and propagating the action concept across the whole video by perceiving the discriminative action snippets with instance information from the same video. Simultaneously, we incorporate denoised pseudo-labels as supervision, where we correct the controversial prediction based on the feature space distribution during training, thereby alleviating false detection caused by noise background features. Furthermore, we design an ambiguous feature mining module, which maximizes the feature affinity information of action and background in ambiguous snippets to generate more accurate latent action and background snippets and learns more precise action instance boundaries through contrastive learning of action and background snippets. Extensive experiments show that DASP-Net achieves state-of-the-art results on THUMOS14 and ActivityNet1.2 datasets.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4391387871",
    "type": "article"
  },
  {
    "title": "Detection of Adversarial Facial Accessory Presentation Attacks Using Local Face Differential",
    "doi": "https://doi.org/10.1145/3643831",
    "publication_date": "2024-02-15",
    "publication_year": 2024,
    "authors": "Fei Peng; Le Qin; Min Long; Jin Li",
    "corresponding_authors": "",
    "abstract": "To counter adversarial facial accessory presentation attacks (PAs), a detection method based on local face differential is proposed in this article. It extracts the local face differential features from a suspected face image and a reference face image, and then adaptively fuses the differential features of different local face regions to detect adversarial facial accessory PAs. Meanwhile, the principle of the proposed method is explained by theoretically investigating the local facial differences between a bona fide presentation and an adversarial facial accessory PA when they are compared with a reference face image. To evaluate the proposed method, this article builds a database with different adversarial examples (AEs), presentation attack instruments (PAIs), illumination conditions, and cameras. The experimental results show that it can effectively distinguish between adversarial facial accessory PAs and bona fide presentations, and it has good generalization ability to unseen AEs, PAIs, illumination conditions, and cameras. Moreover, it outperforms the existing AE detection and presentation attack detection methods in detecting adversarial facial accessory PAs.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4391840338",
    "type": "article"
  },
  {
    "title": "MS-GDA: Improving Heterogeneous Recipe Representation via Multinomial Sampling Graph Data Augmentation",
    "doi": "https://doi.org/10.1145/3648620",
    "publication_date": "2024-02-20",
    "publication_year": 2024,
    "authors": "Liangzhe Chen; Wei Li; Xiaohui Cui; Zhenyu Wang; Stefano Berretti; Shaohua Wan",
    "corresponding_authors": "",
    "abstract": "We study the problem of classifying different cooking styles, based on the recipe. The difficulty is that the same food ingredients, seasoning, and the very similar instructions result in different flavors, with different cooking styles. Existing methods have limitations: they mainly focus on homogeneous data (e.g., instruction or image), ignoring heterogeneous data (e.g., flavor compound or ingredient), which certainly hurts the classification performance. This is because collecting enough available heterogeneous data of a recipe is a non-trivial task. In this paper, we present a new heterogeneous data augmentation method to improve classification performance. Specifically, we first construct a heterogeneous recipe graph network to represent heterogeneous data, which includes four main-stream types of heterogeneous data: ingredient, flavor compound, image, and instruction. Then, we draw a sequence of augmented graphs for Semi-Supervised learning through multinomial sampling. The probability distribution of sampling depends on the Cosine distance between the nodes of graph. In this way, we name our approach as Multinomial Sampling Graph Data Augmentation (MS-GDA). Extensive experiments demonstrate that MS-GDA significantly outperforms SOTA baselines on cuisine classification and region prediction with the recipe benchmark dataset. Code is available at https://github.com/LiangzheChen/MS-GDA .",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4391957013",
    "type": "article"
  },
  {
    "title": "An Optimal Edge-weighted Graph Semantic Correlation Framework for Multi-view Feature Representation Learning",
    "doi": "https://doi.org/10.1145/3649466",
    "publication_date": "2024-02-27",
    "publication_year": 2024,
    "authors": "Lei Gao; Zheng Guo; Ling Guan",
    "corresponding_authors": "",
    "abstract": "In this article, we present an optimal edge-weighted graph semantic correlation (EWGSC) framework for multi-view feature representation learning. Different from most existing multi-view representation methods, local structural information and global correlation in multi-view feature spaces are exploited jointly in the EWGSC framework, leading to a new and high-quality multi-view feature representation. Specifically, a novel edge-weighted graph model is first conceptualized and developed to preserve local structural information in each of the multi-view feature spaces. Then, the explored structural information is integrated with a semantic correlation algorithm, labeled multiple canonical correlation analysis (LMCCA), to form a powerful platform for effectively exploiting local and global relations across multi-view feature spaces jointly. We then theoretically verified the relation between the upper limit on the number of projected dimensions and the optimal solution to the multi-view feature representation problem. To validate the effectiveness and generality of the proposed framework, we conducted experiments on five datasets of different scales, including visual-based (University of California Irvine (UCI) iris database, Olivetti Research Lab (ORL) face database, and Caltech 256 database), text-image-based (Wiki database), and video-based (Ryerson Multimedia Lab (RML) audio-visual emotion database) examples. The experimental results show the superiority of the proposed framework on multi-view feature representation over state-of-the-art algorithms.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4392199017",
    "type": "article"
  },
  {
    "title": "Enhanced Video Super-Resolution Network towards Compressed Data",
    "doi": "https://doi.org/10.1145/3651309",
    "publication_date": "2024-03-06",
    "publication_year": 2024,
    "authors": "Feng Li; Yixuan Wu; Anqi Li; Huihui Bai; Runmin Cong; Yao Zhao",
    "corresponding_authors": "",
    "abstract": "Video super-resolution (VSR) algorithms aim at recovering a temporally consistent high-resolution (HR) video from its corresponding low-resolution (LR) video sequence. Due to the limited bandwidth during video transmission, most available videos on the internet are compressed. Nevertheless, few existing algorithms consider the compression factor in practical applications. In this paper, we propose an enhanced VSR model towards compressed videos, termed as ECVSR, to simultaneously achieve compression artifacts reduction and SR reconstruction end-to-end. ECVSR contains a motion-excited temporal adaption network (METAN) and a multi-frame SR network (SRNet). The METAN takes decoded LR video frames as input and models inter-frame correlations via bidirectional deformable alignment and motion-excited temporal adaption, where temporal differences are calculated as motion prior to excite the motion-sensitive regions of temporal features. In SRNet, cascaded recurrent multi-scale blocks (RMSB) are employed to learn deep spatio-temporal representations from adapted multi-frame features. Then, we build a reconstruction module for spatio-temporal information integration and HR frame reconstruction, which is followed by a detail refinement module for texture and visual quality enhancement. Extensive experimental results on compressed videos demonstrate the superiority of our method for compressed VSR. Code will be available at https://github.com/lifengcs/ECVSR .",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4392518683",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on Integrity of Multimedia and Multimodal Data in Internet of Things",
    "doi": "https://doi.org/10.1145/3643040",
    "publication_date": "2024-03-08",
    "publication_year": 2024,
    "authors": "Amit Kumar Singh; Deepa Kundur; Mauro Conti",
    "corresponding_authors": "",
    "abstract": "Internet of Things (IoT) systems cannot successfully realize the notion of ubiquitous connectivity of everything if they are not capable to truly include 'multimedia things'. However, the current research and development activities in the field do not ...",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4392599063",
    "type": "article"
  },
  {
    "title": "Automatic Lyric Transcription and Automatic Music Transcription from Multimodal Singing",
    "doi": "https://doi.org/10.1145/3651310",
    "publication_date": "2024-03-12",
    "publication_year": 2024,
    "authors": "Xiangming Gu; Longshen Ou; Wei Zeng; Jianan Zhang; N. H. Wong; Ye Wang",
    "corresponding_authors": "",
    "abstract": "Automatic lyric transcription (ALT) refers to transcribing singing voices into lyrics, while automatic music transcription (AMT) refers to transcribing singing voices into note events, i.e., musical MIDI notes. Despite these two tasks having significant potential for practical application, they are still nascent. This is because the transcription of lyrics and note events solely from singing audio is notoriously difficult due to the presence of noise contamination, e.g., musical accompaniment, resulting in a degradation of both the intelligibility of sung lyrics and the recognizability of sung notes. To address this challenge, we propose a general framework for implementing multimodal ALT and AMT systems. Additionally, we curate the first multimodal singing dataset, comprising N20EMv1 and N20EMv2, which encompasses audio recordings and videos of lip movements, together with ground truth for lyrics and note events. For model construction, we propose adapting self-supervised learning models from the speech domain as acoustic encoders and visual encoders to alleviate the scarcity of labeled data. We also introduce a residual cross-attention mechanism to effectively integrate features from the audio and video modalities. Through extensive experiments, we demonstrate that our single-modal systems exhibit state-of-the-art performance on both ALT and AMT tasks. Subsequently, through single-modal experiments, we also explore the individual contributions of each modality to the multimodal system. Finally, we combine these and demonstrate the effectiveness of our proposed multimodal systems, particularly in terms of their noise robustness.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4392705114",
    "type": "article"
  },
  {
    "title": "Multi-agent DRL-based Multipath Scheduling for Video Streaming with QUIC",
    "doi": "https://doi.org/10.1145/3649139",
    "publication_date": "2024-03-15",
    "publication_year": 2024,
    "authors": "Xueqiang Han; Biao Han; Jinrong Li; Congxi Song",
    "corresponding_authors": "",
    "abstract": "The popularization of video streaming brings challenges in satisfying diverse Quality of Service (QoS) requirements. The multipath extension of the Quick UDP Internet Connection (QUIC) protocol, also called MPQUIC, has the potential to improve video streaming performance with multiple simultaneously transmitting paths. The multipath scheduler of MPQUIC determines how to distribute the packets onto different paths. However, while applying current multipath schedulers into MPQUIC, our experimental results show that they fail to adapt to various receive buffer sizes of different devices and comprehensive QoS requirements of video streaming. These problems are especially severe under heterogeneous and dynamic network environments. To tackle these problems, we propose MARS, a Multi-agent deep Reinforcement learning (MADRL)-based Multipath QUIC Scheduler, which is able to promptly adapt to dynamic network environments. It exploits the MADRL method to learn a neural network for each path and generate scheduling policy. Besides, it introduces a novel multi-objective reward function that takes out-of-order queue size and different QoS metrics into consideration to realize adaptive scheduling optimization. We implement MARS in an MPQUIC prototype and deploy in Dynamic Adaptive Streaming over HTTP system. Then, we compare it with the state-of-the-art multipath schedulers in both emulated and real-world networks. Experimental results show that MARS outperforms the other schedulers with better adaptive capability regarding the receive buffer sizes and QoS.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4392849905",
    "type": "article"
  },
  {
    "title": "Feature Extraction Matters More: An Effective and Efficient Universal Deepfake Disruptor",
    "doi": "https://doi.org/10.1145/3653457",
    "publication_date": "2024-03-20",
    "publication_year": 2024,
    "authors": "Long Tang; Dengpan Ye; Zhenhao Lu; Yunming Zhang; Chuanxi Chen",
    "corresponding_authors": "",
    "abstract": "Face manipulation can modify a victim’s facial attributes (e.g., age or hair color) in an image, which is an important component of deepfakes. Adversarial examples are an emerging approach to combat the threat of visual misinformation to society. To efficiently protect facial images from being forged, designing a universal face anti-manipulation disruptor is essential. However, existing works treat deepfake disruption as an end-to-end process, ignoring the functional difference between feature extraction and image reconstruction. In this work, we propose FOUND , a novel F eature- O utput ensemble UN iversal D isruptor against face manipulation networks, which explores a new opinion considering attacking feature-extraction (encoding) modules as the critical task in deepfake disruption. We conduct an effective two-stage disruption process. We first perform ensemble disruption on multi-model encoders, maximizing the Wasserstein distance between features before and after the adversarial attack. Then we develop a Gradient-Ensemble strategy to enhance the disruption effect by simplifying the complex optimization problem of disrupting ensemble end-to-end models. Extensive experiments indicate that one FOUND generated with a few facial images can successfully disrupt multiple face manipulation models on cross-attribute and cross-face images, surpassing state-of-the-art universal disruptors in both success rate and efficiency.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4393002335",
    "type": "article"
  },
  {
    "title": "Temporal Scene Montage for Self-Supervised Video Scene Boundary Detection",
    "doi": "https://doi.org/10.1145/3654669",
    "publication_date": "2024-03-26",
    "publication_year": 2024,
    "authors": "Jiawei Tan; Pingan Yang; Lu Chen; Hongxing Wang",
    "corresponding_authors": "",
    "abstract": "Once a video sequence is organized as basic shot units, it is of great interest to temporally link shots into semantic-compact scene segments to facilitate long video understanding. However, it still challenges existing video scene boundary detection methods to handle various visual semantics and complex shot relations in video scenes. We proposed a novel self-supervised learning method, Video Scene Montage for Boundary Detection (VSMBD), to extract rich shot semantics and learn shot relations using unlabeled videos. More specifically, we present Video Scene Montage (VSM) to synthesize reliable pseudo scene boundaries, which learns task-related semantic relations between shots in a self-supervised manner. To lay a solid foundation for modeling semantic relations between shots, we decouple visual semantics of shots into foreground and background. Instead of costly learning from scratch as in most previous self-supervised learning methods, we build our model upon large-scale pre-trained visual encoders to extract the foreground and background features. Experimental results demonstrate VSMBD trains a model with strong capability in capturing shot relations, surpassing previous methods by significant margins. The code is available at https://github.com/mini-mind/VSMBD.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4393181814",
    "type": "article"
  },
  {
    "title": "Text-Guided Synthesis of Masked Face Images",
    "doi": "https://doi.org/10.1145/3654667",
    "publication_date": "2024-03-30",
    "publication_year": 2024,
    "authors": "Tricha Anjali; V. Masilamani",
    "corresponding_authors": "",
    "abstract": "The COVID-19 pandemic has made us all understand that wearing a face mask protects us from the spread of respiratory viruses. The face authentication systems, which are trained on the basis of facial key points such as the eyes, nose, and mouth, found it difficult to identify the person when the majority of the face is covered by the face mask. Removing the mask for authentication will cause the infection to spread. The possible solutions are: (a) to train the face recognition systems to identify the person with the upper face features (b) Reconstruct the complete face of the person with a generative model. (c) train the model with a dataset of the masked faces of the people. In this paper, we explore the scope of generative models for image synthesis. We used stable diffusion to generate masked face images of popular celebrities on various text prompts. A realistic dataset of 15K masked face images of 100 celebrities is generated and is called the Realistic Synthetic Masked Face Dataset (RSMFD). The model and the generated dataset will be made public so that researchers can augment the dataset. According to our knowledge, this is the largest masked face recognition dataset with realistic images. The generated images were tested on popular deep face recognition models and achieved significant results. The dataset is also trained and tested on some of the famous image classification models, and the results are competitive. The dataset is available on this link:- https://drive.google.com/drive/folders/1yetcgUOL1TOP4rod1geGsOkIrIJHtcEw?usp=sharing",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4393342163",
    "type": "article"
  },
  {
    "title": "Effective Video Summarization by Extracting Parameter-free Motion Attention",
    "doi": "https://doi.org/10.1145/3654670",
    "publication_date": "2024-03-30",
    "publication_year": 2024,
    "authors": "Tingting Han; Quan Zhou; Jun Yu; Yu Zhou; Jianhui Zhang; Sicheng Zhao",
    "corresponding_authors": "",
    "abstract": "Video summarization remains a challenging task despite increasing research efforts. Traditional methods focus solely on long-range temporal modeling of video frames, overlooking important local motion information that cannot be captured by frame-level video representations. In this article, we propose the Parameter-free Motion Attention Module (PMAM) to exploit the crucial motion clues potentially contained in adjacent video frames, using a multi-head attention architecture. The PMAM requires no additional training for model parameters, leading to an efficient and effective understanding of video dynamics. Moreover, we introduce the Multi-feature Motion Attention Network (MMAN), integrating the PMAM with local and global multi-head attention based on object-centric and scene-centric video representations. The synergistic combination of local motion information, extracted by the proposed PMAM, with long-range interactions modeled by the local and global multi-head attention mechanism, can significantly enhance the performance of video summarization. Extensive experimental results on the benchmark datasets, SumMe and TVSum, demonstrate that the proposed MMAN outperforms other state-of-the-art methods, resulting in remarkable performance gains.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4393342188",
    "type": "article"
  },
  {
    "title": "RSUIGM: Realistic Synthetic Underwater Image Generation with Image Formation Model",
    "doi": "https://doi.org/10.1145/3656473",
    "publication_date": "2024-04-08",
    "publication_year": 2024,
    "authors": "Chaitra Desai; Sujay Benur; Ujwala Patil; Uma Mudenagudi",
    "corresponding_authors": "",
    "abstract": "In this paper, we propose to synthesize realistic underwater images with a novel image formation model, considering both downwelling depth and line of sight (LOS) distance as cue and call it as Realistic Synthetic Underwater Image Generation Model, RSUIGM. The light interaction in the ocean is a complex process and demands specific modeling of direct and backscattering phenomenon to capture the degradations. Most of the image formation models rely on complex radiative transfer models and in-situ measurements for synthesizing and restoration of underwater images. Typical image formation models consider only line of sight distance z and ignore downwelling depth d in the estimation of effect of direct light scattering. We derive the dependencies of downwelling irradiance in direct light estimation for generation of synthetic underwater images unlike state-of-the-art image formation models. We propose to incorporate the derived downwelling irradiance in estimation of direct light scattering for modeling the image formation process and generate realistic synthetic underwater images with the proposed RSUIGM, and name it as RSUIGM dataset . We demonstrate the effectiveness of the proposed RSUIGM by using RSUIGM dataset in training deep learning based restoration methods. We compare the quality of restored images with state-of-the-art methods using benchmark real underwater image datasets and achieve improved results. In addition, we validate the distribution of realistic synthetic underwater images versus real underwater images both qualitatively and quantitatively. The proposed RSUIGM dataset is available here.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4394569799",
    "type": "article"
  },
  {
    "title": "Psychology-Guided Environment Aware Network for Discovering Social Interaction Groups from Videos",
    "doi": "https://doi.org/10.1145/3657295",
    "publication_date": "2024-04-09",
    "publication_year": 2024,
    "authors": "Jiaqi Yu; Jinhai Yang; Hua Yang; Renjie Pan; Pingrui Lai; Guangtao Zhai",
    "corresponding_authors": "",
    "abstract": "Social interaction is a common phenomenon in human societies. Different from discovering groups based on the similarity of individuals’ actions, social interaction focuses more on the mutual influence between people. Although people can easily judge whether or not there are social interactions in a real-world scene, it is difficult for an intelligent system to discover social interactions. Initiating and concluding social interactions are greatly influenced by an individual’s social cognition and the surrounding environment, which are closely related to psychology. Thus, converting the psychological factors that impact social interactions into quantifiable visual representations and creating a model for interaction relationships poses a significant challenge. To this end, we propose a Psychology-Guided Environment Aware Network (PEAN) that models social interaction among people in videos using supervised learning. Specifically, we divide the surrounding environment into scene-aware visual-based and human-aware visual-based descriptions. For the scene-aware visual clue, we utilize 3D features as global visual representations. For the human-aware visual clue, we consider instance-based location and behaviour-related visual representations to map human-centred interaction elements in social psychology: distance, openness, and orientation. In addition, we design an environment aware mechanism to integrate features from visual clues, with a Transformer to explore the relation between individuals and construct pairwise interaction strength features. The interaction intensity matrix reflecting the mutual nature of the interaction is obtained by processing the interaction strength features with the interaction discovery module. An interaction constrained loss function composed of interaction critical loss function and smooth F β loss function is proposed to optimize the whole framework to improve the distinction of the interaction matrix and alleviate class imbalance caused by pairwise interaction sparsity. Given the diversity of real-world interactions, we collect a new dataset named Social Basketball Activity Dataset (Soical-BAD), covering complex social interactions. Our method achieves the best performance among social-CAD, social-BAD, and their combined dataset named Video Social Interaction Dataset (VSID).",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4394606330",
    "type": "article"
  },
  {
    "title": "DBGAN: Dual Branch Generative Adversarial Network for Multi-Modal MRI Translation",
    "doi": "https://doi.org/10.1145/3657298",
    "publication_date": "2024-04-10",
    "publication_year": 2024,
    "authors": "Jun Lyu; Shouang Yan; M. Shamim Hossain",
    "corresponding_authors": "",
    "abstract": "Existing magnetic resonance imaging translation models rely on generative adversarial networks, primarily employing simple convolutional neural networks. Unfortunately, these networks struggle to capture global representations and contextual relationships within magnetic resonance images. While the advent of Transformers enables capturing long-range feature dependencies, they often compromise the preservation of local feature details. To address these limitations and enhance both local and global representations, we introduce DBGAN , a novel dual-branch generative adversarial network. In this framework, the Transformer branch comprises sparse attention blocks and dense self-attention blocks, allowing for a wider receptive field while simultaneously capturing local and global information. The convolutional neural network branch, built with integrated residual convolutional layers, enhances local modeling capabilities. Additionally, we propose a fusion module that cleverly integrates features extracted from both branches. Extensive experimentation on two public datasets and one clinical dataset validates significant performance improvements with DBGAN. On Brats2018, it achieves a 10% improvement in MAE, 3.2% in PSNR, and 4.8% in SSIM for image generation tasks compared to RegGAN. Notably, the generated MRIs receive positive feedback from radiologists, underscoring the potential of our proposed method as a valuable tool in clinical settings.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4394683396",
    "type": "article"
  },
  {
    "title": "Rank-based Hashing for Effective and Efficient Nearest Neighbor Search for Image Retrieval",
    "doi": "https://doi.org/10.1145/3659580",
    "publication_date": "2024-04-16",
    "publication_year": 2024,
    "authors": "Vinicius Kawai; Lucas Pascotti Valem; Alexandro Baldassin; Edson Borin; Daniel Carlos Guimarães Pedronette; Longin Jan Latecki",
    "corresponding_authors": "",
    "abstract": "The large and growing amount of digital data creates a pressing need for approaches capable of indexing and retrieving multimedia content. A traditional and fundamental challenge consists of effectively and efficiently performing nearest-neighbor searches. After decades of research, several different methods are available, including trees, hashing, and graph-based approaches. Most of the current methods exploit learning to hash approaches based on deep learning. In spite of effective results and compact codes obtained, such methods often require a significant amount of labeled data for training. Unsupervised approaches also rely on expensive training procedures usually based on a huge amount of data. In this work, we propose an unsupervised data-independent approach for nearest neighbor searches, which can be used with different features, including deep features trained by transfer learning. The method uses a rank-based formulation and exploits a hashing approach for efficient ranked list computation at query time. A comprehensive experimental evaluation was conducted on seven public datasets, considering deep features based on CNNs and Transformers. Both effectiveness and efficiency aspects were evaluated. The proposed approach achieves remarkable results in comparison to traditional and state-of-the-art methods. Hence, it is an attractive and innovative solution, especially when costly training procedures need to be avoided.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4394844169",
    "type": "article"
  },
  {
    "title": "Seventeen Years of the ACM Transactions on Multimedia Computing, Communications and Applications: A Bibliometric Overview",
    "doi": "https://doi.org/10.1145/3660347",
    "publication_date": "2024-04-18",
    "publication_year": 2024,
    "authors": "Walayat Hussain; Honghao Gao; Rafiul Karim; Abdulmotaleb El Saddik",
    "corresponding_authors": "",
    "abstract": "ACM Transactions on Multimedia Computing, Communications, and Applications has been dedicated to advancing multimedia research, fostering discoveries, innovations, and practical applications since 2005. The journal consistently publishes top-notch, original research in emerging fields through open submissions, calls for articles, special issues, rigorous review processes, and diverse research topics. This study aims to delve into an extensive bibliometric analysis of the journal, utilising various bibliometric indicators. The article seeks to unveil the latent implications within the journal’s scholarly landscape from 2005 to 2022. The data primarily draws from the Web of Science Core Collection database. The analysis encompasses diverse viewpoints, including yearly publication rates and citations, identifying highly cited articles, and assessing the most prolific authors, institutions, and countries. The article employs VOSviewer-generated graphical maps, effectively illustrating networks of co-citations, keyword co-occurrences, and institutional and national bibliographic couplings. Furthermore, the study conducts a comprehensive global and temporal examination of co-occurrences of the author’s keywords. This investigation reveals the emergence of numerous novel keywords over the past decades.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4394925399",
    "type": "article"
  },
  {
    "title": "Domain-invariant and Patch-discriminative Feature Learning for General Deepfake Detection",
    "doi": "https://doi.org/10.1145/3657297",
    "publication_date": "2024-04-27",
    "publication_year": 2024,
    "authors": "Jian Zhang; Jiangqun Ni; Fan Nie; Jiwu Huang",
    "corresponding_authors": "",
    "abstract": "Hyper-realistic avatars in the metaverse have already raised security concerns about deepfake techniques, deepfakes involving generated video “recording” may be mistaken for a real recording of the people it depicts. As a result, deepfake detection has drawn considerable attention in the multimedia forensic community. Though existing methods for deepfake detection achieve fairly good performance under the intra-dataset scenario, many of them gain unsatisfying results in the case of cross-dataset testing with more practical value, where the forged faces in training and testing datasets are from different domains. To tackle this issue, in this paper, we propose a novel Domain-Invariant and Patch-Discriminative feature learning framework - DI&amp;PD. For image-level feature learning, a single-side adversarial domain generalization is introduced to eliminate domain variances and learn domain-invariant features in training samples from different manipulation methods, along with the global and local random crop augmentation strategy to generate more data views of forged images at various scales. A graph structure is then built by splitting the learned image-level feature maps, with each spatial location corresponding to a local patch, which facilitates patch representation learning by message-passing among similar nodes. Two types of center losses are utilized to learn more discriminative features in both image-level and patch-level embedding spaces. Extensive experimental results on several datasets demonstrate the effectiveness and generalization of the proposed method compared with other state-of-the-art methods.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4395703732",
    "type": "article"
  },
  {
    "title": "Learning Commonsense-aware Moment-Text Alignment for Fast Video Temporal Grounding",
    "doi": "https://doi.org/10.1145/3663368",
    "publication_date": "2024-05-01",
    "publication_year": 2024,
    "authors": "Ziyue Wu; Junyu Gao; Shucheng Huang; Changsheng Xu",
    "corresponding_authors": "",
    "abstract": "Grounding temporal video segments described in natural language queries effectively and efficiently is a crucial capability needed in vision-and-language fields. In this article, we deal with the fast video temporal grounding (FVTG) task, aiming at localizing the target segment with high speed and favorable accuracy. Most existing approaches adopt elaborately designed cross-modal interaction modules to improve the grounding performance, which suffer from the test-time bottleneck. Although several common space-based methods enjoy the high-speed merit during inference, they can hardly capture the comprehensive and explicit relations between visual and textual modalities. In this article, to tackle the dilemma of the speed–accuracy tradeoff, we propose a commonsense-aware cross-modal alignment network (C 2 AN) that incorporates commonsense-guided visual and text representations into a complementary common space for fast video temporal grounding. Specifically, the commonsense concepts are explored and exploited by extracting the structural semantic information from a language corpus. Then, a commonsense-aware interaction module is designed to obtain bridged visual and text features by utilizing the learned commonsense concepts. Finally, to maintain the original semantic information of textual queries, a cross-modal complementary common space is optimized to obtain matching scores for performing FVTG. Extensive results on two challenging benchmarks show that our C 2 AN method performs favorably against states of the art while running at high speed. Our code is available at https://github.com/ZiyueWu59/CCA",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4396568588",
    "type": "article"
  },
  {
    "title": "EOGT: Video Anomaly Detection with Enhanced Object Information and Global Temporal Dependency",
    "doi": "https://doi.org/10.1145/3662185",
    "publication_date": "2024-05-06",
    "publication_year": 2024,
    "authors": "Ruoyan Pi; Peng Wu; Xiangteng He; Yuxin Peng",
    "corresponding_authors": "",
    "abstract": "Video anomaly detection (VAD) aims to identify events or scenes in videos that deviate from typical patterns. Existing approaches primarily focus on reconstructing or predicting frames to detect anomalies and have shown improved performance in recent years. However, they often depend highly on local spatio-temporal information and face the challenge of insufficient object feature modeling. To address the above issues, this article proposes a video anomaly detection framework with E nhanced O bject Information and G lobal T emporal Dependencies (EOGT) and the main novelties are: (1) A L ocal O bject A nomaly S tream (LOAS) is proposed to extract local multimodal spatio-temporal anomaly features at the object level. LOAS integrates two modules: a D iffusion-based O bject R econstruction N etwork (DORN) with multimodal conditions detects anomalies with object RGB information; and an O bject P ose A nomaly Refiner (OPA) discovers anomalies with human pose information. (2) A G lobal T emporal S trengthening S tream (GTSS) with video-level temporal dependencies is proposed, which leverages video-level temporal dependencies to identify long-term and video-specific anomalies effectively. Both streams are jointly employed in EOGT to learn multimodal and multi-scale spatio-temporal anomaly features for VAD, and we finally fuse the anomaly features and scores to detect anomalies at the frame level. Extensive experiments are conducted to verify the performance of EOGT on three public datasets: ShanghaiTech Campus, CUHK Avenue, and UCSD Ped2.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4396671396",
    "type": "article"
  },
  {
    "title": "SEMScene: Semantic-Consistency Enhanced Multi-Level Scene Graph Matching for Image-Text Retrieval",
    "doi": "https://doi.org/10.1145/3664816",
    "publication_date": "2024-08-16",
    "publication_year": 2024,
    "authors": "Yuankun Liu; Xiang Yuan; Haochen Li; Zhijie Tan; J.-B. Huang; J. Xiao; Weiping Li; Tong Mo",
    "corresponding_authors": "",
    "abstract": "Image-text retrieval, a fundamental cross-modal task, performs similarity reasoning for images and texts. The primary challenge for image-text retrieval is cross-modal semantic heterogeneity, where the semantic features of visual and textual modalities are rich but distinct. Scene graph is an effective representation for images and texts as it explicitly models objects and their relations. Existing scene graph based methods have not fully taken the features regarding various granularities implicit in scene graph into consideration (e.g., triplets), the inadequate feature matching incurs the absence of non-trivial semantic information (e.g., inner relations among triplets). Therefore, we propose a S emantic-Consistency E nhanced M ulti-Level Scene Graph Matching (SEMScene) network, which exploits the semantic relevance between visual and textual scene graphs from fine-grained to coarse-grained. Firstly, under the scene graph representation, we perform feature matching including low-level node matching, mid-level semantic triplet matching, and high-level holistic scene graph matching. Secondly, to enhance the semantic-consistency for object-fused triplets carrying key correlation information, we propose a dual-step constraint mechanism in mid-level matching. Thirdly, to guide the model to learn the semantic-consistency of matched image-text pairs, we devise effective loss functions for each stage of the dual-step constraint. Comprehensive experiments on Flickr30K and MS-COCO datasets demonstrate that SEMScene achieves state-of-the-art performances with significant improvements.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4396827020",
    "type": "article"
  },
  {
    "title": "Detail-preserving Joint Image Upsampling",
    "doi": "https://doi.org/10.1145/3665246",
    "publication_date": "2024-05-15",
    "publication_year": 2024,
    "authors": "Yang Yang; Shuailong Qiu; Lanling Zeng; Zhigeng Pan",
    "corresponding_authors": "",
    "abstract": "Image operators can be instrumental to computational imaging and photography. However, many of them are computationally intensive. In this article, we propose an effective yet efficient joint upsampling method to accelerate various image operators. We show that edge-preserving filtering can be facilitated with a downsampling-and-upsampling process. Moreover, when the extent of smoothing is mild, the process is detail preserving, i.e., the fine details lost in the low-resolution (LR) images can be accurately restored in the high-resolution (HR) images. Given an HR input and an LR output of an operator, we downsample the HR input and calculate its affinities to the HR input. By applying the affinities to the LR output, we promote its resolution. Due to the strong detail-preserving property, the HR output derived in the previous step may exhibit aliasing artifacts around the salient edges. We further refine it based on the linear relations in a small neighborhood to rid the artifacts. Experiments on various image operators show that our method achieves superior quality over the state-of-the-art joint upsampling methods. Furthermore, the running time of our method is linear to the number of pixels. Our naive implementation derives 1080P images in real time (24 fps) on an NVIDIA GTX 3070 GPU.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4396927851",
    "type": "article"
  },
  {
    "title": "Online Cross-modal Hashing With Dynamic Prototype",
    "doi": "https://doi.org/10.1145/3665249",
    "publication_date": "2024-05-16",
    "publication_year": 2024,
    "authors": "X. Kang; Xingbo Liu; Wen Xue; Xiushan Nie; Yilong Yin",
    "corresponding_authors": "",
    "abstract": "Online cross-modal hashing has received increasing attention due to its efficiency and effectiveness in handling cross-modal streaming data retrieval. Despite the promising performance, these methods mainly focus on the supervised learning paradigm, demanding expensive and laborious work to obtain clean annotated data. Existing unsupervised online hashing methods mostly struggle to construct instructive semantic correlations among data chunks, resulting in the forgetting of accumulated data distribution. To this end, we propose a Dynamic Prototype-based Online Cross-modal Hashing method, called DPOCH. Based on the pre-learned reliable common representations, DPOCH generates prototypes incrementally as sketches of accumulated data and updates them dynamically for adapting streaming data. Thereafter, the prototype-based semantic embedding and similarity graphs are designed to promote stability and generalization of the hashing process, thereby obtaining globally adaptive hash codes and hash functions. Experimental results on benchmarked datasets demonstrate that the proposed DPOCH outperforms state-of-the-art unsupervised online cross-modal hashing methods.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4396956808",
    "type": "article"
  },
  {
    "title": "Skeleton-aware Graph-based Adversarial Networks for Human Pose Estimation from Sparse IMUs",
    "doi": "https://doi.org/10.1145/3669904",
    "publication_date": "2024-05-29",
    "publication_year": 2024,
    "authors": "Kaixin Chen; Lin Zhang; Zhong Wang; Shengjie Zhao; Yicong Zhou",
    "corresponding_authors": "",
    "abstract": "Recently, sparse-inertial human pose estimation (SI-HPE) with only a few IMUs has shown great potential in various fields. The most advanced work in this area achieved fairish results using only six IMUs. However, there are still two major issues that remain to be addressed. First, existing methods typically treat SI-HPE as a temporal sequential learning problem and often ignore the important spatial prior of skeletal topology. Second, there are far more synthetic data in their training data than real data, and the data distribution of synthetic data and real data is quite different, which makes it difficult for the model to be applied to more diverse real data. To address these issues, we propose “Graph-based Adversarial Inertial Poser (GAIP)”, which tracks body movements using sparse data from six IMUs. To make full use of the spatial prior, we design a multi-stage pose regressor with graph convolution to explicitly learn the skeletal topology. A joint position loss is also introduced to implicitly mine spatial information. To enhance the generalization ability, we propose supervising the pose regression with an adversarial loss from a discriminator, bringing the ability of adversarial networks to learn implicit constraints into full play. Additionally, we construct a real dataset that includes hip support movements and a synthetic dataset containing various motion categories to enrich the diversity of inertial data for SI-HPE. Extensive experiments demonstrate that GAIP produces results with more precise limb movement amplitudes and relative joint positions, accompanied by smaller joint angle and position errors compared to state-of-the-art counterparts. The datasets and codes are publicly available at https://cslinzhang.github.io/GAIP/ .",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4399125760",
    "type": "article"
  },
  {
    "title": "Mix-DDPM: Enhancing Diffusion Models through Fitting Mixture Noise with Global Stochastic Offset",
    "doi": "https://doi.org/10.1145/3672080",
    "publication_date": "2024-06-07",
    "publication_year": 2024,
    "authors": "Hanzhang Wang; Deming Zhai; Xiong Zhou; Junjun Jiang; Xianming Liu",
    "corresponding_authors": "",
    "abstract": "Denoising diffusion probabilistic models (DDPM) have shown impressive performance in various domains as a class of deep generative models. In this paper, we introduce the Mixture noise-based DDPM (Mix-DDPM), which considers the Markov diffusion posterior as a Gaussian mixture model. Specifically, Mix-DDPM randomly selects a Gaussian component and then adds the chosen Gaussian noise, which can be demonstrated as a more efficient way to perturb the signals into a simple known distribution. We further define the reverse probabilistic model as a parameterized Gaussian mixture kernel. Due to the intractability in calculating the KL divergence between Gaussian mixture models, we derive a variational bound to maximize the likelihood, offering a concise formulation for optimizing the denoising model and valuable insights for designing the sampling strategies. Our theoretical derivation highlights that Mix-DDPM need only shift image which requires the inclusion of a global stochastic offset in both the diffusion and reverse processes , which can be efficiently implemented with just several lines of code. The global stochastic offset effectively fits a Gaussian mixture distribution enhancing the degrees of freedom of the entire diffusion model. Furthermore, we present three streamlined sampling strategies that interface with diverse fast dedicated solvers for diffusion ordinary differential equations, boosting the efficacy of image representation in the sampling phase and alleviating the issue of slow generation speed, thereby enhancing both efficiency and accuracy. Extensive experiments on benchmark datasets demonstrate the effectiveness of Mix-DDPM and its superiority over the original DDPM.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4399431796",
    "type": "article"
  },
  {
    "title": "Towards Long Form Audio-visual Video Understanding",
    "doi": "https://doi.org/10.1145/3672079",
    "publication_date": "2024-06-07",
    "publication_year": 2024,
    "authors": "Wenxuan Hou; Guangyao Li; Yapeng Tian; Di Hu",
    "corresponding_authors": "",
    "abstract": "We live in a world filled with never-ending streams of multimodal information. As a more natural recording of the real scenario, long form audio-visual videos are expected as an important bridge for better exploring and understanding the world. In this paper, we propose the multisensory temporal event localization task in long form videos and strive to tackle the associated challenges. To facilitate this study, we first collect a large-scale Long Form Audio-visual Video (LFAV) dataset with 5,175 videos and an average video length of 210 seconds. Each collected video is elaborately annotated with diversified modality-aware events, in a long-range temporal sequence. We then propose an event-centric framework for localizing multisensory events as well as understanding their relations in long form videos. It includes three phases in different levels: snippet prediction phase to learn snippet features, event extraction phase to extract event-level features, and event interaction phase to study event relations. Experiments demonstrate that the proposed method, utilizing the new LFAV dataset, exhibits considerable effectiveness in localizing multiple modality-aware events within long form videos. We hope that our newly collected dataset and novel approach serve as a cornerstone for furthering research in the realm of long form audio-visual video understanding. Project page: https://gewu-lab.github.io/LFAV/ .",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4399431911",
    "type": "article"
  },
  {
    "title": "On the Security of Selectively Encrypted HEVC Video Bitstreams",
    "doi": "https://doi.org/10.1145/3672568",
    "publication_date": "2024-06-12",
    "publication_year": 2024,
    "authors": "Chen Chen; Lingfeng Qu; Hadi Amirpour; Xingjun Wang; Christian Timmerer; Zhihong Tian",
    "corresponding_authors": "",
    "abstract": "With the growing applications of video, ensuring its security has become of utmost importance. Selective encryption (SE) has gained significant attention in the field of video content protection due to its compatibility with video codecs, favorable visual distortion, and low time complexity. However, few studies consider SE security under cryptographic attacks. To fill this gap, we analyze the security concerns of encrypted bitstreams by SE schemes and propose two known plaintext attacks (KPAs). Then the corresponding defense is presented against the KPAs. To validate the effectiveness of the KPA, it is applied to attack two existing SE schemes with superior visual degradation in HEVC videos. Firstly, the encrypted bitstreams are generated using the HEVC encoder with SE (HESE). Secondly, the video sequences are encoded using H.265/HEVC. During encoding, the selected syntax elements are recorded. Then the recorded syntax elements are imported into the HEVC decoder using decryption (HDD). By utilizing the encryption parameters and the imported data in the HDD, it becomes possible to reconstruct a significant portion of the original syntax elements before encryption. Finally, the reconstructed syntax elements are compared with the encrypted syntax elements in the HDD, allowing the design of a pseudo-key stream (PKS) through the inverse of the encryption operations. The PKS is used to decrypt the existing SE scheme, and the experimental results provide evidence that the two existing SE schemes are vulnerable to the proposed KPAs. In the case of single bitstream estimation (SBE), the average correct rate of key stream estimation exceeds 93%. Moreover, with multi-bitstream complementation (MBC), the average estimation accuracy can be further improved to 99%.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4399569822",
    "type": "article"
  },
  {
    "title": "Unified View Empirical Study for Large Pretrained Model on Cross-Domain Few-Shot Learning",
    "doi": "https://doi.org/10.1145/3673231",
    "publication_date": "2024-06-19",
    "publication_year": 2024,
    "authors": "Linhai Zhuo; Yuqian Fu; Jingjing Chen; Yixin Cao; Yu‐Gang Jiang",
    "corresponding_authors": "",
    "abstract": "The challenge of cross-domain few-shot learning (CD-FSL) stems from the substantial distribution disparities between target and source domain images, necessitating a model with robust generalization capabilities. In this work, we posit that large-scale pretrained models are pivotal in addressing the CD-FSL task owing to their exceptional representational and generalization prowess. To our knowledge, no existing research comprehensively investigates the utility of large-scale pretrained models in the CD-FSL context. Addressing this gap, our study presents an exhaustive empirical assessment of the Contrastive Language–Image Pre-Training model within the CD-FSL task. We undertake a comparison spanning six dimensions: base model, transfer module, classifier, loss, data augmentation, and training schedule. Furthermore, we establish a straightforward baseline model, E-base, based on our empirical analysis, underscoring the importance of our investigation. Experimental results substantiate the efficacy of our model, yielding a mean gain of 1.2% in 5-way 5-shot evaluations on the BSCD dataset.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4399804469",
    "type": "article"
  },
  {
    "title": "Unbiased Feature Learning with Causal Intervention for Visible-Infrared Person Re-identification",
    "doi": "https://doi.org/10.1145/3674737",
    "publication_date": "2024-06-27",
    "publication_year": 2024,
    "authors": "Bowen Yuan; Jiahao Lu; Sisi You; Bing‐Kun Bao",
    "corresponding_authors": "",
    "abstract": "Visible-infrared person re-identification (VI-ReID) aims to match individuals across different modalities. Existing methods can learn class-separable features but still struggle with modality gaps within class due to the modality-specific information, which is discriminative in one modality but not present in another (e.g., a black striped shirt). The presence of the interfering information creates a spurious correlation with the class label, which hinders alignment across modalities. To this end, we propose an Unbiased feature learning method based on Causal inTervention for VI-ReID from three aspects. Firstly, through the proposed structural causal graph, we demonstrate that modality-specific information acts as a confounder that restricts the intra-class feature alignment. Secondly, we propose a causal intervention method to remove the confounder using an effective approximation of backdoor adjustment, which involves adjusting the spurious correlation between features and labels. Thirdly, we incorporate the proposed approximation method into the basic VI-ReID model. Specifically, the confounder can be removed by adjusting the extracted features with a set of weighted pre-trained class prototypes from different modalities, where the weight is adapted based on the features. Extensive experiments on the SYSU-MM01 and RegDB datasets demonstrate that our method outperforms state-of-the-art methods. Code is available at https://github.com/NJUPT-MCC/UCT .",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4400074742",
    "type": "article"
  },
  {
    "title": "Auxiliary Feature Fusion and Noise Suppression for HOI Detection",
    "doi": "https://doi.org/10.1145/3674980",
    "publication_date": "2024-06-27",
    "publication_year": 2024,
    "authors": "Sixian Chan; Xianpeng Zeng; Xinhua Wang; Jie Hu; Cong Bai",
    "corresponding_authors": "",
    "abstract": "In recent years, one-stage HOI (Human–Object Interaction) detection methods tend to divide the original task into multiple sub-tasks by using a multi-branch network structure. However, there is no sufficient attention to information communication between these branches. The inference approach in the cascaded structure is singular, while fully parallel methods will disrupt the associations between different pieces of information. Besides, noise interference may occur during the fusion of different features and thus affect the detection performance. To address these issues, this article proposes a one-stage three-branch parallel HOI detection method, which treats HOI as three separate sub-tasks (human detection, object detection, and interaction detection) and leverages three distinct reasoning relationships to generate richer relational information. Firstly , an auxiliary feature fusion (AFF) module is introduced, which integrates features originally extracted independently to form fused features enriched with supplementary information. This approach strengthens communication between branches in the network while handling the three sub-tasks concurrently, thereby facilitating the exchange of more contextual information. Secondly , to mitigate noise interference generated during the fusion process, a fusion noise suppression (FNS) module is introduced, which effectively suppresses noise and enhances the model’s performance in interaction detection tasks. Finally , experiments are conducted on two major benchmark datasets, and experimental results show that our HOI detection method is superior to previous methods. Also, ablation studies confirm the effectiveness of all the components in our proposed method.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4400075398",
    "type": "article"
  },
  {
    "title": "Unsupervised Adversarial Example Detection of Vision Transformers for Trustworthy Edge Computing",
    "doi": "https://doi.org/10.1145/3674981",
    "publication_date": "2024-07-02",
    "publication_year": 2024,
    "authors": "Jiaxing Li; Yu‐an Tan; Jie Yang; Zhengdao Li; Heng Ye; Chenxiao Xia; Yuanzhang Li",
    "corresponding_authors": "",
    "abstract": "Many edge computing applications based on computer vision have harnessed the power of deep learning. As an emerging deep learning model for vision, Vision Transformer models have recently achieved record-breaking performance in various vision tasks. But many recent studies on the robustness of the Vision Transformer have shown that the Vision Transformer is still vulnerable to adversarial attacks and is easily affected by adversarial attacks, causing the model to misclassify the input. In this work, we ask an intriguing question: “Can Adversarial Perturbations against Vision Transformers be detected with model explanations?” Driven by this question, we observe that benign samples and adversarial examples have different attribution maps after applying the Grad-CAM interpretability method on the Vision Transformer model. We demonstrate that an adversarial example is a Feature Shift of the input data, which leads to an Attention Deviation of the visual model. We propose a framework for capturing the Attention Deviation of vision models to defend against adversarial attacks. Furthermore, experiments show that our model achieves expectative results.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4400246412",
    "type": "article"
  },
  {
    "title": "VRVul-Discovery: BiLSTM-based Vulnerability Discovery for Virtual Reality Devices in Metaverse",
    "doi": "https://doi.org/10.1145/3677609",
    "publication_date": "2024-07-12",
    "publication_year": 2024,
    "authors": "Letian Sha; Xiao Chen; Fu Xiao; Zhong Wang; Zhangbo Long; Qianyu Fan; Jiankuo Dong",
    "corresponding_authors": "",
    "abstract": "The rapid development of the metaverse has brought about numerous security challenges. Virtual Reality (VR) , as one of the core technologies, plays a crucial role in the metaverse. The security of VR devices directly impacts user authentication and privacy. Currently, no attention has been paid to the vulnerabilities and security risks of VR devices. This paper employs a bi-layer BiLSTM neural network to conduct a root cause analysis for user authentication and scene interaction when users enter metaverse environment using VR devices. By establishing the mapping between vulnerable VR firmware file attributes and metaverse interaction scenarios, we implement a vulnerability discovery and verification prototype called VRVul-Discovery, based on the concept of vulnerability discovery. Experiment results demonstrate that VRVul-Discovery provides high-accuracy determinations of firmware vulnerability attributes and scenarios susceptible to hijacking. In the end, the prototype system discovers seven unknown vulnerabilities, all of which are authenticated.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4400580303",
    "type": "article"
  },
  {
    "title": "Lightweight Food Recognition via Aggregation Block and Feature Encoding",
    "doi": "https://doi.org/10.1145/3680285",
    "publication_date": "2024-07-22",
    "publication_year": 2024,
    "authors": "Yancun Yang; Weiqing Min; Jingru Song; Guorui Sheng; Lili Wang; Shuqiang Jiang",
    "corresponding_authors": "",
    "abstract": "Food image recognition has recently been given considerable attention in the multimedia field in light of its possible implications on health. The characteristics of the dispersed distribution of ingredients in food images put forward higher requirements on the long-range information extraction ability of neural networks, leading to more complex and deeper models. Nevertheless, the lightweight version of food image recognition is essential for improved implementation on end devices and sustained server-side expansion. To address this issue, we present Aggregation Feature Net (AFNet), a lightweight network that is capable of effectively capturing both global and local features from food images. In AFNet, we develop a novel convolution based on a residual model by encoding global features through row-wise and column-wise information integration. Merging aggregation block with classic local convolution yields a framework that works as the backbone of the network. Based on the efficient use of parameters by the aggregation block, we constructed a lightweight food image recognition network with fewer layers and a smaller scale, assisted by a new type of activation function. Experimental results on four popular food recognition datasets demonstrate that our approach achieves state-of-the-art performance with higher accuracy and fewer FLOPs and parameters. For example, in comparison to the current state-of-the-art model of MobileViTv2, AFNet achieved 88.4% accuracy of the top-1 level on the ETHZ Food-101 dataset, with similar parameters and FLOPs but 1.4% more accuracy. The source code will be provided in supplementary materials.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4400881964",
    "type": "article"
  },
  {
    "title": "Cross-attention based two-branch networks for document image forgery localization in the Metaverse",
    "doi": "https://doi.org/10.1145/3686158",
    "publication_date": "2024-08-07",
    "publication_year": 2024,
    "authors": "Yalin Song; Jiang Wen-bin; Xiuli Chai; Zhihua Gan; Mengyuan Zhou; Lei Chen",
    "corresponding_authors": "",
    "abstract": "In recent years, the Metaverse has garnered significant attention in social and Metahuman realms, showcasing substantial value and immense developmental potential through its integration of virtual and real worlds. However, this integration has also raised security concerns. For instance, in digital image forensics, the malicious dissemination of false images by wrongdoers could result in serious consequences and the propagation of misinformation. This paper presented a novel two-branch network (abbreviated as CAFTB-Net) to detect and localize the forged regions of document images in the Metaverse. One branch extracts manipulation trace directly from spatial information, e.g., unnatural smears, anomalies between pixels, etc. The other branch employs an SRM filter to transform the input image from the color domain into the noise domain, effectively extracting anomalies, such as global noise inconsistencies from the noise domain. Compared to spatial domain features, the discontinuity of forgery traces within the noise domain aids the network in authenticating the document image. The two branches extract local and global features in the forged document images. Finally, we propose a cross-attention module to fuse local spatial features and global noise features. Extensive experimental results demonstrate that the proposed network achieves F1 scores of 0.819 and 0.948 on the SACP and ICDAR datasets, respectively, with AUC scores of 0.933 and 0.764, outperforming some of the state-of-the-art algorithms.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4401391900",
    "type": "article"
  },
  {
    "title": "From Recognition to Prediction: Leveraging Sequence Reasoning for Action Anticipation",
    "doi": "https://doi.org/10.1145/3687474",
    "publication_date": "2024-08-28",
    "publication_year": 2024,
    "authors": "Xin Liu; Chao Hao; Zitong Yu; Huanjing Yue; Jingyu Yang",
    "corresponding_authors": "",
    "abstract": "The action anticipation task refers to predicting what action will happen based on observed videos, which requires the model to have a strong ability to summarize the present and then reason about the future. Experience and common sense suggest that there is a significant correlation between different actions, which provides valuable prior knowledge for the action anticipation task. However, previous methods have not effectively modeled this underlying statistical relationship. To address this issue, we propose a novel end-to-end video modeling architecture that utilizes attention mechanisms, named Anticipation via Recognition and Reasoning (ARR). ARR decomposes the action anticipation task into action recognition and sequence reasoning tasks and effectively learns the statistical relationship between actions by next action prediction (NAP). In comparison to existing temporal aggregation strategies, ARR is able to extract more effective features from observable videos to make more reasonable predictions. In addition, to address the challenge of relationship modeling that requires extensive training data, we propose an innovative approach for the unsupervised pre-training of the decoder, which leverages the inherent temporal dynamics of video to enhance the reasoning capabilities of the network. Extensive experiments on the Epic-kitchen-100, EGTEA Gaze+, and 50salads datasets demonstrate the efficacy of the proposed methods. The code is available at https://github.com/linuxsino/ARR .",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4401946582",
    "type": "article"
  },
  {
    "title": "DEGAN: Detail-Enhanced Generative Adversarial Network for Monocular Depth based 3D Reconstruction",
    "doi": "https://doi.org/10.1145/3690826",
    "publication_date": "2024-08-30",
    "publication_year": 2024,
    "authors": "Caixia Liu; Yali Chen; Minhong Zhu; Chenhui Hao; Haisheng Li; Xiaochuan Wang",
    "corresponding_authors": "",
    "abstract": "Although deep networks based 3D reconstruction methods can recover the 3D geometry given few inputs, they may produce unfaithful reconstruction when predicting occluded parts of 3D objects. To address the issue, we propose Detail-Enhanced Generative Adversarial Network (DEGAN) which consists of Encoder-Decoder based Generator (EDGen) and Voxel-Point Embedding Network based Discriminator (VPDis) for 3D reconstruction from a monocular depth image of an object. Firstly, EDGen decodes the features from the 2.5D voxel grid representation of an input depth image, and generates the 3D occupancy grid under GAN losses and a sampling point loss. The sampling loss can improve the accuracy of predicted points with high uncertainty. VPDis helps reconstruct the details under voxel and point adversarial losses, respectively. Experimental results show that DEGAN not only outperforms several state-of-the-art methods on both public ModelNet and ShapeNet datasets, but also predicts more reliable occluded/missing parts of 3D objects.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4402043846",
    "type": "article"
  },
  {
    "title": "Streaming Media over LEO Satellite Networking: A Measurement-Based Analysis and Optimization",
    "doi": "https://doi.org/10.1145/3694976",
    "publication_date": "2024-09-06",
    "publication_year": 2024,
    "authors": "Hao Fang; Haoyuan Zhao; Feng Wang; Yi Ching Chou; Long Chen; Jianxin Shi; Jiangchuan Liu",
    "corresponding_authors": "",
    "abstract": "Recently, Low Earth orbit Satellite Networks (LSNs) have been suggested as a critical and promising component toward high-bandwidth and low-latency global coverage in the upcoming 6G communication infrastructure. SpaceX's Starlink is arguably the largest and most operational LSN to date. There have been practical uses of Starlink across diverse networked applications, including those with stringent demands, such as multimedia applications. Given the mixed and inconsistent feedback from end users, it remains unclear whether today's LSNs, in particular Starlink, are ready for realtime multimedia. In this paper, we present a systematic measurement study on realtime multimedia services over Starlink, seeking insights into their operations and performance in this new generation of networking. Our findings demonstrate that Starlink can handle most video-on-demand (VoD) and live-streaming services with properly configured buffers but suffers from video pauses or audio cut-offs during interactive videoconferencing. We identify the key factors that impact the performance of LSN, particularly for multimedia services, including satellite switching, routing strategies and weather conditions. Our findings offer valuable hints into future enhancements for multimedia services over LSNs. Specifically, we further propose a Weather Aware Buffer Based Rate Adaption algorithm based on our observations on weather impacts, which is capable of maximizing the quality of experience for VoD applications with seamless integration of dynamic weather conditions.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4402306597",
    "type": "article"
  },
  {
    "title": "Fast Unsupervised Cross-modal Hashing With Robust Factorization and Dual Projection",
    "doi": "https://doi.org/10.1145/3694684",
    "publication_date": "2024-09-09",
    "publication_year": 2024,
    "authors": "Xingbo Liu; Jiamin Li; Xiushan Nie; Xuening Zhang; Yilong Yin",
    "corresponding_authors": "",
    "abstract": "Unsupervised hashing has attracted extensive attention in effectively and efficiently tackling large-scale cross-modal retrieval task. Existing methods typically try to mine the latent common subspace across multimodal data without any category annotation. Despite the exciting progress, there are still three challenges that need to be further addressed: 1) efficiently improving the robustness during latent common subspace learning; 2) harmoniously embedding the intra-modal inherence and inter-modal relevance of multimodal data into Hamming space; and 3) effectively reducing the training time complexity and making the model scalable for large-scale datasets. To well address the above challenges, this study proposes a method named Fast Unsupervised Cross-modal Hashing (FUCH). Specifically, FUCH proposes a semantic-aware collective matrix factorization to learn robust representation via exploiting latent category-specific attributes, and introduces Cauchy loss to measure the factorization process. Accordingly, the above process can effectively embed potential discriminative information into common space, while making the model insensitive for outliers. Moreover, FUCH designs a dual projection learning scheme, which not only learns modality-unique hash functions to excavate individual properties, but also learns modality-mutual hash functions to multimodal correlational properties. Experimental results on three benchmark datasets verify the effectiveness of FUCH under various scenarios.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4402354724",
    "type": "article"
  },
  {
    "title": "Category-level Pose Estimation and Iterative Refinement for Monocular RGB-D Image",
    "doi": "https://doi.org/10.1145/3695877",
    "publication_date": "2024-09-11",
    "publication_year": 2024,
    "authors": "Yongtang Bao; Chunjian Su; Yutong Qi; Yanbing Geng; Haojie Li",
    "corresponding_authors": "",
    "abstract": "Category-level pose estimation is proposed to predict the 6D pose of objects under a specific category and has wide applications in fields such as robotics, virtual reality, and autonomous driving. With the development of VR/AR technology, pose estimation has gradually become a research hotspot in 3D scene understanding. However, most methods fail to fully utilize geometric and color information to solve intra-class shape variations, which leads to inaccurate prediction results. To solve the above problems, we propose a novel pose estimation and iterative refinement network, use an attention mechanism to fuse multi-modal information to obtain color features after a coordinate transformation, and design iterative modules to ensure the accuracy of object geometric features. Specifically, we use an encoder-decoder architecture to implicitly generate a coarse-grained initial pose and refine it through an iterative refinement module. In addition, due to the differences between rotation and position estimation, we design a multi-head pose decoder that utilizes the local geometry and global features. Finally, we design a transformer-based coordinate transformation attention module to extract pose-sensitive features from RGB images and supervise color information by correlating point cloud features in different coordinate systems. We train and test our network on the synthetic dataset CAMERA25 and the real dataset REAL275. Experimental results show that our method achieves state-of-the-art performance on multiple evaluation metrics.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4402455728",
    "type": "article"
  },
  {
    "title": "Weight-based Privacy-preserving Asynchronous SplitFed for Multimedia Healthcare Data",
    "doi": "https://doi.org/10.1145/3695876",
    "publication_date": "2024-09-14",
    "publication_year": 2024,
    "authors": "Veronika Stephanie; Ibrahim Khalil; Mohammed Atiquzzaman",
    "corresponding_authors": "",
    "abstract": "Multimedia significantly enhances modern healthcare by facilitating the analysis and sharing of diverse data, including medical images, videos, and sensor data. Integrating Artificial Intelligence (AI) for multimedia data classification shows promise in improving healthcare services, data analysis, and decision-making. However, ensuring privacy in AI-integrated healthcare systems remains a challenge, especially with data continuously transmitted over networks. Synchronous Federated Learning (FL) is designed to address these privacy concerns by allowing end devices to collaboratively train a machine learning model without sharing data. Nonetheless, FL alone does not fully resolve privacy issues and faces efficiency challenges, particularly with devices of varying computational capabilities. In this paper, we introduce APP-SplitFed, an Asynchronous, Privacy-Preserving Split-Federated Learning approach for smart healthcare systems. This method reduces computational demands on resource-limited devices and uses a weight-based aggregation method to allow devices of differing computational power to contribute effectively, ensuring optimal model performance and rapid convergence. Additionally, we incorporate a secure aggregation method to prevent adversaries from identifying individual models owned by healthcare institutions.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4402534067",
    "type": "article"
  },
  {
    "title": "Deep Shape-Texture Statistics for Completely Blind Image Quality Evaluation",
    "doi": "https://doi.org/10.1145/3694977",
    "publication_date": "2024-09-20",
    "publication_year": 2024,
    "authors": "Yixuan Li; Peilin Chen; Hanwei Zhu; Keyan Ding; Leida Li; Shiqi Wang",
    "corresponding_authors": "",
    "abstract": "Opinion-Unaware Blind Image Quality Assessment (OU-BIQA) models aim to predict image quality without training on reference images and subjective quality scores. Thereinto, image statistical comparison is a classic paradigm, while the performance is limited by the representation ability of visual descriptors. Deep features as visual descriptors have advanced IQA in recent research, but they are discovered to be highly texture-biased and lack shape-bias. On this basis, we find out that image shape and texture cues respond differently toward distortions, and the absence of either one results in an incomplete image representation. Therefore, to formulate a well-rounded statistical description for images, we utilize the shape-biased and texture-biased deep features produced by Deep Neural Networks (DNNs) simultaneously. More specifically, we design a Shape-Texture Adaptive Fusion (STAF) module to merge shape and texture information, based on which we formulate quality-relevant image statistics. The perceptual quality is quantified by the variant Mahalanobis distance between the inner and outer Deep Shape-Texture Statistics (DSTS), wherein the inner and outer statistics respectively describe the quality fingerprints of the distorted image and natural images. The proposed DSTS delicately utilizes shape-texture statistical relations between different data scales in the deep domain and achieves state-of-the-art (SOTA) quality prediction performance on images with artificial and authentic distortions.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4402679011",
    "type": "article"
  },
  {
    "title": "Motion-aware Self-supervised RGBT Tracking with Multi-modality Hierarchical Transformers",
    "doi": "https://doi.org/10.1145/3698399",
    "publication_date": "2024-10-03",
    "publication_year": 2024,
    "authors": "Shenglan Li; Rui Yao; Yong Zhou; Hancheng Zhu; Jiaqi Zhao; Zhiwen Shao; Abdulmotaleb El Saddik",
    "corresponding_authors": "",
    "abstract": "Supervised RGBT (SRGBT) tracking tasks need both expensive and time-consuming annotations. Therefore, the implementation of Self-Supervised RGBT (SSRGBT) tracking methods has become increasingly important. Straightforward SSRGBT tracking methods use pseudo-labels for tracking, but inaccurate pseudo-labels can lead to object drift, which severely affects tracking performance. This paper proposes a self-supervised RGBT object tracking method (S2OTFormer) to bridge the gap between tracking methods supervised under pseudo-labels and ground truth labels. Firstly, to provide more robust appearance features for motion cues, we introduce a Multi-Modal Hierarchical Transformer module (MHT) for feature fusion. This module allocates weights to both modalities and strengthens the expressive capability of the MHT module through multiple nonlinear layers to fully utilize the complementary information of the two modalities. Secondly, in order to solve the problems of motion blur caused by camera motion and inaccurate appearance information caused by pseudo-labels, we introduce a Motion-Aware Mechanism (MAM). The MAM extracts the average motion vectors from the previous multi-frame search frame features and constructs the consistency loss with the motion vectors of the current search frame features. The motion vectors of inter-frame objects are obtained by reusing the inter-frame attention map to predict coordinate positions. Finally, to further reduce the effect of inaccurate pseudo-labels, we propose an Attention-Based Multi-Scale Enhancement Module. By introducing cross-attention to achieve more precise and accurate object tracking, this module overcomes the receptive field limitations of traditional CNN tracking heads. We demonstrate the effectiveness of S2OTFormer on four large-scale public datasets through extensive comparisons as well as numerous ablation experiments. The source code is available at https://github.com/LiShenglana/S2OTFormer .",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4403096679",
    "type": "article"
  },
  {
    "title": "Ischemic Stroke Segmentation by Transformer and Convolutional Neural Network Using Few-Shot Learning",
    "doi": "https://doi.org/10.1145/3699513",
    "publication_date": "2024-10-07",
    "publication_year": 2024,
    "authors": "Fatima Alshehri; Ghulam Muhammad",
    "corresponding_authors": "",
    "abstract": "Stroke is a major factor in causing disability and fatalities. Doctors use computerized tomography (CT) and magnetic resonance imaging (MRI) scans to assess the severity of a stroke. Automatic image segmentation can help doctors diagnose strokes more quickly and accurately, but it is challenging due to the variability of stroke lesions and the limited availability of labeled data. Deep learning is the cutting-edge technique of machine learning and artificial intelligence, which needs an extensive labeled dataset for effective training. Unfortunately, in the medical domain, the availability of labeled data is severely limited, posing a challenge for conventional deep- learning approaches. In this article, we introduce a system that utilizes deep learning in the form of fusing transformer-based and convolutional neural network (CNN)-based features and few-shot learning techniques to segment ischemic strokes in multimedia MRIs. To accomplish this, we employ two different methods. The first method involves parallel fusion, where we combine CNN-based and transformer-based features. The second method utilizes serial fusion, combining CNN-based and transformer models using few-shot learning. Through the integration of transformer and CNN models, we can extract both global and local features and enhance the system's performance. Moreover, we tackle the issue of limited labeled data by integrating few-shot learning techniques. Additionally, our system optimizes efficiency by selecting only the slices with lesions, disregarding unlesioned slices. The system under consideration is trained with the BraTS2020 dataset, evaluated on the ISLES 2015 dataset, and contrasted the performance with cutting-edge systems. The suggested system attains a dice coefficient score of 0.76, surpassing the scores of previous cutting-edge systems by a substantial margin.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4403185908",
    "type": "article"
  },
  {
    "title": "A New Tensor Summary Statistic for Real-Time Detection of Stealthy Anomaly in Avatar Interaction",
    "doi": "https://doi.org/10.1145/3689429",
    "publication_date": "2024-10-17",
    "publication_year": 2024,
    "authors": "Jiuzhen Zeng; Laurence T. Yang; Chao Wang; Jiangtao Su; Xianjun Deng",
    "corresponding_authors": "",
    "abstract": "Avatar is one of the most intuitive central components in Metaverse, and faces serious security problem particularly during the interacting with each other. In this paper, we consider the problem of timely detecting the stealthy anomaly in the avatar interaction, which is crucial for the security and privacy in Metaverse. With this goal, a new tensor summary statistic is proposed first to well depict the statistical discrepancy between normal and anomalous interaction volume samples, even when anomalies are stealthy. The proposed tensor summary statistic is established from the tensor linear representation residual which naturally implies the statistical probability that an interaction volume sample lies within or deviates from the tensor lateral space. Moreover, a convex optimization programme is introduced to robustly recover the tensor lateral space in the presence of anomalous samples, thereby enhancing the robustness of our tensor summary statistic. On the basis of the tensor summary statistic, a non-parametric statistic framework is developed for the real-time detection of the stealthy interaction volume anomaly. We also provide theoretical analysis concerning its detection performance and parameter selection. Extensive experiments using synthetic and real-world datasets verify our effectiveness and superiority. Compared with benchmark methods, the proposed detection scheme achieves significantly lower detection delay and higher false alarm period, particularly in detection of stealthy anomalies with a low change rate.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4403512878",
    "type": "article"
  },
  {
    "title": "CLIP-DFGS: A Hard Sample Mining Method for CLIP in Generalizable Person Re-Identification",
    "doi": "https://doi.org/10.1145/3701036",
    "publication_date": "2024-10-21",
    "publication_year": 2024,
    "authors": "Haijun Zhao; Lei Qi; Xin Geng",
    "corresponding_authors": "",
    "abstract": "Recent advancements in pre-trained vision-language models like CLIP have shown promise in person re-identification (ReID) applications. However, their performance in generalizable person re-identification tasks remains suboptimal. The large-scale and diverse image-text pairs used in CLIP's pre-training may lead to a lack or insufficiency of certain fine-grained features. In light of these challenges, we propose a hard sample mining method called DFGS (Depth-First Graph Sampler), based on depth-first search, designed to offer sufficiently challenging samples to enhance CLIP's ability to extract fine-grained features. DFGS can be applied to both the image encoder and the text encoder in CLIP. By leveraging the powerful cross-modal learning capabilities of CLIP, we aim to apply our DFGS method to extract challenging samples and form mini-batches with high discriminative difficulty, providing the image model with more efficient and challenging samples that are difficult to distinguish, thereby enhancing the model's ability to differentiate between individuals. Our results demonstrate significant improvements over other methods, confirming the effectiveness of DFGS in providing challenging samples that enhance CLIP's performance in generalizable person re-identification.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4403600736",
    "type": "article"
  },
  {
    "title": "Robust Multimodal Representation under Uncertain Missing Modalities",
    "doi": "https://doi.org/10.1145/3702003",
    "publication_date": "2024-10-26",
    "publication_year": 2024,
    "authors": "Guilin Lan; Yeqian Du; Zhouwang Yang",
    "corresponding_authors": "",
    "abstract": "Multimodal representation learning has gained significant attention across various fields, yet it faces challenges when dealing with missing modalities in real-world applications. Existing solutions are confined to specific scenarios, such as single-modality missing or missing modalities in test cases, thereby restricting their applicability. To address a more general scenario of uncertain missing modalities in both training and testing phases, we propose Robust Multimodal Representation under Uncertain Missing Modalities (RMRU). This framework projects each modality's representation into a shared subspace, enabling the reconstruction of any missing modalities within a unified model. We propose an interaction refinement module that utilizes cross-modal attention to enhance these reconstructions, particularly beneficial in scenarios with limited complete modality data. Furthermore, we introduce an iterative training strategy that alternately trains different modules to effectively utilize both complete and incomplete modality data. Experimental results on four benchmark datasets demonstrate the superiority of RMRU over existing baselines, particularly in scenarios with a high rate of missing modalities. Remarkably, our proposed RMRU can be broadly applied to diverse scenarios, regardless of modality types and quantities.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4403784230",
    "type": "article"
  },
  {
    "title": "ATMNet: Adaptive Texture Migration Network for Guided Depth Super-Resolution",
    "doi": "https://doi.org/10.1145/3702642",
    "publication_date": "2024-11-01",
    "publication_year": 2024,
    "authors": "Kehua Guo; Xuyang Tan; Xiangyuan Zhu; Shaojun Guo; Zhipeng Xi",
    "corresponding_authors": "",
    "abstract": "Guided Depth Super-Resolution (GDSR) aims to enhance the level of detail in low-resolution depth images by utilizing the information present in the corresponding high-resolution RGB images. While existing methods utilize different approaches to guide the RGB image to the source image, they often ignore the texture similarity between these two images and usually suffer from unsatisfactory outline reconstruction of the depth map. In this paper, we introduce an adaptive texture migration network (ATMNet) designed to mine rich feature information from RGB images and migrate them to the depth image. Specifically, we propose a multi-modal feature extractor (MMFE) to extract private and shared features between the depth map and RGB image. In addition, we present a texture migration module (TMM) to remap and fuse the features extracted from the raw image pairs. Last but not least, we develop a weighted adaptive loss to enhance the reconstruction of the edge areas in the depth map. Extensive experiments on public datasets such as Middlebury, NYUv2, and DIML demonstrate that our method outperforms the existing state-of-the-art GDSR methods and strikes a remarkable balance between performance and efficiency. The source code is available at https://github.com/MuggleTan/ATMNet .",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4403985990",
    "type": "article"
  },
  {
    "title": "Domain-aware Multimodal Dialog Systems with Distribution-based User Characteristic Modeling",
    "doi": "https://doi.org/10.1145/3704811",
    "publication_date": "2024-11-19",
    "publication_year": 2024,
    "authors": "Xiaolin Chen; Xuemeng Song; Jianhui Zuo; Yinwei Wei; Liqiang Nie; Tat‐Seng Chua",
    "corresponding_authors": "",
    "abstract": "Textual response generation is a pivotal yet challenging task for multimodal task-oriented dialog systems, which targets at generating the appropriate textual response given the multimodal context. Although existing efforts have obtained remarkable advancements, they ignore the potential of the domain information in revealing the key points of the user intention and the user's history dialogs in indicating the user's characteristics. To address this issue, in this work, we propose a novel domain-aware multimodal dialog system with distribution-based user characteristic modeling (named DMDU). In particular, DMDU contains three vital components: context-knowledge embedding extraction , domain-aware response generation and distribution-based user characteristic injection . Specifically, the context-knowledge embedding extraction component aims to extract the embedding of multimodal context and related knowledge following existing studies. The domain-aware response generation component targets at conducting domain-aware fine-grained intention modeling based on the context and knowledge embedding, and thus fulfills the textual response generation. Moreover, the distribution-based user characteristic injection component first captures the user's characteristics and current intention with the Gaussian distribution, and then conducts the sampling-based contrastive semantic regularization to promote the context representation learning. Experimental results on the public dataset demonstrate the effectiveness of DMDU. We release codes to promote other researchers.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4404525274",
    "type": "article"
  },
  {
    "title": "Solutions, Challenges and Opportunities in Volumetric Video Streaming: An Architectural Perspective",
    "doi": "https://doi.org/10.1145/3705321",
    "publication_date": "2024-11-21",
    "publication_year": 2024,
    "authors": "Abdelhak Bentaleb; May Lim; Sarra Hammoudi; Saad Harous; Roger Zimmermann",
    "corresponding_authors": "",
    "abstract": "Volumetric video streaming technologies are the future of immersive media services such as virtual, augmented, and mixed-reality experiences. The challenges surrounding such technologies are tremendous due to the high network bandwidth needed to produce high-quality and low-latency streams. Many techniques and solutions have been proposed across the streaming workflow to mitigate such challenges. To better understand and organize these developments, this survey adopts an architectural framework to showcase current and emerging techniques and solutions for volumetric video streaming while highlighting some of their characteristic challenges and opportunities.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4404587438",
    "type": "article"
  },
  {
    "title": "LiLTv2: Language-substitutable Layout-Image Transformer for Visual Information Extraction",
    "doi": "https://doi.org/10.1145/3708351",
    "publication_date": "2024-12-11",
    "publication_year": 2024,
    "authors": "Jiapeng Wang; Zening Lin; Dayi Huang; Longfei Xiong; Lianwen Jin",
    "corresponding_authors": "",
    "abstract": "Visual information extraction (VIE) has experienced substantial growth and heightened interest due to its pivotal role in intelligent document processing. However, most existing related pre-trained models typically can only process the data from a certain (set of) language(s)—often just English, representing a distinct limitation. To solve it, we present a L anguage-subst i tutable L ayout-Image T ransformer ( LiLTv2 ). It can be pre-trained just once on monolingual documents and then collaborate with off-the-shelf textual models in other languages during fine-tuning. Firstly, LiLTv2 utilizes a new dual-stream model architecture, one stream for substitutable text information and the other for layout and image information. Then, LiLTv2 has improved upon the optimization strategy and the diverse tasks adopted in the pre-training stage. Finally, we innovatively propose a teacher-student knowledge distillation learning with segment-level multi-modal features named SegKD. Extensive experimental results on widely-used benchmarks can demonstrate the superior effectiveness of our method.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4405282449",
    "type": "article"
  },
  {
    "title": "Learning Semantic-Aware Representation in Visual-Language Models for Multi-Label Recognition with Partial Labels",
    "doi": "https://doi.org/10.1145/3708991",
    "publication_date": "2024-12-23",
    "publication_year": 2024,
    "authors": "Haoxian Ruan; Zhihua Xu; Zhijing Yang; Yongyi Lu; Jinghui Qin; Tianshui Chen",
    "corresponding_authors": "",
    "abstract": "Multi-label recognition with partial labels (MLR-PL), in which only some labels are known while others are unknown for each image, is a practical task in computer vision, since collecting large-scale and complete multi-label datasets is difficult in real application scenarios. Recently, vision language models (e.g. CLIP) have demonstrated impressive transferability to downstream tasks in data limited or label limited settings. However, current CLIP-based methods suffer from semantic confusion in MLR task due to the lack of fine-grained information in the single global visual and textual representation for all categories. In this work, we address this problem by introducing a semantic decoupling module and a category-specific prompt optimization method in CLIP-based framework. Specifically, the semantic decoupling module following the visual encoder learns category-specific feature maps by utilizing the semantic-guided spatial attention mechanism. Moreover, the category-specific prompt optimization method is introduced to learn text representations aligned with category semantics. Therefore, the prediction of each category is independent, which alleviate the semantic confusion problem. Extensive experiments on Microsoft COCO 2014 and Pascal VOC 2007 datasets demonstrate that the proposed framework significantly outperforms current state-of-art methods with a simpler model structure. Additionally, visual analysis shows that our method effectively separates information from different categories and achieves better performance compared to CLIP-based baseline method.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4405706077",
    "type": "article"
  },
  {
    "title": "A graphics architecture for high-end interactive television terminals",
    "doi": "https://doi.org/10.1145/1201730.1201735",
    "publication_date": "2006-11-01",
    "publication_year": 2006,
    "authors": "Pablo César; Petri Vuorimaa; Juha Vierinen",
    "corresponding_authors": "",
    "abstract": "This article presents a graphics software architecture for next-generation digital television receivers. We propose that such receivers should include a standardised Java-based procedural environment capable of rendering 2D/3D graphics and video, and a declarative environment supporting W3C recommendations such as SMIL and XForms. We also introduce a graphics architecture model that meets such requirements. As a proof-of-concept, a prototype implementation of the model is presented. This implementation enhances television content by allowing the user to play 3D graphics games, to run Java applications, and to browse XML-based documents while meeting current hardware restrictions.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W1994315388",
    "type": "article"
  },
  {
    "title": "Metadata handling",
    "doi": "https://doi.org/10.1145/1201730.1201736",
    "publication_date": "2006-11-01",
    "publication_year": 2006,
    "authors": "Chitra L. Madhwacharyula; Marc Davis; Philippe Mulhem; Mohan Kankanhalli",
    "corresponding_authors": "",
    "abstract": "This article addresses the problem of processing the annotations of preexisting video productions to enable reuse and repurposing of metadata. We introduce the concept of automatic content-based editing of preexisting semantic home video metadata. We propose a formal representation and implementation techniques for reusing and repurposing semantic video metadata in concordance with the actual video editing operations. A novel representation for metadata editing is proposed and an implementation framework for editing the metadata in accordance with the video editing operations is demonstrated. Conflict resolution and regularization operations are defined and implemented in the context of the video metadata editing operations.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2030060245",
    "type": "article"
  },
  {
    "title": "The berkeley software MPEG-1 video decoder",
    "doi": "https://doi.org/10.1145/1047936.1047944",
    "publication_date": "2005-02-01",
    "publication_year": 2005,
    "authors": "Ketan Mayer‐Patel; Brian C. Smith; Lawrence A. Rowe",
    "corresponding_authors": "",
    "abstract": "This article reprises the description of the Berkeley software-only MPEG-1 video decoder originally published in the proceedings of the 1st International ACM Conference on Multimedia in 1993. The software subsequently became widely used in a variety of research systems and commercial products. Its main impact was to provide a platform for experimenting with streaming compressed video and to expose the strengths and weaknesses of software-only video decoding using general purpose computing architectures. This article compares the original performance results with experiments run on a modern processor to demonstrate the gains of processing power in the past ten years relative to this specific application and discusses the history of MPEG-1 video software decoding and the Berkeley MPEG research group.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2076791745",
    "type": "article"
  },
  {
    "title": "Modeling context in haptic perception, rendering, and visualization",
    "doi": "https://doi.org/10.1145/1152149.1152153",
    "publication_date": "2006-08-01",
    "publication_year": 2006,
    "authors": "Kanav Kahol; Priyamvada Tripathi; Troy McDaniel; Laura Bratton; Sethuraman Panchanathan",
    "corresponding_authors": "",
    "abstract": "Haptic perception refers to the ability of human beings to perceive spatial properties through touch-based sensations. In haptics, contextual clues about material,shape, size, texture, and weight configurations of an object are perceived by individuals leading to recognition of the object and its spatial features. In this paper, we present strategies and algorithms to model context in haptic applications that allow users to haptically explore objects in virtual reality/augmented reality environments. Initial results show significant improvement in accuracy and efficiency of haptic perception in augmented reality environments when compared to conventional approaches that do not model context in haptic rendering.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2120196709",
    "type": "article"
  },
  {
    "title": "Statistical admission control using delay distribution measurements",
    "doi": "https://doi.org/10.1145/1201730.1201732",
    "publication_date": "2006-11-01",
    "publication_year": 2006,
    "authors": "Kartik Gopalan; Lan Huang; Gang Peng; Tzi‐cker Chiueh; Yow-Jian Lin",
    "corresponding_authors": "",
    "abstract": "Growth of performance sensitive applications, such as voice and multimedia, has led to widespread adoption of resource virtualization by a variety of service providers (xSPs). For instance, Internet Service Providers (ISPs) increasingly differentiate their offerings by means of customized services, such as virtual private networks (VPN) with Quality of Service (QoS) guarantees or QVPNs. Similarly Storage Service Providers (SSPs) use storage area networks (SAN)/network attached storage (NAS) technology to provision virtual disks with QoS guarantees or QVDs. The key challenge faced by these xSPs is to maximize the number of virtual resource units they can support by exploiting the statistical multiplexing nature of the customers' input request load.While a number of measurement-based admission control algorithms utilize statistical multiplexing along the bandwidth dimension, they do not satisfactorily exploit statistical multiplexing along the delay dimension to guarantee distinct per-virtual-unit delay bounds. This article presents Delay Distribution Measurement (DDM) based admission control algorithm, the first measurement-based approach that effectively exploits statistical multiplexing along the delay dimension. In other words, DDM exploits the well-known fact that the actual delay experienced by most service requests (packets or disk I/O requests) for a virtual unit is usually far smaller than its worst-case delay bound requirement because multiple virtual units rarely send request bursts at the same time. Additionally, DDM supports virtual units with distinct probabilistic delay guarantees---virtual units that can tolerate more delay violations can reserve fewer resources than those that tolerate less, even though they require the same delay bound. Comprehensive trace-driven performance evaluation of QVPNs (using Voice over IP traces) and QVDs (using video stream, TPC-C, and Web search I/O traces) shows that, when compared to deterministic admission control, DDM can potentially increase the number of admitted virtual units (and resource utilization) by up to a factor of 3.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2163864896",
    "type": "article"
  },
  {
    "title": "Efficient sampling of training set in large and noisy multimedia data",
    "doi": "https://doi.org/10.1145/1236471.1236473",
    "publication_date": "2007-08-01",
    "publication_year": 2007,
    "authors": "Surong Wang; Manoranjan Dash; Liang-Tien Chia; Min Xu",
    "corresponding_authors": "",
    "abstract": "As the amount of multimedia data is increasing day-by-day thanks to less expensive storage devices and increasing numbers of information sources, machine learning algorithms are faced with large-sized and noisy datasets. Fortunately, the use of a good sampling set for training influences the final results significantly. But using a simple random sample (SRS) may not obtain satisfactory results because such a sample may not adequately represent the large and noisy dataset due to its blind approach in selecting samples. The difficulty is particularly apparent for huge datasets where, due to memory constraints, only very small sample sizes are used. This is typically the case for multimedia applications, where data size is usually very large. In this article we propose a new and efficient method to sample of large and noisy multimedia data. The proposed method is based on a simple distance measure that compares the histograms of the sample set and the whole set in order to estimate the representativeness of the sample. The proposed method deals with noise in an elegant manner which SRS and other methods are not able to deal with. We experiment on image and audio datasets. Comparison with SRS and other methods shows that the proposed method is vastly superior in terms of sample representativeness, particularly for small sample sizes although time-wise it is comparable to SRS, the least expensive method in terms of time.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2053744138",
    "type": "article"
  },
  {
    "title": "A scalable and extensible segment-event-object-based sports video retrieval system",
    "doi": "https://doi.org/10.1145/1352012.1352017",
    "publication_date": "2008-05-01",
    "publication_year": 2008,
    "authors": "Dian Tjondronegoro; Yi‐Ping Phoebe Chen; Adrien Joly",
    "corresponding_authors": "",
    "abstract": "Sport video data is growing rapidly as a result of the maturing digital technologies that support digital video capture, faster data processing, and large storage. However, (1) semi-automatic content extraction and annotation, (2) scalable indexing model, and (3) effective retrieval and browsing, still pose the most challenging problems for maximizing the usage of large video databases. This article will present the findings from a comprehensive work that proposes a scalable and extensible sports video retrieval system with two major contributions in the area of sports video indexing and retrieval. The first contribution is a new sports video indexing model that utilizes semi-schema-based indexing scheme on top of an Object-Relationship approach. This indexing model is scalable and extensible as it enables gradual index construction which is supported by ongoing development of future content extraction algorithms. The second contribution is a set of novel queries which are based on XQuery to generate dynamic and user-oriented summaries and event structures. The proposed sports video retrieval system has been fully implemented and populated with soccer, tennis, swimming, and diving video. The system has been evaluated against 20 users to demonstrate and confirm its feasibility and benefits. The experimental sports genres were specifically selected to represent the four main categories of sports domain: period-, set-point-, time (race)-, and performance-based sports. Thus, the proposed system should be generic and robust for all types of sports.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2084191641",
    "type": "article"
  },
  {
    "title": "Client-centered multimedia content adaptation",
    "doi": "https://doi.org/10.1145/1556134.1556139",
    "publication_date": "2009-08-01",
    "publication_year": 2009,
    "authors": "Yong Wei; Suchendra M. Bhandarkar; Kang Li",
    "corresponding_authors": "",
    "abstract": "The design and implementation of a client-centered multimedia content adaptation system suitable for a mobile environment comprising of resource-constrained handheld devices or clients is described. The primary contributions of this work are: (1) the overall architecture of the client-centered content adaptation system, (2) a data-driven multi-level Hidden Markov model (HMM)-based approach to perform both video segmentation and video indexing in a single pass, and (3) the formulation and implementation of a Multiple-choice Multidimensional Knapsack Problem (MMKP)-based video personalization strategy. In order to segment and index video data, a video stream is modeled at both the semantic unit level and video program level. These models are learned entirely from training data and no domain-dependent knowledge about the structure of video programs is used. This makes the system capable of handling various kinds of videos without having to manually redefine the program model. The proposed MMKP-based personalization strategy is shown to include more relevant video content in response to the client's request than the existing 0/1 knapsack problem and fractional knapsack problem-based strategies, and is capable of satisfying multiple client-side constraints simultaneously. Experimental results on CNN news videos and Major League Soccer (MLS) videos are presented and analyzed.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2104570225",
    "type": "article"
  },
  {
    "title": "Structure-Aware Multimodal Feature Fusion for RGB-D Scene Classification and Beyond",
    "doi": "https://doi.org/10.1145/3115932",
    "publication_date": "2018-04-30",
    "publication_year": 2018,
    "authors": "Anran Wang; Jianfei Cai; Jiwen Lu; Tat‐Jen Cham",
    "corresponding_authors": "",
    "abstract": "While convolutional neural networks (CNNs) have been excellent for object recognition, the greater spatial variability in scene images typically means that the standard full-image CNN features are suboptimal for scene classification. In this article, we investigate a framework allowing greater spatial flexibility, in which the Fisher vector (FV)-encoded distribution of local CNN features, obtained from a multitude of region proposals per image, is considered instead. The CNN features are computed from an augmented pixel-wise representation consisting of multiple modalities of RGB, HHA, and surface normals, as extracted from RGB-D data. More significantly, we make two postulates: (1) component sparsity—that only a small variety of region proposals and their corresponding FV GMM components contribute to scene discriminability, and (2) modal nonsparsity—that features from all modalities are encouraged to coexist. In our proposed feature fusion framework, these are implemented through regularization terms that apply group lasso to GMM components and exclusive group lasso across modalities. By learning and combining regressors for both proposal-based FV features and global CNN features, we are able to achieve state-of-the-art scene classification performance on the SUNRGBD Dataset and NYU Depth Dataset V2. Moreover, we further apply our feature fusion framework on an action recognition task to demonstrate that our framework can be generalized for other multimodal well-structured features. In particular, for action recognition, we enforce interpart sparsity to choose more discriminative body parts, and intermodal nonsparsity to make informative features from both appearance and motion modalities coexist. Experimental results on the JHMDB and MPII Cooking Datasets show that our feature fusion is also very effective for action recognition, achieving very competitive performance compared with the state of the art.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2803166488",
    "type": "article"
  },
  {
    "title": "Image Captioning via Semantic Guidance Attention and Consensus Selection Strategy",
    "doi": "https://doi.org/10.1145/3271485",
    "publication_date": "2018-10-10",
    "publication_year": 2018,
    "authors": "Jie Wu; Haifeng Hu; Yi Wu",
    "corresponding_authors": "",
    "abstract": "Recently, a series of attempts have incorporated spatial attention mechanisms into the task of image captioning, which achieves a remarkable improvement in the quality of generative captions. However, the traditional spatial attention mechanism adopts latent and delayed semantic representations to decide which area should be paid more attention to, resulting in inaccurate semantic guidance and the introduction of redundant information. In order to optimize the spatial attention mechanism, we propose the Semantic Guidance Attention (SGA) mechanism in this article. Specifically, SGA utilizes semantic word representations to provide an intuitive semantic guidance that focuses accurately on semantic-related regions. Moreover, we reduce the difficulty of generating fluent sentences by updating the attention information in time. At the same time, the beam search algorithm is widely used to predict words during sequence generation. This algorithm generates a sentence according to the probabilities of words, so it is easy to push out a generic sentence and discard some distinctive captions. In order to overcome this limitation, we design the Consensus Selection (CS) strategy to choose the most descriptive and informative caption, which is selected by the semantic similarity of captions instead of the probabilities of words. The consensus caption is determined by selecting the one with the highest cumulative semantic similarity with respect to the reference captions. Our proposed model (SGA-CS) is validated on Flickr30k and MSCOCO, which shows that SGA-CS outperforms state-of-the-art approaches. To our best knowledge, SGA-CS is the first attempt to jointly produce semantic attention guidance and select descriptive captions for image captioning tasks, achieving one of the best performance ratings among any cross-entropy training methods.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2897927512",
    "type": "article"
  },
  {
    "title": "Exploiting unconscious user signals in multimodal human-computer interaction",
    "doi": "https://doi.org/10.1145/2502433",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Elisabeth André",
    "corresponding_authors": "Elisabeth André",
    "abstract": "This article presents the idea of empathic stimulation that relies on the power and potential of unconsciously conveyed attentive and emotional information to facilitate human-machine interaction. Starting from a historical review of related work presented at past ACM Multimedia conferences, we discuss challenges that arise when exploiting unconscious human signals for empathic stimulation, such as the real-time analysis of psychological user states and the smooth adaptation of the human-machine interface based on this analysis. A classical application field that might benefit from the idea of unconscious human-computer interaction is the exploration of massive datasets.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W1993206555",
    "type": "article"
  },
  {
    "title": "Traffic prediction and QoS transmission of real-time live VBR videos in WLANs",
    "doi": "https://doi.org/10.1145/2043612.2043614",
    "publication_date": "2011-11-01",
    "publication_year": 2011,
    "authors": "Wen‐Kuang Kuo; Kuo-Wei Wu",
    "corresponding_authors": "",
    "abstract": "As the demand for broadband multimedia wireless services is increasing, improving quality of service (QoS) of the widely deployed IEEE 802.11 wireless LANs (WLANs) has become crucial. To support the QoS required by a wide range of applications, the IEEE 802.11 working group has defined a new standard—the IEEE 802.11e. Substantial studies have been performed on traffic scheduling for variable bit rate (VBR) video transport over 802.11e WLANs. However, within those studies, relatively little attention has been devoted to the QoS transmission of real-time live VBR videos. In this paper, we present a novel traffic scheduling algorithm for IEEE 802.11e that aims at achieving high channel utilization while still guaranteeing QoS requirements for real-time live VBR videos. The novel characteristic of this algorithm, compared to published literatures, is that it predicts the bandwidth requirements for future traffic using a novel traffic predictor designed to provide simple yet accurate online prediction. Analyses using real life MPEG video traces indicate that the proposed traffic predictor significantly outperforms previously published technique with respect to the prediction error. The proposed traffic predictor can also be used independently to estimate any MPEG traffic. The performance of the proposed traffic scheduling algorithm is also investigated by comparing several existing scheduling algorithms. Simulation results demonstrate that the proposed traffic scheduling algorithm surpasses other mechanisms in terms of channel utilization, buffer usage, video quality and packet loss rate.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2030536196",
    "type": "article"
  },
  {
    "title": "Over twenty years of eigenfaces",
    "doi": "https://doi.org/10.1145/2490824",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Matthew Turk",
    "corresponding_authors": "Matthew Turk",
    "abstract": "The inaugural ACM Multimedia Conference coincided with a surge of interest in computer vision technologies for detecting and recognizing people and their activities in images and video. Face recognition was the first of these topics to broadly engage the vision and multimedia research communities. The Eigenfaces approach was, deservedly or not, the method that captured much of the initial attention, and it continues to be taught and used as a benchmark over 20 years later. This article is a brief personal view of the genesis of Eigenfaces for face recognition and its relevance to the multimedia community.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2037396294",
    "type": "article"
  },
  {
    "title": "A unified context model for web image retrieval",
    "doi": "https://doi.org/10.1145/2240136.2240141",
    "publication_date": "2012-07-01",
    "publication_year": 2012,
    "authors": "Linjun Yang; Bo Geng; Alan Hanjalić; Xian‐Sheng Hua",
    "corresponding_authors": "",
    "abstract": "Content-based web image retrieval based on the query-by-example (QBE) principle remains a challenging problem due to the semantic gap as well as the gap between a user's intent and the representativeness of a typical image query. In this article, we propose to address this problem by integrating query-related contextual information into an advanced query model to improve the performance of QBE-based web image retrieval. We consider both the local and global context of the query image. The local context can be inferred from the web pages and the click-through log associated with the query image, while the global context is derived from the entire corpus comprising all web images and the associated web pages. To effectively incorporate the local query context we propose a language modeling based approach to deal with the combined structured query representation from the contextual and visual information. The global query context is integrated by the multi-modal relevance model to “reconstruct” the query from the document models indexed in the corpus. In this way, the global query context is employed to address the noise or missing information in the query and its local context, so that a comprehensive and robust query model can be obtained. We evaluated the proposed approach on a representative product image dataset collected from the web and demonstrated that the inclusion of the local and global query contexts significantly improves the performance of QBE-based web image retrieval.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2045329095",
    "type": "article"
  },
  {
    "title": "Label-to-region with continuity-biased bi-layer sparsity priors",
    "doi": "https://doi.org/10.1145/2379790.2379792",
    "publication_date": "2012-11-01",
    "publication_year": 2012,
    "authors": "Xiaobai Liu; Shuicheng Yan; Bin Cheng; Jinhui Tang; Tat-Sheng Chua; Hai Jin",
    "corresponding_authors": "",
    "abstract": "In this work, we investigate how to reassign the fully annotated labels at image level to those contextually derived semantic regions, namely Label-to-Region (L2R), in a collective manner. Given a set of input images with label annotations, the basic idea of our approach to L2R is to first discover the patch correspondence across images, and then propagate the common labels shared in image pairs to these correlated patches. Specially, our approach consists of following aspects. First, each of the input images is encoded as a Bag-of-Hierarchical-Patch (BOP) for capturing the rich cues at variant scales, and the individual patches are expressed by patch-level feature descriptors. Second, we present a sparse representation formulation for discovering how well an image or a semantic region can be robustly reconstructed by all the other image patches from the input image set. The underlying philosophy of our formulation is that an image region can be sparsely reconstructed with the image patches belonging to the other images with common labels, while the robustness in label propagation across images requires that these selected patches come from very few images. This preference of being sparse at both patch and image level is named bi-layer sparsity prior . Meanwhile, we enforce the preference of choosing larger-size patches in reconstruction, referred to as continuity-biased prior in this work, which may further enhance the reliability of L2R assignment. Finally, we harness the reconstruction coefficients to propagate the image labels to the matched patches, and fuse the propagation results over all patches to finalize the L2R task. As a by-product, the proposed continuity-biased bi-layer sparse representation formulation can be naturally applied to perform image annotation on new testing images. Extensive experiments on three public image datasets clearly demonstrate the effectiveness of our proposed framework in both L2R assignment and image annotation.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2111249294",
    "type": "article"
  },
  {
    "title": "Query Expansion for Content-Based Similarity Search Using Local and Global Features",
    "doi": "https://doi.org/10.1145/3063595",
    "publication_date": "2017-05-31",
    "publication_year": 2017,
    "authors": "Michael E. Houle; Xiguo Ma; Vincent Oria; Jichao Sun",
    "corresponding_authors": "",
    "abstract": "This article presents an efficient and totally unsupervised content-based similarity search method for multimedia data objects represented by high-dimensional feature vectors. The assumption is that the similarity measure is applicable to feature vectors of arbitrary length. During the offline process, different sets of features are selected by a generalized version of the Laplacian Score in an unsupervised way for individual data objects in the database. Online retrieval is performed by ranking the query object in the feature spaces of candidate objects. Those candidates for which the query object is ranked highly are selected as the query results. The ranking scheme is incorporated into an automated query expansion framework to further improve the semantic quality of the search result. Extensive experiments were conducted on several datasets to show the capability of the proposed method in boosting effectiveness without losing efficiency.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2623263547",
    "type": "article"
  },
  {
    "title": "Spott",
    "doi": "https://doi.org/10.1145/3092834",
    "publication_date": "2017-06-28",
    "publication_year": 2017,
    "authors": "Florian Vandecasteele; Karel Vandenbroucke; Dimitri Schuurman; Steven Verstockt",
    "corresponding_authors": "",
    "abstract": "Spott is an innovative second screen mobile multimedia application which offers viewers relevant information on objects (e.g., clothing, furniture, food) they see and like on their television screens. The application enables interaction between TV audiences and brands, so producers and advertisers can offer potential consumers tailored promotions, e-shop items, and/or free samples. In line with the current views on innovation management, the technological excellence of the Spott application is coupled with iterative user involvement throughout the entire development process. This article discusses both of these aspects and how they impact each other. First, we focus on the technological building blocks that facilitate the (semi-) automatic interactive tagging process of objects in the video streams. The majority of these building blocks extensively make use of novel and state-of-the-art deep learning concepts and methodologies. We show how these deep learning based video analysis techniques facilitate video summarization, semantic keyframe clustering, and (similar) object retrieval. Secondly, we provide insights in user tests that have been performed to evaluate and optimize the application’s user experience. The lessons learned from these open field tests have already been an essential input in the technology development and will further shape the future modifications to the Spott application.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2729868168",
    "type": "article"
  },
  {
    "title": "An Analytic System for User Gender Identification through User Shared Images",
    "doi": "https://doi.org/10.1145/3095077",
    "publication_date": "2017-06-28",
    "publication_year": 2017,
    "authors": "Ming Cheung; James She",
    "corresponding_authors": "",
    "abstract": "Many social media applications, such as recommendation, virality prediction, and marketing, make use of user gender, which may not be explicitly specified or kept privately. Meanwhile, advanced mobile devices have become part of our lives and a huge amount of content is being generated by users every day, especially user shared images shared by individuals in social networks. This particular form of user generated content is widely accessible to others due to the sharing nature. When user gender is only accessible to exclusive parties, these user shared images are proved to be an easier way to identify user gender. This work investigated 3,152,344 images by 7,450 users from Fotolog and Flickr, two image-oriented social networks. It is observed that users who share visually similar images are more likely to have the same gender. A multimedia big data system that utilizes this phenomenon is proposed for user gender identification with 79% accuracy. These findings are useful for information or services in any social network with intensive image sharing.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2733413540",
    "type": "article"
  },
  {
    "title": "An Efficient Computation Framework for Connection Discovery using Shared Images",
    "doi": "https://doi.org/10.1145/3115951",
    "publication_date": "2017-08-29",
    "publication_year": 2017,
    "authors": "Ming Cheung; Xiaopeng Li; James She",
    "corresponding_authors": "",
    "abstract": "With the advent and popularity of the social network, social graphs become essential to improve services and information relevance to users for many social media applications to predict follower/followee relationship, community membership, and so on. However, the social graphs could be hidden by users due to privacy concerns or kept by social media. Recently, connections discovered from user-shared images using machine-generated labels are proved to be more accessible alternatives to social graphs. But real-time discovery is difficult due to high complexity, and many applications are not possible. This article proposes an efficient computation framework for connection discovery using user-shared images, which is suitable for any image processing and computer vision techniques for connection discovery on the fly. The framework includes the architecture of online computation to facilitate real-time processing, offline computation for a complete processing, and online/offline communication. The proposed framework is implemented to demonstrate its effectiveness by speeding up connection discovery through user-shared images. By studying 300K+ user-shared images from two popular social networks, it is proven that the proposed computation framework reduces 90% of runtime with a comparable accurate with existing frameworks.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2752940591",
    "type": "article"
  },
  {
    "title": "Delay-Aware Quality Optimization in Cloud-Assisted Video Streaming System",
    "doi": "https://doi.org/10.1145/3152116",
    "publication_date": "2017-12-13",
    "publication_year": 2017,
    "authors": "Jiyan Wu; Bo Cheng; Yuan Yang; Ming Wang; Junliang Chen",
    "corresponding_authors": "",
    "abstract": "Cloud-assisted video streaming has emerged as a new paradigm to optimize multimedia content distribution over the Internet. This article investigates the problem of streaming cloud-assisted real-time video to multiple destinations (e.g., cloud video conferencing, multi-player cloud gaming, etc.) over lossy communication networks. The user diversity and network dynamics result in the delay differences among multiple destinations. This research proposes &lt;underline&gt;D&lt;/underline&gt;ifferentiated cloud-&lt;underline&gt;A&lt;/underline&gt;ssisted &lt;underline&gt;VI&lt;/underline&gt;deo &lt;underline&gt;S&lt;/underline&gt;treaming (DAVIS) framework, which proactively leverages such delay differences in video coding and transmission optimization. First, we analytically formulate the optimization problem of joint coding and transmission to maximize received video quality. Second, we develop a quality optimization framework that integrates the video representation selection and FEC (Forward Error Correction) packet interleaving. The proposed DAVIS is able to effectively perform differentiated quality optimization for multiple destinations by taking advantage of the delay differences in cloud-assisted video streaming system. We conduct the performance evaluation through extensive experiments with the Amazon EC2 instances and Exata emulation platform. Evaluation results show that DAVIS outperforms the reference cloud-assisted streaming solutions in video quality and delay performance.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2780802604",
    "type": "article"
  },
  {
    "title": "Implicit Emotion Communication",
    "doi": "https://doi.org/10.1145/3152128",
    "publication_date": "2017-12-20",
    "publication_year": 2017,
    "authors": "Rodrigo Ceballos; Beatrice Ionascu; Wanjoo Park; Mohamad Eid",
    "corresponding_authors": "",
    "abstract": "Today, ubiquitous digital communication systems do not have an intuitive, natural way of communicating emotion, which, in turn, affects the degree to which humans can emotionally connect and interact with one another. To address this problem, a more natural, intuitive, and implicit emotion communication system was designed and created that employs asymmetry-based EEG emotion classification for detecting the emotional state of the sender and haptic feedback (in the form of tactile gestures) for displaying emotions for a receiver. Emotions are modeled in terms of valence (positive/negative emotions) and arousal (intensity of the emotion). Performance analysis shows that the proposed EEG subject-dependent emotion classification model with Free Asymmetry features allows for more flexible feature-generation schemes than other existing algorithms and attains an average accuracy of 92.5% for valence and 96.5% for arousal, outperforming previous-generation schemes in high feature space. As for the haptic feedback, a tactile gesture authoring tool and a haptic jacket were developed to design tactile gestures that can intensify emotional reactions in terms of valence and arousal. Experimental study demonstrated that subject-independent emotion transmission through tactile gestures is effective for the arousal dimension of an emotion but is less effective for valence. Consistency in subject-dependent responses for both valence and arousal suggests that personalized tactile gestures would be more effective.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2781046831",
    "type": "article"
  },
  {
    "title": "Robust Multi-Variate Temporal Features of Multi-Variate Time Series",
    "doi": "https://doi.org/10.1145/3152123",
    "publication_date": "2018-01-04",
    "publication_year": 2018,
    "authors": "Sicong Liu; Silvestro Roberto Poccia; K. Selçuk Candan; Maria Luisa Sapino; Xiaolan Wang",
    "corresponding_authors": "",
    "abstract": "Many applications generate and/or consume multi-variate temporal data, and experts often lack the means to adequately and systematically search for and interpret multi-variate observations. In this article, we first observe that multi-variate time series often carry localized multi-variate temporal features that are robust against noise. We then argue that these multi-variate temporal features can be extracted by simultaneously considering, at multiple scales, temporal characteristics of the time series along with external knowledge , including variate relationships that are known a priori. Relying on these observations, we develop data models and algorithms to detect robust multi-variate temporal (RMT) features that can be indexed for efficient and accurate retrieval and can be used for supporting data exploration and analysis tasks. Experiments confirm that the proposed RMT algorithm is highly effective and efficient in identifying robust multi-scale temporal features of multi-variate time series.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2781660332",
    "type": "article"
  },
  {
    "title": "Full 3D Reconstruction of Non-Rigidly Deforming Objects",
    "doi": "https://doi.org/10.1145/3177756",
    "publication_date": "2018-03-26",
    "publication_year": 2018,
    "authors": "Hassan Afzal; Djamila Aouada; Bruno Mirbach; Björn Ottersten",
    "corresponding_authors": "",
    "abstract": "In this article, we discuss enhanced full 360° 3D reconstruction of dynamic scenes containing non-rigidly deforming objects using data acquired from commodity depth or 3D cameras. Several approaches for enhanced and full 3D reconstruction of non-rigid objects have been proposed in the literature. These approaches suffer from several limitations due to requirement of a template, inability to tackle large local deformations and topology changes, inability to tackle highly noisy and low-resolution data, and inability to produce online results. We target online and template-free enhancement of the quality of noisy and low-resolution full 3D reconstructions of dynamic non-rigid objects. For this purpose, we propose a view-independent recursive and dynamic multi-frame 3D super-resolution scheme for noise removal and resolution enhancement of 3D measurements. The proposed scheme tracks the position and motion of each 3D point at every timestep by making use of the current acquisition and the result of the previous iteration. The effects of system blur due to per-point tracking are subsequently tackled by introducing a novel and efficient multi-level 3D bilateral total variation regularization. These characteristics enable the proposed scheme to handle large deformations and topology changes accurately. A thorough evaluation of the proposed scheme on both real and simulated data is carried out. The results show that the proposed scheme improves upon the performance of the state-of-the-art methods and is able to accurately enhance the quality of low-resolution and highly noisy 3D reconstructions while being robust to large local deformations.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2794685201",
    "type": "article"
  },
  {
    "title": "Over- and Under-Exposure Reconstruction of a Single Plenoptic Capture",
    "doi": "https://doi.org/10.1145/3199514",
    "publication_date": "2018-05-22",
    "publication_year": 2018,
    "authors": "Wei Hu; Mozhdeh Seifi; Erik Reinhard",
    "corresponding_authors": "",
    "abstract": "Light field images, for example, taken with plenoptic cameras, offer interesting post-processing opportunities, including depth-of-field management, depth estimation, viewpoint selection, and 3D image synthesis. Like most capture devices, however, plenoptic cameras have a limited dynamic range, so that over- and under-exposed areas in plenoptic images are commonplace. We therefore present a straightforward and robust plenoptic reconstruction technique based on the observation that vignetting causes peripheral views to receive less light than central views. Thus, corresponding pixels in different views can be used to reconstruct illumination, especially in areas where information missing in one view is present in another. Our algorithm accurately reconstructs under- and over-exposed regions (known as declipping), additionally affording an increase in peak luminance by up to two f-stops, and a comparable lowering of the noise floor. The key advantages of this approach are that no hardware modifications are necessary to improve the dynamic range, that no multiple exposure techniques are required, and therefore that no ghosting or other artifacts are introduced.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2803824436",
    "type": "article"
  },
  {
    "title": "Paillier Cryptosystem based Mean Value Computation for Encrypted Domain Image Processing Operations",
    "doi": "https://doi.org/10.1145/3325194",
    "publication_date": "2019-08-31",
    "publication_year": 2019,
    "authors": "Mohsin Shah; Weiming Zhang; Honggang Hu; Nenghai Yu",
    "corresponding_authors": "",
    "abstract": "Due to its large storage facility and high-end computing capability, cloud computing has received great attention as a huge amount of personal multimedia data and computationally expensive tasks can be outsourced to the cloud. However, the cloud being third-party semi-trusted, is prone to information leakage, raising privacy risks. Signal processing in the encrypted domain has emerged as a new research paradigm on privacy-preserving processing over outsourced data by semi-trusted cloud. In this article, we propose a solution for non-integer mean value computation in the homomorphic encrypted domain without any interactive protocol between the client and the service provider. Using the proposed solution, various image processing operations, such as local smoothing filter, un-sharp masking, and histogram equalization, can be performed in the encrypted domain at the cloud server without any privacy concerns. Our experimental results from standard test images reveal that these image processing operations can be performed without pre-processing, without client-server interactive protocol, and without any error between the encrypted domain and the plain domain.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2972867308",
    "type": "article"
  },
  {
    "title": "Spatial Preserved Graph Convolution Networks for Person Re-identification",
    "doi": "https://doi.org/10.1145/3362988",
    "publication_date": "2020-01-31",
    "publication_year": 2020,
    "authors": "Zhaoju Li; Zongwei Zhou; Nan Jiang; Zhenjun Han; Junliang Xing; Jianbin Jiao",
    "corresponding_authors": "",
    "abstract": "Person Re-identification is a very challenging task due to inter-class ambiguity caused by similar appearances, and large intra-class diversity caused by viewpoints, illuminations, and poses. To address these challenges, in this article, a graph convolution network based model for person re-identification is proposed to learn more discriminative feature embeddings, where a graph-structured relationship between person images and person parts are together integrated. Graph convolution networks extract common characteristics of the same person, while pyramid feature embedding exploits parts relations and learns stable representation with each person image. We achieve a very competitive performance respectively on three widely used datasets, indicating that the proposed approach significantly outperforms the baseline methods and achieves the state-of-the-art performance.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W3018514853",
    "type": "article"
  },
  {
    "title": "Eye-based Recognition for User Identification on Mobile Devices",
    "doi": "https://doi.org/10.1145/3399659",
    "publication_date": "2020-11-30",
    "publication_year": 2020,
    "authors": "Huiru Shao; Jing Li; Jia Zhang; Hui Yu; Jiande Sun",
    "corresponding_authors": "",
    "abstract": "User identification is becoming more and more important for Apps on mobile devices. However, the identity recognition based on eyes, e.g., iris recognition, is rarely used on mobile devices comparing with those based on face and fingerprint due to its extra cost in hardware and complicated operations during recognition. In this article, an eye-based recognition method is designed for identity recognition on mobile devices, which can be implemented just like face recognition. In the proposed method, the eye feature is composed of the static and dynamic features, where the periocular feature extracted by deep neural network from the eye image is used as the static feature, and the motion feature of saccadic velocity is selected as the dynamic feature. The eye images can be captured by the normal camera on mobile devices just like faces, and dynamic features can provide living information to increase the difficulty of forgery. The GazeCapture dataset is used to test the proposed method, because the eye images in this dataset are captured by mobile devices during daily use. The recognition accuracy of the proposed method on the GazeCapture dataset can reach 96.87% only based on the periocular feature and can be enhanced to 97.99% when it is fused with the saccadic feature. The experiment results show that the performance of the proposed method can be comparative to that of iris recognition methods. It demonstrates that the proposed method is a practical reference for the eye-based identity recognition, and the proposed method provides one more biometric choice for mobile devices.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W3041229290",
    "type": "article"
  },
  {
    "title": "A Secure Multimedia Processing through Blockchain in Smart Healthcare Systems",
    "doi": "https://doi.org/10.1145/3396852",
    "publication_date": "2020-05-07",
    "publication_year": 2020,
    "authors": "Jaafar Alghazo; Geetanjali Rathee; Sharmidev Gupta; Mohammad Tabrez Quasim; Sivaram Murugan; Ghazanfar Latif; Vigneswaran Dhasarathan",
    "corresponding_authors": "",
    "abstract": "The rapidly growing, complex and key area of multimedia processing is medical area where the huge amount of patient?s data is disregarded. Further, medial fields used to interrelate with multimedia formats because of their daily real communication. Healthcare associations are renovating themselves into added efficient, coordinated and user-centered methods through several upcoming techniques. Though, the management of large information such as patient information, medical reports escorts to augments the human labors and security hazards. Therefore, to conquer these problems, IoT-healthcare came into existence that enhances the patients care by reducing the cost of resources in an effective way. Along with lot of IoT benefits, associations are afraid to use them because of its compromise initiated by several intruders. For benefiting their own purposes, intruders hack the patient?s online record for selling to third party for doing the researchers analysis. Further, doctors also make the money for their concession by forcing the patients to buy the medicines from their recommended firms. So as to avert these problems, Blockchain technology has been came across in healthcare systems that may keep track of every activity of the entities. The patients and other entities are recognized through bio metric methods so that even if hackers try to compromise the devices or to access the stored information; the patients may easily identify the intruders or alteration in stored record. In this paper, we have projected a secure IoT healthcare mechanism using Blockchain technique that initially identify the patients and other entities using bio metric passwords and further can analyze the modifications in their records. The proposed scheme results have been validated against traditional method by offering 88% success rate over certain parametric scenarios such as grey hole attack, falsification attack and probabilistic authentication scenarios.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W3041589018",
    "type": "article"
  },
  {
    "title": "Compressed Imaging Reconstruction with Sparse Random Projection",
    "doi": "https://doi.org/10.1145/3447431",
    "publication_date": "2021-02-28",
    "publication_year": 2021,
    "authors": "Peihao Yang; Linghe Kong; Meikang Qiu; Xue Liu; Guihai Chen",
    "corresponding_authors": "",
    "abstract": "As the Internet of Things thrives, monitors and cameras produce tons of image data every day. To efficiently process these images, many compressed imaging frameworks are proposed. A compressed imaging framework comprises two parts, image signal measurement and reconstruction. Although a plethora of measurement devices have been designed, the development of the reconstruction is relatively lagging behind. Nowadays, most of existing reconstruction algorithms in compressed imaging are optimization problem solvers based on specific priors. The computation burdens of these optimization algorithms are enormous and the solutions are usually local optimums. Meanwhile, it is inconvenient to deploy these algorithms on cloud, which hinders the popularization of compressed imaging. In this article, we dive deep into the random projection to build reconstruction algorithms for compressed imaging. We first fully utilize the information in the measurement procedure and propose a combinatorial sparse random projection (SRP) reconstruction algorithm. Then, we generalize the SRP to a novel distributed algorithm called Cloud-SRP (CSRP), which enables efficient reconstruction on cloud. Moreover, we explore the combination of SRP with conventional optimization reconstruction algorithms and propose the Iterative-SRP (ISRP), which converges to a guaranteed fixed point. With minor modifications on the naive optimization algorithms, the ISRP yields better reconstructions. Experiments on real ghost imaging reconstruction reveal that our algorithms are effective. And simulation experiments show their advantages over the classical algorithms.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W3156660294",
    "type": "article"
  },
  {
    "title": "Server Allocation for Massively Multiplayer Online Cloud Games Using Evolutionary Optimization",
    "doi": "https://doi.org/10.1145/3433027",
    "publication_date": "2021-05-11",
    "publication_year": 2021,
    "authors": "Meiqi Zhao; Jianmin Zheng; Elvis S. Liu",
    "corresponding_authors": "",
    "abstract": "In recent years, Massively Multiplayer Online Games (MMOGs) are becoming popular, partially due to their sophisticated graphics and broad virtual world, and cloud gaming is demanded more than ever especially when entertaining with light and portable devices. This article considers the problem of server allocation for running MMOG on cloud, aiming to reduce the cost on cloud gaming service and meanwhile enhance the quality of service. The problem is formulated into minimizing an objective function involving the cost of server rental, the cost of data transfer and the network latency during the gaming time. A genetic algorithm is developed to solve the minimization problem for processing simultaneous server allocation for the players who log into the system at the same time while many existing players are playing the same game. Extensive experiments based on the player behavior in “World of Warcraft” are conducted to evaluate the proposed method and compare with the state-of-the-art as well. The experimental results show that the method gives a lower cost and a shorter network latency in most of the time.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W3161453596",
    "type": "article"
  },
  {
    "title": "Fast Accurate and Automatic Brushstroke Extraction",
    "doi": "https://doi.org/10.1145/3429742",
    "publication_date": "2021-05-11",
    "publication_year": 2021,
    "authors": "Yunfei Fu; Hongchuan Yu; Chih-Kuo Yeh; Tong‐Yee Lee; Jianjun Zhang",
    "corresponding_authors": "",
    "abstract": "Brushstrokes are viewed as the artist’s “handwriting” in a painting. In many applications such as style learning and transfer, mimicking painting, and painting authentication, it is highly desired to quantitatively and accurately identify brushstroke characteristics from old masters’ pieces using computer programs. However, due to the nature of hundreds or thousands of intermingling brushstrokes in the painting, it still remains challenging. This article proposes an efficient algorithm for brush Stroke extraction based on a Deep neural network, i.e., DStroke. Compared to the state-of-the-art research, the main merit of the proposed DStroke is to automatically and rapidly extract brushstrokes from a painting without manual annotation, while accurately approximating the real brushstrokes with high reliability. Herein, recovering the faithful soft transitions between brushstrokes is often ignored by the other methods. In fact, the details of brushstrokes in a master piece of painting (e.g., shapes, colors, texture, overlaps) are highly desired by artists since they hold promise to enhance and extend the artists’ powers, just like microscopes extend biologists’ powers. To demonstrate the high efficiency of the proposed DStroke, we perform it on a set of real scans of paintings and a set of synthetic paintings, respectively. Experiments show that the proposed DStroke is noticeably faster and more accurate at identifying and extracting brushstrokes, outperforming the other methods.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W3161788554",
    "type": "article"
  },
  {
    "title": "Multi-peak Graph-based Multi-instance Learning for Weakly Supervised Object Detection",
    "doi": "https://doi.org/10.1145/3432861",
    "publication_date": "2021-06-14",
    "publication_year": 2021,
    "authors": "Ruyi Ji; Zeyu Liu; Libo Zhang; Jian–wei Liu; Xin Zuo; Yanjun Wu; Chen Zhao; Haofeng Wang; Lin Yang",
    "corresponding_authors": "",
    "abstract": "Weakly supervised object detection (WSOD), aiming to detect objects with only image-level annotations, has become one of the research hotspots over the past few years. Recently, much effort has been devoted to WSOD for the simple yet effective architecture and remarkable improvements have been achieved. Existing approaches using multiple-instance learning usually pay more attention to the proposals individually, ignoring relation information between proposals. Besides, to obtain pseudo-ground-truth boxes for WSOD, MIL-based methods tend to select the region with the highest confidence score and regard those with small overlap as background category, which leads to mislabeled instances. As a result, these methods suffer from mislabeling instances and lacking relations between proposals, degrading the performance of WSOD. To tackle these issues, this article introduces a multi-peak graph-based model for WSOD. Specifically, we use the instance graph to model the relations between proposals, which reinforces multiple-instance learning process. In addition, a multi-peak discovery strategy is designed to avert mislabeling instances. The proposed model is trained by stochastic gradients decent optimizer using back-propagation in an end-to-end manner. Extensive quantitative and qualitative evaluations on two publicly challenging benchmarks, PASCAL VOC 2007 and PASCAL VOC 2012, demonstrate the superiority and effectiveness of the proposed approach.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W3167363556",
    "type": "article"
  },
  {
    "title": "Residual Refinement Network with Attribute Guidance for Precise Saliency Detection",
    "doi": "https://doi.org/10.1145/3440694",
    "publication_date": "2021-07-22",
    "publication_year": 2021,
    "authors": "Feng Lin; Wengang Zhou; Jiajun Deng; Bin Li; Yan Lu; Houqiang Li",
    "corresponding_authors": "",
    "abstract": "As an important topic in the multimedia and computer vision fields, salient object detection has been researched for years. Recently, state-of-the-art performance has been witnessed with the aid of the fully convolutional networks (FCNs) and the various pyramid-like encoder-decoder frameworks. Starting from a common encoder-decoder architecture, we enhance a residual refinement network with feature purification for better saliency estimation. To this end, we improve the global knowledge streams with intermediate supervisions for global saliency estimation and design a specific feature subtraction module for residual learning, respectively. On the basis of the strengthened network, we also introduce an attribute encoding sub-network (AENet) with a grid aggregation block (GAB) to guide the final saliency predictor to obtain more accurate saliency maps. Furthermore, the network is trained with a novel constraint loss besides the traditional cross-entropy loss to yield the finer results. Extensive experiments on five public benchmarks show our method achieves better or comparable performance compared with previous state-of-the-art methods.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W3183652235",
    "type": "article"
  },
  {
    "title": "Video Decolorization Based on the CNN and LSTM Neural Network",
    "doi": "https://doi.org/10.1145/3446619",
    "publication_date": "2021-07-22",
    "publication_year": 2021,
    "authors": "Shiguang Liu; Huixin Wang; Xiaoli Zhang",
    "corresponding_authors": "",
    "abstract": "Video decolorization is the process of transferring three-channel color videos into single-channel grayscale videos, which is essentially the decolorization operation of video frames. Most existing video decolorization algorithms directly apply image decolorization methods to decolorize video frames. However, if we only take the single-frame decolorization result into account, it will inevitably cause temporal inconsistency and flicker phenomenon meaning that the same local content between continuous video frames may display different gray values. In addition, there are often similar local content features between video frames, which indicates redundant information. To solve the preceding problems, this article proposes a novel video decolorization algorithm based on the convolutional neural network and the long short-term memory neural network. First, we design a local semantic content encoder to learn and extract the same local content of continuous video frames, which can better preserve the contrast of video frames. Second, a temporal feature controller based on the bi-directional recurrent neural networks with Long short-term memory units is employed to refine the local semantic features, which can greatly maintain temporal consistency of the video sequence to eliminate the flicker phenomenon. Finally, we take advantages of deconvolution to decode the features to produce the grayscale video sequence. Experiments have indicated that our method can better preserve the local contrast of video frames and the temporal consistency over the state of the-art.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W3185121218",
    "type": "article"
  },
  {
    "title": "A Real-Time Effective Fusion-Based Image Defogging Architecture on FPGA",
    "doi": "https://doi.org/10.1145/3446241",
    "publication_date": "2021-07-22",
    "publication_year": 2021,
    "authors": "Gaoming Du; Jiting Wu; Hongfang Cao; Kun Xing; Zhenmin Li; Duoli Zhang; Xiaolei Wang",
    "corresponding_authors": "",
    "abstract": "Foggy weather reduces the visibility of photographed objects, causing image distortion and decreasing overall image quality. Many approaches (e.g., image restoration, image enhancement, and fusion-based methods) have been proposed to work out the problem. However, most of these defogging algorithms are facing challenges such as algorithm complexity or real-time processing requirements. To simplify the defogging process, we propose a fusional defogging algorithm on the linear transmission of gray single-channel. This method combines gray single-channel linear transform with high-boost filtering according to different proportions. To enhance the visibility of the defogging image more effectively, we convert the RGB channel into a gray-scale single channel without decreasing the defogging results. After gray-scale fusion, the data in the gray-scale domain should be linearly transmitted. With the increasing real-time requirements for clear images, we also propose an efficient real-time FPGA defogging architecture. The architecture optimizes the data path of the guided filtering to speed up the defogging speed and save area and resources. Because the pixel reading order of mean and square value calculations are identical, the shift register in the box filter after the average and the computation of the square values is separated from the box filter and put on the input terminal for sharing, saving the storage area. What’s more, using LUTs instead of the multiplier can decrease the time delays of the square value calculation module and increase efficiency. Experimental results show that the linear transmission can save 66.7% of the total time. The architecture we proposed can defog efficiently and accurately, meeting the real-time defogging requirements on 1920 × 1080 image size.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W3185868934",
    "type": "article"
  },
  {
    "title": "Dual-Stream Structured Graph Convolution Network for Skeleton-Based Action Recognition",
    "doi": "https://doi.org/10.1145/3450410",
    "publication_date": "2021-11-12",
    "publication_year": 2021,
    "authors": "Chunyan Xu; Rong Liu; Tong Zhang; Zhen Cui; Jian Yang; Chunlong Hu",
    "corresponding_authors": "",
    "abstract": "In this work, we propose a dual-stream structured graph convolution network ( DS-SGCN ) to solve the skeleton-based action recognition problem. The spatio-temporal coordinates and appearance contexts of the skeletal joints are jointly integrated into the graph convolution learning process on both the video and skeleton modalities. To effectively represent the skeletal graph of discrete joints, we create a structured graph convolution module specifically designed to encode partitioned body parts along with their dynamic interactions in the spatio-temporal sequence. In more detail, we build a set of structured intra-part graphs, each of which can be adopted to represent a distinctive body part (e.g., left arm, right leg, head). The inter-part graph is then constructed to model the dynamic interactions across different body parts; here each node corresponds to an intra-part graph built above, while an edge between two nodes is used to express these internal relationships of human movement. We implement the graph convolution learning on both intra- and inter-part graphs in order to obtain the inherent characteristics and dynamic interactions, respectively, of human action. After integrating the intra- and inter-levels of spatial context/coordinate cues, a convolution filtering process is conducted on time slices to capture these temporal dynamics of human motion. Finally, we fuse two streams of graph convolution responses in order to predict the category information of human action in an end-to-end fashion. Comprehensive experiments on five single/multi-modal benchmark datasets (including NTU RGB+D 60, NTU RGB+D 120, MSR-Daily 3D, N-UCLA, and HDM05) demonstrate that the proposed DS-SGCN framework achieves encouraging performance on the skeleton-based action recognition task.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W3214094165",
    "type": "article"
  },
  {
    "title": "Y-Net: Dual-branch Joint Network for Semantic Segmentation",
    "doi": "https://doi.org/10.1145/3460940",
    "publication_date": "2021-11-12",
    "publication_year": 2021,
    "authors": "Yizhen Chen; Haifeng Hu",
    "corresponding_authors": "",
    "abstract": "Most existing segmentation networks are built upon a “ U -shaped” encoder–decoder structure, where the multi-level features extracted by the encoder are gradually aggregated by the decoder. Although this structure has been proven to be effective in improving segmentation performance, there are two main drawbacks. On the one hand, the introduction of low-level features brings a significant increase in calculations without an obvious performance gain. On the other hand, general strategies of feature aggregation such as addition and concatenation fuse features without considering the usefulness of each feature vector, which mixes the useful information with massive noises. In this article, we abandon the traditional “ U -shaped” architecture and propose Y-Net, a dual-branch joint network for accurate semantic segmentation. Specifically, it only aggregates the high-level features with low-resolution and utilizes the global context guidance generated by the first branch to refine the second branch. The dual branches are effectively connected through a Semantic Enhancing Module, which can be regarded as the combination of spatial attention and channel attention. We also design a novel Channel-Selective Decoder (CSD) to adaptively integrate features from different receptive fields by assigning specific channelwise weights, where the weights are input-dependent. Our Y-Net is capable of breaking through the limit of singe-branch network and attaining higher performance with less computational cost than “ U -shaped” structure. The proposed CSD can better integrate useful information and suppress interference noises. Comprehensive experiments are carried out on three public datasets to evaluate the effectiveness of our method. Eventually, our Y-Net achieves state-of-the-art performance on PASCAL VOC 2012, PASCAL Person-Part, and ADE20K dataset without pre-training on extra datasets.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W3214262346",
    "type": "article"
  },
  {
    "title": "Bandwidth adaptation for 3D mesh preview streaming",
    "doi": "https://doi.org/10.1145/2537854",
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "Shanghong Zhao; Wei Tsang Ooi; Axel Carlier; Géraldine Morin; Vincent Charvillat",
    "corresponding_authors": "",
    "abstract": "Online galleries of 3D models typically provide two ways to preview a model before the model is downloaded and viewed by the user: (i) by showing a set of thumbnail images of the 3D model taken from representative views (or keyviews); (ii) by showing a video of the 3D model as viewed from a moving virtual camera along a path determined by the content provider. We propose a third approach called preview streaming for mesh-based 3D objects: by streaming and showing parts of the mesh surfaces visible along the virtual camera path. This article focuses on the preview streaming architecture and framework and presents our investigation into how such a system would best handle network congestion effectively. We present three basic methods: (a) stop-and-wait , where the camera pauses until sufficient data is buffered; (b) reduce-speed , where the camera slows down in accordance to reduce network bandwidth; and (c) reduce-quality , where the camera continues to move at the same speed but fewer vertices are sent and displayed, leading to lower mesh quality. We further propose two advanced methods: (d) keyview-aware , which trades off mesh quality and camera speed appropriately depending on how close the current view is to the keyviews, and (e) adaptive-zoom , which improves visual quality by moving the virtual camera away from the original path. A user study reveals that our keyview-aware method is preferred over the basic methods. Moreover, the adaptive-zoom scheme compares favorably to the keyview-aware method, showing that path adaptation is a viable approach to handling bandwidth variation.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W1993714043",
    "type": "article"
  },
  {
    "title": "3DTI Amphitheater",
    "doi": "https://doi.org/10.1145/2700297",
    "publication_date": "2015-02-24",
    "publication_year": 2015,
    "authors": "Shannon Chen; Zhenhuan Gao; Klara Nahrstedt; Indranil Gupta",
    "corresponding_authors": "",
    "abstract": "3DTI Amphitheater is a live broadcasting system for dissemination of 3DTI (3D Tele-immersive) content. The virtual environment constructed by the system mimics an amphitheater in the real world, where performers interact with each other in the central circular stage, and the audience is placed in virtual seats that surround the stage. Users of the Amphitheater can be geographically dispersed and the streams created by the performer sites are disseminated in a P2P network among the participants. To deal with the high bandwidth demand and strict latency bound of the service, we identify the hierarchical priority of streams in construction of the content dissemination forest. Result shows that the Amphitheater outperforms prior 3DTI systems by boosting the application QoS by a factor of 2.8 while sustaining the same hundred-scale audience group.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W1999634590",
    "type": "article"
  },
  {
    "title": "QuGu",
    "doi": "https://doi.org/10.1145/2725469",
    "publication_date": "2015-06-02",
    "publication_year": 2015,
    "authors": "Yang Li; Azzedine Boukerche",
    "corresponding_authors": "",
    "abstract": "Video dissemination over Vehicular Ad Hoc Networks is an attractive technology that supports many novel applications. The merit of this work lies in the design of an efficient video dissemination protocol that provides high video quality at different data rates for urban scenarios. Our objective is to improve received video quality while meeting delay and packet loss. In this work, we first employ a reliable scheme known as connected dominating set, which is an efficient receiver-based routing scheme for broadcasting video content. To avoid repeated computing of the connected dominating set, we add three statuses to each node. In nonscalable video coding, the distribution of lost frames can cause a major impact on video quality at the receiver's end. Therefore, for the second step, we employ Interleaving to spread out the burst losses and to reduce the influence of loss distributions. Although Interleaving can reduce the influence of cluster frame loss, single packet loss is also a concern due to collisions, and to intermittent disconnection in the topology. In order to fix these single packet losses, we propose a store-carry-forward scheme for the nodes in order to retransmit the local buffer stored packets. The results, when compared to the selected base protocols, show that our proposed protocol is an efficient solution for video dissemination over urban Vehicular Ad Hoc Networks.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2203905818",
    "type": "article"
  },
  {
    "title": "Video Management and Resource Allocation for a Large-Scale VoD Cloud",
    "doi": "https://doi.org/10.1145/2983638",
    "publication_date": "2016-09-21",
    "publication_year": 2016,
    "authors": "Zhangyu Chang; S.-H. Gary Chan",
    "corresponding_authors": "",
    "abstract": "We consider providing large-scale Netflix-like video-on-demand (VoD) service on a cloud platform, where cloud proxy servers are placed close to user pools. Videos may have heterogeneous popularity at different geo-locations. A repository provides video backup for the network, and the proxy servers collaboratively store and stream videos. To deploy the VoD cloud, the content provider rents resources consisting of link capacities among servers, server storage, and server processing capacity to handle remote requests. We study how to minimize the deployment cost by jointly optimizing video management (in terms of video placement and retrieval at servers) and resource allocation (in terms of link, storage, and processing capacities), subject to a certain user delay requirement on video access. We first formulate the joint optimization problem and show that it is NP-hard. To address it, we propose Resource allocation And Video management Optimization (RAVO), a novel and efficient algorithm based on linear programming with proven optimality gap. For a large video pool, we propose a video clustering algorithm to substantially reduce the run-time computational complexity without compromising performance. Using extensive simulation and trace-driven real data, we show that RAVO achieves close-to-optimal performance, outperforming other advanced schemes significantly (often by multiple times).",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2522378242",
    "type": "article"
  },
  {
    "title": "A Content-Aware Video Adaptation Service to Support Mobile Video",
    "doi": "https://doi.org/10.1145/2983636",
    "publication_date": "2016-11-08",
    "publication_year": 2016,
    "authors": "Stefan Wilk; Denny Stohr; Wolfgang Effelsberg",
    "corresponding_authors": "",
    "abstract": "Adaptive video streaming systems rely on the availability of different quality versions of a video. Such a system can dynamically adjust the quality of a video stream during its playback depending on the available network throughput. Even if the necessary throughput is available, mobile users can benefit from limiting the generated data traffic as most cellular network contracts have data caps. Usually, if the cap is reached, the throughput is throttled to a speed that does not allow video streaming. Existing systems react to varying network conditions but often neglect content-specific adaptation needs. Content inspection can help to save data traffic when a higher bitrate representation would not increase the perceived quality. In this work, we present the Video Adaptation Service (VAS), a support service for a content-aware video adaptation for mobile devices. Based on the video content, the adaptation process is improved for both the available network resources and the perception of the user. By leveraging the content properties of a video stream, the system is able to maintain a stable video quality and at the same time reduce the generated data traffic. The system is evaluated with different adaptation schemes and shows that content-specific adaptation can both increase the perceived quality as well as reduce the data traffic. Additionally, we demonstrate the practical feasibility of this approach by integrating the VAS into Dynamic Adaptive Streaming over the Hypertext Transfer Protocol.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2527743668",
    "type": "article"
  },
  {
    "title": "Consistent Synchronization of Action Order with Least Noticeable Delays in Fast-Paced Multiplayer Online Games",
    "doi": "https://doi.org/10.1145/3003727",
    "publication_date": "2016-12-16",
    "publication_year": 2016,
    "authors": "Jingxi Xu; Benjamin W. Wah",
    "corresponding_authors": "",
    "abstract": "When running multiplayer online games on IP networks with losses and delays, the order of actions may be changed when compared to the order run on an ideal network with no delays and losses. To maintain a proper ordering of events, traditional approaches either use rollbacks to undo certain actions or local lags to introduce additional delays. Both may be perceived by players because their changes are beyond the just-noticeable-difference (JND) threshold. In this article, we propose a novel method for ensuring a strongly consistent completion order of actions, where strong consistency refers to the same completion order as well as the same interval between any completion time and the corresponding ideal reference completion time under no network delay. We find that small adjustments within the JND on the duration of an action would not be perceivable, as long as the duration is comparable to the network round-trip time. We utilize this property to control the vector of durations of actions and formulate the search of the vector as a multidimensional optimization problem. By using the property that players are generally more sensitive to the most prominent delay effect (with the highest probability of noticeability P notice or the probability of correctly noticing a change when compared to the reference), we prove that the optimal solution occurs when P notice of the individual adjustments are equal. As this search can be done efficiently in polynomial time ( ∼ 5ms) with a small amount of space ( ∼ 160KB), the search can be done at runtime to determine the optimal control. Last, we evaluate our approach on the popular open-source online shooting game BZFlag.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2564416777",
    "type": "article"
  },
  {
    "title": "Fine-grained Human Analysis under Occlusions and Perspective Constraints in Multimedia Surveillance",
    "doi": "https://doi.org/10.1145/3476839",
    "publication_date": "2022-01-25",
    "publication_year": 2022,
    "authors": "Rita Cucchiara; Matteo Fabbri",
    "corresponding_authors": "",
    "abstract": "Human detection in the wild is a research topic of paramount importance in computer vision, and it is the starting step for designing intelligent systems oriented to human interaction that work in complete autonomy. To achieve this goal, computer vision and machine learning should aim at superhuman capabilities. In this work, we address the problem of fine-grained human analysis under occlusions and perspective constraints. More specifically, we discuss some issues and some possible solutions to effectively detect people using pose estimation methods and to detect humans under occlusions both in the two-dimensional (2D) image plane and in the 3D space exploiting single monocular cameras. Dealing with occlusion can be done at the joint level or pixel level: We discuss two different solutions, the former based on a supervised neural network architecture for detecting occluded joints and the latter based on a semi-supervised specialized GAN that exploits both appearance and human shape attributes to determine the missing parts of the visible shape. To deal with perspective constraints, we further discuss a neural approach based on a double architecture that learns to create an optimal neural representation, which is useful to reconstruct the 3D position of human keypoints starting with simple RGB images. All these approaches have a critical point in common: the need for large annotated datasets. To have large, fair, consistent, transparent, and ethical datasets, we propose the adoption of synthetic datasets as, for example, JTA and MOTSynth. In this article, we discuss the pros and cons of using synthetic datasets while tackling several human-centered AI issues with respect to European GDPR rules for privacy. We further explore and discuss an application in the field of risk assessment by space occupancy estimation during the COVID-19 pandemic called Inter-Homines.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4206910645",
    "type": "article"
  },
  {
    "title": "Modeling the User Experience of Watching 360° Videos with Head-Mounted Displays",
    "doi": "https://doi.org/10.1145/3463825",
    "publication_date": "2022-01-27",
    "publication_year": 2022,
    "authors": "Ching-Ling Fan; Tse-Hou Hung; Cheng-Hsin Hsu",
    "corresponding_authors": "",
    "abstract": "Conducting user studies to quantify the Quality of Experience (QoE) of watching the increasingly more popular 360° videos in Head-Mounted Displays (HMDs) is time-consuming, tedious, and expensive. Deriving QoE models, however, is very challenging because of the diverse viewing behaviors and complex QoE features and factors. In this article, we compile a wide spectrum of QoE features and factors that may contribute to the overall QoE. We design and conduct a user study to build a dataset of the overall QoE, QoE features, and QoE factors. Using the dataset, we derive the QoE models for both the Mean Opinion Score (MOS) and Individual Score (IS), where MOS captures the aggregated QoE across all subjects, while IS captures the QoE of individual subjects. Our derived overall QoE models achieve 0.98 and 0.91 in Pearson’s Linear Correlation Coefficient (PLCC) for MOS and IS, respectively. Besides, we make several new observations on our user study results, such as (1) content factors dominate the overall QoE across all factor categories, (2) Video Multi-Method Assessment Fusion (VMAF) is the dominating factor among content factors, and (3) the perceived cybersickness is affected by human factors more among others. Our proposed user study design is useful for QoE modeling (specifically) and subjective evaluations (in general) of emerging 360° tiled video streaming to HMDs.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4210509783",
    "type": "article"
  },
  {
    "title": "Semantic Guided Single Image Reflection Removal",
    "doi": "https://doi.org/10.1145/3510821",
    "publication_date": "2022-02-18",
    "publication_year": 2022,
    "authors": "Yunfei Liu; Yu Li; Shaodi You; Feng Lu",
    "corresponding_authors": "",
    "abstract": "Reflection is common when we see through a glass window, which not only is a visual disturbance but also influences the performance of computer vision algorithms. Removing the reflection from a single image, however, is highly ill-posed since the color at each pixel needs to be separated into two values belonging to the clear background and the reflection, respectively. To solve this, existing methods use additional priors such as reflection layer smoothness, double reflection effect, and color consistency to distinguish the two layers. However, these low-level priors may not be consistently valid in real cases. In this paper, inspired by the fact that human beings can separate the two layers easily by recognizing the objects and understanding the scene, we propose to use the object semantic cue, which is high-level information, as the guidance to help reflection removal. Based on the data analysis, we develop a multi-task end-to-end deep learning method with a semantic guidance component, to solve reflection removal and semantic segmentation jointly. Extensive experiments on different datasets show significant performance gain when using high-level object-oriented information. We also demonstrate the application of our method to other computer vision tasks.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4213259866",
    "type": "article"
  },
  {
    "title": "Multilayer Video Encoding for QoS Managing of Video Streaming in VANET Environment",
    "doi": "https://doi.org/10.1145/3491433",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Bechir Alaya; Lamaa Sellami",
    "corresponding_authors": "",
    "abstract": "Efficient delivery and maintenance of the quality of service (QoS) of audio/video streams transmitted over VANETs for mobile and heterogeneous nodes are one of the major challenges in the convergence of this network type and these services. In this context, we propose an inter-layer approach for multimedia stream transmission in a VANET environment (VSMENET). The main idea of our work is based on the dynamic adaptation of the transmission rate according to the physical rate available in the VANET. VSMENET is all about eliminating downtime during video playback by vehicle users. This involves adapting the quality of the video to the actual performance of the VANETs, intelligent encoding of video on the Road Side Units (RSU) side, and finally continuous maintenance of the calculation tasks on the RSU side and sufficient video data on the vehicle node side. Thus, we are interested in the process of evaluating the strict parameters of the VANETs, influencing the video transmission. For example, we propose, on the one hand, an architecture for intelligent data selection and good clock synchronization, and, on the other hand, efficient management of the availability and consumption of video data. We used the NetSim simulator to test the proposed approach performance. To this end, several algorithms such as OCLFEC, MAC, ShieldHEVC, and AntArmour have been implemented for such a performance comparison. Our work suggests that VSMENET is well concerning the average lifetime of the video packets and their delivery rate (more than 9% gain compared with other approaches).",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4214829753",
    "type": "article"
  },
  {
    "title": "Improving Crowd Density Estimation by Fusing Aerial Images and Radio Signals",
    "doi": "https://doi.org/10.1145/3492346",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Kai‐Wei Yang; Yen-Yun Huang; Jen-Wei Huang; Ya-Rou Hsu; Changlin Wan; Hong-Han Shuai; Li‐Chun Wang; Wen-Huang Cheng",
    "corresponding_authors": "",
    "abstract": "A recent line of research focuses on crowd density estimation from RGB images for a variety of applications, for example, surveillance and traffic flow control. The performance drops dramatically for low-quality images, such as occlusion, or poor light conditions. However, people are equipped with various wireless devices, allowing the received signals to be easily collected at the base station. As such, another line of research utilizes received signals for crowd counting. Nevertheless, received signals offer only information regarding the number of people, while an accurate density map cannot be derived. As unmanned aerial vehicles (UAVs) are now treated as flying base stations and equipped with cameras, we make the first attempt to leverage both RGB images and received signals for crowd density estimation on UAVs. Specifically, we propose a novel network to effectively fuse the RGB images and received signal strength (RSS) information. Moreover, we design a new loss function that considers the uncertainty from RSS and makes the prediction consistent with the received signals. Experimental results show that the proposed method successfully helps break the limit of traditional crowd density estimation methods and achieves state-of-the-art performance. The proposed dataset is released as a public download for future research.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4214863169",
    "type": "article"
  },
  {
    "title": "A Spatial Relationship Preserving Adversarial Network for 3D Reconstruction from a Single Depth View",
    "doi": "https://doi.org/10.1145/3506733",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Caixia Liu; Dehui Kong; Shaofan Wang; Jinghua Li; Baocai Yin",
    "corresponding_authors": "",
    "abstract": "Recovering the geometry of an object from a single depth image is an interesting yet challenging problem. While previous learning based approaches have demonstrated promising performance, they don’t fully explore spatial relationships of objects, which leads to unfaithful and incomplete 3D reconstruction. To address these issues, we propose a Spatial Relationship Preserving Adversarial Network (SRPAN) consisting of 3D Capsule Attention Generative Adversarial Network (3DCAGAN) and 2D Generative Adversarial Network (2DGAN) for coarse-to-fine 3D reconstruction from a single depth view of an object. Firstly, 3DCAGAN predicts the coarse geometry using an encoder-decoder based generator and a discriminator. The generator encodes the input as latent capsules represented as stacked activity vectors with local-to-global relationships (i.e., the contribution of components to the whole shape), and then decodes the capsules by modeling local-to-local relationships (i.e., the relationships among components) in an attention mechanism. Afterwards, 2DGAN refines the local geometry slice-by-slice, by using a generator learning a global structure prior as guidance, and stacked discriminators enforcing local geometric constraints. Experimental results show that SRPAN not only outperforms several state-of-the-art methods by a large margin on both synthetic datasets and real-world datasets, but also reconstructs unseen object categories with a higher accuracy.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4214926633",
    "type": "article"
  },
  {
    "title": "Hiding Message Using a Cycle Generative Adversarial Network",
    "doi": "https://doi.org/10.1145/3495566",
    "publication_date": "2022-03-12",
    "publication_year": 2022,
    "authors": "Wuzhen Shi; Shaohui Liu",
    "corresponding_authors": "",
    "abstract": "Training an image steganography is an unsupervised problem, because it is impossible to obtain an ideal supervised steganographic image corresponding to the cover image and secret message. Inspired by the success of cycle generative adversarial networks in unsupervised tasks such as style transfer, this article proposes to use a cycle generative adversarial network to solve the problem of unsupervised image steganography. Specifically, this article jointly trains five networks, i.e., a steganographic network, an inverse steganographic network, a hidden message reconstruction network, and two discriminative networks, which together constitute a hidden message cycle generative adversarial network (HCGAN). Compared with the recent image steganography based on generative adversative network, HCGAN provides more accurate supervised information, which makes the training process of HCGAN converge faster and the performance of the trained image steganography network is better. In addition, this article introduces an image steganographic network based on residual learning and shows that residual learning can effectively improve the performance of steganography. Furthermore, to the best of our knowledge, we are the first to propose an inverse steganographic network for eliminating steganographic message from steganographic images, which can be used to avoid steganographic message being discovered or acquired by a third party. The experimental results show that compared with the steganography based on generative adversarial network, the proposed HCGAN has a higher correct decoding rate, better visual quality of steganographic image, and higher secrecy.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4220920063",
    "type": "article"
  },
  {
    "title": "Boosting Hyperspectral Image Classification with Dual Hierarchical Learning",
    "doi": "https://doi.org/10.1145/3522713",
    "publication_date": "2022-03-12",
    "publication_year": 2022,
    "authors": "Shuo Wang; Huixia Ben; Yanbin Hao; Xiangnan He; Meng Wang",
    "corresponding_authors": "",
    "abstract": "Hyperspectral image (HSI) classification aims at predicting the pixel-wise labels in an image, where there are only a few labeled pixel samples (hard labels) for training. It is a challenging task since the classification process is susceptible to over-fitting under training with limited samples. To relieve this problem, we propose a method based on dual hierarchical learning. First, we employ a connectionist hyperspectral convolution (HC) network to capture the representations of the pixels from different receptive fields. Specifically, an HC is designed to learn the correlation among adjacent pixels and is further extended to a connectionist hierarchical structure. These operations use the correlation to enhance one-pixel learning from multiple receptive fields. Second, we analyze the properties in the hyperspectral image and introduce a hierarchical pseudo label generation algorithm to enrich the supervision of the label information. Finally, we design a dual hierarchical learning strategy to help all HC layers learn from both the hard labels and the hierarchical pseudo labels. In other words, it addresses the HSI classification problem from different views. For inference, we employ two fusion strategies to find a better prediction. The experimental results on four popular HSI benchmarks, i.e., Salinas-A, IndianPines, PaviaU, and PaviaC, demonstrate the effectiveness of the proposed method. Our code is publicly available on GitHub: https://github.com/ShuoWangCS/HSI-DHL.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4220978858",
    "type": "article"
  },
  {
    "title": "Deep Inter Prediction with Error-Corrected Auto-Regressive Network for Video Coding",
    "doi": "https://doi.org/10.1145/3528173",
    "publication_date": "2022-04-01",
    "publication_year": 2022,
    "authors": "Yuzhang Hu; Wenhan Yang; Jiaying Liu; Zongming Guo",
    "corresponding_authors": "",
    "abstract": "Modern codecs remove temporal redundancy of a video via inter prediction, i.e., searching previously coded frames for similar blocks and storing motion vectors to save bit-rates. However, existing codecs adopt block-level motion estimation, where a block is regressed by reference blocks linearly and is doomed to fail to deal with non-linear motions. In this article, we generate virtual reference frames (VRFs) with previously reconstructed frames via deep networks to offer an additional candidate, which is not constrained to linear motion structure and further significantly improves coding efficiency. More specifically, we propose a novel deep Auto-Regressive Moving-Average (ARMA) model, Error-Corrected Auto-Regressive Network (ECAR-Net), equipped with the powers of the conventional statistic ARMA models and deep networks jointly for reference frame prediction. Similar to conventional ARMA models, the ECAR-Net consists of two stages: Auto-Regression (AR) stage and Error-Correction (EC) stage, where the first part predicts the signal at the current time-step based on previously reconstructed frames, while the second one compensates for the output of the AR stage to obtain finer details. Different from the statistic AR models only focusing on short-term temporal dependency, the AR model of our ECAR-Net is further injected with the long-term dynamics mechanism, where long temporal information is utilized to help predict motions more accurately. Furthermore, ECAR-Net works in a configuration-adaptive way, i.e., using different dynamics and error definitions for the Low Delay B and Random Access configurations, which helps improve the adaptivity and generality in diverse coding scenarios. With the well-designed network, our method surpasses HEVC on average 5.0% and 6.6% BD-rate saving for the luma component under the Low Delay B and Random Access configurations and also obtains on average 1.54% BD-rate saving over VVC. Furthermore, ECAR-Net works in a configuration-adaptive way, i.e., using different dynamics and error definitions for the Low Delay B and Random Access configurations, which helps improve the adaptivity and generality in diverse coding scenarios.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4220997001",
    "type": "article"
  },
  {
    "title": "Discriminative Visual Similarity Search with Semantically Cycle-consistent Hashing Networks",
    "doi": "https://doi.org/10.1145/3532519",
    "publication_date": "2022-04-20",
    "publication_year": 2022,
    "authors": "Zheng Zhang; Jianing Wang; Lei Zhu; Guangming Lu",
    "corresponding_authors": "",
    "abstract": "Deep hashing has great potential in large-scale visual similarity search due to its preferable efficiency in storage and computation. Technically, deep hashing for visual similarity search inherits the powerful representation capability of deep neural networks, and it encodes visual features into compact binary codes by preserving representative semantic visual features. Works in this field mainly focus on building the relationship between the visual and objective hash spaces, while they seldom study the triadic cross-domain semantic knowledge transfer among visual, semantic, and hashing spaces, leading to a serious semantic ignorance problem during space transformation. In this article, we propose a novel deep tripartite semantically interactive hashing framework, dubbed Semantically Cycle-consistent Hashing Networks (SCHNs), for discriminative hash code learning. Particularly, we construct a flexible semantic space and a transitive latent space, in conjunction with the visual space, to jointly deduce the privileged discriminative hash space. Specifically, a new semantic space is conceived to strengthen the flexibility and completeness of categories in the semantic feature inference phase. At the same time, a transitive latent space is formulated to explore and uncover the shared semantic interactivity embedded in visual and semantic features. Moreover, to further ensure semantic consistency across multiple spaces, we propose to build a cyclic adversarial learning module to preserve and keep their semantic concurrence during space transformation. Notably, our SCHN, for the first time, establishes the cyclic principle of deep semantic-preserving hashing by adaptive semantic parsing across different spaces in a single-modal visual similarity search. In addition, the entire learning framework is jointly optimized in an end-to-end manner. Extensive experiments performed on diverse large-scale datasets evidence the superiority of our method against other state-of-the-art deep hashing algorithms. The source codes of this article are available at https://github.com/JalinWang/SCHN.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4224301727",
    "type": "article"
  },
  {
    "title": "Facial-expression-aware Emotional Color Transfer Based on Convolutional Neural Network",
    "doi": "https://doi.org/10.1145/3464382",
    "publication_date": "2022-01-27",
    "publication_year": 2022,
    "authors": "Shiguang Liu; Huixin Wang; Min Pei",
    "corresponding_authors": "",
    "abstract": "Emotional color transfer aims to change the evoked emotion of a source image to that of a target image by adjusting color distribution. Most of existing emotional color transfer methods only consider the low-level visual features of an image and ignore the facial expression features when the image contains a human face, which would cause incorrect emotion evaluation for the given image. In addition, previous emotional color transfer methods may easily result in ambiguity between the emotion of resulting image and target image. For example, if the background of the target image is dark while the facial expression is happiness, then previous methods would directly transfer dark color to the source image, neglecting the facial emotion in the image. To solve this problem, we propose a new facial-expression-aware emotional color transfer framework. Given a target image with facial expression features, we first predict the facial emotion label of the image through the emotion classification network. Then, facial emotion labels are matched with pre-trained emotional color transfer models. Finally, we use the matched emotion model to transfer the color of the target image to the source image. Considering none of the existing emotion image databases, which focus on images that contain face and background, we built an emotion database for our new emotional color transfer framework that is called “Face-Emotion database.” Experiments demonstrate that our method can successfully capture and transfer facial emotions, outperforming state-of-the-art methods.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4226169731",
    "type": "article"
  },
  {
    "title": "A Low Distortion and Steganalysis-resistant Reversible Data Hiding for 2D Engineering Graphics",
    "doi": "https://doi.org/10.1145/3539661",
    "publication_date": "2022-06-08",
    "publication_year": 2022,
    "authors": "Fei Peng; Jiang Wen-yan; Min Long",
    "corresponding_authors": "",
    "abstract": "To reduce the distortion resulting from the large number of crossing quantization cells and resist steganalysis, a reversible data hiding scheme for 2D engineering graphics is put forward based on reversible dual-direction quantization index modulation (RDQIM). The quantization cell index of the host data is first computed, and its distances to the embedding cells in both the left and the right directions are calculated. After that, the data hiding is performed by modifying the data to the nearest embedding cell. To guarantee the reversibility, each quantization cell is further subdivided into three sub-cells, and the source quantization interval of the host data is marked by the index of the located sub-cell. The data extraction is accomplished by calculating the index of the quantization cell where the stego data is in. Meanwhile, the lossless recovery of the stego data is realized by combining the index of the located sub-cell and the relative distance within the sub-cell. Besides, different embedding strategies are adopted for different types of entities to achieve steganalysis-resistant ability. Experimental results and analysis show that the proposed scheme can strike a good balance among imperceptibility, semi-fragility, and steganalysis-resistant ability. Moreover, under the same conditions, the average imperceptibility and the average capacity are, respectively, improved by at least 7.487% and 41.045% compared with the existing methods.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4281606855",
    "type": "article"
  },
  {
    "title": "Machine Learning Based Content-Agnostic Viewport Prediction for 360-Degree Video",
    "doi": "https://doi.org/10.1145/3474833",
    "publication_date": "2022-02-16",
    "publication_year": 2022,
    "authors": "Sam Van Damme; Maria Torres Vega; Filip De Turck",
    "corresponding_authors": "",
    "abstract": "Accurate and fast estimations or predictions of the (near) future location of the users of head-mounted devices within the virtual omnidirectional environment open a plethora of opportunities in application domains such as interactive immersive gaming and tele-surgery. Therefore, the past years have seen growing attention to models for viewport prediction in 360֯ environments. Among the approaches, content-agnostic, trajectory-based methods have the potential to provide very fast solutions, as they do not require complex analysis of the videos to provide a prediction. However, accurate trajectory-based viewport prediction is rather difficult due to the intrinsic variability in user behaviour. Furthermore, even when making use of machine learning, current approaches tend to be brute-force and heavily tailored to specific datasets with little comparison to existing benchmarks or publicly available studies. This article presents a generic, content-agnostic viewport prediction method consisting of a window-based approach combined with a preprocessing system to classify behavioural patterns in terms of user clustering and trajectory correlation. Moreover, as the state of the art does not provide a comparative analysis of different approaches, this work contributes to this. Based on the obtained results, a combined prediction model is proposed and evaluated. Our method shows a 36.8% to 53.9% improvement when compared to the static prediction baseline for a prediction horizon of 8 seconds. In addition, a 11.5% to 24.0% improvement to a brute-force machine learning prediction approach is obtained. As such, this work contributes towards the creation of more generic and structured solutions for content-agnostic viewport prediction in terms of data representation, preprocessing and modelling.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4283375183",
    "type": "article"
  },
  {
    "title": "<i> L <sup>2</sup> BEC <sup>2</sup> </i> : Local Lightweight Bidirectional Encoding and Channel Attention Cascade for Video Frame Interpolation",
    "doi": "https://doi.org/10.1145/3547660",
    "publication_date": "2022-07-15",
    "publication_year": 2022,
    "authors": "Dengyong Zhang; Pu Huang; Xiangling Ding; Feng Li; Wenjie Zhu; Yun Song; Gaobo Yang",
    "corresponding_authors": "",
    "abstract": "Video frame interpolation (VFI) is of great importance for many video applications, yet it is still challenging even in the era of deep learning. Some existing VFI models directly exploit existing lightweight network frameworks, thus making synthesized in-between frames blurry and creating artifacts due to imprecise motion representation. The other existing VFI models typically depend on heavy model architectures with a large number of parameters, preventing them from being deployed on small terminals. To address these issues, we propose a local lightweight VFI network ( L 2 BEC 2 ) that leverages bidirectional encoding structure with channel attention cascade. Specifically, we improve visual quality by introducing a forward and backward encoding structure with channel attention cascade to better characterize motion information. Furthermore, we introduce a local lightweight strategy into the state-of-the-art Adaptive Collaboration of Flows (AdaCoF) model to simplify its model parameters. Compared with the original AdaCoF model, the proposed L 2 BEC 2 obtains performance gain at the cost of only one-third of the number of parameters and performs favorably against the state-of-the-art works on public datasets. Our source code is available at https://github.com/Pumpkin123709/LBEC.git .",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4285591283",
    "type": "article"
  },
  {
    "title": "A Large-Scale Synthetic Gait Dataset Towards in-the-Wild Simulation and Comparison Study",
    "doi": "https://doi.org/10.1145/3517199",
    "publication_date": "2022-07-21",
    "publication_year": 2022,
    "authors": "Pengyi Zhang; Huanzhang Dou; Wenhu Zhang; Yuhan Zhao; Zequn Qin; Dongping Hu; Yi Fang; Xi Li",
    "corresponding_authors": "",
    "abstract": "Gait recognition has a rapid development in recent years. However, current gait recognition focuses primarily on ideal laboratory scenes, leaving the gait in the wild unexplored. One of the main reasons is the difficulty of collecting in-the-wild gait datasets, which must ensure diversity of both intrinsic and extrinsic human gait factors. To remedy this problem, we propose to construct a large-scale gait dataset with the help of controllable computer simulation. In detail, to diversify the intrinsic factors of gait, we generate numerous characters with diverse attributes and associate them with various types of walking styles. To diversify the extrinsic factors of gait, we build a complicated scene with a dense camera layout. Then we design an automatic generation toolkit under Unity3D for simulating the walking scenarios and capturing the gait data. As a result, we obtain a dataset simulating towards the in-the-wild scenario, called VersatileGait, which has more than one million silhouette sequences of 10,000 subjects with diverse scenarios. VersatileGait possesses several nice properties, including huge dataset size, diverse pedestrian attributes, complicated camera layout, high-quality annotations, small domain gap with the real one, good scalability for new demands, and no privacy issues. By conducting a series of experiments, we first explore the effects of different factors on gait recognition. We further illustrate the effectiveness of using our dataset to pre-train models, which obtain considerable performance gain on CASIA-B, OU-MVLP, and CASIA-E. Besides, we show the great potential of the fine-grained labels other than the ID label in improving the efficiency and effectiveness of models. Our dataset and its corresponding generation toolkit are available at https://github.com/peterzpy/VersatileGait.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4286492853",
    "type": "article"
  },
  {
    "title": "Deep Saliency Mapping for 3D Meshes and Applications",
    "doi": "https://doi.org/10.1145/3550073",
    "publication_date": "2022-07-27",
    "publication_year": 2022,
    "authors": "Stavros Nousias; Gerasimos Arvanitis; Aris S. Lalos; Κωνσταντίνος Μουστάκας",
    "corresponding_authors": "",
    "abstract": "Nowadays, three-dimensional (3D) meshes are widely used in various applications in different areas (e.g., industry, education, entertainment and safety). The 3D models are captured with multiple RGB-D sensors, and the sampled geometric manifolds are processed, compressed, simplified, stored, and transmitted to be reconstructed in a virtual space. These low-level processing applications require the accurate representation of the 3D models that can be achieved through saliency estimation mechanisms that identify specific areas of the 3D model representing surface patches of importance. Therefore, saliency maps guide the selection of feature locations facilitating the prioritization of 3D manifold segments and attributing to vertices more bits during compression or lower decimation probability during simplification, since compression and simplification are counterparts of the same process. In this work, we present a novel deep saliency mapping approach applied to 3D meshes, emphasizing decreasing the execution time of the saliency map estimation, especially when compared with the corresponding time by other relevant approaches. Our method utilizes baseline 3D importance maps to train convolutional neural networks. Furthermore, we present applications that utilize the extracted saliency, namely feature-aware multiscale compression and simplification frameworks.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4288050560",
    "type": "article"
  },
  {
    "title": "Query-Guided Prototype Learning with Decoder Alignment and Dynamic Fusion in Few-Shot Segmentation",
    "doi": "https://doi.org/10.1145/3555314",
    "publication_date": "2022-08-12",
    "publication_year": 2022,
    "authors": "Yiming Tang; Yi Yu",
    "corresponding_authors": "",
    "abstract": "Few-shot segmentation aims to segment objects belonging to a specific class under the guidance of a few annotated examples. Most existing approaches follow the prototype learning paradigm and generate category prototypes by squeezing masked feature maps extracted from images in the support set. These support prototypes may lead to inaccurate predictions when directly compared with features extracted from the query set due to the considerable distribution discrepancy between support and query features. We propose a query-guided prototype learning architecture to address this problem from two aspects: (i) We propose a cross-alignment loss for training the segmentation decoder. This loss function will help the decoder improve its robustness against the distribution discrepancy between support and query features. (ii) We build a dynamic fusion module to strengthen the original support prototype with another prototype extracted from query features. Experiments show that our method achieves promising results compared to previous prototype learning methods on PASCAL-5 i and COCO-20 i datasets.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4291144033",
    "type": "article"
  },
  {
    "title": "A Deep Graph Network with Multiple Similarity for User Clustering in Human–Computer Interaction",
    "doi": "https://doi.org/10.1145/3549954",
    "publication_date": "2022-08-24",
    "publication_year": 2022,
    "authors": "Yan Kang; Bin Pu; Yongqi Kou; Yun Yang; Jianguo Chen; Khan Muhammad; Po Yang; Lida Xu; Mohammad Hijji",
    "corresponding_authors": "",
    "abstract": "User counterparts, such as user attributes in social networks or user interests, are the keys to more natural Human–Computer Interaction (HCI) . In addition, users’ attributes and social structures help us understand the complex interactions in HCI. Most previous studies have been based on supervised learning to improve the performance of HCI. However, in the real world, owing to signal malfunctions in user devices, large amounts of abnormal information, unlabeled data, and unsupervised approaches (e.g., the clustering method) based on mining user attributes are particularly crucial. This paper focuses on improving the clustering performance of users’ attributes in HCI and proposes a deep graph embedding network with feature and structure similarity (called DGENFS ) to cluster users’ attributes in HCI applications based on feature and structure similarity. The DGENFS model consists of a Feature Graph Autoencoder (FGA) module, a Structure Graph Attention Network (SGAT) module, and a Dual Self-supervision (DSS) module. First, we design an attributed graph clustering method to divide users into clusters by making full use of their attributes. To take full advantage of the information of human feature space, a k-neighbor graph is generated as a feature graph based on the similarity between human features. Then, the FGA and SGAT modules are utilized to extract the representations of human features and topological space, respectively. Next, an attention mechanism is further developed to learn the importance weights of different representations to effectively integrate human features and social structures. Finally, to learn cluster-friendly features, the DSS module unifies and integrates the features learned from the FGA and SGAT modules. DSS explores the high-confidence cluster assignment as a soft label to guide the optimization of the entire network. Extensive experiments are conducted on five real-world data sets on user attribute clustering. The experimental results demonstrate that the proposed DGENFS model achieves the most advanced performance compared with nine competitive baselines.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4293212235",
    "type": "article"
  },
  {
    "title": "Distance and Direction Based Deep Discriminant Metric Learning for Kinship Verification",
    "doi": "https://doi.org/10.1145/3531014",
    "publication_date": "2022-04-23",
    "publication_year": 2022,
    "authors": "Xiaoke Zhu; Changlong Li; Xiaopan Chen; Xinyu Zhang; Xiao‐Yuan Jing",
    "corresponding_authors": "",
    "abstract": "Image-based kinship verification is an important task in computer vision and has many applications in practice, such as missing children search and family album construction, among others. Due to the differences in age, gender, expression and appearance, there usually exists a large discrepancy between the facial images of parent and child. This makes kinship verification a challenging task. In this article, we propose a Distance and Direction Based Deep Discriminant Metric Learning (D 4 ML) approach for kinship verification. The basic idea of D 4 ML is to make full use of the discriminant information contained in the facial images of parent and child such that the network can learn more a discriminating distance metric. Specifically, D 4 ML learns the metric by utilizing the discriminant information from two perspectives: distance-based perspective and direction-based perspective. From the distance-based perspective, the designed loss function is used to minimize the distance between images having kinship and maximize the distance between images without kinship. In practice, the gender difference and large age gap may significantly increase the distance between facial images of parent and child. Therefore, learning the metric only from a distance-based perspective is insufficient. Considering that two vectors with a large distance may appear with high similarity in direction, D 4 ML also employs the direction-based loss function in the training process. Both kinds of loss function work together to improve the discriminability of the learned metric. Experimental results on four small size publicly available datasets demonstrate the effectiveness of our approach. Source code of our approach can be found at https://github.com/lclhenu/D4ML .",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4293236025",
    "type": "article"
  },
  {
    "title": "Realtime Recognition of Dynamic Hand Gestures in Practical Applications",
    "doi": "https://doi.org/10.1145/3561822",
    "publication_date": "2022-09-08",
    "publication_year": 2022,
    "authors": "Yi Xiao; Tong Liu; Yu Han; Yue Liu; Yongtian Wang",
    "corresponding_authors": "",
    "abstract": "Dynamic hand gesture acting as a semaphoric gesture is a practical and intuitive mid-air gesture interface. Nowadays benefiting from the development of deep convolutional networks, the gesture recognition has already achieved a high accuracy, however, when performing a dynamic hand gesture such as gestures of direction commands, some unintentional actions are easily misrecognized due to the similarity of the hand poses. This hinders the application of dynamic hand gestures and cannot be solved by just improving the accuracy of the applied algorithm on public datasets, thus it is necessary to study such problems from the perspective of human-computer interaction. In this article, two methods are proposed to avoid misrecognition by introducing activation delay and using asymmetric gesture design. First the temporal process of a dynamic hand gesture is decomposed and redefined, then a realtime dynamic hand gesture recognition system is built through a two-dimensional convolutional neural network. In order to investigate the influence of activation delay and asymmetric gesture design on system performance, a user study is conducted and experimental results show that the two proposed methods can effectively avoid misrecognition. The two methods proposed in this article can provide valuable guidance for researchers when designing realtime recognition system in practical applications.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4296118163",
    "type": "article"
  },
  {
    "title": "Mirror Segmentation via Semantic-aware Contextual Contrasted Feature Learning",
    "doi": "https://doi.org/10.1145/3566127",
    "publication_date": "2022-11-05",
    "publication_year": 2022,
    "authors": "Haiyang Mei; Letian Yu; Ke Xu; Yang Wang; Xin Yang; Xiaopeng Wei; Rynson W. H. Lau",
    "corresponding_authors": "",
    "abstract": "Mirrors are everywhere in our daily lives. Existing computer vision systems do not consider mirrors, and hence may get confused by the reflected content inside a mirror, resulting in a severe performance degradation. However, separating the real content outside a mirror from the reflected content inside it is non-trivial. The key challenge is that mirrors typically reflect contents similar to their surroundings, making it very difficult to differentiate the two. In this article, we present a novel method to segment mirrors from a single RGB image. To the best of our knowledge, this is the first work to address the mirror segmentation problem with a computational approach. We make the following contributions: First, we propose a novel network, called MirrorNet+, for mirror segmentation, by modeling both contextual contrasts and semantic associations. Second, we construct the first large-scale mirror segmentation dataset, which consists of 4,018 pairs of images containing mirrors and their corresponding manually annotated mirror masks, covering a variety of daily-life scenes. Third, we conduct extensive experiments to evaluate the proposed method and show that it outperforms the related state-of-the-art detection and segmentation methods. Fourth, we further validate the effectiveness and generalization capability of the proposed semantic awareness contextual contrasted feature learning by applying MirrorNet+ to other vision tasks, i.e., salient object detection and shadow detection. Finally, we provide some applications of mirror segmentation and analyze possible future research directions. Project homepage: https://mhaiyang.github.io/TOMM2022-MirrorNet+/index.html .",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4308239176",
    "type": "article"
  },
  {
    "title": "Distilled Meta-learning for Multi-Class Incremental Learning",
    "doi": "https://doi.org/10.1145/3576045",
    "publication_date": "2023-01-17",
    "publication_year": 2023,
    "authors": "Hao Liu; Zhaoyu Yan; Bing Liu; Jiaqi Zhao; Yong Zhou; Abdulmotaleb El Saddik",
    "corresponding_authors": "",
    "abstract": "Meta-learning approaches have recently achieved promising performance in multi-class incremental learning. However, meta-learners still suffer from catastrophic forgetting, i.e., they tend to forget the learned knowledge from the old tasks when they focus on rapidly adapting to the new classes of the current task. To solve this problem, we propose a novel distilled meta-learning (DML) framework for multi-class incremental learning that integrates seamlessly meta-learning with knowledge distillation in each incremental stage. Specifically, during inner-loop training, knowledge distillation is incorporated into the DML to overcome catastrophic forgetting. During outer-loop training, a meta-update rule is designed for the meta-learner to learn across tasks and quickly adapt to new tasks. By virtue of the bilevel optimization, our model is encouraged to reach a balance between the retention of old knowledge and the learning of new knowledge. Experimental results on four benchmark datasets demonstrate the effectiveness of our proposal and show that our method significantly outperforms other state-of-the-art incremental learning methods.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4317036100",
    "type": "article"
  },
  {
    "title": "Novel View Synthesis from a Single Unposed Image via Unsupervised Learning",
    "doi": "https://doi.org/10.1145/3587467",
    "publication_date": "2023-03-11",
    "publication_year": 2023,
    "authors": "Bingzheng Liu; Jianjun Lei; Bo Peng; Chuanbo Yu; Wanqing Li; Nam Ling",
    "corresponding_authors": "",
    "abstract": "Novel view synthesis aims to generate novel views from one or more given source views. Although existing methods have achieved promising performance, they usually require paired views with different poses to learn a pixel transformation. This article proposes an unsupervised network to learn such a pixel transformation from a single source image. In particular, the network consists of a token transformation module that facilities the transformation of the features extracted from a source image into an intrinsic representation with respect to a pre-defined reference pose and a view generation module that synthesizes an arbitrary view from the representation. The learned transformation allows us to synthesize a novel view from any single source image of an unknown pose. Experiments on the widely used view synthesis datasets have demonstrated that the proposed network is able to produce comparable results to the state-of-the-art methods despite the fact that learning is unsupervised and only a single source image is required for generating a novel view. The code will be available upon the acceptance of the article.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4323925641",
    "type": "article"
  },
  {
    "title": "Local Bidirection Recurrent Network for Efficient Video Deblurring with the Fused Temporal Merge Module",
    "doi": "https://doi.org/10.1145/3587468",
    "publication_date": "2023-03-13",
    "publication_year": 2023,
    "authors": "Chen Li; Li Song; Rong Xie; Wenjun Zhang",
    "corresponding_authors": "",
    "abstract": "Video deblurring methods exploit the correlation between consecutive blurry inputs to generate sharp frames. However, designing an effective and efficient method is a challenging problem for video deblurring. To guarantee the effectiveness and further improve the deblurring performance, we adopt the recurrent-based method as the baseline and reconsider the recurrent mechanism as well as the temporal feature alignment in the state-of-the-art methods. For the recurrent mechanism, we add the local backward connection to the global forward recurrent backbone to effectively exploit accurate future information. For the temporal alignment, we adopt a fused temporal merge module that exploits the superiority of flow-based and kernel-based methods with progressive correlation volumes estimation. In addition, we evaluate our method with both synthetic datasets (GoPro, DVD) and a realistic dataset (BSD). The experimental results demonstrate that our method achieves significant performance improvement with a slight computational cost increase against the state-of-the-art video deblurring methods. The extended ablation studies verify the effectiveness of our model.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4324052865",
    "type": "article"
  },
  {
    "title": "Semantic Enhanced Video Captioning with Multi-feature Fusion",
    "doi": "https://doi.org/10.1145/3588572",
    "publication_date": "2023-03-20",
    "publication_year": 2023,
    "authors": "Tian-Zi Niu; Shan-Shan Dong; Zhen-Duo Chen; Xin Luo; Shanqing Guo; Zi Huang; Xin-Shun Xu",
    "corresponding_authors": "",
    "abstract": "Video captioning aims to automatically describe a video clip with informative sentences. At present, deep learning-based models have become the mainstream for this task and achieved competitive results on public datasets. Usually, these methods leverage different types of features to generate sentences, e.g., semantic information, 2D or 3D features. However, some methods only treat semantic information as a complement of visual representations and cannot fully exploit it; some of them ignore the relationship between different types of features. In addition, most of them select multiple frames of a video with an equally spaced sampling scheme, resulting in much redundant information. To address these issues, we present a novel video-captioning framework, Semantic Enhanced video captioning with Multi-feature Fusion, SEMF for short. It optimizes the use of different types of features from three aspects. First, a semantic encoder is designed to enhance meaningful semantic features through a semantic dictionary to boost performance. Second, a discrete selection module pays attention to important features and obtains different contexts at different steps to reduce feature redundancy. Finally, a multi-feature fusion module uses a novel relation-aware attention mechanism to separate the common and complementary components of different features to provide more effective visual features for the next step. Moreover, the entire framework can be trained in an end-to-end manner. Extensive experiments are conducted on Microsoft Research Video Description Corpus (MSVD) and MSR-Video to Text (MSR-VTT) datasets. The results demonstrate that SEMF is able to achieve state-of-the-art results.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4327950607",
    "type": "article"
  },
  {
    "title": "Affective Feedback Synthesis Towards Multimodal Text and Image Data",
    "doi": "https://doi.org/10.1145/3589186",
    "publication_date": "2023-03-24",
    "publication_year": 2023,
    "authors": "Puneet Kumar; Gaurav Bhatt; Omkar Ingle; Daksh Goyal; Balasubramanian Raman",
    "corresponding_authors": "",
    "abstract": "In this article, we have defined a novel task of affective feedback synthesis that generates feedback for input text and corresponding images in a way similar to humans responding to multimodal data. A feedback synthesis system has been proposed and trained using ground-truth human comments along with image–text input. We have also constructed a large-scale dataset consisting of images, text, Twitter user comments, and the number of likes for the comments by crawling news articles through Twitter feeds. The proposed system extracts textual features using a transformer-based textual encoder. The visual features have been extracted using a Faster region-based convolutional neural networks model. The textual and visual features have been concatenated to construct multimodal features that the decoder uses to synthesize the feedback. We have compared the results of the proposed system with baseline models using quantitative and qualitative measures. The synthesized feedbacks have been analyzed using automatic and human evaluation. They have been found to be semantically similar to the ground-truth comments and relevant to the given text–image input.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4360851779",
    "type": "article"
  },
  {
    "title": "Towards Food Image Retrieval via Generalization-Oriented Sampling and Loss Function Design",
    "doi": "https://doi.org/10.1145/3600095",
    "publication_date": "2023-05-29",
    "publication_year": 2023,
    "authors": "Jiajun Song; Zhuo Li; Weiqing Min; Shuqiang Jiang",
    "corresponding_authors": "",
    "abstract": "Food computing has increasingly received widespread attention in the multimedia field. As a basic task of food computing, food image retrieval has wide applications, that is, food image retrieval can help users to find the desired food from a large number of food images. Besides, the retrieved information can be applied to establish a richer database for the subsequent food content-related recommendation. Food image retrieval aims to achieve better performance on novel categories. Thus, it is worth studying to transfer the embedding ability from the training set to the unseen test set, that is, the generalization of the model. Food is influenced by various factors, such as culture and geography, leading to great differences between domains, such as Asian food and western food. Therefore, it is challenging to study the generalization of the model in food image retrieval. In this article, we improve the classical metric learning framework and propose a generalization-oriented sampling strategy, which boosts the generalization of the model by maximizing the intra-class distance from a proportion of positive pairs to avoid the excessive distance compression in the embedding space. Considering that the existing optimization process is in an opposite direction to our proposed sampling strategy, we further propose an adaptive gradient assignment policy named gradient-adaptive optimization , which can alleviate the intra-class distance compression during optimization by assigning different gradients to different samples. Extensive evaluation on three popular food image datasets demonstrates the effectiveness of the proposed method. We also experiment on three popular general datasets to prove that solving the problem from the generalization can also improve the performance of general image retrieval. Code is available at https://github.com/Jiajun-ISIA/Generalization-oriented-Sampling-and-Loss .",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4378651382",
    "type": "article"
  },
  {
    "title": "Transformer-Based Relational Inference Network for Complex Visual Relational Reasoning",
    "doi": "https://doi.org/10.1145/3605781",
    "publication_date": "2023-06-22",
    "publication_year": 2023,
    "authors": "Mingkui Tan; Zhiquan Wen; Leyuan Fang; Qi Wu",
    "corresponding_authors": "",
    "abstract": "Visual Relational Reasoning is the basis of many vision-and-language based tasks (e.g., visual question answering and referring expression comprehension). In this article, we regard the complex referring expression comprehension (c-REF) task as the reasoning basis, in which c-REF seeks to localise a target object in an image guided by a complex query. Such queries often contain complex logic and thus impose two critical challenges for reasoning: (i) Comprehending the complex queries is difficult since these queries usually refer to multiple objects and their relationships; (ii) Reasoning among multiple objects guided by the queries and then localising the target correctly are non-trivial. To address the above challenges, we propose a Transformer-based Relational Inference Network (Trans-RINet). Specifically, to comprehend the queries, we mimic the language-comprehending mechanism of humans, and devise a language decomposition module to decompose the queries into four types, i.e., basic attributes, absolute location, visual relationship and relative location. We further devise four modules to address the corresponding information. In each module, we consider the intra-(i.e., between the objects) and inter-modality relationships(i.e., between the queries and objects) to improve the reasoning ability. Moreover, we construct a relational graph to represent the objects and their relationships, and devise a multi-step reasoning method to progressively understand the complex logic. Since each type of the queries is closely related, we let each module interact with each other before making a decision. Extensive experiments on the CLEVR-Ref+, Ref-Reasoning, and CLEVR-CoGenT datasets demonstrate the superior reasoning performance of our Trans-RINet.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4381611105",
    "type": "article"
  },
  {
    "title": "Cascading Blend Network for Image Inpainting",
    "doi": "https://doi.org/10.1145/3608952",
    "publication_date": "2023-07-13",
    "publication_year": 2023,
    "authors": "Yiting Jin; Jie Wu; Wanliang Wang; Yidong Yan; Jiawei Jiang; Jianwei Zheng",
    "corresponding_authors": "",
    "abstract": "Image inpainting refers to filling in unknown regions with known knowledge, which is in full flourish accompanied by the popularity and prosperity of deep convolutional networks. Current inpainting methods have excelled in completing small-sized corruption or specifically masked images. However, for large-proportion corrupted images, most attention-based and structure-based approaches, though reported with state-of-the-art performance, fail to reconstruct high-quality results due to the short consideration of semantic relevance. To relieve the above problem, in this paper, we propose a novel image inpainting approach, namely cascading blend network (CBNet), to strengthen the capacity of feature representation. As a whole, we introduce an adjacent transfer attention (ATA) module in the decoder, which preserves contour structure reasonably from the deep layer and blends structure-texture information from the shadow layer. In a coarse to delicate manner, a multi-scale contextual blend (MCB) block is further designed to felicitously assemble the multi-stage feature information. In addition, to ensure a high qualified hybrid of the feature information, extra deep supervision is applied to the intermediate features through a cascaded loss. Qualitative and quantitative experiments on the Paris StreetView, CelebA, and Places2 datasets demonstrate the superior performance of our approach compared with most state-of-the-art algorithms.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4384201152",
    "type": "article"
  },
  {
    "title": "1DIEN: Cross-session Electrocardiogram Authentication Using 1D Integrated EfficientNet",
    "doi": "https://doi.org/10.1145/3609800",
    "publication_date": "2023-07-17",
    "publication_year": 2023,
    "authors": "Liping Zhang; Shukai Chen; Fei Lin; Wei Ren; Kim‐Kwang Raymond Choo; Geyong Min",
    "corresponding_authors": "",
    "abstract": "The potential of using electrocardiogram (ECG), an important physiological signal for humans, as a new biometric trait has been demonstrated, and ongoing efforts have focused on utilizing deep learning (e.g., 2D neural networks) to improve authentication accuracy (with some efficiency tradeoffs). In most of the existing ECG-based authentication approaches, the ECG recordings for enrollment and testing are collected within short intervals (e.g., within an hour). However, since ECG biometrics change over time, this design may decrease authentication accuracy when ECG recordings are collected weeks or even months prior. In this article, we propose 1D Integrated EfficientNet (1DIEN) to achieve cross-session ECG authentication. We adopt 1D neural networks as a lightweight alternative to 2D neural networks, and a voting scheme is designed to reduce variance and improve general authentication performance. We use three public ECG databases (i.e., an inter-session database, a mixed-session database, and an intra-session database) to evaluate our proposed 1DIEN under different authentication scenarios. The experimental results show that our approach achieves satisfactory performance for ECG authentication at a 3-month interval and is suitable for practical applications.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4384523552",
    "type": "article"
  },
  {
    "title": "Visual Security Index Combining CNN and Filter for Perceptually Encrypted Light Field Images",
    "doi": "https://doi.org/10.1145/3612924",
    "publication_date": "2023-08-03",
    "publication_year": 2023,
    "authors": "Wenying Wen; Minghui Huang; Yushu Zhang; Yuming Fang; Yifan Zuo",
    "corresponding_authors": "",
    "abstract": "Visual security index (VSI) represents a quantitative index for the visual security evaluation of perceptually encrypted images. Recently, the research on visual security of encrypted light field (LF) images faces two challenges. One is that the existing perceptually encrypted image databases are often too small, which is easy to cause overfitting in convolutional neural network (CNN). The other is that existing VSI models did not take a full account the intrinsic characteristics of the LF images and highly relied on handcrafted feature extraction. In this article, we construct a new database of perceptually encrypted LF images, called the PE-SLF, which is 2.6 times as big as the existing largest perceptual encrypted image database. Moreover, a novel visual security index (VSI) model is proposed by taking into full consideration the intrinsic spatial-angular characteristics of the LF images and the outstanding capabilities of CNN in feature extraction. First, we exploit CNN to detect the texture and structure features of encrypted sub-aperture images in the spatial domain. Second, we apply the Gabor filter to detect the Gabor feature over the epi-polar plane images in angular domain. Last, the spatial and angular similarity measurements are subsequently calculated for jointly yielding the final visual security score. Experimental results on the constructed PE-SLF demonstrate that the proposed VSI model is closer to the perception of HVS in visual security evaluation of encrypted LF images compared to other classical and state-of-the-art models.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4385549224",
    "type": "article"
  },
  {
    "title": "On Content-Aware Post-Processing: Adapting Statistically Learned Models to Dynamic Content",
    "doi": "https://doi.org/10.1145/3612925",
    "publication_date": "2023-08-16",
    "publication_year": 2023,
    "authors": "Yichi Zhang; Gongchun Ding; Dandan Ding; Zhan Ma; Zhu Li",
    "corresponding_authors": "",
    "abstract": "Learning-based post-processing methods generally produce neural models that are statistically optimal on their training datasets. These models, however, neglect intrinsic variations of local video content and may fail to process unseen content. To address this issue, this article proposes a content-aware approach for the post-processing of compressed videos. We develop a backbone network, called BackboneFormer , where a Fast Transformer using Separable Self-Attention, Spatial Attention, and Channel Attention is devised to support underlying feature embedding and aggregation. Furthermore, we introduce Meta-learning to strengthen BackboneFormer for better performance. Specifically, we propose Meta Post-Processing (Meta-PP) which leverages the Meta-learning framework to drive BackboneFormer to capture and analyze input video variations for spontaneous updating. Since the original frame is unavailable to the decoder, we devise a Compression Degradation Estimation model where a low-complexity neural model and classic operators are used collaboratively to estimate the compression distortion. The estimated distortion is then utilized to guide the BackboneFormer model for dynamic updating of weighting parameters. Experimental results demonstrate that the proposed BackboneFormer itself gains about 3.61% Bjøntegaard delta bit-rate reduction over Versatile Video Coding in the post-processing task and “BackboneFormer + Meta-PP” attains 4.32%, costing only 50K and 61K parameters, respectively. The computational complexity of MACs is 49k/pixel and 50k/pixel, which represents only about 16% of state-of-the-art methods having similar coding gains.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4385877289",
    "type": "article"
  },
  {
    "title": "A Feature Map is Worth a Video Frame: Rethinking Convolutional Features for Visible-Infrared Person Re-identification",
    "doi": "https://doi.org/10.1145/3617375",
    "publication_date": "2023-08-24",
    "publication_year": 2023,
    "authors": "Qiaolin He; Zhijie Zheng; Haifeng Hu",
    "corresponding_authors": "",
    "abstract": "Visible-Infrared Person Re-identification (VI-ReID) aims to search for the identity of the same person across different spectra. The feature maps obtained from the convolutional layers are generally used for loss calculation in the later stages of the model in VI-ReID, but their role in the early and middle stages of the model remains unexplored. In this article, we propose a novel Rethinking Convolutional Features (ReCF) approach for VI-ReID. ReCF consists of two modules: Middle Feature Generation (MFG), which utilizes the feature maps in the early stage to reduce significant modality gap, and Temporal Feature Aggregation (TFA), which uses the feature maps in the middle stage to aggregate multi-level features for enlarging the receptive field. MFG generates middle modality features in the form of a learnable convolution layer as a bridge between RGB and IR modalities, which is more flexible than using fixed-parameter grayscale images and yields a better middle modality to further reduce the modality gap. TFA first treats the convolution process as a video sequence, and the feature map of each convolution layer can be considered a worthwhile video frame. Based on this, we can obtain a multi-level receptive field and a temporal refinement. In addition, we introduce a color-unrelated loss and a modality-unrelated loss to constrain the modality features for providing a common feature representation space. Experimental results on the challenging VI-ReID datasets demonstrate that our proposed method achieves state-of-the-art performance.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4386136611",
    "type": "article"
  },
  {
    "title": "Detecting Deepfake Videos Using Spatiotemporal Trident Network",
    "doi": "https://doi.org/10.1145/3623639",
    "publication_date": "2023-09-13",
    "publication_year": 2023,
    "authors": "Kaihan Lin; Weihong Han; Shudong Li; Zhaoquan Gu; Huimin Zhao; Yangyang Mei",
    "corresponding_authors": "",
    "abstract": "The widespread dissemination of Deepfake in social networks has posed serious security risks, thus necessitating the development of an effective Deepfake detection technique. Currently, video-based detectors have not been explored as extensively as image-based detectors. Most existing video-based methods only consider temporal features without combining spatial features, and do not mine deeper-level subtle forgeries, resulting in limited detection performance. In this paper, a novel spatiotemporal trident network (STN) is proposed to detect both spatial and temporal inconsistencies of Deepfake videos. Since there is a large amount of redundant information in Deepfake video frames, we introduce convolutional block attention module (CBAM) on the basis of the I3D network and optimize the structure to make the network better focus on the meaningful information of the input video. Aiming at the defects in the deeper-level subtle forgeries, we designed three feature extraction modules (FEMs) of RGB, optical flow, and noise to further extract deeper video frame information. Extensive experiments on several well-known datasets demonstrate that our method has promising performance, surpassing several state-of-the-art Deepfake video detection methods.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4386710331",
    "type": "article"
  },
  {
    "title": "How Will You Pod? Implications of Creators’ Perspectives for Designing Innovative Podcasting Tools",
    "doi": "https://doi.org/10.1145/3625099",
    "publication_date": "2023-09-20",
    "publication_year": 2023,
    "authors": "Jemily Rime; Alan Archer-Boyd; Tom Collins",
    "corresponding_authors": "",
    "abstract": "While centred on the medium of audio, podcasts are often a multimedia concern, and one that has become hugely popular in recent years, though relatively little is known about the perspectives of podcast creators and their visions of innovation. This article details the results of an exploratory study conducted to enhance our understanding of potential innovation in the field of podcasting. Sixteen podcast creators were interviewed about their work and what they wanted from next-generation podcasts, in order to understand the requirements and expectations of tools that could be built to create new forms of audio-based programming. Through a combination of qualitative and quantitative analysis, we reveal novel findings, such as the duality between “listener-centric” and “creator-centric” innovations, to improve listener experience but also to unleash new creative possibilities in a streamlined production workflow. We shed light on what podcast creators envision as “next-generation podcasting,” the archetypal podcast production workflow, and creators’ expectations of podcasting tools. Combining these findings, we identify how the workflow could be modified to include new steps that will help to realise podcast creators’ visions. This study crystalises on important information about podcasters—their behavior and perspectives on the future of the medium—which will allow further research, design, and development in the field to be founded upon empirical observations.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4386889814",
    "type": "article"
  },
  {
    "title": "Task Recommendation via Heterogeneous Multi-modal Features and Decision Fusion in Mobile Crowdsensing",
    "doi": "https://doi.org/10.1145/3626239",
    "publication_date": "2023-10-02",
    "publication_year": 2023,
    "authors": "Jian Wang; Xiao Wang; Guosheng Zhao",
    "corresponding_authors": "",
    "abstract": "In the decision-making process of the behavior of mobile crowdsensing, using a single view to learn a user's preference will lead to a mismatch between the user's wishes and the final task recommendation list, resulting in the low efficiency of the model recommendation. Aiming at the lack of perceptual representation and cognitive fusion of multimodal coupled information, a task recommendation method based on heterogeneous multimodal features and decision fusion is proposed. According to the content characteristics of multi-source data in the user's historical task set, several task-task similarity matrices are constructed to align feature dimensions and feature semantics. Using the improved similarity network fusion algorithm, networks composed of multiple content similarity matrices are effectively fused into a similarity network. Considering the influence of the time factor, the tasks that have had interest drift are filtered out from the set of tasks that the user has participated in. Finally, the updated similarity network is clustered to predict the current preference of the user for new tasks. Experimental results based on simulation and real datasets show that the proposed method can effectively improve the accuracy and efficiency of task assignments while improving user satisfaction.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4387264742",
    "type": "article"
  },
  {
    "title": "Vocoder Detection of Spoofing Speech Based on GAN Fingerprints and Domain Generalization",
    "doi": "https://doi.org/10.1145/3630751",
    "publication_date": "2023-10-28",
    "publication_year": 2023,
    "authors": "Fan Li; Yanxiang Chen; Haiyang Liu; Zuxing Zhao; Yuanzhi Yao; Xin Liao",
    "corresponding_authors": "",
    "abstract": "As an important part of the text-to-speech (TTS) system, vocoders convert acoustic features into speech waveforms. The difference in vocoders is key to producing different types of forged speech in the TTS system. With the rapid development of general adversarial networks (GANs), an increasing number of GAN vocoders have been proposed. Detectors often encounter vocoders of unknown types, which leads to a decline in the generalization performance of models. However, existing studies lack research on detection generalization based on GAN vocoders. To solve this problem, this study proposes vocoder detection of spoofed speech based on GAN fingerprints and domain generalization. The framework can widen the distance between real speech and forged speech in feature space, improving the detection model’s performance. Specifically, we utilize a fingerprint extractor based on an autoencoder to extract GAN fingerprints from vocoders. We then weight them to the forged speech for subsequent classification to learn the forged speech features with high differentiation. Subsequently, domain generalization is used to further improve the generalization ability of the model for unseen forgery types. We achieve domain generalization using domain-adversarial learning and asymmetric triplet loss to learn a better generalized feature space in which real speech is compact and forged speech synthesized by different vocoders is dispersed. Finally, to optimize the training process, curriculum learning is used to dynamically adjust the contributions of the samples with different difficulties in the training process. Experimental results show that the proposed method achieves the most advanced detection results among four GAN vocoders. The code is available at https://github.com/multimedia-infomation-security/GAN-Vocoder-detection .",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4387999491",
    "type": "article"
  },
  {
    "title": "Transform-Equivariant Consistency Learning for Temporal Sentence Grounding",
    "doi": "https://doi.org/10.1145/3634749",
    "publication_date": "2023-11-27",
    "publication_year": 2023,
    "authors": "Daizong Liu; Xiaoye Qu; Jianfeng Dong; Pan Zhou; Zichuan Xu; Haozhao Wang; Xing Di; Weining Lu; Yu Cheng",
    "corresponding_authors": "",
    "abstract": "This paper addresses the temporal sentence grounding (TSG). Although existing methods have made decent achievements in this task, they not only severely rely on abundant video-query paired data for training, but also easily fail into the dataset distribution bias. To alleviate these limitations, we introduce a novel Equivariant Consistency Regulation Learning (ECRL) framework to learn more discriminative query-related frame-wise representations for each video, in a self-supervised manner. Our motivation comes from that the temporal boundary of the query-guided activity should be consistently predicted under various video-level transformations. Concretely, we first design a series of spatio-temporal augmentations on both foreground and background video segments to generate a set of synthetic video samples. In particular, we devise a self-refine module to enhance the completeness and smoothness of the augmented video. Then, we present a novel self-supervised consistency loss (SSCL) applied on the original and augmented videos to capture their invariant query-related semantic by minimizing the KL-divergence between the sequence similarity of two videos and a prior Gaussian distribution of timestamp distance. At last, a shared grounding head is introduced to predict the transform-equivariant query-guided segment boundaries for both the original and augmented videos. Extensive experiments on three challenging datasets (ActivityNet, TACoS, and Charades-STA) demonstrate both effectiveness and efficiency of our proposed ECRL framework.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4389057668",
    "type": "article"
  },
  {
    "title": "Principal Component Approximation Network for Image Compression",
    "doi": "https://doi.org/10.1145/3637490",
    "publication_date": "2023-12-13",
    "publication_year": 2023,
    "authors": "Shupei Zhang; Chenqiu Zhao; Anup Basu",
    "corresponding_authors": "",
    "abstract": "In this work, we propose a novel principal component approximation network (PCANet) for image compression. The proposed network is based on the assumption that a set of images can be decomposed into several shared feature matrices, and an image can be reconstructed by the weighted sum of these matrices. The proposed PCANet is specifically devised to learn and approximate these feature matrices and weight vectors, which are used to encode images for compression. Unlike previous deep learning-based methods, a distinctive aspect of our approach is its consideration of network size in the bit-rate computation. Despite this inclusion, our proposed method yields promising results. Through extensive experiments conducted on standard datasets, we demonstrate the effectiveness of our approach in comparison to state-of-the-art techniques. To the best of our knowledge, this is the first machine learning approach that includes the size of networks during bitrate computation in image compression.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4389675231",
    "type": "article"
  },
  {
    "title": "DoubleAUG: Single-domain Generalized Object Detector in Urban via Color Perturbation and Dual-style Memory",
    "doi": "https://doi.org/10.1145/3634683",
    "publication_date": "2023-12-18",
    "publication_year": 2023,
    "authors": "Lei Qi; Peng Dong; Xiong Tan; Hui Xue; Xin Geng",
    "corresponding_authors": "",
    "abstract": "Object detection in urban scenarios is crucial for autonomous driving in intelligent traffic systems. However, unlike conventional object detection tasks, urban-scene images vary greatly in style. For example, images taken on sunny days differ significantly from those taken on rainy days. Therefore, models trained on sunny day images may not generalize well to rainy day images. In this paper, we aim to solve the single-domain generalizable object detection task in urban scenarios, meaning that a model trained on images from one weather condition should be able to perform well on images from any other weather conditions. To address this challenge, we propose a novel Double AUGmentation (DoubleAUG) method that includes image- and feature-level augmentation schemes. In the image-level augmentation, we consider the variation in color information across different weather conditions and propose a Color Perturbation (CP) method that randomly exchanges the RGB channels to generate various images. In the feature-level augmentation, we propose to utilize a Dual-Style Memory (DSM) to explore the diverse style information on the entire dataset, further enhancing the model’s generalization capability. Extensive experiments demonstrate that our proposed method outperforms state-of-the-art methods. Furthermore, ablation studies confirm the effectiveness of each module in our proposed method. Moreover, our method is plug-and-play and can be integrated into existing methods to further improve model performance.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4389919300",
    "type": "article"
  },
  {
    "title": "QoE Estimation of WebRTC-based Audio-visual Conversations from Facial and Speech Features",
    "doi": "https://doi.org/10.1145/3638251",
    "publication_date": "2023-12-21",
    "publication_year": 2023,
    "authors": "Gülnaziye Bingöl; Simone Porcu; Alessandro Floris; Luigi Atzori",
    "corresponding_authors": "",
    "abstract": "The utilization of user’s facial- and speech-related features for the estimation of the Quality of Experience (QoE) of multimedia services is still underinvestigated despite its potential. Currently, only the use of either facial or speech features individually has been proposed, and relevant limited experiments have been performed. To advance in this respect, in this study, we focused on WebRTC-based videoconferencing, where it is often possible to capture both the facial expressions and vocal speech characteristics of the users. First, we performed thorough statistical analysis to identify the most significant facial- and speech-related features for QoE estimation, which we extracted from the participants’ audio-video data collected during a subjective assessment. Second, we trained individual QoE estimation machine learning-based models on the separated facial and speech datasets. Finally, we employed data fusion techniques to combine the facial and speech datasets into a single dataset to enhance the QoE estimation performance due to the integrated knowledge provided by the fusion of facial and speech features. The obtained results demonstrate that the data fusion technique based on the Improved Centered Kernel Alignment (ICKA) allows for reaching a mean QoE estimation accuracy of 0.93, whereas the values of 0.78 and 0.86 are reached when using only facial or speech features, respectively.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4390051549",
    "type": "article"
  },
  {
    "title": "Controlling Media Player with Hands: A Transformer Approach and a Quality of Experience Assessment",
    "doi": "https://doi.org/10.1145/3638560",
    "publication_date": "2023-12-25",
    "publication_year": 2023,
    "authors": "Alessandro Floris; Simone Porcu; Luigi Atzori",
    "corresponding_authors": "",
    "abstract": "In this article, we propose a Hand Gesture Recognition (HGR) system based on a novel deep transformer (DT) neural network for media player control. The extracted hand skeleton features are processed by separate transformers for each finger in isolation to better identify the finger characteristics to drive the following classification. The achieved HGR accuracy (0.853) outperforms state-of-the-art HGR approaches when tested on the popular NVIDIA dataset. Moreover, we conducted a subjective assessment involving 30 people to evaluate the Quality of Experience (QoE) provided by the proposed DT-HGR for controlling a media player application compared with two traditional input devices, i.e., mouse and keyboard. The assessment participants were asked to evaluate objective (accuracy) and subjective (physical fatigue, usability, pragmatic quality, and hedonic quality) measurements. We found that (i) the accuracy of DT-HGR is very high (91.67%), only slightly lower than that of traditional alternative interaction modalities; and that (ii) the perceived quality for DT-HGR in terms of satisfaction, comfort, and interactivity is very high, with an average Mean Opinion Score (MOS) value as high as 4.4, whereas the alternative approaches did not reach 3.8, which encourages a more pervasive adoption of the natural gesture interaction.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4390195748",
    "type": "article"
  },
  {
    "title": "A criterion-based multilayer access control approach for multimedia applications and the implementation considerations",
    "doi": "https://doi.org/10.1145/1413862.1413870",
    "publication_date": "2008-11-01",
    "publication_year": 2008,
    "authors": "Leon Pan; Chang N. Zhang",
    "corresponding_authors": "",
    "abstract": "In this article, a novel criterion-based multilayer access control (CBMAC) approach is presented to enhance existing access control models such as Role-Based, Mandatory, and Discretionary Access Control models to support multilayer (multilevel) access control. The proposed approach is based on a set of predefined security criteria which are extracted from authorization rules. The security attributes of objects and users are specified by security criterion expressions (serving as locks) and the elements (serving as keys) of security criterion subsets respectively. An object embedded with a number of security criterion expressions becomes a secure object while a user associated with a security criterion subset is called a secure user. The multilayer access control is achieved by evaluating the embedded security criterion expressions (actuating locks) by the elements (keys) in a user's security criterion subset. The paper also provides the details of integrating the proposed approach with existing access control models and presents the implementation considerations of Criterion-Based Role-Based Multilayer Access Control, the integration of CBMAC and Role-Based Access Control.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2023869259",
    "type": "article"
  },
  {
    "title": "Multimedia simplification for optimized MMS synthesis",
    "doi": "https://doi.org/10.1145/1198302.1198307",
    "publication_date": "2007-02-01",
    "publication_year": 2007,
    "authors": "WeiQi Yan; Mohan Kankanhalli",
    "corresponding_authors": "",
    "abstract": "We propose a novel transcoding technique called multimedia simplification which is based on experiential sampling. Multimedia simplification helps optimize the synthesis of MMS (multimedia messaging service) messages for mobile phones. Transcoding is useful in overcoming the limitations of these compact devices. The proposed approach aims at reducing the redundancy in the multimedia data captured by multiple types of media sensors. The simplified data is first stored into a gallery for further usage. Once a request for MMS is received, the MMS server makes use of the simplified media from the gallery. The multimedia data is aligned with respect to the timeline for MMS message synthesis. We demonstrate the use of the proposed techniques for two applications, namely, soccer video and home care monitoring video. The MMS sent to the receiver can basically reflect the gist of important events of interest to the user. Our technique is targeted towards users who are interested in obtaining salient multimedia information via mobile devices.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2063397639",
    "type": "article"
  },
  {
    "title": "Predictive real-time perceptual compression based on eye-gaze-position analysis",
    "doi": "https://doi.org/10.1145/1386109.1386116",
    "publication_date": "2008-08-01",
    "publication_year": 2008,
    "authors": "Oleg V. Komogortsev; Javed Khan",
    "corresponding_authors": "",
    "abstract": "This article designs a real-time perceptual compression system (RTPCS) based on eye-gaze-position analysis. Our results indicate that the eye-gaze-position containment metric provides more efficient and effective evaluation of an RTPCS than the eye fixation containment. The presented RTPCS is designed for a network communication scenario with a feedback loop delay. The proposed RTPCS uses human visual system properties to compensate for the delay and to provide high ratios of multimedia compression.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2127674130",
    "type": "article"
  },
  {
    "title": "Audio-visual atoms for generic video concept classification",
    "doi": "https://doi.org/10.1145/1823746.1823748",
    "publication_date": "2010-08-01",
    "publication_year": 2010,
    "authors": "Wei Jiang; Courtenay V. Cotton; Shih‐Fu Chang; Dan Ellis; Alexander C. Loui",
    "corresponding_authors": "",
    "abstract": "We investigate the challenging issue of joint audio-visual analysis of generic videos targeting at concept detection. We extract a novel local representation, Audio-Visual Atom (AVA), which is defined as a region track associated with regional visual features and audio onset features. We develop a hierarchical algorithm to extract visual atoms from generic videos, and locate energy onsets from the corresponding soundtrack by time-frequency analysis. Audio atoms are extracted around energy onsets. Visual and audio atoms form AVAs, based on which discriminative audio-visual codebooks are constructed for concept detection. Experiments over Kodak's consumer benchmark videos confirm the effectiveness of our approach.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2002044752",
    "type": "article"
  },
  {
    "title": "Enabling multiparty 3D tele-immersive environments with ViewCast",
    "doi": "https://doi.org/10.1145/1865106.1865113",
    "publication_date": "2010-11-01",
    "publication_year": 2010,
    "authors": "Zhenyu Yang; Wanmin Wu; Klara Nahrstedt; Gregorij Kurillo; Růžena Bajcsy",
    "corresponding_authors": "",
    "abstract": "Three-dimensional tele-immersive (3DTI) environments have great potential to promote collaborative work among geographically distributed users. However, most existing 3DTI systems only work with two sites due to the huge demand of resources and the lack of a simple yet powerful networking model to handle connectivity, scalability, and quality-of-service (QoS) guarantees.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4247365723",
    "type": "article"
  },
  {
    "title": "Design and evaluation of a testbed for mobile TV networks",
    "doi": "https://doi.org/10.1145/2071396.2071399",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Mohamed Hefeeda; Cheng-Hsin Hsu",
    "corresponding_authors": "",
    "abstract": "This article presents the design of a complete, open-source, testbed for broadcast networks that offer mobile TV services. Although basic architectures and protocols have been developed for such networks, detailed performance tuning and analysis are still needed, especially when these networks scale to serve many diverse TV channels to numerous subscribers. The detailed performance analysis could also motivate designing new protocols and algorithms for enhancing future mobile TV networks. Currently, many researchers evaluate the performance of mobile TV networks using simulation and/or theoretical modeling methods. These methods, while useful for early assessment, typically abstract away many necessary details of actual, fairly complex, networks. Therefore, an open-source platform for evaluating new ideas in a real mobile TV network is needed. This platform is currently not possible with commercial products, because they are sold as black boxes without the source code. In this article, we summarize our experiences in designing and implementing a testbed for mobile TV networks. We integrate off-the-shelf hardware components with carefully designed software modules to realize a scalable testbed that covers almost all aspects of real networks. We use our testbed to empirically analyze various performance aspects of mobile TV networks and validate/refute several claims made in the literature as well as discover/quantify multiple important performance tradeoffs.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W1969336521",
    "type": "article"
  },
  {
    "title": "Large-scale multilabel propagation based on efficient sparse graph construction",
    "doi": "https://doi.org/10.1145/2542205.2542209",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Xiangyu Chen; Yadong Mu; Hairong Liu; Shuicheng Yan; Yong Rui; Tat‐Seng Chua",
    "corresponding_authors": "",
    "abstract": "With the popularity of photo-sharing websites, the number of web images has exploded into unseen magnitude. Annotating such large-scale data will cost huge amount of human resources and is thus unaffordable. Motivated by this challenging problem, we propose a novel sparse graph based multilabel propagation (SGMP) scheme for super large scale datasets. Both the efficacy and accuracy of the image annotation are further investigated under different graph construction strategies, where Gaussian noise and non-Gaussian sparse noise are simultaneously considered in the formulations of these strategies. Our proposed approach outperforms the state-of-the-art algorithms by focusing on: (1) For large-scale graph construction, a simple yet efficient LSH ( Locality Sensitive Hashing )-based sparse graph construction scheme is proposed to speed up the construction. We perform the multilabel propagation on this hashing-based graph construction, which is derived with LSH approach followed by sparse graph construction within the individual hashing buckets; (2) To further improve the accuracy, we propose a novel sparsity induced scalable graph construction scheme, which is based on a general sparse optimization framework. Sparsity essentially implies a very strong prior: for large scale optimization, the values of most variables shall be zeros when the solution reaches the optimum. By utilizing this prior, the solutions of large-scale sparse optimization problems can be derived by solving a series of much smaller scale subproblems; (3) For multilabel propagation, different from the traditional algorithms that propagate over individual label independently, our proposed propagation first encodes the label information of an image as a unit label confidence vector and naturally imposes inter-label constraints and manipulates labels interactively. Then, the entire propagation problem is formulated on the concept of Kullback-Leibler divergence defined on probabilistic distributions, which guides the propagation of the supervision information. Extensive experiments on the benchmark dataset NUS-WIDE with 270k images and its lite version NUS-WIDE-LITE with 56k images well demonstrate the effectiveness and scalability of the proposed multi-label propagation scheme.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2013062964",
    "type": "article"
  },
  {
    "title": "Robust image annotation via simultaneous feature and sample outlier pursuit",
    "doi": "https://doi.org/10.1145/2501643.2501646",
    "publication_date": "2013-08-01",
    "publication_year": 2013,
    "authors": "Jian Dong; Bin Cheng; Xiangyu Chen; Tat‐Seng Chua; Shuicheng Yan; Xi Zhou",
    "corresponding_authors": "",
    "abstract": "Graph-based semi-supervised image annotation has achieved great success in a variety of studies, yet it essentially and intuitively suffers from both the irrelevant/noisy features (referred to as feature outliers) and the unusual/corrupted samples (referred to as sample outliers). In this work, we investigate how to derive robust sample affinity matrix via simultaneous feature and sample outlier pursuit. This task is formulated as a Dual-outlier and Prior-driven Low-Rank Representation (DP-LRR) problem, which possesses convexity in objective function. In DP-LRR, the clean data are assumed to be self-reconstructible with low-rank coefficient matrix as in LRR; while the error matrix is decomposed as the sum of a row-wise sparse matrix and a column-wise sparse matrix, the ℓ 2,1 -norm minimization of which encourages the pursuit of feature and sample outliers respectively. The DP-LRR is further regularized by the priors from side information, that is, the inhomogeneous data pairs. An efficient iterative procedure based on linearized alternating direction method is presented to solve the DP-LRR problem, with closed-form solutions within each iteration. The derived low-rank reconstruction coefficient matrix is then fed into any graph based semi-supervised label propagation algorithm for image annotation, and as a by-product, the cleaned data from DP-LRR can also be utilized as a better image representation to generally boost image annotation performance. Extensive experiments on MIRFlickr, Corel30K, NUS-WIDE-LITE and NUS-WIDE databases well demonstrate the effectiveness of the proposed formulation for robust image annotation.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2017198039",
    "type": "article"
  },
  {
    "title": "Quality of data delivery in peer-to-peer video streaming",
    "doi": "https://doi.org/10.1145/2089085.2089089",
    "publication_date": "2012-02-01",
    "publication_year": 2012,
    "authors": "Xiaosong Lou; Kai Hwang",
    "corresponding_authors": "",
    "abstract": "QoS in a P2P video streaming system is evaluated in three stages: content generation, data delivery and video playback. We use jitter-free probability as the main performance metric to study Quality of Data delivery (QoD). A new model that incorporates both bandwidth and data availability of P2P network is proposed. Our model relies on a sharing factor that models data availability among all peers. We simulate on a minimalistic network to demonstrate how to apply the analytical model to design a P2P video streaming system with a very low jitter rate. Our simulation experimental results reveal that the lower bound on jitter-free probability is indeed effective to reflect the QoD of the entire system. Our model captures the impact of many design choices, including upload bandwidth limit, peer selection strategies, and video stream chunking schemes.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2031729048",
    "type": "article"
  },
  {
    "title": "Modeling the effect of user interactions on mesh-based P2P VoD streaming systems",
    "doi": "https://doi.org/10.1145/2457450.2457455",
    "publication_date": "2013-05-01",
    "publication_year": 2013,
    "authors": "Zhen Zhao; Sameer Samarth; Wei Tsang Ooi",
    "corresponding_authors": "",
    "abstract": "User interactions such as seeks and pauses are widely supported by existing Peer-to-Peer Video-on-Demand (P2P VoD) streaming systems. Their effect on the streaming system, however, has not been well studied. Seeks cause peers to skip part of the video, making them stay in the system for shorter time, and thus contribute less. On the other hand, only part of the video is downloaded due to seeks, reducing peers' demand from the system. It is unclear which factor dominates the effect of seeks on the streaming system. Pauses during playback, on one hand, allow peers to stay longer in the system and upload more content. When interleaved with seeks, however, long pauses may increase peers' demand unnecessarily as peers may download content that will eventually be skipped by subsequent forward seeks. The collective effect of seeks and pauses, together with the known random peer departure, is unintuitive and needs to be addressed properly so as to understand the effect of human factors on the streaming system performance. In this article, we develop an analytical model to both qualitatively and quantitatively study the effect of seeks and pauses on mesh-based P2P VoD streaming systems, in particular, the effect on the server cost. Our model can help in understanding how human factors such as seeks and pauses affect the streaming system performance, tuning a P2P VoD system towards better system performance and stability, and providing a framework for capacity planning.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2046127701",
    "type": "article"
  },
  {
    "title": "Experiential media systems",
    "doi": "https://doi.org/10.1145/2502432",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Hari Sundaram",
    "corresponding_authors": "Hari Sundaram",
    "abstract": "This article presents a personalized narrative on the early discussions within the Multimedia community and the subsequent research on experiential media systems. I discuss two different research initiatives—design of real-time, immersive multimedia feedback environments for stroke rehabilitation; exploratory environments for events that exploited the user's ability to make connections. I discuss the issue of foundations: the question of multisensory integration and superadditivity; the need for identification of “first-class” Multimedia problems; expanding the scope of Multimedia research.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2078557347",
    "type": "article"
  },
  {
    "title": "Efficient matchings and mobile augmented reality",
    "doi": "https://doi.org/10.1145/2348816.2348826",
    "publication_date": "2012-09-01",
    "publication_year": 2012,
    "authors": "Wei Guan; Suya You; Ulrich Newmann",
    "corresponding_authors": "",
    "abstract": "With the fast-growing popularity of smart phones in recent years, augmented reality (AR) on mobile devices is gaining more attention and becomes more demanding than ever before. However, the limited processors in mobile devices are not quite promising for AR applications that require real-time processing speed. The challenge exists due to the fact that, while fast features are usually not robust enough in matchings, robust features like SIFT or SURF are not computationally efficient. There is always a tradeoff between robustness and efficiency and it seems that we have to sacrifice one for the other. While this is true for most existing features, researchers have been working on designing new features with both robustness and efficiency. In this article, we are not trying to present a completely new feature. Instead, we propose an efficient matching method for robust features. An adaptive scoring scheme and a more distinctive descriptor are also proposed for performance improvements. Besides, we have developed an outdoor augmented reality system that is based on our proposed methods. The system demonstrates that not only it can achieve robust matchings efficiently, it is also capable to handle large occlusions such as passengers and moving vehicles, which is another challenge for many AR applications.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2081104213",
    "type": "article"
  },
  {
    "title": "Automatic evaluation of video summaries",
    "doi": "https://doi.org/10.1145/2240136.2240138",
    "publication_date": "2012-07-01",
    "publication_year": 2012,
    "authors": "Víctor Valdés; José M. Martínez",
    "corresponding_authors": "",
    "abstract": "This article describes a method for the automatic evaluation of video summaries based on the training of individual predictors for different quality measures from the TRECVid 2008 BBC Rushes Summarization Task. The obtained results demonstrate that, with a large set of evaluation data, it is possible to train fully automatic evaluation systems based on visual features automatically extracted from the summaries. The proposed approach will enable faster and easier estimation of the results of newly developed abstraction algorithms and the study of which summary characteristics influence their perceived quality.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2094435204",
    "type": "article"
  },
  {
    "title": "Creating Segments and Effects on Comics by Clustering Gaze Data",
    "doi": "https://doi.org/10.1145/3078836",
    "publication_date": "2017-05-31",
    "publication_year": 2017,
    "authors": "Ishwarya Thirunarayanan; Khimya Khetarpal; Sanjeev J. Koppal; Olivier Le Meur; John M. Shea; Eakta Jain",
    "corresponding_authors": "",
    "abstract": "Traditional comics are increasingly being augmented with digital effects, such as recoloring, stereoscopy, and animation. An open question in this endeavor is identifying where in a comic panel the effects should be placed. We propose a fast, semi-automatic technique to identify effects-worthy segments in a comic panel by utilizing gaze locations as a proxy for the importance of a region. We take advantage of the fact that comic artists influence viewer gaze towards narrative important regions. By capturing gaze locations from multiple viewers, we can identify important regions and direct a computer vision segmentation algorithm to extract these segments. The challenge is that these gaze data are noisy and difficult to process. Our key contribution is to leverage a theoretical breakthrough in the computer networks community towards robust and meaningful clustering of gaze locations into semantic regions, without needing the user to specify the number of clusters. We present a method based on the concept of relative eigen quality that takes a scanned comic image and a set of gaze points and produces an image segmentation. We demonstrate a variety of effects such as defocus, recoloring, stereoscopy, and animations. We also investigate the use of artificially generated gaze locations from saliency models in place of actual gaze locations.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2622080291",
    "type": "article"
  },
  {
    "title": "Multifeature Selection for 3D Human Action Recognition",
    "doi": "https://doi.org/10.1145/3177757",
    "publication_date": "2018-05-22",
    "publication_year": 2018,
    "authors": "Min Huang; Songzhi Su; Hongbo Zhang; Guorong Cai; Dongying Gong; Donglin Cao; Shaozi Li",
    "corresponding_authors": "",
    "abstract": "In mainstream approaches for 3D human action recognition, depth and skeleton features are combined to improve recognition accuracy. However, this strategy results in high feature dimensions and low discrimination due to redundant feature vectors. To solve this drawback, a multi-feature selection approach for 3D human action recognition is proposed in this paper. First, three novel single-modal features are proposed to describe depth appearance, depth motion, and skeleton motion. Second, a classification entropy of random forest is used to evaluate the discrimination of the depth appearance based features. Finally, one of the three features is selected to recognize the sample according to the discrimination evaluation. Experimental results show that the proposed multi-feature selection approach significantly outperforms other approaches based on single-modal feature and feature fusion.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2804587839",
    "type": "article"
  },
  {
    "title": "Efficient QoE-Aware Scheme for Video Quality Switching Operations in Dynamic Adaptive Streaming",
    "doi": "https://doi.org/10.1145/3269494",
    "publication_date": "2019-02-07",
    "publication_year": 2019,
    "authors": "Iheanyi Irondi; Qi Wang; Christos Grecos; José M. Alcaraz Calero; Pablo Casaseca‐de‐la‐Higuera",
    "corresponding_authors": "",
    "abstract": "Dynamic Adaptive Streaming over HTTP (DASH) is a popular over-the-top video content distribution technique that adapts the streaming session according to the user's network condition typically in terms of downlink bandwidth. This video quality adaptation can be achieved by scaling the frame quality, spatial resolution or frame rate. Despite the flexibility on the video quality scaling methods, each of these quality scaling dimensions has varying effects on the Quality of Experience (QoE) for end users. Furthermore, in video streaming, the changes in motion over time along with the scaling method employed have an influence on QoE, hence the need to carefully tailor scaling methods to suit streaming applications and content type. In this work, we investigate an intelligent DASH approach for the latest video coding standard H.265 and propose a heuristic QoE-aware cost-efficient adaptation scheme that does not switch unnecessarily to the highest quality level but rather stays temporarily at an intermediate quality level in certain streaming scenarios. Such an approach achieves a comparable and consistent level of quality under impaired network conditions as commonly found in Internet and mobile networks while reducing bandwidth requirements and quality switching overhead. The rationale is based on our empirical experiments, which show that an increase in bitrate does not necessarily mean noticeable improvement in QoE. Furthermore, our work demonstrates that the Signal-to-Noise Ratio (SNR) and the spatial resolution scalability types are the best fit for our proposed algorithm. Finally, we demonstrate an innovative interaction between quality scaling methods and the polarity of switching operations. The proposed QoE-aware scheme is implemented and empirical results show that it is able to reduce bandwidth requirements by up to 41% whilst achieving equivalent QoE compared with a representative DASH reference implementation.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2891373576",
    "type": "article"
  },
  {
    "title": "Detecting Online Counterfeit-goods Seller using Connection Discovery",
    "doi": "https://doi.org/10.1145/3311785",
    "publication_date": "2019-05-31",
    "publication_year": 2019,
    "authors": "Ming Cheung; James She; Weiwei Sun; Jiantao Zhou",
    "corresponding_authors": "",
    "abstract": "With the advancement of social media and mobile technology, any smartphone user can easily become a seller on social media and e-commerce platforms, such as Instagram and Carousell in Hong Kong or Taobao in China. A seller shows images of their products and annotates their images with suitable tags that can be searched easily by others. Those images could be taken by the seller, or the seller could use images shared by other sellers. Among sellers, some sell counterfeit goods, and these sellers may use disguising tags and language, which make detecting them a difficult task. This article proposes a framework to detect counterfeit sellers by using deep learning to discover connections among sellers from their shared images. Based on 473K shared images from Taobao, Instagram, and Carousell, it is proven that the proposed framework can detect counterfeit sellers. The framework is 30% better than approaches using object recognition in detecting counterfeit sellers. To the best of our knowledge, this is the first work to detect online counterfeit sellers from their shared images.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2950981278",
    "type": "article"
  },
  {
    "title": "Beauty Is in the Eye of the Beholder",
    "doi": "https://doi.org/10.1145/3328993",
    "publication_date": "2019-04-30",
    "publication_year": 2019,
    "authors": "Magzhan Kairanbay; John See; Lai-Kuan Wong",
    "corresponding_authors": "",
    "abstract": "Aesthetics is a subjective concept that is likely to be perceived differently among people of different ages, genders, and cultural backgrounds. While techniques that directly compute this concept in images has seen increasing attention by the multimedia and machine-learning community, there are very few attempts at encoding the influences from the photographer’s viewpoint. This work demonstrates how the aesthetic quality of photos can be better learned by accounting for the demographic background of a photographer. A new AVA-PD (Photographer Demographic) dataset is created to supplement the AVA dataset by providing photographers’ age, gender and location attributes. Two deep convolutional neural network (CNN) architectures are proposed to utilize demographic information for aesthetic prediction of photos; both are shown to yield better prediction capabilities compared to most existing approaches. By leveraging on AVA-PD meta-data, we also present some additional machine-learnable tasks such as identifying the photographer and predicting photography styles from a person’s gallery of photos.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2964039637",
    "type": "article"
  },
  {
    "title": "Learning Click-Based Deep Structure-Preserving Embeddings with Visual Attention",
    "doi": "https://doi.org/10.1145/3328994",
    "publication_date": "2019-08-08",
    "publication_year": 2019,
    "authors": "Yehao Li; Yingwei Pan; Ting Yao; Hongyang Chao; Yong Rui; Tao Mei",
    "corresponding_authors": "",
    "abstract": "One fundamental problem in image search is to learn the ranking functions (i.e., the similarity between query and image). Recent progress on this topic has evolved through two paradigms: the text-based model and image ranker learning. The former relies on image surrounding texts, making the similarity sensitive to the quality of textual descriptions. The latter may suffer from the robustness problem when human-labeled query-image pairs cannot represent user search intent precisely. We demonstrate in this article that the preceding two limitations can be well mitigated by learning a cross-view embedding that leverages click data. Specifically, a novel click-based Deep Structure-Preserving Embeddings with visual Attention (DSPEA) model is presented, which consists of two components: deep convolutional neural networks followed by image embedding layers for learning visual embedding, and a deep neural networks for generating query semantic embedding. Meanwhile, visual attention is incorporated at the top of the convolutional neural network to reflect the relevant regions of the image to the query. Furthermore, considering the high dimension of the query space, a new click-based representation on a query set is proposed for alleviating this sparsity problem. The whole network is end-to-end trained by optimizing a large margin objective that combines cross-view ranking constraints with in-view neighborhood structure preservation constraints. On a large-scale click-based image dataset with 11.7 million queries and 1 million images, our model is shown to be powerful for keyword-based image search with superior performance over several state-of-the-art methods and achieves, to date, the best reported NDCG@25 of 52.21%.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2967238391",
    "type": "article"
  },
  {
    "title": "Pseudo-3D Attention Transfer Network with Content-aware Strategy for Image Captioning",
    "doi": "https://doi.org/10.1145/3336495",
    "publication_date": "2019-08-08",
    "publication_year": 2019,
    "authors": "Jie Wu; Haifeng Hu; Liang Yang",
    "corresponding_authors": "",
    "abstract": "In this article, we propose a novel Pseudo-3D Attention Transfer network with Content-aware Strategy (P3DAT-CAS) for the image captioning task. Our model is composed of three parts: the Pseudo-3D Attention (P3DA) network, the P3DA-based Transfer (P3DAT) network, and the Content-aware Strategy (CAS). First, we propose P3DA to take full advantage of three-dimensional (3D) information in convolutional feature maps and capture more details. Most existing attention-based models only extract the 2D spatial representation from convolutional feature maps to decide which area should be paid more attention to. However, convolutional feature maps are 3D and different channel features can detect diverse semantic attributes associated with images. P3DA is proposed to combine 2D spatial maps with 1D semantic-channel attributes and generate more informative captions. Second, we design the transfer network to maintain and transfer the key previous attention information. The traditional attention-based approaches only utilize the current attention information to predict words directly, whereas transfer network is able to learn long-term attention dependencies and explore global modeling pattern. Finally, we present CAS to provide a more relevant and distinct caption for each image. The captioning model trained by maximum likelihood estimation may generate the captions that have a weak correlation with image contents, resulting in the cross-modal gap between vision and linguistics. However, CAS is helpful to convey the meaningful visual contents accurately. P3DAT-CAS is evaluated on Flickr30k and MSCOCO, and it achieves very competitive performance among the state-of-the-art models.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2969136062",
    "type": "article"
  },
  {
    "title": "A Unified Tensor-based Active Appearance Model",
    "doi": "https://doi.org/10.1145/3338841",
    "publication_date": "2019-10-15",
    "publication_year": 2019,
    "authors": "Zhenhua Feng; Josef Kittler; Bill Christmas; Xiao‐Jun Wu",
    "corresponding_authors": "",
    "abstract": "Appearance variations result in many difficulties in face image analysis. To deal with this challenge, we present a Unified Tensor-based Active Appearance Model (UT-AAM) for jointly modelling the geometry and texture information of 2D faces. For each type of face information, namely shape and texture, we construct a unified tensor model capturing all relevant appearance variations. This contrasts with the variation-specific models of the classical tensor AAM. To achieve the unification across pose variations, a strategy for dealing with self-occluded faces is proposed to obtain consistent shape and texture representations of pose-varied faces. In addition, our UT-AAM is capable of constructing the model from an incomplete training dataset, using tensor completion methods. Last, we use an effective cascaded-regression-based method for UT-AAM fitting. With these advancements, the utility of UT-AAM in practice is considerably enhanced. As an example, we demonstrate the improvements in training facial landmark detectors through the use of UT-AAM to synthesise a large number of virtual samples. Experimental results obtained on a number of well-known face datasets demonstrate the merits of the proposed approach.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2980958851",
    "type": "article"
  },
  {
    "title": "Autonomous Semantic Community Detection via Adaptively Weighted Low-rank Approximation",
    "doi": "https://doi.org/10.1145/3355393",
    "publication_date": "2019-11-15",
    "publication_year": 2019,
    "authors": "Liang Yang; Yuexue Wang; Junhua Gu; Xiaochun Cao; Xiao Wang; Di Jin; Guiguang Ding; Jungong Han; Weixiong Zhang",
    "corresponding_authors": "",
    "abstract": "Identification of semantic community structures is important for understanding the interactions and sentiments of different groups of people and predicting the social emotion. A robust community detection method needs to autonomously determine the number of communities and community structure for a given network. Nonnegative matrix factorization (NMF), a component decomposition approach for latent sentiment discovery, has been extensively used for community detection. However, the existing NMF-based methods require the number of communities to be determined a priori , limiting their applicability in practice of affective computing. Here, we develop a novel NMF-based method to autonomously determine the number of semantic communities and community structure simultaneously. In our method, we use an initial number of semantic communities, larger than the actual number, in the NMF formulation, and then suppress some of the communities by introducing an adaptively weighted group-sparse low-rank regularization to derive the target number of communities and at the same time the corresponding community structure. Our method not only maintains the efficiency without increasing the complexity compared to the original NMF method but also can be straightforwardly extended to handle the non-network data. We thoroughly examine the new method, showing its superior performance over several competing methods on synthetic and large real-world social networks.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3002379294",
    "type": "article"
  },
  {
    "title": "Visual Arts Search on Mobile Devices",
    "doi": "https://doi.org/10.1145/3326336",
    "publication_date": "2019-04-30",
    "publication_year": 2019,
    "authors": "Hui Mao; James She; Ming Cheung",
    "corresponding_authors": "",
    "abstract": "Visual arts, especially paintings, appear everywhere in our daily lives. They are not only liked by art lovers but also by ordinary people, both of whom are curious about the stories behind these artworks and also interested in exploring related artworks. Among various methods, the mobile visual search has its merits in providing an alternative solution to text and voice searches, which are not always applicable. Mobile visual search for visual arts is far more challenging than the general image visual search. Conventionally, visual search, such as searching products and plant, focuses on locating images containing similar objects. Hence, approaches are designed to locate objects and extract scale-invariant features from distorted photos that are captured by the mobile camera. However, the objects are only part of the visual art piece; the background and the painting style are both important factors that are not considered in the conventional approaches. In this article, an empirical investigation is conducted to study issues in photos taken by mobile cameras, such as orientation variance and motion blur, and how they influence the results of the mobile visual arts search. Based on the empirical investigation results, a photo-rectification pipeline is designed to rectify the photos into perfect images for feature extraction. A new method is proposed to learn high discriminative features for visual arts, which considers both the content information and style information in visual arts. Apart from conducting solid experiments, a real-world system is built to prove the effectiveness of the proposed methods. To the best of our knowledge, this is the first article to solve problems for visual arts search on mobile devices.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3007134661",
    "type": "article"
  },
  {
    "title": "Steganographer Detection via Multi-Scale Embedding Probability Estimation",
    "doi": "https://doi.org/10.1145/3352691",
    "publication_date": "2019-11-30",
    "publication_year": 2019,
    "authors": "Sheng-hua Zhong; Yuantian Wang; Tongwei Ren; Mingjie Zheng; Yan Liu; Gangshan Wu",
    "corresponding_authors": "",
    "abstract": "Steganographer detection aims to identify the guilty user who utilizes steganographic methods to hide secret information in the spread of multimedia data, especially image data, from a large amount of innocent users on social networks. A true embedding probability map illustrates the probability distribution of embedding secret information in the corresponding images by specific steganographic methods and settings, which has been successfully used as the guidance for content-adaptive steganographic and steganalytic methods. Unfortunately, in real-world situation, the detailed steganographic settings adopted by the guilty user cannot be known in advance. It thus becomes necessary to propose an automatic embedding probability estimation method. In this article, we propose a novel content-adaptive steganographer detection method via embedding probability estimation. The embedding probability estimation is first formulated as a learning-based saliency detection problem and the multi-scale estimated map is then integrated into the CNN to extract steganalytic features. Finally, the guilty user is detected via an efficient Gaussian vote method with the extracted steganalytic features. The experimental results prove that the proposed method is superior to the state-of-the-art methods in both spatial and frequency domains.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3012653625",
    "type": "article"
  },
  {
    "title": "Soul Dancer",
    "doi": "https://doi.org/10.1145/3340463",
    "publication_date": "2019-11-30",
    "publication_year": 2019,
    "authors": "Yuxin Hou; Hongxun Yao; Xiaoshuai Sun; Haoran Li",
    "corresponding_authors": "",
    "abstract": "Body language is one of the most common ways of expressing human emotion. In this article, we make the first attempt to generate an action video with a specific emotion from a single person image. The goal of the emotion-based action generation task (EBAG) is to generate action videos expressing a specific type of emotion given a single reference image with a full human body. We divide the task into two parts and propose a two-stage framework to generate action videos with specified emotions. At the first stage, we propose an emotion-based pose sequence generation approach (EPOSE-GAN) for translating the emotion to a pose sequence. At the second stage, we generate the target video frames according to the three inputs including the source pose and the target pose as the motion information and the source image as the appearance reference by using conditional GAN model with an online training strategy. Our framework produces the pose sequence and transforms the action independently, which highlights the fundamental role that the high-level pose feature plays in generating action video with a specific emotion. The proposed method has been evaluated on the “Soul Dancer” dataset which is built for action emotion analysis and generation. The experimental results demonstrate that our framework can effectively solve the emotion-based action generation task. However, the gap in the details of the appearance between the generated action video and the real-world video still exists, which indicates that the emotion-based action generation task has great research potential.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3014955389",
    "type": "article"
  },
  {
    "title": "FIN",
    "doi": "https://doi.org/10.1145/3381086",
    "publication_date": "2020-05-22",
    "publication_year": 2020,
    "authors": "Xiaofan Luo; Fukoeng Wong; Haifeng Hu",
    "corresponding_authors": "",
    "abstract": "Multi-layer detection is a widely used method in the field of object detection. It extracts multiple feature maps with different resolutions from the backbone network to detect objects of different scales, which can effectively cope with the problem of object scale change in object detection. Although the multi-layer detection utilizes multiple detection layers to alleviate the burden of one single detection layer and can improve the detection accuracy to some extent, this method has two limitations. First, manually assigning anchor boxes of different sizes to different feature maps is too dependent on the human experience. Second, there is a semantic gap between each detection layer in multi-layer detection. The same detector needs to simultaneously process the detection layers with inconsistent semantic strength, which increases the optimization difficulty of the detector. In this article, we propose a feature integrated network (FIN) based on single layer detection to deal with the problems mentioned above. Different from the existing methods, we design a series of verification experiments based on the multi-layer detection model, which shows that the shallow high-resolution feature map has the potential to simultaneously and effectively detect objects of various scales. Considering that the semantic information of the shallow feature map is weak, we propose two modules to enhance the representation ability of the single detection layer. First, we propose a detection adaptation network (DANet) to extract powerful feature maps that are useful for object detection tasks. Second, we combine global context information and local detail information with a verified hourglass module (VHM) to generate a single feature map with high resolution and rich semantic information so that we can assign all anchor boxes to this detection layer. In our model, all the detection operations are concentrated on a high-resolution feature map whose semantic information and detailed information are enhanced as much as possible. Therefore, the proposed model can solve the problem of anchor assignment and inconsistent semantic strength between multiple detection layers mentioned above. A large number of experiments on the Pattern Analysis, Statistical Modelling and Computational Learning Visual Object Classes (PASCAL VOC) and Microsoft Common Objects in Context (MS COCO) datasets show that our model has good detection performance for objects of various sizes. The proposed model can achieve&lt;?brk?&gt; 81.9 mAP when the size of the input image is 300 × 300.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3028729812",
    "type": "article"
  },
  {
    "title": "Upgrading the Newsroom",
    "doi": "https://doi.org/10.1145/3396520",
    "publication_date": "2020-07-05",
    "publication_year": 2020,
    "authors": "Fangyu Liu; Rémi Lebret; Didier Orel; Philippe Sordet; Karl Aberer",
    "corresponding_authors": "",
    "abstract": "We propose an automated image selection system to assist photo editors in selecting suitable images for news articles. The system fuses multiple textual sources extracted from news articles and accepts multilingual inputs. It is equipped with char-level word embeddings to help both modeling morphologically rich languages, e.g., German, and transferring knowledge across nearby languages. The text encoder adopts a hierarchical self-attention mechanism to attend more to both key words within a piece of text and informative components of a news article. We extensively experiment our system on a large-scale text-image database containing multimodal multilingual news articles collected from Swiss local news media websites. The system is compared with multiple baselines with ablation studies and is shown to beat existing text-image retrieval methods in a weakly supervised learning setting. Besides, we also offer insights on the advantage of using multiple textual sources and multilingual data.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3038649929",
    "type": "article"
  },
  {
    "title": "Cross-Domain Brain CT Image Smart Segmentation via Shared Hidden Space Transfer FCM Clustering",
    "doi": "https://doi.org/10.1145/3357233",
    "publication_date": "2020-04-30",
    "publication_year": 2020,
    "authors": "Kaijian Xia; Hongsheng Yin; Yong Jin; Shi Qiu; Hongru Zhao",
    "corresponding_authors": "",
    "abstract": "Clustering is an important issue in brain medical image segmentation. Original medical images used for clinical diagnosis are often insufficient for clustering in the current domain. As there are sufficient medical images in the related domains, transfer clustering can improve the clustering performance of the current domain by transferring knowledge across the related domains. In this article, we propose a novel shared hidden space transfer fuzzy c- means (FCM) clustering called SHST-FCM for cross-domain brain computed tomography (CT) image segmentation. SHST-FCM projects both the data samples of the source domain and target domain into the shared hidden space, such that the distributions of the two domains are as close as possible. In the learned shared subspace, the data samples of the source domain serve as the auxiliary knowledge to aid the clustering process in the target domain. Extensive experiments on brain CT medical image datasets indicate the effectiveness of the proposed method.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3078669760",
    "type": "article"
  },
  {
    "title": "Social-sensed Image Aesthetics Assessment",
    "doi": "https://doi.org/10.1145/3414843",
    "publication_date": "2020-10-31",
    "publication_year": 2020,
    "authors": "Chaoran Cui; Peiguang Lin; Xiushan Nie; Muwei Jian; Yilong Yin",
    "corresponding_authors": "",
    "abstract": "Image aesthetics assessment aims to endow computers with the ability to judge the aesthetic values of images, and its potential has been recognized in a variety of applications. Most previous studies perform aesthetics assessment purely based on image content. However, given the fact that aesthetic perceiving is a human cognitive activity, it is necessary to consider users’ perception of an image when judging its aesthetic quality. In this article, we regard users’ social behavior as the reflection of their perception of images and harness these additional clues to improve image aesthetics assessment. Specifically, we first merge the raw social interactions between users and images into clusters as the social labels of images, so the collective social behavioral information associated with an image can be well represented over a structured and compact space. Then, we develop a novel deep multi-task network to jointly learn social labels in different modalities from social images and apply it to common web images. In this manner, our approach is readily generalized to web images without social behavioral information. Finally, we introduce a high-level fusion sub-network to the aesthetics model, in which the social and visual representations of images are well balanced for aesthetics assessment. Experimental results on two benchmark datasets well verify the effectiveness of our approach and highlight the benefits of different types of social behavioral information for image aesthetics assessment.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3114087940",
    "type": "article"
  },
  {
    "title": "Residual-guided In-loop Filter Using Convolution Neural Network",
    "doi": "https://doi.org/10.1145/3460820",
    "publication_date": "2021-11-12",
    "publication_year": 2021,
    "authors": "Wei Jia; Li Li; Zhu Li; Xiang Zhang; Shan Liu",
    "corresponding_authors": "",
    "abstract": "The block-based coding structure in the hybrid video coding framework inevitably introduces compression artifacts such as blocking, ringing, and so on. To compensate for those artifacts, extensive filtering techniques were proposed in the loop of video codecs, which are capable of boosting the subjective and objective qualities of reconstructed videos. Recently, neural network-based filters were presented with the power of deep learning from a large magnitude of data. Though the coding efficiency has been improved from traditional methods in High-Efficiency Video Coding (HEVC), the rich features and information generated by the compression pipeline have not been fully utilized in the design of neural networks. Therefore, in this article, we propose the Residual-Reconstruction-based Convolutional Neural Network (RRNet) to further improve the coding efficiency to its full extent, where the compression features induced from bitstream in form of prediction residual are fed into the network as an additional input to the reconstructed frame. In essence, the residual signal can provide valuable information about block partitions and can aid reconstruction of edge and texture regions in a picture. Thus, more adaptive parameters can be trained to handle different texture characteristics. The experimental results show that our proposed RRNet approach presents significant BD-rate savings compared to HEVC and the state-of-the-art CNN-based schemes, indicating that residual signal plays a significant role in enhancing video frame reconstruction.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3211930001",
    "type": "article"
  },
  {
    "title": "Multiview Image Block Compressive Sensing with Joint Multiphase Decoding for Visual Sensor Network",
    "doi": "https://doi.org/10.1145/2818712",
    "publication_date": "2015-10-20",
    "publication_year": 2015,
    "authors": "Mansoor Ebrahim; Wai Chong Chia",
    "corresponding_authors": "",
    "abstract": "In this article, a multiview image compression framework, which involves the use of Block-based Compressive Sensing (BCS) and Joint Multiphase Decoding (JMD), is proposed for a Visual Sensor Network (VSN). In the proposed framework, one of the sensor nodes is configured to serve as the reference node, the others as nonreference nodes. The images are encoded independently using the BCS to produce two observed measurements that are transmitted to the host workstation. In this case, the nonreference nodes always encoded the images (I NR ) at a lower subrate when compared with the images from the reference nodes (I R ). The idea is to improve the reconstruction of I NR using I R . After the two observed measurements are received by the host workstation, they are first decoded independently, then image registration is applied to align I R onto the same plane of I NR . The aligned I R is then fused with I NR , using wavelets to produce the projected image I P . Subsequently, the difference between the measurements of the I P and I NR is calculated. The difference is then decoded and added to I P to produce the final reconstructed I NR . The simulation results show that the proposed framework is able to improve the quality of I NR on average by 2dB to 3dB at lower subrates when compared with other Compressive Sensing (CS)--based multiview image compression frameworks.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W1988077085",
    "type": "article"
  },
  {
    "title": "DIP",
    "doi": "https://doi.org/10.1145/2568223",
    "publication_date": "2014-04-01",
    "publication_year": 2014,
    "authors": "Rossano Gaeta; Marco Grangetto; Lorenzo Bovio",
    "corresponding_authors": "",
    "abstract": "Peer-to-peer live streaming applications are vulnerable to malicious actions of peers that deliberately modify data to decrease or prevent the fruition of the media (pollution attack). In this article we propose DIP , a fully distributed, accurate, and robust algorithm for the identification of polluters. DIP relies on checks that are computed by peers upon completing reception of all blocks composing a data chunk. A check is a special message that contains the set of peer identifiers that provided blocks of the chunk as well as a bit to signal if the chunk has been corrupted. Checks are periodically transmitted by peers to their neighbors in the overlay network; peers receiving checks use them to maintain a factor graph. This graph is bipartite and an incremental belief propagation algorithm is run on it to compute the probability of a peer being a polluter. Using a prototype deployed over PlanetLab we show by extensive experimentation that DIP allows honest peers to identify polluters with very high accuracy and completeness, even when polluters collude to deceive them. Furthermore, we show that DIP is efficient, requiring low computational, communication, and storage overhead at each peer.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W1989962378",
    "type": "article"
  },
  {
    "title": "PROPANE",
    "doi": "https://doi.org/10.1145/2602222",
    "publication_date": "2014-08-01",
    "publication_year": 2014,
    "authors": "Richard W. Pazzi; Azzedine Boukerche",
    "corresponding_authors": "",
    "abstract": "Image-Based Rendering (IBR) has become widely known by its relatively low requirements for generating new scenes based on a sequence of reference images. This characteristic of IBR shows a remarkable potential impact in rendering complex 3D virtual environments on graphics-constrained devices, such as head-mounted displays, set-top boxes, media streaming devices, and so on. If well exploited, IBR coupled with remote rendering would enable the exploration of complex virtual environments on these devices. However, remote rendering requires the transmission of a large volume of images. In addition, existing solutions consider limited and/or deterministic navigation schemes as a means of decreasing the volume of streamed data. This article proposes the PRO gressive PAN orama Str E aming protocol (PROPANE) to offer users a smoother virtual navigation experience by prestreaming the imagery data required to generate new views as the user wanders within a 3D environment. PROPANE is based on a very simple yet effective trigonometry model and uses a strafe (lateral movement) technique to minimize the delay between image updates at the client end. This article introduces the concept of key partial panoramas, namely panorama segments that cover movements in any direction by simply strafing from an appropriate key partial panorama and streaming the amount of lost pixels. Therefore, PROPANE can provide a constrained device with sufficient imagery data to cover a future user's viewpoints, thereby minimizing the impact of transmission delay and jitter. PROPANE has been implemented and compared to two baseline remote rendering schemes. The evaluation results show that the proposed technique outperforms the selected and closely related existing schemes by minimizing the response time while not limiting the user to predefined paths as opposed to previous protocols.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W1997601956",
    "type": "article"
  },
  {
    "title": "Smartening Up the Student Learning Experience with Ubiquitous Media",
    "doi": "https://doi.org/10.1145/2808203",
    "publication_date": "2015-10-21",
    "publication_year": 2015,
    "authors": "Diana Bental; Eliza Papadopoulou; N. Taylor; M. Howard Williams; Fraser R. Blackmun; Idris S. Ibrahim; Mei Yii Lim; Ioannis Mimtsoudis; Stuart Whyte; Edel Jennings",
    "corresponding_authors": "",
    "abstract": "This article describes how an experimental platform for social, mobile and ubiquitous computing has been used in a wide-ranging longitudinal “in the wild” case study of the platform with a set of third-party services. The article outlines some of the relevant aspects of the platform, including built-in support for community formation, for context sensitivity, automated learning and adaptation to the user, and for management of privacy and trust relationships. The platform architecture is based on the notion of Cooperating Smart Spaces (CSSs), where a CSS is a partition of the platform corresponding to a single user and distributed over the devices belonging to that user. Three of the case study services were intended for use in a physical environment specifically created to support ubiquitous intelligence; they were highly interactive and used shared screens, voice input and gestural interaction. Another three ubiquitous services were available throughout the university environment as mobile and desktop services. The case study exploited this architecture's ability to integrate multiple novel applications and interface devices and to deliver them flexibly in these different environments. The platform proved to be stable and reliable and the study shows that treating a provider of services and resources (the University) as a CSS is instrumental in enabling the platform to provide this range of services across differing environments.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2049523797",
    "type": "article"
  },
  {
    "title": "Scalable multimedia content analysis on parallel platforms using python",
    "doi": "https://doi.org/10.1145/2517151",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "Ekaterina Gonina; Gerald Friedland; Eric Battenberg; Penporn Koanantakool; Michael Driscoll; Evangelos Georganas; Kurt Keutzer",
    "corresponding_authors": "",
    "abstract": "In this new era dominated by consumer-produced media there is a high demand for web-scalable solutions to multimedia content analysis. A compelling approach to making applications scalable is to explicitly map their computation onto parallel platforms. However, developing efficient parallel implementations and fully utilizing the available resources remains a challenge due to the increased code complexity, limited portability and required low-level knowledge of the underlying hardware. In this article, we present PyCASP, a Python-based framework that automatically maps computation onto parallel platforms from Python application code to a variety of parallel platforms. PyCASP is designed using a systematic, pattern-oriented approach to offer a single software development environment for multimedia content analysis applications. Using PyCASP, applications can be prototyped in a couple hundred lines of Python code and automatically scale to modern parallel processors. Applications written with PyCASP are portable to a variety of parallel platforms and efficiently scale from a single desktop Graphics Processing Unit (GPU) to an entire cluster with a small change to application code. To illustrate our approach, we present three multimedia content analysis applications that use our framework: a state-of-the-art speaker diarization application, a content-based music recommendation system based on the Million Song Dataset, and a video event detection system for consumer-produced videos. We show that across this wide range of applications, our approach achieves the goal of automatic portability and scalability while at the same time allowing easy prototyping in a high-level language and efficient performance of low-level optimized code.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2052970639",
    "type": "article"
  },
  {
    "title": "Decoder-Complexity-Aware Encoding of Motion Compensation for Multiple Heterogeneous Receivers",
    "doi": "https://doi.org/10.1145/2700300",
    "publication_date": "2015-02-24",
    "publication_year": 2015,
    "authors": "Mohsen Jamali Langroodi; Joseph G. Peters; Shervin Shirmohammadi",
    "corresponding_authors": "",
    "abstract": "For mobile multimedia systems, advances in battery technology have been much slower than those in memory, graphics, and processing power, making power consumption a major concern in mobile systems. The computational complexity of video codecs, which consists of CPU operations and memory accesses, is one of the main factors affecting power consumption. In this article, we propose a method that achieves near-optimal video quality while respecting user-defined bounds on the complexity needed to decode a video. We specifically focus on the motion compensation process, including motion vector prediction and interpolation, because it is the single largest component of computation-based power consumption. We start by formulating a scenario with a single receiver as a rate-distortion optimization problem and we develop an efficient decoder-complexity-aware video encoding method to solve it. Then we extend our approach to handle multiple heterogeneous receivers, each with a different complexity requirement. We test our method experimentally using the H.264 standard for the single receiver scenario and the H.264 SVC extension for the multiple receiver scenario. Our experimental results show that our method can achieve up to 97% of the optimal solution value in the single receiver scenario, and an average of 97% of the optimal solution value in the multiple receiver scenario. Furthermore, our tests with actual power measurements show a power saving of up to 23% at the decoder when the complexity threshold is halved in the encoder.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2167667873",
    "type": "article"
  },
  {
    "title": "Segmentation of Discriminative Patches in Human Activity Video",
    "doi": "https://doi.org/10.1145/2750780",
    "publication_date": "2015-08-24",
    "publication_year": 2015,
    "authors": "Bo Zhang; Nicola Conci; Francesco G. B. De Natale",
    "corresponding_authors": "",
    "abstract": "In this article, we present a novel approach to segment discriminative patches in human activity videos. First, we adopt the spatio-temporal interest points (STIPs) to represent significant motion patterns in the video sequence. Then, nonnegative sparse coding is exploited to generate a sparse representation of each STIP descriptor. We construct the feature vector for each video by applying a two-stage sum-pooling and l 2 -normalization operation. After training a multi-class classifier through the error-correcting code SVM, the discriminative portion of each video is determined as the patch that has the highest confidence while also being correctly classified according to the video category. Experimental results show that the video patches extracted by our method are more separable, while preserving the perceptually relevant portion of each activity.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2230807910",
    "type": "article"
  },
  {
    "title": "Link-Aware Reconfigurable Point-to-Point Video Streaming for Mobile Devices",
    "doi": "https://doi.org/10.1145/2771438",
    "publication_date": "2015-08-24",
    "publication_year": 2015,
    "authors": "Suk Kyu Lee; Seungho Yoo; Jongtack Jung; Hwangnam Kim; Jihoon Ryoo",
    "corresponding_authors": "",
    "abstract": "Even though people of all social standings use current mobile devices in the wide spectrum of purpose from entertainment tools to communication means, some issues with real-time video streaming in hostile wireless environment still exist. In this article, we introduce CoSA , a link-aware real-time video streaming system for mobile devices. The proposed system utilizes a 3D camera to distinguish the region of importance (ROI) and non-ROI region within the video frame. Based on the link-state feedback from the receiver, the proposed system allocates a higher bandwidth for the region that is classified as ROI and a lower bandwidth for non-ROI in the video stream by reducing the video's bit rate. We implemented CoSA in a real test-bed where the IEEE 802.11 is employed as a medium for wireless networking. Furthermore, we verified the effectiveness of the proposed system by conducting a thorough empirical study. The results indicate that the proposed system enables real-time video streaming while maintaining a consistent visual quality by dynamically reconfiguring video coding parameters according to the link quality.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2233502960",
    "type": "article"
  },
  {
    "title": "Trustworthy Authentication on Scalable Surveillance Video with Background Model Support",
    "doi": "https://doi.org/10.1145/2978573",
    "publication_date": "2016-09-15",
    "publication_year": 2016,
    "authors": "Zhuo Wei; Zheng Yan; Yongdong Wu; Robert H. Deng",
    "corresponding_authors": "",
    "abstract": "H.264/SVC (Scalable Video Coding) codestreams, which consist of a single base layer and multiple enhancement layers, are designed for quality, spatial, and temporal scalabilities. They can be transmitted over networks of different bandwidths and seamlessly accessed by various terminal devices. With a huge amount of video surveillance and various devices becoming an integral part of the security infrastructure, the industry is currently starting to use the SVC standard to process digital video for surveillance applications such that clients with different network bandwidth connections and display capabilities can seamlessly access various SVC surveillance (sub)codestreams. In order to guarantee the trustworthiness and integrity of received SVC codestreams, engineers and researchers have proposed several authentication schemes to protect video data. However, existing algorithms cannot simultaneously satisfy both efficiency and robustness for SVC surveillance codestreams. Hence, in this article, a highly efficient and robust authentication scheme, named TrustSSV (Trust Scalable Surveillance Video), is proposed. Based on quality/spatial scalable characteristics of SVC codestreams, TrustSSV combines cryptographic and content-based authentication techniques to authenticate the base layer and enhancement layers, respectively. Based on temporal scalable characteristics of surveillance codestreams, TrustSSV extracts, updates, and authenticates foreground features for each access unit dynamically with background model support. Using SVC test sequences, our experimental results indicate that the scheme is able to distinguish between content-preserving and content-changing manipulations and to pinpoint tampered locations. Compared with existing schemes, the proposed scheme incurs very small computation and communication costs.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2519067850",
    "type": "article"
  },
  {
    "title": "Evaluating the Privacy Risk of User-Shared Images",
    "doi": "https://doi.org/10.1145/2978568",
    "publication_date": "2016-09-15",
    "publication_year": 2016,
    "authors": "Ming Cheung; James She",
    "corresponding_authors": "",
    "abstract": "User-shared images are shared on social media about a user’s life and interests that are widely accessible to others due to their sharing nature. Unlike for online profiles and social graphs, most users are unaware of the privacy risks relating to shared images, as they do not directly disclose characteristics such as gender and origin. Recently, however, user-shared images have been proven to be an accessible alternative to social graphs for online friendship recommendation and gender identification. This article evaluates 1.6M user-shared images from an image-oriented social network, Fotolog, and concludes how they can create privacy risks by proposing a system for de-anonymization, as well as inferring information on online profiles with the user-shared images. It is concluded that given user-shared images, using social graphs is 2 and 2.5 times more effective in de-anonymization than using origins or genders. With two showcases, it is also proven that using user-shared images is effective in online friendship recommendation, gender identification, and origin inference. To the best of our knowledge, this is the first article to evaluate the privacy issue qualitatively with big multimedia data from a real social network.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2519658420",
    "type": "article"
  },
  {
    "title": "Mobile Video Streaming over Dynamic Single-Frequency Networks",
    "doi": "https://doi.org/10.1145/2983635",
    "publication_date": "2016-11-02",
    "publication_year": 2016,
    "authors": "Saleh Almowuena; Mohamed Hefeeda",
    "corresponding_authors": "",
    "abstract": "The demand for multimedia streaming over mobile networks has been steadily increasing over the past several years. For instance, it has become common for mobile users to stream full TV episodes, sports events, and movies while on the go. Unfortunately, this growth in demand has strained the wireless networks despite the significant increase of their capacities with recent generations. Hence, efficient utilization of the expensive and limited wireless spectrum remains an important problem, especially in the context of multimedia streaming services that consume a large portion of the bandwidth capacity. In this article, we introduce the idea of dynamically configuring cells in wireless cellular networks to form single-frequency networks based on the multimedia traffic demands from users in each cell. We formulate the resource allocation problem in such complex networks with the goal of maximizing the number of served multimedia streams, and we prove that this problem is NP-Complete. Then we present an optimal solution to maximize the number of served multimedia streams within a cellular network. This optimal solution, however, may suffer from an exponential time complexity in the worst case, which is not practical for real-time streaming over large-scale networks. Therefore, we propose a heuristic algorithm with polynomial running time to provide faster and more practical solution for real-time deployments. Through detailed packet-level simulations, we assess the performance of the proposed algorithms with respect to the average service ratio, energy saving, video quality, frame loss rate, initial buffering time, rate of re-buffering events, and bandwidth overhead. We show that the proposed algorithms achieve substantial improvements in all of these performance metrics compared to the state-of-the-art approaches. For example, for the service ratio metric, our algorithms can serve up to 11 times more users compared to the unicast approach, and they achieve up to 54% improvement over the closest multicast approaches in the literature.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2546620535",
    "type": "article"
  },
  {
    "title": "Seeing Crucial Parts: Vehicle Model Verification via a Discriminative Representation Model",
    "doi": "https://doi.org/10.1145/3474596",
    "publication_date": "2022-01-25",
    "publication_year": 2022,
    "authors": "Liqian Liang; Congyan Lang; Zun Li; Jian Zhao; Tao Wang; Songhe Feng",
    "corresponding_authors": "",
    "abstract": "Widely used surveillance cameras have promoted large amounts of street scene data, which contains one important but long-neglected object: the vehicle. Here we focus on the challenging problem of vehicle model verification. Most previous works usually employ global features (e.g., fully connected features) to further perform vehicle-level deep metric learning (e.g., triplet-based network). However, we argue that it is noteworthy to investigate the distinctiveness of local features and consider vehicle-part-level metric learning by reducing the intra-class variance as much as possible. In this article, we introduce a simple yet powerful deep model—the enforced intra-class alignment network (EIA-Net)—which can learn a more discriminative image representation by localizing key vehicle parts and jointly incorporating two distance metrics: vehicle-level embedding and vehicle-part-sensitive embedding. For learning features, we propose an effective feature extraction module that is composed of two components: the regional proposal network (RPN)-based network and part-based CNN. The RPN is used to define key vehicle regions and aggregate local features on these regions, whereas part-based CNN offers supplementary global features for the RPN-based network. The fusion features learned by feature extraction module are cast into the deep metric learning module. Especially, we derived an enforced intra-class alignment loss by re-utilizing key vehicle part information to enhance reducing intra-class variance. Furthermore, we modify the coupled cluster loss to model the vehicle-level embedding by enlarging the inter-class variance while shortening intra-class variance. Extensive experiments over benchmark datasets VehicleID and CompCars have shown that the proposed EIA-Net significantly outperforms the state-of-the-art approaches for vehicle model verification. Furthermore, we also conduct comprehensive experiments on vehicle re-identification datasets (i.e., VehicleID and VeRi776) to validate the generalization ability effectiveness of our proposed method.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4207072681",
    "type": "article"
  },
  {
    "title": "Privacy-preserving Motion Detection for HEVC-compressed Surveillance Video",
    "doi": "https://doi.org/10.1145/3472669",
    "publication_date": "2022-01-27",
    "publication_year": 2022,
    "authors": "Changming Liu; Xiaojing Ma; Sixing Cao; Jiayun Fu; Bin Zhu",
    "corresponding_authors": "",
    "abstract": "In the cloud era, a large amount of data is uploaded to and processed by public clouds. The risk of privacy leakage has become a major concern for cloud users. Cloud-based video surveillance requires motion detection, which may reveal the privacy of people in a surveillance video. Privacy-preserving video surveillance allows motion detection while protecting privacy. The existing scheme [ 25 ], designed to detect motion on encrypted and H.264-compressed surveillance videos, does not work well on more advanced video compression schemes such as HEVC. In this article, we propose the first motion detection method on encrypted and HEVC-compressed videos. It adopts a novel approach that exploits inter-prediction reference relationships among coding blocks to detect motion regions. The partition pattern and the number of coding bits of each detection block used in prior art are also used to help detect motion regions. Spatial and temporal consistency of a moving object and Kalman filtering are applied to segment connected/merged motion regions, remove noise and background motions, and refine trajectories and shapes of detected moving objects. Experimental results indicate that our detection method achieves high detection recall, precision, and F1-score for surveillance videos of both high and low resolutions with various scenes. It has a similarly high detection accuracy on encrypted and HEVC-compressed videos as that of the existing motion detection method [ 25 ] on encrypted and H.264-compressed videos. Our proposed method incurs no bit-rate overhead and has a very low computational complexity for both motion detection and encryption of HEVC videos.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4210271954",
    "type": "article"
  },
  {
    "title": "Defining Scents: A Systematic Literature Review of Olfactory-based Computing Systems",
    "doi": "https://doi.org/10.1145/3470975",
    "publication_date": "2022-01-27",
    "publication_year": 2022,
    "authors": "Amanda Holloman; Chris Crawford",
    "corresponding_authors": "",
    "abstract": "The human sense of smell is a primal ability that has the potential to reveal unexplored relationships between user behaviors and technology. Humans use millions of olfactory receptor cells to observe the environment around them. Olfaction studies are gaining popularity with the progression of scent delivering (commercial and prototype) devices. This influx of research features various software and hardware designs. Additionally, previous studies have explored numerous target audiences and evaluation methodologies. This article presents a systematic review of pertinent literature that investigates olfactory-based computing (OBC) systems in the field of Human-Computer Interaction. Last, this article highlights state-of-the-art study/system designs, evaluation methods, and offers insights on ways to address current challenges/contributions relevant to OBC technologies.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4210345114",
    "type": "article"
  },
  {
    "title": "Optimizing Immersive Video Coding Configurations Using Deep Learning: A Case Study on TMIV",
    "doi": "https://doi.org/10.1145/3471191",
    "publication_date": "2022-01-27",
    "publication_year": 2022,
    "authors": "Chih-Fan Hsu; Tse-Hou Hung; Cheng-Hsin Hsu",
    "corresponding_authors": "",
    "abstract": "Immersive video streaming technologies improve Virtual Reality (VR) user experience by providing users more intuitive ways to move in simulated worlds, e.g., with 6 Degree-of-Freedom (6DoF) interaction mode. A naive method to achieve 6DoF is deploying cameras at numerous different positions and orientations that may be required based on users’ movement, which unfortunately is expensive, tedious, and inefficient. A better solution for realizing 6DoF interactions is to synthesize target views on-the-fly from a limited number of source views. While such view synthesis is enabled by the recent Test Model for Immersive Video (TMIV) codec, TMIV dictates manually-composed configurations, which cannot exercise the tradeoff among video quality, decoding time, and bandwidth consumption. In this article, we study the limitation of TMIV and solve its configuration optimization problem by searching for the optimal configuration in a huge configuration space. We first identify the critical parameters in the TMIV configurations. Then, we introduce two Neural Network (NN) -based algorithms from two heterogeneous aspects: (i) a Convolutional Neural Network (CNN) algorithm solving a regression problem and (ii) a Deep Reinforcement Learning (DRL) algorithm solving a decision making problem, respectively. We conduct both objective and subjective experiments to evaluate the CNN and DRL algorithms on two diverse datasets: an equirectangular and a perspective projection dataset. The objective evaluations reveal that both algorithms significantly outperform the default configurations. In particular, with the equirectangular (perspective) projection dataset, the proposed algorithms only require 95% (23%) decoding time, stream 79% (23%) views, and improve the utility by 6% (73%) on average. The subjective evaluations confirm the proposed algorithms consume fewer resources while achieving comparable Quality of Experience (QoE) than the default and the optimal TMIV configurations.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4210446517",
    "type": "article"
  },
  {
    "title": "Image-Based Personality Questionnaire Design",
    "doi": "https://doi.org/10.1145/3503489",
    "publication_date": "2022-03-26",
    "publication_year": 2022,
    "authors": "Xiaowen Huang; Jitao Sang; Changsheng Xu",
    "corresponding_authors": "",
    "abstract": "This article explores the problem of image-based personality questionnaire design. Compared with the traditional text-based personality questionnaire, the image-based personality questionnaire is more natural, truthful, and language insensitive. Instead of responding to textual questions, the subjects are provided a set of “choose-your-favorite-image” visual questions. With each question, consisting of image options describing the same semantic concept, the subjects are requested to choose their favorite image. Based on responses to typically 15 to 25 questions, we can accurately estimate the subjects’ personality traits in five dimensions. The solution to design such an image-based personality questionnaire consists of concept-question identification and image-option selection. We have presented a preliminary framework to regularize these two steps in this exploratory study. A demo automatically adapting between desktop and mobile devices is available at http://120.27.209.14/vbfi . Subjective and objective evaluations have demonstrated the feasibility of accurately estimating a subject’s personality in a limited round of questions.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4220746003",
    "type": "article"
  },
  {
    "title": "Boosting Vision-and-Language Navigation with Direction Guiding and Backtracing",
    "doi": "https://doi.org/10.1145/3526024",
    "publication_date": "2022-03-17",
    "publication_year": 2022,
    "authors": "Jingwen Chen; Jianjie Luo; Yingwei Pan; Yehao Li; Ting Yao; Hongyang Chao; Tao Mei",
    "corresponding_authors": "",
    "abstract": "Vision-and-Language Navigation (VLN) has been an emerging and fast-developing research topic, where an embodied agent is required to navigate in a real-world environment based on natural language instructions. In this article, we present a Direction-guided Navigator Agent (DNA) that novelly integrates direction clues derived from instructions into the essential encoder-decoder navigation framework. Particularly, DNA couples the standard instruction encoder with an additional direction branch which sequentially encodes the direction clues in the instructions to boost navigation. Furthermore, an Instruction Flipping mechanism is uniquely devised to enable fast data augmentation as well as a follow-up backtracing for navigating the agent in a backward direction. Such a way naturally amplifies the grounding of instruction in the local visual scenes along both forward and backward directions, and thus strengthens the alignment between instruction and action sequence. Extensive experiments conducted on Room to Room (R2R) dataset validate our proposal and demonstrate quantitatively compelling results.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4220843948",
    "type": "article"
  },
  {
    "title": "Opportunistic Transmission for Video Streaming over Wild Internet",
    "doi": "https://doi.org/10.1145/3488722",
    "publication_date": "2022-03-12",
    "publication_year": 2022,
    "authors": "Jiawei Huang; Qichen Su; Weihe Li; Zhuoran Liu; Tao Zhang; Sen Liu; Ping Zhong; Wanchun Jiang; Jianxin Wang",
    "corresponding_authors": "",
    "abstract": "The video streaming system employs adaptive bitrate (ABR) algorithms to optimize a user’s quality of experience. However, it is hard for ABR algorithms to choose the right bitrate consistently under highly dynamic bandwidth fluctuations in wild Internet. In this article, we propose a building block on the client side named Opportunistic Chunk Replacement Mechanism (OCRM) to help existing ABR algorithms make full use of the available bandwidth to improve the network utilization and viewing experience of users. Specifically, the servers take advantages of the spare bandwidth to opportunistically transmit high-quality chunks (called opportunistic chunks ) with low priority to the client, without incurring any extra delay. Then, the client player replaces the low-quality chunks with the opportunistic ones that have high quality. We compare OCRM with state-of-the-art ABR algorithms by using trace-driven experiments spanning a wide variety of quality of experience metrics and network conditions. The test results show that OCRM effectively achieves high network utilization and improves the user’s viewing experience by up to 35%.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4220886777",
    "type": "article"
  },
  {
    "title": "A Novel Reversible Data Hiding Scheme Based on Pixel-Residual Histogram",
    "doi": "https://doi.org/10.1145/3534565",
    "publication_date": "2022-05-12",
    "publication_year": 2022,
    "authors": "Mengyao Xiao; Xiaolong Li; Yao Zhao; Bin Ma; Guodong Guo",
    "corresponding_authors": "",
    "abstract": "Prediction-error expansion (PEE) is the most popular reversible data hiding (RDH) technique due to its efficient capacity-distortion tradeoff. With the generated prediction-error histogram (PEH) and adaptively selected expansion bins, the image redundancy is well exploited by PEE. However, for the most widely used rhombus predictor, the rounding operation which groups different prediction-errors into one value is completely unnecessary. The embedding can be extended to a general case by removing the rounding operation, and more histogram bins can be derived for expansion with a new mapping mechanism. Therefore, in this article, instead of pixel prediction-error, we propose to compute the pixel residuals without the rounding operation, and a new embedding mechanism based on pixel-residual histogram (PRH) modification is devised. In PRH, four bins correspond to one bin in PEH. Then, different from the one-to-one mapping between the prediction-error and pixel modification, a four-to-one mapping between the pixel-residual and pixel modification is established, and the performance is optimized by adaptively selecting four expansion bin pairs for embedding. Since more modification selections are considered, better performance can be obtained. Moreover, the proposed scheme is extended to the two-dimensional (2D) histogram and multiple histograms based embedding, and the performance is further enhanced. The superiority of the proposed method is experimentally verified by comparing it with some state-of-the-art works.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4280551073",
    "type": "article"
  },
  {
    "title": "3D Skeleton and Two Streams Approach to Person Re-identification Using Optimized Region Matching",
    "doi": "https://doi.org/10.1145/3538490",
    "publication_date": "2022-05-25",
    "publication_year": 2022,
    "authors": "Qing Han; Huiting Liu; Weidong Min; Tiemei Huang; Deyu Lin; Qi Wang",
    "corresponding_authors": "",
    "abstract": "Person re-identification (Re-ID) is a challenging and arduous task due to non-overlapping views, complex background, and uncontrollable occlusion in video surveillance. An existing method for capturing pedestrian local region information is to divide person regions into horizontal stripes, which may lead to invalid features and erroneous learning. To solve this problem, this paper proposes a 3D skeleton and a two-stream approach to person Re-ID. The first stream of the method uses the 3D skeleton for background filtering and region segmentation. The second stream uses Siamese net to extract the global descriptor. The features of the two streams are fused to preserve the integrity of the person. An optimized region matching method for metric learning is designed. Extensive comparing experiments were conducted with state-of-the-art Re-ID methods on the Market-1501, CUHK03, and DukeMTMC-reID datasets. Experimental results show that the proposed method outperforms the existing methods in recognition accuracy.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4281488254",
    "type": "article"
  },
  {
    "title": "Context Prior Guided Semantic Modeling for Biomedical Image Segmentation",
    "doi": "https://doi.org/10.1145/3558520",
    "publication_date": "2022-08-25",
    "publication_year": 2022,
    "authors": "Huisi Wu; Zhaoze Wang; Zhuoying Li; Zhenkun Wen; Jing Qin",
    "corresponding_authors": "",
    "abstract": "Most state-of-the-art deep networks proposed for biomedical image segmentation are developed based on U-Net. While remarkable success has been achieved, its inherent limitations hinder it from yielding more precise segmentation. First, its receptive field is limited due to the fixed kernel size, which prevents the network from modeling global context information. Second, when spatial information captured by shallower layer is directly transmitted to higher layers by skip connections, the process inevitably introduces noise and irrelevant information to feature maps and blurs their semantic meanings. In this article, we propose a novel segmentation network equipped with a new context prior guidance (CPG) module to overcome these limitations for biomedical image segmentation, namely context prior guidance network (CPG-Net). Specifically, we first extract a set of context priors under the supervision of a coarse segmentation and then employ these context priors to model the global context information and bridge the spatial-semantic gap between high-level features and low-level features. The CPG module contains two major components: context prior representation (CPR) and semantic complement flow (SCF). CPR is used to extract pixels belonging to the same objects and hence produce more discriminative features to distinguish different objects. We further introduce deep semantic information for each CPR by the SCF mechanism to compensate the semantic information diluted during the decoding. We extensively evaluate the proposed CPG-Net on three famous biomedical image segmentation tasks with diverse imaging modalities and semantic environments. Experimental results demonstrate the effectiveness of our network, consistently outperforming state-of-the-art segmentation networks in all the three tasks. Codes are available at https://github.com/zzw-szu/CPGNet .",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4293088847",
    "type": "article"
  },
  {
    "title": "Learning Explicit and Implicit Dual Common Subspaces for Audio-visual Cross-modal Retrieval",
    "doi": "https://doi.org/10.1145/3564608",
    "publication_date": "2022-09-22",
    "publication_year": 2022,
    "authors": "Donghuo Zeng; Jianming Wu; Gen Hattori; Rong Xu; Yi Yu",
    "corresponding_authors": "",
    "abstract": "Audio-visual tracks in video contain rich semantic information with potential in many applications and research. Since the audio-visual data have inconsistent distributions and because of the heterogeneous nature of representations, the heterogeneous gap between modalities makes them impossible to compare directly. To bridge the modality gap, a frequently adopted approach is to simultaneously project audio-visual data into a common subspace to capture the commonalities and characteristics of modalities for measurement, which has been extensively studied in relation to the issues of modality-common and modality-specific feature learning in previous research. However, it is difficult for existing methods to address the tradeoff between both issues; e.g., the modality-common feature is learned from the latent commonalities of audio-visual data or the correlated features as aligned projections, in which the modality-specific feature can be lost. To solve the tradeoff, we propose a novel end-to-end architecture, which synchronously projects audio-visual data into the explicit and the implicit dual common subspaces. The explicit subspace is used to learn modality-common features and reduce the modality gap of explicitly paired audio-visual data, where the representation-specific details are abandoned to retain the common underlying structure of audio-visual data. The implicit subspace is used to learn modality-specific features, where each modality privately pulls apart the feature distances between different categories to maintain the category-based distinctions, by minimizing the distance between audio-visual features and corresponding labels. The comprehensive experimental results on two audio-visual datasets, VEGAS and AVE, demonstrate that our proposed model for using two different common subspaces for audio-visual cross-modal learning is effective and significantly outperforms the state-of-the-art cross-modal models that learn features from a single common subspace by 4.30% and 2.30% in terms of average MAP on the VEGAS and AVE datasets, respectively.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4296641465",
    "type": "article"
  },
  {
    "title": "PAV-SOD: A New Task towards Panoramic Audiovisual Saliency Detection",
    "doi": "https://doi.org/10.1145/3565267",
    "publication_date": "2022-09-30",
    "publication_year": 2022,
    "authors": "Yi Zhang; Fang-Yi Chao; Wassim Hamidouche; Olivier Déforges",
    "corresponding_authors": "",
    "abstract": "Object-level audiovisual saliency detection in 360° panoramic real-life dynamic scenes is important for exploring and modeling human perception in immersive environments, also for aiding the development of virtual, augmented, and mixed reality applications in fields such as education, social network, entertainment, and training. To this end, we propose a new task, p anoramic a udio v isual s alient o bject d etection, ( PAV-SOD 1 ), which aims to segment the objects grasping most of the human attention in 360° panoramic videos reflecting real-life daily scenes. To support the task, we collect PAVS10K , the first p anoramic video dataset for a udio v isual s alient object detection, which consists of 67 4K-resolution equirectangular videos with per-video labels including hierarchical scene categories and associated attributes depicting specific challenges for conducting PAV-SOD , and 10,465 uniformly sampled video frames with manually annotated object-level and instance-level pixel-wise masks. The coarse-to-fine annotations enable multi-perspective analysis regarding PAV-SOD modeling. We further systematically benchmark 13 state-of-the-art salient object detection (SOD)/video object segmentation (VOS) methods based on our PAVS10K . Besides, we propose a new baseline network, which takes advantage of both visual and audio cues of 360° video frames by using a new conditional variational auto-encoder (CVAE). Our C VAE-based a udio v isual net work, namely, CAV-Net , consists of a spatial-temporal visual segmentation network, a convolutional audio-encoding network, and audiovisual distribution estimation modules. As a result, our CAV-Net outperforms all competing models and is able to estimate the aleatoric uncertainties within PAVS10K . With extensive experimental results, we gain several findings about PAV-SOD challenges and insights towards PAV-SOD model interpretability. We hope that our work could serve as a starting point for advancing SOD towards immersive media.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4298140583",
    "type": "article"
  },
  {
    "title": "Multiple Temporal Pooling Mechanisms for Weakly Supervised Temporal Action Localization",
    "doi": "https://doi.org/10.1145/3567828",
    "publication_date": "2022-10-13",
    "publication_year": 2022,
    "authors": "Peng Dou; Ying Zeng; Zhuoqun Wang; Haifeng Hu",
    "corresponding_authors": "",
    "abstract": "Recent action localization works learn in a weakly supervised manner to avoid the expensive cost of human labeling. Those works are mostly based on the Multiple Instance Learning framework, where temporal pooling is an indispensable part that usually relies on the guidance of snippet-level Class Activation Sequences (CAS) . However, we observe that previous works only leverage a simple convolutional neural network for the generation of CAS, which ignores the weak discriminative foreground action segments and the background ones, and meanwhile, the relationship between different actions has not been considered. To solve this problem, we propose multiple temporal pooling mechanisms (MTP) for a more sufficient information utilization. Specifically, with the design of the Foreground Variance Branch, Dual Foreground Attention Branch and Hybrid Attention Fine-tuning Branch, MTP can leverage more effective information from different aspects and generate different CASs to guide the learning of temporal pooling. Moreover, different loss functions are designed for a better optimization of individual branches, aiming to effectively distinguish the action from the background. Our method shows excellent results on the THUMOS14 and ActivityNet1.2 datasets.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4304891649",
    "type": "article"
  },
  {
    "title": "Local Eyebrow Feature Attention Network for Masked Face Recognition",
    "doi": "https://doi.org/10.1145/3569943",
    "publication_date": "2022-10-27",
    "publication_year": 2022,
    "authors": "Baojin Huang; Zhongyuan Wang; Guangcheng Wang; Zhen Han; Kui Jiang",
    "corresponding_authors": "",
    "abstract": "During the COVID-19 coronavirus epidemic, wearing masks has become increasingly popular. Traditional occlusion face recognition algorithms are almost ineffective for such heavy mask occlusion. Therefore, it is urgent to improve the recognition performance of the existing face recognition technology on masked faces. Due to the limited visible feature points of the masked face image relative to the normal face image, we have to exploit the identification potential of eyebrow (referring to eyes and brows) features. This article proposes a local eyebrow feature attention network for masked face recognition, which consists of feature extraction, eyebrow region pooling, and feature fusion. To highlight the eyebrow region, we first use the eyebrow region pooling to separate the local features of eyebrows from the learned overall facial features. We then make full use of the symmetry of left and right eyebrows to emphasize their discriminant ability, due to the inadequate fine information of the low-resolution eyebrows. In particular, in view of the symmetrical similarity between eyebrow pairs and the subordinate relationship between facial components and the whole, we propose a feature fusion model based on graph convolutional network (GCN) to learn the feature association structure of eye features, brow features, and global facial features. We construct the benchmark datasets for masked face recognition to validate our approach, including real-world masked face recognition dataset (RMFRD) and synthetic masked face recognition dataset (SMFRD). Extensive experimental results on both public datasets and our built masked face datasets show that our approach significantly outperforms the state-of-the-arts.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4307366741",
    "type": "article"
  },
  {
    "title": "FastCNN: Towards Fast and Accurate Spatiotemporal Network for HEVC Compressed Video Enhancement",
    "doi": "https://doi.org/10.1145/3569583",
    "publication_date": "2022-10-27",
    "publication_year": 2022,
    "authors": "Zhijie Huang; Jun Sun; Xiaopeng Guo",
    "corresponding_authors": "",
    "abstract": "Deep neural networks have achieved remarkable success in HEVC compressed video quality enhancement. However, most existing multiframe-based methods either deliver unsatisfactory results or consume a significant amount of resources to leverage temporal information of neighboring frames. For the sake of practicality, a thorough investigation of the architecture design of the video quality enhancement network regarding enhancement performance, model parameters, and running speed is essential. In this article, we first propose an efficient alignment module that can quickly and accurately aggregate the spatiotemporal information of neighboring frames. The proposed module estimates deformable offsets progressively in lower-resolution space motivated by the observation of offset correlations between adjacent pixels. Then, the quantization parameter (QP) that represents compression level prior knowledge is utilized to guide aligned feature enhancement. By combining alignment feature distillation with residual feature correction, we obtain an efficient QP attention block. To save the storage space of the network, we design a hash buffer to store QP embedding features. These efficient components allow our network to effectively exploit temporal redundancies and obtain favorable enhancement capability while maintaining a lightweight structure and fast running speed. Extensive experiments demonstrate that the proposed approach outperforms state-of-the-art methods over different QPs by up to 0.09 to 0.11 dB, whereas the inference time can be reduced by up to 69%.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4307638415",
    "type": "article"
  },
  {
    "title": "Efficient Single-image Super-resolution Using Dual path Connections with Multiple scale Learning",
    "doi": "https://doi.org/10.1145/3570164",
    "publication_date": "2022-11-03",
    "publication_year": 2022,
    "authors": "Bin-Cheng Yang; Gangshan Wu",
    "corresponding_authors": "",
    "abstract": "Deep convolutional neural networks have been demonstrated to be effective for single-image super-resolution in recent years. On the one hand, residual connections and dense connections have been used widely to ease forward information and backward gradient flows to boost performance. However, current methods use residual connections and dense connections separately in most network layers in a sub-optimal way. On the other hand, although various networks and methods have been designed to improve computation efficiency, save parameters, or utilize training data of multiple scale factors for each other to boost performance, they either do super-resolution in high-resolution space to have a high computation cost or cannot share parameters between models of different scale factors to save parameters and inference time. To tackle these challenges, we propose an efficient single-image super-resolution network using dual path connections with multiple scale learning (EMSRDPN). By introducing dual path connections inspired by Dual path Networks into EMSRDPN, it uses residual connections and dense connections in an integrated way in most network layers. Dual path connections have the benefits of both reusing common features of residual connections and exploring new features of dense connections to learn a good representation for single-image super-resolution. To utilize the feature correlation of multiple scale factors, EMSRDPN shares all network units in low-resolution space between different scale factors to learn shared features and only uses a separate reconstruction unit for each scale factor, which can utilize training data of multiple scale factors to help each other to boost performance, meanwhile, which can save parameters and support shared inference for multiple scale factors to improve efficiency. Experiments show EMSRDPN achieves better performance and comparable or even better parameter and inference efficiency over state-of-the-art methods. Code will be available at https://github.com/yangbincheng/EMSRDPN .",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4308448249",
    "type": "article"
  },
  {
    "title": "Structure-aware Video Style Transfer with Map Art",
    "doi": "https://doi.org/10.1145/3572030",
    "publication_date": "2022-11-23",
    "publication_year": 2022,
    "authors": "Thi-Ngoc-Hanh Le; Ya-Hsuan Chen; Tong‐Yee Lee",
    "corresponding_authors": "",
    "abstract": "Changing the style of an image/video while preserving its content is a crucial criterion to access a new neural style transfer algorithm. However, it is very challenging to transfer a new map art style to a certain video in which “content” comprises a map background and animation objects. In this article, we present a novel comprehensive system that solves the problems in transferring map art style in such video. Our system takes as input an arbitrary video, a map image, and an off-the-shelf map art image. It then generates an artistic video without damaging the functionality of the map and the consistency in details. To solve this challenge, we propose a novel network, Map Art Video Network (MAViNet), the tailored objective functions, and a rich training set with rich animation contents and different map structures. We have evaluated our method on various challenging cases and many comparisons with those of the related works. Our method substantially outperforms state-of-the-art methods in terms of visual quality and meets the mentioned criteria in this research domain.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4309764746",
    "type": "article"
  },
  {
    "title": "Melody Generation from Lyrics with Local Interpretability",
    "doi": "https://doi.org/10.1145/3572031",
    "publication_date": "2022-11-29",
    "publication_year": 2022,
    "authors": "Wei Duan; Yi Yu; Xulong Zhang; Suhua Tang; Wei Li; Keizo Oyama",
    "corresponding_authors": "",
    "abstract": "Melody generation aims to learn the distribution of real melodies to generate new melodies conditioned on lyrics, which has been a very interesting topic in the area of artificial intelligence and music. However, a challenging issue still limits the quality and reliability of melody generation conditioned on lyrics: how to enhance the interpretability between the input lyrics and generated melodies so humans can understand their relationships. To solve this issue, in this article, we propose a model for melody generation from lyrics with local interpretability, which contains two significant contributions: (i) Mutual information between input lyrics and generated melody is exploited to instruct the training of the network, which avoids the loss of content consistency during the training stage. (ii) Transformer is explored to efficiently extract semantic features from lyrics sequences, which provides more interpretable correlations between different syllables in lyrics. Experiments on a large-scale dataset with paired lyrics-melodies demonstrate that the proposed approach can generate higher-quality melodies from lyrics compared with existing methods.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4310987263",
    "type": "article"
  },
  {
    "title": "Attention, Please! Adversarial Defense via Activation Rectification and Preservation",
    "doi": "https://doi.org/10.1145/3572843",
    "publication_date": "2022-11-29",
    "publication_year": 2022,
    "authors": "Shangxi Wu; Jitao Sang; Kaiyuan Xu; Jiaming Zhang; Jian Yu",
    "corresponding_authors": "",
    "abstract": "This study provides a new understanding of the adversarial attack problem by examining the correlation between adversarial attack and visual attention change. In particular, we observed that: (1) images with incomplete attention regions are more vulnerable to adversarial attacks; and (2) successful adversarial attacks lead to deviated and scattered activation map. Therefore, we use the mask method to design an attention-preserving loss and a contrast method to design a loss that makes the model’s attention rectification. Accordingly, an attention-based adversarial defense framework is designed, under which better adversarial training or stronger adversarial attacks can be performed through the above constraints. We hope the attention-related data analysis and defense solution in this study will shed some light on the mechanism behind the adversarial attack and also facilitate future adversarial defense/attack model design.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4310988119",
    "type": "article"
  },
  {
    "title": "EMES: Efficient Multi-Encoding Schemes for HEVC-based Adaptive Bitrate Streaming",
    "doi": "https://doi.org/10.1145/3575659",
    "publication_date": "2022-12-08",
    "publication_year": 2022,
    "authors": "Vignesh V Menon; Hadi Amirpour; M. Ghanbari; Christian Timmerer",
    "corresponding_authors": "",
    "abstract": "In HTTP Adaptive Streaming (HAS), videos are encoded at multiple bitrates and spatial resolutions ( i.e. , representations ) to adapt to the heterogeneity of network conditions, device attributes, and end-user preferences. Encoding the same video segment at multiple representations increases costs for content providers. State-of-the-art multi-encoding schemes improve the encoding process by utilizing encoder analysis information from already encoded representation(s) to reduce the encoding time of the remaining representations. These schemes typically use the highest bitrate representation as the reference to accelerate the encoding of the remaining representations. Nowadays, most streaming services utilize cloud-based encoding techniques, enabling a fully parallel encoding process to reduce the overall encoding time. The highest bitrate representation has the highest encoding time than the other representations. Thus, utilizing it as the reference encoding is unfavorable in a parallel encoding setup as the overall encoding time is bound by its encoding time. This paper provides a comprehensive study of various multi-rate and multi-encoding schemes in both serial and parallel encoding scenarios. Furthermore, it introduces novel heuristics to limit the Rate Distortion Optimization (RDO) process across various representations. Based on these heuristics, three multi-encoding schemes are proposed, which rely on encoder analysis sharing across different representations: (i) optimized for the highest compression efficiency , (ii) optimized for the best compression efficiency-encoding time savings trade-off , and (iii) optimized for the best encoding time savings . Experimental results demonstrate that the proposed multi-encoding schemes (i) , (ii) , and (iii) reduce the overall serial encoding time by 34.71%, 45.27%, and 68.76% with a 2.3%, 3.1%, and 4.5% bitrate increase to maintain the same VMAF, respectively compared to stand-alone encodings. The overall parallel encoding time is reduced by 22.03%, 20.72%, and 76.82% compared to stand-alone encodings for schemes (i) , (ii) , and (iii) , respectively.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4311080304",
    "type": "article"
  },
  {
    "title": "SSR-Net: A Spatial Structural Relation Network for Vehicle Re-identification",
    "doi": "https://doi.org/10.1145/3578578",
    "publication_date": "2022-12-29",
    "publication_year": 2022,
    "authors": "Zheming Xu; Lili Wei; Congyan Lang; Songhe Feng; Tao Wang; Adrian G. Borş; Hongzhe Liu",
    "corresponding_authors": "",
    "abstract": "Vehicle re-identification (Re-ID) represents the task aiming to identify the same vehicle from images captured by different cameras. Recent years have seen various feature learning-based approaches merely focusing on feature representations including global features or local features to obtain more subtle details to identify highly similar vehicles. However, few such methods consider the spatial geometrical structure relationship among local regions or between the global and local regions. By contrast, in this study, we propose a Spatial Structural Relation Network (SSR-Net) that explores the above-mentioned two kinds of relations simultaneously to learn more discriminative features by modeling the spatial structure information and global context information. In this article, we propose to adopt a Graph Convolution Network (GCN), for modeling spatial structural relationships among characteristic features. The GCN model aggregating the local and global features is shown to be more discriminative and robust to several car image transformations. To improve the performance of our proposed network, we jointly combine the classification loss with metric learning loss. Extensive experiments conducted on the public VehicleID and VeRi-776 datasets validate the effectiveness of our approach in comparison with recent works.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4313331801",
    "type": "article"
  },
  {
    "title": "Handling multiple points of view in a multimedia data warehouse",
    "doi": "https://doi.org/10.1145/1152149.1152152",
    "publication_date": "2006-08-01",
    "publication_year": 2006,
    "authors": "Anne-Muriel Arigon; Anne Tchounikine; Maryvonne Miquel",
    "corresponding_authors": "",
    "abstract": "Data warehouses are dedicated to collecting heterogeneous and distributed data in order to perform decision analysis. Based on multidimensional model, OLAP commercial environments such as they are currently designed in traditional applications are used to provide means for the analysis of facts that are depicted by numeric data (e.g., sales depicted by amount or quantity sold). However, in numerous fields, like in medical or bioinformatics, multimedia data are used as valuable information in the decisional process. One of the problems when integrating multimedia data as facts in a multidimensional model is to deal with dimensions built on descriptors that can be obtained by various computation modes on raw multimedia data. Taking into account these computation modes makes possible the characterization of the data by various points of view depending on the user's profile, his best-practices, his level of expertise, and so on. We propose a new multidimensional model that integrates functional dimension versions allowing the descriptors of the multidimensional data to be computed by different functions. With this approach, the user is able to obtain and choose multiple points of view on the data he analyses. This model is used to develop an OLAP application for navigation into a hypercube integrating various functional dimension versions for the calculus of descriptors in a medical use case.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2057090015",
    "type": "article"
  },
  {
    "title": "Feature synthesized EM algorithm for image retrieval",
    "doi": "https://doi.org/10.1145/1352012.1352014",
    "publication_date": "2008-05-01",
    "publication_year": 2008,
    "authors": "Rui Li; Bir Bhanu; Anlei Dong",
    "corresponding_authors": "",
    "abstract": "As a commonly used unsupervised learning algorithm in Content-Based Image Retrieval (CBIR), Expectation-Maximization (EM) algorithm has several limitations, including the curse of dimensionality and the convergence at a local maximum. In this article, we propose a novel learning approach, namely Coevolutionary Feature Synthesized Expectation-Maximization (CFS-EM), to address the above problems. The CFS-EM is a hybrid of coevolutionary genetic programming (CGP) and EM algorithm applied on partially labeled data. CFS-EM is especially suitable for image retrieval because the images can be searched in the synthesized low-dimensional feature space, while a kernel-based method has to make classification computation in the original high-dimensional space. Experiments on real image databases show that CFS-EM outperforms Radial Basis Function Support Vector Machine (RBF-SVM), CGP, Discriminant-EM (D-EM) and Transductive-SVM (TSVM) in the sense of classification performance and it is computationally more efficient than RBF-SVM in the query phase.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W1990759220",
    "type": "article"
  },
  {
    "title": "Video quality estimation in wireless IP networks",
    "doi": "https://doi.org/10.1145/1324287.1324290",
    "publication_date": "2008-01-01",
    "publication_year": 2008,
    "authors": "F. Babich; M. D'Orlando; F. Vatta",
    "corresponding_authors": "",
    "abstract": "This article proposes three methods to estimate the distortion deriving from packet losses in wireless video communication. The proposed methods take into account the short-term properties of the encoded video sequences. A suitable set of functions is adopted to model the distortion envelope resulting from multiple losses. The estimated performance is compared with the actual distortion, evaluated by decoding the received sequence with a properly designed decoder. Numerical results confirm the accuracy of the proposed models in approximating the actual Mean Square Error (MSE) for a wide range of loss rates. Some applications of the proposed algorithms are presented.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2039485311",
    "type": "article"
  },
  {
    "title": "Multimedia sensor fusion for retrieving identity in biometric access control systems",
    "doi": "https://doi.org/10.1145/1865106.1865110",
    "publication_date": "2010-11-01",
    "publication_year": 2010,
    "authors": "Girija Chetty; Matthew White",
    "corresponding_authors": "",
    "abstract": "In this article, we propose a novel multimedia sensor fusion approach based on heterogeneous sensors for biometric access control applications. The proposed fusion technique uses multiple acoustic and visual sensors for extracting dominant biometric cues, and combines them with nondominant cues. The performance evaluation of the proposed fusion protocol and a novel cascaded authentication approach using a 3D stereovision database shows a significant improvement in performance and robustness, with equal error rates of 42.9% (audio only), 32% (audio + 3D face + 2D lip features), 15% (audio + 3D face + 2D eye features), and 7.3% (audio-3D face + 2D lip + 2D eye-eyebrows) respectively.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2098126059",
    "type": "article"
  },
  {
    "title": "ISP-friendly P2P live streaming",
    "doi": "https://doi.org/10.1145/2089085.2089088",
    "publication_date": "2012-02-01",
    "publication_year": 2012,
    "authors": "Zhijie Shen; Roger Zimmermann",
    "corresponding_authors": "",
    "abstract": "Peer-to-Peer (P2P) applications generate large amounts of Internet network traffic. The wide-reaching connectivity of P2P systems is creating resource inefficiencies for network providers. Recent studies have demonstrated that localizing cross-ISP (Internet service provider) traffic can mitigate this challenge. However, bandwidth sensitivity and display quality requirements complicate the ISP-friendly design for live streaming systems. To this date, although some prior techniques focusing on live streaming systems exist, the correlation between traffic localization and streaming quality guarantee has not been well explored. Additionally, the proposed solutions are often not easy to apply in practice. In our presented work, we demonstrate that the cross-ISP traffic of P2P live streaming systems can be significantly reduced with little impact on the streaming quality. First, we analytically investigate and quantify the tradeoff between traffic localization and streaming quality guarantee, determining the lower bound of the inter-AS (autonomous system) streaming rate below which streaming quality cannot be preserved. Based on the analysis, we further propose a practical ISP-friendly solution, termed IFPS , which requires only minor changes to the peer selection mechanism and can easily be integrated into both new and existing systems. Additionally, the significant opportunity for localizing traffic is underscored by our collected traces from PPLive, which also enabled us to derive realistic parameters to guide our simulations. The experimental results demonstrate that IFPS reduces cross-ISP traffic from 81% up to 98% while keeping streaming quality virtually unaffected.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W1969375069",
    "type": "article"
  },
  {
    "title": "Exploiting content relevance and social relevance for personalized ad recommendation on internet TV",
    "doi": "https://doi.org/10.1145/2501643.2501648",
    "publication_date": "2013-08-01",
    "publication_year": 2013,
    "authors": "Bo Wang; Jinqiao Wang; Hanqing Lu",
    "corresponding_authors": "",
    "abstract": "There have been not many interactions between the two dominant forms of mass communication: television and the Internet, while nowadays the appearance of Internet television makes them more closely. Different with traditional TV in a passive mode of transmission, Internet TV makes it more possible to make personalized service recommendation because of the interactivity between users and the Internet. In this article, we introduce a scheme to provide targeted ad recommendation to Internet TV users by exploiting the content relevance and social relevance. First, we annotate TV videos in terms of visual content analysis and textual analysis by aligning visual and textual information. Second, with user-user, video-video and user-video relationships, we employ Multi-Relationship based Probabilistic Matrix Factorization (MRPMF) to learn representative tags for modeling user preference. And then semantic content relevance (between product/ad and TV video) and social relevance (between product/ad and user interest) are calculated by projecting the corresponding tags into our advertising concept space. Finally, with relevancy scores we make ranking for relevant product/ads to effectively provide users personalized recommendation. The experimental results demonstrate attractiveness and effectiveness of our proposed approach.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W1983041797",
    "type": "article"
  },
  {
    "title": "Introduction to special issue on social media",
    "doi": "https://doi.org/10.1145/2037676.2037682",
    "publication_date": "2011-10-01",
    "publication_year": 2011,
    "authors": "Susanne Boll; Ramesh Jain; Jiebo Luo; Dong Xu",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W1992451340",
    "type": "article"
  },
  {
    "title": "A joint layered scheme for reliable and secure mobile JPEG-2000 streaming",
    "doi": "https://doi.org/10.1145/2240136.2240143",
    "publication_date": "2012-07-01",
    "publication_year": 2012,
    "authors": "Xinglei Zhu; Chang Wen Chen",
    "corresponding_authors": "",
    "abstract": "This article presents a novel joint layered approach to simultaneously achieve both reliable and secure mobile JPEG-2000 image streaming. With a priori knowledge of JPEG-2000 source coding and channel coding, the proposed joint system integrates authentication into the media error protection components to ensure that every source-decodable media unit is authenticated. By such a dedicated design, the proposed scheme protects both compressed JPEG-2000 codestream and the authentication data from wireless channel impairments. It is fundamentally different from many existing systems that consider the problem of media authentication separately from the other operations in the media transmission system. By utilizing the contextual relationship, such as coding dependency and content importance between media slices for authentication hash appending, the proposed scheme generates an extremely low authentication overhead. Under this joint layered coding framework, an optimal rate allocation algorithm for source coding, channel coding, and media authentication is developed to guarantee end-to-end media quality. Experiment results on JPEG-2000 images validate the proposed scheme and demonstrate that the performance of the proposed scheme is approaching its upper bound, in which case no authentication is applied to the media stream.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W1992522424",
    "type": "article"
  },
  {
    "title": "Blind robust watermarking of 3d motion data",
    "doi": "https://doi.org/10.1145/1671954.1671956",
    "publication_date": "2010-02-01",
    "publication_year": 2010,
    "authors": "Parag Agarwal; Balakrishnan Prabhakaran",
    "corresponding_authors": "",
    "abstract": "The article addresses the problem of copyright protection for 3D motion-captured data by designing a robust blind watermarking mechanism. The mechanism segments motion capture data and identifies clusters of 3D points per segment. A watermark can be embedded and extracted within these clusters by using a proposed extension of 3D quantization index modulation. The watermarking scheme is blind in nature and the encoded watermarks are shown to be imperceptible, and secure. The resulting hiding capacity has bounds based on cluster size. The watermarks are shown to be robust against attacks such as uniform affine transformations (scaling, rotation, and translation), cropping, reordering, and noise addition. The time complexity for watermark embedding and extraction is estimated as O( n log n ) and O( n 2 log n ), respectively.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W1993167916",
    "type": "article"
  },
  {
    "title": "A conditional access system with efficient key distribution and revocation for mobile pay-TV systems",
    "doi": "https://doi.org/10.1145/2487268.2487271",
    "publication_date": "2013-06-01",
    "publication_year": 2013,
    "authors": "Lo‐Yao Yeh; Jiun‐Long Huang",
    "corresponding_authors": "",
    "abstract": "Current mobile pay-TV systems have two types of Conditional Access Systems (CAS): group-key-based and public-key systems. The best feature of group-key-based systems is the ability to enjoy the broadcast nature in delivery multimedia contents, while the major advantage of public-key systems is consolidating the security foundation to withstand various attacks, such as collusion attacks. However, the problems of group-key-based systems include collusion attacks, lack of nonrepudiation, and troublesome key distribution. Even worse, the benefit of broadcast efficiency is confined to a group size of no more than 512 subscribers. For public-key systems, the poor delivery scalability is the major shortcoming because the unique private key feature is only suitable for one-to-one delivery. In this article, we introduce a scalable access control scheme to integrate the merits of broadcasting regardless of group size and sound security assurance, including fine-grained access control and collusion attack resistance. For subscriber revocation, a single message is broadcast to the other subscribers to get the updated key, thus significantly boosting subscriber revocation scalability. Due to mobile subscribers' dynamic movements, this article also analyzes the benefit of retransmission cases in our system. Through the performance evaluation and functionality comparison, the proposed scheme should be a decent candidate to enhance the security strength and transmission efficiency in a mobile pay-TV system.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2019350846",
    "type": "article"
  },
  {
    "title": "Image registration for foveated panoramic sensing",
    "doi": "https://doi.org/10.1145/2168996.2168997",
    "publication_date": "2012-05-01",
    "publication_year": 2012,
    "authors": "Fadi Dornaika; James H. Elder",
    "corresponding_authors": "",
    "abstract": "This article addresses the problem of registering high-resolution, small field-of-view images with low-resolution panoramic images provided by a panoramic catadioptric video sensor. Such systems may find application in surveillance and telepresence systems that require a large field of view and high resolution at selected locations. Although image registration has been studied in more conventional applications, the problem of registering panoramic and conventional video has not previously been addressed, and this problem presents unique challenges due to (i) the extreme differences in resolution between the sensors (more than a 16:1 linear resolution ratio in our application), and (ii) the resolution inhomogeneity of panoramic images. The main contributions of this article are as follows. First, we introduce our foveated panoramic sensor design. Second, we show how a coarse registration can be computed from the raw images using parametric template matching techniques. Third, we propose two refinement methods allowing automatic and near real-time registration between the two image streams. The first registration method is based on matching extracted interest points using a closed form method. The second registration method is featureless and based on minimizing the intensity discrepancy allowing the direct recovery of both the geometric and the photometric transforms. Fourth, a comparison between the two registration methods is carried out, which shows that the featureless method is superior in accuracy. Registration examples using the developed methods are presented.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2020183999",
    "type": "article"
  },
  {
    "title": "A collusion attack optimization strategy for digital fingerprinting",
    "doi": "https://doi.org/10.1145/2344436.2344442",
    "publication_date": "2012-09-01",
    "publication_year": 2012,
    "authors": "Hui Feng; Hefei Ling; Fuhao Zou; WeiQi Yan; Zhengding Lu",
    "corresponding_authors": "",
    "abstract": "Collusion attack is a cost-efficient attack for digital fingerprinting. In this article, we propose a novel collusion attack strategy, Iterative Optimization Collusion Attack (IOCA) , which is based upon the gradient attack and the principle of informed watermark embedding. We evaluate the performance of the proposed collusion attack strategy in defeating four typical fingerprinting schemes under a well-constructed evaluation framework. The simulation results show that the proposed strategy performs more effectively than the gradient attack, and adopting no more than three fingerprinted copies can sufficiently collapse examined fingerprinting schemes. Meanwhile, the content resulted from the proposed attack still preserves high perceptual quality.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2024635071",
    "type": "article"
  },
  {
    "title": "Optimal layered multicast",
    "doi": "https://doi.org/10.1145/1925101.1925102",
    "publication_date": "2011-02-01",
    "publication_year": 2011,
    "authors": "Ajay Gopinathan; Zongpeng Li",
    "corresponding_authors": "",
    "abstract": "Recent advances in network coding research dramatically changed the underlying structure of optimal multicast routing algorithms and made them efficiently computable. While most such algorithm design assumes a single file/layer being multicast, layered coding introduces new challenges into the paradigm due to its cumulative decoding nature. Layered coding is designed to handle heterogeneity in receiver capacities, and a node may decode layer k only if it successfully receives all layers in 1.. k . We show that recently proposed optimization models for layered multicast do not correctly address this challenge. We argue that in order to achieve the absolute maximum throughput (or minimum cost), it is necessary to decouple the application-layer throughput from network-layer throughput. In particular, a node should be able to receive a nonconsecutive layer or a partial layer even if it cannot decode and utilize it (e.g., for playback in media streaming applications). The rationale is that nodes at critical network locations need to receive data just for helping other peers. We present a mathematical programming model that addresses these challenges and achieves absolute optimal performance. Simulation results show considerable throughput gain (cost reduction) compared with previous models, in a broad range of network scenarios. We then provide a formal proof that the layered multicast problem is NP-complete. We design a randomized rounding algorithm to approximate the optimal layered multicast, and show the efficacy of our technique using simulations. We then proceed to further generalize our model by studying the optimal progression of layer sizes. We show that such optimization is nonconvex, and apply a simulated annealing algorithm to solve it, with flexible trade-off between solution quality and running time. We verify the effectiveness of the new model and the simulated annealing algorithm through extensive simulations, and point out insights on the connection between optimal layer size progression and node capacity distribution.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2056880433",
    "type": "article"
  },
  {
    "title": "Web-accessible geographic integration and calibration of webcams",
    "doi": "https://doi.org/10.1145/2422956.2422964",
    "publication_date": "2013-02-01",
    "publication_year": 2013,
    "authors": "Austin Abrams; Robert Pless",
    "corresponding_authors": "",
    "abstract": "A global network of webcams offers unique viewpoints from tens of thousands of locations. Understanding the geographic context of this imagery is vital in using these cameras for quantitative environmental monitoring or surveillance applications. We derive robust geo-calibration constraints that allow users to geo-register static or pan-tilt-zoom cameras by specifying a few corresponding points, and describe our Web interface suitable for novices. We discuss design decisions that support our scalable, publicly accessible Web service that allows webcam textures to be displayed live on 3D geographic models. Finally, we demonstrate several multimedia applications for geo-calibrated cameras.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2109484429",
    "type": "article"
  },
  {
    "title": "CZLoD",
    "doi": "https://doi.org/10.1145/2348816.2348818",
    "publication_date": "2012-09-01",
    "publication_year": 2012,
    "authors": "Wanmin Wu; Ahsan Arefin; Gregorij Kurillo; Pooja Agarwal; Klara Nahrstedt; Růžena Bajcsy",
    "corresponding_authors": "",
    "abstract": "This article presents a psychophysical study that measures the perceptual thresholds of a new factor called Color-plus-Depth Level-of-Details (CZLoD) peculiar to polygon-based 3D tele-immersive video. The results demonstrate the existence of Just Noticeable Degradation and Just Unacceptable Degradation thresholds on the factor. In light of the results, we design and implement a real-time perception-based quality adaptor for 3D tele-immersive video. Our experimental results show that the adaptation scheme can reduce resource usage (e.g., CPU cycles) while considerably enhancing the overall perceived visual quality. Our analysis confirms the potential temporal and spatial performance benefits achievable with CZLoD adaptation.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2130695590",
    "type": "article"
  },
  {
    "title": "Photorealistic Face Completion with Semantic Parsing and Face Identity-Preserving Features",
    "doi": "https://doi.org/10.1145/3300940",
    "publication_date": "2019-02-13",
    "publication_year": 2019,
    "authors": "Ruijun Ma; Haifeng Hu; Weixuan Wang; Jia Xu; Zhengming Li",
    "corresponding_authors": "",
    "abstract": "Tremendous progress on deep learning has shown exciting potential for a variety of face completion tasks. However, most learning-based methods are limited to handle general or structure specified face images (e.g., well-aligned faces). In this article, we propose a novel face completion algorithm, called Learning and Preserving Face Completion Network (LP-FCN), which simultaneously parses face images and extracts face identity-preserving (FIP) features. By tackling these two tasks in a mutually boosting way, the LP-FCN can guide an identity preserving inference and ensure pixel faithfulness of completed faces. In addition, we adopt a global discriminator and a local discriminator to distinguish real images from synthesized ones. By training with a combined identity preserving, semantic parsing and adversarial loss, the LP-FCN encourages the completion results to be semantically valid and visually consistent for more complicated image completion tasks. Experiments show that our approach obtains similar visual quality, but achieves better performance on unaligned faces completion and fine detailed synthesis against the state-of-the-art methods.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2911980510",
    "type": "article"
  },
  {
    "title": "Discovering Latent Topics by Gaussian Latent Dirichlet Allocation and Spectral Clustering",
    "doi": "https://doi.org/10.1145/3290047",
    "publication_date": "2019-01-23",
    "publication_year": 2019,
    "authors": "Bo Yuan; Xinbo Gao; Zhenxing Niu; Qi Tian",
    "corresponding_authors": "",
    "abstract": "Today, diversifying the retrieval results of a certain query will improve customers’ search efficiency. Showing the multiple aspects of information provides users an overview of the object, which helps them fast target their demands. To discover aspects, research focuses on generating image clusters from initially retrieved results. As an effective approach, latent Dirichlet allocation (LDA) has been proved to have good performance on discovering high-level topics. However, traditional LDA is designed to process textual words, and it needs the input as discrete data. When we apply this algorithm to process continuous visual images, a common solution is to quantize the continuous features into discrete form by a bag-of-visual-words algorithm. During this process, quantization error will lead to information that inevitably is lost. To construct a topic model with complete visual information, this work applies Gaussian latent Dirichlet allocation (GLDA) on the diversity issue of image retrieval. In this model, traditional multinomial distribution is substituted with Gaussian distribution to model continuous visual features. In addition, we propose a two-phase spectral clustering strategy, called dual spectral clustering , to generate clusters from region level to image level. The experiments on the challenging landmarks of the DIV400 database show that our proposal improves relevance and diversity by about 10% compared to traditional topic models.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2912607070",
    "type": "article"
  },
  {
    "title": "Reconstructing 3D Face Models by Incremental Aggregation and Refinement of Depth Frames",
    "doi": "https://doi.org/10.1145/3287309",
    "publication_date": "2019-01-23",
    "publication_year": 2019,
    "authors": "Pietro Pala; Stefano Berretti",
    "corresponding_authors": "",
    "abstract": "Face recognition from two-dimensional (2D) still images and videos is quite successful even with “in the wild” conditions. Instead, less consolidated results are available for the cases in which face data come from non-conventional cameras, such as infrared or depth. In this article, we investigate this latter scenario assuming that a low-resolution depth camera is used to perform face recognition in an uncooperative context. To this end, we propose, first, to automatically select a set of frames from the depth sequence of the camera because they provide a good view of the face in terms of pose and distance. Then, we design a progressive refinement approach to reconstruct a higher-resolution model from the selected low-resolution frames. This process accounts for the anisotropic error of the existing points in the current 3D model and the points in a newly acquired frame so that the refinement step can progressively adjust the point positions in the model using a Kalman-like estimation. The quality of the reconstructed model is evaluated by considering the error between the reconstructed models and their corresponding high-resolution scans used as ground truth. In addition, we performed face recognition using the reconstructed models as probes against a gallery of reconstructed models and a gallery with high-resolution scans. The obtained results confirm the possibility to effectively use the reconstructed models for the face recognition task.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2913738669",
    "type": "article"
  },
  {
    "title": "Watch Me from Distance (WMD)",
    "doi": "https://doi.org/10.1145/3312574",
    "publication_date": "2019-05-31",
    "publication_year": 2019,
    "authors": "Pradeep K. Atrey; Bakul Trehan; Mukesh Saini",
    "corresponding_authors": "",
    "abstract": "Preserving the privacy of people in video surveillance systems is quite challenging, and a significant amount of research has been done to solve this problem in recent times. Majority of existing techniques are based on detecting bodily cues such as face and/or silhouette and obscuring them so that people in the videos cannot be identified. We observe that merely hiding bodily cues is not enough for protecting identities of the individuals in the videos. An adversary, who has prior contextual knowledge about the surveilled area, can identify people in the video by exploiting the implicit inference channels such as behavior, place, and time. This article presents an anonymous surveillance system, called Watch Me from Distance (WMD), which advocates for outsourcing of surveillance video monitoring (similar to call centers) to the long-distance sites where professional security operators watch the video and alert the local site when any suspicious or abnormal event takes place. We find that long-distance monitoring helps in decoupling the contextual knowledge of security operators. Since security operators at the remote site could turn into adversaries, a trust computation model to determine the credibility of the operators is presented as an integral part of the proposed system. The feasibility study and experiments suggest that the proposed system provides more robust measures of privacy yet maintains surveillance effectiveness.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2949257490",
    "type": "article"
  },
  {
    "title": "Low-Complexity Scalable Extension of the High-Efficiency Video Coding (SHVC) Encoding System",
    "doi": "https://doi.org/10.1145/3313185",
    "publication_date": "2019-05-31",
    "publication_year": 2019,
    "authors": "Liquan Shen; Ping An; Guorui Feng",
    "corresponding_authors": "",
    "abstract": "The scalable extension of the high-efficiency video coding (SHVC) system adopts a hierarchical quadtree-based coding unit (CU) that is suitable for various texture and motion properties of videos. Currently, the test model of SHVC identifies the optimal CU size by performing an exhaustive quadtree depth-level search, which achieves a high compression efficiency at a heavy cost in terms of the computational complexity. However, many interactive multimedia applications, such as remote monitoring and video surveillance, which are sensitive to time delays, have insufficient computational power for coding high-definition (HD) and ultra-high-definition (UHD) videos. Therefore, it is important, yet challenging, to optimize the SHVC coding procedure and accelerate video coding. In this article, we propose a fast CU quadtree depth-level decision algorithm for inter-frames on enhancement layers that is based on an analysis of inter-layer, spatial, and temporal correlations. When motion/texture properties of coding regions can be identified early, a fast algorithm can be designed for adapting CU depth-level decision procedures to video contents and avoiding unnecessary computations during CU depth-level traversal. The proposed algorithm determines the motion activity level at the treeblock size of the hierarchical quadtree by utilizing motion vectors from its corresponding blocks at the base layer. Based on the motion activity level, neighboring encoded CUs that have larger correlations are preferentially selected to predict the optimal depth level of the current treeblock. Finally, two parameters, namely, the motion activity level and the predicted CU depth level, are used to identify a subset of candidate CU depth levels and adaptively optimize CU depth-level decision processes. The experimental results demonstrate that the proposed scheme can run approximately three times faster than the most recent SHVC reference software, with a negligible loss of compression efficiency. The proposed scheme is efficient for all types of scalable video sequences under various coding conditions and outperforms state-of-the-art fast SHVC and HEVC algorithms. Our scheme is a suitable candidate for interactive HD/UHD video applications that are expected to operate in real-time and power-constrained scenarios.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2949618166",
    "type": "article"
  },
  {
    "title": "Appearance-consistent Video Object Segmentation Based on a Multinomial Event Model",
    "doi": "https://doi.org/10.1145/3321507",
    "publication_date": "2019-05-31",
    "publication_year": 2019,
    "authors": "Yadang Chen; Chuanyan Hao; Alex X. Liu; Enhua Wu",
    "corresponding_authors": "",
    "abstract": "In this study, we propose an effective and efficient algorithm for unconstrained video object segmentation, which is achieved in a Markov random field (MRF). In the MRF graph, each node is modeled as a superpixel and labeled as either foreground or background during the segmentation process. The unary potential is computed for each node by learning a transductive SVM classifier under supervision by a few labeled frames. The pairwise potential is used for the spatial-temporal smoothness. In addition, a high-order potential based on the multinomial event model is employed to enhance the appearance consistency throughout the frames. To minimize this intractable feature, we also introduce a more efficient technique that simply extends the original MRF structure. The proposed approach was evaluated in experiments with different measures and the results based on a benchmark demonstrated its effectiveness compared with other state-of-the-art algorithms.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2949848774",
    "type": "article"
  },
  {
    "title": "6K and 8K Effective Resolution with 4K HEVC Decoding Capability for 360 Video Streaming",
    "doi": "https://doi.org/10.1145/3335053",
    "publication_date": "2019-04-30",
    "publication_year": 2019,
    "authors": "Alireza Zare; Maryam Homayouni; Alireza Aminlou; Miska M. Hannuksela; Moncef Gabbouj",
    "corresponding_authors": "",
    "abstract": "The recent Omnidirectional MediA Format (OMAF) standard, which specifies the delivery of 360° video content, supports only equirectangular projection (ERP) and cubemap projection and their region-wise packing with a limitation on video decoding capability to the maximum resolution of 4K (e.g., 4,096 × 2,048). Streaming of 4K ERP content allows only a limited viewport resolution, which is lower than the resolution of many current head-mounted displays (HMDs). Therefore, to take full advantage of high-resolution HMDs, delivery of 360° video content beyond 4K resolution needs to be enabled. In this regard, we propose two specific mixed-resolution packing schemes of 6K (e.g., 6,144 × 3,072) and 8K (e.g., 8,192 × 4,096) ERP content and their realization in tile-based streaming, while complying with the 4K decoding constraint and the High Efficiency Video Coding standard. The proposed packing schemes offer 6K and 8K effective resolution at the viewport. Using our proposed test methodology, experimental results indicate that the proposed layouts significantly decrease streaming bitrates when compared to mixed-quality viewport-adaptive streaming of 4K ERP. Our results further indicate that 8K-effective packing outperforms 6K-effective packing especially in high-quality videos.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2965453671",
    "type": "article"
  },
  {
    "title": "Color Theme--based Aesthetic Enhancement Algorithm to Emulate the Human Perception of Beauty in Photos",
    "doi": "https://doi.org/10.1145/3328991",
    "publication_date": "2019-04-30",
    "publication_year": 2019,
    "authors": "Karen Panetta; Long Bao; Sos С. Agaian; Victor Oludare",
    "corresponding_authors": "",
    "abstract": "Fine Art Photography is one of the most popular art forms, which creates lasting impressions that elicit various human emotional reactions. Photo aesthetic enhancement aims at improving the aesthetic level of the photo to please humans by updating color appearance or modifying the geometry structure of objects within that photo. Even though several aesthetic enhancement methods have been proposed, to our knowledge, there is no research to explore, highlight, and accentuate photos’ intrinsic aesthetic value to elicit a stronger response from the human observer about the photos’ theme. To meet this challenge, a new multimedia technology called automatic color theme--based aesthetic enhancement (CT-AEA) is proposed by leveraging big online data to perform timely collection and learning of humans’ current aesthetic perception-behavior over photos and color themes in art, fashion, and design. Unlike existing aesthetic enhancement that examines the composition, such as the geometric structure of the image contents and color/luminance-related (color tone and luminance distribution) characteristics, this CT-AEA takes into consideration the importance of a suitable color theme, namely a set of dominant colors for the design when assessing the aesthetic appearance of a photo. This algorithm is composed of (1) utilizing the knowledge gained from the human evaluator's perception of beauty from existing online datasets, rather than simply applying prior existing knowledge of color harmony theory; (2) developing a new color theme difference equation that exhibits order-invariance and percentage-sensitive properties; (3) designing an optimal color theme recommendation to maximize the aesthetic performance, while minimizing the color modification cost to solve the problems of color inconsistencies and distortion. Experimental results, quantitative measure, and comparison tests demonstrate the algorithm's effectiveness, advantages, and potential for use in many color-related art and design applications.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2968208888",
    "type": "article"
  },
  {
    "title": "Eigenvector-Based Distance Metric Learning for Image Classification and Retrieval",
    "doi": "https://doi.org/10.1145/3340262",
    "publication_date": "2019-08-20",
    "publication_year": 2019,
    "authors": "Zhangcheng Wang; Ya Li; Richang Hong; Xinmei Tian",
    "corresponding_authors": "",
    "abstract": "Distance metric learning has been widely studied in multifarious research fields. The mainstream approaches learn a Mahalanobis metric or learn a linear transformation. Recent related works propose learning a linear combination of base vectors to approximate the metric. In this way, fewer variables need to be determined, which is efficient when facing high-dimensional data. Nevertheless, such works obtain base vectors using additional data from related domains or randomly generate base vectors. However, obtaining base vectors from related domains requires extra time and additional data, and random vectors introduce randomness into the learning process, which requires sufficient random vectors to ensure the stability of the algorithm. Moreover, the random vectors cannot capture the rich information of the training data, leading to a degradation in performance. Considering these drawbacks, we propose a novel distance metric learning approach by introducing base vectors explicitly learned from training data. Given a specific task, we can make a sparse approximation of its objective function using the top eigenvalues and corresponding eigenvectors of a predefined integral operator on the reproducing kernel Hilbert space. Because the process of generating eigenvectors simply refers to the training data of the considered task, our proposed method does not require additional data and can reflect the intrinsic information of the input features. Furthermore, the explicitly learned eigenvectors do not result in randomness, and we can extend our method to any kernel space without changing the objective function. We only need to learn the coefficients of these eigenvectors, and the only hyperparameter that we need to determine is the number of eigenvectors that we utilize. Additionally, an optimization algorithm is proposed to efficiently solve this problem. Extensive experiments conducted on several datasets demonstrate the effectiveness of our proposed method.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2969301371",
    "type": "article"
  },
  {
    "title": "Characterizing Subtle Facial Movements via Riemannian Manifold",
    "doi": "https://doi.org/10.1145/3342227",
    "publication_date": "2019-11-30",
    "publication_year": 2019,
    "authors": "Xiaopeng Hong; Wei Peng; Mehrtash Harandi; Ziheng Zhou; Matti Pietikäinen; Guoying Zhao",
    "corresponding_authors": "",
    "abstract": "Characterizing subtle facial movements from videos is one of the most intensive topics in computer vision research. It is, however, challenging, since (1) the intensity of subtle facial muscle movement is usually low, (2) the duration may be transient, and (3) datasets containing spontaneous subtle movements with reliable annotations are painful to obtain and often of small sizes. This article is targeted at addressing these problems for characterizing subtle facial movements from both the aspects of motion elucidation and description. First, we propose an efficient method for elucidating hidden and repressed movements to make them easier to get noticed. We explore the feasibility of linearizing motion magnification and temporal interpolation, which is obscured by the architecture of existing methods. On this basis, we propose a consolidated framework, termed MOTEL, to expand temporal duration and amplify subtle facial movements simultaneously. Second, we make our contribution to dynamic description. One major challenge is to capture the intrinsic temporal variations caused by movements and omit extrinsic ones caused by different individuals and various environments. To diminish the influences of such extrinsic diversity, we propose the tangent delta descriptor to characterize the dynamics of short-term movements using the differences between points on the tangent spaces to the manifolds, rather than the points themselves. We then relax the trajectory-smooth assumption of the conventional manifold-based trajectory modeling methods and incorporate the tangent delta descriptor with the sequential inference approaches to cover the period of facial movements. The proposed motion modeling approach is validated by a series of experiments on publicly available datasets in the tasks of micro-expression recognition and visual speech recognition .",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2996398361",
    "type": "article"
  },
  {
    "title": "A cognitive approach for effective coding and transmission of 3D video",
    "doi": "https://doi.org/10.1145/2037676.2037680",
    "publication_date": "2011-10-01",
    "publication_year": 2011,
    "authors": "Simone Milani; G. Calvagno",
    "corresponding_authors": "",
    "abstract": "Future multimedia applications will rely on the transmission of 3D video contents within heterogeneous fruition scenarios, and as a matter of fact, the reliable delivery of 3D video signals proves to be a crucial issue in such communications. To this purpose, multimedia communication experts have been designing cross-layer strategies to improve the quality of the perceived 3D experience. This article presents a new cross-layer strategy, called Cognitive Source Coding (CSC), that defines a new 3D video system able to identify the different elements of the 3D scene and choose the most appropriate coding strategy.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2998663567",
    "type": "article"
  },
  {
    "title": "Semantic Concept Network and Deep Walk-based Visual Question Answering",
    "doi": "https://doi.org/10.1145/3300938",
    "publication_date": "2019-04-30",
    "publication_year": 2019,
    "authors": "Qun Li; Fu Xiao; Le An; Xianzhong Long; Xiaochuan Sun",
    "corresponding_authors": "",
    "abstract": "Visual Question Answering (VQA) is a hot-spot in the intersection of computer vision and natural language processing research and its progress has enabled many in high-level applications. This work aims to describe a novel VQA model based on semantic concept network construction and deep walk. Extracting visual image semantic representation is a significant and effective method for spanning the semantic gap. Moreover, current research has shown that co-occurrence patterns of concepts can enhance semantic representation. This work is motivated by the challenge that semantic concepts have complex interrelations and the relationships are similar to a network. Therefore, we construct a semantic concept network adopted by leveraging Word Activation Forces (WAFs), and mine the co-occurrence patterns of semantic concepts using deep walk. Then the model performs polynomial logistic regression on the basis of the extracted deep walk vector along with the visual image feature and question feature. The proposed model effectively integrates visual and semantic features of the image and natural language question. The experimental results show that our algorithm outperforms competitive baselines on three benchmark image QA datasets. Furthermore, through experiments in image annotation refinement and semantic analysis on pre-labeled LabelMe dataset, we test and verify the effectiveness of our constructed concept network for mining concept co-occurrence patterns, sensible concept clusters, and hierarchies.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3003487319",
    "type": "article"
  },
  {
    "title": "Multi-scale Supervised Attentive Encoder-Decoder Network for Crowd Counting",
    "doi": "https://doi.org/10.1145/3356019",
    "publication_date": "2020-01-31",
    "publication_year": 2020,
    "authors": "Anran Zhang; Xiaolong Jiang; Baochang Zhang; Xianbin Cao",
    "corresponding_authors": "",
    "abstract": "Crowd counting is a popular topic with widespread applications. Currently, the biggest challenge to crowd counting is large-scale variation in objects. In this article, we focus on overcoming this challenge by proposing a novel Attentive Encoder-Decoder Network (AEDN), which is supervised on multiple feature scales to conduct crowd counting via density estimation. This work has three main contributions. First, we augment the traditional encoder-decoder architecture with our proposed residual attention blocks, which, beyond skip-connected encoded features, further extend the decoded features with attentive features. AEDN is better at establishing long-range dependencies between the encoder and decoder, therefore promoting more effective fusion of multi-scale features for handling scale-variations. Second, we design a new KL-divergence-based distribution loss to supervise the scale-aware structural differences between two density maps, which complements the pixel-isolated MSE loss and better optimizes AEDN to generate high-quality density maps. Third, we adopt a multi-scale supervision scheme, such that multiple KL divergences and MSE losses are deployed at all decoding stages, providing more thorough supervisions for different feature scales. Extensive experimental results on four public datasets, including ShanghaiTech Part A, ShanghaiTech Part B, UCF-CC-50, and UCF-QNRF, reveal the superiority and efficacy of the proposed method, which outperforms most state-of-the-art competitors.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3015199478",
    "type": "article"
  },
  {
    "title": "ACMNet",
    "doi": "https://doi.org/10.1145/3362065",
    "publication_date": "2020-01-31",
    "publication_year": 2020,
    "authors": "Hui Chen; Guiguang Ding; Zijia Lin; Sicheng Zhao; Xiaopeng Gu; Wenyuan Xu; Jungong Han",
    "corresponding_authors": "",
    "abstract": "Cross-modality human behavior analysis has attracted much attention from both academia and industry. In this article, we focus on the cross-modality image-text retrieval problem for human behavior analysis, which can learn a common latent space for cross-modality data and thus benefit the understanding of human behavior with data from different modalities. Existing state-of-the-art cross-modality image-text retrieval models tend to be fine-grained region-word matching approaches, where they begin with measuring similarities for each image region or text word followed by aggregating them to estimate the global image-text similarity. However, it is observed that such fine-grained approaches often encounter the similarity bias problem, because they only consider matched text words for an image region or matched image regions for a text word for similarity calculation, but they totally ignore unmatched words/regions, which might still be salient enough to affect the global image-text similarity. In this article, we propose an Adaptive Confidence Matching Network (ACMNet), which is also a fine-grained matching approach, to effectively deal with such a similarity bias. Apart from calculating the local similarity for each region(/word) with its matched words(/regions), ACMNet also introduces a confidence score for the local similarity by leveraging the global text(/image) information, which is expected to help measure the semantic relatedness of the region(/word) to the whole text(/image). Moreover, ACMNet also incorporates the confidence scores together with the local similarities in estimating the global image-text similarity. To verify the effectiveness of ACMNet, we conduct extensive experiments and make comparisons with state-of-the-art methods on two benchmark datasets, i.e., Flickr30k and MS COCO. Experimental results show that the proposed ACMNet can outperform the state-of-the-art methods by a clear margin, which well demonstrates the effectiveness of the proposed ACMNet in human behavior analysis and the reasonableness of tackling the mentioned similarity bias issue.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3022592147",
    "type": "article"
  },
  {
    "title": "Evaluation of Shared Resource Allocation Using SAND for ABR Streaming",
    "doi": "https://doi.org/10.1145/3388926",
    "publication_date": "2020-04-30",
    "publication_year": 2020,
    "authors": "Stefan Pham; Patrick Heeren; Calvin Schmidt; Dániel Silhavy; Stefan Arbanowski",
    "corresponding_authors": "",
    "abstract": "Adaptive bitrate media streaming clients adjust the quality of media content depending on the current network conditions. The shared resource allocation (SRA) feature defined in MPEG-SAND (server and network assisted DASH) allows servers to allocate bandwidth to streaming clients. This enables coordination and prioritization of clients that are connected to the same network bottleneck (e.g., to maximize the number of clients that can play back a stream fluently). In this article, we evaluate different bandwidth limitation strategies and analyze the effects on the clients. For this purpose, a testbed using multiple Raspberry Pis was created. The results show that in various scenarios, SRA improves the fairness and the QoE of streaming sessions. Solely allocating a maximum quality level to the client is not sufficient in some cases. Therefore, additional means, such as limiting bandwidth on the client or traffic shaping with software-defined networking for SRA, are evaluated.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3033684283",
    "type": "article"
  },
  {
    "title": "Learning Joint Structure for Human Pose Estimation",
    "doi": "https://doi.org/10.1145/3392302",
    "publication_date": "2020-07-05",
    "publication_year": 2020,
    "authors": "Shenming Feng; Haifeng Hu",
    "corresponding_authors": "",
    "abstract": "Recently, tremendous progress has been achieved on human pose estimation with the development of convolutional neural networks (CNNs). However, current methods still suffer from severe occlusion, back view, and large pose variation due to the lack of consideration of the spatial relationship between different joints, which can provide strong cues for localizing the hidden keypoints. In this work, we design a Structural Pose Network (SPN) to take full advantage of joint structure for human pose estimation under unconstrained environment. Specifically, the proposed model is composed of two subnets: Structure Residual Network (SRN) and Structure Improving Network (SIN). Given an input image, SRN first captures rich joint structure as priors through a multi-branch feature extraction module, following a hourglass network with pyramid residual units to enlarge the receptive field and further obtain structural feature representations. SIN, based on coordinate regression, can optimize the spatial relationship of different joints via the attention mechanism, thus refining the initial prediction from SRN. In addition, we propose a novel structure-consistency constraint, which can maintain the structural consistency between the joints and body parts via estimating whether the joints are located in their corresponding parts. At the same time, an online hard regions mining (OHRM) strategy is introduced to drive the network to pay corresponding attention to different body parts. The experimental results on three challenging datasets show that our method outperforms other state-of-the-art algorithms.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3082133990",
    "type": "article"
  },
  {
    "title": "Visual Comfort for Stereoscopic 3D by Using Motion Sensors on 3D Mobile Devices",
    "doi": "https://doi.org/10.1145/2808211",
    "publication_date": "2015-10-21",
    "publication_year": 2015,
    "authors": "Chung‐Hua Chu",
    "corresponding_authors": "Chung‐Hua Chu",
    "abstract": "Advanced 3D mobile devices attract a lot of attentions for 3D visualization nowadays. Stereoscopic images and video taken from the 3D mobile devices are uncomfortable for 3D viewing experiences due to the limited hardware for stereoscopic 3D stabilization. The existing stereoscopic 3D stabilization methods are computationally inefficient for the 3D mobile devices. In this article, we point out that this critical issue deteriorates the 3D viewing experiences on the 3D mobile devices. To improve visual comfort, we propose an efficient and effective algorithm to stabilize the stereoscopic images and video for the 3D mobile devices. To rectify the video jitter, we use the gyroscope and accelerometer embedded on the mobile devices to obtain the geometry information of the cameras. Using a different method than video-content-based motion estimation, our algorithm based on the gyroscope and acceleration data can achieve higher accuracy to effectively stabilize the video. Therefore, our approach is robust in video stabilization even under poor lighting and substantial foreground motion. Our algorithm outperforms previous approaches in not only smaller running time but also the better comfort of the stereoscopic 3D visualization for the 3D mobile devices.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W1982707498",
    "type": "article"
  },
  {
    "title": "A Hamming Embedding Kernel with Informative Bag-of-Visual Words for Video Semantic Indexing",
    "doi": "https://doi.org/10.1145/2535938",
    "publication_date": "2014-04-01",
    "publication_year": 2014,
    "authors": "Feng Wang; Wan‐Lei Zhao; Chong‐Wah Ngo; Bernard Mérialdo",
    "corresponding_authors": "",
    "abstract": "In this article, we propose a novel Hamming embedding kernel with informative bag-of-visual words to address two main problems existing in traditional BoW approaches for video semantic indexing. First, Hamming embedding is employed to alleviate the information loss caused by SIFT quantization. The Hamming distances between keypoints in the same cell are calculated and integrated into the SVM kernel to better discriminate different image samples. Second, to highlight the concept-specific visual information, we propose to weight the visual words according to their informativeness for detecting specific concepts. We show that our proposed kernels can significantly improve the performance of concept detection.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W1997360595",
    "type": "article"
  },
  {
    "title": "Saving disk energy in video servers by combining caching and prefetching",
    "doi": "https://doi.org/10.1145/2537856",
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "Minseok Song; Yeongju Lee; Euiseok Kim",
    "corresponding_authors": "",
    "abstract": "Maintenance and upgrades to the significant storage infrastructure in a video server often create a heterogenous disk array. We show how to manage the energy consumption of such an array by combining caching and prefetching techniques. We first examine how seek operations affect disk energy consumption, and then analyze the relationship between the amount of prefetched data and the number of seeks, and the effect of the size of the prefetching buffer on energy consumption. Based on this, we propose a new data prefetching scheme in which the amount of data prefetched for each video stream is dynamically adjusted to allow for the bit-rates of streams and the power characteristics of different disks. We next examine the impact of caching on disk power consumption and propose a new caching scheme that prioritizes each stream based on the ratio of the amount of energy that can be saved to its cache requirement, so as to make effective use of limited caching space. We address the trade-off between caching and prefetching and propose an algorithm that dynamically divides the entire buffer space into prefetching and caching regions, with the aim of minimizing overall disk energy consumption. Experimental results show that our scheme can reduce disk energy consumption between 26% and 31%, compared to a server without prefetching and caching.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2034219054",
    "type": "article"
  },
  {
    "title": "Large-Area, Multilayered, and High-Resolution Visual Monitoring Using a Dual-Camera System",
    "doi": "https://doi.org/10.1145/2645862",
    "publication_date": "2015-01-07",
    "publication_year": 2015,
    "authors": "Chih‐Wei Lin; Kuan‐Wen Chen; Shen-Chi Chen; Cheng‐Wu Chen; Yi‐Ping Hung",
    "corresponding_authors": "",
    "abstract": "Large-area, high-resolution visual monitoring systems are indispensable in surveillance applications. To construct such systems, high-quality image capture and display devices are required. Whereas high-quality displays have rapidly developed, as exemplified by the announcement of the 85-inch 4K ultrahigh-definition TV by Samsung at the 2013 Consumer Electronics Show (CES), high-resolution surveillance cameras have progressed slowly and remain not widely used compared with displays. In this study, we designed an innovative framework, using a dual-camera system comprising a wide-angle fixed camera and a high-resolution pan-tilt-zoom (PTZ) camera to construct a large-area, multilayered, and high-resolution visual monitoring system that features multiresolution monitoring of moving objects. First, we developed a novel calibration approach to estimate the relationship between the two cameras and calibrate the PTZ camera. The PTZ camera was calibrated based on the consistent property of distinct pan-tilt angle at various zooming factors, accelerating the calibration process without affecting accuracy; this calibration process has not been reported previously. After calibrating the dual-camera system, we used the PTZ camera and synthesized a large-area and high-resolution background image. When foreground targets were detected in the images captured by the wide-angle camera, the PTZ camera was controlled to continuously track the user-selected target. Last, we integrated preconstructed high-resolution background and low-resolution foreground images captured using the wide-angle camera and the high-resolution foreground image captured using the PTZ camera to generate a large-area, multilayered, and high-resolution view of the scene.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2059510267",
    "type": "article"
  },
  {
    "title": "Secure Social Multimedia Big Data Sharing Using Scalable JFE in the TSHWT Domain",
    "doi": "https://doi.org/10.1145/2978571",
    "publication_date": "2016-09-15",
    "publication_year": 2016,
    "authors": "Conghuan Ye; Hefei Ling; Zenggang Xiong; Fuhao Zou; Cong Liu; Fang Xu",
    "corresponding_authors": "",
    "abstract": "With the advent of social networks and cloud computing, the amount of multimedia data produced and communicated within social networks is rapidly increasing. In the meantime, social networking platforms based on cloud computing have made multimedia big data sharing in social networks easier and more efficient. The growth of social multimedia, as demonstrated by social networking sites such as Facebook and YouTube, combined with advances in multimedia content analysis, underscores potential risks for malicious use, such as illegal copying, piracy, plagiarism, and misappropriation. Therefore, secure multimedia sharing and traitor tracing issues have become critical and urgent in social networks. In this article, a joint fingerprinting and encryption (JFE) scheme based on tree-structured Haar wavelet transform (TSHWT) is proposed with the purpose of protecting media distribution in social network environments. The motivation is to map hierarchical community structure of social networks into a tree structure of Haar wavelet transform for fingerprinting and encryption. First, fingerprint code is produced using social network analysis (SNA). Second, the content is decomposed based on the structure of fingerprint code by the TSHWT. Then, the content is fingerprinted and encrypted in the TSHWT domain. Finally, the encrypted contents are delivered to users via hybrid multicast-unicast. The proposed method, to the best of our knowledge, is the first scalable JFE method for fingerprinting and encryption in the TSHWT domain using SNA. The use of fingerprinting along with encryption using SNA not only provides a double layer of protection for social multimedia sharing in social network environment but also avoids big data superposition effect. Theory analysis and experimental results show the effectiveness of the proposed JFE scheme.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2520064023",
    "type": "article"
  },
  {
    "title": "Toward an Adaptive Screencast Platform",
    "doi": "https://doi.org/10.1145/2886778",
    "publication_date": "2016-11-08",
    "publication_year": 2016,
    "authors": "Chih-Fan Hsu; Ching-Ling Fan; Tsung‐Han Tsai; Chun‐Ying Huang; Cheng-Hsin Hsu; Kuan‐Ta Chen",
    "corresponding_authors": "",
    "abstract": "The binding between computing devices and displays is becoming dynamic and adaptive, and screencast technologies enable such binding over wireless networks. In this article, we design and conduct the first detailed measurement study on the performance of the state-of-the-art screencast technologies. Several commercial and one open-source screencast technologies are considered in our detailed analysis, which leads to several insights: (1) there is no single winning screencast technology, indicating room to further enhance the screencast technologies; (2) hardware video encoders significantly reduce the CPU usage at the expense of slightly higher GPU usage and end-to-end delay, and should be adopted in future screencast technologies; (3) comprehensive error resilience tools are needed as wireless communication is vulnerable to packet loss; (4) emerging video codecs designed for screen contents lead to a better Quality of Experience (QoE) of screencast; and (5) rate adaptation mechanisms are critical to avoiding degraded QoE due to network dynamics. As a case study, we propose a nonintrusive yet accurate available bandwidth estimation mechanism. Real experiments demonstrate the practicality and efficiency of our proposed solution. Our measurement methodology, open-source screencast platform, and case study allow researchers and developers to quantitatively evaluate other design considerations, which will lead to optimized screencast technologies.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2556458506",
    "type": "article"
  },
  {
    "title": "Predicting Occupation from Images by Combining Face and Body Context Information",
    "doi": "https://doi.org/10.1145/3009911",
    "publication_date": "2016-12-02",
    "publication_year": 2016,
    "authors": "Wei-Ta Chu; Chih-Hao Chiu",
    "corresponding_authors": "",
    "abstract": "Facial images embed age, gender, and other rich information that is implicitly related to occupation. In this work, we advocate that occupation prediction from a single facial image is a doable computer vision problem. We extract multilevel hand-crafted features associated with locality-constrained linear coding and convolutional neural network features as image occupation descriptors. To avoid the curse of dimensionality and overfitting, a boost strategy called multichannel SVM is used to integrate features from face and body. Intra- and interclass visual variations are jointly considered in the boosting framework to further improve performance. In the evaluation, we verify the effectiveness of predicting occupation from face and demonstrate promising performance obtained by combining face and body information. More importantly, our work further integrates deep features into the multichannel SVM framework and shows significantly better performance over the state of the art.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2558415188",
    "type": "article"
  },
  {
    "title": "Self-supervised Multi-view Learning via Auto-encoding 3D Transformations",
    "doi": "https://doi.org/10.1145/3597613",
    "publication_date": "2023-05-22",
    "publication_year": 2023,
    "authors": "Xiang Gao; Wei Hu; Guo-Jun Qi",
    "corresponding_authors": "",
    "abstract": "3D object representation learning is a fundamental challenge in computer vision to infer about the 3D world. Recent advances in deep learning have shown their efficiency in 3D object recognition, among which view-based methods have performed best so far. However, feature learning of multiple views in existing methods is mostly performed in a supervised fashion, which often requires a large amount of data labels with high costs. In contrast, self-supervised learning aims to learn multi-view feature representations without involving labeled data. To this end, we propose a novel self-supervised framework to learn Multi-View Transformation Equivariant Representations (MV-TER), exploring the equivariant transformations of a 3D object and its projected multiple views that we derive. Specifically, we perform a 3D transformation on a 3D object and obtain multiple views before and after the transformation via projection. Then, we train a representation encoding module to capture the intrinsic 3D object representation by decoding 3D transformation parameters from the fused feature representations of multiple views before and after the transformation. Experimental results demonstrate that the proposed MV-TER significantly outperforms the state-of-the-art view-based approaches in 3D object classification and retrieval tasks and show the generalization to real-world datasets. The code is available at https://github.com/gyshgx868/mvter .",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4377244980",
    "type": "article"
  },
  {
    "title": "DiRaC-I: Identifying Diverse and Rare Training Classes for Zero-Shot Learning",
    "doi": "https://doi.org/10.1145/3603147",
    "publication_date": "2023-05-31",
    "publication_year": 2023,
    "authors": "Sandipan Sarma; Arijit Sur",
    "corresponding_authors": "",
    "abstract": "Zero-Shot Learning (ZSL) is an extreme form of transfer learning that aims at learning from a few “seen classes” to have an understanding about the “unseen classes” in the wild. Given a dataset in ZSL research, most existing works use a predetermined, disjoint set of seen-unseen classes to evaluate their methods. These seen (training) classes might be sub-optimal for ZSL methods to appreciate the diversity and rarity of an object domain. Inspired by strategies like active learning, it is intuitive that intelligently selecting the training classes can improve ZSL performance. In this work, we propose a framework called Diverse and Rare Class Identifier (DiRaC-I) which, given an attribute-based dataset, can intelligently yield the most suitable “seen classes” for training ZSL models. DiRaC-I has two main goals – constructing a diversified set of seed classes, and using them to initialize a visual-semantic mining algorithm for acquiring the classes capturing both diversity and rarity in the object domain adequately. These classes can then be used as “seen classes” to train ZSL models for image classification. We simulate a real-world scenario where visual samples of novel object classes in the wild are available to neither DiRaC-I nor the ZSL models during training and conducted extensive experiments on two benchmark data sets for zero-shot image classification — CUB and SUN. Our results demonstrate DiRaC-I helps ZSL models to achieve significant classification accuracy improvements – specifically, up to 8% for CUB and up to 5% for SUN dataset. Additionally, while recognizing classes exhibiting rare attributes we also observe a performance boost for ZSL models, which is up to 10% and 7% for CUB and SUN datasets, respectively.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4378807209",
    "type": "article"
  },
  {
    "title": "An Applied Image Cryptosystem on Moore’s Automaton Operating on δ ( <i> q <sub>k</sub> </i> )/𝔽 <sub>2</sub>",
    "doi": "https://doi.org/10.1145/3614433",
    "publication_date": "2023-08-09",
    "publication_year": 2023,
    "authors": "Subhrajyoti Deb; Abhilash Kumar Das; Nirmalya Kar",
    "corresponding_authors": "",
    "abstract": "The volume of multimedia-based image data or video frames in Web 3.0 is constantly increasing, owing to the advancement of real-time data transmission. However, security vulnerabilities frequently impair the performance of real-time applications. Many researchers have recently proposed image encryption schemes based on a high-dimensional chaotic system due to properties such as ergodicity and initial state sensitivity. Nonetheless, most schemes have suffered from excessive computational complexity, low security, and the generation of cryptographically secure random numbers. To overcome these challenges, an efficient and highly secure cryptosystem is necessary for safe multimedia transmission in Web 3.0. This article proposes a novel work on the image cryptosystem based on the Escalation function with a one-time key-oriented Moore’s Automaton over a finite field 𝔽 2 . The Escalation function is a nonlinear scrambling technique for plaintext images that goes through the confusion phase and plays an essential role in row-column permutation. To make the algorithm more secure and robust in the diffusion phase, the proposed Moore’s Automaton produced ciphertext images through a highly random key stream generated by the combination of a logistic map and cyclic group. Specifically, the proposed Moore’s Automaton operates on δ ( q k )/𝔽 2 to render random binary bits into unpredictable sequences to construct ciphertext images. Our new finding quickens the speed and provides adequate key space, and pixel distributions are more uniform, have high entropy value, and are secure against differential and statistical attacks.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4385699316",
    "type": "article"
  },
  {
    "title": "Layered unequal loss protection with pre-interleaving for fast progressive image transmission over packet-loss channels",
    "doi": "https://doi.org/10.1145/1111604.1111606",
    "publication_date": "2005-11-01",
    "publication_year": 2005,
    "authors": "Jianfei Cai; Xiang-Jun Li; Chang Wen Chen",
    "corresponding_authors": "",
    "abstract": "Most existing unequal loss protection (ULP) schemes do not consider the minimum quality requirement and usually have high computation complexity. In this research, we propose a layered ULP (L-ULP) scheme to solve these problems. In particular, we use the rate-based optimal solution with a local search to find the average forward error correction (FEC) allocation and use the gradient search to find the FEC solution for each layer. Experimental results show that the executing time of L-ULP is much faster than the traditional ULP scheme but the average distortion is worse. Therefore, we further propose to combine the L-ULP with the pre-interleaving to have an improved L-ULP (IL-ULP) system. By using the pre-interleaving, we are able to delay the occurrence of the first unrecoverable loss in the source bitstream and thus improve the loss resilience performance. With the better loss resilience performance in the source bitstream, our proposed IL-ULP scheme is allowed to have a weaker FEC protection and allocate more bits to the source coding which leads to the improvement of overall performance. Experimental results show that our proposed IL-ULP scheme even outperforms the global optimal result obtained by any traditional ULP scheme while the complexity of IL-ULP is almost the same as L-ULP.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2150293110",
    "type": "article"
  },
  {
    "title": "Robust content-based MPEG-4 XMT scene structure authentication and multimedia content location",
    "doi": "https://doi.org/10.1145/1236471.1236477",
    "publication_date": "2007-08-01",
    "publication_year": 2007,
    "authors": "Ziad Sakr; Nicolas D. Georganas",
    "corresponding_authors": "",
    "abstract": "For the past decade, there have been numerous research works focusing on the protection of digital images, audio, video, 3D virtual scenes, and software data from unauthorized use and distribution. With the emerging technology of the MPEG-4 standard, MPEG-4 scenes that may include images, video, audio, and 3D objects can easily be built using the text-based MPEG-4 XMT standard. XMT allows content authors to exchange their content with other authors, tools, or service providers and facilitates interoperability with MPEG-4, X3D, and SMIL. In order for owners and designers to protect and/or authenticate their work, some form of security needs to be applied into the MPEG-4 XMT structure and its media content. Unlike images or videos, watermarking an XMT structure is not an easy task, since the structure contains no noise components to embed the watermark. This article is the first one proposing a novel robust algorithm for the authentication of a given MPEG-4 XMT structured scene and the location of its multimedia content.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W1993349026",
    "type": "article"
  },
  {
    "title": "Compact and progressive plant models for streaming in networked virtual environments",
    "doi": "https://doi.org/10.1145/1556134.1556138",
    "publication_date": "2009-08-01",
    "publication_year": 2009,
    "authors": "Sébastien Mondet; Wei Cheng; Géraldine Morin; Romulus Grigoraş; Frédéric Boudon; Wei Tsang Ooi",
    "corresponding_authors": "",
    "abstract": "Just as in the real world, plants are important objects in virtual worlds for creating pleasant and realistic environments, especially those involving natural scenes. As such, much effort has been made in realistic modeling of plants. As the trend moves towards networked and distributed virtual environments, however, the current models are inadequate as they are not designed for progressive transmissions. In this article, we fill in this gap by proposing a progressive representation for plants based on generalized cylinders. We model the shape and thickness of branches in a plant as Bézier curves, group the curves according to the similarity, and differentially code the curves to represent the plant in a compact and progressive manner. To facilitate the transmission of the plants, we quantify the visual contribution of each branch and use this weight in packet scheduling. We show the efficiency of our representations and the effectiveness of our packet scheduler through experiments over a wide area network.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W1968693256",
    "type": "article"
  },
  {
    "title": "A study of content authentication in proxy-enabled multimedia delivery systems",
    "doi": "https://doi.org/10.1145/1596990.1596992",
    "publication_date": "2009-10-01",
    "publication_year": 2009,
    "authors": "Robert H. Deng; Yanjiang Yang",
    "corresponding_authors": "",
    "abstract": "Compared with the direct server-user approach, the server-proxy-user architecture for multimedia delivery promises significantly improved system scalability. The introduction of the intermediary transcoding proxies between content servers and end users in this architecture, however, brings unprecedented challenges to content security. In this article, we present a systematic study on the end-to-end content authentication problem in the server-proxy-user context, where intermediary proxies transcode multimedia content dynamically. We present a formal model for the authentication problem, propose a concrete construction for authenticating generic data modality and formally prove its security. We then apply the generic construction to authenticating specific multimedia formats, for example, JPEG2000 code-streams and MPEG-4 video streams. The prototype implementation shows that our scheme is suitable for practical applications.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2032675933",
    "type": "article"
  },
  {
    "title": "Cumulative Quality Modeling for HTTP Adaptive Streaming",
    "doi": "https://doi.org/10.1145/3423421",
    "publication_date": "2021-02-28",
    "publication_year": 2021,
    "authors": "Huyen T. T. Tran; Nam Pham Ngoc; Tobias Hoßfeld; Michael Seufert; Truong Cong Thang",
    "corresponding_authors": "",
    "abstract": "HTTP Adaptive Streaming has become the de facto choice for multimedia delivery. However, the quality of adaptive video streaming may fluctuate strongly during a session due to throughput fluctuations. So, it is important to evaluate the quality of a streaming session over time. In this article, we propose a model to estimate the cumulative quality for HTTP Adaptive Streaming. In the model, a sliding window of video segments is employed as the basic building block. Through statistical analysis using a subjective dataset, we identify four important components of the cumulative quality model, namely the minimum window quality, the last window quality, the maximum window quality, and the average window quality. Experiment results show that the proposed model achieves high prediction performance and outperforms related quality models. In addition, another advantage of the proposed model is its simplicity and effectiveness for deployment in real-time estimation. Our subjective dataset as well as the source code of the proposed model have been made publicly available at https://sites.google.com/site/huyenthithanhtran1191/cqmdatabase .",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3152638807",
    "type": "article"
  },
  {
    "title": "Bi-manual Haptic-based Periodontal Simulation with Finger Support and Vibrotactile Feedback",
    "doi": "https://doi.org/10.1145/3421765",
    "publication_date": "2021-02-28",
    "publication_year": 2021,
    "authors": "Said Chehabeddine; Muhammad Hassan Jamil; Wanjoo Park; Dianne L. Sefo; Peter M. Loomer; Mohamad Eid",
    "corresponding_authors": "",
    "abstract": "The rise of virtual reality and haptic technologies has created exciting new applications in medical training and education. In a dental simulation, haptic technology can create the illusion of substances (teeth, gingiva, bone, etc.) by providing interaction forces within a simulated virtual world of the mouth. In this article, a haptic periodontal training simulation system, named Haptodont, is developed and evaluated for simulating periodontal probing. Thirty-two faculty members from New York University College of Dentistry were recruited and divided into three groups to evaluate three fundamental functionalities: Group 1 evaluated bi-manual 3 Degrees of Freedome (DoF) haptic interaction, Group 2 evaluated bi-manual 3 DoF haptic interaction with a finger support mechanism, and Group 3 evaluated bi-manual 3 DoF haptic interaction with finger support mechanism and vibrotactile feedback. The probe and mirror interactions were simulated with the Geomagic Touch haptic device whereas the finger support was implemented using the Novint Falcon device. The three groups conducted two probing tasks: healthy gingiva scenario with no pockets (2- to 3-mm depth) and periodontitis scenario with deep pockets (4- to 8-mm depth). Results demonstrated that experts performed comparably to clinical settings in terms of probing depth error (within 0.3 to 0.6 mm) and probing forces (less than 0.5 N). Furthermore, the finger support mechanism significantly improved the probing accuracy for periodontitis condition in the lingual region. The argument that probing the lingual region is more difficult than the buccal region is supported by quantitative evidence (significantly higher probing depth error and probing force). Further research is planned to improve the usability of the finger support, integrate the Haptodont system into the pre-clinical curriculum, and evaluate the Haptodont system with dental students as a learning tool.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3153448570",
    "type": "article"
  },
  {
    "title": "Lifelog Image Retrieval Based on Semantic Relevance Mapping",
    "doi": "https://doi.org/10.1145/3446209",
    "publication_date": "2021-07-22",
    "publication_year": 2021,
    "authors": "Qianli Xu; Ana García del Molino; Jie Lin; Fen Fang; Vigneshwaran Subbaraju; Liyuan Li; Joo‐Hwee Lim",
    "corresponding_authors": "",
    "abstract": "Lifelog analytics is an emerging research area with technologies embracing the latest advances in machine learning, wearable computing, and data analytics. However, state-of-the-art technologies are still inadequate to distill voluminous multimodal lifelog data into high quality insights. In this article, we propose a novel semantic relevance mapping ( SRM ) method to tackle the problem of lifelog information access. We formulate lifelog image retrieval as a series of mapping processes where a semantic gap exists for relating basic semantic attributes with high-level query topics. The SRM serves both as a formalism to construct a trainable model to bridge the semantic gap and an algorithm to implement the training process on real-world lifelog data. Based on the SRM, we propose a computational framework of lifelog analytics to support various applications of lifelog information access, such as image retrieval, summarization, and insight visualization. Systematic evaluations are performed on three challenging benchmarking tasks to show the effectiveness of our method.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3186150303",
    "type": "article"
  },
  {
    "title": "Pedestrian-Aware Panoramic Video Stitching Based on a Structured Camera Array",
    "doi": "https://doi.org/10.1145/3460511",
    "publication_date": "2021-11-12",
    "publication_year": 2021,
    "authors": "Anqi Zhu; Lin Zhang; Juntao Chen; Yicong Zhou",
    "corresponding_authors": "",
    "abstract": "The panorama stitching system is an indispensable module in surveillance or space exploration. Such a system enables the viewer to understand the surroundings instantly by aligning the surrounding images on a plane and fusing them naturally. The bottleneck of existing systems mainly lies in alignment and naturalness of the transition of adjacent images. When facing dynamic foregrounds, they may produce outputs with misaligned semantic objects, which is evident and sensitive to human perception. We solve three key issues in the existing workflow that can affect its efficiency and the quality of the obtained panoramic video and present Pedestrian360, a panoramic video system based on a structured camera array (a spatial surround-view camera system). First, to get a geometrically aligned 360○ view in the horizontal direction, we build a unified multi-camera coordinate system via a novel refinement approach that jointly optimizes camera poses. Second, to eliminate the brightness and color difference of images taken by different cameras, we design a photometric alignment approach by introducing a bias to the baseline linear adjustment model and solving it with two-step least-squares. Third, considering that the human visual system is more sensitive to high-level semantic objects, such as pedestrians and vehicles, we integrate the results of instance segmentation into the framework of dynamic programming in the seam-cutting step. To our knowledge, we are the first to introduce instance segmentation to the seam-cutting problem, which can ensure the integrity of the salient objects in a panorama. Specifically, in our surveillance oriented system, we choose the most significant target, pedestrians, as the seam avoidance target, and this accounts for the name Pedestrian360 . To validate the effectiveness and efficiency of Pedestrian360, a large-scale dataset composed of videos with pedestrians in five scenes is established. The test results on this dataset demonstrate the superiority of Pedestrian360 compared to its competitors. Experimental results show that Pedestrian360 can stitch videos at a speed of 12 to 26 fps, which depends on the number of objects in the shooting scene and their frequencies of movements. To make our reported results reproducible, the relevant code and collected data are publicly available at https://cslinzhang.github.io/Pedestrian360-Homepage/ .",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3212523225",
    "type": "article"
  },
  {
    "title": "Dual-Stream Guided-Learning via <i>a Priori</i> Optimization for Person Re-identification",
    "doi": "https://doi.org/10.1145/3447715",
    "publication_date": "2021-11-30",
    "publication_year": 2021,
    "authors": "Junyi Wu; Yan Huang; Qiang Wu; Zhipeng Gao; Jianqiang Zhao; Liqin Huang",
    "corresponding_authors": "",
    "abstract": "The task of person re-identification (re-ID) is to find the same pedestrian across non-overlapping camera views. Generally, the performance of person re-ID can be affected by background clutter. However, existing segmentation algorithms cannot obtain perfect foreground masks to cover the background information clearly. In addition, if the background is completely removed, some discriminative ID-related cues (i.e., backpack or companion) may be lost. In this article, we design a dual-stream network consisting of a Provider Stream (P-Stream) and a Receiver Stream (R-Stream). The R-Stream performs an a priori optimization operation on foreground information. The P-Stream acts as a pusher to guide the R-Stream to concentrate on foreground information and some useful ID-related cues in the background. The proposed dual-stream network can make full use of the a priori optimization and guided-learning strategy to learn encouraging foreground information and some useful ID-related information in the background. Our method achieves Rank-1 accuracy of 95.4% on Market-1501, 89.0% on DukeMTMC-reID, 78.9% on CUHK03 (labeled), and 75.4% on CUHK03 (detected), outperforming state-of-the-art methods.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4205659497",
    "type": "article"
  },
  {
    "title": "Looking forward 10 years to multimedia successes",
    "doi": "https://doi.org/10.1145/2490825",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Lawrence A. Rowe",
    "corresponding_authors": "Lawrence A. Rowe",
    "abstract": "A panel at ACM Multimedia 2012 addressed research successes in the past 20 years. While the panel focused on the past, this article discusses successes since the ACM SIGMM 2003 Retreat and suggests research directions in the next ten years. While significant progress has been made, more research is required to allow multimedia to impact our everyday computing environment. The importance of hardware changes on future research directions is discussed. We believe ubiquitous computing—meaning abundant computation and network bandwidth—should be applied in novel ways to solve multimedia grand challenges and continue the IT revolution of the past century.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W1982797333",
    "type": "article"
  },
  {
    "title": "Supporting region-of-interest cropping through constrained compression",
    "doi": "https://doi.org/10.1145/2000486.2000491",
    "publication_date": "2011-08-01",
    "publication_year": 2011,
    "authors": "Wu‐chi Feng; Thanh Dang; John Kassebaum; Tim Bauman",
    "corresponding_authors": "",
    "abstract": "The ability to create super high-resolution video is becoming relative easy to do either through a single high-definition video camera or panoramic video that automatically stitches multiple views together. As an example of the former, the motion picture industry now has 6000 × 4000 pixel full-rate video cameras available. This means that supporting region-of-interest cropping will become more important in the future. In this article, we propose a mechanism to support region-of-interest adaptation of stored video. The proposed approach creates a compression-compliant stream (e.g., MPEG-2), while still allowing it to be cropped. Fortunately, video standards like MPEG-2 specify the format of a compliant stream, and not the algorithm to get there. As a result, there is an opportunity to allow system researchers and implementers ways to optimize for applications. We show various fundamental tradeoffs that are made in order to support region-of-interest cropping with super high-resolution video which we received from a local motion-picture firm.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2003283853",
    "type": "article"
  },
  {
    "title": "Waiting-time prediction in scalable on-demand video streaming",
    "doi": "https://doi.org/10.1145/1671962.1671967",
    "publication_date": "2010-03-01",
    "publication_year": 2010,
    "authors": "Nabil J. Sarhan; Mohammad Alsmirat; Musab S. Al-Hadrusi",
    "corresponding_authors": "",
    "abstract": "Providing video streaming users with expected waiting times enhances their perceived quality-of-service (QoS) and encourages them to wait. In the absence of any waiting-time feedback, users are more likely to defect because of the uncertainty as to when their services will start. We analyze waiting-time predictability in scalable video streaming. We propose two prediction schemes and study their effectiveness when applied with various stream merging techniques and scheduling policies. The results demonstrate that the waiting time can be predicted accurately, especially when enhanced cost-based scheduling is applied. The combination of waiting-time prediction and cost-based scheduling leads to outstanding performance benefits.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2009348090",
    "type": "article"
  },
  {
    "title": "Optimizing consistency by maximizing bandwidth usage in distributed interactive applications",
    "doi": "https://doi.org/10.1145/1865106.1865114",
    "publication_date": "2010-11-01",
    "publication_year": 2010,
    "authors": "Damien Marshall; Séamus McLoone; Tomás Ward",
    "corresponding_authors": "",
    "abstract": "A key factor determining the success of a Distributed Interactive Application (DIA) is the maintenance of a consistent shared virtual world. To help maintain consistency, a number of Information Management techniques have been developed. However, unless carefully tuned to the underlying network, they can negatively impact on consistency. This work presents a novel adaptive algorithm for optimizing consistency by maximizing available bandwidth usage in DIAs. This algorithm operates by estimating bandwidth from trends in network latency, and modifying data transmission rates to match the estimated value. Results presented within demonstrate that this approach can help optimise consistency levels in a DIA.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2070756229",
    "type": "article"
  },
  {
    "title": "Efficient targeted search using a focus and context video browser",
    "doi": "https://doi.org/10.1145/2379790.2379793",
    "publication_date": "2012-11-01",
    "publication_year": 2012,
    "authors": "Ork de Rooij; Marcel Worring",
    "corresponding_authors": "",
    "abstract": "Currently there are several interactive content-based video retrieval techniques and systems available. However, retrieval performance depends heavily on the means of interaction. We argue that effective CBVR requires efficient, specialized user interfaces. In this article we propose guidelines for such an interface, and we propose an effective CBVR engine: the ForkBrowser, which builds upon the principle of focus and context. This browser is evaluated using a combination of user simulation and real user evaluation. Results indicate that the ideas have merit, and that the browser performs very well when compared to the state-of-the-art in video retrieval.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2103069297",
    "type": "article"
  },
  {
    "title": "Identification of Reconstructed Speech",
    "doi": "https://doi.org/10.1145/3004055",
    "publication_date": "2017-01-17",
    "publication_year": 2017,
    "authors": "Haojun Wu; Yong Wang; Jiwu Huang",
    "corresponding_authors": "",
    "abstract": "Both voice conversion and hidden Markov model-- (HMM) based speech synthesis can be used to produce artificial voices of a target speaker. They have shown great negative impacts on speaker verification (SV) systems. In order to enhance the security of SV systems, the techniques to detect converted/synthesized speech should be taken into consideration. During voice conversion and HMM-based synthesis, speech reconstruction is applied to transform a set of acoustic parameters to reconstructed speech. Hence, the identification of reconstructed speech can be used to distinguish converted/synthesized speech from human speech. Several related works on such identification have been reported. The equal error rates (EERs) lower than 5% of detecting reconstructed speech have been achieved. However, through the cross-database evaluations on different speech databases, we find that the EERs of several testing cases are higher than 10%. The robustness of detection algorithms to different speech databases needs to be improved. In this article, we propose an algorithm to identify the reconstructed speech. Three different speech databases and two different reconstruction methods are considered in our work, which has not been addressed in the reported works. The high-dimensional data visualization approach is used to analyze the effect of speech reconstruction on Mel-frequency cepstral coefficients (MFCC) of speech signals. The Gaussian mixture model supervectors of MFCC are used as acoustic features. Furthermore, a set of commonly used classification algorithms are applied to identify reconstructed speech. According to the comparison among different classification methods, linear discriminant analysis-ensemble classifiers are chosen in our algorithm. Extensive experimental results show that the EERs lower than 1% can be achieved by the proposed algorithm in most cases, outperforming the reported state-of-the-art identification techniques.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2573790841",
    "type": "article"
  },
  {
    "title": "Recognizing Human Actions with Outlier Frames by Observation Filtering and Completion",
    "doi": "https://doi.org/10.1145/3089250",
    "publication_date": "2017-06-28",
    "publication_year": 2017,
    "authors": "Shih-Yao Lin; Yen‐Yu Lin; Chu‐Song Chen; Yi‐Ping Hung",
    "corresponding_authors": "",
    "abstract": "This article addresses the problem of recognizing partially observed human actions. Videos of actions acquired in the real world often contain corrupt frames caused by various factors. These frames may appear irregularly, and make the actions only partially observed. They change the appearance of actions and degrade the performance of pretrained recognition systems. In this article, we propose an approach to address the corrupt-frame problem without knowing their locations and durations in advance. The proposed approach includes two key components: outlier filtering and observation completion . The former identifies and filters out unobserved frames, and the latter fills up the filtered parts by retrieving coherent alternatives from training data. Hidden Conditional Random Fields (HCRFs) are then used to recognize the filtered and completed actions. Our approach has been evaluated on three datasets, which contain both fully observed actions and partially observed actions with either real or synthetic corrupt frames. The experimental results show that our approach performs favorably against the other state-of-the-art methods, especially when corrupt frames are present.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2732477051",
    "type": "article"
  },
  {
    "title": "Sparse Representation-Based Semi-Supervised Regression for People Counting",
    "doi": "https://doi.org/10.1145/3106156",
    "publication_date": "2017-08-01",
    "publication_year": 2017,
    "authors": "Hongbo Zhang; Bineng Zhong; Qing Lei; Ji‐Xiang Du; Jialin Peng; Duansheng Chen; Xiao Ke",
    "corresponding_authors": "",
    "abstract": "Label imbalance and the insufficiency of labeled training samples are major obstacles in most methods for counting people in images or videos. In this work, a sparse representation-based semi-supervised regression method is proposed to count people in images with limited data. The basic idea is to predict the unlabeled training data, select reliable samples to expand the labeled training set, and retrain the regression model. In the algorithm, the initial regression model, which is learned from the labeled training data, is used to predict the number of people in the unlabeled training dataset. Then, the unlabeled training samples are regarded as an over-complete dictionary. Each feature of the labeled training data can be expressed as a sparse linear approximation of the unlabeled data. In turn, the labels of the labeled training data can be estimated based on a sparse reconstruction in feature space. The label confidence in labeling an unlabeled sample is estimated by calculating the reconstruction error. The training set is updated by selecting unlabeled samples with minimal reconstruction errors, and the regression model is retrained on the new training set. A co-training style method is applied during the training process. The experimental results demonstrate that the proposed method has a low mean square error and mean absolute error compared with those of state-of-the-art people-counting benchmarks.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2739687869",
    "type": "article"
  },
  {
    "title": "Progressive Motion Vector Clustering for Motion Estimation and Auxiliary Tracking",
    "doi": "https://doi.org/10.1145/2700296",
    "publication_date": "2015-02-05",
    "publication_year": 2015,
    "authors": "Ke Chen; Zhong Zhou; Wei Wu",
    "corresponding_authors": "",
    "abstract": "The motion vector similarity between neighboring blocks is widely used in motion estimation algorithms. However, for nonneighboring blocks, they may also have similar motions due to close depths or belonging to the same object inside the scene. Therefore, the motion vectors usually have several kinds of patterns, which reveal a clustering structure. In this article, we propose a progressive clustering algorithm, which periodically counts the motion vectors of the past blocks to make incremental clustering statistics. These statistics are used as the motion vector predictors for the following blocks. It is proved to be much more efficient for one block to find the best-matching candidate with the predictors. We also design the clustering based search with CUDA for GPU acceleration. Another interesting application of the clustering statistics is persistent static object tracking. Based on the statistics, several auxiliary tracking areas are created to guide the object tracking. Even when the target object has significant changes in appearance or it disappears occasionally, its position still can be predicted. The experiments on Xiph.org Video Test Media dataset illustrate that our clustering based search algorithm outperforms the mainstream and some state-of-the-art motion estimation algorithms. It is 33 times faster on average than the full search algorithm with only slightly higher mean-square error values in the experiments. The tracking results show that the auxiliary tracking areas help to locate the target object effectively.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2016813870",
    "type": "article"
  },
  {
    "title": "Efficient MAC for Real-Time Video Streaming over Wireless LAN",
    "doi": "https://doi.org/10.1145/2744412",
    "publication_date": "2015-06-02",
    "publication_year": 2015,
    "authors": "Eilwoo Baik; Amit Pande; Prasant Mohapatra",
    "corresponding_authors": "",
    "abstract": "Wireless communication systems are highly prone to channel errors. With video being a major player in Internet traffic and undergoing exponential growth in wireless domain, we argue for the need of a Video-aware MAC (VMAC) to significantly improve the throughput and delay performance of real-time video streaming service. VMAC makes two changes to optimize wireless LAN for video traffic: (a) It incorporates a Perceptual-Error-Tolerance (PET) to the MAC frames by reducing MAC retransmissions while minimizing any impact on perceptual video quality; and (b) It uses a group NACK-based Adaptive Window (NAW) of MAC frames to improve both throughput and delay performance in varying channel conditions. Through simulations and experiments, we observe 56--89% improvement in throughput and 34--48% improvement in delay performance over legacy DCF and 802.11e schemes. VMAC also shows 15--78% improvement over legacy schemes with multiple clients.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2140816266",
    "type": "article"
  },
  {
    "title": "CelebrityNet",
    "doi": "https://doi.org/10.1145/2801125",
    "publication_date": "2015-08-24",
    "publication_year": 2015,
    "authors": "Li-Jia Li; David A. Shamma; Xiangnan Kong; Sina Jafarpour; Roelof van Zwol; Xuanhui Wang",
    "corresponding_authors": "",
    "abstract": "Photos are an important information carrier for implicit relationships. In this article, we introduce an image based social network, called CelebrityNet , built from implicit relationships encoded in a collection of celebrity images. We analyze the social properties reflected in this image-based social network and automatically infer communities among the celebrities. We demonstrate the interesting discoveries of the CelebrityNet. We particularly compare the inferred communities with human manually labeled ones and show quantitatively that the automatically detected communities are highly aligned with that of human interpretation. Inspired by the uniqueness of visual content and tag concepts within each community of the CelebrityNet, we further demonstrate that the constructed social network can serve as a knowledge base for high-level visual recognition tasks. In particular, this social network is capable of significantly improving the performance of automatic image annotation and classification of unknown images.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2261926893",
    "type": "article"
  },
  {
    "title": "Depth-Based View-Invariant Blind 3D Image Watermarking",
    "doi": "https://doi.org/10.1145/2957751",
    "publication_date": "2016-08-03",
    "publication_year": 2016,
    "authors": "Shuvendu Rana; Arijit Sur",
    "corresponding_authors": "",
    "abstract": "With the huge advance in Internet technology as well as the availability of low-cost 3D display devices, 3D image transmission has become popular in recent times. Since watermarking has become regarded as a potential Digital Rights Management (DRM) tools in the past decade, 3D image watermarking is an emerging research topic. With the introduction of the Depth Image-Based Rendering (DIBR) technique, 3D image watermarking is a more challenging task, especially for synthetic view generation. In this article, synthetic view generation is regarded as a potential attack, and a blind watermarking scheme is proposed that can resist it. In the proposed scheme, the watermark is embedded into the low-pass filtered dependent view region of 3D images. Block Discrete Cosine Transformation (DCT) is used for spatial-filtration of the dependent view region to find the DC coefficient with horizontally shifted coherent regions from the left and right view to make the scheme robust against synthesis view attack. A comprehensive set of experiments have been carried out to justify the robustness of the proposed scheme over related existing schemes with respect to Stereo JPEG compression and different noise addition attacks.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2482889576",
    "type": "article"
  },
  {
    "title": "Congestion-Aware MAC Layer Adaptation to Improve Video Telephony over Wi-Fi",
    "doi": "https://doi.org/10.1145/2983634",
    "publication_date": "2016-11-15",
    "publication_year": 2016,
    "authors": "Wei Chen; Liangping Ma; Chien-Chung Shen",
    "corresponding_authors": "",
    "abstract": "In wireless networks such as those based on IEEE 802.11, packet losses due to fading and interference are often misinterpreted as indications of congestion by the congestion control protocol at higher layers, causing an unnecessary decrease in the data sending rate. For delay-constrained applications such as video telephony, packet losses may result in excessive artifacts or freeze in the decoded video. We propose a simple and yet effective mechanism to detect and reduce channel-caused packet losses by adjusting the retry limit parameter of the IEEE 802.11 protocol while taking into account the delay requirement of the traffic. Since the retry limit is left configurable in the IEEE 802.11 standard and does not require cross-layer coordination, our scheme can be easily implemented and incrementally deployed. Experimental results of applying the proposed scheme to a WebRTC-based real-time video communication prototype show significant performance gain compared to the case where the retry limit is configured statically.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2550799741",
    "type": "article"
  },
  {
    "title": "Special Section on AI-empowered Multimedia Data Analytics for Smart Healthcare",
    "doi": "https://doi.org/10.1145/3505281",
    "publication_date": "2022-01-25",
    "publication_year": 2022,
    "authors": "M. Shamim Hossain; Rita Cucchiara; Ghulam Muhammad; Diana P. Tobón; Abdulmotaleb El Saddik",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4206937885",
    "type": "article"
  },
  {
    "title": "Online Learning for Adaptive Video Streaming in Mobile Networks",
    "doi": "https://doi.org/10.1145/3460819",
    "publication_date": "2022-01-27",
    "publication_year": 2022,
    "authors": "Theodoros Karagkioules; Georgios S. Paschos; Nikolaos Liakopoulos; Attilio Fiandrotti; Dimitrios Tsilimantos; Marco Cagnazzo",
    "corresponding_authors": "",
    "abstract": "In this paper, we propose a novel algorithm for video bitrate adaptation in HTTP Adaptive Streaming (HAS), based on online learning. The proposed algorithm, named Learn2Adapt (L2A) , is shown to provide a robust bitrate adaptation strategy which, unlike most of the state-of-the-art techniques, does not require parameter tuning, channel model assumptions, or application-specific adjustments. These properties make it very suitable for mobile users, who typically experience fast variations in channel characteristics. Experimental results, over real 4G traffic traces, show that L2A improves on the overall Quality of Experience (QoE) and in particular the average streaming bitrate, a result obtained independently of the channel and application scenarios.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4210281808",
    "type": "article"
  },
  {
    "title": "Authentication of LINE Chat History Files by Information Hiding",
    "doi": "https://doi.org/10.1145/3474225",
    "publication_date": "2022-01-27",
    "publication_year": 2022,
    "authors": "Da-Chun Wu; Yu-Tsung Hsu",
    "corresponding_authors": "",
    "abstract": "With the prevalence of smartphones, message exchanges via mobile chatting programs like LINE have become popular. The messages in the form of chat records in a LINE chat history, after being downloaded for legal uses, might be tampered with illicitly. A novel method for authenticating the chat history against such attacks is proposed. The signal used for authenticating each chat-record segment is created by concatenating the ID label of the segment and a digest yielded by hashing the segment content. The signal is then encoded by three types of spacing code, namely half space, full space, and tab, and embedded into the blank-space areas created by the tab codes in the chat records of the segment. Authentication of a history file is accomplished by extracting the authentication signals embedded in the file and comparing them with the original signals computed directly from the file. The embedded signals are invisible, arousing no suspicion from the hacker. The signals are fragile, because any modification of the records can be detected by the authentication process. Experiments for testing four types of tampering with text files of four languages have been conducted, yielding correct authentication results that show the feasibility of the proposed method.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4210531658",
    "type": "article"
  },
  {
    "title": "Sparse LIDAR Measurement Fusion with Joint Updating Cost for Fast Stereo Matching",
    "doi": "https://doi.org/10.1145/3471870",
    "publication_date": "2022-01-27",
    "publication_year": 2022,
    "authors": "Peng Yao; Jieqing Feng",
    "corresponding_authors": "",
    "abstract": "The complementary virtues of active and passive depth sensors inspire the LIDAR-Stereo fusion for enhancing the accuracy of stereo matching. However, most of the fusion based stereo matching algorithms have exploited dense LIDAR priors with single fusion methodology. In this paper, we intend to break these fetters, utilizing sparse LIDAR priors with multi-step fusion strategy for obtaining accurate disparity estimation more efficiently. At first, random sparse sampling LIDAR depth measurements are provided in Naive Fusion for updating the matching cost of Semi-Global Matching (SGM). Then Neighborhood Based Fusion is performed based on the former step for further updating the cost. Subsequently, Diffusion Based Fusion is utilized to update both the cost and disparities. At last, Tree Filtering is applied for removing speckle outliers and smoothing disparities. Performance evaluations on various stereo data sets demonstrate that the proposed algorithm outperforms other most challenging stereo matching algorithms significantly with approximately real-time implementation efficiency. Furthermore, it is worth pointing out that our proposal surprisingly possesses one of the top ten performances on Middlebury v.3 online evaluation system even if it has not been adopted any learning-based techniques.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4210655689",
    "type": "article"
  },
  {
    "title": "Boolean-based Two-in-One Secret Image Sharing by Adaptive Pixel Grouping",
    "doi": "https://doi.org/10.1145/3517140",
    "publication_date": "2022-02-18",
    "publication_year": 2022,
    "authors": "Xiaotian Wu; Yao Peng",
    "corresponding_authors": "",
    "abstract": "The two-in-one secret image sharing (TiOSIS) technique is a hybrid scheme that protects a secret image by combining visual cryptography (VCS) and polynomial-based secret image sharing (PSIS). There are two decoding methods available in TiOSIS: stacking-to-see decryption and lossless image recovery. However, the majority of current TiOSIS methods use Lagrange interpolation to precisely reconstruct the secret, which would result in intense computations. In this article, an efficient TiOSIS scheme using Boolean XOR operation for lossless image recovery is proposed. The proposed scheme consists of three building blocks: shared data generation, shadow construction, and image decryption. In shared data generation, the grayscale secret image is processed by a Boolean-based SIS to derive the shared bits. In shadow construction, an adaptive pixel grouping (APG) strategy is utilized to determine a grouping pattern. The halftone image adjustment algorithm is adopted to generate a suitable halftone image. With the grouping pattern and halftone image, we construct the shadows via the group-pixel embedding and sharing approach. In image decryption, we can reveal the secret image by stacking-to-see decoding or Boolean-based lossless image recovery. Extensive experiments and comparisons are illustrated to show the effectiveness and benefits of the proposed scheme.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4212852447",
    "type": "article"
  },
  {
    "title": "Deeply Activated Salient Region for Instance Search",
    "doi": "https://doi.org/10.1145/3510004",
    "publication_date": "2022-02-18",
    "publication_year": 2022,
    "authors": "Hui-Chu Xiao; Wan‐Lei Zhao; Lin Jie; Yi-Geng Hong; Chong‐Wah Ngo",
    "corresponding_authors": "",
    "abstract": "The performance of instance search relies heavily on the ability to locate and describe a wide variety of object instances in a video/image collection. Due to the lack of a proper mechanism for locating instances and deriving feature representation, instance search is generally only effective when the instances are from known object categories. In this article, a simple but effective instance-level feature representation approach is presented. Different from the existing approaches, the issues of class-agnostic instance localization and distinctive feature representation are considered. The former is achieved by detecting salient instance regions from an image by a layer-wise back-propagation process. The back-propagation starts from the last convolution layer of a pre-trained CNNs that is originally used for classification. The back-propagation proceeds layer by layer until it reaches the input layer. This allows the salient instance regions in the input image from both known and unknown categories to be activated. Each activated salient region covers the full or, more usually, a major range of an instance. The distinctive feature representation is produced by average-pooling on the feature map of a certain layer with the detected instance region. Experiments show that this kind of feature representation demonstrates considerably better performance than most of the existing approaches.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4213091441",
    "type": "article"
  },
  {
    "title": "Domain-invariant Graph for Adaptive Semi-supervised Domain Adaptation",
    "doi": "https://doi.org/10.1145/3487194",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Jinfeng Li; Weifeng Liu; Yicong Zhou; Jun Yu; Dapeng Tao; Changsheng Xu",
    "corresponding_authors": "",
    "abstract": "Domain adaptation aims to generalize a model from a source domain to tackle tasks in a related but different target domain. Traditional domain adaptation algorithms assume that enough labeled data, which are treated as the prior knowledge are available in the source domain. However, these algorithms will be infeasible when only a few labeled data exist in the source domain, thus the performance decreases significantly. To address this challenge, we propose a Domain-invariant Graph Learning (DGL) approach for domain adaptation with only a few labeled source samples. Firstly, DGL introduces the Nyström method to construct a plastic graph that shares similar geometric property with the target domain. Then, DGL flexibly employs the Nyström approximation error to measure the divergence between the plastic graph and source graph to formalize the distribution mismatch from the geometric perspective. Through minimizing the approximation error, DGL learns a domain-invariant geometric graph to bridge the source and target domains. Finally, we integrate the learned domain-invariant graph with the semi-supervised learning and further propose an adaptive semi-supervised model to handle the cross-domain problems. The results of extensive experiments on popular datasets verify the superiority of DGL, especially when only a few labeled source samples are available.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4214823095",
    "type": "article"
  },
  {
    "title": "Objective Object Segmentation Visual Quality Evaluation: Quality Measure and Pooling Method",
    "doi": "https://doi.org/10.1145/3491229",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Ran Shi; Jing Ma; King Ngi Ngan; Jian Xiong; Tong Qiao",
    "corresponding_authors": "",
    "abstract": "Objective object segmentation visual quality evaluation is an emergent member of the visual quality assessment family. It aims to develop an objective measure instead of a subjective survey to evaluate the object segmentation quality in agreement with human visual perception. It is an important benchmark for assessing and comparing the performances of object segmentation methods in terms of visual quality. Despite its essential role, sufficient study compared with other visual quality evaluation studies is still lacking. In this article, we propose a novel full-reference objective measure that includes a two-level single object segmentation visual quality measure and a pooling method for multiple object segmentation overall visual quality. The single object segmentation visual quality measure combines a pixel-level sub-measure and a region-level sub-measure for evaluating the similarity of area, shape, and object completeness between the segmentation result and the ground truth in terms of human visual perception. For the proposed multiple object segmentation overall visual quality pooling method, the rank of each object’s segmentation quality as a novel factor is integrated into the weighted harmonic mean to evaluate the overall quality. To evaluate the performance of our proposed measure, we tested it on an object segmentation subjective visual quality assessment database. The experimental results demonstrate that our proposed two-level measure and pooling method with good robustness perform better in matching subjective assessments compared with other state-of-the-art objective measures.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4214824854",
    "type": "article"
  },
  {
    "title": "Distributed Gateway Selection for Video Streaming in VANET Using IP Multicast",
    "doi": "https://doi.org/10.1145/3491388",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Debanjan Roy Chowdhury; Sukumar Nandi; Diganta Goswami",
    "corresponding_authors": "",
    "abstract": "The volume of video traffic as infotainment service over vehicular ad hoc network (VANET) has rapidly increased for past few years. Providing video streaming as VANET infotainment service is very challenging because of high mobility and heterogeneity of vehicular networks. While the number of mobile gateways (vehicles connected to the Internet) needs to be minimized to reduce service cost, the streaming quality also needs to be satisfactory for end-users. Existing works either focus on gateway minimization or focus on enhancing user satisfaction. We propose a video streaming solution, namely, DGSVS, which does gateway minimization with the constrained time data delivery to end-users. We formulate our constrained gateway minimization problem as minimum set covering (MSC) problem and solve with a distributed approximation method for MSC. We assume that only a subset of vehicles in VANET run DGSVS application. Therefore, instead of application layer cooperation for gateway-client association, network layer cooperation is proposed. We propose a novel multicast protocol DSS-CAST for this purpose, which is specialized in streaming data distribution for dynamic scenarios. We compare the performance of DGSVS with other existing protocols and found that DGSVS is most effective in service cost minimization while it is able to achieve competitive QoE performance.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4214857872",
    "type": "article"
  },
  {
    "title": "Deep Learning-Based Intra Mode Derivation for Versatile Video Coding",
    "doi": "https://doi.org/10.1145/3563699",
    "publication_date": "2022-09-16",
    "publication_year": 2022,
    "authors": "Linwei Zhu; Yun Zhang; Na Li; Gangyi Jiang; Sam Kwong",
    "corresponding_authors": "",
    "abstract": "In intra coding, Rate Distortion Optimization (RDO) is performed to achieve the optimal intra mode from a pre-defined candidate list. The optimal intra mode is also required to be encoded and transmitted to the decoder side besides the residual signal, where lots of coding bits are consumed. To further improve the performance of intra coding in Versatile Video Coding (VVC) , an intelligent intra mode derivation method is proposed in this paper, termed as Deep Learning based Intra Mode Derivation (DLIMD) . In specific, the process of intra mode derivation is formulated as a multi-class classification task, which aims to skip the module of intra mode signaling for coding bits reduction. The architecture of DLIMD is developed to adapt to different quantization parameter settings and variable coding blocks including non-square ones, where only one single trained model is required. Different from the existing deep learning based classification problems, the hand-crafted features are also fed into intra mode derivation network besides the learned features from feature learning network. To compete with traditional methods, one additional binary flag is utilized in the video codec to indicate the selected scheme with RDO. Extensive experimental results reveal that the proposed method can achieve 2.28%, 1.74%, and 2.18% bit rate reduction on average for Y, U, and V components on the platform of VVC test model, which outperforms the state-of-the-art works.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4223545924",
    "type": "article"
  },
  {
    "title": "CAQoE: A Novel No-Reference Context-aware Speech Quality Prediction Metric",
    "doi": "https://doi.org/10.1145/3529394",
    "publication_date": "2022-04-13",
    "publication_year": 2022,
    "authors": "Rahul Kumar Jaiswal; Rajesh Kumar Dubey",
    "corresponding_authors": "",
    "abstract": "The quality of speech degrades while communicating over Voice over Internet Protocol applications, for example, Google Meet, Microsoft Skype, and Apple FaceTime, due to different types of background noise present in the surroundings. It reduces human perceived Quality of Experience (QoE). Along this line, this article proposes a novel speech quality prediction metric that can meet human’s desired QoE level. Our motivation is driven by the lack of evidence showing speech quality metrics that can distinguish different noise degradations before predicting the quality of speech. The quality of speech in noisy environments is improved by speech enhancement algorithms, and for measuring and monitoring the quality of speech, objective speech quality metrics are used. With the integration of these components, a novel no-reference context-aware QoE prediction metric (CAQoE) is proposed in this article, which initially identifies the context or noise type or degradation type of the input noisy speech signal and then predicts context-specific speech quality for that input speech signal. It will have of great importance in deciding the speech enhancement algorithms if the types of degradations causing poor speech quality are known along with the quality metric. Results demonstrate that the proposed CAQoE metric outperforms in different contexts as compared to the metric where contexts are not identified before predicting the quality of speech, even in the presence of limited size speech corpus having different contexts available from the NOIZEUS speech database.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4223634949",
    "type": "article"
  },
  {
    "title": "Cascaded Structure-Learning Network with Using Adversarial Training for Robust Facial Landmark Detection",
    "doi": "https://doi.org/10.1145/3474595",
    "publication_date": "2022-02-16",
    "publication_year": 2022,
    "authors": "Shenming Feng; Xingzhong Nong; Haifeng Hu",
    "corresponding_authors": "",
    "abstract": "Recently, great progress has been achieved on facial landmark detection based on convolutional neural network, while it is still challenging due to partial occlusion and extreme head pose. In this paper, we propose a Cascaded Structure-Learning Network (CSLN) with using adversarial training to improve the performance of 2D facial landmark detection by taking the structure of facial landmarks into account. In the first stage, we improve the original stacked hourglass network, which applies a multi-branch module to capture different scales of features, a progressive convolution structure to compensate for the missing structural features in hourglass networks, and a pyramid inception structure to expand the receptive field. Specially, by introducing a discriminator, we use the adversarial training strategy to urge the improved hourglass network for generating more accurate heatmaps. The second stage, which is based on attention mechanism, optimizes the spatial correlations between different facial landmarks by reusing the structural features. Moreover, we propose a novel region loss, which can adaptively allocate proper weights to different regions. In this way, the network can focus more on those occluded landmarks. The experimental results on several datasets, i.e. 300W, COFW, and AFLW, show that our proposed method achieves superior performance compared with the state-of-the-art methods.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4283380418",
    "type": "article"
  },
  {
    "title": "Adaptive Text Denoising Network for Image Caption Editing",
    "doi": "https://doi.org/10.1145/3532627",
    "publication_date": "2022-07-14",
    "publication_year": 2022,
    "authors": "Mengqi Yuan; Bing‐Kun Bao; Zhiyi Tan; Changsheng Xu",
    "corresponding_authors": "",
    "abstract": "Image caption editing, which aims at editing the inaccurate descriptions of the images, is an interdisciplinary task of computer vision and natural language processing. As the task requires encoding the image and its corresponding inaccurate caption simultaneously and decoding to generate an accurate image caption, the encoder-decoder framework is widely adopted for image caption editing. However, existing methods mostly focus on the decoder, yet ignore a big challenge on the encoder: the semantic inconsistency between image and caption. To this end, we propose a novel A daptive T ext D enoising Net work (ATD-Net) to filter out noises at the word level and improve the model’s robustness at sentence level. Specifically, at the word level, we design a cross-attention mechanism called Textual Attention Mechanism (TAM), to differentiate the misdescriptive words. The TAM is designed to encode the inaccurate caption word by word based on the content of both image and caption. At the sentence level, in order to minimize the influence of misdescriptive words on the semantic of an entire caption, we introduce a Bidirectional Encoder to extract the correct semantic representation from the raw caption. The Bidirectional Encoder is able to model the global semantics of the raw caption, which enhances the robustness of the framework. We extensively evaluate our proposals on the MS-COCO image captioning dataset and prove the effectiveness of our method when compared with the state-of-the-arts.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4285389151",
    "type": "article"
  },
  {
    "title": "Toward A No-reference Omnidirectional Image Quality Evaluation by Using Multi-perceptual Features",
    "doi": "https://doi.org/10.1145/3549544",
    "publication_date": "2022-07-21",
    "publication_year": 2022,
    "authors": "Yun Liu; Xiaohua Yin; Zuliang Wan; Guanghui Yue; Zhi Zheng",
    "corresponding_authors": "",
    "abstract": "Compared to ordinary images, omnidirectional image (OI) usually has a broader view and a higher resolution, and image quality assessment (IQA) can help people to understand and improve their visual experience. However, the current IQA works cannot achieve good performance. To address this, we proposed a novel visual perception-based no-reference/blind omnidirectional image quality assessment (NR/B-OIQA) model. The gradient-based global structural features and gray-level co-occurrence matrix-based local structural features are combined together to highlight the rich quality-aware structural information. And a novel steganalysis real model-based color descriptor is extracted to reflect the color information that ignored in most IQA models. With a multi-scale visual perception, we take image entropy and the natural scene statistics features to convey the high-level semantics and quantify the unnaturalness of omnidirectional images. Finally, we apply support vector regression to predict the objective quality value based on the subjective scores and extracted all features. Experiments are conducted on OIQA and CVIQD2018 Databases, and the results illustrate that our model has more reliable performance and stronger competitiveness and receives better conformity with the subjective values.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4286111111",
    "type": "article"
  },
  {
    "title": "No-reference Quality Assessment for Contrast-distorted Images Based on Gray and Color-gray-difference Space",
    "doi": "https://doi.org/10.1145/3555355",
    "publication_date": "2022-08-08",
    "publication_year": 2022,
    "authors": "Yang Yang; Yingqiu Ding; Ming Cheng; Weiming Zhang",
    "corresponding_authors": "",
    "abstract": "No-reference image quality assessment is a basic and challenging problem in the field of image processing. Among them, contrast distortion has a great impact on the perception of image quality. However, there are relatively few studies on no-reference quality assessment of contrast-distorted images. This article proposes a no-reference quality assessment algorithm for contrast-distorted images based on gray and color-gray-difference (CGD) space. In terms of gray space, we consider the local and global aspects, and use the distribution characteristics of the grayscale histogram to represent global features, while local features are described by the fusion of Local Binary Pattern (LBP) operator and gradient. In terms of CGD space, we first randomly extract patches from the entire image and then extract appropriate quality perception features in the patch’s CGD histogram. Finally, the AdaBoosting back propagation (BP) neural network is used to train the prediction model to predict the quality of the contrast-distorted image. Extensive analysis and cross-validation are carried out on five contrast-related image databases, and the experimental results have proved the superiority of this method compared with recent related algorithms.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4290647926",
    "type": "article"
  },
  {
    "title": "Dynamic Transfer Exemplar based Facial Emotion Recognition Model Toward Online Video",
    "doi": "https://doi.org/10.1145/3538385",
    "publication_date": "2022-05-20",
    "publication_year": 2022,
    "authors": "Anqi Bi; Xiaoyang Tian; Shuihua Wang‎; Yudong Zhang",
    "corresponding_authors": "",
    "abstract": "In this article, we focus on the dynamic facial emotion recognition from online video. We combine deep neural networks with transfer learning theory and propose a novel model named DT-EFER. In detail, DT-EFER uses GoogLeNet to extract the deep features of key images from video clips. Then to solve the dynamic facial emotion recognition scenario, the framework introduces transfer learning theory. Thus, to improve the recognition performance, model DT-EFER focuses on the differences between key images instead of those images themselves. Moreover, the time complexity of this model is not high, even if previous exemplars are introduced here. In contrast to other exemplar-based models, experiments based on two datasets, namely, BAUM-1s and Extended Cohn–Kanade, have shown the efficiency of the proposed DT-EFER model.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4293249608",
    "type": "article"
  },
  {
    "title": "Sim2Word: Explaining Similarity with Representative Attribute Words via Counterfactual Explanations",
    "doi": "https://doi.org/10.1145/3563039",
    "publication_date": "2022-09-08",
    "publication_year": 2022,
    "authors": "Ruoyu Chen; Jingzhi Li; Hua Zhang; Changchong Sheng; Li Liu; Xiaochun Cao",
    "corresponding_authors": "",
    "abstract": "Recently, we have witnessed substantial success using the deep neural network in many tasks. Although there still exist concerns about the explainability of decision making, it is beneficial for users to discern the defects in the deployed deep models. Existing explainable models either provide the image-level visualization of attention weights or generate textual descriptions as post hoc justifications. Different from existing models, in this article we propose a new interpretation method that explains the image similarity models by salience maps and attribute words. Our interpretation model contains visual salience maps generation and the counterfactual explanation generation. The former has two branches: global identity relevant region discovery and multi-attribute semantic region discovery. The first branch aims to capture the visual evidence supporting the similarity score, which is achieved by computing counterfactual feature maps. The second branch aims to discover semantic regions supporting different attributes, which helps to understand which attributes in an image might change the similarity score. Then, by fusing visual evidence from two branches, we can obtain the salience maps indicating important response evidence. The latter will generate the attribute words that best explain the similarity using the proposed erasing model. The effectiveness of our model is evaluated on the classical face verification task. Experiments conducted on two benchmarks—VGGFace2 and Celeb-A—demonstrate that our model can provide convincing interpretable explanations for the similarity. Moreover, our algorithm can be applied to evidential learning cases, such as finding the most characteristic attributes in a set of face images, and we verify its effectiveness on the VGGFace2 dataset.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4296117664",
    "type": "article"
  },
  {
    "title": "UEFPN: Unified and Enhanced Feature Pyramid Networks for Small Object Detection",
    "doi": "https://doi.org/10.1145/3561824",
    "publication_date": "2022-09-08",
    "publication_year": 2022,
    "authors": "Ziteng Qiao; Dianxi Shi; Xiaodong Yi; Yanyan Shi; Yuhui Zhang; Yangyang Liu",
    "corresponding_authors": "",
    "abstract": "Object detection models based on feature pyramid networks have made significant progress in general object detection. However, small object detection is still a challenge for the existing models. In this paper, we think that two factors in the existing feature pyramid networks inhibit the performance of small object detection. The first one is that the different feature domains of shallow and deep layer features inhibit the model performance. The second one is that the accumulation of upper layer features leads to feature aliasing effect on the lower layer features, which interferes with the representations of small object features. Therefore, we propose Unified and Enhanced Feature Pyramid Networks (UEFPN) to improve the APs and ARs of small object detection. It has the following three characteristics: (1) Using the deep features of high-resolution image and original image to form the multi-scale features of unified domain. (2) In multi-scale features fusion, we learn the importance of upper layer features with the Channel Attention Fusion module (CAF) , to optimize feature aliasing effect and enhance the context information of shallow layer features. (3) UEFPN can be quickly applied to different models. The results of many experiments show that the models with UEFPN achieve significant performance improvement in small object detection compared with the baseline models.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4296118075",
    "type": "article"
  },
  {
    "title": "Real-time Image Enhancement with Attention Aggregation",
    "doi": "https://doi.org/10.1145/3564607",
    "publication_date": "2022-09-26",
    "publication_year": 2022,
    "authors": "Qiqi Gao; Jie Li; Tiejun Zhao; Yadong Wang",
    "corresponding_authors": "",
    "abstract": "Image enhancement has stimulated significant research works over the past years for its great application potential in video conferencing scenarios. Nevertheless, most existing image enhancement approaches are still struggling to find a good tradeoff that reduces the computational cost as much as possible while maintaining plausible result quality. Recently, curve-based mapping methods are proposed and have shown great potential for real-time and high-quality image enhancement of arbitrary resolutions. In this article, we take advantage of the curve-based mapping representation and focus on further improving the enhancement quality and robustness, while minimizing additional computational costs. Specifically, we (1) carefully re-formulate the curve function to improve learning stability, and (2) aggregate different semantic attention into the curve regression process, which can overcome the major problems of curve-based methods that generate moderate results with low contrast. The semantic attention is jointly learned with the supervision from class activation mapping of pre-trained feature extractors, thus reducing the manual annotation cost of semantic labels. Experiments have shown that our proposed method significantly improves curve-based methods both qualitatively and quantitatively, achieving visually plausible results compared with other deep neural network-based enhancement methods, and maintains a very low computational cost, i.e., taking 18.7 ms for a 360p image on a single P40 GPU. Extensive experiments demonstrate that our method is also capable of video enhancement tasks.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4297219256",
    "type": "article"
  },
  {
    "title": "Adaptive Streaming in P2P Live Video Systems",
    "doi": "https://doi.org/10.1145/2912123",
    "publication_date": "2016-05-24",
    "publication_year": 2016,
    "authors": "Maria Luisa Merani; Laura Natali",
    "corresponding_authors": "",
    "abstract": "Dynamic Adaptive Streaming over HTTP (DASH) is a recently proposed standard that offers different versions of the same media content to adapt the delivery process over the Internet to dynamic bandwidth fluctuations and different user device capabilities. The peer-to-peer (P2P) paradigm for video streaming allows to leverage the cooperation among peers, guaranteeing to serve every video request with increased scalability and reduced cost. We propose to combine these two approaches in a P2P-DASH architecture, exploiting the potentiality of both. The new platform is made of several swarms, and a different DASH representation is streamed within each of them; unlike client-server DASH architectures, where each client autonomously selects which version to download according to current network conditions and to its device resources, we put forth a new rate control strategy implemented at peer site to maintain a good viewing quality to the local user and to simultaneously guarantee the successful operation of the P2P swarms. The effectiveness of the solution is demonstrated through simulation and it indicates that the P2P-DASH platform is able to warrant its users a very good performance, much more satisfying than in a conventional P2P environment where DASH is not employed. Through a comparison with a reference DASH system modeled via the Integer Linear Programming (ILP) approach, the new system is shown to outperform such reference architecture. To further validate the proposal, both in terms of robustness and scalability, system behavior is investigated in the critical condition of a flash crowd, showing that the strong upsurge of new users can be successfully revealed and gradually accommodated.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4299352839",
    "type": "article"
  },
  {
    "title": "Detection of Recolored Image by Texture Features in Chrominance Components",
    "doi": "https://doi.org/10.1145/3571076",
    "publication_date": "2022-11-09",
    "publication_year": 2022,
    "authors": "Yushu Zhang; Nuo Chen; Shuren Qi; Mingfu Xue; Zhongyun Hua",
    "corresponding_authors": "",
    "abstract": "Image recoloring is an emerging editing technique that can change the color style of an image by modifying pixel values without altering the original image content. With the rapid proliferation of social network and image editing techniques, recolored images (RIs) have raised new security issues in society. Existing detection methods have good performance in detecting RIs for certain categories of recoloring techniques. However, the performance on the handcrafted recoloring scenario is still poor due to the influence of human prior knowledge. To deal with this problem, we explore a solution from the perspective of chrominance texture artifacts to improve the generalization ability. The results of the analysis show that natural images (NIs) and RIs have textural disparities in different color components, especially in the chrominance components (i.e., Cb, Cr, and H). Based on such new prior knowledge of statistical discriminability, we propose a feature set to capture texture features in chrominance components for identifying RIs. Extensive experimental results show that the proposed method can accurately identify RIs with certain categories of recoloring techniques, and outperforms existing methods in the scenario of handcrafted recoloring.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4308732143",
    "type": "article"
  },
  {
    "title": "HCMS: Hierarchical and Conditional Modality Selection for Efficient Video Recognition",
    "doi": "https://doi.org/10.1145/3572776",
    "publication_date": "2022-12-02",
    "publication_year": 2022,
    "authors": "Zejia Weng; Zuxuan Wu; Hengduo Li; Jingjing Chen; Yu–Gang Jiang",
    "corresponding_authors": "",
    "abstract": "Videos are multimodal in nature. Conventional video recognition pipelines typically fuse multimodal features for improved performance. However, this is not only computationally expensive but also neglects the fact that different videos rely on different modalities for predictions. This paper introduces Hierarchical and Conditional Modality Selection (HCMS), a simple yet efficient multimodal learning framework for efficient video recognition. HCMS operates on a low-cost modality, i.e., audio clues, by default, and dynamically decides on-the-fly whether to use computationally-expensive modalities, including appearance and motion clues, on a per-input basis. This is achieved by the collaboration of three LSTMs that are organized in a hierarchical manner. In particular, LSTMs that operate on high-cost modalities contain a gating module, which takes as inputs lower-level features and historical information to adaptively determine whether to activate its corresponding modality; otherwise it simply reuses historical information. We conduct extensive experiments on two large-scale video benchmarks, FCVID and ActivityNet, and the results demonstrate the proposed approach can effectively explore multimodal information for improved classification performance while requiring much less computation.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4310877696",
    "type": "article"
  },
  {
    "title": "Dual Scene Graph Convolutional Network for Motivation Prediction",
    "doi": "https://doi.org/10.1145/3572914",
    "publication_date": "2022-12-01",
    "publication_year": 2022,
    "authors": "Yuyang Wanyan; Xiaoshan Yang; Xuan Ma; Changsheng Xu",
    "corresponding_authors": "",
    "abstract": "Humans can easily infer the motivations behind human actions from only visual data by comprehensively analyzing the complex context information and utilizing abundant life experiences. Inspired by humans’ reasoning ability, existing motivation prediction methods have improved image-based deep classification models using the commonsense knowledge learned by pre-trained language models. However, the knowledge learned from public text corpora is probably incompatible with the task-specific data of the motivation prediction, which may impact the model performance. To address this problem, this paper proposes a dual scene graph convolutional network (dual-SGCN) to comprehensively explore the complex visual information and semantic context prior from the image data for motivation prediction. The proposed dual-SGCN has a visual branch and a semantic branch. For the visual branch, we build a visual graph based on scene graph where object nodes and relation edges are represented by visual features. For the semantic branch, we build a semantic graph where nodes and edges are directly represented by the word embeddings of the object and relation labels. In each branch, node-oriented and edge-oriented message passing is adopted to propagate interaction information between different nodes and edges. Besides, a multi-modal interactive attention mechanism is adopted to cooperatively attend and fuse the visual and semantic information. The proposed dual-SGCN is learned in an end-to-end form by a multi-task co-training scheme. In the inference stage, Total Direct Effect is adopted to alleviate the bias caused by the semantic context prior. Extensive experiments demonstrate that the proposed method achieves state-of-the-art performance.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4311057659",
    "type": "article"
  },
  {
    "title": "MixOOD: Improving Out-of-distribution Detection with Enhanced Data Mixup",
    "doi": "https://doi.org/10.1145/3578935",
    "publication_date": "2022-09-30",
    "publication_year": 2022,
    "authors": "Taocun Yang; Yaping Huang; Yanlin Xie; Junbo Liu; Shengchun Wang",
    "corresponding_authors": "",
    "abstract": "Detecting out-of-distribution (OOD) inputs for deep learning models is a critical task when models are deployed in real-world environments. Recently, a large number of works have been dedicated to tackling the OOD detection problem. One of the most straightforward and effective ways is OOD training, which adds heterogeneous auxiliary data in the training stage. However, the extra auxiliary data cannot be involved arbitrarily. A high-quality and powerful auxiliary dataset must contain samples that belong to OOD but are close to in-distribution (ID), which can teach the model to learn more information about OOD samples, furthermore, distinguish OOD from ID. The key issue for this problem is how to simply acquire such distinctive OOD samples. In this article, we propose an enhanced Mixup-based OOD (MixOOD) detection strategy that can be attached to any threshold-based OOD detecting method. Different from the traditional Mixup designed for ID data augmentation, our proposed MixOOD generates augmented images with deliberately modified Mixup and then uses them as auxiliary OOD data to leverage the OOD detection. We test our method with classical OOD detecting approaches like Maximum Softmax Probability, Energy Score, and Out-of-distribution detector for Neural networks. Experiments show that models with MixOOD can better distinguish in- and out-of-distribution samples than the original version of each approach.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4313590636",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on Affective Services based on Representation Learning",
    "doi": "https://doi.org/10.1145/3567836",
    "publication_date": "2022-10-31",
    "publication_year": 2022,
    "authors": "Yin Zhang⋆; Iztok Humar; Jia Liu; Alireza Jolfaei",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4320507094",
    "type": "article"
  },
  {
    "title": "Autoregressive GAN for Semantic Unconditional Head Motion Generation",
    "doi": "https://doi.org/10.1145/3635154",
    "publication_date": "2023-12-06",
    "publication_year": 2023,
    "authors": "Louis Airale; Xavier Alameda-Pineda; Stéphane Lathuilière; Dominique Vaufreydaz",
    "corresponding_authors": "",
    "abstract": "In this work, we address the task of unconditional head motion generation to animate still human faces in a low-dimensional semantic space from a single reference pose. Different from traditional audio-conditioned talking head generation that seldom puts emphasis on realistic head motions, we devise a GAN-based architecture that learns to synthesize rich head motion sequences over long duration while maintaining low error accumulation levels.In particular, the autoregressive generation of incremental outputs ensures smooth trajectories, while a multi-scale discriminator on input pairs drives generation toward better handling of high- and low-frequency signals and less mode collapse.We experimentally demonstrate the relevance of the proposed method and show its superiority compared to models that attained state-of-the-art performances on similar tasks.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4308168920",
    "type": "article"
  },
  {
    "title": "VL-NMS: Breaking Proposal Bottlenecks in Two-stage Visual-language Matching",
    "doi": "https://doi.org/10.1145/3579095",
    "publication_date": "2023-01-04",
    "publication_year": 2023,
    "authors": "Chenchi Zhang; Wenbo Ma; Jun Xiao; Hanwang Zhang; Jian Shao; Yueting Zhuang; Long Chen",
    "corresponding_authors": "",
    "abstract": "The prevailing framework for matching multimodal inputs is based on a two-stage process: (1) detecting proposals with an object detector and (2) matching text queries with proposals. Existing two-stage solutions mostly focus on the matching step. In this article, we argue that these methods overlook an obvious mismatch between the roles of proposals in the two stages: they generate proposals solely based on the detection confidence (i.e., query-agnostic), hoping that the proposals contain all instances mentioned in the text query (i.e., query-aware). Due to this mismatch, chances are that proposals relevant to the text query are suppressed during the filtering process, which in turn bounds the matching performance. To this end, we propose VL-NMS, which is the first method to yield query-aware proposals at the first stage. VL-NMS regards all mentioned instances as critical objects and introduces a lightweight module to predict a score for aligning each proposal with a critical object. These scores can guide the NMS operation to filter out proposals irrelevant to the text query, increasing the recall of critical objects, and resulting in a significantly improved matching performance. Since VL-NMS is agnostic to the matching step, it can be easily integrated into any state-of-the-art two-stage matching method. We validate the effectiveness of VL-NMS on three multimodal matching tasks, namely referring expression grounding, phrase grounding, and image-text matching. Extensive ablation studies on several baselines and benchmarks consistently demonstrate the superiority of VL-NMS.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4313590975",
    "type": "article"
  },
  {
    "title": "Counterfactual Scenario-relevant Knowledge-enriched Multi-modal Emotion Reasoning",
    "doi": "https://doi.org/10.1145/3583690",
    "publication_date": "2023-02-10",
    "publication_year": 2023,
    "authors": "Hao Líu; Xiaoshan Yang; Changsheng Xu",
    "corresponding_authors": "",
    "abstract": "Multi-modal video emotion reasoning (MERV) has recently attracted increasing attention due to its potential application in human-computer interaction. This task needs to not only recognize utterance-level emotions for conspicuous speakers, but also perceive the emotions of non-speakers in videos. Existing methods focus on modeling multi-modal multi-level contexts to capture emotion-relevant clues from the complex scenarios in videos. However, the context information is far from enough to infer the emotion labels of non-speakers due to the large gap between the scenario situation and emotions labels. Inspired by the observation that humans can find solutions to complex problems with the leverage of experience and knowledge, we propose SK-MER , a Scenario-relevant Knowledge-enhanced Multi-modal Emotion Reasoning framework for MERV task, which can leverage external knowledge to enhance the video scenario understanding and emotion reasoning. Specifically, we use scenario concepts extracted from videos to build knowledge subgraphs from external knowledge bases. The knowledge subgraphs are then utilized to obtain scenario-relevant knowledge representations through dynamic knowledge graph attention. Next, we incorporate the knowledge representations into context modeling to enhance emotion reasoning with external scenario-relevant knowledge. In addition, we propose a counterfactual knowledge representation learning approach to obtain more effective scenario-relevant knowledge representations. Extensive experimental results on MEmoR dataset show that the proposed SK-MER framework achieves new state-of-the-art results.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4319967889",
    "type": "article"
  },
  {
    "title": "Unsupervised Domain Adaptation by Causal Learning for Biometric Signal-based HCI",
    "doi": "https://doi.org/10.1145/3583885",
    "publication_date": "2023-02-15",
    "publication_year": 2023,
    "authors": "Qingfeng Dai; Yongkang Wong; Guofei Sun; Yanwei Wang; Zhou Zhou; Mohan Kankanhalli; Xiangdong Li; Weidong Geng",
    "corresponding_authors": "",
    "abstract": "Biometric signal based human-computer interface (HCI) has attracted increasing attention due to its wide application in healthcare, entertainment, neurocomputing, and so on. In recent years, deep learning-based approaches have made great progress on biometric signal processing. However, the state-of-the-art (SOTA) approaches still suffer from model degradation across subjects or sessions. In this work, we propose a novel unsupervised domain adaptation approach for biometric signal-based HCI via causal representation learning. Specifically, three kinds of interventions on biometric signals (i.e., subjects, sessions, and trials) can be selected to generalize deep models across the selected intervention. In the proposed approach, a generative model is trained for producing intervened features that are subsequently used for learning transferable and causal relations with three modes. Experiments on the EEG-based emotion recognition task and sEMG-based gesture recognition task are conducted to confirm the superiority of our approach. An improvement of +0.21% on the task of inter-subject EEG-based emotion recognition is achieved using our approach. Besides, on the task of inter-session sEMG-based gesture recognition, our approach achieves improvements of +1.47%, +3.36%, +1.71%, and +1.01% on sEMG datasets including CSL-HDEMG, CapgMyo DB-b, 3DC, and Ninapro DB6, respectively. The proposed approach also works on the task of inter-trial sEMG-based gesture recognition and an average improvement of +0.66% on Ninapro databases is achieved. These experimental results show the superiority of the proposed approach compared with the SOTA unsupervised domain adaptation methods on HCIs based on biometric signal.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4320890076",
    "type": "article"
  },
  {
    "title": "Visual Paraphrase Generation with Key Information Retained",
    "doi": "https://doi.org/10.1145/3585010",
    "publication_date": "2023-02-27",
    "publication_year": 2023,
    "authors": "Jiayuan Xie; Jiali Chen; Yi Cai; Qingbao Huang; Qing Li",
    "corresponding_authors": "",
    "abstract": "Visual paraphrase generation task aims to rewrite a given image-related original sentence into a new paraphrase, where the paraphrase needs to have the same expressed meaning as the original sentence but have a difference in expression form. Existing studies mainly extract two semantic vectors to represent the entire image and the entire original sentence, respectively, for paraphrase generation. However, these semantic vectors for an image or a sentence may lead to the model failing to focus on some key objects in the original sentence, which may generate semantically inconsistent sentences by changing key object information. In this article, we propose an object-level paraphrase generation model, which generates paraphrases by adjusting the permutation of key objects and modifying their associated descriptions. To adjust the permutation of key objects, an object-sorting module aims to obtain new object sequences based on the key object information and original sentences. Then, a sequence generation module sequentially generates paraphrases based on the permutation of the newly object sequences. Each generation step focuses on different image features associated with different key objects to generate descriptions with differences. Furthermore, we use a semantic discriminator module to promote the generated paraphrase to be semantically close to the original sentence. Specifically, the loss function of the discriminator penalizes the excessive distance between the paraphrase and the original sentence. Extensive experiments on the MS COCO dataset show that the proposed model outperforms the baselines.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4322397514",
    "type": "article"
  },
  {
    "title": "Context-Based Novel Histogram Bin Stretching Algorithm for Automatic Contrast Enhancement",
    "doi": "https://doi.org/10.1145/3597303",
    "publication_date": "2023-05-16",
    "publication_year": 2023,
    "authors": "Kankanala Srinivas; Ashish Kumar Bhandari",
    "corresponding_authors": "",
    "abstract": "This article presents CHBS, a novel context-based histogram bin stretching method that enhances the contrast by increasing the range of gray levels and randomness among the gray levels. It comprises image spatial contextual information and discrete cosine transform (DCT). It constitutes the global enhancement with the context-based histogram bin stretching and local details with the DCT. First, it uses the spatial similarities among surrounding pixels to generate random numbers. Unlike the other methods, the similarity map is generated based on the neighboring pixels’ mutual relationship. Intensity values are distributed among the available dynamic range to generate a global contrast-enhanced image. Second, the DCT is further applied to the previous contrast-enhanced image to adjust its local details automatically. Several experiments are conducted on the different levels of contrast degraded images. Both subjective and objective assessment outcomes validate that the projected approach is better or comparable with several state-of-the-art approaches in terms of brightness preservation, richer details, and natural appearance.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4376643802",
    "type": "article"
  },
  {
    "title": "Complementary Coarse-to-Fine Matching for Video Object Segmentation",
    "doi": "https://doi.org/10.1145/3596496",
    "publication_date": "2023-05-16",
    "publication_year": 2023,
    "authors": "Zhen Chen; Ming Yang; Shiliang Zhang",
    "corresponding_authors": "",
    "abstract": "Semi-supervised Video Object Segmentation (VOS) needs to establish pixel-level correspondences between a video frame and preceding segmented frames to leverage their segmentation clues. Most works rely on features at a single scale to establish those correspondences, e.g., perform dense matching with Convolutional Neural Network (CNN) features from a deep layer. Differently, this work explores complementary features at different scales to pursue more robust feature matching. A coarse feature from a deep layer is first adopted to get coarse pixel-level correspondences. We hence evaluate the quality of those correspondences, and select pixels with low-quality correspondences for fine-scale feature matching. Segmentation clues of previous frames are propagated by both coarse and fine-scale correspondences, which are fused with appearance features for object segmentation. Compared with previous works, this coarse-to-fine matching scheme is more robust to distractions by similar objects and better preserves object details. The sparse fine-scale matching also ensures a fast inference speed. On popular VOS datasets including DAVIS and YouTube-VOS, the proposed method shows promising performance compared with recent works.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4376647170",
    "type": "article"
  },
  {
    "title": "Attentional Composition Networks for Long-Tailed Human Action Recognition",
    "doi": "https://doi.org/10.1145/3603253",
    "publication_date": "2023-06-09",
    "publication_year": 2023,
    "authors": "Haoran Wang; Yajie Wang; Baosheng Yu; Yibing Zhan; Chunfeng Yuan; Wankou Yang",
    "corresponding_authors": "",
    "abstract": "The problem of long-tailed visual recognition has been receiving increasing research attention. However, the long-tailed distribution problem remains underexplored for video-based visual recognition. To address this issue, in this article we propose a compositional learning based solution for video-based human action recognition. Our method, named Attentional Composition Networks (ACN), first learns verb-like and preposition-like components, then shuffles these components to generate samples for the tail classes in the feature space to augment the data for the tail classes. Specifically, during training, we represent each action video by a graph that captures the spatial-temporal relations (edges) among detected human/object instances (nodes). Then, ACN utilizes the position information to decompose each action into a set of verb and preposition representations using the edge features in the graph. After that, the verb and preposition features from different videos are combined via an attention structure to synthesize feature representations for tail classes. This way, we can enrich the data for the tail classes and consequently improve the action recognition for these classes. To evaluate the compositional human action recognition, we further contribute a new human action recognition dataset, namely NEU-Interaction (NEU-I). Experimental results on both Something-Something V2 and the proposed NEU-I demonstrate the effectiveness of the proposed method for long-tailed, few-shot, and zero-shot problems in human action recognition. Source code and the NEU-I dataset are available at https://github.com/YajieW99/ACN .",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4379986307",
    "type": "article"
  },
  {
    "title": "Enhancement of Information Carrying and Decoding for Visual Cryptography with Error Correction",
    "doi": "https://doi.org/10.1145/3612927",
    "publication_date": "2023-08-05",
    "publication_year": 2023,
    "authors": "Ching‐Nung Yang; Xiaotian Wu; Min-Jung Chung",
    "corresponding_authors": "",
    "abstract": "Recently, three visual cryptography schemes with t -error-correcting capability (VCSs- t EC) were introduced for preventing the shadows carrying additional information from being corrupted by noise interference. However, the concerns on VCS- t EC, such as the average amount of carrying information, decoding of information from shadows, and the demonstration way of a secret, should be considered and improved. In this article, two schemes, namely the ( k, n ) probabilistic VCS- t EC (PVCS- t EC) and the (2, n ) deterministic VCS- t EC (DVCS- t EC), are proposed. The concept of probabilistic VCS is combined with the Bose-Chaudhuri-Hocquenghem code (BCH code) for designing the ( k, n ) PVCS- t EC. Furthermore, some constant-weight BCH codewords are adopted to build the (2, n )-DVCS- t EC. Comprehensive results and experiments are demonstrated to clarify the enhancement of information-carrying and decoding by the two proposed methods.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4385607309",
    "type": "article"
  },
  {
    "title": "Boosting Scene Graph Generation with Contextual Information",
    "doi": "https://doi.org/10.1145/3615868",
    "publication_date": "2023-08-15",
    "publication_year": 2023,
    "authors": "Shiqi Sun; Danlan Huang; Xiaoming Tao; Chengkang Pan; Guangyi Liu; Chang Wen Chen",
    "corresponding_authors": "",
    "abstract": "Scene graph generation (SGG) has been developed to detect objects and their relationships from the visual data and has attracted increasing attention in recent years. Existing works have focused on extracting object context for SGG. However, very few works have attempted to exploit implicit contextual correlations among relationships of the objects. Furthermore, most existing SGG schemes rely on high-level features to predict the predicates while overlooking the potential inherent association of low-level features with the object relationships. We present in this article a novel scheme to capture enhanced contextual information for both objects and relationships. We design a Dual-branch Context Analysis Transformer (DCAT) architecture to extract both object context and relationship context from the visual data with dual transformer branches and then effectively fuse both high-level and low-level features by an adaptive approach to facilitate relationship prediction. Specifically, we first conduct feature representation learning to enrich relation representations by the visual, spatial, and linguistic feature extractors. Next, two transformer branches are designed to leverage the modeling of global associative interaction and mine the hidden association among objects and relationships. Then, we devise a novel feature disentangling method to decouple contextualized high-level features with guidance from the visual semantics. Finally, we develop a refined attention module to perform low-level feature recalibration for the refinement of the final predicate prediction. Experiments on Visual Genome and Action Genome datasets demonstrate the effectiveness of DCAT for both image and video SGG settings. Moreover, we also test the quality of the generated image scene graphs to verify the generalizability on downstream tasks like sentence-to-graph retrieval and image retrieval.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4385828216",
    "type": "article"
  },
  {
    "title": "<i> S <sup>2</sup> CL-Leaf Net </i> : Recognizing Leaf Images Like Human Botanists",
    "doi": "https://doi.org/10.1145/3615659",
    "publication_date": "2023-08-15",
    "publication_year": 2023,
    "authors": "C.L. Zou; Rui Wang; Cheng Jin; Sanyi Zhang; Xin Wang",
    "corresponding_authors": "",
    "abstract": "Automatically classifying plant leaves is a challenging fine-grained classification task because of the diversity in leaf morphology, including size, texture, shape, and venation. Although powerful deep learning-based methods have achieved great improvement in leaf classification, these methods still require a large number of well-labeled samples for supervised training, which is difficult to get. In contrast, relying on the specific coarse-to-fine classification strategy, human botanists only require a small number of samples for accurate leaf recognition. Inspired by the classification strategy of human botanists, we propose a novel S 2 CL-Leaf Net , which exploits multi-granularity clues with a hierarchical attention mechanism and boosts the learning ability with the supervised sampling contrastive learning with limited training samples to classify plant leaves as human botanists do. Specifically, to fully explore and exploit the subtle details of the leaves, a novel sampling transformation mechanism is combined with the supervised contrastive learning to enhance the network’s perception of details by amplifying the discriminative regions with a weighted sampling of different regions. Furthermore, we construct the hierarchical attention mechanism to produce attention maps of different granularity, which helps to discover details in leaves that are important for classification. Experiments are conducted on the open-access leaf datasets, including Flavia, Swedish, and LeafSnap, which prove the effectiveness of the proposed S 2 CL-Leaf Net .",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4385836232",
    "type": "article"
  },
  {
    "title": "Incremental Audio-Visual Fusion for Person Recognition in Earthquake Scene",
    "doi": "https://doi.org/10.1145/3614434",
    "publication_date": "2023-08-15",
    "publication_year": 2023,
    "authors": "Sisi You; Yukun Zuo; Hantao Yao; Changsheng Xu",
    "corresponding_authors": "",
    "abstract": "Earthquakes have a profound impact on social harmony and property, resulting in damage to buildings and infrastructure. Effective earthquake rescue efforts require rapid and accurate determination of whether any survivors are trapped in the rubble of collapsed buildings. While deep learning algorithms can enhance the speed of rescue operations using single-modal data (either visual or audio), they are confronted with two primary challenges: insufficient information provided by single-modal data and catastrophic forgetting. In particular, the complexity of earthquake scenes means that single-modal features may not provide adequate information. Additionally, catastrophic forgetting occurs when the model loses the information learned in a previous task after training on subsequent tasks, due to non-stationary data distributions in changing earthquake scenes. To address these challenges, we propose an innovative approach that utilizes an incremental audio-visual fusion model for person recognition in earthquake rescue scenarios. Firstly, we leverage a cross-modal hybrid attention network to capture discriminative temporal context embedding, which uses self-attention and cross-modal attention mechanisms to combine multi-modality information, enhancing the accuracy and reliability of person recognition. Secondly, an incremental learning model is proposed to overcome catastrophic forgetting, which includes elastic weight consolidation and feature replay modules. Specifically, the elastic weight consolidation module slows down learning on certain weights based on their importance to previously learned tasks. The feature replay module reviews the learned knowledge by reusing the features conserved from the previous task, thus preventing catastrophic forgetting in dynamic environments. To validate the proposed algorithm, we collected the Audio-Visual Earthquake Person Recognition (AVEPR) dataset from earthquake films and real scenes. Furthermore, the proposed method gets 85.41% accuracy while learning the 10th new task, which demonstrates the effectiveness of the proposed method and highlights its potential to significantly improve earthquake rescue efforts.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4385838976",
    "type": "article"
  },
  {
    "title": "An Image Arbitrary-Scale Super-Resolution Network Using Frequency-domain Information",
    "doi": "https://doi.org/10.1145/3616376",
    "publication_date": "2023-08-16",
    "publication_year": 2023,
    "authors": "Jing Fang; Yinbo Yu; Zhongyuan Wang; Xin Ding; Ruimin Hu",
    "corresponding_authors": "",
    "abstract": "Image super-resolution (SR) is a technique to recover lost high-frequency information in low-resolution (LR) images. Since spatial-domain information has been widely exploited, there is a new trend to involve frequency-domain information in SR tasks. Besides, image SR is typically application-oriented and various computer vision tasks call for image arbitrary magnification. Therefore, in this article, we study image features in the frequency domain to design a novel image arbitrary-scale SR network. First, we statistically analyze LR-HR image pairs of several datasets under different scale factors and find that the high-frequency spectra of different images under different scale factors suffer from different degrees of degradation, but the valid low-frequency spectra tend to be retained within a certain distribution range. Then, based on this finding, we devise an adaptive scale-aware feature division mechanism using deep reinforcement learning, which can accurately and adaptively divide the frequency spectrum into the low-frequency part to be retained and the high-frequency one to be recovered. Finally, we design a scale-aware feature recovery module to capture and fuse multi-level features for reconstructing the high-frequency spectrum at arbitrary scale factors. Extensive experiments on public datasets show the superiority of our method compared with state-of-the-art methods.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4385874054",
    "type": "article"
  },
  {
    "title": "VirtualLoc: Large-scale Visual Localization Using Virtual Images",
    "doi": "https://doi.org/10.1145/3622788",
    "publication_date": "2023-09-04",
    "publication_year": 2023,
    "authors": "Yuan Xiong; Jingru Wang; Zhong Zhou",
    "corresponding_authors": "",
    "abstract": "Robust and accurate camera pose estimation is fundamental in computer vision. Learning-based regression approaches acquire six-degree-of-freedom camera parameters accurately from visual cues of an input image. However, most are trained on street-view and landmark datasets. These approaches can hardly be generalized to overlooking use cases, such as the calibration of the surveillance camera and unmanned aerial vehicle. Besides, reference images captured from the real world are rare and expensive, and their diversity is not guaranteed. In this article, we address the problem of using alternative virtual images for visual localization training. This work has the following principle contributions: First, we present a new challenging localization dataset containing six reconstructed large-scale three-dimensional scenes, 10,594 calibrated photographs with condition changes, and 300k virtual images with pixelwise labeled depth, relative surface normal, and semantic segmentation. Second, we present a flexible multi-feature fusion network trained on virtual image datasets for robust image retrieval. Third, we propose an end-to-end confidence map prediction network for feature filtering and pose estimation. We demonstrate that large-scale rendered virtual images are beneficial to visual localization. Using virtual images can solve the diversity problem of real images and leverage labeled multi-feature data for deep learning. Experimental results show that our method achieves remarkable performance surpassing state-of-the-art approaches. To foster research on improvement for visual localization using synthetic images, we release our benchmark at https://github.com/YuanXiong/contributions .",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4386422526",
    "type": "article"
  },
  {
    "title": "Complex Scenario Image Retrieval via Deep Similarity-aware Hashing",
    "doi": "https://doi.org/10.1145/3624016",
    "publication_date": "2023-09-13",
    "publication_year": 2023,
    "authors": "Xiushan Nie; Yang Shi; Ziyu Meng; Jin Huang; Weili Guan; Yilong Yin",
    "corresponding_authors": "",
    "abstract": "When performing hashing-based image retrieval, it is difficult to learn discriminative hash codes especially for the multi-label, zero-shot and fine-grained settings. This is due to the fact that the similarities vary, even within the same category, under the conditions of complex scenario settings. To address this problem, this study develops a deep similarity-aware hashing method for complex scenario image retrieval (DEPISH). DEPISH more focuses on the samples that are difficult to distinguish from other images (i.e., “difficult samples”), such as images that contain multiple semantics. It dynamically divides attention among samples according to their difficulty levels with a margin weighting strategy. Furthermore, by adding special terms in the model, DEPISH is capable of avoiding the inconsistency between the hash code representation and true similarity among negative samples. In addition, unlike the existing methods that use a pre-defined similarity matrix with fixed values, the DEPISH adopts an adaptive similarity matrix, which accurately captures the various similarities among all samples. The results of our experiment on multiple benchmark datasets containing complex scenarios (i.e., multi-label, zero-shot, and fine-grained datasets) verify the effectiveness of this method.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4386710209",
    "type": "article"
  },
  {
    "title": "Special Issue on Deep Learning for Intelligent Human Computer Interaction",
    "doi": "https://doi.org/10.1145/3605151",
    "publication_date": "2023-09-25",
    "publication_year": 2023,
    "authors": "Zhihan Lv; Fabio Poiesi; Qi Dong; Jaime Lloret; Houbing Song",
    "corresponding_authors": "",
    "abstract": "introduction Share on Special Issue on Deep Learning for Intelligent Human Computer Interaction Editors: Zhihan Lv Uppsala University, China Uppsala University, China 0000-0003-2525-3074Search about this author , Fabio Poiesi Fondazione Bruno Kessler, Italy Fondazione Bruno Kessler, Italy 0000-0002-9769-1279Search about this author , Qi Dong Amazon AWS AI, USA Amazon AWS AI, USA 0000-0002-3376-5654Search about this author , Jaime Lloret Universitat Politecnica de Valencia, Valencia, Spain Universitat Politecnica de Valencia, Valencia, Spain 0000-0002-0862-0533Search about this author , Houbing Song Embry-Riddle Aeronautical University, USA Embry-Riddle Aeronautical University, USA 0000-0003-2631-9223Search about this author Authors Info & Claims ACM Transactions on Multimedia Computing, Communications, and ApplicationsVolume 20Issue 2Article No.: 38pp 1–5https://doi.org/10.1145/3605151Published:25 September 2023Publication History 0citation45DownloadsMetricsTotal Citations0Total Downloads45Last 12 Months45Last 6 weeks26 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4387016122",
    "type": "article"
  },
  {
    "title": "Generative Image Steganography based on Guidance Feature Distribution",
    "doi": "https://doi.org/10.1145/3625297",
    "publication_date": "2023-09-26",
    "publication_year": 2023,
    "authors": "Youqiang Sun; Jianyi Liu; Ru Zhang",
    "corresponding_authors": "",
    "abstract": "Without modification, generative steganography is more secure than modification-based steganography. However, existing generative steganography methods still have limitations, such as low embedding capacity and poor quality. To solve these issues, a synthesis-based generative steganographic model is proposed in this article. In the image synthesis task, guidance features are utilized to synthesize images with specific styles and attributes. Due to the consistency of the guidance features before and after image synthesis, the features can be used as cover for steganography. The proposed model adopts the mean and standard deviation to quantify the distribution of guidance features, enabling the secret hiding within different trends of the feature distribution. By controlling the statistical dispersion of the embedded guidance features through the mean and standard deviation, the original feature distribution is preserved, and the synthesized image maintains good generation quality. The space of guidance features contains styles and attribute descriptions of various images, offering a large space for information hiding. According to the experimental results, compared with existing steganography, the proposed steganographic model achieves better quality and hidden capacity with strong robustness.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4387055638",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on DNA-centric Modeling and Practice for Next-generation Computing and Communication Systems",
    "doi": "https://doi.org/10.1145/3578364",
    "publication_date": "2023-09-27",
    "publication_year": 2023,
    "authors": "Suyel Namasudra; Pascal Lorenz; Seifedine Kadry; Syed Ahmad Chan Bukhari",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4387099076",
    "type": "article"
  },
  {
    "title": "Hierarchical Learning and Dummy Triplet Loss for Efficient Deepfake Detection",
    "doi": "https://doi.org/10.1145/3626101",
    "publication_date": "2023-10-05",
    "publication_year": 2023,
    "authors": "Nicolas Beuve; Wassim Hamidouche; Olivier Déforges",
    "corresponding_authors": "",
    "abstract": "The advancement of generative models has made it easier to create highly realistic Deepfake videos. This accessibility has led to a surge in research on Deepfake detection to mitigate potential misuse. Typically, Deepfake detection models utilize binary backbones, even though the training dataset contains additional exploitable information, such as the Deepfake generation method employed for each video. However, recent findings suggest that inferring a binary class from a multi-class backbone yields superior performance compared to directly employing a binary backbone. Building upon this research, our article introduces two novel methods to infer a binary class from a multi-class backbone. The first method, named root dummies , leverages the dummy triplet loss, which employs fixed vectors (i.e., dummies) instead of mined positives and negatives in the triplet loss. By training the multi-class backbone with these dummies, we can easily infer a binary class during testing by adjusting the number of dummies (from six during training to two during inference). Through this approach, we achieve an accuracy improvement of 0.23% compared to the existing inference method, without requiring additional training. The second proposed method is transfer learning. It involves training a classifier, such as a support vector machine, to predict binary classes based on the image embeddings generated by the multi-class backbone. Although this method necessitates additional training, it further enhances the model’s performance, resulting in an accuracy increase of 1.79%. In summary, our proposed methods improve the accuracy of Deepfake detection by simply modifying the number of classes during training, making them suitable for integration into a variety of existing Deepfake training pipelines. Additionally, to foster reproducible research, we have made the source code of our solution publicly available at https://github.com/beuve/DmyT .",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4387367882",
    "type": "article"
  },
  {
    "title": "Feature Disentanglement Network: Multi-Object Tracking Needs More Differentiated Features",
    "doi": "https://doi.org/10.1145/3626825",
    "publication_date": "2023-10-09",
    "publication_year": 2023,
    "authors": "Wen Guo; Wuzhou Quan; Junyu Gao; Tianzhu Zhang; Changsheng Xu",
    "corresponding_authors": "",
    "abstract": "To reduce computational redundancies, a common approach is to integrate detection and re-identification (Re-ID) into a single network in multi-object tracking (MOT), referred to as “tracking by detection.” Most of the previous research has focused on resolving the conflict between the detection and Re-ID branches, considering it a simple coupling. In our work, we uncover that the entangled state between the detection and Re-ID tasks is much more complex than previous idea, resulting in a form of competition that degrades performance. To address the preceding issue, we propose a feature disentanglement network that deeply disentangles the intricately interwoven latent space of features and provides differentiated feature maps for each individual task. Furthermore, considering the demand for shallow semantic features in the feature re-ID branch, we also introduce a feature re-globalization module to enrich the shallow semantics. By integrating two distinct networks into a one-shot online MOT method, we develop a robust MOT tracker (named HDGTrack ). We conduct extensive experiments on a number of benchmarks, and our experimental results demonstrate that our method significantly outperforms state-of-the-art MOT methods. Besides, HDGTrack is efficient and can run at 13.9 (MOT17) and 8.7 (MOT20) frames per second.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4387460682",
    "type": "article"
  },
  {
    "title": "Pedestrian Attribute Recognition via Spatio-temporal Relationship Learning for Visual Surveillance",
    "doi": "https://doi.org/10.1145/3632624",
    "publication_date": "2023-11-13",
    "publication_year": 2023,
    "authors": "Zhenyu Liu; Da Li; Xinyu Zhang; Zhang Zhang; Peng Zhang; Caifeng Shan; Jungong Han",
    "corresponding_authors": "",
    "abstract": "Pedestrian attribute recognition (PAR) aims at predicting the visual attributes of a pedestrian image. PAR has been used as soft biometrics for visual surveillance and IoT security. Most of the current PAR methods are developed based on discrete images. However, it is challenging for the image-based method to handle the occlusion and action-related attributes in real-world applications. Recently, video-based PAR has attracted much attention in order to exploit the temporal cues in the video sequences for better PAR. Unfortunately, existing methods usually ignore the correlations among different attributes and the relations between attributes and spatio regions. To address this problem, we propose a novel method for video-based PAR by exploring the relationships among different attributes in both the spatio and temporal domains. More specifically, a spatio-temporal saliency module (STSM) is introduced to capture the key visual patterns from the video sequences, and a module for spatio-temporal attribute relationship learning (STARL) is proposed to mine the correlations among these patterns. Meanwhile, a large-scale benchmark for video-based PAR, RAP-Video, is built by extending the image-based dataset RAP-2, which contains 83,216 tracklets with 25 scenes. To the best of our knowledge, this is the largest dataset for video-based PAR. Extensive experiments are performed on the proposed benchmark as well as on MARS Attribute and DukeMTMC-Video Attribute. The superior performance demonstrates the effectiveness of the proposed method.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4388638391",
    "type": "article"
  },
  {
    "title": "Scene Text Recognition via Dual-path Network with Shape-driven Attention Alignment",
    "doi": "https://doi.org/10.1145/3633517",
    "publication_date": "2023-11-21",
    "publication_year": 2023,
    "authors": "Yijie Hu; Bin Dong; Kaizhu Huang; Lei Ding; Wei Wang; Xiaowei Huang; Qiufeng Wang",
    "corresponding_authors": "",
    "abstract": "Scene text recognition (STR), one typical sequence-to-sequence problem, has drawn much attention recently in multimedia applications. To guarantee good performance, it is essential for STR to obtain aligned character-wise features from the whole-image feature maps. While most present works adopt fully data-driven attention-based alignment, such practice ignores specific character geometric information. In this article, built upon a group of learnable geometric points, we propose a novel shape-driven attention alignment method that is able to obtain character-wise features. Concretely, we first design a corner detector to generate a shape map to guide the attention alignments explicitly, where a series of points can be learned to represent character-wise features flexibly. We then propose a dual-path network with a mutual learning and cooperating strategy that successfully combines CNN with a ViT-based model, leading to further accuracy improvement. We conduct extensive experiments to evaluate the proposed method on various scene text benchmarks, including six popular regular and irregular datasets, two more challenging datasets (i.e., WordArt and OST), and three Chinese datasets. Experimental results indicate that our method can achieve superior performance with a comparable model size against many state-of-the-art models.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4388848051",
    "type": "article"
  },
  {
    "title": "Self-Adaptive Representation Learning Model for Multi-Modal Sentiment and Sarcasm Joint Analysis",
    "doi": "https://doi.org/10.1145/3635311",
    "publication_date": "2023-12-05",
    "publication_year": 2023,
    "authors": "Yazhou Zhang; Yang Yu; Mengyao Wang; Min Huang; M. Shamim Hossain",
    "corresponding_authors": "",
    "abstract": "Sentiment and sarcasm are intimate and complex, as sarcasm often deliberately elicits an emotional response in order to achieve its specific purpose. Current challenges in multi-modal sentiment and sarcasm joint detection mainly include multi-modal representation fusion and the modeling of the intrinsic relationship between sentiment and sarcasm. To address these challenges, we propose a single-input stream self-adaptive representation learning model ( SRLM ) for sentiment and sarcasm joint recognition. Specifically, we divide the image into blocks to learn its serialized features and fuse textual feature as input to the target model. Then, we introduce an adaptive representation learning network using a gated network approach for sarcasm and sentiment classification. In this framework, each task is equipped with its dedicated expert network responsible for learning task-specific information, while the shared expert knowledge is acquired and weighted through the gating network. Finally, comprehensive experiments conducted on two publicly available datasets, namely Memotion and MUStARD, demonstrate the effectiveness of the proposed model when compared to state-of-the-art baselines. The results reveal a notable improvement on the performance of sentiment and sarcasm tasks.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4389338020",
    "type": "article"
  },
  {
    "title": "Synthetic Data for Object Detection with Neural Networks: State of the Art Survey of Domain Randomisation Techniques",
    "doi": "https://doi.org/10.1145/3637064",
    "publication_date": "2023-12-11",
    "publication_year": 2023,
    "authors": "Adam Westerski; Fong Wee Teck",
    "corresponding_authors": "",
    "abstract": "Machine learning relies heavily on access to large and well-maintained datasets. In this article, we focus on Computer Vision and object detection applications to survey past research on automatic generation of annotated datasets that does not require costly and time-consuming human labelling. In specific, we analyse research done in the area of Domain Randomisation applied to Neural Networks predominant in object detection since the last decade. We propose a set of criteria for comparison of previously published works, and utilise these criteria to make conclusions about various trends in the area, similarities/differences and key discoveries made since conception. The purpose of this work is to advise practitioner on leading solutions and help researchers gain better understanding of the landscape. The key takeaways from our analysis show the current state of the art solutions within the mid-quartile range allow object detection with typically about 1-25% performance decrease in comparison to manually annotated datasets; while the top performant approaches above the upper quartile gain about 2-32% lead over real data training in their specific application areas. Our survey shows the future outlook is more research into 3D generation techniques, with most innovative yet complex techniques related to end-to-end modifications of entire network architectures to suit synthetic data training.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4389572037",
    "type": "article"
  },
  {
    "title": "Estimation of certainty for responses to multiple-choice questionnaires using eye movements",
    "doi": "https://doi.org/10.1145/1413862.1413867",
    "publication_date": "2008-11-01",
    "publication_year": 2008,
    "authors": "Minoru Nakayama; Yosiyuki Takahasi",
    "corresponding_authors": "",
    "abstract": "To examine the feasibility of estimating the degree of strength of belief (SOB) of responses using eye movements, the scan paths of eye movements were analyzed while subjects reviewed their own responses to multiple choice tasks. All fixation points of eye movements were classified into visual areas, or cells, which corresponded with the positions of answers. Two estimation procedures are proposed using eye-movement data. The first one is identifying SOB using scan-path transitions. By comparing subject's reports of high and low SOB and eye-movement estimations, a significant correct rate of discrimination of SOB was observed. When the threshold of discrimination was controlled, a high rate of correct responses was obtained if it was set at a low level. The second procedure is conducting SOB discrimination using support vector machines (SVM) trained with features of fixations. Subject's gazing features were analyzed while they reviewed their own responses. A discrimination model for SOB was trained with several combinations of features to see whether performance of a significant level could be obtained. As a result, a trained model with 3 features (which consist of interval time, vertical difference, and length between fixations) can provide significant discrimination performance for SOB. These results provide evidence that strength of belief can be estimated using eye movements",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2051298200",
    "type": "article"
  },
  {
    "title": "Sequential Articulated Motion Reconstruction from a Monocular Image Sequence",
    "doi": "https://doi.org/10.1145/3180420",
    "publication_date": "2018-03-26",
    "publication_year": 2018,
    "authors": "Yong Su; Zhiyong Feng; Jianhai Zhang; Weilong Peng; Meng Xing",
    "corresponding_authors": "",
    "abstract": "In this article, we present a sequential approach for articulated motion estimation from a 2D skeleton sequence. This is a challenging task due to the complexity of human movements and the inherent depth ambiguities. The proposed approach models the human movement on a kinematic manifold with the tangent bundle, which is a natural geometrical representation of articulated motion. Combined with a second-order stochastic dynamic model based on the Markov hypothesis, we generalize the Extended Rauch Tung Striebel smoother to a Riemannian manifold to simulate the process of human movement. The human motor system might violate the Markov hypothesis when the human body is subject to external forces, and therefore a refinement stage is introduced to correct the estimation error. Specifically, the current estimation is refined in a feasible solution region consisting of a set of local estimations. This region is called a simplex, in which each element can be represented by a convex hull of all ingredients. We have proved that the refinement problem can be converted into a convex optimization problem with the simplicial constraint. Since the proposed formulation conforms to the principles of kinematic and spatio-temporal continuity of articulated motion, the reconstruction ambiguity can be alleviated essentially. The performance of the proposed algorithm is conducted on multiple synthetic sequences from the CMU and the HDM05 MoCap databases. The results show that, without requiring any training data, the proposed approach achieves greater accuracy over state-of-the-art baselines. Furthermore, the proposed approach outperforms two baselines on real sequences from the Human3.6m MoCap database.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2795032393",
    "type": "article"
  },
  {
    "title": "A Hybrid Approach for Spatio-Temporal Validation of Declarative Multimedia Documents",
    "doi": "https://doi.org/10.1145/3267127",
    "publication_date": "2018-10-30",
    "publication_year": 2018,
    "authors": "Joel dos Santos; Débora C. Muchaluat-Saade; Cécile Roisin; Nabil Layaïda",
    "corresponding_authors": "",
    "abstract": "Declarative multimedia documents represent the description of multimedia applications in terms of media items and relationships among them. Relationships specify how media items are dynamically arranged in time and space during runtime. Although a declarative approach usually facilitates the authoring task, authors can still make mistakes due to incorrect use of language constructs or inconsistent or missing relationships in a document. In order to properly support multimedia application authoring, it is important to provide tools with validation capabilities. Document validation can indicate possible inconsistencies in a given document to an author so that it can be revised before deployment. Although very useful, multimedia validation tools are not often provided by authoring tools. This work proposes a multimedia validation approach that relies on a formal model called Simple Hypermedia Model ( SHM ). SHM is used for representing a document for the purpose of validation. An SHM document is validated using a hybrid approach based on two complementary techniques. The first one captures the document’s spatio-temporal layout in terms of its state throughout its execution by means of a rewrite theory, and validation is performed through model-checking . The second one captures the document’s layout in terms of intervals and event occurrences by means of Satisfiability Modulo Theories (SMT) formulas, and validation is performed through SMT solving. Due to different characteristics of both approaches, each validation technique complements the other in terms of expressiveness of SHM and tests to be checked. We briefly present validation tools that use our approach. They were evaluated with real NCL documents and by usability tests.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2899058258",
    "type": "article"
  },
  {
    "title": "Understanding the Dynamics of Social Interactions",
    "doi": "https://doi.org/10.1145/3300937",
    "publication_date": "2019-01-31",
    "publication_year": 2019,
    "authors": "Rim Trabelsi; Jagannadan Varadarajan; Le Zhang; Issam Jabri; Yong Pei; Fethi Smach; Ammar Bouallégue; Pierre Moulin",
    "corresponding_authors": "",
    "abstract": "In this article, we deal with the problem of understanding human-to-human interactions as a fundamental component of social events analysis. Inspired by the recent success of multi-modal visual data in many recognition tasks, we propose a novel approach to model dyadic interaction by means of features extracted from synchronized 3D skeleton coordinates, depth, and Red Green Blue (RGB) sequences. From skeleton data, we extract new view-invariant proxemic features, named Unified Proxemic Descriptor (UProD), which is able to incorporate intrinsic and extrinsic distances between two interacting subjects. A novel key frame selection method is introduced to identify salient instants of the interaction sequence based on the joints’ energy. From Red Green Blue Depth (RGBD) videos, more holistic CNN features are extracted by applying an adaptive pre-trained Convolutional Neural Networks (CNNs) on optical flow frames. For better understanding the dynamics of interactions, we expand the boundaries of dyadic interactions analysis by proposing a fundamentally new modeling for non-treated problem aiming to discern the active from the passive interactor. Extensive experiments have been carried out on four multi-modal and multi-view interactions datasets. The experimental results demonstrate the superiority of our proposed techniques against the state-of-the-art approaches.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2917151376",
    "type": "article"
  },
  {
    "title": "An Image Cues Coding Approach for 3D Human Pose Estimation",
    "doi": "https://doi.org/10.1145/3368066",
    "publication_date": "2019-11-30",
    "publication_year": 2019,
    "authors": "Meng Xing; Zhiyong Feng; Yong Su; Jianhai Zhang",
    "corresponding_authors": "",
    "abstract": "Although Deep Convolutional Neural Networks (DCNNs) facilitate the evolution of 3D human pose estimation, ambiguity remains the most challenging problem in such tasks. Inspired by the Human Perception Mechanism (HPM), we propose an image-to-pose coding method to fill the gap between image cues and 3D poses, thereby alleviating the ambiguity of 3D human pose estimation. First, in 3D pose space, we divide the whole 3D pose space into multiple subregions named pose codes , turning a disambiguation problem into a classification problem. The proposed coding mechanism covers multiple camera views and provides a complete description for 3D pose space. Second, it is noteworthy that the articulated structure of the human body lies on a sophisticated product manifold and the error accumulation in the chain structure will undoubtedly affect the coding performance. Therefore, in image space, we extract the image cues from independent local image patches rather than the whole image. The mapping relationship between image cues and 3D pose codes is established by a set of DCNNs. The image-to-pose coding method transforms the implicit image cues into explicit constraints. Finally, the image-to-pose coding method is integrated into a linear matching mechanism to construct a 3D pose estimation method that effectively alleviates the ambiguity. We conduct extensive experiments on widely used public benchmarks. The experimental results show that our method effectively alleviates the ambiguity in 3D pose recovery and is robust to the variations of view.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3001061767",
    "type": "article"
  },
  {
    "title": "Unsupervised Learning of Human Action Categories in Still Images with Deep Representations",
    "doi": "https://doi.org/10.1145/3362161",
    "publication_date": "2019-11-30",
    "publication_year": 2019,
    "authors": "Yunpeng Zheng; Xuelong Li; Xiaoqiang Lu",
    "corresponding_authors": "",
    "abstract": "In this article, we propose a novel method for unsupervised learning of human action categories in still images. In contrast to previous methods, the proposed method explores distinctive information of actions directly from unlabeled image databases, attempting to learn discriminative deep representations in an unsupervised manner to distinguish different actions. In the proposed method, action image collections can be used without manual annotations. Specifically, (i) to deal with the problem that unsupervised discriminative deep representations are difficult to learn, the proposed method builds a training dataset with surrogate labels from the unlabeled dataset, then learns discriminative representations by alternately updating convolutional neural network (CNN) parameters and the surrogate training dataset in an iterative manner; (ii) to explore the discriminatory information among different action categories, training batches for updating the CNN parameters are built with triplet groups and the triplet loss function is introduced to update the CNN parameters; and (iii) to learn more discriminative deep representations, a Random Forest classifier is adopted to update the surrogate training dataset, and more beneficial triplet groups then can be built with the updated surrogate training dataset. Extensive experiments on four benchmark datasets demonstrate the effectiveness of the proposed method.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3004908934",
    "type": "article"
  },
  {
    "title": "SCeVE",
    "doi": "https://doi.org/10.1145/3377353",
    "publication_date": "2020-05-22",
    "publication_year": 2020,
    "authors": "Shanthi Vellingiri; Ryan P. McMahan; Balakrishnan Prabhakaran",
    "corresponding_authors": "",
    "abstract": "Authoring a collaborative, interactive Mixed Reality (MR) tour requires flexible design and development of various software modules for tasks such as managing geographically distributed participants, adaptable travel and virtual camera techniques, data logging for assessment of the incorporated techniques, as well as for evaluating the Quality of Experiences (QoE). In most cases, authors might have to develop all these software modules, instead of focusing only on the virtual environment design. In this article, we propose SCeVE, a component-based framework that supports flexible design and authoring of interactive MR tours by offering ease of access to four major design choices: (i) S ynchronization, (ii) C ollaborative e xploration, (iii) V isualization, and (iv) E valuation. Based on tour requirements, an author can access one or more components (or software libraries) of design choices via SCeVE’s API (Application Programming Interface) services , as demonstrated by the two case studies on group travel in a plant walk MR tour. SCeVE framework is innovative in the sense that it facilitates group travel in virtual environments involving “live” models of participants from geographically distributed sites. SCeVE empowers authors to focus only on the design of the required virtual environments. They can quickly build a diverse set of collaborative MR tours by utilizing the flexibility of SCeVE in terms of the various available options for traveling, rendering on multiple devices, and virtual camera viewpoint computation strategies. By providing data logs of various components, SCeVE facilitates performance evaluation of the various strategies used as well as the user experience in collaborative MR tours. SCeVE is designed in an extensible manner, allowing authors to add devices and software services as additional components.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3032272567",
    "type": "article"
  },
  {
    "title": "<i>CryptoLesion</i>",
    "doi": "https://doi.org/10.1145/3380743",
    "publication_date": "2020-05-31",
    "publication_year": 2020,
    "authors": "Vishesh Kumar Tanwar; Balasubramanian Raman; Amitesh Singh Rajput; Rama Bhargava",
    "corresponding_authors": "",
    "abstract": "The low-cost, accessing flexibility, agility, and mobility of cloud infrastructures have attracted medical organizations to store their high-resolution data in encrypted form. Besides storage, these infrastructures provide various image processing services for plain (non-encrypted) images. Meanwhile, the privacy and security of uploaded data depend upon the reliability of the service provider(s). The enforcement of laws towards privacy policies in health-care organizations, for not disclosing their patient’s sensitive and private medical information, restrict them to utilize these services. To address these privacy concerns for melanoma detection, we propose CryptoLesion , a privacy-preserving model for segmenting lesion region using whale optimization algorithm (WOA) over the cloud in the encrypted domain (ED). The user’s image is encrypted using a permutation ordered binary number system and a random stumble matrix. The task of segmentation is accomplished by dividing an encrypted image into a pre-defined number of clusters whose optimal centroids are obtained by WOA in ED, followed by the assignment of each pixel of an encrypted image to the unique centroid. The qualitative and quantitative analysis of CryptoLesion is evaluated over publicly available datasets provided in The International Skin Imaging Collaboration Challenges in 2016, 2017, 2018, and PH 2 dataset. The segmented results obtained by CryptoLesion are found to be comparable with the winners of respective challenges. CryptoLesion is proved to be secure from a probabilistic viewpoint and various cryptographic attacks. To the best of our knowledge, CryptoLesion is first moving towards the direction of lesion segmentation in ED.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3035369590",
    "type": "article"
  },
  {
    "title": "Proposal Complementary Action Detection",
    "doi": "https://doi.org/10.1145/3361845",
    "publication_date": "2020-04-30",
    "publication_year": 2020,
    "authors": "Suguo Zhu; Xiaoxian Yang; Jun Yu; Zhenying Fang; Meng Wang; Qingming Huang",
    "corresponding_authors": "",
    "abstract": "Temporal action detection not only requires correct classification but also needs to detect the start and end times of each action accurately. However, traditional approaches always employ sliding windows or actionness to predict the actions, and it is different to train to model with sliding windows or actionness by end-to-end means. In this article, we attempt a different idea to detect the actions end-to-end, which can calculate the probabilities of actions directly through one network as one part of the results. We present PCAD, a novel proposal complementary action detector to deal with video streams under continuous, untrimmed conditions. Our approach first uses a simple fully 3D convolutional network to encode the video streams and then generates candidate temporal proposals for activities by using anchor segments. To generate more precise proposals, we also design a boundary proposal network to offer some complementary information for the candidate proposals. Finally, we learn an efficient classifier to classify the generated proposals into different activities and refine their temporal boundaries at the same time. Our model can achieve end-to-end training by jointly optimizing classification loss and regression loss. When evaluating on the THUMOS’14 detection benchmark, PCAD achieves state-of-the-art performance in high-speed models.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3045008069",
    "type": "article"
  },
  {
    "title": "An Adaptive Bitrate Switching Algorithm for Speech Applications in Context of WebRTC",
    "doi": "https://doi.org/10.1145/3458751",
    "publication_date": "2021-11-12",
    "publication_year": 2021,
    "authors": "Mohannad Alahmadi; Peter Počta; Hugh Melvin",
    "corresponding_authors": "",
    "abstract": "Web Real-Time Communication (WebRTC) combines a set of standards and technologies to enable high-quality audio, video, and auxiliary data exchange in web browsers and mobile applications. It enables peer-to-peer multimedia sessions over IP networks without the need for additional plugins. The Opus codec, which is deployed as the default audio codec for speech and music streaming in WebRTC, supports a wide range of bitrates. This range of bitrates covers narrowband, wideband, and super-wideband up to fullband bandwidths. Users of IP-based telephony always demand high-quality audio. In addition to users’ expectation, their emotional state, content type, and many other psychological factors; network quality of service; and distortions introduced at the end terminals could determine their quality of experience. To measure the quality experienced by the end user for voice transmission service, the E-model standardized in the ITU-T Rec. G.107 (a narrowband version), ITU-T Rec. G.107.1 (a wideband version), and the most recent ITU-T Rec. G.107.2 extension for the super-wideband E-model can be used. In this work, we present a quality of experience model built on the E-model to measure the impact of coding and packet loss to assess the quality perceived by the end user in WebRTC speech applications. Based on the computed Mean Opinion Score, a real-time adaptive codec parameter switching mechanism is used to switch to the most optimum codec bitrate under the present network conditions. We present the evaluation results to show the effectiveness of the proposed approach when compared with the default codec configuration in WebRTC.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3135224875",
    "type": "article"
  },
  {
    "title": "A Multiple Sieve Approach Based on Artificial Intelligent Techniques and Correlation Power Analysis",
    "doi": "https://doi.org/10.1145/3433165",
    "publication_date": "2021-05-18",
    "publication_year": 2021,
    "authors": "Yaoling Ding; Liehuang Zhu; An Wang; Yuan Li; Yongjuan Wang; Siu Ming Yiu; Keke Gai",
    "corresponding_authors": "",
    "abstract": "Side-channel analysis achieves key recovery by analyzing physical signals generated during the operation of cryptographic devices. Power consumption is one kind of these signals and can be regarded as a multimedia form. In recent years, many artificial intelligence technologies have been combined with classical side-channel analysis methods to improve the efficiency and accuracy. A simple genetic algorithm was employed in Correlation Power Analysis (CPA) when apply to cryptographic algorithms implemented in parallel. However, premature convergence caused failure in recovering the whole key, especially when plenty of large S-boxes were employed in the target primitive, such as in the case of AES. In this article, we investigate the reason of premature convergence and propose a Multiple Sieve Method (MS-CPA), which overcomes this problem and reduces the number of traces required in correlation power analysis. Our method can be adjusted to combine with key enumeration algorithms and further improves the efficiency. Simulation experimental results depict that our method reduces the required number of traces by <?TeX $63.7\\%$?> and <?TeX $30.77\\%$?> , compared to classic CPA and the Simple-Genetic-Algorithm-based CPA (SGA-CPA), respectively, when the success rate is fixed to <?TeX $90\\%$?> . Real experiments performed on SAKURA-G confirm that the number of traces required for recovering the correct key in our method is almost equal to the minimum number that makes the correlation coefficients of correct keys stand out from the wrong ones and is much less than the numbers of traces required in CPA and SGA-CPA. When combining with key enumeration algorithms, our method has better performance. For the traces number being 200 (noise standard deviation <?TeX $\\sigma = 3.0$?> ), the attacks success rate of our method is <?TeX $85\\%$?> , which is much higher than the classic CPA with key enumeration ( <?TeX $10\\%$?> success rate). Moreover, we adjust our method to work on that DPA contest v1 dataset and achieve a better result (40.04 traces) than the winning proposal (42.42 traces).",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3162669007",
    "type": "article"
  },
  {
    "title": "Deep Active Context Estimation for Automated COVID-19 Diagnosis",
    "doi": "https://doi.org/10.1145/3457124",
    "publication_date": "2021-10-26",
    "publication_year": 2021,
    "authors": "Bingzhi Chen; Yishu Liu; Zheng Zhang; Yingjian Li; Zhao Zhang; Guangming Lu; Hongbing Yu",
    "corresponding_authors": "",
    "abstract": "Many studies on automated COVID-19 diagnosis have advanced rapidly with the increasing availability of large-scale CT annotated datasets. Inevitably, there are still a large number of unlabeled CT slices in the existing data sources since it requires considerable consuming labor efforts. Notably, cinical experience indicates that the neighboring CT slices may present similar symptoms and signs. Inspired by such wisdom, we propose DACE, a novel CNN-based deep active context estimation framework, which leverages the unlabeled neighbors to progressively learn more robust feature representations and generate a well-performed classifier for COVID-19 diagnosis. Specifically, the backbone of the proposed DACE framework is constructed by a well-designed Long-Short Hierarchical Attention Network (LSHAN), which effectively incorporates two complementary attention mechanisms, i.e., short-range channel interactions (SCI) module and long-range spatial dependencies (LSD) module, to learn the most discriminative features from CT slices. To make full use of such available data, we design an efficient context estimation criterion to carefully assign the additional labels to these neighbors. Benefiting from two complementary types of informative annotations from -nearest neighbors, i.e., the majority of high-confidence samples with pseudo labels and the minority of low-confidence samples with hand-annotated labels, the proposed LSHAN can be fine-tuned and optimized in an incremental learning manner. Extensive experiments on the Clean-CC-CCII dataset demonstrate the superior performance of our method compared with the state-of-the-art baselines.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3209956785",
    "type": "article"
  },
  {
    "title": "Detecting Non-Aligned Double JPEG Compression Based on Amplitude-Angle Feature",
    "doi": "https://doi.org/10.1145/3464388",
    "publication_date": "2021-11-12",
    "publication_year": 2021,
    "authors": "Jinwei Wang; Wei Huang; Xiangyang Luo; Yun-Qing Shi; Sunil Kumar Jha",
    "corresponding_authors": "",
    "abstract": "Due to the popularity of JPEG format images in recent years, JPEG images will inevitably involve image editing operation. Thus, some tramped images will leave tracks of Non-aligned double JPEG ( NA-DJPEG ) compression. By detecting the presence of NA-DJPEG compression, one can verify whether a given JPEG image has been tampered with. However, only few methods can identify NA-DJPEG compressed images in the case that the primary quality factor is greater than the secondary quality factor. To address this challenging task, this article proposes a novel feature extraction scheme based optimized pixel difference ( OPD ), which is a new measure for blocking artifacts. Firstly, three color channels (RGB) of a reconstructed image generated by decompressing a given JPEG color image are mapped into spherical coordinates to calculate amplitude and two angles (azimuth and zenith). Then, 16 histograms of OPD along the horizontal and vertical directions are calculated in the amplitude and two angles, respectively. Finally, a set of features formed by arranging the bin values of these histograms is used for binary classification. Experiments demonstrate the effectiveness of the proposed method, and the results show that it significantly outperforms the existing typical methods in the mentioned task.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3211538617",
    "type": "article"
  },
  {
    "title": "Health Status Prediction with Local-Global Heterogeneous Behavior Graph",
    "doi": "https://doi.org/10.1145/3457893",
    "publication_date": "2021-11-12",
    "publication_year": 2021,
    "authors": "Xuan Ma; Xiaoshan Yang; Junyu Gao; Changsheng Xu",
    "corresponding_authors": "",
    "abstract": "Health management is getting increasing attention all over the world. However, existing health management mainly relies on hospital examination and treatment, which are complicated and untimely. The emerging of mobile devices provides the possibility to manage people's health status in a convenient and instant way. Estimation of health status can be achieved with various kinds of data streams continuously collected from wearable sensors. However, these data streams are multi-source and heterogeneous, containing complex temporal structures with local contextual and global temporal aspects, which makes the feature learning and data joint utilization challenging. We propose to model the behavior-related multi-source data streams with a local-global graph, which contains multiple local context sub-graphs to learn short term local context information with heterogeneous graph neural networks and a global temporal sub-graph to learn long term dependency with self-attention networks. Then health status is predicted based on the structure-aware representation learned from the local-global behavior graph. We take experiments on StudentLife dataset, and extensive results demonstrate the effectiveness of our proposed model.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3211779447",
    "type": "article"
  },
  {
    "title": "Using Multisensory Content to Impact the Quality of Experience of Reading Digital Books",
    "doi": "https://doi.org/10.1145/3458676",
    "publication_date": "2021-11-12",
    "publication_year": 2021,
    "authors": "Ellen P. Silva; Natália Vieira; Glauco Amorim; Renata Mousinho; Gustavo Paiva Guedes; Gheorghiță Ghinea; Joel dos Santos",
    "corresponding_authors": "",
    "abstract": "Multisensorial books enrich a story with either traditional multimedia content or sensorial effects. The main idea is to increase children’s interest in reading by enhancing their QoE while reading. Studies on enriched and/or augmented e-books also propose synchronizing additional content with text. However, they usually focus on audio, vídeo, images, or haptic feedback. In this work, we present MBook , a tool for presenting multisensorial books. It decouples the book’s textual content from the additional content, as well as its synchronization, and rendering. Thus, a change in the additional content or its synchronization does not require changes to the book’s content. To enable fine-grained synchronization, MBook captures the reading position using an eye-tracker. Experimental results with students within the 13- to 19-year-old age group point to MBook being able to provide good usability.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3212331438",
    "type": "article"
  },
  {
    "title": "Multifeature analysis and semantic context learning for image classification",
    "doi": "https://doi.org/10.1145/2457450.2457454",
    "publication_date": "2013-05-01",
    "publication_year": 2013,
    "authors": "Qianni Zhang; Ebroul Izquierdo",
    "corresponding_authors": "",
    "abstract": "This article introduces an image classification approach in which the semantic context of images and multiple low-level visual features are jointly exploited. The context consists of a set of semantic terms defining the classes to be associated to unclassified images. Initially, a multiobjective optimization technique is used to define a multifeature fusion model for each semantic class. Then, a Bayesian learning procedure is applied to derive a context model representing relationships among semantic classes. Finally, this context model is used to infer object classes within images. Selected results from a comprehensive experimental evaluation are reported to show the effectiveness of the proposed approaches.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W1982387099",
    "type": "article"
  },
  {
    "title": "Privacy preserving continuous multimedia streaming in MANETs",
    "doi": "https://doi.org/10.1145/2501643.2501645",
    "publication_date": "2013-08-01",
    "publication_year": 2013,
    "authors": "Kazuya Sakai; Wei‐Shinn Ku; Min-Te Sun; Roger Zimmermann",
    "corresponding_authors": "",
    "abstract": "At present, mobile devices are prevalent with end users and continuous media streaming services in mobile ad-hoc networks (MANETs) support popular applications. It is required for applications that stream isochronous media that the network link be continuously available. In this study, we introduce two group-server scheduling schemes to improve link continuity: static group-server scheduling and dynamic group-server scheduling . With our solution, if one of the current links between a client and a server instance breaks, the client can still download the multimedia content from another scheduled server peer. In addition, we incorporate the data link layer constraints as well as privacy concerns into our protocol design. The simulation results show that the proposed schemes significantly improve the effective link duration, overall system performance, and degree of privacy in MANETs.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W1996066613",
    "type": "article"
  },
  {
    "title": "A framework for network aware caching for video on demand systems",
    "doi": "https://doi.org/10.1145/2501643.2501652",
    "publication_date": "2013-08-01",
    "publication_year": 2013,
    "authors": "Bogdan Cărbunar; Rahul Potharaju; Michael Pearce; Venugopal Vasudevan; Michael Needham",
    "corresponding_authors": "",
    "abstract": "Video on Demand (VoD) services allow users to select and locally consume remotely stored content. We investigate the use of caching to solve the scalability issues of several existing VoD providers. We propose metrics and goals that define the requirements of a caching framework for CDNs of VoD systems. Using data logs collected from Motorola equipment from Comcast VoD deployments we show that several classic caching solutions do not satisfy the proposed goals. We address this issue by developing novel techniques for predicting future values of several metrics of interest. We rely on computed predictions to define the penalty imposed on the system, both network and caching sites, when not storing individual items. We use item penalties to devise novel caching and static content placement strategies. We use the previously mentioned data logs to validate our solutions and show that they satisfy all the defined goals.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2002945439",
    "type": "article"
  },
  {
    "title": "Mixtape",
    "doi": "https://doi.org/10.1145/3105969",
    "publication_date": "2017-08-01",
    "publication_year": 2017,
    "authors": "Luciana Fujii Pontello; Pedro H. F. Holanda; Bruno Guilherme; João Paulo V. Cardoso; Olga Goussevskaia; Ana Paula Couto da Silva",
    "corresponding_authors": "",
    "abstract": "In this work, we explore the increasing demand for novel user interfaces to navigate large media collections. We implement a geometric data structure to store and retrieve item-to-item similarity information and propose a novel navigation framework that uses vector operations and real-time user feedback to direct the outcome. The framework is scalable to large media collections and is suitable for computationally constrained devices. In particular, we implement this framework in the domain of music. To evaluate the effectiveness of the navigation process, we propose an automatic evaluation framework, based on synthetic user profiles, which allows us to quickly simulate and compare navigation paths using different algorithms and datasets. Moreover, we perform a real user study. To do that, we developed and launched Mixtape , a simple web application that allows users to create playlists by providing real-time feedback through liking and skipping patterns.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2740778368",
    "type": "article"
  },
  {
    "title": "Enhanced User Context-Aware Reputation Measurement of Multimedia Service",
    "doi": "https://doi.org/10.1145/2978569",
    "publication_date": "2016-10-12",
    "publication_year": 2016,
    "authors": "Shangguang Wang; Ao Zhou; Wei Lei; Zhiwen Yu; Ching‐Hsien Hsu; Fangchun Yang",
    "corresponding_authors": "",
    "abstract": "Reputation plays an important role for users in choosing or paying for multimedia applications or services. Some efficient multimedia reputation-measurement approaches have been proposed to achieve accurate reputation measurement based on feedback ratings that users give to a multimedia service after invoking. However, the implementation of these approaches suffers from the problems of wide abuse and low utilization of user context. In this article, we study the relationship between user context and feedback ratings according to which one user often gives different feedback ratings to the same multimedia service in different user contexts. We further propose an enhanced user context-aware reputation-measurement approach for multimedia services that is accurate in two senses: (1) Each multimedia service has three reputation values with three different user context levels when its feedback ratings are sufficient and (2) the reputation of a multimedia service with different user context levels is found using user context sensitivity and user similarity when its feedback ratings are limited or not available. Experimental results based on a real-world dataset show that our approach outperforms other approaches in terms of accuracy.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2530137731",
    "type": "article"
  },
  {
    "title": "Content-Adaptive Display Power Saving for Internet Video Applications on Mobile Devices",
    "doi": "https://doi.org/10.1145/2996461",
    "publication_date": "2016-11-09",
    "publication_year": 2016,
    "authors": "Yao Liu; Mengbai Xiao; Ming Zhang; Xin Li; Mian Dong; Zhan Ma; Zhenhua Li; Lei Guo; Songqing Chen",
    "corresponding_authors": "",
    "abstract": "Backlight scaling is a technique proposed to reduce the display panel power consumption by strategically dimming the backlight. However, for mobile video applications, a computationally intensive luminance compensation step must be performed in combination with backlight scaling to maintain the perceived appearance of video frames. This step, if done by the Central Processing Unit (CPU), could easily offset the power savings via backlight dimming. Furthermore, computing the backlight scaling values requires per-frame luminance information, which is typically too energy intensive to compute on mobile devices. In this article, we propose Content-Adaptive Display (CAD) for two typical Internet mobile video applications: video streaming and real-time video communication. CAD uses the mobile device’s Graphics Processing Unit (GPU) rather than the CPU to perform luminance compensation at reduced power consumption. For video streaming where video frames are available in advance, we compute the backlight scaling schedule using a more efficient dynamic programming algorithm than existing work. For real-time video communication where video frames are generated on the fly, we propose a greedy algorithm to determine the backlight scaling at runtime. We implement CAD in one video streaming application and one real-time video call application on the Android platform and use a Monsoon power meter to measure the real power consumption. Experiment results show that CAD can save more than 10% overall power consumption for up to 55.7% videos during video streaming and up to 31.0% overall power consumption in real-time video calls.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2550411998",
    "type": "article"
  },
  {
    "title": "MultiFusion",
    "doi": "https://doi.org/10.1145/1865106.1865109",
    "publication_date": "2010-11-01",
    "publication_year": 2010,
    "authors": "Xiangyu Wang; Mohan Kankanhalli",
    "corresponding_authors": "",
    "abstract": "The multimodal data usually contain complementary, correlated and redundant information. Thus, multimodal fusion is useful for many multisensor applications. Here, a novel multimodal fusion algorithm is proposed, which is referred to as “MultiFusion.” The approach adopts a boosting structure where the atomic event is considered as the fusion unit. The correlation of multimodal data is used to form an overall classifier in each iteration. Moreover, by adopting the Adaboost-like structure, the overall fusion performance is improved. Both the simulation experiment and the real application show the effectiveness of the MultiFusion approach. Our approach can be applied in different multimodal applications to exploit the multimedia data characteristics and improve the performance.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2036914792",
    "type": "article"
  },
  {
    "title": "Chunk Duration--Aware SDN-Assisted DASH",
    "doi": "https://doi.org/10.1145/3337681",
    "publication_date": "2019-08-20",
    "publication_year": 2019,
    "authors": "Ihsan Mert Ozcelik; Cem Ersoy",
    "corresponding_authors": "",
    "abstract": "Although Dynamic Adaptive Streaming over HTTP (DASH) is the pillar of multimedia content delivery mechanisms, its purely client-based adaptive video bitrate mechanisms have quality-of-experience fairness and stability problems in the existence of multiple DASH clients and highly fluctuating background traffic on the same shared bottleneck link. Varying chunk duration among different titles of multiple video providers exacerbates this problem. With the help of the global network view provided by the software-defined networking paradigm, we propose a centralized joint optimization module-assisted adaptive video bitrate mechanism that takes diversity of chunk sizes among different content into account. Our system collects possible video bitrate levels and chunk duration from DASH clients and simply calculates the optimal video bitrates per client based on the available capacity and chunk duration of each client’s selected content while not invading users’ privacy. By continuously following the background traffic flows, it asynchronously updates the target video bitrate levels to avoid both buffer stall events and network underutilization issues rather than bandwidth slicing, which brings about scalability problems in practice. It also guarantees fair startup delays for video sessions with various chunk duration. Our experiments clearly show that our proposed approach considering diversity of chunk duration and that background traffic fluctuations can significantly provide a better and fair quality of experience in terms of structural similarity--based video quality and startup delay compared to both purely client-based and state-of-the-art software-defined networking--based adaptive bitrate mechanisms.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2969307139",
    "type": "article"
  },
  {
    "title": "Advanced Stereo Seam Carving by Considering Occlusions on Both Sides",
    "doi": "https://doi.org/10.1145/3321513",
    "publication_date": "2019-08-20",
    "publication_year": 2019,
    "authors": "Yongyi Gong; Shangru Li; Kanoksak Wattanachote; Xiaonan Luo",
    "corresponding_authors": "",
    "abstract": "Stereo image retargeting plays a significant role in the field of image processing, which aims at making major objects as prominent as possible when the resolution of an image is changed, including maintaining disparity and depth information at the same time. Some seam carving methods are proposed to preserve the geometric consistency of the images. However, the regions of occlusion on both sides are not considered properly. In this article, we propose a solution to solve this problem. A new strategy of seams finding is designed by considering occluded and occluding regions on both of the input images, and leaving geometric consistency in both images intact. We also introduced the method of line segment detection and superpixel segmentation to further improve the quality of the images. Imaging effects are optimized in the process and visual comfort, which is also influenced by other factors, can be boosted as well.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2969315987",
    "type": "article"
  },
  {
    "title": "Deep Scalable Supervised Quantization by Self-Organizing Map",
    "doi": "https://doi.org/10.1145/3328995",
    "publication_date": "2019-08-20",
    "publication_year": 2019,
    "authors": "Min Wang; Wengang Zhou; Qi Tian; Houqiang Li",
    "corresponding_authors": "",
    "abstract": "Approximate Nearest Neighbor (ANN) search is an important research topic in multimedia and computer vision fields. In this article, we propose a new deep supervised quantization method by Self-Organizing Map to address this problem. Our method integrates the Convolutional Neural Networks and Self-Organizing Map into a unified deep architecture. The overall training objective optimizes supervised quantization loss as well as classification loss. With the supervised quantization objective, we minimize the differences on the maps between similar image pairs and maximize the differences on the maps between dissimilar image pairs. By optimization, the deep architecture can simultaneously extract deep features and quantize the features into suitable nodes in self-organizing map. To make the proposed deep supervised quantization method scalable for large datasets, instead of constructing a larger self-organizing map, we propose to divide the input space into several subspaces and construct self-organizing map in each subspace. The self-organizing maps in all the subspaces implicitly construct a large self-organizing map, which costs less memory and training time than directly constructing a self-organizing map with equal size. The experiments on several public standard datasets prove the superiority of our approaches over the existing ANN search methods. Besides, as a by-product, our deep architecture can be directly applied to visualization with little modification, and promising performance is demonstrated in the experiments.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2998319917",
    "type": "article"
  },
  {
    "title": "A Deep Learning Approach for Face Hallucination Guided by Facial Boundary Responses",
    "doi": "https://doi.org/10.1145/3377874",
    "publication_date": "2020-02-29",
    "publication_year": 2020,
    "authors": "Mengyan Li; Zhaoyu Zhang; Guochen Xie; Jun Yu",
    "corresponding_authors": "",
    "abstract": "Face hallucination is a domain-specific super-resolution (SR) problem of learning a mapping between a low-resolution (LR) face image and its corresponding high-resolution (HR) image. Tremendous progress on deep learning has shown exciting potential for a variety of face hallucination tasks. However, most deep-learning–based methods are limited to handle facial appearance information without paying attention to facial structure priors. In this article, we propose an open source 1 Boundary-aware Dual-branch Network (BDN) for face hallucination, which simultaneously extracts face features and estimates facial boundary responses from LR inputs, ultimately fusing them to reconstruct HR results. Specifically, we first upsample LR face images to HR feature maps, and then feed the upsampled HR features into a memory unit and an attention unit synchronously to obtain the refined features and predict facial boundary responses. Next, they are fed into a feature map fusion unit to combine facial appearance and structure information by a spatial attention mechanism. Moreover, we employ a series of stacked units to boost performance before recovering HR face images. Finally, a discriminative network is developed to improve visual quality by introducing adversarial learning strategy. Extensive experiments show that the proposed approach achieves superior face hallucination results against the state-of-the-art ones.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3009963939",
    "type": "article"
  },
  {
    "title": "Hyperspectral Reconstruction with Redundant Camera Spectral Sensitivity Functions",
    "doi": "https://doi.org/10.1145/3386313",
    "publication_date": "2020-05-22",
    "publication_year": 2020,
    "authors": "Xian‐Hua Han; Yinqiang Zheng; Jiande Sun; Yen‐Wei Chen",
    "corresponding_authors": "",
    "abstract": "High-resolution hyperspectral (HS) reconstruction has recently achieved significantly progress, among which the method based on the fusion of the RGB and HS images of the same scene can greatly improve the reconstruction performance compared with those based on the individually spectral or spatial enhancement. It is well known that the HS image is obtained only via the costly hypersoectral sensor, whereas the RGB images can be provided by low-price RGB cameras and the spectral sensitivity (SS) functions of RGB cameras are usually different. Thus, this study proposes a HS reconstruction, which fuses merely two RGB images with redundant spectral responses. In this work, we design a new RGB camera via shifting the SS of an existed RGB camera, which can provide similar strength of spectral response with different spectral centers of SS, and fuse the new achieved color image with an existed RGB image by a deep ResNet. Experiments validate that fusion of two existed RGB images can provide impressive HS reconstruction performance and further improvement can be achieved by integrating the color image of the simulated SS with the RGB image.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3047647477",
    "type": "article"
  },
  {
    "title": "A Practical Learning-based Approach for Viewer Scheduling in the Crowdsourced Live Streaming",
    "doi": "https://doi.org/10.1145/3397226",
    "publication_date": "2020-04-30",
    "publication_year": 2020,
    "authors": "Rui-Xiao Zhang; Ming Ma; Tianchi Huang; Haitian Pang; Xin Yao; Chenglei Wu; Lifeng Sun",
    "corresponding_authors": "",
    "abstract": "Scheduling viewers effectively among different Content Delivery Network (CDN) providers is challenging owing to the extreme diversity in the crowdsourced live streaming (CLS) scenarios. Abundant algorithms have been proposed in recent years, which, however, suffer from a critical limitation: Due to their inaccurate feature engineering or naive rules, they cannot optimally schedule viewers. To address this concern, we put forward LTS (Learn to Schedule), a novel scheduling algorithm that can adapt to the dynamics from both viewer traffics and CDN performance. In detail, we first propose LTS-RL, an approach that schedules CLS viewers based on deep reinforcement learning (DRL). Since LTS-RL is trained in an end-to-end way, it can automatically learn scheduling algorithms without any pre-programmed models or assumptions about the environment dynamics. At the same time, to practically deploy LTS-RL, we then use the decision tree and imitation learning to convert LTS-RL into a more light-weighted and interpretable model, which is denoted as Fast-LTS. After the extensive evaluation of the real data from a leading CLS platform in China, we demonstrate that our proposed model (both LTS-RL and Fast-LTS) can improve the average quality of experience (QoE) over state-of-the-art approaches by 8.71--15.63%. At the same time, we also demonstrate that Fast-LTS can faithfully convert the complicated LTS-RL with slight performance degradation (&lt; 2%), while significantly reducing the decision time (×7--10).",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3058810697",
    "type": "article"
  },
  {
    "title": "An empirical analysis of serendipitous media sharing among campus-wide wireless users",
    "doi": "https://doi.org/10.1145/1870121.1870127",
    "publication_date": "2011-01-01",
    "publication_year": 2011,
    "authors": "Surendar Chandra; Xuwen Yu",
    "corresponding_authors": "",
    "abstract": "Contemporary systems use centralized as well as peer-to-peer mechanisms for the large scale distribution of media objects. In this work, we investigate a serendipitous mechanism for directly sharing media objects among a local community of wireless users. This localized sharing is attractive when wide area network connectivity is undesirable, expensive or unavailable; especially when the shared media objects are large. With some restrictions, such localized sharing of media objects is also acceptable to content owners. However, localized sharing has to contend with far fewer media providers who may also not offer the variety of objects available from wide-area services. We collected empirical data from the widely deployed Apple iTunes application for our analysis. We showed that users are already making a significant amount of media objects available for serendipitous sharing. Our analysis showed that the shared object annotations exhibited a Zipfian long tail distribution. The availability patterns of wireless iTunes users and the object annotations makes serendipitous sharing inappropriate for scenarios that require access to a specific object. Instead, mechanisms that allow the user to specify classes of interesting objects are better suited for such users. Also, given the smaller scale of these systems, serendipitous sharing can benefit from approaches that allow users to disseminate a compact representation of their shared objects. Though the wireless user availability rates was not as high as what was observed in a corporate desktop setting, a large fraction of the users showed high temporal consistency. This allows for high availability with reasonable replication during weekday daytime hours. We answer important questions regarding the viability of a campus-wide media sharing system.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W1995290537",
    "type": "article"
  },
  {
    "title": "Building an efficient transcoding overlay for P2P streaming to heterogeneous devices",
    "doi": "https://doi.org/10.1145/2089085.2089087",
    "publication_date": "2012-02-01",
    "publication_year": 2012,
    "authors": "Dongyu Liu; Fei Li; Bo Shen; Songqing Chen",
    "corresponding_authors": "",
    "abstract": "With the increasing deployment of Internet P2P/overlay streaming systems, more and more clients use mobile devices, such as smart phones and PDAs, to access these Internet streaming services. Compared to wired desktops, mobile devices normally have a smaller screen size, a less color depth, and lower bandwidth and thus cannot correctly and effectively render and display the data streamed to desktops. To address this problem, in this paper, we propose PAT (Peer-Assisted Transcoding) to enable effective online transcoding in P2P/overlay streaming. PAT has the following unique features. First, it leverages active peer cooperation without demanding infrastructure support such as transcoding servers. Second, as online transcoding is computationally intensive while the various devices used by participating clients may have limited computing power and related resources (e.g., battery, bandwidth), an additional overlay, called metadata overlay, is constructed to instantly share the intermediate transcoding result of a transcoding procedure with other transcoding nodes to minimize the total computing overhead in the system. The experimental results collected within a realistically simulated testbed show that by consuming 6% extra bandwidth, PAT could save up to 58% CPU cycles for online transcoding.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2022229153",
    "type": "article"
  },
  {
    "title": "Using simulcast and scalable video coding to efficiently control channel switching delay in mobile tv broadcast networks",
    "doi": "https://doi.org/10.1145/1925101.1925103",
    "publication_date": "2011-02-01",
    "publication_year": 2011,
    "authors": "Cheng-Hsin Hsu; Mohamed Hefeeda",
    "corresponding_authors": "",
    "abstract": "Many mobile TV standards dictate using energy saving schemes to increase the viewing time on mobile devices, since mobile receivers are battery powered. The most common scheme for saving energy is to make the base station broadcast the video data of a TV channel in bursts with a bit-rate much higher than the encoding rate of the video stream, which enables mobile devices to turn off their radio frequency circuits when not receiving bursts. Broadcasting TV channels in bursts, however, increases channel switching delay. The switching delay is important, because long and variable switching delays are annoying to users and may turn them away from the mobile TV service. In this article, we first analyze the burst broadcasting scheme currently used in many deployed mobile TV networks, and we show that it is not efficient in terms of controlling the channel switching delay. We then propose new schemes to guarantee that a given maximum switching delay is not exceeded and that the energy consumption of mobile devices is minimized. We prove the correctness of the proposed schemes and analytically analyze the achieved energy saving. We also use scalable video coding to generalize the proposed schemes in order to support mobile devices with heterogeneous resources. We implement the proposed schemes in a mobile TV testbed to show their practicality and to validate our theoretical analysis. The experimental results show that the proposed schemes: (i) significantly increase the energy saving achieved on mobile devices: up to 95% saving is observed, and (ii) support both homogeneous and heterogeneous mobile devices.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2025929411",
    "type": "article"
  },
  {
    "title": "Placing Videos on a Semantic Hierarchy for Search Result Navigation",
    "doi": "https://doi.org/10.1145/2578394",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "Song Tan; Yu‐Gang Jiang; Chong‐Wah Ngo",
    "corresponding_authors": "",
    "abstract": "Organizing video search results in a list view is widely adopted by current commercial search engines, which cannot support efficient browsing for complex search topics that have multiple semantic facets. In this article, we propose to organize video search results in a highly structured way. Specifically, videos are placed on a semantic hierarchy that accurately organizes various facets of a given search topic. To pick the most suitable videos for each node of the hierarchy, we define and utilize three important criteria: relevance, uniqueness, and diversity. Extensive evaluations on a large YouTube video dataset demonstrate the effectiveness of our approach.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2048558554",
    "type": "article"
  },
  {
    "title": "ShotVis",
    "doi": "https://doi.org/10.1145/2808210",
    "publication_date": "2015-10-21",
    "publication_year": 2015,
    "authors": "Biao Zhu; Hongxin Zhang; Wei Chen; Feng Xia; Ross Maciejewski",
    "corresponding_authors": "",
    "abstract": "While visualization has been widely used as a data presentation tool in both desktop and mobile devices, the rapid visualization of information from images is still underexplored. In this work, we present a smartphone image acquisition and visualization approach for text-based data. Our prototype, ShotVis, takes images of text captured from mobile devices and extracts information for visualization. First, scattered characters in the text are processed and interactively reformulated to be stored as structured data (i.e., tables of numbers, lists of words, sentences). From there, ShotVis allows users to interactively bind visual forms to the underlying data and produce visualizations of the selected forms through touch-based interactions. In this manner, ShotVis can quickly summarize text from images into word clouds, scatterplots, and various other visualizations all through a simple click of the camera. In this way, ShotVis facilitates the interactive exploration of text data captured via cameras in smartphone devices. To demonstrate our prototype, several case studies are presented along with one user study to demonstrate the effectiveness of our approach.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2067953788",
    "type": "article"
  },
  {
    "title": "Region- and action-aware virtual world clients",
    "doi": "https://doi.org/10.1145/2422956.2422962",
    "publication_date": "2013-02-01",
    "publication_year": 2013,
    "authors": "Yichuan Wang; Ting‐An Lin; Cheng-Hsin Hsu; Xin Liu",
    "corresponding_authors": "",
    "abstract": "We propose region- and action-aware virtual world clients. To develop such clients, we present a parameterized network traffic model, based on a large collection of Second Life traces gathered by us. Our methodology is also applicable to virtual worlds other than Second Life. With the traffic model, various optimization criteria can be adopted, including visual quality, response time, and energy consumption. We use energy consumption as the show case, and demonstrate via trace-driven simulations that, compared to two existing schemes, a mobile client can save up to 36% and 41% communication energy by selectively turning on its WiFi network interface.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2073841221",
    "type": "article"
  },
  {
    "title": "Providing hierarchical lookup service for P2P-VoD systems",
    "doi": "https://doi.org/10.1145/2089085.2089092",
    "publication_date": "2012-02-01",
    "publication_year": 2012,
    "authors": "Tieying Zhang; Xueqi Cheng; Jianming Lv; Zhenhua Li; Weisong Shi",
    "corresponding_authors": "",
    "abstract": "Supporting random jump in P2P-VoD systems requires efficient lookup for the “best” suppliers, where “best” means the suppliers should meet two requirements: content match and network quality match . Most studies use a DHT-based method to provide content lookup; however, these methods are neither able to meet the network quality requirements nor suitable for VoD streaming due to the large overhead. In this paper, we propose Mediacoop, a novel hierarchical lookup scheme combining both content and quality match to provide random jumps for P2P-VoD systems. It exploits the play position to efficiently locate the candidate suppliers with required data (content match), and performs refined lookup within the candidates to meet quality match. Theoretical analysis and simulation results show that Mediacoop is able to achieve lower jump latency and control overhead than the typical DHT-based method. Moreover, we implement Mediacoop in a BitTorrent-like P2P-VoD system called CoolFish and make optimizations for such “total cache” applications. The implementation and evaluation in CoolFish show that Mediacoop is able to improve user experiences, especially the jump latency, which verifies the practicability of our design.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2091428676",
    "type": "article"
  },
  {
    "title": "Similarity Search over the Cloud Based on Image Descriptors' Dimensions Value Cardinalities",
    "doi": "https://doi.org/10.1145/2716315",
    "publication_date": "2015-06-02",
    "publication_year": 2015,
    "authors": "Stefanos Antaris; Dimitrios Rafailidis",
    "corresponding_authors": "",
    "abstract": "In recognition that in modern applications billions of images are stored into distributed databases in different logical or physical locations, we propose a similarity search strategy over the cloud based on the dimensions value cardinalities of image descriptors. Our strategy has low preprocessing requirements by dividing the computational cost of the preprocessing steps into several nodes over the cloud and locating the descriptors with similar dimensions value cardinalities logically close. New images are inserted into the distributed databases over the cloud efficiently, by supporting dynamical update in real-time. The proposed insertion algorithm has low computational complexity, depending exclusively on the dimensionality of descriptors and a small subset of descriptors with similar dimensions value cardinalities. Finally, an efficient query processing algorithm is proposed, where the dimensions of image descriptors are prioritized in the searching strategy, assuming that dimensions of high value cardinalities have more discriminative power than the dimensions of low ones. The computation effort of the query processing algorithm is divided into several nodes over the cloud infrastructure. In our experiments with seven publicly available datasets of image descriptors, we show that the proposed similarity search strategy outperforms competitive methods of single node, parallel and cloud-based architectures, in terms of preprocessing cost, search time and accuracy.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2260808725",
    "type": "article"
  },
  {
    "title": "Opinion Question Answering by Sentiment Clip Localization",
    "doi": "https://doi.org/10.1145/2818711",
    "publication_date": "2015-11-02",
    "publication_year": 2015,
    "authors": "Lei Pang; Chong‐Wah Ngo",
    "corresponding_authors": "",
    "abstract": "This article considers multimedia question answering beyond factoid and how-to questions. We are interested in searching videos for answering opinion-oriented questions that are controversial and hotly debated. Examples of questions include “Should Edward Snowden be pardoned?” and “Obamacare—unconstitutional or not?”. These questions often invoke emotional response, either positively or negatively, hence are likely to be better answered by videos than texts, due to the vivid display of emotional signals visible through facial expression and speaking tone. Nevertheless, a potential answer of duration 60s may be embedded in a video of 10min, resulting in degraded user experience compared to reading the answer in text only. Furthermore, a text-based opinion question may be short and vague, while the video answers could be verbal, less structured grammatically, and noisy because of errors in speech transcription. Direct matching of words or syntactic analysis of sentence structure, such as adopted by factoid and how-to question-answering, is unlikely to find video answers. The first problem, the answer localization, is addressed by audiovisual analysis of the emotional signals in videos for locating video segments likely expressing opinions. The second problem, questions and answers matching, is tackled by a deep architecture that nonlinearly matches text words in questions and speeches in videos. Experiments are conducted on eight controversial topics based on questions crawled from Yahoo! Answers and Internet videos from YouTube.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2269165052",
    "type": "article"
  },
  {
    "title": "Hyperspectral Image Reconstruction Using Multi-scale Fusion Learning",
    "doi": "https://doi.org/10.1145/3477396",
    "publication_date": "2022-01-27",
    "publication_year": 2022,
    "authors": "Xian‐Hua Han; Yinqiang Zheng; Yen‐Wei Chen",
    "corresponding_authors": "",
    "abstract": "Hyperspectral imaging is a promising imaging modality that simultaneously captures several images for the same scene on narrow spectral bands, and it has made considerable progress in different fields, such as agriculture, astronomy, and surveillance. However, the existing hyperspectral (HS) cameras sacrifice the spatial resolution for providing the detail spectral distribution of the imaged scene, which leads to low-resolution (LR) HS images compared with the common red-green-blue (RGB) images. Generating a high-resolution HS (HR-HS) image via fusing an observed LR-HS image with the corresponding HR-RGB image has been actively studied. Existing methods for this fusing task generally investigate hand-crafted priors to model the inherent structure of the latent HR-HS image, and they employ optimization approaches for solving it. However, proper priors for different scenes can possibly be diverse, and to figure it out for a specific scene is difficult. This study investigates a deep convolutional neural network (DCNN)-based method for automatic prior learning, and it proposes a novel fusion DCNN model with multi-scale spatial and spectral learning for effectively merging an HR-RGB and LR-HS images. Specifically, we construct an U-shape network architecture for gradually reducing the feature sizes of the HR-RGB image (Encoder-side) and increasing the feature sizes of the LR-HS image (Decoder-side), and we fuse the HR spatial structure and the detail spectral attribute in multiple scales for tackling the large resolution difference in spatial domain of the observed HR-RGB and LR-HS images. Then, we employ multi-level cost functions for the proposed multi-scale learning network to alleviate the gradient vanish problem in long-propagation procedure. In addition, for further improving the reconstruction performance of the HR-HS image, we refine the predicted HR-HS image using an alternating back-projection method for minimizing the reconstruction errors of the observed LR-HS and HR-RGB images. Experiments on three benchmark HS image datasets demonstrate the superiority of the proposed method in both quantitative values and visual qualities.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4210722684",
    "type": "article"
  },
  {
    "title": "An Empirical Method for Causal Inference of Constructs for QoE in Haptic–Audiovisual Communications",
    "doi": "https://doi.org/10.1145/3473986",
    "publication_date": "2022-01-27",
    "publication_year": 2022,
    "authors": "Shuji Tasaka",
    "corresponding_authors": "Shuji Tasaka",
    "abstract": "This article proposes an empirical method for inferring causal directions in multidimensional Quality of Experience (QoE) in multimedia communications, noting that causation in QoE is perceptual. As an example for modeling framework, we pick up a Bayesian structural equation model (SEM) previously built for haptic audiovisual interactive communications. The SEM includes three constructs (Audiovisual quality, Haptic quality, and User experience quality), which are latent variables each representing a group of observed variables with similar characteristics. In the SEM, the causal directions of the constructs were assumed by resorting to the domain knowledge. This article aims at proposing a methodology for inferring causal directions of constructs in general by verifying the assumption of causal directions in the SEM through their observed data alone. For that purpose, we compare six SEMs each with different causal directions of constructs, one of which is the one from the domain knowledge. The proposed method is based on QoE prediction by a Bayesian approach with Markov chain Monte Carlo (MCMC) simulation. Setting observed scores to the indicators of exogenous variables in each SEM, we predict values of all the indicators; we then assess the mean square error (MSE) between predicted QoE and mean opinion score (MOS) from observed scores and estimate the probability distribution of the MSE in each SEM. We can compare any two SEMs to find which is more plausible by examining the probability that the MSE for one SEM is smaller than or equal to that for the other. These probabilities are estimated with MCMC simulation. The method indicates that the causal directions thus inferred for the haptic audiovisual interactive communications adequately support the original ones drawn from the domain knowledge. In addition, we demonstrate that QoE can behave like the “impact-perceive-adapt” model of the effects of delayed haptic and visual feedback on performance in a collaborative environment, which Jay, Glencross, and Hubbold proposed in 2007, and that it accompanies reversal of plausible causal directions like a flip–flop.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4210747167",
    "type": "article"
  },
  {
    "title": "Robust Unsupervised Gaze Calibration Using Conversation and Manipulation Attention Priors",
    "doi": "https://doi.org/10.1145/3472622",
    "publication_date": "2022-01-27",
    "publication_year": 2022,
    "authors": "Rémy Siegfried; Jean‐Marc Odobez",
    "corresponding_authors": "",
    "abstract": "Gaze estimation is a difficult task, even for humans. However, as humans, we are good at understanding a situation and exploiting it to guess the expected visual focus of attention of people, and we usually use this information to retrieve people’s gaze. In this article, we propose to leverage such situation-based expectation about people’s visual focus of attention to collect weakly labeled gaze samples and perform person-specific calibration of gaze estimators in an unsupervised and online way. In this context, our contributions are the following: (i) we show how task contextual attention priors can be used to gather reference gaze samples, which is a cumbersome process otherwise; (ii) we propose a robust estimation framework to exploit these weak labels for the estimation of the calibration model parameters; and (iii) we demonstrate the applicability of this approach on two human-human and human-robot interaction settings, namely conversation and manipulation. Experiments on three datasets validate our approach, providing insights on the priors effectiveness and on the impact of different calibration models, particularly the usefulness of taking head pose into account.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4210811389",
    "type": "article"
  },
  {
    "title": "SLAM for Indoor Parking: A Comprehensive Benchmark Dataset and a Tightly Coupled Semantic Framework",
    "doi": "https://doi.org/10.1145/3510856",
    "publication_date": "2022-02-18",
    "publication_year": 2022,
    "authors": "Xuan Shao; Ying Shen; Lin Zhang; Shengjie Zhao; Dandan Zhu; Yicong Zhou",
    "corresponding_authors": "",
    "abstract": "For the task of autonomous indoor parking, various Visual-Inertial Simultaneous Localization And Mapping (SLAM) systems are expected to achieve comparable results with the benefit of complementary effects of visual cameras and the Inertial Measurement Units. To compare these competing SLAM systems, it is necessary to have publicly available datasets, offering an objective way to demonstrate the pros/cons of each SLAM system. However, the availability of such high-quality datasets is surprisingly limited due to the profound challenge of the groundtruth trajectory acquisition in the Global Positioning Satellite denied indoor parking environments. In this article, we establish BeVIS, a large-scale Be nchmark dataset with V isual (front-view), I nertial and S urround-view sensors for evaluating the performance of SLAM systems developed for autonomous indoor parking, which is the first of its kind where both the raw data and the groundtruth trajectories are available. In BeVIS, the groundtruth trajectories are obtained by tracking artificial landmarks scattered in the indoor parking environments, whose coordinates are recorded in a surveying manner with a high-precision Electronic Total Station. Moreover, the groundtruth trajectories are comprehensively evaluated in terms of two respects, the reprojection error and the pose volatility, respectively. Apart from BeVIS, we propose a novel tightly coupled semantic SLAM framework, namely VIS SLAM -2, leveraging V isual (front-view), I nertial, and S urround-view sensor modalities, specially for the task of autonomous indoor parking. It is the first work attempting to provide a general form to model various semantic objects on the ground. Experiments on BeVIS demonstrate the effectiveness of the proposed VIS SLAM -2. Our benchmark dataset BeVIS is publicly available at https://shaoxuan92.github.io/BeVIS .",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4212828534",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2000486",
    "publication_date": "2011-08-01",
    "publication_year": 2011,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Many distributed multimedia applications rely on video analysis algorithms for automated video and image processing. Little is known, however, about the minimum video quality required to ensure an accurate performance of these algorithms. In an attempt ...",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4239595057",
    "type": "paratext"
  },
  {
    "title": "Spontaneous Facial Behavior Analysis Using Deep Transformer-based Framework for Child–computer Interaction",
    "doi": "https://doi.org/10.1145/3539577",
    "publication_date": "2022-05-26",
    "publication_year": 2022,
    "authors": "Abdul Qayyum; Imran Razzak; M. Tanveer; Moona Mazher",
    "corresponding_authors": "",
    "abstract": "A fascinating challenge in robotics-human interaction is imitating the emotion recognition capability of humans to robots with the aim to make human-robotics interaction natural, genuine and intuitive. To achieve the natural interaction in affective robots, human-machine interfaces, and autonomous vehicles, understanding our attitudes and opinions is very important, and it provides a practical and feasible path to realize the connection between machine and human. Multimodal interface that includes voice along with facial expression can manifest a large range of nuanced emotions compared to purely textual interfaces and provide a great value to improve the intelligence level of effective communication. Interfaces that fail to manifest or ignore user emotions may significantly impact the performance and risk being perceived as cold, socially inept, untrustworthy, and incompetent. To equip a child well for life, we need to help our children identify their feelings, manage them well, and express their needs in healthy, respectful, and direct ways. Early identification of emotional deficits can help to prevent low social functioning in children. In this work, we analyzed the child’s spontaneous behavior using multimodal facial expression and voice signal presenting multimodal transformer-based last feature fusion for facial behavior analysis in children to extract contextualized representations from RGB video sequence and Hematoxylin and eosin video sequence and then using these representations followed by pairwise concatenations of contextualized representations using cross-feature fusion technique to predict users emotions. To validate the performance of the proposed framework, we have performed experiments with the different pairwise concatenations of contextualized representations that showed significantly better performance than state-of-the-art method. Besides, we perform t-distributed stochastic neighbor embedding visualization to visualize the discriminative feature in lower dimension space and probability density estimation to visualize the prediction capability of our proposed model.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4281571560",
    "type": "article"
  },
  {
    "title": "PMAL: A Proxy Model Active Learning Approach for Vision Based Industrial Applications",
    "doi": "https://doi.org/10.1145/3534932",
    "publication_date": "2022-06-21",
    "publication_year": 2022,
    "authors": "Abbas Khan; Ijaz Ul Haq; Tanveer Hussain; Khan Muhammad; Mohammad Hijji; Muhammad Sajjad; Victor Hugo C. de Albuquerque; Sung Wook Baik",
    "corresponding_authors": "",
    "abstract": "Deep Learning models’ performance strongly correlate with availability of annotated data; however, massive data labeling is laborious, expensive, and error-prone when performed by human experts. Active Learning (AL) effectively handles this challenge by selecting the uncertain samples from unlabeled data collection, but the existing AL approaches involve repetitive human feedback for labeling uncertain samples, thus rendering these techniques infeasible to be deployed in industry related real-world applications. In the proposed Proxy Model based Active Learning technique (PMAL) , this issue is addressed by replacing human oracle with a deep learning model, where human expertise is reduced to label only two small subsets of data for training proxy model and initializing the AL loop. In the PMAL technique, firstly, proxy model is trained with a small subset of labeled data, which subsequently acts as an oracle for annotating uncertain samples. Secondly, active model's training, uncertain samples extraction via uncertainty sampling, and annotation through proxy model is carried out until predefined iterations to achieve higher accuracy and labeled data. Finally, the active model is evaluated using testing data to verify the effectiveness of our technique for practical applications. The correct annotations by the proxy model are ensured by employing the potentials of explainable artificial intelligence. Similarly, emerging vision transformer is used as an active model to achieve maximum accuracy. Experimental results reveal that the proposed method outperforms the state-of-the-art in terms of minimum labeled data usage and improves the accuracy with 2.2%, 2.6%, and 1.35% on Caltech-101, Caltech-256, and CIFAR-10 datasets, respectively. Since the proposed technique offers a highly reasonable solution to exploit huge multimedia data, it can be widely used in different evolutionary industrial domains.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4283209249",
    "type": "article"
  },
  {
    "title": "Generating Virtual Wire Sculptural Art from 3D Models",
    "doi": "https://doi.org/10.1145/3475798",
    "publication_date": "2022-02-16",
    "publication_year": 2022,
    "authors": "Chih-Kuo Yeh; Thi-Ngoc-Hanh Le; Zhi-Ying Hou; Tong‐Yee Lee",
    "corresponding_authors": "",
    "abstract": "Wire sculptures are objects sculpted by the use of wires. In this article, we propose practical methods to create 3D virtual wire sculptural art from a given 3D model. In contrast, most of the previous 3D wire art results are reconstructed from input 2D wire art images. Artists usually tend to design their wire art with a single wire if possible. If not possible, they try to create it with the least number of wires. To follow this general design trend, our proposed method generates 3D virtual wire art with the minimum number of continuous wire lines. To achieve this goal, we first adopt a greedy approach to extract important edges of a given 3D model. These extracted important edges become the basis for the subsequent lines to roughly represent the shape of the input model. Then, we connect them with the minimum number of continuous wire lines by the order obtained by optimally solving a traveling salesman problem with some constraints. Finally, we smooth the obtained 3D wires to simulate the real 3D wire results by artists. In addition, we also provide a user interface to control the winding of wires by their design preference. Finally, we experimentally show our 3D virtual wire results and evaluate these created results. As a result, the proposed method is computed effectively and interactively, and results are appealing and comparable to real 3D wire art work.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4283393667",
    "type": "article"
  },
  {
    "title": "Quantum Fourier Convolutional Network",
    "doi": "https://doi.org/10.1145/3514249",
    "publication_date": "2022-07-05",
    "publication_year": 2022,
    "authors": "Feihong Shen; Jun Liu",
    "corresponding_authors": "",
    "abstract": "The neural network and quantum computing are both significant and appealing fields, with their interactive disciplines promising for large-scale computing tasks that are untackled by conventional computers. However, both developments are restricted by the scope of the hardware development. Nevertheless, many neural network algorithms had been proposed before GPUs became powerful enough for running very deep models. Similarly, quantum algorithms can also be proposed as knowledge reserve before real quantum computers are easily accessible. Specifically, taking advantage of both the neural networks and quantum computation and designing quantum deep neural networks (QDNNs) for acceleration on the Noisy Intermediate-Scale Quantum (NISQ) processors is also an important research problem. As one of the most widely used neural network architectures, convolutional neural network (CNN) remains to be accelerated by quantum mechanisms, with only a few attempts having been demonstrated. In this article, we propose a new hybrid quantum-classical circuit, namely, Quantum Fourier Convolutional Network (QFCN). Our model achieves exponential speedup compared with classical CNN theoretically and improves over the existing best result of quantum CNN. We demonstrate the potential of this architecture by applying it on different deep learning tasks, including traffic prediction and image classification.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4283813083",
    "type": "article"
  },
  {
    "title": "Improved Random Grid-based Cheating Prevention Visual Cryptography Using Latin Square",
    "doi": "https://doi.org/10.1145/3550275",
    "publication_date": "2022-08-03",
    "publication_year": 2022,
    "authors": "Sophie C. C. Sun; Yongkang Zhao; Fang‐Wei Fu; Yawei Ren",
    "corresponding_authors": "",
    "abstract": "Visual cryptography scheme is a method of encrypting secret image into n noiselike shares. The secret image can be reconstructed by stacking adequate shares. In the past two decades, many schemes have been proposed to realize the cheating prevention visual cryptography scheme (CPVCS). Significantly, Ren et al. [ 9 ] first introduced the idea of CPVCS with the help of Latin square. Inspired by their work, in this article, a new reliable scheme is proposed. More precisely, to facilitate the certification process, we embed meaningful characters into the randomly chosen authentication patterns in each divided blocks. Furthermore, we fix the security vulnerability in the stacked results of share S g and verification Ver g , where 1≤ g ≤ n . Since the improved scheme encrypts the secret image by utilizing random grids, the generated shares have no pixel expansion. Finally, theoretical analysis and experimental results are conducted to evaluate the efficiency and security of the proposed scheme.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4289544556",
    "type": "article"
  },
  {
    "title": "GHOSM: Graph-based Hybrid Outline and Skeleton Modelling for Shape Recognition",
    "doi": "https://doi.org/10.1145/3554922",
    "publication_date": "2022-08-04",
    "publication_year": 2022,
    "authors": "Basheer Alwaely; Charith Abhayaratne",
    "corresponding_authors": "",
    "abstract": "An efficient and accurate shape detection model plays a major role in many research areas. With the emergence of more complex shapes in real-life applications, shape recognition models need to capture the structure with more effective features to achieve high accuracy rates for shape recognition. This article presents a new method for 2D/3D shape recognition based on graph spectral domain handcrafted features, which are formulated by exploiting both an outline and a skeleton shape through the global outline and internal details. A fully connected graph is generated over the shape outline to capture the global outline representation while a hierarchically clustered graph with adaptive connectivity is formed on the skeleton to capture the structural descriptions of the shape. We demonstrate the ability of the Fiedler vector to provide the graph partitioning of the skeleton graph. The performance evaluation demonstrates the efficiency of the proposed method compared to state-of-the-art studies with increments of 4.09%, 2.2%, and 14.02% for 2D static hand gestures, 2D shapes, and 3D shapes, respectively.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4289837329",
    "type": "article"
  },
  {
    "title": "Distill-DBDGAN: Knowledge Distillation and Adversarial Learning Framework for Defocus Blur Detection",
    "doi": "https://doi.org/10.1145/3557897",
    "publication_date": "2022-08-22",
    "publication_year": 2022,
    "authors": "Sankaraganesh Jonna; Moushumi Medhi; Rajiv R. Sahay",
    "corresponding_authors": "",
    "abstract": "Defocus blur detection (DBD) aims to segment the blurred regions from a given image affected by defocus blur. It is a crucial pre-processing step for various computer vision tasks. With the increasing popularity of small mobile devices, there is a need for a computationally efficient method to detect defocus blur accurately. We propose an efficient defocus blur detection method that estimates the probability of each pixel being focused or blurred in resource-constraint devices. Despite remarkable advances made by the recent deep learning-based methods, they still suffer from several challenges such as background clutter, scale sensitivity, indistinguishable low-contrast focused regions from out-of-focus blur, and especially high computational cost and memory requirement. To address the first three challenges, we develop a novel deep network that efficiently detects blur map from the input blurred image. Specifically, we integrate multi-scale features in the deep network to resolve the scale ambiguities and simultaneously modeled the non-local structural correlations in the high-level blur features. To handle the last two issues, we eventually frame our DBD algorithm to perform knowledge distillation by transferring information from the larger teacher network to a compact student network. All the networks are adversarially trained in an end-to-end manner to enforce higher order consistencies between the output and the target distributions. Experimental results demonstrate the state-of-the-art performance of the larger teacher network, while our proposed lightweight DBD model imitates the output of the teacher network without significant loss in accuracy. The codes, pre-trained model weights, and the results will be made publicly available.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4292596089",
    "type": "article"
  },
  {
    "title": "Robust Long-Term Tracking via Localizing Occluders",
    "doi": "https://doi.org/10.1145/3557896",
    "publication_date": "2022-08-18",
    "publication_year": 2022,
    "authors": "Binfei Chu; Yiting Lin; Bineng Zhong; Zhenjun Tang; Xianxian Li; Jing Wang",
    "corresponding_authors": "",
    "abstract": "Occlusion is known as one of the most challenging factors in long-term tracking because of its unpredictable shape. Existing works devoted into the design of loss functions, training strategies or model architectures, which are considered to have not directly touched the key point. Alternatively, we came up with a direct and natural idea that is discarding things that covers the target. We propose a novel occluder-aware representation learning framework to develop this idea. First, we design a local occluders detection module (LODM) to localize the occluders, which works on the principle that discriminates the non-noumenal part from a target based on the general knowledge of this category. An extra dataset and a clustering strategy is proposed to support this general knowledge. Second, we devise a feature reconstruction module to guide the occluder-aware representation learning. With the help of above methods, our localizing occluders tracker, called LOTracker, can learn an occluder-free representation and promote the performance that tracks with occlusion scenarios. Extensive experimental results show that our LOTracker achieves a state-of-the-art performance in multiple benchmarks such as LaSOT, VOTLT2018, VOTLT2019, and OxUvALT.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4293067233",
    "type": "article"
  },
  {
    "title": "Is it Violin or Viola? Classifying the Instruments’ Music Pieces using Descriptive Statistics",
    "doi": "https://doi.org/10.1145/3563218",
    "publication_date": "2022-09-14",
    "publication_year": 2022,
    "authors": "Chong Hong Tan; KokSheik Wong; Vishnu Monn Baskaran; Kiki Adhinugraha; David Taniar",
    "corresponding_authors": "",
    "abstract": "Classifying music pieces based on their instrument sounds is pivotal for analysis and application purposes. Given its importance, techniques using machine learning have been proposed to classify violin and viola music pieces. The violin and viola are two different instruments with three overlapping strings of the same notes, and it is challenging for ordinary people or even musicians to distinguish the sound produced by these instruments. However, the classification of musical instrument pieces was barely performed by prior research. To solve this problem, we propose a technique using descriptive statistics to reliably distinguish between violin and viola music pieces. Likewise, a similar technique on the basis of histogram is introduced alongside the main descriptive statistics approach. These approaches are derived based on the nature of the instruments’ strings and the range of their pieces. We also solve the problem in the current literature which divide the audio into segments for processing instead of managing the whole song. Thereby, we compile a dataset of recordings that comprises of violin and viola solo pieces from the Baroque, Classical, Romantic, and Modern eras. Experiment results suggest that our approach achieves high accuracy on solo pieces as compared to other methods with 0.97 accuracy on Baroque pieces.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4295678390",
    "type": "article"
  },
  {
    "title": "Feedback Chain Network For Hippocampus Segmentation",
    "doi": "https://doi.org/10.1145/3571744",
    "publication_date": "2022-11-18",
    "publication_year": 2022,
    "authors": "Heyu Huang; Runmin Cong; Lianhe Yang; Ling Du; Cong Wang; Sam Kwong",
    "corresponding_authors": "",
    "abstract": "The hippocampus plays a vital role in the diagnosis and treatment of many neurological disorders. Recent years, deep learning technology has made great progress in the field of medical image segmentation, and the performance of related tasks has been constantly refreshed. In this paper, we focus on the hippocampus segmentation task and propose a novel hierarchical feedback chain network. The feedback chain structure unit learns deeper and wider feature representation of each encoder layer through the hierarchical feature aggregation feedback chains, and achieves feature selection and feedback through the feature handover attention module. Then, we embed a global pyramid attention unit between the feature encoder and the decoder to further modify the encoder features, including the pair-wise pyramid attention module for achieving adjacent attention interaction and the global context modeling module for capturing the long-range knowledge. The proposed approach achieves state-of-the-art performance on three publicly available datasets, compared with existing hippocampus segmentation approaches. The code and results can be found from the link of https://github.com/easymoneysniper183/sematic_seg.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4309468608",
    "type": "article"
  },
  {
    "title": "ProposalVLAD with Proposal-Intra Exploring for Temporal Action Proposal Generation",
    "doi": "https://doi.org/10.1145/3571747",
    "publication_date": "2022-11-24",
    "publication_year": 2022,
    "authors": "Kai Xing; Tao Li; Xuanhan Wang",
    "corresponding_authors": "",
    "abstract": "Temporal action proposal generation aims to localize temporal segments of human activities in videos. Current boundary-based proposal generation methods can generate proposals with precise boundary but often suffer from the inferior quality of confidence scores used for proposal retrieving. In this article, we propose an effective and end-to-end action proposal generation method, named ProposalVLAD, with Proposal-Intra Exploring Network (PVPI-Net). We first propose a ProposalVLAD module to dynamically generate global features of the entire video, then we combine the global features and proposal local features to generate the final feature representations for all candidate proposals. Then, we design a novel Proposal-Intra Loss function (PI-Loss) to generate more reliable proposal confidence scores. Extensive experiments on large-scale and challenging datasets demonstrate the effectiveness of our proposed method. Experimental results show that our PVPI-Net achieves significant improvements on two benchmark datasets (i.e., THUMOS’14 and ActivityNet-1.3) and sets new records for temporal action detection task.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4309857266",
    "type": "article"
  },
  {
    "title": "Multi-Source Knowledge Reasoning Graph Network for Multi-Modal Commonsense Inference",
    "doi": "https://doi.org/10.1145/3573201",
    "publication_date": "2022-12-01",
    "publication_year": 2022,
    "authors": "Xuan Ma; Xiaoshan Yang; Changsheng Xu",
    "corresponding_authors": "",
    "abstract": "As a crucial part of natural language processing, event-centered commonsense inference task has attracted increasing attention. With a given observed event, the intention and reaction of the people involved in the event are required to be inferred with artificial intelligent algorithms. To solve this problem, sequence-to-sequence methods are widely studied, where the event is first encoded into a specific representation and then decoded to generate the results. However, all the existing methods learn the event representation only with the textual information, while the visual information is ignored, which is actually helpful for the commonsense reference. In this article, we first define a new task of multi-modal commonsense reference with both textual and visual information. A new event-centered multi-modal dataset is also provided. Then we propose a multi-source knowledge reasoning graph network to solve this task, where three kinds of relational knowledge are considered. Multi-modal correlations are learned to get the event’s multi-modal representation from a global perspective. Intra-event object relations are explored to capture the fine-grained event feature with an object graph. Inter-event semantic relations are also explored through the external knowledge to understand the semantic associations among events with an event graph. We conduct extensive experiments on the new dataset, and the results show the effectiveness of our method.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4311057502",
    "type": "article"
  },
  {
    "title": "Occlusion Boundary Prediction and Transformer Based Depth-Map Refinement From Single Image",
    "doi": "https://doi.org/10.1145/3640015",
    "publication_date": "2024-01-10",
    "publication_year": 2024,
    "authors": "Praful Hambarde; Gourav Wadhwa; Santosh Kumar Vipparthi; Subrahmanyam Murala; Abhinav Dhall",
    "corresponding_authors": "",
    "abstract": "Due to the numerous applications of boundary maps and occlusion orientation maps (ORI-maps) in high-level vision problems, accurate estimation of these maps is a crucial task. The existing deep networks employ a single-stream network to estimate the relation between boundary map and ORI-map estimation. However, these networks fail to explore significant individual information separately. To resolve this problem, in this paper, we propose a novel two-stream generative adversarial network (GAN) for boundary map and ORI-map estimation, named OBP-GAN. The proposed OBP-GAN consists of two streams known as BP-GAN and OR-GAN. The BP-GAN estimates the boundary map, and the OR-GAN predicts the ORI-map. The boundary and ORI-map can also be useful cues for the task of depth-map refinement from single images. Therefore, in this work, we propose a transformer-based depth-map refinement network (TRANSDMR-GAN) for refining the depth estimated from monocular images using boundary and ORI-map. We conducted extensive analyses on indoor and outdoor datasets to validate our proposed OBP-GAN and TRANSDMR-GAN. The extensive experimental analysis and ablation study demonstrate the ability of the proposed OBP-GAN to generate state-of-the-art occlusion boundary maps. Furthermore, we show that the proposed network, TRANSDMR-GAN, can generate an edge-enhanced depth map without degrading the accuracy of the initial depth map.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4390702507",
    "type": "article"
  },
  {
    "title": "DashReStreamer: Framework for Creation of Impaired Video Clips under Realistic Network Conditions",
    "doi": "https://doi.org/10.1145/3640016",
    "publication_date": "2024-01-09",
    "publication_year": 2024,
    "authors": "Kerim Hodžić; Mirsad Ćosović; Saša Mrdović; Jason J. Quinlan; Darijo Raca",
    "corresponding_authors": "",
    "abstract": "The continuous rise of multimedia entertainment has led to an increased demand for delivering outstanding user experience of multimedia content. However, modelling user-perceived Quality of Experience (QoE) is a challenging task, resulting in efforts for better understanding and measurement of user-perceived QoE. To evaluate user QoE, subjective quality assessment, where people watch and grade videos, and objective quality assessment in which videos are graded using one or many objective metrics are conducted. While there is a plethora of video databases available for subjective and objective video quality assessment, these videos are artificially infused with various temporal and spatial impairments. Videos being assessed are artificially distorted with startup delay, bitrate changes, and stalls due to rebuffering events. To conduct a more credible quality assessment, a reproduction of original user experiences while watching different types of streams on different types and quality of networks is needed. To aid current efforts in bridging the gap between the mapping of objective video QoE metrics to user experience, we developed DashReStreamer, an open-source framework for re-creating adaptively streamed video in real networks. The framework takes inputs in the form of video logs captured by the client in a non-regulated setting, along with an.mpd file or a YouTube URL. The ultimate result is a video sequence that encompasses all the data extracted from the video log. DashReStreamer also calculates popular video quality metrics like PSNR, SSIM, MS-SSIM and VMAF. Finally, DashReStreamer allows creating impaired video sequences from the popular streaming platform, YouTube. As a demonstration of framework usage we created a database of 332 realistic video clips, based on video logs collected from real mobile and wireless networks. Every video clip is supplemented with bandwidth trace and video logs used in its creation and also with objective metrics calculation reports. In addition to dataset, we performed subjective evaluation of video content, assessing its effect on overall user QoE. We believe that this dataset and framework will allow the research community to better understand the impacts of video QoE dynamics.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4390768785",
    "type": "article"
  },
  {
    "title": "Bi-directional Block Encoding for Reversible Data Hiding over Encrypted Images",
    "doi": "https://doi.org/10.1145/3638771",
    "publication_date": "2024-01-12",
    "publication_year": 2024,
    "authors": "Xingyu Liu; Zhongyun Hua; Shuang Yi; Yushu Zhang; Yicong Zhou",
    "corresponding_authors": "",
    "abstract": "Reversible data hiding over encrypted images (RDH-EI) technology is a viable solution for privacy-preserving cloud storage, as it enables the reversible embedding of additional data into images while maintaining image confidentiality. Since the data hiders, e.g., cloud servers, are willing to embed as much data as possible for storage, management, or other processing purposes, a large embedding capacity is desirable in an RDH-EI scheme. In this article, we introduce a novel bi-directional block encoding (BDBE) method, which, for the first time, encodes the distances of values in a binary sequence from both ends. This approach allows for encoding images with smaller sizes compared to traditional and state-of-the-art encoding methods. Leveraging the BDBE technique, we propose a high-capacity RDH-EI scheme. In this scheme, the content owner initially predicts the image pixels and then employs BDBE to encode the prediction errors, creating space for data embedding. The resulting encoded data are subsequently encrypted using a secure stream cipher, such as the Advanced Encryption Standard, before being transmitted to a data hider. The data hider can embed confidential information within the encrypted image for the purposes of storage, management, or other processing. Upon receiving the data, an authorized receiver can accurately recover the original image and the embedded data without any loss. Experimental results demonstrate that our RDH-EI scheme achieves a significantly larger embedding capacity compared to several state-of-the-art schemes.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4390829365",
    "type": "article"
  },
  {
    "title": "MCFNet: Multi-Attentional Class Feature Augmentation Network for Real-Time Scene Parsing",
    "doi": "https://doi.org/10.1145/3639053",
    "publication_date": "2024-01-15",
    "publication_year": 2024,
    "authors": "Wang Xi-zhong; Rui Liu; Xin Yang; Qiang Zhang; Dongsheng Zhou",
    "corresponding_authors": "",
    "abstract": "For real-time scene parsing tasks, capturing multi-scale semantic features and performing effective feature fusion is crucial. However, many existing solutions ignore stripe-shaped things like poles, traffic lights and are so computationally expensive that cannot meet the high real-time requirements. This article presents a novel model, the Multi-Attention Class Feature Augmentation Network (MCFNet) to address this challenge. MCFNet is designed to capture long-range dependencies across different scales with low computational cost and to perform a weighted fusion of feature maps. It features the BAM (Strip Matrix Based Attention Module) for extracting strip objects in images. The BAM module replaces the conventional self-attention method using square matrices with strip matrices, which allows it to focus more on strip objects while reducing computation. Additionally, MCFNet has a parallel branch that focuses on global information based on self-attention to avoid wasting computation. The two branches are merged to enhance the performance of traditional self-attention modules. Experimental results on two mainstream datasets demonstrate the effectiveness of MCFNet. On the Camvid and Cityscapes test sets, MCFNet achieved 207.5 FPS/73.5% mIoU and 136.1 FPS/71.63% mIoU, respectively. The experiments show that MCFNet outperforms other models on the Camvid dataset and can significantly improve the performance of real-time scene parsing tasks.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4390878797",
    "type": "article"
  },
  {
    "title": "Semantic-Consistency-guided Learning on Deep Features for Unsupervised Salient Object Detection",
    "doi": "https://doi.org/10.1145/3640816",
    "publication_date": "2024-01-22",
    "publication_year": 2024,
    "authors": "Ying Ying Zhang; Shuo Zhang; Ming Hui",
    "corresponding_authors": "",
    "abstract": "Unsupervised salient object detection is an important task in many real-world scenarios where pixel-wise label information is of scarce availability. Despite its significance, this problem remains rarely explored, with a few works that consider unsupervised salient object detection methods based on the fused graph from the sum fusion of multiple deep feature similarity matrices. However, these methods ignore the interrelation of the low-level feature similarity matrices and the high-level semantic similarity matrice, which degrades the quality of the fused graph. In this article, we propose a semantic-consistency-guided multi-graph fusion learning algorithm for unsupervised saliency detection, where the consistency and inconsistency between multiple low-level feature similarity matrices and the high-level semantic similarity matrice are explored to promote the robustness and quality of the fused graph. In the first stage, a semantic-consistency-guided multi-graph fusion learning method is proposed to exploit consistency and inconsistency of multiple low-level deep features and the high-level semantic feature. The semantic-consistency-guided similarity matrices are computed for preliminary saliency ranking. In the following saliency refinement stage, the semantic-enhanced similarity matrices are built by the cross diffusion to fuse the multiple low-level deep features and the high semantic deep feature. Based on the semantic-enhanced similarity matrices, the refinement saliency maps are calculated in a semantic-enhanced cellular automata manner. Furthermore, the final ensemble stage of the large margin semi-supervised classification views the preliminary ranking results and refinement results as features, adopts the large margin graphs for saliency ensemble. Extensive evaluations over four benchmark datasets show that the proposed unsupervised method performs favorably against the state-of-the-art approaches and is competitive with some supervised deep learning-based methods.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4391104468",
    "type": "article"
  },
  {
    "title": "Synthesized Image Training Techniques: On Improving Model Performance using Confusion.",
    "doi": "https://doi.org/10.1145/3641856",
    "publication_date": "2024-01-24",
    "publication_year": 2024,
    "authors": "Azeez Idris; Mohammed Khaleel; Wallapak Tavanapong; Piet C. de Groen",
    "corresponding_authors": "",
    "abstract": "The performance of supervised deep learning image classifiers has significantly improved with large, labeled datasets and increased computing power. However, obtaining large, labeled image datasets in areas like medicine is expensive. This study seeks to improve model performance on limited labeled datasets by reducing confusion. We observed that misclassification (or confusion) between classes is usually more prevalent between specific classes. Thus, we developed synthesized image training techniques (SIT2), a novel confusion-based training framework that identifies pairs of classes with high confusion and synthesizes not-sure images from these pairs. The not-sure images are utilized in three new training strategies as follows. (1) The not-sure training strategy pretrains a model using not-sure images and the original training images. (2) The sure-or-not strategy pretrains with synthesized sure or not-sure images. (3) The multi-label strategy pretrains with synthesized images but predicts the original class(es) of the synthesized images. Finally, the pretrained model is finetuned on the original dataset. An extensive evaluation was conducted on five medical and non-medical datasets. Several improvements are statistically significant, which shows the promising future of our confusion-based training framework.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4391169593",
    "type": "article"
  },
  {
    "title": "Immersive Multimedia Service Caching in Edge Cloud with Renewable Energy",
    "doi": "https://doi.org/10.1145/3643818",
    "publication_date": "2024-01-31",
    "publication_year": 2024,
    "authors": "M. Shamim Hossain; Yixue Hao; Long Hu; Jia Liu; Gang Wei; Chen Min",
    "corresponding_authors": "",
    "abstract": "Immersive service caching, based on the intelligent edge cloud, can meet delay-sensitive service requirements. Although numerous service caching solutions for edge clouds have been designed, they have not been well explored. Moreover, to the best of our knowledge, there is no work to consider the immersive service caching scheme under the supply of renewable energy. In this article, we investigate the service caching problem under the renewable energy supply to minimize service latency while making full use of renewable energy. Specifically, we formulate the service caching and renewable energy harvesting problem, which considers the dynamic renewable energy, unknown service requests, and limited capacity of the edge cloud. To solve this problem, we propose an effective algorithm, called OSCRE. Our algorithm first uses Lyapunov optimization to convert the time-average problem into time-independence optimization and thus realizes optimal renewable energy harvesting. Then, it realizes the service caching scheme using data-driven combinatorial multi-armed bandit learning. The simulation results show that the OSCRE scheme can save service latency while making sufficient use of renewable energy.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4391387999",
    "type": "article"
  },
  {
    "title": "SWRM: Similarity Window Reweighting and Margin for Long-Tailed Recognition",
    "doi": "https://doi.org/10.1145/3643816",
    "publication_date": "2024-02-07",
    "publication_year": 2024,
    "authors": "Qiong Chen; T.S. Huang; Qingfa Liu",
    "corresponding_authors": "",
    "abstract": "Real-world data usually obeys a long-tailed distribution, where a few classes have higher number of samples compared to the other classes. Recent studies have been proposed to alleviate the extreme data imbalance from different perspectives. In this article, we experimentally find that due to the easily confusing visual features between some head- and tail classes, the cross-entropy model is prone to misclassify tail samples to similar head classes. Therefore, to alleviate the influence of the confusion on model performance and improve the classification of tail classes, we propose a Similarity Window Reweighting and Margin (SWRM) algorithm, where the SWRM consists of Similarity Window Reweighting (SWR) and Similarity Window Margin (SWM) algorithms. For the confusable head- and tail classes, SWR assigns larger weights to tail classes and smaller weights to head classes. Therefore, the model can enlarge the importance of tail classes and effectively improve their classification. Moreover, SWR considers the difference in label frequency and the impact of category similarity simultaneously, so that the weight coefficients are more reasonable and efficacious. SWM generates adaptive margins that are proportional to the ratio of the classifier’s weight norm, thus promoting the learning of tail classifier with small weight norm. Our SWRM effectively eliminates the confusion between head- and tail classes and alleviates the misclassification issues. Extensive experiments on three long-tailed datasets, i.e., CIFAR100-LT, ImageNet-LT, and Places-LT, verify our proposed method’s effectiveness and superiority over comparative methods.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4391592341",
    "type": "article"
  },
  {
    "title": "DATRA-MIV: Decoder-Adaptive Tiling and Rate Allocation for MPEG Immersive Video",
    "doi": "https://doi.org/10.1145/3648371",
    "publication_date": "2024-02-19",
    "publication_year": 2024,
    "authors": "Jong-Beom Jeong; Soonbin Lee; Eun‐Seok Ryu",
    "corresponding_authors": "",
    "abstract": "The emerging immersive video coding standard moving picture experts group (MPEG) immersive video (MIV), which is ongoing standardization by MPEG-Immersive (MPEG-I) group, enables six degrees of freedom in a virtual reality environment that represents both natural and computer-generated scenes using multi-view video compression. The MIV eliminates the redundancy between multi-view videos and merges the residuals into multiple pictures, called an atlas. Thus, bitstreams with encoded atlases are generated and corresponding number of decoders are needed, which is challenging for the lightweight device with a single decoder. This article proposes a decoder-adaptive tiling and rate allocation method for MIV to overcome the challenge. First, the proposed method divides atlases into subpictures considering two aspects: (i) subpicture bitstream extracting and merging into one bitstream to use a single decoder and (ii) separation of each source view from the atlases for rate allocation. Second, the atlases are encoded by versatile video coding (VVC), using an extractable subpicture to divide the atlases into subpictures. Third, each subpicture bitstream is extracted, and asymmetric quality allocation for each subpictures is conducted by considering the residuals in the subpicture. Fourth, mixed-quality subpictures were merged by using the proposed bitstream merger. Fifth, the merged bitstream is decoded by using a single decoder. Finally, the viewing area of the user is synthesized by using the reconstructed atlases. Experimental results with the VVC test model (VTM) show that the proposed method achieves a 21.37% Bjøntegaard delta rate saving for immersive video peak signal-to-noise ratio and a 26.76% decoding runtime saving compared to the VTM anchor configuration. Moreover, it supports bitstreams for multiple decoders and single decoder without re-encoding, transcoding, or a substantial increase of the server-side storage.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4391947697",
    "type": "article"
  },
  {
    "title": "Optimizing Camera Motion with MCTS and Target Motion Modeling in Multi-Target Active Object Tracking",
    "doi": "https://doi.org/10.1145/3648369",
    "publication_date": "2024-02-21",
    "publication_year": 2024,
    "authors": "Zheng Chen; Jian Zhao; Mingyu Yang; Wengang Zhou; Houqiang Li",
    "corresponding_authors": "",
    "abstract": "In this work, we are dedicated to multi-target active object tracking (AOT), where the goal is to achieve continuous tracking of targets through real-time control of camera. This form of active camera control can be applied to unmanned aerial vehicles (UAV), intelligent robots, and sports events. Our work is conducted in an environment featuring multiple cameras and targets, where our goal is to maximize target coverage. Contrasting with previous research, our work introduces additional degrees of freedom for the cameras, allowing them not only to rotate but also to move along boundary lines. In addition, we model the motion of target to predict the future position of the target in environment. With target’s future position, we use Monte Carlo Tree Search (MCTS) method to find the optimal action of camera. Since the action space is large, we propose to leverage the action selection from multi-agent reinforcement learning (MARL) network to prune the search tree of Monte Carlo Tree Search method, so as to find the optimal action more efficiently. We establish a multi-target 2D environment to simulate several sports games, and experimental results demonstrate that our method can effectively improve the target coverage. The code is available at: http://github.com/HopeChanger/ActiveObjectTracking .",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4391996773",
    "type": "article"
  },
  {
    "title": "Scene Graph Lossless Compression with Adaptive Prediction for Objects and Relations",
    "doi": "https://doi.org/10.1145/3649503",
    "publication_date": "2024-02-26",
    "publication_year": 2024,
    "authors": "Weiyao Lin; Yufeng Zhang; Wenrui Dai; Huabin Liu; John See; Hongkai Xiong",
    "corresponding_authors": "",
    "abstract": "The scene graph is a novel data structure describing objects and their pairwise relationship within image scenes. As the size of scene graphs in vision and multimedia applications increases, the need for lossless storage and transmission of such data becomes more critical. However, the compression of scene graphs is less studied because of the complicated data structures involved and complex distributions. Existing solutions usually involve general-purpose compressors or graph structure compression methods, which are weak at reducing the redundancy in scene graph data. This article introduces a novel lossless compression framework with adaptive predictors for the joint compression of objects and relations in scene graph data. The proposed framework comprises a unified prior extractor and specialized element predictors to adapt to different data elements. Furthermore, to exploit the context information within and between graph elements, Graph Context Convolution is proposed to support different graph context modeling schemes for different graph elements. Finally, an overarching framework incorporates the learned distribution model to predict numerical data under complicated conditional constraints. Experiments conducted on labeled or generated scene graphs demonstrate the effectiveness of the proposed framework for scene graph lossless compression.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4392157966",
    "type": "article"
  },
  {
    "title": "GreenABR+: Generalized Energy-Aware Adaptive Bitrate Streaming",
    "doi": "https://doi.org/10.1145/3649898",
    "publication_date": "2024-08-16",
    "publication_year": 2024,
    "authors": "Bekir Turkkan; Ting Dai; Adithya Raman; Tevfik Kosar; Changyou Chen; Muhammed Bulut; Jarosław Żola; Daby Sow",
    "corresponding_authors": "",
    "abstract": "Adaptive bitrate (ABR) algorithms play a critical role in video streaming by making optimal bitrate decisions in dynamically changing network conditions to provide a high quality of experience (QoE) for users. However, most existing ABRs suffer from limitations such as predefined rules and incorrect assumptions about streaming parameters. They often prioritize higher bitrates and ignore the corresponding energy footprint, resulting in increased energy consumption, especially for mobile device users. Additionally, most ABR algorithms do not consider perceived quality, leading to suboptimal user experience. This article proposes a novel ABR scheme called GreenABR+, which utilizes deep reinforcement learning to optimize energy consumption during video streaming while maintaining high user QoE. Unlike existing rule-based ABR algorithms, GreenABR+ makes no assumptions about video settings or the streaming environment. GreenABR+ model works on different video representation sets and can adapt to dynamically changing conditions in a wide range of network scenarios. Our experiments demonstrate that GreenABR+ outperforms state-of-the-art ABR algorithms by saving up to 57% in streaming energy consumption and 57% in data consumption while providing up to 25% more perceptual QoE due to up to 87% less rebuffering time and near-zero capacity violations. The generalization and dynamic adaptability make GreenABR+ a flexible solution for energy-efficient ABR optimization.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4392472215",
    "type": "article"
  },
  {
    "title": "WaRENet: A Novel Urban Waterlogging Risk Evaluation Network",
    "doi": "https://doi.org/10.1145/3651163",
    "publication_date": "2024-03-05",
    "publication_year": 2024,
    "authors": "Xiaoya Yu; Kejun Wu; You Yang; Qiong Liu",
    "corresponding_authors": "",
    "abstract": "In this article, we propose a novel urban waterlogging risk evaluation network (WaRENet) to evaluate the risk of waterlogging. The WaRENet distinguishes whether an urban image involves waterlogging by classification module, and estimates the waterlogging risk levels by multi-class reference objects detection module (MCROD). First, in the waterlogging scene classification, ResNet combined with Se-block is used to identify the waterlogging scene, and lightweight gradient-weighted class activation mapping (Grad-CAM) is also integrated to roughly locate overall waterlogging areas with low computational burden. Second, in the MCROD module, we detect reference objects, e.g., cars and persons in waterlogging scenes. The positional relationship between water depths and reference objects serves as risk indicators for accurately evaluating waterlogging risk. Specifically, we incorporate switchable atrous convolution (SAC) into YOLOv5 to solve occlusions and varying scales problems in complex waterlogging scenes. Moreover, we construct a large-scale urban waterlogging dataset called UrbanWaterloggingRiskDataset (UWRDataset) with 6,351 images for waterlogging scene classification and 3,217 images for reference objects detection. Experimental results on the dataset show that our WaRENet outperforms all comparison methods. The waterlogging scene classification module achieves accuracy of 95.99%. The MCROD module obtains mAP of 54.9%, while maintaining a high processing speed of 70.04 fps.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4392472224",
    "type": "article"
  },
  {
    "title": "Review and Analysis of RGBT Single Object Tracking Methods: A Fusion Perspective",
    "doi": "https://doi.org/10.1145/3651308",
    "publication_date": "2024-03-07",
    "publication_year": 2024,
    "authors": "Zhihao Zhang; Jun Wang; Shengjie Li; Lei Jin; Hao Wu; Jian Zhao; Bo Zhang",
    "corresponding_authors": "",
    "abstract": "Visual tracking is a fundamental task in computer vision with significant practical applications in various domains, including surveillance, security, robotics, and human-computer interaction. However, it may face limitations in visible light data, such as low-light environments, occlusion, and camouflage, which can significantly reduce its accuracy. To cope with these challenges, researchers have explored the potential of combining the visible and infrared modalities to improve tracking performance. By leveraging the complementary strengths of visible and infrared data, RGB-infrared fusion tracking has emerged as a promising approach to address these limitations and improve tracking accuracy in challenging scenarios. In this article, we present a review on RGB-infrared fusion tracking. Specifically, we categorize existing RGBT tracking methods into four categories based on their underlying architectures, feature representations, and fusion strategies, namely feature decoupling based method, feature selecting based method, collaborative graph tracking method, and traditional fusion method. Furthermore, we provide a critical analysis of their strengths, limitations, representative methods, and future research directions. To further demonstrate the advantages and disadvantages of these methods, we present a review of publicly available RGBT tracking datasets and analyze the main results on public datasets. Moreover, we discuss some limitations in RGBT tracking at present and provide some opportunities and future directions for RGBT visual tracking, such as dataset diversity, unsupervised and weakly supervised applications. In conclusion, our survey aims to serve as a useful resource for researchers and practitioners interested in the emerging field of RGBT tracking, and to promote further progress and innovation in this area.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4392551959",
    "type": "article"
  },
  {
    "title": "Backdoor Two-Stream Video Models on Federated Learning",
    "doi": "https://doi.org/10.1145/3651307",
    "publication_date": "2024-03-07",
    "publication_year": 2024,
    "authors": "Jing Zhao; Hongwei Yang; Hui He; Jie Peng; Weizhe Zhang; Jiangqun Ni; Arun Kumar Sangaiah; Aniello Castiglione",
    "corresponding_authors": "",
    "abstract": "Video models on federated learning (FL) enable continual learning of the involved models for video tasks on end-user devices while protecting the privacy of end-user data. As a result, the security issues on FL, e.g., the backdoor attacks on FL and their defense have increasingly become the domains of extensive research in recent years. The backdoor attacks on FL are a class of poisoning attacks, in which an attacker, as one of the training participants, submits poisoned parameters and thus injects the backdoor into the global model after aggregation. Existing backdoor attacks against videos based on FL only poison RGB frames, which makes it that the attack could be easily mitigated by two-stream model neutralization. Therefore, it is a big challenge to manipulate the most advanced two-stream video model with a high success rate by poisoning only a small proportion of training data in the framework of FL. In this paper, a new backdoor attack scheme incorporating the rich spatial and temporal structures of video data is proposed, which injects the backdoor triggers into both the optical flow and RGB frames of video data through multiple rounds of model aggregations. In addition, the adversarial attack is utilized on the RGB frames to further boost the robustness of the attacks. Extensive experiments on real-world datasets verify that our methods outperform the state-of-the-art backdoor attacks and show better performance in terms of stealthiness and persistence.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4392552355",
    "type": "article"
  },
  {
    "title": "Suitable and Style-Consistent Multi-Texture Recommendation for Cartoon Illustrations",
    "doi": "https://doi.org/10.1145/3652518",
    "publication_date": "2024-03-12",
    "publication_year": 2024,
    "authors": "Huisi Wu; Zhaoze Wang; Yifan Li; Xueting Liu; Tong‐Yee Lee",
    "corresponding_authors": "",
    "abstract": "Texture plays an important role in cartoon illustrations to display object materials and enrich visual experiences. Unfortunately, manually designing and drawing an appropriate texture is not easy even for proficient artists, let alone novice or amateur people. While there exist tons of textures on the Internet, it is not easy to pick an appropriate one using traditional text-based search engines. Although several texture pickers have been proposed, they still require the users to browse the textures by themselves, which is still labor-intensive and time-consuming. In this article, an automatic texture recommendation system is proposed for recommending multiple textures to replace a set of user-specified regions in a cartoon illustration with visually pleasant look. Two measurements, the suitability measurement and the style-consistency measurement, are proposed to make sure that the recommended textures are suitable for cartoon illustration and at the same time mutually consistent in style. The suitability is measured based on the synthesizability, cartoonity, and region fitness of textures. The style-consistency is predicted using a learning-based solution since it is subjective to judge whether two textures are consistent in style. An optimization problem is formulated and solved via the genetic algorithm. Our method is validated on various cartoon illustrations, and convincing results are obtained.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4392715643",
    "type": "article"
  },
  {
    "title": "New Metrics and Dataset for Biological Development Video Generation",
    "doi": "https://doi.org/10.1145/3653456",
    "publication_date": "2024-03-20",
    "publication_year": 2024,
    "authors": "Pedro Celard; Eva Iglesias; José Manuel Sorribes-Fdez; L. Borrajo; Adrián Seara Vieira",
    "corresponding_authors": "",
    "abstract": "Image generative models have advanced in many areas to produce synthetic images of high resolution and detail. This success has enabled its use in the biomedical field, paving the way for the generation of videos showing the biological evolution of its content. Despite the power of generative video models, their use has not yet extended to time-based development, focusing almost exclusively on generating motion in space. This situation is largely due to the lack of specific datasets and metrics to measure the individual quality of videos, particularly when there is no ground truth available for comparison. We propose a new dataset, called GoldenDOT , which tracks the evolution of apples cut in parallel over 10 days, allowing to observe their progress over time while remaining static. In addition, four new metrics are proposed that provide different analyses of the generated videos as a whole and individually. In this article, the proposed dataset and measures are used to study three state-of-the-art video generative models and their feasibility for video generation with biological development: Temporal GAN version 2 (TGANv2), Low-Dimensional Video Discriminator Generative Adversarial Network (LDVDGAN), and Video Diffusion Model (VDM). Among them, the TGANv2 model has managed to obtain the best results in most metrics, including those already known in the state of the art, demonstrating the viability of the new proposed metrics and their congruence with these standard measures.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4393002365",
    "type": "article"
  },
  {
    "title": "Double Reference Guided Interactive 2D and 3D Caricature Generation",
    "doi": "https://doi.org/10.1145/3655624",
    "publication_date": "2024-04-01",
    "publication_year": 2024,
    "authors": "Xin Huang; Dong Liang; Hongrui Cai; Yunfeng Bai; Juyong Zhang; Feng Tian; Jinyuan Jia",
    "corresponding_authors": "",
    "abstract": "In this paper, we propose the first geometry and texture (double) referenced interactive 2D and 3D caricature generating and editing method. The main challenge of caricature generation lies in the fact that it not only exaggerates the facial geometry but also refreshes the facial texture. We address this challenge by utilizing the semantic segmentation maps as an intermediary domain, removing the influence of photo texture while preserving the person-specific geometry features. Specifically, our proposed method consists of two main components: 3D-CariNet and CariMaskGAN. 3D-CariNet uses sketches or caricatures to exaggerate the input photo into several types of 3D caricatures. To generate a CariMask, we geometrically exaggerate the photos using the projection of exaggerated 3D landmarks, after which CariMask is converted into a caricature by CariMaskGAN. In this step, users can edit and adjust the geometry of caricatures freely. Moreover, we propose a semantic detail preprocessing approach that considerably increases the details of generated caricatures and allows modification of hair strands, wrinkles, and beards. By rendering high-quality 2D caricatures as textures, we produce 3D caricatures with a variety of texture styles. Extensive experimental results have demonstrated that our method can produce higher-quality caricatures as well as support interactive modification with ease.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4393378880",
    "type": "article"
  },
  {
    "title": "Multi-Domain Image-to-Image Translation with Cross-Granularity Contrastive Learning",
    "doi": "https://doi.org/10.1145/3656048",
    "publication_date": "2024-05-16",
    "publication_year": 2024,
    "authors": "Huiyuan Fu; Jin Liu; Ting Yu; Xin Wang; Huadóng Ma",
    "corresponding_authors": "",
    "abstract": "The objective of multi-domain image-to-image translation is to learn the mapping from a source domain to a target domain in multiple image domains while preserving the content representation of the source domain. Despite the importance and recent efforts, most previous studies disregard the large style discrepancy between images and instances in various domains, or fail to capture instance details and boundaries properly, resulting in poor translation results for rich scenes. To address these problems, we present an effective architecture for multi-domain image-to-image translation that only requires one generator. Specifically, we provide detailed procedures for capturing the features of instances throughout the learning process, as well as learning the relationship between the style of the global image and that of a local instance in the image by enforcing the cross-granularity consistency. In order to capture local details within the content space, we employ a dual contrastive learning strategy that operates at both the instance and patch levels. Extensive studies on different multi-domain image-to-image translation datasets reveal that our proposed method outperforms state-of-the-art approaches.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4393949409",
    "type": "article"
  },
  {
    "title": "MEDUSA: A Dynamic Codec Switching Approach in HTTP Adaptive Streaming",
    "doi": "https://doi.org/10.1145/3656175",
    "publication_date": "2024-04-05",
    "publication_year": 2024,
    "authors": "Daniele Lorenzi; Farzad Tashtarian; Hermann Hellwagner; Christian Timmerer",
    "corresponding_authors": "",
    "abstract": "HTTP Adaptive Streaming (HAS) solutions utilize various Adaptive BitRate (ABR) algorithms to dynamically select appropriate video representations, aiming at adapting to fluctuations in network bandwidth. However, current ABR implementations have a limitation in that they are designed to function with one set of video representations, i.e., the bitrate ladder, which differ in bitrate and resolution, but are encoded with the same video codec. When multiple codecs are available, current ABR algorithms select one of them prior to the streaming session and stick to it throughout the entire streaming session. Although newer codecs are generally preferred over older ones, their compression efficiencies differ depending on the content’s complexity , which varies over time. Therefore, it is necessary to select the appropriate codec for each video segment to reduce the requested data while delivering the highest possible quality. In this article, we first provide a practical example where we compare compression efficiencies of different codecs on a set of video sequences. Based on this analysis, we formulate the optimization problem of selecting the appropriate codec for each user and video segment (on a per-segment basis in the outmost case), refining the selection of the ABR algorithms by exploiting key metrics, such as the perceived segment quality and size. Subsequently, to address the scalability issues of this centralized model, we introduce a novel distributed plug-in ABR algorithm for Video on Demand (VoD) applications called MEDUSA to be deployed on top of existing ABR algorithms. MEDUSA enhances the user’s Quality of Experience (QoE) by utilizing a multi-objective function that considers the quality and size of video segments when selecting the next representation. Using quality information and segment size from the modified Media Presentation Description (MPD) , MEDUSA utilizes buffer occupancy to prioritize quality or size by assigning specific weights in the objective function. To show the impact of MEDUSA, we compare the proposed plug-in approach on top of state-of-the-art techniques with their original implementations and analyze the results for different network traces, video content, and buffer capacities. According to the experimental findings, MEDUSA shows the ability to improve QoE for various test videos and scenarios. The results reveal an impressive improvement in the QoE score of up to 42% according to the ITU-T P.1203 model (mode 0). Additionally, MEDUSA can reduce the transmitted data volume by up to more than 40% achieving a QoE similar to the techniques compared, reducing the burden on streaming service providers for delivery costs.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4393992235",
    "type": "article"
  },
  {
    "title": "A Self-Defense Copyright Protection Scheme for NFT Image Art Based on Information Embedding",
    "doi": "https://doi.org/10.1145/3652519",
    "publication_date": "2024-04-06",
    "publication_year": 2024,
    "authors": "Fan Wang; Zhangjie Fu; Xiang Zhang",
    "corresponding_authors": "",
    "abstract": "Non-convertible tokens (NFTs) have become a fundamental part of the metaverse ecosystem due to its uniqueness and immutability. However, existing copyright protection schemes of NFT image art relied on the NFTs itself minted by third-party platforms. A minted NFT image art only tracks and verifies the entire transaction process, but the legitimacy of the source and ownership of its mapped digital image art cannot be determined. The original author or authorized publisher lack an active defense mechanism to prove ownership of the digital image art mapped by the unauthorized NFT. Therefore, we propose a self-defense copyright protection scheme for NFT image art based on information embedding in this paper, called SDCP-IE. The original author or authorized publisher can embed the copyright information into the published digital image art without damaging its visual effect in advance. Different from the existing information embedding works, the proposed SDCP-IE can generally enhance the invisibility of copyright information with different embedding capacity. Furthermore, considering the scenario of copyright information being discovered or even destroyed by unauthorized parties, the designed SDCP-IE can efficiently generate enhanced digital image art to improve the security performance of embedded image, thus resisting the detection of multiple known and unknown detection models simultaneously. The experimental results have also shown that the PSNR values of enhanced embedded image are all over 57db on three datasets BOSSBase, BOWS2 and ALASKA#2. Moreover, compared with existing information embedding works, the enhanced embedded images generated by SDCP-IE reaches the best transferability performance on the advanced CNN-based detection models. When the target detector is the pre-trained SRNet at 0.4bpp, the test error rate of SDCP-IE at 0.4bpp on the evaluated detection model YeNet reaches 53.38%, which is 4.92%, 28.62% and 7.05% higher than that of the UTGAN, SPS-ENH and Xie-Model, respectively.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4394010896",
    "type": "article"
  },
  {
    "title": "High Fidelity Makeup via 2D and 3D Identity Preservation Net",
    "doi": "https://doi.org/10.1145/3656475",
    "publication_date": "2024-04-08",
    "publication_year": 2024,
    "authors": "Jinliang Liu; Zhedong Zheng; Zongxin Yang; Yi Yang",
    "corresponding_authors": "",
    "abstract": "In this article, we address the challenging makeup transfer task, aiming to transfer makeup from a reference image to a source image while preserving facial geometry and background consistency. Existing deep neural network-based methods have shown promising results in aligning facial parts and transferring makeup textures. However, they often neglect the facial geometry of the source image, leading to two adverse effects: (1) alterations in geometrically relevant facial features, causing face flattening and loss of personality, and (2) difficulties in maintaining background consistency, as networks cannot clearly determine the face-background boundary. To jointly tackle these issues, we propose the High Fidelity Makeup via two-dimensional (2D) and 3D Identity Preservation Network (IP23-Net), to the best of our knowledge, a novel framework that leverages facial geometry information to generate more realistic results. Our method comprises a 3D Shape Identity Encoder, which extracts identity and 3D shape features. We incorporate a 3D face reconstruction model to ensure the three-dimensional effect of face makeup, thereby preserving the characters’ depth and natural appearance. To preserve background consistency, our Background Correction Decoder automatically predicts an adaptive mask for the source image, distinguishing the foreground and background. In addition to popular benchmarks, we introduce a new large-scale High Resolution Synthetic Makeup Dataset containing 335,230 diverse high-resolution face images to evaluate our method’s generalization ability. Experiments demonstrate that IP23-Net achieves high-fidelity makeup transfer while effectively preserving background consistency. The code will be made publicly available.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4394569843",
    "type": "article"
  },
  {
    "title": "SigFormer: Sparse Signal-Guided Transformer for Multi-Modal Action Segmentation",
    "doi": "https://doi.org/10.1145/3657296",
    "publication_date": "2024-04-10",
    "publication_year": 2024,
    "authors": "Qi Liu; Xinchen Liu; K Liu; Xiaoyan Gu; Wu Liu",
    "corresponding_authors": "",
    "abstract": "Multi-modal human action segmentation is a critical and challenging task with a wide range of applications. Nowadays, the majority of approaches concentrate on the fusion of dense signals (i.e., RGB, optical flow, and depth maps). However, the potential contributions of sparse IoT sensor signals, which can be crucial for achieving accurate recognition, have not been fully explored. To make up for this, we introduce a S parse s i gnal- g uided Transformer ( SigFormer ) to combine both dense and sparse signals. We employ mask attention to fuse localized features by constraining cross-attention within the regions where sparse signals are valid. However, since sparse signals are discrete, they lack sufficient information about the temporal action boundaries. Therefore, in SigFormer, we propose to emphasize the boundary information at two stages to alleviate this problem. In the first feature extraction stage, we introduce an intermediate bottleneck module to jointly learn both category and boundary features of each dense modality through the inner loss functions. After the fusion of dense modalities and sparse signals, we then devise a two-branch architecture that explicitly models the interrelationship between action category and temporal boundary. Experimental results demonstrate that SigFormer outperforms the state-of-the-art approaches on a multi-modal action segmentation dataset from real industrial environments, reaching an outstanding F1 score of 0.958. The codes and pre-trained models have been made available at https://github.com/LIUQI-creat/SigFormer .",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4394686885",
    "type": "article"
  },
  {
    "title": "High Efficiency Deep-learning Based Video Compression",
    "doi": "https://doi.org/10.1145/3661311",
    "publication_date": "2024-04-23",
    "publication_year": 2024,
    "authors": "Lv Tang; Xinfeng Zhang",
    "corresponding_authors": "",
    "abstract": "Although deep learning technique has achieved significant improvement on image compression, but its advantages are not fully explored in video compression, which leads to the performance of deep-learning-based video compression (DLVC) is obviously inferior to that of hybrid video coding framework. In this article, we proposed a novel network to improve the performance of DLVC from its most important modules, including Motion Process (MP), Residual Compression (RC), and Frame Reconstruction (FR). In MP, we design a split second-order attention and multi-scale feature extraction module to fully remove the warping artifacts from multi-scale feature space and pixel space, which can help reduce the distortion in the following process. In RC, we propose a channel selection mechanism to gradually drop redundant information while preserving informative channels for a better rate-distortion performance. Finally, in FR, we introduce a residual multi-scale recurrent network to improve the quality of the current reconstructed frame by progressively exploiting temporal context information between it and its several previous reconstructed frames. Extensive experiments are conducted on the three widely used video compression datasets (HEVC, UVG, and MCL-JVC), and the performance demonstrates the superiority of our proposed approach over the state-of-the-art methods.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4395038433",
    "type": "article"
  },
  {
    "title": "Unbiased Semantic Representation Learning Based on Causal Disentanglement for Domain Generalization",
    "doi": "https://doi.org/10.1145/3659953",
    "publication_date": "2024-04-24",
    "publication_year": 2024,
    "authors": "Xuanyu Jin; Ni Li; Wanzeng Kong; Jiajia Tang; Bing Yang",
    "corresponding_authors": "",
    "abstract": "Domain generalization primarily mitigates domain shift among multiple source domains, generalizing the trained model to an unseen target domain. However, the spurious correlation usually caused by context prior (e.g., background) makes it challenging to get rid of the domain shift. Therefore, it is critical to model the intrinsic causal mechanism. The existing domain generalization methods only attend to disentangle the semantic and context-related features by modeling the causation between input and labels, which totally ignores the unidentifiable but important confounders. In this article, a Causal Disentangled Intervention Model (CDIM) is proposed for the first time, to the best of our knowledge, to construct confounders via causal intervention. Specifically, a generative model is employed to disentangle the semantic and context-related features. The contextual information of each domain from generative model can be considered as a confounder layer, and the center of all context-related features is utilized for fine-grained hierarchical modeling of confounders. Then the semantic and confounding features from each layer are combined to train an unbiased classifier, which exhibits both transferability and robustness across an unknown distribution domain. CDIM is evaluated on three widely recognized benchmark datasets, namely, Digit-DG, PACS, and NICO, through extensive ablation studies. The experimental results clearly demonstrate that the proposed model achieves state-of-the-art performance.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4395074201",
    "type": "article"
  },
  {
    "title": "Progressive Adapting and Pruning: Domain-Incremental Learning for Saliency Prediction",
    "doi": "https://doi.org/10.1145/3661312",
    "publication_date": "2024-04-24",
    "publication_year": 2024,
    "authors": "Kaihui Yang; Junwei Han; Guangyu Guo; Chaowei Fang; Yingzi Fan; Lechao Cheng; Dingwen Zhang",
    "corresponding_authors": "",
    "abstract": "Saliency prediction (SAP) plays a crucial role in simulating the visual perception function of human beings. In practical situations, humans can quickly grasp saliency extraction in new image domains. However, current SAP methods mainly concentrate on training models in single domains, which do not effectively handle diverse content and styles present in real-world images. As a result, it would be of great significance if SAP models could efficiently adjust to new image domains. To this end, this article aims to design SAP models that can imitate the incremental learning ability of human beings on multiple image domains and name domain-incremental saliency prediction (DISAP). To make a tradeoff between preventing the forgetting of historical domains and achieving high performance on new domains, we propose a progressively updated domain incremental encoder. This encoder consists of a domain-sharing branch and a domain-specific branch. The domain-sharing branch includes a feature selection mechanism to preserve crucial parameters after fine-tuning the model on each current domain. The remaining parameters are reserved to absorb knowledge from future domains. Furthermore, to capture the unique characteristics of each domain with relatively low computational overhead, we introduce a lightweight design to construct the domain-specific branch, enabling effective adaptation to new domains. Extensive experiments are conducted on multiple domain-incremental learning settings formed by four saliency prediction datasets, including Salicon, MIT1003, the art subset of CAT2000, and WebSal. The results demonstrate that our method outperforms existing methods significantly. The code is available at https://github.com/KaIi-github/DIL4SAP .",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4395074206",
    "type": "article"
  },
  {
    "title": "Integrated Sensing, Communication, and Computing for Cost-effective Multimodal Federated Perception",
    "doi": "https://doi.org/10.1145/3661313",
    "publication_date": "2024-04-26",
    "publication_year": 2024,
    "authors": "Ning Chen; Zhipeng Cheng; Xuwei Fan; Zhang Liu; Bangzhen Huang; Yifeng Zhao; Lianfen Huang; Xiaojiang Du; Mohsen Guizani",
    "corresponding_authors": "",
    "abstract": "Federated learning (FL) is a prominent paradigm of 6G edge intelligence (EI), which mitigates privacy breaches and high communication pressure caused by conventional centralized model training in the artificial intelligence of things (AIoT). The execution of multimodal federated perception (MFP) services comprises three sub-processes, including sensing-based multimodal data generation, communication-based model transmission, and computing-based model training, ultimately competitive on available underlying multi-domain physical resources such as time, frequency, and computing power. How to reasonably coordinate the multi-domain resources scheduling among sensing, communication, and computing, therefore, is vital to the MFP networks. To address the above issues, this article explores service-oriented resource management with integrated sensing, communication, and computing (ISCC). Specifically, employing the incentive mechanism of the MFP service market, the resources management problem is defined as a social welfare maximization problem, where the concept of “expanding resources” and “reducing costs” is used to enhance learning performance gain and reduce resource costs. Experimental results demonstrate the effectiveness and robustness of the proposed resource scheduling mechanisms.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4395675676",
    "type": "article"
  },
  {
    "title": "HKA: A Hierarchical Knowledge Alignment Framework for Multimodal Knowledge Graph Completion",
    "doi": "https://doi.org/10.1145/3664288",
    "publication_date": "2024-05-11",
    "publication_year": 2024,
    "authors": "Yunhui Xu; Youru Li; Muhao Xu; Zhenfeng Zhu; Yao Zhao",
    "corresponding_authors": "",
    "abstract": "Recent years have witnessed the successful application of knowledge graph techniques in structured data processing, while how to incorporate knowledge from visual and textual modalities into knowledge graphs has been given less attention. To better organize them, Multimodal Knowledge Graphs (MKGs), comprising the structural triplets of traditional Knowledge Graphs (KGs) together with entity-related multimodal data (e.g., images and texts), have been introduced consecutively. However, it is still a great challenge to explore MKGs due to their inherent incompleteness. Although most existing Multimodal Knowledge Graph Completion (MKGC) approaches can infer missing triplets based on available factual triplets and multimodal information, they almost ignore the modal conflicts and supervisory effect, failing to achieve a more comprehensive understanding of entities. To address these issues, we propose a novel H ierarchical K nowledge A lignment ( HKA ) framework for MKGC. Specifically, a macro-knowledge alignment module is proposed to capture global semantic relevance between modalities for dealing with modal conflicts in MKG. Furthermore, a micro-knowledge alignment module is also developed to reveal the local consistency information through inter- and intra-modality supervisory effects more effectively. By integrating different modal predictions, a final decision can be made. Experimental results on three benchmark MKGC tasks have demonstrated the effectiveness of the proposed HKA framework.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4396831703",
    "type": "article"
  },
  {
    "title": "Expanding-Window Zigzag Decodable Fountain Codes for Scalable Multimedia Transmission",
    "doi": "https://doi.org/10.1145/3664610",
    "publication_date": "2024-05-11",
    "publication_year": 2024,
    "authors": "Yuli Zhao; Yin Zhang; Francis C. M. Lau; Hai Yu; Zhiliang Zhu; Bin Zhang",
    "corresponding_authors": "",
    "abstract": "In this article, we present a coding method called expanding-window zigzag decodable fountain code with unequal error protection property (EWF-ZD UEP code) to achieve scalable multimedia transmission. The key idea of the EWF-ZD UEP code is to utilize bit-shift operation and expanding-window strategy to improve the decoding performance of the high-priority data without performance deterioration of the low-priority data. To provide more protection for the high-priority data, we precode the different importance level using LDPC codes of varying code rates. The generalized variable nodes of different importance levels are further grouped into several windows. Each window is associated with a selection probability and a bit-shift distribution. The combination of bit-shift and symbol exclusive-or operations is used to generate an encoded symbol. Theoretical and simulation results on input symbols of two importance levels reveal that the proposed EWF-ZD UEP code exhibits UEP property. With a small bit shift, the decoding delay for recovering high-priority input symbols is decreased without degrading the decoding performance of the low-priority input symbols. Moreover, according to the simulation results on scalable video coding, our scheme provides better basic video quality at a lower proportion of received symbols compared to three state-of-art UEP fountain codes.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4396833766",
    "type": "article"
  },
  {
    "title": "Illumination-Aware Low-Light Image Enhancement with Transformer and Auto-Knee Curve",
    "doi": "https://doi.org/10.1145/3664653",
    "publication_date": "2024-05-15",
    "publication_year": 2024,
    "authors": "Jinwang Pan; Xianming Liu; Yuanchao Bai; Deming Zhai; Junjun Jiang; Debin Zhao",
    "corresponding_authors": "",
    "abstract": "Images captured under low-light conditions suffer from several combined degradation factors, including low brightness, low contrast, noise, and color bias. Many learning-based techniques attempt to learn the low-to-clear mapping between low-light and normal-light images. However, they often fall short when applied to low-light images taken in wide-contrast scenes because uneven illumination brings illumination-varying noise and the enhanced images are easily over-saturated in highlight areas. In this article, we present a novel two-stage method to tackle the problem of uneven illumination distribution in low-light images. Under the assumption that noise varies with illumination, we design an illumination-aware transformer network for the first stage of image restoration. In this stage, we introduce the Illumination-aware Attention Block featured with Illumination-aware Multi-head Self-attention, which incorporates different scales of illumination features to guide the attention module, thereby enhancing the denoising and reconstruction capabilities of the restoration network. In the second stage, we innovatively introduce a cubic auto-knee curve transfer with a global parameter predictor to alleviate the over-exposure caused by uneven illumination. We also adopt a white balance correction module to address color bias issues at this stage. Extensive experiments on various benchmarks demonstrate the advantages of our method over state-of-the-art methods qualitatively and quantitatively.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4396927925",
    "type": "article"
  },
  {
    "title": "Continuous Space-Time Video Super-Resolution with Multi-stage Motion Information Reorganization",
    "doi": "https://doi.org/10.1145/3665646",
    "publication_date": "2024-05-21",
    "publication_year": 2024,
    "authors": "Yuantong Zhang; Daiqin Yang; Zhenzhong Chen; Wenpeng Ding",
    "corresponding_authors": "",
    "abstract": "Space-time video super-resolution (ST-VSR) aims to simultaneously expand a given source video to a higher frame rate and resolution. However, most existing schemes either consider fixed intermediate time and scale or fail to exploit long-range temporal information due to model design or inefficient motion estimation and compensation. To address these problems, we propose a continuous ST-VSR method to convert the given video to any frame rate and spatial resolution with Multi- s tage M otion information r eorganization (MsMr). To achieve time-arbitrary interpolation, we propose a forward warping guided frame synthesis module and an optical flow-guided context consistency loss to better approximate extreme motion and preserve similar structures among input and prediction frames. To realize continuous spatial upsampling, we design a memory-friendly cascading depth-to-space module. Meanwhile, with the sophisticated reorganization of optical flow, MsMr realizes more efficient motion estimation and motion compensation, making it possible to propagate information from long-range neighboring frames and achieve better reconstruction quality. Extensive experiments show that the proposed algorithm is flexible and performs better on various datasets than the state-of-the-art methods. The code will be available at https://github.com/hahazh/LD-STVSR .",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4398163233",
    "type": "article"
  },
  {
    "title": "SNIPPET: A Framework for Subjective Evaluation of Visual Explanations Applied to DeepFake Detection",
    "doi": "https://doi.org/10.1145/3665248",
    "publication_date": "2024-05-22",
    "publication_year": 2024,
    "authors": "Yuqing Yang; Boris Joukovsky; José Oramas; Tinne Tuytelaars; Nikos Deligiannis",
    "corresponding_authors": "",
    "abstract": "Explainable Artificial Intelligence (XAI) attempts to help humans understand machine learning decisions better and has been identified as a critical component toward increasing the trustworthiness of complex black-box systems, such as deep neural networks. In this article, we propose a generic and comprehensive framework named SNIPPET and create a user interface for the subjective evaluation of visual explanations, focusing on finding human-friendly explanations. SNIPPET considers human-centered evaluation tasks and incorporates the collection of human annotations. These annotations can serve as valuable feedback to validate the qualitative results obtained from the subjective assessment tasks. Moreover, we consider different user background categories during the evaluation process to ensure diverse perspectives and comprehensive evaluation. We demonstrate SNIPPET on a DeepFake face dataset. Distinguishing real from fake faces is a non-trivial task even for humans that depends on rather subtle features, making it a challenging use case. Using SNIPPET, we evaluate four popular XAI methods which provide visual explanations: Gradient-weighted Class Activation Mapping, Layer-wise Relevance Propagation, attention rollout, and Transformer Attribution. Based on our experimental results, we observe preference variations among different user categories. We find that most people are more favorable to the explanations of rollout. Moreover, when it comes to XAI-assisted understanding, those who have no or lack relevant background knowledge often consider that visual explanations are insufficient to help them understand. We open-source our framework for continued data collection and annotation at https://github.com/XAI-SubjEvaluation/SNIPPET .",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4398207561",
    "type": "article"
  },
  {
    "title": "Domain Adaptive Thermal Object Detection with Unbiased Granularity Alignment",
    "doi": "https://doi.org/10.1145/3665892",
    "publication_date": "2024-05-23",
    "publication_year": 2024,
    "authors": "Caijuan Shi; Yuanfan Zheng; Zhen Chen",
    "corresponding_authors": "",
    "abstract": "Domain Adaptive Object Detection (DAOD) alleviates the challenge of labor-intensive annotations by transferring semantic information from a labeled source domain to an unlabeled target domain. However, the DAOD suffers from biased discrimination and negative transfer in the thermal domain due to the inherent heterogeneity between the RGB and thermal images. To address the above issues, we propose the Unbiased Granularity Alignment (UGA) framework to facilitate the unified alignment for RGB-Thermal DAOD. Specifically, we devise a Channel Self-encoding Adaptation (CSA) module to mitigate biased discrimination from the discriminative enhancement perspective. CSA aligns the intra-domain channel subspace for inter-domain channel harmonizing. Upon revisiting instance alignment, we uncovered inaccuracies proposals and unstable positive sample phenomena. Therefore, we propose the Relative Relationship Adaptation (RRA) module to mitigate negative transfer. RRA ensures inter-domain semantic consistency through sparse instance alignment. Extensive experiments are conducted on visible-to-thermal and visible-to-visible benchmarks to validate the effectiveness, and our UGA framework outperforms state-of-the-art by a remarkable margin. The code of our UGA is available at https://github.com/zyfone/UGA .",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4398236405",
    "type": "article"
  },
  {
    "title": "Multi Fine-Grained Fusion Network for Depression Detection",
    "doi": "https://doi.org/10.1145/3665247",
    "publication_date": "2024-06-01",
    "publication_year": 2024,
    "authors": "Li Zhou; Zhenyu Liu; Yutong Li; Y. Y. Duan; Huimin Yu; Bin Hu",
    "corresponding_authors": "",
    "abstract": "Depression is an illness that involves emotional and mental health. Currently, depression detection through interviews is the most popular way. With the advancement of natural language processing and sentiment analysis, automated interview-based depression detection is strongly supported. However, current multimodal depression detection models fail to adequately capture the fine-grained features of depressive behaviors, making it difficult for the models to accurately characterize the subtle changes in depressive symptoms. To address this problem, we propose a Multi Fine-Grained Fusion Network (MFFNet). The core idea of this model is to extract and fuse the information of different scale feature pairs through a Multi-Scale Fastformer (MSfastformer), and then use the Recurrent Pyramid Model to integrate the features of different resolutions, promoting the interaction of multi-level information. Through the interaction of multi-scale and multi-resolution features, it aims to explore richer feature representations. To validate the effectiveness of our proposed MFFNet model, we conduct experiments on two depression interview datasets. The experimental results show that the MFFNet model performs better in depression detection compared to other benchmark multimodal models.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4399262530",
    "type": "article"
  },
  {
    "title": "VoiceStyle: Voice-based Face Generation Via Cross-modal Prototype Contrastive Learning",
    "doi": "https://doi.org/10.1145/3671002",
    "publication_date": "2024-06-05",
    "publication_year": 2024,
    "authors": "Wuyang Chen; Boqing Zhu; Kele Xu; Yong Dou; D. L. Feng",
    "corresponding_authors": "",
    "abstract": "Can we predict a person’s appearance solely based on their voice? This paper explores this question by focusing on generating a face from an unheard voice segment. Our proposed method, VoiceStyle, combines cross-modal representation learning with generation modeling, enabling us to incorporate voice semantic cues into the generated face. In the first stage, we introduce cross-modal prototype contrastive learning (CMPC) to establish the association between voice and face. Recognizing the presence of false negative and deviate positive instances in real-world unlabeled data, we not only use voice-face pairs in the same video but also construct additional semantic positive pairs through unsupervised clustering, enhancing the learning process. Moreover, we recalibrate instances based on their similarity to cluster centers in the other modality. In the second stage, we harness the powerful generative capabilities of StyleGAN to produce faces. We optimize the latent code in StyleGAN’s latent space, guided by the learned voice-face alignment. To address the importance of selecting an appropriate starting point for optimization, we aim to automatically find an optimal starting point by utilizing the face prototype derived from the voice input. The entire pipeline can be implemented in a self-supervised manner, eliminating the need for manually labeled annotations. Through extensive experiments, we demonstrate the effectiveness and performance of our VoiceStyle method in both cross-modal representation learning and voice-based face generation.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4399359896",
    "type": "article"
  },
  {
    "title": "Mustang: Improving QoE for Real-Time Video in Cellular Networks by Masking Jitter",
    "doi": "https://doi.org/10.1145/3672399",
    "publication_date": "2024-06-10",
    "publication_year": 2024,
    "authors": "Evelyn Yu; Jianer Zhou; Zhenyu Li; Gareth Tyson; Weichao Li; Xinyi Zhang; Zhiwei Xu; Gaogang Xie",
    "corresponding_authors": "",
    "abstract": "The advent of 5G and interactive live broadcasting has led to a growing trend of people preferring real-time interactive video services on mobile devices, particularly mobile phones. In this work, we measure the performance of Google congestion control (GCC) in cellular networks, which is the default congestion control algorithm for Web Real-Time Communications (WebRTC). Our measurements show that GCC sometimes makes bitrate decisions which are harmful to quality of experience (QoE) in cellular networks with high jitter. We further find that the frame delivery time (FDT) in the player can mitigate network jitter and maintain QoE. Moreover, the receiving rate is better to reflect the network congestion than RTT in cellular networks. Based on these measurements and findings, we propose Mustang, an algorithm designed to overcome the jitter in cellular networks. Mustang makes use of the FDT and receiving rate as feedback information to the sender. Then the sender adjusts its sending rate based on the information to guarantee QoE. We have implemented Mustang in WebRTC and evaluated it in both emulated and real cellular networks. The experimental results show that Mustang can improve WebRTC’s both QoS and QoE performance. For QoS, Mustang increases the sending rate by 72.1% and has similar RTT and packet loss when compared with GCC, while it is about 30% better for QoE.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4399493576",
    "type": "article"
  },
  {
    "title": "Deepfake Video Detection Using Facial Feature Points and Ch-Transformer",
    "doi": "https://doi.org/10.1145/3672566",
    "publication_date": "2024-06-12",
    "publication_year": 2024,
    "authors": "Rui Yang; Rushi Lan; Zhenrong Deng; Xiaonan Luo; Xiyan Sun",
    "corresponding_authors": "",
    "abstract": "With the development of Metaverse technology, the avatar in Metaverse has faced serious security and privacy concerns. Analyzing facial features to distinguish between genuine and manipulated facial videos holds significant research importance for ensuring the authenticity of characters in the virtual world and for mitigating discrimination, as well as preventing malicious use of facial data. To address this issue, the Facial Feature Points and Ch-Transformer (FFP-ChT) deepfake video detection model is designed based on the clues of different facial feature points distribution in real and fake videos and different displacement distances of real and fake facial feature points between frames. The face video input is first detected by the BlazeFace model, and the face detection results are fed into the FaceMesh model to extract 468 facial feature points. Then the Lucas-Kanade (LK) optical flow method is used to track the points of the face, the face calibration algorithm is introduced to re-calibrate the facial feature points, and the jitter displacement is calculated by tracking the facial feature points between frames. Finally, the Class-head (Ch) is designed in the transformer, and the facial feature points and facial feature point displacement are jointly classified through the Ch-Transformer model. In this way, the designed Ch-Transformer classifier is able to accurately and effectively identify deepfake videos. Experiments on open datasets clearly demonstrate the effectiveness and generalization capabilities of our approach.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4399569783",
    "type": "article"
  },
  {
    "title": "Boosting Semi-Supervised Learning with Dual-Threshold Screening and Similarity Learning",
    "doi": "https://doi.org/10.1145/3672563",
    "publication_date": "2024-06-12",
    "publication_year": 2024,
    "authors": "Zechen Liang; Yuan‐Gen Wang; Wei Lu; Xiaochun Cao",
    "corresponding_authors": "",
    "abstract": "How to effectively utilize unlabeled data for training is a key problem in Semi-Supervised Learning (SSL). Existing SSL methods often consider the unlabeled data whose predictions are beyond a fixed threshold (e.g., 0.95), and discard those less than 0.95. We argue that these discarded data have a large proportion, are of hard sample, and will benefit the model training if used properly. In this paper, we propose a novel method to take full advantage of the unlabeled data, termed DTS-SimL, which includes two core designs: Dual-Threshold Screening and Similarity Learning. Except for the fixed threshold, DTS-SimL extracts another class-adaptive threshold from the labeled data. Such a class-adaptive threshold can screen many unlabeled data whose predictions are lower than 0.95 but over the extracted one for model training. On the other hand, we design a new similar loss to perform similarity learning for all the highly-similar unlabeled data, which can further mine the valuable information from the unlabeled data. Finally, for more effective training of DTS-SimL, we construct an overall loss function by assigning four different losses to four different types of data. Extensive experiments are conducted on five benchmark datasets, including CIFAR-10, CIFAR-100, SVHN, Mini-ImageNet, and DomainNet-Real. Experimental results show that the proposed DTS-SimL achieves state-of-the-art classification accuracy. The code is publicly available at https://github.com/GZHU-DVL/DTS-SimL .",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4399570150",
    "type": "article"
  },
  {
    "title": "Multimodal Fusion for Talking Face Generation Utilizing Speech-related Facial Action Units",
    "doi": "https://doi.org/10.1145/3672565",
    "publication_date": "2024-06-17",
    "publication_year": 2024,
    "authors": "Zhilei Liu; Xiaoxing Liu; Sen Chen; Jiaxing Liu; Longbiao Wang; Chongke Bi",
    "corresponding_authors": "",
    "abstract": "Talking face generation is to synthesize a lip-synchronized talking face video by inputting an arbitrary face image and corresponding audio clips. The current talking face model can be divided into four parts: visual feature extraction, audio feature processing, multimodal feature fusion, and rendering module. For the visual feature extraction part, existing methods face the challenge of complex learning task with noisy features, this paper introduces an attention-based disentanglement module to disentangle the face into Audio-face and Identity-face using speech-related facial action unit (AU) information. For the multimodal feature fusion part, existing methods ignore not only the interaction and relationship of cross-modal information but also the local driving information of the mouth muscles. This study proposes a novel generative framework that incorporates a dilated non-causal temporal convolutional self-attention network as a multimodal fusion module to enhance the learning of cross-modal features. The proposed method employs both audio- and speech-related facial action units (AUs) as driving information. Speech-related AU information can facilitate more accurate mouth movements. Given the high correlation between speech and speech-related AUs, we propose an audio-to-AU module to predict speech-related AU information. Finally, we present a diffusion model for the synthesis of talking face images. We verify the effectiveness of the proposed model on the GRID and TCD-TIMIT datasets. An ablation study is also conducted to verify the contribution of each component. The results of quantitative and qualitative experiments demonstrate that our method outperforms existing methods in terms of both image quality and lip-sync accuracy. Code is available at https://mftfg-au.github.io/Multimodal_Fusion/ .",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4399740326",
    "type": "article"
  },
  {
    "title": "KF-VTON: Keypoints-Driven Flow Based Virtual Try-On Network",
    "doi": "https://doi.org/10.1145/3673903",
    "publication_date": "2024-06-19",
    "publication_year": 2024,
    "authors": "Zizhao Wu; Siyu Liu; Peioyan Lu; Ping Yang; Yongkang Wong; Xiaoling Gu; Mohan Kankanhalli",
    "corresponding_authors": "",
    "abstract": "Image-based virtual try-on aims to fit a target garment to a reference person. Most existing methods are limited to solving the Garment-To-Person (G2P) try-on task that transfers a garment from a clean product image to the reference person and do not consider the Person-To-Person (P2P) try-on task that transfers a garment from a clothed person image to the reference person, which limits the practical applicability. The P2P try-on task is more challenging due to spatial discrepancies caused by different poses, body shapes, and views between the reference person and the target person. To address this issue, we propose a novel Keypoints-Driven Flow Based Virtual Try-On Network (KF-VTON) for handling both the G2P and P2P try-on tasks. Our KF-VTON has two key innovations: (1) We propose a new keypoints-driven flow based deformation model to warp the garment. This model establishes spatial correspondences between the target garment and reference person by combining the robustness of Thin-plate Spline (TPS) based deformation and the flexibility of appearance flow based deformation. (2) We investigate a powerful Context-aware Spatially Adaptive Normalization (CSAN) generative module to synthesize the final try-on image. Particularly, CSAN integrates rich contextual information with semantic parsing guidance to properly infer unobserved garment appearances. Extensive experiments demonstrate that our KF-VTON is capable of producing photo-realistic and high-fidelity try-on results for the G2P as well as P2P try-on tasks and surpasses previous state-of-the-art methods both quantitatively and qualitatively. Our code is available at https://github.com/OIUIU/KF-VTON .",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4399804303",
    "type": "article"
  },
  {
    "title": "HCCL: <b>H</b> ierarchical <b>C</b> ounterfactual <b>C</b> ontrastive <b>L</b> earning for Robust Visual Question Answering",
    "doi": "https://doi.org/10.1145/3673902",
    "publication_date": "2024-06-27",
    "publication_year": 2024,
    "authors": "Dongze Hao; Qunbo Wang; Xinxin Zhu; Jing Liu",
    "corresponding_authors": "",
    "abstract": "Despite most state-of-the-art models having achieved amazing performance in Visual Question Answering (VQA) , they usually utilize biases to answer the question. Recently, some studies synthesize counterfactual training samples to help the model to mitigate the biases. However, these synthetic samples need extra annotations and often contain noises. Moreover, these methods simply add synthetic samples to the training data to train the model with the cross-entropy loss, which cannot make the best use of synthetic samples to mitigate the biases. In this article, to mitigate the biases in VQA more effectively, we propose a Hierarchical Counterfactual Contrastive Learning (HCCL) method. Firstly, to avoid introducing noises and extra annotations, our method automatically masks the unimportant features in original pairs to obtain positive samples and create mismatched question-image pairs as negative samples. Then our method uses feature-level and answer-level contrastive learning to make the original sample close to positive samples in the feature space, while away from negative samples in both feature and answer spaces. In this way, the VQA model can learn the robust multimodal features and focus on both visual and language information to produce the answer. Our HCCL method can be adopted in different baselines, and the experimental results on VQA v2, VQA-CP, and GQA-OOD datasets show that our method is effective in mitigating the biases in VQA, which improves the robustness of the VQA model.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4400074770",
    "type": "article"
  },
  {
    "title": "DAG-YOLO: A Context-feature Adaptive Fusion Rotating Detection Network in Remote Sensing Images",
    "doi": "https://doi.org/10.1145/3674978",
    "publication_date": "2024-06-27",
    "publication_year": 2024,
    "authors": "Zexiang Guo; Xiaohai He; Yu Tao Yang; Linbo Qing; Honggang Chen",
    "corresponding_authors": "",
    "abstract": "Object detection in remote sensing image (RSI) research has seen significant advancements, particularly with the advent of deep learning. However, challenges such as orientation, scale, aspect ratio variations, dense object distribution, and category imbalances remain. To address these challenges, we present DAG-YOLO, a one-stage context-feature adaptive weighted fusion network that incorporates through three innovative parts. First, we integrate 1D Gaussian Angle-coding with YOLOv5 to convert the angle regression task into a classification task, establishing a more robust rotating object detection baseline, GLR-YOLO. Second, we introduce the Dual Branch Context Adaptive Modeling module, which enhances feature extraction capabilities by capturing global context information. Third, we design an adaptive detect head with the Adaptive Global Feature Aggregation and Reweighting (AGFAR) module. AGFAR addresses feature inconsistency among different output layers of the Feature Pyramid Network, retaining useful semantic information and elevating detection accuracy. Extensive experiments on public datasets DOTA-v1.0, DOTA-v1.5, and UCAS-AOD showcase mAP scores of 77.75%, 73.79%, and 90.27%, respectively. Our proposed method has the best performance among the current mainstream SOTA methods, which proves its effectiveness in RSI object detection.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4400074823",
    "type": "article"
  },
  {
    "title": "Monocular Depth and Ego-motion Estimation with Scale Based on Superpixel and Normal Constraints",
    "doi": "https://doi.org/10.1145/3674977",
    "publication_date": "2024-07-01",
    "publication_year": 2024,
    "authors": "Junxin Lu; Yongbin Gao; Jieyu Chen; Jenq‐Neng Hwang; Hamido Fujita; Zhijun Fang",
    "corresponding_authors": "",
    "abstract": "Three-dimensional perception in intelligent virtual and augmented reality (VR/AR) and autonomous vehicles (AV) applications is critical and attracting significant attention. The self-supervised monocular depth and ego-motion estimation serves as a more intelligent learning approach that provides the required scene depth and location for 3D perception. However, the existing self-supervised learning methods suffer from scale ambiguity, boundary blur, and imbalanced depth distribution, limiting the practical applications of VR/AR and AV. In this article, we propose a new self-supervised learning framework based on superpixel and normal constraints to address these problems. Specifically, we formulate a novel 3D edge structure consistency loss to alleviate the boundary blur of depth estimation. To address the scale ambiguity of estimated depth and ego-motion, we propose a novel surface normal network for efficient camera height estimation. The surface normal network is composed of a deep fusion module and a full-scale hierarchical feature aggregation module. Meanwhile, to realize the global smoothing and boundary discriminability of the predicted normal map, we introduce a novel fusion loss which is based on the consistency constraints of the normal in edge domains and superpixel regions. Experiments are conducted on several benchmarks, and the results illustrate that the proposed approach outperforms the state-of-the-art methods in depth, ego-motion, and surface normal estimation.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4400199641",
    "type": "article"
  },
  {
    "title": "Harnessing Representative Spatial-Temporal Information for Video Question Answering",
    "doi": "https://doi.org/10.1145/3675399",
    "publication_date": "2024-07-05",
    "publication_year": 2024,
    "authors": "Yuanyuan Wang; Meng Liu; Xuemeng Song; Liqiang Nie",
    "corresponding_authors": "",
    "abstract": "Video question answering, aiming to answer a natural language question related to the given video, has become prevalent in the past few years. Although remarkable improvements have been obtained, it is still exposed to the challenge of insufficient comprehension of video content. To this end, we propose a spatial-temporal representative visual exploitation network for video question answering, which enhances the understanding of the video by merely summarizing representative visual information. In order to explore representative object information, we advance adaptive attention based on uncertainty estimation. At the same time, to capture representative frame-level and clip-level visual information, we structure a much more compact set of representations iteratively in an expectation-maximization manner to deprecate noisy information. Both the quantitative and qualitative results on NExT-QA, TGIF-QA, MSRVTT-QA, and MSVD-QA datasets demonstrate the superiority of our model over several state-of-the-art approaches.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4400359195",
    "type": "article"
  },
  {
    "title": "Exploiting Backdoors of Face Synthesis Detection with Natural Triggers",
    "doi": "https://doi.org/10.1145/3677380",
    "publication_date": "2024-07-11",
    "publication_year": 2024,
    "authors": "Xiaoxuan Han; Songlin Yang; Wei Wang; Ziwen He; Jing Dong",
    "corresponding_authors": "",
    "abstract": "Deep neural networks have enhanced face synthesis detection in discriminating Artificial Intelligence Generated Content (AIGC). However, their security is threatened by the injection of carefully crafted triggers during model training (i.e., backdoor attacks). Although existing backdoor defenses and manual data selection are able to mitigate those using human-eye-sensitive triggers, such as patches or adversarial noises, the more challenging natural backdoor triggers remain insufficiently researched. To further investigate natural triggers, we propose a novel analysis-by-synthesis backdoor attack against face synthesis detection models, which embeds natural triggers in the latent space. We study such backdoor vulnerability from two perspectives: (1) Model Discrimination (Optimization-Based Trigger) : we adopt a substitute detection model and find the trigger by minimizing the cross-entropy loss; (2) Data Distribution (Custom Trigger) : we manipulate the uncommon facial attributes in the long-tailed distribution to generate poisoned samples without the supervision from detection models. Furthermore, to evaluate the detection models towards the latest AIGC, we utilize both the state-of-the-art StyleGAN and Stable Diffusion for trigger generation. Finally, these backdoor triggers introduce specific semantic features to the generated poisoned samples (e.g., skin textures and smile), which are more natural and robust. Extensive experiments show that our method is superior over existing pixel space backdoor attacks on three levels: (1) Attack Success Rate : achieving an attack success rate exceeding 99 \\(\\%\\) , comparable to baseline methods, with less than 0.1 \\(\\%\\) model accuracy drop and under 3 \\(\\%\\) poisoning rate; (2) Backdoor Defense : showing superior robustness when faced with existing backdoor defenses (e.g., surpassing baseline methods by over 30 \\(\\%\\) after a 15-degree rotation); (3) Human Inspection : being less human-eye-sensitive from a user study with 46 participants and a collection of 2,300 data points.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4400536026",
    "type": "article"
  },
  {
    "title": "COBIRAS: Offering a Continuous Bit Rate Slide to Maximize DASH Streaming Bandwidth Utilization",
    "doi": "https://doi.org/10.1145/3677379",
    "publication_date": "2024-07-12",
    "publication_year": 2024,
    "authors": "Michael Seufert; Marius Spangenberger; Fabian Poignée; Florian Wamser; Werner Robitza; Christian Timmerer; Tobias Hoßfeld",
    "corresponding_authors": "",
    "abstract": "Reaching close-to-optimal bandwidth utilization in dynamic adaptive streaming over HTTP (DASH) systems can, in theory, be achieved with a small discrete set of bit rate representations. This includes typical bit rate ladders used in state-of-the-art DASH systems. In practice, however, we demonstrate that bandwidth utilization, and consequently the quality of experience (QoE), can be improved by offering a continuous set of bit rate representations, i.e., a continuous bit rate slide (COBIRAS). Moreover, we find that the buffer fill behavior of different standard adaptive bit rate (ABR) algorithms is sub-optimal in terms of bandwidth utilization. To overcome this issue, we leverage COBIRAS’ flexibility to request segments with any arbitrary bit rate and propose a novel ABR algorithm MinOff , which helps maximizing bandwidth utilization by minimizing download off-phases during streaming. To avoid extensive storage requirements with COBIRAS and to demonstrate the feasibility of our approach, we design and implement a proof-of-concept DASH system for video streaming that relies on just-in-time encoding ( JITE ), which reduces storage consumption on the DASH server. Finally, we conduct a performance evaluation on our testbed and compare a state-of-the-art DASH system with few bit rate representations and our JITE DASH system, which can offer a COBIRAS, in terms of bandwidth utilization and video QoE for different ABR algorithms.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4400580705",
    "type": "article"
  },
  {
    "title": "Reconstruction-free Image Compression for Machine Vision via Knowledge Transfer",
    "doi": "https://doi.org/10.1145/3678471",
    "publication_date": "2024-07-17",
    "publication_year": 2024,
    "authors": "Hanyue Tu; Li Li; Wengang Zhou; Houqiang Li",
    "corresponding_authors": "",
    "abstract": "Reconstruction-free image compression for machine vision aims to perform machine vision tasks directly on compressed-domain representations instead of reconstructed images. Existing reports have validated the feasibility of compressed-domain machine vision. However, we observe that when using recently learned compression models, the performance gap between compressed-domain and pixel-domain vision tasks is still large due to the lack of some natural inductive biases in pixel-domain convolutional neural networks. In this article, we attempt to address this problem by transferring knowledge from the pixel domain to the compressed domain. A knowledge transfer loss defined at both output level and feature level is proposed to narrow the gap between the compressed domain and the pixel domain. In addition, we modify neural networks for pixel-domain vision tasks to better suit compressed-domain inputs. Experimental results on several machine vision tasks show that the proposed method improves the accuracy of compressed-domain vision tasks significantly, which even outperforms learning on reconstructed images while avoiding the computational cost of image reconstruction.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4400729457",
    "type": "article"
  },
  {
    "title": "Learning Compressed Artifact for JPEG Manipulation Localization Using Wide-Receptive-Field Network",
    "doi": "https://doi.org/10.1145/3678883",
    "publication_date": "2024-07-18",
    "publication_year": 2024,
    "authors": "Fengyong Li; Huajun Zhai; Teng Liu; Xinpeng Zhang; Chuan Qin",
    "corresponding_authors": "",
    "abstract": "JPEG image manipulation localization aims to accurately classify and locate tampered regions in JPEG images. Existing image manipulation localization schemes usually consider diverse data streams of spatial domain, e.g. noise inconsistency and local content inconsistency. They, however, easily ignore an objective scenario: data stream features of spatial domain are hard to directly apply to compressed image format, e.g., JPEG, because tampered JPEG images may contain severe re-compression inconsistency and re-compression artifacts, when they are re-compressed to JPEG format. As a result, the traditional localization schemes relying on general data streams of spatial domain may result in a large number of false detection of tampered region in JPEG images. To address the above problem, we a new JPEG image manipulation localization scheme, in which a wide-receptive-field attention network is designed to effectively learn JPEG compressed artifacts. We firstly introduce the wide-receptive-field attention mechanism to re-construct U-Net network, which can effectively capture contextual information of JPEG images and analyze tampering traces from different image regions. Furthermore, a flexible JPEG compressed artifact learning module is designed to capture the image noise caused by JPEG compression, in which the weights can be adjusted flexibly based on image quality, without the need for decompression operations on JPEG images. Our proposed method can significantly strength the differentiation capability of detection model for tampered and non-tampered regions. A series of experiments are performed over different image sets, and the results demonstrate that the proposed scheme can achieve an overall localization performance for multi-scale JPEG manipulation regions and outperform most of state-of-the-art schemes in terms of detection accuracy, generalization and robustness.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4400788706",
    "type": "article"
  },
  {
    "title": "Few-Shot Face Sketch-to-Photo Synthesis via Global-Local Asymmetric Image-to-Image Translation",
    "doi": "https://doi.org/10.1145/3672400",
    "publication_date": "2024-07-20",
    "publication_year": 2024,
    "authors": "Yongkang Li; Qifan Liang; Zhen Han; W. Mai; Zhongyuan Wang",
    "corresponding_authors": "",
    "abstract": "Face sketch-to-photo synthesis is widely used in law enforcement and digital entertainment, which can be achieved by Image-to-Image (I2I) translation. Traditional I2I translation algorithms usually regard the bidirectional translation of two image domains as two symmetric processes, so the two translation networks adopt the same structure. However, due to the scarcity of face sketches and the abundance of face photos, the sketch-to-photo and photo-to-sketch processes are asymmetric. Considering this issue, we propose a few-shot face sketch-to-photo synthesis model based on asymmetric I2I translation, where the sketch-to-photo process uses a feature-embedded generating network, while the photo-to-sketch process uses a style transfer network. On this basis, a three-stage asymmetric training strategy with style transfer as the trigger is proposed to optimize the proposed model by utilizing the advantage that the style transfer network only needs few-shot face sketches for training. Additionally, we discover that stylistic differences between the global and local sketch faces lead to inconsistencies between the global and local sketch-to-photo processes. Thus, a dual branch of the global face and local face is adopted in the sketch-to-photo synthesis model to learn the specific transformation processes for global structure and local details. Finally, the high-quality synthetic face photo can be generated through the global-local face fusion sub-network. Extensive experimental results demonstrate that the proposed Global-Local Asymmetric (GLAS) I2I translation algorithm compared to SOTA methods, at least improves FSIM by 0.0126, and reduces LPIPS (alex), LPIPS (squeeze), and LPIPS (vgg) by 0.0610, 0.0883, and 0.0719, respectively.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4400850951",
    "type": "article"
  },
  {
    "title": "Deep Plug-and-Play Non-Iterative Cluster for 3D Global Feature Extraction",
    "doi": "https://doi.org/10.1145/3679204",
    "publication_date": "2024-07-20",
    "publication_year": 2024,
    "authors": "Zhenyu Li; Shanshan Gao; Deqian Mao; Shouwen Song; Lei Li; Yuanfeng Zhou",
    "corresponding_authors": "",
    "abstract": "Efficient and accurate point cloud feature extraction is crucial for critical tasks such as 3D recognition and semantic segmentation. However, existing global feature extraction methods for 3D data often require designing different models for different input types (point clouds, voxels, and maps). This article proposes an efficient plug-and-play non-iterative clustering method (NICM) to establish a unified point cloud global feature extraction paradigm suitable for any input type to solve the above problems. The core idea of the NICM is to construct the connection between a single point and other global points based only on the cosine similarity between center points to achieve global feature extraction, which has linear complexity characteristics and can be combined with any existing feature extraction model. Additionally, to better integrate the features extracted by NICM and the original model, this article designs an adaptive feature fusion module is designed based on the gate unit, which retains similar features and effectively fuses dissimilar features based on their importance to downstream tasks. We have applied our method to downstream tasks such as point cloud recognition, part segmentation and scene segmentation. Sufficient experiments have proven that our method can provide comprehensive and robust features for the original model, and effectively improve the performance of downstream tasks.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4400850957",
    "type": "article"
  },
  {
    "title": "Adaptive Cloud VR Gaming Optimized by Gamer QoE Models",
    "doi": "https://doi.org/10.1145/3680551",
    "publication_date": "2024-07-25",
    "publication_year": 2024,
    "authors": "Kuan-Yu Lee; Ashutosh Singla; Pablo César; Cheng-Hsin Hsu",
    "corresponding_authors": "",
    "abstract": "Cloud Virtual Reality (VR) gaming offloads computationally-intensive VR games to resourceful data centers. However, ensuring good Quality of Experience (QoE) in cloud VR gaming is inherently challenging as VR gamers demand high visual quality, short response time, and negligible cybersickness. In this article, we study the QoE of cloud VR gaming and build a QoE-optimized system in a few steps. First, we establish a cloud VR gaming testbed capable of emulating various network conditions. Using the testbed, we conduct comprehensive QoE evaluations using a user study to evaluate the influence of diverse factors, such as encoding settings, network conditions, and game genres, on gamer QoE scores. Second, we construct the very first QoE models for cloud VR gaming using our QoE evaluation results. Our QoE models achieve up to 0.93 ( \\(\\sigma=0.02\\) ) in Pearson Linear Correlation Coefficient (PLCC) and 0.92 ( \\(\\sigma=0.02\\) ) in Spearman Rank-Order Correlation Coefficient (SROCC), where \\(\\sigma\\) stands for the standard deviation. Last, we leverage our QoE models for dynamically adapting encoding settings in our testbed. Extensive experiments revealed that, compared to the current practice, our adaptive cloud VR gaming system improves: (i) overall quality by 0.87 ( \\(\\sigma=0.44\\) ), (ii) visual quality by 0.61 ( \\(\\sigma=0.45\\) ), and (iii) interaction quality by 1.20 ( \\(\\sigma=0.48\\) ) on average in 5-point Mean Opinion Score (MOS).",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4400981668",
    "type": "article"
  },
  {
    "title": "Multi-model Style-aware Diffusion Learning for Semantic Image Synthesis",
    "doi": "https://doi.org/10.1145/3686155",
    "publication_date": "2024-08-02",
    "publication_year": 2024,
    "authors": "Yunfang Niu; Lingxiang Wu; Yufeng Zhang; Yousong Zhu; Guibo Zhu; Jinqiao Wang",
    "corresponding_authors": "",
    "abstract": "Semantic image synthesis aims to generate images from given semantic layouts, which is a challenging task that requires training models to capture the relationship between layouts and images. Previous works are usually based on Generative Adversarial Networks (GAN) or autoregressive (AR) models. However, the GAN model's training process is unstable, and the AR model’s performance is seriously affected by the independent image encoder and the unidirectional generation bias. Due to the above limitations, these methods tend to synthesize unrealistic, poorly aligned images and only consider single-style image generation. In this paper, we propose a Multi-model Style-aware Diffusion Learning (MSDL) framework for semantic image synthesis, including a training module and a sampling module. In the training module, a layout-to-image model is introduced to transfer the learned knowledge from a model pretrained with massive weak correlated text-image pairs data, making the training process more efficient. In the sampling module, we designed a map-guidance technique and creatively designed a multi-model style-guidance strategy for creating images in multiple styles, e.g., oil painting, Disney Cartoon, and pixel style. We evaluate our method on Cityscapes, ADE20K, and COCO-Stuff, making visual comparisons and computing with multiple metrics such as FID, LPIPS, etc. Experimental results demonstrate that our model is highly competitive, especially in terms of fidelity and diversity.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4401249388",
    "type": "article"
  },
  {
    "title": "MSSA: Multi-Representation Semantics-Augmented Set Abstraction for 3D Object Detection",
    "doi": "https://doi.org/10.1145/3686157",
    "publication_date": "2024-08-02",
    "publication_year": 2024,
    "authors": "Huaijin Liu; Ji‐Xiang Du; Yong Zhang; Hongbo Zhang; Jiandian Zeng",
    "corresponding_authors": "",
    "abstract": "Accurate recognition and localization of 3D objects is a fundamental research problem in 3D computer vision. Benefiting from transformation-free point cloud processing and flexible receptive fields, point-based methods have become accurate in 3D point cloud modeling, but still fall behind voxel-based competitors in 3D detection. We observe that the set abstraction module, commonly utilized by point-based methods for downsampling points, tends to retain excessive irrelevant background information, thus hindering the effective learning of features for object detection tasks. To address this issue, we propose MSSA, a Multi-representation Semantics-augmented Set Abstraction for 3D object detection. Specifically, we first design a backbone network to encode different representation features of point clouds, which extracts point-wise features through PointNet to preserve fine-grained geometric structure features, and adopts VoxelNet to extract voxel features and BEV features to enhance the semantic features of key points. Second, to efficiently fuse different representation features of keypoints, we propose a Point feature-guided Voxel feature and BEV feature fusion (PVB-Fusion) module to adaptively fuse multi-representation features and remove noise. At last, a novel Multi-representation Semantic-guided Farthest Point Sampling (MS-FPS) algorithm is designed to help set abstraction modules progressively downsample point clouds, thereby improving instance recall and detection performance with more important foreground points. We evaluate MSSA on the widely used KITTI dataset and the more challenging nuScenes dataset. Experimental results show that compared to PointRCNN, our method improves the AP of “moderate” level for three classes of objects by 7.02%, 6.76%, and 5.44%, respectively. Compared to the advanced point-voxel-based method PV-RCNN, our method improves the AP of “moderate” level by 1.23%, 2.84%, and 0.55% for the three classes, respectively.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4401249403",
    "type": "article"
  },
  {
    "title": "Face Reconstruction-Based Generalized Deepfake Detection Model with Residual Outlook Attention",
    "doi": "https://doi.org/10.1145/3686162",
    "publication_date": "2024-08-02",
    "publication_year": 2024,
    "authors": "Zenan Shi; Wenyu Liu; Haipeng Chen",
    "corresponding_authors": "",
    "abstract": "With the continuous development of deep counterfeiting technology, the information security in our daily life is under serious threat. While existing face forgery detection methods exhibit impressive accuracy when applied to datasets such as FaceForensics++ and Celeb- DF, they falter significantly when confronted with out-of-domain scenarios. This causes specialization of learned representations to known forgery patterns presented in the training set, rendering it difficult to detect forgeries with unknown patterns. To address this challenge, we propose a novel end-to-end F ace R econstruction-based G eneralized D eepfake D etection model with Residual Outlook Attention, named FRG2D , which emphasizes the robust visual representations of genuine faces and discerns the subtle differences between authentic and manipulated facial images. Our methodology entails reconstructing authentic face images using an encoder-decoder architecture based on U-net, facilitating a deeper understanding of disparities between genuine and manipulated facial images. Furthermore, we integrate the convolutional block attention module (CBAM) and channel attention block (CAB) to selectively focus the network’s attention on salient features within real face images. Furthermore, we employ Residual Outlook Attention (ROA) to guide the network’s focus towards precise features within manipulated facial images. Simultaneously, the computed reconstruction differences obtained through Residual Outlook Attention serves as the ultimate representation fed into the classifier for face forgery detection. Both the reconstruction and classification learning processes are optimized end-to-end. Through extensive experimentation, our model demonstrated a substantial improvement in deepfake detection across unknown domains, while maintaining a high accuracy within the known domain.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4401249478",
    "type": "article"
  },
  {
    "title": "EdgeStreaming: Secure Computation Intelligence in Distributed Edge Networks for Streaming Analytics",
    "doi": "https://doi.org/10.1145/3686161",
    "publication_date": "2024-08-02",
    "publication_year": 2024,
    "authors": "Pei-Gen Ye; W. Wang; Bing Mi; Kongyang Chen",
    "corresponding_authors": "",
    "abstract": "In modern information systems, real-time streaming data are generated in various vertical application scenarios, such as industrial security cameras, household intelligent devices, mobile robots and among others. However, these low-end devices can hardly provide real-time and accurate data analysis functionalities due to their limited on-board performances. Traditional centralized server computing also suffers from its prolonged transmission latency, resulting in huge response time. To deal with this problem, this paper presents a novel distributed computation intelligent system with nearby edge devices, abbreviated as EdgeStreaming, to facilitate rapid and accurate analysis of streaming data. Firstly, we thoroughly explore the available edge devices surrounding the terminal to generate an internally interconnected edge network. This edge network real-time perceives and updates the internal resource status of each edge device, such as computational and storage resources. Dynamic allocation of external computational or storage demands can be made based on the current load of individual edge devices. Consequently, the streaming data perceived by external terminal devices can be transmitted in real-time to any edge gateway. The edge network employs a well-designed task scheduling strategy to partition and allocate streaming data processing demands to one or multiple edge devices. Additionally, it customizes computational requirements judiciously, for instance, by utilizing model compression to expedite computation speed. We deployed an edge network comprising multiple Raspberry Pis, NVIDIA Jetson Nano, and Jetson NVIDIA TX2 devices, successfully achieving real-time analysis and detection of video streaming data. We believe our work provides new technological support for the real-time processing of streaming data.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4401249780",
    "type": "article"
  },
  {
    "title": "Target Structure Learning Framework for Unsupervised Multi-Class Domain Adaptation",
    "doi": "https://doi.org/10.1145/3686156",
    "publication_date": "2024-08-03",
    "publication_year": 2024,
    "authors": "Jingzheng Li; Hailong Sun; Lei Chai; Jiyi Li",
    "corresponding_authors": "",
    "abstract": "Unsupervised multi-class domain adaptation (multi-class UDA) has recently been proposed to fill the gap between empirically practical methods for multi-class classification and well-founded theory with the setting of binary classification. Nevertheless, the multi-class UDA methods use model predictions to characterize the disagreement of multi-class scoring hypotheses, which is used to optimize the divergence between domain distributions. Such self-training manner may bring inaccurate model predictions, which would damage the target structure due to the absence of labels of the target domain, leading to sub-optimal performance. On the other hand, this disagreement between multi-class scoring hypotheses does not involve the relationships among all of the multiple classes. It causes that multi-class UDA cannot properly connect the advanced practical UDA methods that consider class-conditional distribution alignment. Thus, we propose to exploit the target structure information and then incorporate it into multi-class UDA to achieve class-conditional distribution alignment. We theoretically and experimentally explain the importance of accurate target structure information to reduce the expected error on the target domain. Notably, our method achieves state-of-the-art results on three commonly-used benchmarks with different scales. In addition, using the target structure information, we propose a variant to cope with noisy open-world source domains such as noisy labels and out-of-distribution samples, enhancing the robustness of our method. The source code is available at https://github.com/jingzhengli/Multi_Class_UDA .",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4401286543",
    "type": "article"
  },
  {
    "title": "Federated Learning Using Multi-Modal Sensors with Heterogeneous Privacy Sensitivity Levels",
    "doi": "https://doi.org/10.1145/3686801",
    "publication_date": "2024-08-05",
    "publication_year": 2024,
    "authors": "Chih-Fan Hsu; Y. Li; Chung-Chi Tsai; J. Wang; Cheng-Hsin Hsu",
    "corresponding_authors": "",
    "abstract": "Data from multi-modal sensors, such as Red-Green-Blue (RGB) cameras, thermal cameras, microphones, and mmWave radars, have gradually been adopted in various classification problems for better accuracy. Some sensors, like RGB cameras and microphones, however, capture privacy-invasive data, which are less likely to be used in centralized learning. Although the Federated Learning (FL) paradigm frees clients from sharing their sensor data, doing so results in reduced classification accuracy and increased training time. In this article, we introduce a novel Heterogeneous Privacy Federated Learning (HPFL) paradigm to better capitalize on the less privacy-invasive sensor data, such as thermal images and mmWave point clouds, by uploading them to the server for closing the performance gap between FL and centralized learning. HPFL not only allows clients to keep the more privacy-invasive sensor data private, such as RGB images and human voices, but also gives each client total freedom to define the levels of their privacy concern on individual sensor modalities. For example, more sensitive users may prefer to keep their thermal images private, while others do not mind sharing these images. We carry out extensive experiments to evaluate the HPFL paradigm using two representative classification problems: semantic segmentation and emotion recognition. Several key findings demonstrate the merits of HPFL: (i) compared to FedAvg, it improves foreground accuracy by 18.20% in semantic segmentation and boosts the F1-score by 4.20% in emotion recognition, (ii) with heterogeneous privacy concern levels, it achieves an even larger F1-score improvement of 6.17–16.05% in emotion recognition, and (iii) it also outperforms the state-of-the-art FL approaches by 12.04–17.70% in foreground accuracy and 2.54–4.10% in F1-score.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4401330596",
    "type": "article"
  },
  {
    "title": "Blind 3D Video Stabilization with Spatio-Temporally Varying Motion Blur",
    "doi": "https://doi.org/10.1145/3686159",
    "publication_date": "2024-08-08",
    "publication_year": 2024,
    "authors": "Hengwei Li; Wei Wang; Xiao Wang; Xin Yuan; Xin Xu",
    "corresponding_authors": "",
    "abstract": "Video stabilization is a challenging task that attempts to compensate for the overall frame shake during video acquisition. Existing three-dimensional video stabilization methods aim at modeling camera perspective projection through either data-driven training or explicit motion estimation. However, the above methods are difficult to effectively solve the issue of shaky videos with abrupt object movements, resulting in local motion blur in the direction of the movement. This phenomenon is prevalent in real-world scenarios featuring foreground blind motion scenes. Unfortunately, directly combining stabilization and deblurring methods poses challenges when dealing with this situation. In the video, the intensity of motion blur undergoes continuous changes, and the direct combination method inadequately utilizes spatiotemporal information, providing insufficient clues for cross-frame compensation. To alleviate this problem, the Cross-frame-temporal Module framework is proposed to address blind motion blur induced by various conditions, which utilizes cross-frame temporal features to estimate depth maps and camera motion. In this framework, a Blur Transform Network (BTNet) is designed to adapt to spatially varying motion blur, which transforms local regions according to the impact of blur intensities to adapt to the effects of non-uniform motion blur; furthermore, our Temporal-Aware Network (TANet) further suppresses motion blur by leveraging cross-frame temporal features. In addition, the limited availability of pair-training video data containing motion blur limits the application of this approach in practice. The Cross-frame-temporal Module framework adopts an un-pretrained in-test training strategy. Extensive experimental results have demonstrated that our method outperforms state-of-the-art methods.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4401421989",
    "type": "article"
  },
  {
    "title": "Multi-Modal Driven Pose-Controllable Talking Head Generation",
    "doi": "https://doi.org/10.1145/3673901",
    "publication_date": "2024-08-10",
    "publication_year": 2024,
    "authors": "Kuiyuan Sun; Xiaolong Liu; Xiaolong Li; Yao Zhao; Wei Wang",
    "corresponding_authors": "",
    "abstract": "Talking head, driving a source image to generate a talking video using other modality information, has made great progress in recent years. However, there are two main issues: 1) These methods are designed to utilize a single modality of information. 2) Most methods cannot control head pose. To address these problems, we propose a novel framework that can utilize multi-modal information to generate a talking head video, while achieving arbitrary head pose control by a movement sequence. Specifically, first, to extend driving information to multiple modalities, multi-modal information is encoded to a unified semantic latent space to generate expression parameters. Secondly, to disentangle attributes, the 3D Morphable Model (3DMM) is utilized to obtain identity information from the source image, and translation and rotation information from the target image. Thirdly, to control head pose and mouth shape, the source image is warped by a motion field generated by the expression parameter, translation parameter, and angle parameter. Finally, all the above parameters are utilized to render a landmark map, and the warped source image is combined with the landmark map to generate a delicate talking head video. Our experimental results demonstrate that our proposed method is capable of achieving state-of-the-art performance in terms of visual quality, lip-audio synchronization, and head pose control.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4401483653",
    "type": "article"
  },
  {
    "title": "Improving Image Aesthetic Assessment via Multiple Image Joint Learning",
    "doi": "https://doi.org/10.1145/3687128",
    "publication_date": "2024-08-21",
    "publication_year": 2024,
    "authors": "Tengfei Shi; Chenglizhao Chen; Zhenyu Wu; Aimin Hao; Yuming Fang",
    "corresponding_authors": "",
    "abstract": "Image Aesthetic Assessment (IAA) is an emerging paradigm that predicts aesthetic score as the popular aesthetic taste for an image. Previous IAA approaches take a single image as input to predict the aesthetic score of the image. However, we discover that most existing IAA methods fail dramatically to predict the images with a large variance of aesthetic voting distribution. Motivated by the practice that people consider similar experiences to improve the consistence of the voting result, we present a novel Multiple Image joint Learning Network (MILNet) to mimic this natural process. Our novelty is mainly three-fold: (a) Semantic-based retrieval method that constructs aesthetic similarity (the similarity of aesthetic attribution) to select reference images; (b) Graph network reasoning that initializes and updates the weight of intrinsic relationships among multiple images; (c) Adaptive Earth Mover’s Distance (AdaEMD) loss function that adjusts weight for easy and hard instances to mitigate unbalanced distribution of aesthetic datasets. Our evaluation with the benchmark AVA and TAD datasets demonstrates that the proposed MILNet outperforms state-of-the-art IAA methods. The code is available at https://github.com/flyingbird93/MILNet .",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4401717352",
    "type": "article"
  },
  {
    "title": "MMICT: Boosting Multi-Modal Fine-Tuning with In-Context Examples",
    "doi": "https://doi.org/10.1145/3688804",
    "publication_date": "2024-08-21",
    "publication_year": 2024,
    "authors": "Tao Chen; Enwei Zhang; Yuting Gao; Ke Li; Xing Sun; Yan Zhang; Hui Li; Rongrong Ji",
    "corresponding_authors": "",
    "abstract": "Although In-Context Learning (ICL) brings remarkable performance gains to Large Language Models (LLMs), the improvements remain lower than fine-tuning on downstream tasks. This paper introduces Multi-Modal In-Context Tuning (MMICT), a novel multi-modal fine-tuning paradigm that boosts multi-modal fine-tuning by fully leveraging the promising ICL capability of multi-modal LLMs (MM-LLMs). We propose the Multi-Modal Hub (M-Hub), a unified module that captures various multi-modal features according to different inputs and objectives. Based on M-Hub, MMICT enables MM-LLMs to learn from in-context visual-guided textual features and subsequently generate outputs conditioned on the textual-guided visual features. Moreover, leveraging the flexibility of M-Hub, we design a variety of in-context demonstrations. Extensive experiments on a diverse range of downstream multi-modal tasks demonstrate that MMICT significantly outperforms traditional fine-tuning strategy and the vanilla ICT method that directly takes the concatenation of all information from different modalities as input. Our implementation is available at: https://github.com/KDEGroup/MMICT.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4401717397",
    "type": "article"
  },
  {
    "title": "Semi-supervised RGB-D Hand Gesture Recognition via Mutual Learning of Self-supervised Models",
    "doi": "https://doi.org/10.1145/3689644",
    "publication_date": "2024-08-23",
    "publication_year": 2024,
    "authors": "Jian Zhang; Kaihao He; Ting Yu; Jun Yu; Zhenming Yuan",
    "corresponding_authors": "",
    "abstract": "Human hand gesture recognition is important to Human-Computer-Interaction. Gesture recognition based on RGB-D data exploits both RGB and depth images to provide comprehensive results. However, the research under scenario with insufficient annotated data is not adequate. In view of the problem, our insight is to perform self-supervised learning with respect to each modality, transfer the learned information to modality specific classifiers and then fuse their results for final decision. To this end, we propose a semi-supervised hand gesture recognition method known as Mutual Learning of Rotation-Aware Gesture Predictors (MLRAGP), which exploits unlabeled training RGB and depth images via self-supervised learning and achieves multimodal decision fusion through deep mutual learning. For each modality, we rotate both labeled and unlabeled images to fixed angles and train an angle predictor to predict the angles, then we use the feature extraction part of the angle predictor to construct the category predictor and train it through labeled data. We subsequently fuse the category predictors about both modalities by impelling each of them to simulate the probability estimation produced by the other, and making the prediction of labeled images to approach the ground truth annotation. During the training of category predictor and mutual learning, the parameters of feature extractors can be slighted fine-tuned to avoid underfitting. Experimental results on NTU-Microsoft Kinect Hand Gesture dataset and Washington RGB-D dataset demonstrates the superiority of this framework to existing methods.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4401813700",
    "type": "article"
  },
  {
    "title": "Boundary Attention Guided Sparse Feature Learning for Underwater Object Tracking in Edge Computing",
    "doi": "https://doi.org/10.1145/3689824",
    "publication_date": "2024-08-24",
    "publication_year": 2024,
    "authors": "Hongyi Qiu; Ning Li; Pengfei Li; Ruitao Hou; Yuting Zhang; Yün Peng",
    "corresponding_authors": "",
    "abstract": "Underwater visual object tracking is crucial for marine resource exploration and military security. However, due to the effect of insufficient light and turbid background in underwater scenes, efficient and accurate target tracking cannot be realized on underwater edge devices with limited computing resources. To address this problem, we design an underwater object tracking network, namely DBSF, for edge computing devices based on sparse confidence feature learning guided by differential boundary attention. Specifically, we propose a differential boundary attention distribution model to compute the object edge distribution state to enhance the accurate perception of the underwater object edge structure. Then, the differential boundary attention-guided object tracking network learns to perceive the highly discriminative sparse features on the object structure, and computes the object sparse confidence matrix, which reduces the constraints of the edge devices with limited computational resources and ensures the tracking performance. Extensive experiments demonstrate that the DBSF network achieves accurate underwater target recognition and outperforms related advanced methods.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4401842767",
    "type": "article"
  },
  {
    "title": "QR-CLIP: Introducing Explicit Knowledge for Location and Time Reasoning",
    "doi": "https://doi.org/10.1145/3689638",
    "publication_date": "2024-08-24",
    "publication_year": 2024,
    "authors": "Weimin Shi; Dehong Gao; Yuan Xiong; Zhong Zhou",
    "corresponding_authors": "",
    "abstract": "This article focuses on reasoning about the location and time behind images. Given that pre-trained vision-language models (VLMs) exhibit excellent image and text understanding capabilities, most existing methods leverage them to match visual cues with location and time-related descriptions. However, these methods cannot look beyond the actual content of an image, failing to produce satisfactory reasoning results, as such reasoning requires connecting visual details with rich external cues (e.g., relevant event contexts). To this end, we propose a novel reasoning method, QR-CLIP , that aims at enhancing the model’s ability to reason about location and time through interaction with external explicit knowledge such as Wikipedia. Specifically, QR-CLIP consists of two modules: (1) The Quantity module abstracts the image into multiple distinct representations and uses them to search and gather external knowledge from different perspectives that are beneficial to model reasoning. (2) The Relevance module filters the visual features and the searched explicit knowledge and dynamically integrates them to form a comprehensive reasoning result. Extensive experiments demonstrate the effectiveness and generalizability of QR-CLIP . On the WikiTiLo dataset, QR-CLIP boosts the accuracy of location (country) and time reasoning by 7.03% and 2.22%, respectively, over previous SOTA methods. On the more challenging TARA dataset, it improves the accuracy for location and time reasoning by 3.05% and 2.45%, respectively. The source code is at https://github.com/Shi-Wm/QR-CLIP .",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4401847474",
    "type": "article"
  },
  {
    "title": "SeSMR: Secure and Efficient Session-based Multimedia Recommendation in Edge Computing",
    "doi": "https://doi.org/10.1145/3687473",
    "publication_date": "2024-08-28",
    "publication_year": 2024,
    "authors": "Fengyin Li; Hongzhe Liu; Guangshun Li; Yilei Wang; Huiyu Zhou; Shanshan Cao; Tao Li",
    "corresponding_authors": "",
    "abstract": "Session-based multimedia recommendation in edge computing remains an important issue for boosting the utilization of services since service composition has increasingly attracted attention. Existing session-based recommendations (SBRs) model the session sequence with multilevel feature extraction in graph neural networks (GNNs). However, multilevel feature extraction in disentangled graph neural networks causes over-smoothing and privacy leakage. To address the aforementioned problems, Secure and Efficient Session-based Multimedia Recommendation (SeSMR) model is proposed. In the proposed SeSMR model, based on BGV homomorphic encryption, a ciphertext training submodel is proposed to address the privacy leakage, ensuring the security in session-based recommendation. Furthermore, based on the reinforcement of feature activation, a residual attention mechanism is proposed to mitigate over-smoothing while maintaining the independence of multiple features. Finally, based on location coding, a soft attention mechanism is proposed to improve the recommendation accuracy, by introducing the position difference information between items into intra-session and inter-session scenarios. Experiments demonstrate that both Recall and MRR metrics exhibit nearly 2%∼5% improvement.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4401946492",
    "type": "article"
  },
  {
    "title": "Cross-Modal Face Super-Resolution Based on Quasi-Siamese Domain Transfer Fusion Network",
    "doi": "https://doi.org/10.1145/3690387",
    "publication_date": "2024-08-28",
    "publication_year": 2024,
    "authors": "Jiaxing Wen; 阿部 匡伸; Zhen Han; Zhongyuan Wang; Liang Chen",
    "corresponding_authors": "",
    "abstract": "In this paper, we propose a Cross-Modal Face Super-Resolution (CMFSR) method to construct high-resolution (HR) facial images from low-resolution (LR) cross-modal facial images captured respectively by disjoint visible light (VIS) and near-infrared (NIR) cameras. Due to the coupling of modality transformation and information fusion, CMFSR is more difficult to obtain HR reconstructed results compared with traditional super-resolution. To solve this problem, a Quasi-Siamese Domain Transfer Fusion Network (QSDTFN) for CMFSR is proposed in this paper, whose two branches transfer two LR face modality to HR face modality by domain transfer respectively. Different from two completely independent branches in the traditional pseudo-siamese network, only the HR-to-LR face transfer processes of the two branches in our quasi-siamese network are independent, while the LR-to-HR face transfer processes are coupled. This coupled module called the Adaptive Weighted Domain Transfer Fusion Module (AWDTFM) disentangles the modality and identity information in the two LR faces, thus achieving modality transformation and identity information fusion simultaneously. In order to strengthen the optimization on the process of CMFSR, this method further introduces the backward QSDTFN to form a higher-level bidirectional structure with the forward QSDTFN, and specifically designs two types of losses: intra-network loss and inter-network loss, to constrain the modality and identity consistencies within one QSDTFN and between two QSDTFNs respectively. The experimental results on the challenging LR cross-modal face datasets demonstrate that the proposed method performs favorably against the state-of-the-art methods.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4401946610",
    "type": "article"
  },
  {
    "title": "S <sup>3</sup> Agent: Unlocking the Power of VLLM for Zero-Shot Multi-modal Sarcasm Detection",
    "doi": "https://doi.org/10.1145/3690642",
    "publication_date": "2024-08-29",
    "publication_year": 2024,
    "authors": "Peng Wang; Yongheng Zhang; Hao Fei; Qiguang Chen; Yukai Wang; Jiasheng Si; Wenpeng Lü; Min Li; Libo Qin",
    "corresponding_authors": "",
    "abstract": "Multi-modal sarcasm detection involves determining whether a given multi-modal input conveys sarcastic intent by analyzing the underlying sentiment. Recently, vision large language models have shown remarkable success on various of multi-modal tasks. Inspired by this, we systematically investigate the impact of vision large language models in zero-shot multi-modal sarcasm detection task. Furthermore, to capture different perspectives of sarcastic expressions, we propose a multi-view agent framework, S 3 Agent, designed to enhance zero-shot multi-modal sarcasm detection by leveraging three critical perspectives: superficial expression , semantic information , and sentiment expression . Our experiments on the MMSD2.0 dataset, which involves six models and four prompting strategies, demonstrate that our approach achieves state-of-the-art performance. Our method achieves an average improvement of 13.2% in accuracy. Moreover, we evaluate our method on the text-only sarcasm detection task, where it also surpasses baseline approaches.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4401992276",
    "type": "article"
  },
  {
    "title": "Transparent Depth Completion Using Segmentation Features",
    "doi": "https://doi.org/10.1145/3694978",
    "publication_date": "2024-09-09",
    "publication_year": 2024,
    "authors": "Boqian Liu; Haojie Li; Zhihui Wang; Tianfan Xue",
    "corresponding_authors": "",
    "abstract": "Estimating the depth of transparent objects is one of the well-known challenges of RGB-D cameras due to the reflection and refraction effects. Previously, researchers propose to correct the depth of transparent objects by using their estimated segmentation masks, because it is possible to recover the internal depth of an object just from its boundary, as illustrated by those depth-from-silhouette methods. However, these algorithms only use segmentation masks. They ignore the internal structure information from the mask segmentation features, which we argue are more useful for transparent depth estimation. In this work, we demonstrate the effectiveness of segmentation features for transparent object depth estimation. We show that it is even possible to recover the depth map just from segmentation features, without any RGB or depth map as input. Based on this observation, we propose DualTransNet which uses segmentation features for transparent depth completion. In our DualTransNet, we feed segmentation features from an extra module to the main network for better depth completion quality. Extensive experiments have shown the superiority of segmentation features as well as the state-of-the-art performance of our network.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4402354740",
    "type": "article"
  },
  {
    "title": "Reversible Data Hiding in Shared JPEG Images",
    "doi": "https://doi.org/10.1145/3695463",
    "publication_date": "2024-09-09",
    "publication_year": 2024,
    "authors": "Chunqiang Yu; S.-L. Cheng; Xianquan Zhang; Xinpeng Zhang; Zhenjun Tang",
    "corresponding_authors": "",
    "abstract": "Reversible data hiding (RDH) in encrypted images has emerged as an effective technique for securely storing and managing confidential images in the cloud. However, most RDH methods in shared images (RDHSI) are designed for uncompressed images and cannot be applied for JPEG images. To address this issue, we propose a novel RDH in shared JPEG images. Our method consists of JPEG image sharing and data hiding in JPEG shares, which are both conducted on JPEG bit-stream. Specifically, the DC appended bits (DCA) and AC appended bits (ACA) derived from the original JPEG bit-stream are shared by ( \\(k\\) , \\(n\\) ) threshold Chinese remainder theorem based secret sharing (CRTSS) with two different constraints, one for DC sharing and another for AC sharing. The constraint of DC sharing ensures that the DC coefficient shares do not overflow. The constraint of AC sharing ensures the sizes of ACA shares are less than the sizes of the original ACA so that the embedding room can be vacated from each shared JPEG bit-stream. Each data-hider can embed the secret data into the personal JPEG share. The original JPEG image can be recovered losslessly from any \\(k\\) JPEG shares. The proposed sharing and data hiding are both well compatible with the JPEG standard. Experimental results demonstrate that the proposed method not only well preserves the file size whether the JPEG shares or marked JPEG shares, but also achieves outstanding security performance and a high embedding capacity.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4402354963",
    "type": "article"
  },
  {
    "title": "Adversarial Sample Synthesis for Visual Question Answering",
    "doi": "https://doi.org/10.1145/3688848",
    "publication_date": "2024-09-16",
    "publication_year": 2024,
    "authors": "Chuanhao Li; Chenchen Jing; Zhen Li; Yuwei Wu; Yunde Jia",
    "corresponding_authors": "",
    "abstract": "Language prior is a major block to improving the generalization of visual question answering (VQA) models. Recent work has revealed that synthesizing extra training samples to balance training sets is a promising way to alleviate language priors. However, most existing methods synthesize extra samples in a manner independent of training processes, which neglect the fact that the language priors memorized by VQA models are changing during training, resulting in insufficient synthesized samples. In this article, we propose an adversarial sample synthesis method, which synthesizes different adversarial samples by adversarial masking at different training epochs to cope with the changing memorized language priors. The basic idea behind our method is to use adversarial masking to synthesize adversarial samples that will cause the model to make wrong answers. To this end, we design a generative module to carry out adversarial masking by attacking the VQA model and introduce a bias-oriented objective to supervise the training of the generative module. We couple the sample synthesis with the training process of the VQA model, which ensures that the synthesized samples at different training epochs are beneficial to the VQA model. We incorporated the proposed method into three VQA models including UpDn, LMH, and LXMERT and conducted experiments on three datasets including VQA-CP v1, VQA-CP v2, and VQA v2. Experimental results demonstrate that a large improvement of our method, such as 16.22% gains on LXMERT in the overall accuracy of VQA-CP v2.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4402793079",
    "type": "article"
  },
  {
    "title": "Improving Scene Text Retrieval via Stylized Middle Modality",
    "doi": "https://doi.org/10.1145/3696209",
    "publication_date": "2024-09-16",
    "publication_year": 2024,
    "authors": "Shipeng Zhu; J.P. Fang; Pengfei Fang; Hui Xue",
    "corresponding_authors": "",
    "abstract": "Scene text retrieval addresses the challenge of localizing and searching for all text instances within scene images based on a query text. This cross-modal task has significant applications in various domains, such as intelligent transportation systems and social media analysis. In practice, ensuring consistency of the same content between two modalities is crucial in improving retrieval accuracy. This paper addresses the issue by introducing a stylized middle modality, which fuses the graphical query text with the style of the extracted text proposal. To this end, we propose a stylized middle modality learning (SM \\({}^{2}\\) L) framework. The proposed stylized middle modality enables the network to jointly enforce constraints on visual feature coherence and text semantic feature consistency in the optimization phase, thereby minimizing the modality gap in the retrieval space. This brings in two major advantages: 1) SM \\({}^{2}\\) L will pave the way to seamlessly benefit the scene text retrieval and 2) the proposed learning paradigm enables the machine to avoid adding redundant computing resources in the inference phase. Substantial experiments demonstrate that the proposed method outperforms the state-of-the-art retrieval performance considerably.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4402794756",
    "type": "article"
  },
  {
    "title": "I2P Registration by Learning the Underlying Alignment Feature Space from Pixel-to-Point Similarities",
    "doi": "https://doi.org/10.1145/3697839",
    "publication_date": "2024-09-27",
    "publication_year": 2024,
    "authors": "Yunda Sun; Lin Zhang; Zhong Wang; Yang Chen; Shengjie Zhao; Yicong Zhou",
    "corresponding_authors": "",
    "abstract": "Estimating the relative pose between a camera and a LiDAR holds paramount importance in facilitating complex task execution within multi-agent systems. Nonetheless, current methodologies encounter two primary limitations. First, amid the cross-modal feature extraction, they typically employ separate modal branches to extract cross-modal features from images and point clouds. This approach results in the feature spaces of images and point clouds being misaligned, thereby reducing the robustness of establishing correspondences. Second, due to the scale differences between images and point clouds, one-to-many pixel-point correspondences are inevitably encountered, which will mislead the pose optimization. To address these challenges, we propose a framework named \\(\\text{I2P}_{\\text{ppsim}}\\) ( I mage to P oint cloud registration by learning the underlying alignment feature space from P ixel-to- P oint SIM ilarities). Central to \\(\\text{I2P}_{\\text{ppsim}}\\) is a shared feature alignment module (SFAM). It is designed under on a coarse-to-fine architecture and uses a weight-sharing network to construct an alignment feature space. Benefiting from SFAM, \\(\\text{I2P}_{\\text{ppsim}}\\) can effectively identify the co-view regions between images and point clouds and establish high-reliability 2D-3D correspondences. Moreover, to mitigate the one-to-many correspondence issue, we introduce a similarity maximization strategy termed point-max. This strategy effectively filters out outliers, thereby establishing accurate 2D-3D correspondences. To evaluate the efficacy of our framework, we conduct extensive experiments on KITTI Odometry and Oxford Robotcar. The results corroborate the effectiveness of our framework in improving image-to-point cloud registration. To make our results reproducible, the source codes have been released at https://cslinzhang.github.io/I2P .",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4402901625",
    "type": "article"
  },
  {
    "title": "Multi-Scale Feature Attention Fusion for Image Splicing Forgery Detection",
    "doi": "https://doi.org/10.1145/3698770",
    "publication_date": "2024-10-07",
    "publication_year": 2024,
    "authors": "Enji Liang; Kuiyuan Zhang; Zhongyun Hua; Xiaohua Jia",
    "corresponding_authors": "",
    "abstract": "Image splicing is a widely occurrence image tampering technology. With the rapid development of digital image processing technology, detecting image splicing forgery has become significantly challenging. Although various methods have been devised to identify such tampered images, existing approaches have not achieved optimal performance due to limitations in effectively leveraging feature maps of different scales. To address this issue, we propose a novel method for image splicing forgery detection called multi-scale feature attention fusion network (MFAF-Net). We propose a multi-scale atrous feature attention (MAFA) module designed to capture rich contextual features for multi-scale high-level feature fusion. Additionally, we present the multi-branch attention mechanism (MBAM) module to fuse contextual information from various branches for low-level features. This integration enhances the capability of low-level features to produce more refined pixel-level attention. We employ the weighted binary cross-entropy loss and dice loss in the MFAF-Net to overcome the imbalance between positive and negative samples. Extensive experiments demonstrate that the proposed MFAF-Net outperforms state-of-the-art methods. Robustness experiments also show our model exhibits image splicing forgery detection robustness under common attacks.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4403185546",
    "type": "article"
  },
  {
    "title": "Bias Mitigation and Representation Optimization for Noise-Robust Cross-modal Retrieval",
    "doi": "https://doi.org/10.1145/3700596",
    "publication_date": "2024-10-14",
    "publication_year": 2024,
    "authors": "Yu Liu; Haipeng Chen; Guihe Qin; Jincai Song; Xun Yang",
    "corresponding_authors": "",
    "abstract": "The remarkable progress in cross-modal retrieval relies on accurately-annotated multimedia datasets. In practice, most existing datasets used for training cross-modal retrieval models are automatically collected from the Internet to reduce data collection costs. However, it inevitably contains mismatched pairs, i.e. , noisy correspondences, thus degrading the model performance. Recent advances utilize the predicted similarity distribution of individual samples for noise validation and correction, which easily faces two challenging dilemmas: 1) confirmation bias and 2) unstable performance with increasing noise. In light of the above, we propose a generalized B ias M itigation and R epresentation O ptimization framework (BMRO). Specifically, we propose a Bias Estimator (BE) to estimate the unbiased confidence factor of a sample by contrasting it against its nearest neighbors. Unbiased confidence factor can precisely adjust sample contribution and enhance accurate sample division. This facilitates the Adaptive Representation Optimizer (ARO) in providing tailored optimization strategies for clean and noisy samples. ARO performs contrastive learning between clean samples and generated hard samples, thus promoting the generalizability and robustness of the representation. Besides, it utilizes complementary learning to reduce incorrect guidance from noisy samples. Extensive experiments on five visual-text benchmarks verify that our BMRO can significantly improve the matching accuracy and performance stability against noisy correspondences.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4403396149",
    "type": "article"
  },
  {
    "title": "Decomposed Prototype Learning for Few-Shot Scene Graph Generation",
    "doi": "https://doi.org/10.1145/3700877",
    "publication_date": "2024-10-21",
    "publication_year": 2024,
    "authors": "X. Y. Li; Jun Xiao; Guikun Chen; Yinfu Feng; Yi Yang; An-An Liu; Long Chen",
    "corresponding_authors": "",
    "abstract": "Today's scene graph generation (SGG) models typically require abundant manual annotations to learn new predicate types. Therefore, it is difficult to apply them to real-world applications with massive uncommon predicate categories whose annotations are hard to collect. In this paper, we focus on Few-Shot SGG (FSSGG) , which encourages SGG models to be able to quickly transfer previous knowledge and recognize unseen predicates well with only a few examples. However, current methods for FSSGG are hindered by the high intra-class variance of predicate categories in SGG: On one hand, each predicate category commonly has multiple semantic meanings under different contexts. On the other hand, the visual appearance of relation triplets with the same predicate differs greatly under different subject-object compositions. Such great variance of inputs makes it hard to learn generalizable representation for each predicate category with current few-shot learning (FSL) methods. However, we found that this intra-class variance of predicates is highly related to the composed subjects and objects. To model the intra-class variance of predicates with subject-object context, we propose a novel Decomposed Prototype Learning (DPL) model for FSSGG. Specifically, we first construct a decomposable prototype space to capture diverse semantics and visual patterns of subjects and objects for predicates by decomposing them into multiple prototypes. Afterwards, we integrate these prototypes with different weights to generate query-adaptive predicate representation with more reliable semantics for each query sample. We conduct extensive experiments and compare with various baseline methods to show the effectiveness of our method.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4403600844",
    "type": "article"
  },
  {
    "title": "Noise-Resistance Learning via Multi-Granularity Consistency for Unsupervised Domain Adaptive Person Re-Identification",
    "doi": "https://doi.org/10.1145/3702328",
    "publication_date": "2024-11-02",
    "publication_year": 2024,
    "authors": "Yangchun Zhu; Yufei Zheng; Jiawei Liu; Yao Li; Zheng-Jun Zha",
    "corresponding_authors": "",
    "abstract": "Unsupervised domain adaptive person re-identification aims at adapting the re-identification model trained on a labeled source domain to an unlabeled target domain. The mainstream pipeline alternates between clustering-based pseudo-label prediction and representation learning, but the imperfect interaction between these steps generates noisy pseudo labels that diminish the model’s effectiveness. Previous methods reduce noisy pseudo labels impact by assessing consistency only at a single granularity level, overlooking multi-level confidence analysis for better feature representation. To address the issue, we propose a novel Multi-Granularity Consistency Network (MGCN) to perform the noise-resistance learning across different granularity consistency perspectives, including prototype-wise consistency, triplet-wise consistency and list-wise consistency, to suppress the contribution of noisy samples simultaneously. Specifically, the prototype-wise consistency leverages the prototypical output affinity between teacher and student networks to evaluate the reliability of pseudo label of a target sample, thus reducing their negative impact on identity classification loss. Triplet-wise consistency focuses on the triplet distance discrepancies between the teacher and student networks to retain the reliable and informative samples that satisfy triplet distance constraint in the triplet loss, thereby facilitating more effective model training and improved performance in the target domain. Furthermore, the list-wise consistency uses accurate list-wise similarity rankings from the teacher’s memory bank to select more dependable neighboring samples in the student’s memory bank, pulling these closer in feature space to alleviate the detrimental effects of noisy labels in contrastive loss. Based on the multi-granularity consistency, MGCN evaluates the credibility of pseudo labels and adjusts their impact across three re-ID losses for effective domain adaptation. Experimental results demonstrate that our proposed method achieves significant improvements over the existing methods on multiple benchmarks.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4403996998",
    "type": "article"
  },
  {
    "title": "Dynamic Weighted Gating for Enhanced Cross-Modal Interaction in Multimodal Sentiment Analysis",
    "doi": "https://doi.org/10.1145/3702996",
    "publication_date": "2024-11-02",
    "publication_year": 2024,
    "authors": "Nan Wang; Qi Wang",
    "corresponding_authors": "",
    "abstract": "Advancements in multimodal sentiment analysis (MSA) have predominantly focused on leveraging the interdependence of text, acoustic, and visual modalities to enhance sentiment prediction. However, efficiently integrating these modalities remains a challenge. In response, we investigate the design of a practical approach to control the information flow more accurately and propose DWGCI. This multimodal model employs systematic cross-modal interactions and gating mechanisms to analyze sentiment. It ensures comprehensive feature extraction across modalities by utilizing BERT for text features, COVAREP for acoustic features, and CNNs for visual features. To improve the common feature expression of each modality and increase the dynamic interaction between modalities, we introduced a Text-Guided Multimodal Fusion Module (TGMFM). Another integral part of our model is a unique Dynamic Gating Module (DGM) strategically positioned to follow cross-modal interactions. This mechanism represents a significant innovation that can adapt to modality-specific sentiment cues with unprecedented precision dynamically. The model can make nuanced distinctions between modalities, which significantly enhances the accuracy of sentiment prediction. Our DWGCI has been subjected to rigorous testing on two widely used datasets. It demonstrated its superior performance, established a benchmark in MSA for gate fusion innovation, and highlighted its transformative potential.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4403997135",
    "type": "article"
  },
  {
    "title": "Attack-Defending Contrastive Learning for Volumetric Medical Image Zero-Watermarking",
    "doi": "https://doi.org/10.1145/3702230",
    "publication_date": "2024-11-05",
    "publication_year": 2024,
    "authors": "Xiyao Liu; Cundian Yang; Jianbiao He; Hui Fang; Gerald Schaefer; Jian Zhang; Yuesheng Zhu; Shichao Zhang",
    "corresponding_authors": "",
    "abstract": "Zero-watermarking is an emerging distortion-free copyright protection method for volumetric medical images. However, achieving both robustness against various malicious attacks and distinguishability between individual images remains challenging. In this paper, we propose a novel attack-defending contrastive learning zero-watermarking scheme (ADCL-ZW) to tackle the above challenge using deep learning-based representations. In our approach, we design an attack-defending data enrichment mechanism to enhance the watermarking robustness by generating a large number of image samples under various watermarking attacks. Subsequently, features for both watermarking distinguishability and robustness are enhanced through application of a contrastive loss. In particular, we implement a dual-stream Siamese network architecture to effectively handle both signal attacks and geometric attacks in order to enhance the wartermarking performance. Experimental results demonstrate that ADCL-ZW achieves stronger watermarking robustness and a better tradeoff between watermarking robustness and distinguishability compared with state-of-the art zero-watermarking methods. One of the highlighted metrics the false negative rate of ADCL-ZW achieves 0.01 when a fixed false positive rate is set to 1%, which is more than 13.3 times better than the benchmark methods.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4404059332",
    "type": "article"
  },
  {
    "title": "TEC-CNN: Towards Efficient Compressing Convolutional Neural Nets with Low-rank Tensor Decomposition",
    "doi": "https://doi.org/10.1145/3702641",
    "publication_date": "2024-11-05",
    "publication_year": 2024,
    "authors": "Yifan Wang; Liang Feng; Fenglin Cai; Lusi Li; Rui Wu; Jie Li",
    "corresponding_authors": "",
    "abstract": "Most state-of-the-art convolutional neural networks (CNNs) are characterised by excessive parameterisation, leading to a high computational burden. Tensor decomposition has emerged as a model reduction technique for compressing deep neural networks. Previous approaches have predominantly relied on either Tucker decomposition or Canonical Polyadic (CP) decomposition for CNNs. However, CP decomposition exhibits exceptional compression capabilities in comparison to Tucker decomposition, which results in a more pronounced accuracy loss. This paper introduces an efficient model compression method, termed TEC-CNN, designed to achieve significant compression while preserving accuracy levels comparable to those of the original models. In TEC-CNN, convolutional layers are identified to obtain convolutional kernels by analysing given models under the principles of low-rank tensor decomposition, and then, calculating the ranks of convolutional kernels. Furthermore, an efficient decomposition schema for the convolutional kernel is proposed with approximate kernel tensor for reducing parameters. Additionally, a novel format of a convolutional sequence is presented and constructed with a reduced number of parameters to replace the original convolutional layers. Finally, the effectiveness of TEC-CNN is assessed across a range of computer vision tasks. For instance, in CIFAR-100 classification, ResNet18 is compressed to 4.1 MB, while Unext, when applied to image segmentation using the International Skin Imaging Collaboration (ISIC) dataset, is reduced to 3.419 MB. When employed for fire object detection with Yolov7, TEC-CNN achieves a model size reduction of 71.6 MB. Comprehensive experimental results underscore that our approach achieves significant model compression while preserving model performance.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4404059610",
    "type": "article"
  },
  {
    "title": "DIRformer: A Novel Image Restoration Approach Based on U-shaped Transformer and Diffusion Models",
    "doi": "https://doi.org/10.1145/3703632",
    "publication_date": "2024-11-08",
    "publication_year": 2024,
    "authors": "Cong Hu; X X Wei; Xiao‐Jun Wu",
    "corresponding_authors": "",
    "abstract": "Image restoration (IR) involves the retrieval of missing or damaged image information and represents a significant challenge in the field of visual reconstruction. Currently, U-Net based Diffusion Models (DMs) display favorable results when utilized for IR tasks. However, the Diffusion Model (DM) based on U-Net demonstrates shortcomings in capturing the global context for IR. To address this issue, we propose a Novel Image Restoration Approach Based on U-shaped Transformer and Diffusion Models (DIRformer). DIRformer enhances the modeling capacity for long-range dependencies within DMs. In particular, DIRformer replaces the traditional U-Net downsampling with Patch merging, dedicated to improving detail preservation, and replaces upsampling with Dual up-sample, strategically designed to alleviate checkerboard artifacts. Besides, as a lightweight and versatile transformer-based solution for IR, DIRformer incorporates time and degradation mapping into the transformer design, all while preserving the fundamental U-shaped structural framework. We assess the efficacy of DIRformer in a multi-tasking IR setting across four datasets. The experimental performance illustrates that DIRformer achieves competitive performance on distortion metrics, including PSNR and SSIM. Remarkably, our proposed approach is almost 25× smaller and 2× faster than the existing methods while achieving comparable high performance.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4404184225",
    "type": "article"
  },
  {
    "title": "Protein Captioning: Bridging the Gap between Protein Sequences and Natural Languages",
    "doi": "https://doi.org/10.1145/3705322",
    "publication_date": "2024-11-21",
    "publication_year": 2024,
    "authors": "J. Zhang; Hehe Fan; Yi Yang",
    "corresponding_authors": "",
    "abstract": "We introduce the multimodal task of Protein Captioning , which is an easy-to-understand and flexible way for protein analysis. Compared to specific protein recognition or classification tasks, such as enzyme reaction classification and gene ontology term prediction, protein captioning provides comprehensive textural descriptions for proteins, thus playing a key role in bridging the gap between protein sequences and natural languages. To address the problem, we propose a simple yet effective method, Protein-to-Text Generative Pre-trained Transformer (P2T-GPT), to fuse multimodal embeddings and translate the chain of amino acid residues in a protein to a sequence of natural language words, i.e ., text. For the evaluation of protein captioning, we collect the ProteinCap dataset that contains 94,454 protein-text pairs. Experiments on ProteinCap demonstrate the effectiveness of the proposed P2T-GPT on protein captioning. For example, our method obtains improvements of 8.74, 10.03, and 11.05 in the BERTScore compared to the baseline model on ProteinCap- \\(\\alpha,\\beta,\\gamma\\) , respectively. As minor contributions, first, P2T-GPT provides a way to connect protein science and Large Language Models (LLMs). By appending ChatGPT, our method can interact in a conversational way to answer questions given a protein. Second, we show that protein captioning can be treated as a pre-trained task that can benefit a range of downstream tasks, to a certain extent.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4404587505",
    "type": "article"
  },
  {
    "title": "AED-PADA:Improving Generalizability of Adversarial Example Detection via Principal Adversarial Domain Adaptation",
    "doi": "https://doi.org/10.1145/3706061",
    "publication_date": "2024-12-03",
    "publication_year": 2024,
    "authors": "Heqi Peng; Yunhong Wang; Ruijie Yang; Beichen Li; Rui Wang; Yuanfang Guo",
    "corresponding_authors": "",
    "abstract": "Adversarial example detection, which can be conveniently applied in many scenarios, is important in the area of adversarial defense. Unfortunately, existing detection methods suffer from poor generalization performance, because their training process usually relies on the examples generated from a single known adversarial attack and there exists a large discrepancy between the training and unseen testing adversarial examples. To address this issue, we propose a novel method, named Adversarial Example Detection via Principal Adversarial Domain Adaptation (AED-PADA). Specifically, our approach identifies the Principal Adversarial Domains (PADs), i.e., a combination of features of the adversarial examples generated by different attacks, which possesses a large portion of the entire adversarial feature space. Subsequently, we pioneer to exploit Multi-source Unsupervised Domain Adaptation in adversarial example detection, with PADs as the source domains. Experimental results demonstrate the superior generalization ability of our proposed AED-PADA. Note that this superiority is particularly achieved in challenging scenarios characterized by employing the minimal magnitude constraint for the perturbations.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4404965268",
    "type": "article"
  },
  {
    "title": "Vision-Language Models Learn Super Images for Efficient Partially Relevant Video Retrieval",
    "doi": "https://doi.org/10.1145/3708349",
    "publication_date": "2024-12-11",
    "publication_year": 2024,
    "authors": "Taichi Nishimura; Shota Nakada; Masayoshi Kondo",
    "corresponding_authors": "",
    "abstract": "In this paper, we propose an efficient and high-performance method for partially relevant video retrieval. The method aims to retrieve long videos that contain at least one moment relevant to the input text query. The challenge lies in encoding dense frames using visual backbones. This requires models to handle the increased frames, resulting in significant computation costs for long videos. To mitigate the costs, previous studies use lightweight visual backbones, yielding sub-optimal retrieval performance due to their limited capabilities. However, it is undesirable to simply replace the backbones with high-performance large vision-and-language models (VLMs) due to their low efficiency. To address this dilemma, instead of dense frames, we focus on super images, which are created by rearranging the video frames in an \\(N\\times N\\) grid layout. This reduces the number of visual encodings to \\(\\frac{1}{N^{2}}\\) and mitigates the low efficiency of large VLMs. Based on this idea, we make two contributions. First, we explore whether VLMs generalize to super images in a zero-shot setting. To this end, we propose a method called query-attentive super image retrieval (QASIR), which attends to partial moments relevant to the input query. The zero-shot QASIR yields two discoveries: (1) it enables VLMs to generalize to super images and (2) the grid size \\(N\\) , image resolution, and VLM size are key trade-off parameters between performance and computation costs. Second, we introduce fine-tuning and hybrid QASIR that combines high- and low-efficiency models to strike a balance between performance and computation costs. This reveals two findings: (1) the fine-tuning QASIR enhances VLMs to learn super images effectively, and (2) the hybrid QASIR minimizes the performance drop of large VLMs while reducing the computation costs.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4405282415",
    "type": "article"
  },
  {
    "title": "Vulnerability Detection and Improvements of an Image Cryptosystem for Real-Time Visual Protection",
    "doi": "https://doi.org/10.1145/3708546",
    "publication_date": "2024-12-14",
    "publication_year": 2024,
    "authors": "Mohamed Zakariya Talhaoui; Zhelong Wang; Mohamed Amine Midoun; Abdelkarim Smaili; Djamel Eddine Mekkaoui; Mourad Lablack; Ke Zhang",
    "corresponding_authors": "",
    "abstract": "Chaos-based cryptosystems are regarded as highly secure techniques for image encryption. However, despite the considerable enhancement of encryption robustness provided by chaotic systems, security vulnerabilities may still arise, potentially leading to drastic damage in contexts involving sensitive data such as medical or military images. Identifying these vulnerabilities and developing corresponding countermeasures are essential to prevent security breaches and achieve higher protection. From this perspective, this research thoroughly examines the security of an image encryption scheme based on the 1D sine-powered chaotic map. This analysis identifies vulnerabilities within the scheme that can reduce it to a permutation-only scheme. Exploiting the found vulnerabilities, three distinct cryptanalysis attacks are proposed in this work. These attacks enable unauthorized individuals to replicate the encryption and decryption processes without possessing the secret key, posing significant security risks. Under ciphertext-only attack, chosen-plaintext attack, and chosen-ciphertext attack conditions, the proposed attacks demonstrate their effectiveness through simulation and experimentation. Notably, the results indicate that these attacks can be executed within seconds and using only a few special plaintext or ciphertext images. An improved version of the analyzed scheme is introduced to address the identified vulnerabilities and enhance its security and speed.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4405397049",
    "type": "article"
  },
  {
    "title": "Siamese Network-Based Detection of DeepFake Impersonation Attacks with a Person of Interest Approach",
    "doi": "https://doi.org/10.1145/3708352",
    "publication_date": "2024-12-19",
    "publication_year": 2024,
    "authors": "Khouloud Samrouth; Pia El Housseini; Olivier Déforges",
    "corresponding_authors": "",
    "abstract": "Deepfake technology presents critical cybersecurity challenges that have become more popular since easily accessible applications have become more widely available. The proliferation of fake portrait videos constitutes a serious risk to the legal system, society, and personal privacy. The publication of fraudulent explicit content starring celebrities, the circulation of fake political videos, and the use of faked impersonated videos as proof in court of law are all examples of the effects of deepfakes in the real-world. In reaction to this growing threat, we propose a simple yet efficient Person of Interest PoI Siamese network-based model to detect deepfake synthetic content in portrait images, providing a preventative measure against the growing danger of deepfakes. On one side and unlike traditional neural networks, which process inputs independently, our approach leverages a Siamese network that processes two inputs simultaneously using identical sub-networks with shared weights and parameters. This twin structure is particularly effective for our adopted Person-of-Interest (PoI) methodology, where one input is a reference image of a specific individual, and the other is an image that needs to be verified as either real or fake. By ensuring that both the reference and the suspect image are processed in the same way, the network can accurately learn and detect subtle differences, enabling it to determine whether the second image is a genuine representation of the individual or a deepfake. This makes our method particularly relevant for targeted forensic investigations or security applications where an individual’s media integrity is paramount. On the other side, our proposed method does not require any additional complex biological feature extraction. Despite this simplification, our method achieves comparable accuracy to more complex models that rely on biological feature extraction. This efficiency makes our approach practical for implementation on resource-constrained devices, such as mobile phones and Internet of Things (IoT) systems.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4405570847",
    "type": "article"
  },
  {
    "title": "On the Performance Comparisons of Native and Clientless Real-Time Screen-Sharing Technologies",
    "doi": "https://doi.org/10.1145/3437881",
    "publication_date": "2021-05-29",
    "publication_year": 2021,
    "authors": "Chun‐Ying Huang; Yun-chen Cheng; Guan-zhang Huang; Ching-Ling Fan; Cheng-Hsin Hsu",
    "corresponding_authors": "",
    "abstract": "Real-time screen-sharing provides users with ubiquitous access to remote applications, such as computer games, movie players, and desktop applications (apps), anywhere and anytime. In this article, we study the performance of different screen-sharing technologies, which can be classified into native and clientless ones. The native ones dictate that users install special-purpose software, while the clientless ones directly run in web browsers. In particular, we conduct extensive experiments in three steps. First, we identify a suite of the most representative native and clientless screen-sharing technologies. Second, we propose a systematic measurement methodology for comparing screen-sharing technologies under diverse and dynamic network conditions using different performance metrics. Last, we conduct extensive experiments and perform in-depth analysis to quantify the performance gap between clientless and native screen-sharing technologies. We found that our WebRTC-based implementation achieves the best overall performance. More precisely, it consumes a maximum of 3 Mbps bandwidth while reaching a high decoding ratio and delivering good video quality. Moreover, it leads to a steadily high decoding ratio and video quality under dynamic network conditions. By presenting the very first rigorous comparisons of the native and clientless screen-sharing technologies, this article will stimulate more exciting studies on the emerging clientless screen-sharing technologies.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3166037245",
    "type": "article"
  },
  {
    "title": "Invertible Grayscale with Sparsity Enforcing Priors",
    "doi": "https://doi.org/10.1145/3451993",
    "publication_date": "2021-07-22",
    "publication_year": 2021,
    "authors": "Yong Du; Yangyang Xu; Taizhong Ye; Qiang Wen; Chufeng Xiao; Junyu Dong; Guoqiang Han; Shengfeng He",
    "corresponding_authors": "",
    "abstract": "Color dimensionality reduction is believed as a non-invertible process, as re-colorization results in perceptually noticeable and unrecoverable distortion. In this article, we propose to convert a color image into a grayscale image that can fully recover its original colors, and more importantly, the encoded information is discriminative and sparse, which saves storage capacity. Particularly, we design an invertible deep neural network for color encoding and decoding purposes. This network learns to generate a residual image that encodes color information, and it is then combined with a base grayscale image for color recovering. In this way, the non-differentiable compression process (e.g., JPEG) of the base grayscale image can be integrated into the network in an end-to-end manner. To further reduce the size of the residual image, we present a specific layer to enhance Sparsity Enforcing Priors (SEP), thus leading to negligible storage space. The proposed method allows color embedding on a sparse residual image while keeping a high, 35dB PSNR on average. Extensive experiments demonstrate that the proposed method outperforms state-of-the-arts in terms of image quality and tolerability to compression.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3184556287",
    "type": "article"
  },
  {
    "title": "MFECN: Multi-level Feature Enhanced Cumulative Network for Scene Text Detection",
    "doi": "https://doi.org/10.1145/3440087",
    "publication_date": "2021-07-22",
    "publication_year": 2021,
    "authors": "Zhandong Liu; Wengang Zhou; Houqiang Li",
    "corresponding_authors": "",
    "abstract": "Recently, many scene text detection algorithms have achieved impressive performance by using convolutional neural networks. However, most of them do not make full use of the context among the hierarchical multi-level features to improve the performance of scene text detection. In this article, we present an efficient multi-level features enhanced cumulative framework based on instance segmentation for scene text detection. At first, we adopt a Multi-Level Features Enhanced Cumulative ( MFEC ) module to capture features of cumulative enhancement of representational ability. Then, a Multi-Level Features Fusion ( MFF ) module is designed to fully integrate both high-level and low-level MFEC features, which can adaptively encode scene text information. To verify the effectiveness of the proposed method, we perform experiments on six public datasets (namely, CTW1500, Total-text, MSRA-TD500, ICDAR2013, ICDAR2015, and MLT2017), and make comparisons with other state-of-the-art methods. Experimental results demonstrate that the proposed Multi-Level Features Enhanced Cumulative Network (MFECN) detector can well handle scene text instances with irregular shapes (i.e., curved, oriented, and horizontal) and achieves better or comparable results.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3185705915",
    "type": "article"
  },
  {
    "title": "DSI",
    "doi": "https://doi.org/10.1145/1671954.1671957",
    "publication_date": "2010-02-01",
    "publication_year": 2010,
    "authors": "Bo Yang",
    "corresponding_authors": "Bo Yang",
    "abstract": "Considerable research has been done on the content-based multimedia delivery and access in distributed data repositories. As noted in the literature, there is always a trade-off between multimedia quality and access speed. In addition, the overall performance is greatly determined by the distribution of the multimedia data. In this article, an unsupervised multimedia semantic integration approach for a distributed infrastructure, the Distributed Semantic Indexing (DSI), is presented that addresses both the data quality and search performance. With the ability of summarizing content information and guiding data distribution, the proposed approach is distinguished by: (1) logic-based representation and concise abstraction of the semantic contents of multimedia data, which are further integrated to form a general overview of a multimedia data repository—content signature; (2) application of linguistic relationships to construct a hierarchical metadata based on the content signatures allowing imprecise queries; and (3) achieving the optimal performance in terms of search cost. The fundamental structure of the proposed model is presented. The proposed scheme has been simulated and the simulation results are analyzed and compared against several other approaches that have been advocated in the literature.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W1985468885",
    "type": "article"
  },
  {
    "title": "On Optimizing Adaptive Algorithms Based on Rebuffering Probability",
    "doi": "https://doi.org/10.1145/3092837",
    "publication_date": "2017-06-28",
    "publication_year": 2017,
    "authors": "Piotr Wiśniewski; Jordi Mongay Batalla; Andrzej Bęben; Piotr Krawiec; Andrzej Chydziński",
    "corresponding_authors": "",
    "abstract": "Traditionally, video adaptive algorithms aim to select the representation that better fits to the current download rate. In recent years, a number of new approaches appeared that take into account the buffer occupancy and the probability of video rebuffering as important indicators of the representation to be selected. We propose an optimization of the existing algorithm based on rebuffering probability and argue that the algorithm should avoid the situations when the client buffer is full and the download is stopped, since these situations decrease the efficiency of the algorithm. Reducing full buffer states does not increase the rebuffering probability thanks to a clever management of the client buffer, which analyses the buffer occupancy and downloads higher bitrate representations only in the case of high buffer occupancy.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2733358326",
    "type": "article"
  },
  {
    "title": "ALP",
    "doi": "https://doi.org/10.1145/2656203",
    "publication_date": "2015-01-07",
    "publication_year": 2015,
    "authors": "Kiana Calagari; Mohammad Reza Pakravan; Shervin Shirmohammadi; Mohamed Hefeeda",
    "corresponding_authors": "",
    "abstract": "There has been an increasing demand for interactive video transmission over the Internet for applications such as video conferencing, video calls, and telepresence applications. These applications are increasingly moving towards providing High Definition (HD) video quality to users. A key challenge in these applications is to preserve the quality of video when it is transported over best-effort networks that do not guarantee lossless transport of video packets. In such conditions, it is important to protect the transmitted video by using intelligent and adaptive protection schemes. Applications such as HD video conferencing require live interaction among participants, which limits the overall delay the system can tolerate. Therefore, the protection scheme should add little or no extra delay to video transport. We propose a novel Adaptive Loss Protection (ALP) scheme for interactive HD video applications such as video conferencing and video chats. This scheme adds negligible delay to the transmission process and is shown to achieve better quality than other schemes in lossy networks. The proposed ALP scheme adaptively applies four different protection modes to cope with the dynamic network conditions, which results in high video quality in all network conditions. Our ALP scheme consists of four protection modes ; each of these modes utilizes a protection method . Two of the modes rely on the state-of-the-art protection methods, and we propose a new Integrated Loss Protection (ILP) method for the other two modes. In the ILP method we integrate three factors for distributing the protection among packets. These three factors are error propagation, region of interest and header information. In order to decide when to switch between the protection modes, a new metric is proposed based on the effectiveness of each mode in performing protection, rather than just considering network statistics such as packet loss rate. Results show that by using this metric not only the overall quality will be improved but also the variance of quality will decrease. One of the main advantages of the proposed ALP scheme is that it does not increase the bit rate overhead in poor network conditions. Our results show a significant gain in video quality, up to 3dB PSNR improvement is achieved using our scheme, compared to protecting all packets equally with the same amount of overhead.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2021042063",
    "type": "article"
  },
  {
    "title": "Double Verification Secret Sharing Mechanism Based on Adaptive Pixel Pair Matching",
    "doi": "https://doi.org/10.1145/2700291",
    "publication_date": "2015-02-05",
    "publication_year": 2015,
    "authors": "Pei-Yu Lin",
    "corresponding_authors": "Pei-Yu Lin",
    "abstract": "Verifiability is essential for the secret sharing approach, which allows the involved participants to detect cheaters during the secret retrieval process. In this article, we propose a double verification secret sharing (DVSS) mechanism that can not only prevent fraudulent participants but also satisfy the requirements of secret payload, camouflage, image fidelity and lossless revealed secret. DVSS offers double verification process to enhance the cheater detectability; experimental results reveal that the designed scheme can share larger secret capacity and retain superior image quality than the related secret sharing methods.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2074207814",
    "type": "article"
  },
  {
    "title": "Depth Personalization and Streaming of Stereoscopic Sports Videos",
    "doi": "https://doi.org/10.1145/2890103",
    "publication_date": "2016-03-08",
    "publication_year": 2016,
    "authors": "Kiana Calagari; Tarek Elgamal; Khaled Diab; Krzysztof Templin; Piotr Didyk; Wojciech Matusik; Mohamed Hefeeda",
    "corresponding_authors": "",
    "abstract": "Current three-dimensional displays cannot fully reproduce all depth cues used by a human observer in the real world. Instead, they create only an illusion of looking at a three-dimensional scene. This leads to a number of challenges during the content creation process. To assure correct depth reproduction and visual comfort, either the acquisition setup has to be carefully controlled or additional postprocessing techniques have to be applied. Furthermore, these manipulations need to account for a particular setup that is used to present the content, for example, viewing distance or screen size. This creates additional challenges in the context of personal use when stereoscopic content is shown on TV sets, desktop monitors, or mobile devices. We address this problem by presenting a new system for streaming stereoscopic content. Its key feature is a computationally efficient depth adjustment technique which can automatically optimize viewing experience for videos of field sports such as soccer, football, and tennis. Additionally, the method enables depth personalization to allow users to adjust the amount of depth according to their preferences. Our stereoscopic video streaming system was implemented, deployed, and tested with real users.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2296703396",
    "type": "article"
  },
  {
    "title": "Introduction",
    "doi": "https://doi.org/10.1145/2661333",
    "publication_date": "2014-10-01",
    "publication_year": 2014,
    "authors": "Gheorghiță Ghinea; Christian Timmerer; Weisi Lin; Stephen R. Gulliver",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W39866214",
    "type": "article"
  },
  {
    "title": "Adaptive Adversarial Logits Pairing",
    "doi": "https://doi.org/10.1145/3616375",
    "publication_date": "2023-08-21",
    "publication_year": 2023,
    "authors": "Shangxi Wu; Jitao Sang; Kaiyan Xu; Guanhua Zheng; Changsheng Xu",
    "corresponding_authors": "",
    "abstract": "Adversarial examples provide an opportunity as well as impose a challenge for understanding image classification systems. Based on the analysis of the adversarial training solution—Adversarial Logits Pairing (ALP), we observed in this work that: (1) The inference of adversarially robust model tends to rely on fewer high-contribution features compared with vulnerable ones. (2) The training target of ALP does not fit well to a noticeable part of samples, where the logits pairing loss is overemphasized and obstructs minimizing the classification loss. Motivated by these observations, we design an Adaptive Adversarial Logits Pairing (AALP) solution by modifying the training process and training target of ALP. Specifically, AALP consists of an adaptive feature optimization module with Guided Dropout to systematically pursue fewer high-contribution features, and an adaptive sample weighting module by setting sample-specific training weights to balance between logits pairing loss and classification loss. The proposed AALP solution demonstrates superior defense performance on multiple datasets with extensive experiments.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3028437822",
    "type": "article"
  },
  {
    "title": "E-detector: Asynchronous Spatio-temporal for Event-based Object Detection in Intelligent Transportation System",
    "doi": "https://doi.org/10.1145/3584361",
    "publication_date": "2023-02-18",
    "publication_year": 2023,
    "authors": "Shixiong Zhang; Wenmin Wang; Honglei Li; Shenyong Zhang",
    "corresponding_authors": "",
    "abstract": "In intelligent transportation systems, various sensors, including radar and conventional frame cameras, are used to improve system robustness in various challenging scenarios. An event camera is a novel bio-inspired sensor that has attracted the interest of several researchers. It provides a form of neuromorphic vision to capture motion information asynchronously at high speeds. Thus, it possesses advantages for intelligent transportation systems that conventional frame cameras cannot match, such as high temporal resolution, high dynamic range, as well as sparse and minimal motion blur. Therefore, this study proposes an E-detector based on event cameras that asynchronously detect moving objects. The main innovation of our framework is that the spatiotemporal domain of the event camera can be adjusted according to different velocities and scenarios. It overcomes the inherent challenges that traditional cameras face when detecting moving objects in complex environments, such as high speed, complex lighting, and motion blur. Moreover, our approach adopts filter models and transfer learning to improve the performance of event-based object detection. Experiments have shown that our method can detect high-speed moving objects better than conventional cameras using state-of-the-art detection algorithms. Thus, our proposed approach is extremely competitive and extensible, as it can be extended to other scenarios concerning high-speed moving objects. The study findings are expected to unlock the potential of event cameras in intelligent transportation system applications.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4321329202",
    "type": "article"
  },
  {
    "title": "A <sup>2</sup> SC: Adversarial Attacks on Subspace Clustering",
    "doi": "https://doi.org/10.1145/3587097",
    "publication_date": "2023-03-11",
    "publication_year": 2023,
    "authors": "Yikun Xu; Xingxing Wei; Pengwen Dai; Xiaochun Cao",
    "corresponding_authors": "",
    "abstract": "Many studies demonstrate that supervised learning techniques are vulnerable to adversarial examples. However, adversarial threats in unsupervised learning have not drawn sufficient scholarly attention. In this article, we formally address the unexplored adversarial attacks in the equally important unsupervised clustering field and propose the concept of the adversarial set and adversarial set attack for clustering. To illustrate the basic idea, we design a novel adversarial space-mapping attack algorithm to confuse subspace clustering, one of the mainstream branches of unsupervised clustering. It maps a sample into one wrong class by moving it towards the closest point on the linear subspace of the target class, that is, along the normal of the closest point. This simple single-step algorithm has the power to craft the adversarial set where the image samples can be wrongly clustered, even into the targeted labels. Empirical results on different image datasets verify the effectiveness and superiority of our algorithm. We further show that deep supervised learning algorithms (such as VGG and ResNet) are also vulnerable to our crafted adversarial set, which illustrates the good cross-task transferability of the adversarial set.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4323923980",
    "type": "article"
  },
  {
    "title": "Neural Network Assisted Depth Map Packing for Compression Using Standard Hardware Video Codecs",
    "doi": "https://doi.org/10.1145/3588440",
    "publication_date": "2023-03-18",
    "publication_year": 2023,
    "authors": "Matti Siekkinen; Teemu Kämäräinen",
    "corresponding_authors": "",
    "abstract": "Depth maps are needed by various graphics rendering and processing operations. Depth map streaming is often necessary when such operations are performed in a distributed system and it requires in most cases fast performing compression, which is why video codecs are often used. Hardware implementations of standard video codecs enable relatively high resolution and frame rate combinations, even on resource constrained devices, but unfortunately those implementations do not currently support RGB+depth extensions. However, they can be used for depth compression by first packing the depth maps into RGB or YUV frames. We investigate depth map compression using a combination of depth map packing followed by encoding with a standard video codec. We show that the precision at which depth maps are packed has a large and nontrivial impact on the resulting error caused by the combination of the packing scheme and lossy compression when the bitrate is constrained. Consequently, we propose a variable precision packing scheme assisted by a neural network model that predicts the optimal precision for each depth map given a bitrate constraint. We demonstrate that the model yields near optimal predictions and that it can be integrated into a game engine with very low overhead using modern hardware.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4327814765",
    "type": "article"
  },
  {
    "title": "Multi-Agent Semi-Siamese Training for Long-Tail and Shallow Face Learning",
    "doi": "https://doi.org/10.1145/3594669",
    "publication_date": "2023-04-26",
    "publication_year": 2023,
    "authors": "Yichun Tai; Hailin Shi; Dan Zeng; Hang Du; Yibo Hu; Zicheng Zhang; Zhijiang Zhang; Tao Mei",
    "corresponding_authors": "",
    "abstract": "With the recent development of deep convolutional neural networks and large-scale datasets, deep face recognition has made remarkable progress and been widely used in various applications. However, unlike the existing public face datasets, in many real-world scenarios of face recognition, the depth of the training dataset is shallow, which means that only two face images are available for each ID. With the non-uniform increase of samples, such issue is converted to a more general case, known as long-tail face learning, which suffers from data imbalance and intra-class diversity dearth simultaneously. These adverse conditions damage the training and result in the decline of model performance. Based on Semi-Siamese Training, we introduce an advanced solution, named Multi-Agent Semi-Siamese Training (MASST), to address these problems. MASST includes a probe network and multiple gallery agents—the former aims to encode the probe features, and the latter constitutes a stack of networks that encode the prototypes (gallery features). For each training iteration, the gallery network, which is sequentially rotated from the stack, and the probe network form a pair of Semi-Siamese networks. We give the theoretical and empirical analysis that, given the long-tail (or shallow) data and training loss, MASST smooths the loss landscape and satisfies the Lipschitz continuity with the help of multiple agents and the updating gallery queue. The proposed method is out of extra-dependency, and thus can be easily integrated with the existing loss functions and network architectures. It is worth noting that although multiple gallery agents are employed for training, only the probe network is needed for inference, without increasing the inference cost. Extensive experiments and comparisons demonstrate the advantages of MASST for long-tail and shallow face learning.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4367055902",
    "type": "article"
  },
  {
    "title": "Context-Aware 3D Points of Interest Detection via Spatial Attention Mechanism",
    "doi": "https://doi.org/10.1145/3597026",
    "publication_date": "2023-05-11",
    "publication_year": 2023,
    "authors": "Zhenyu Shu; Ling Gao; Shun Yi; Fangyu Wu; Xin Ding; Ting Wan; Shiqing Xin",
    "corresponding_authors": "",
    "abstract": "Detecting points of interest is a fundamental problem in 3D shape analysis and can be beneficial to various tasks in multimedia processing. Traditional learning-based detection methods usually rely on each vertex’s geometric features to discriminate points of interest from other vertices. Observing that points of interest are related to not only geometric features on themselves but also the geometric features of surrounding vertices, we propose a novel context-aware 3D points of interest detection algorithm by adopting the spatial attention mechanism in this article. By designing a context attention module, our approach presents a novel deep neural network to simultaneously pay attention to the geometric features of vertices and their local contexts during extracting points of interest. To obtain satisfactory extraction results, our method adaptively assigns different weights to those features in a data-driven way. Extensive experimental results on SHREC 2007, SHREC 2011, and SHREC 2014 datasets show that our algorithm achieves superior performance over existing methods.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4376136253",
    "type": "article"
  },
  {
    "title": "Zero-shot Scene Graph Generation via Triplet Calibration and Reduction",
    "doi": "https://doi.org/10.1145/3604284",
    "publication_date": "2023-06-08",
    "publication_year": 2023,
    "authors": "Jiankai Li; Yunhong Wang; Weixin Li",
    "corresponding_authors": "",
    "abstract": "Scene Graph Generation (SGG) plays a pivotal role in downstream vision-language tasks. Existing SGG methods typically suffer from poor compositional generalizations on unseen triplets. They are generally trained on incompletely annotated scene graphs that contain dominant triplets and tend to bias toward these seen triplets during inference. To address this issue, we propose a Triplet Calibration and Reduction (T-CAR) framework in this article. In our framework, a triplet calibration loss is first presented to regularize the representations of diverse triplets and to simultaneously excavate the unseen triplets in incompletely annotated training scene graphs. Moreover, the unseen space of scene graphs is usually several times larger than the seen space, since it contains a huge number of unrealistic compositions. Thus, we propose an unseen space reduction loss to shift the attention of excavation to reasonable unseen compositions to facilitate the model training. Finally, we propose a contextual encoder to improve the compositional generalizations of unseen triplets by explicitly modeling the relative spatial relations between subjects and objects. Extensive experiments show that our approach achieves consistent improvements for zero-shot SGG over state-of-the-art methods. The code is available at https://github.com/jkli1998/T-CAR .",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4380052056",
    "type": "article"
  },
  {
    "title": "S3Mix: Same Category Same Semantics Mixing for Augmenting Fine-grained Images",
    "doi": "https://doi.org/10.1145/3605892",
    "publication_date": "2023-06-26",
    "publication_year": 2023,
    "authors": "Zi-Chao Zhang; Zhen-Duo Chen; Zhen-Yu Xie; Xin Luo; Xin-Shun Xu",
    "corresponding_authors": "",
    "abstract": "Data augmentation is a common technique to improve the generalization performance of models for image classification. Although methods such as Mixup and CutMix that mix images randomly are indeed instrumental in general image classification, randomly swapping or masking regions is not friendly to fine-grained images, since the key to fine-grained image classification precisely lies in discriminative and informative regions, and it is unreasonable to generate labels solely consistent with the proportion of synthesis. Some erasing methods like Cutout even endanger fine-grained image classification because of erasing the discriminative regions by chance. In this article, we propose the Same Category Same Semantics Mixing method (S3Mix) corresponding to the characteristics of fine-grained images. Specifically, we limit the mixture to regions of the same category and semantics. The core of the method is two constraints. The exchange with the semantic region ensures the discrimination and semantics integrity of the generated image, and the exchange in the same class avoids the problem of unreasonable label generation. At the same time, we propose a homology loss to promote the semantic relationship between the generated positive image pairs. Experiments have been conducted on four fine-grained datasets, and the results show the proposed method is superior to the traditional image augmentation methods as well as some fine-grained data augmentation methods.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4382024791",
    "type": "article"
  },
  {
    "title": "Double High-Order Correlation Preserved Robust Multi-View Ensemble Clustering",
    "doi": "https://doi.org/10.1145/3612923",
    "publication_date": "2023-08-03",
    "publication_year": 2023,
    "authors": "Xiaojia Zhao; Tingting Xu; Qiangqiang Shen; Youfa Liu; Yongyong Chen; Jingyong Su",
    "corresponding_authors": "",
    "abstract": "Ensemble clustering (EC), utilizing multiple basic partitions (BPs) to yield a robust consensus clustering, has shown promising clustering performance. Nevertheless, most current algorithms suffer from two challenging hurdles: (1) a surge of EC-based methods only focus on pair-wise sample correlation while fully ignoring the high-order correlations of diverse views. (2) they deal directly with the co-association (CA) matrices generated from BPs, which are inevitably corrupted by noise and thus degrade the clustering performance. To address these issues, we propose a novel Double High-Order Correlation Preserved Robust Multi-View Ensemble Clustering (DC-RMEC) method, which preserves the high-order inter-view correlation and the high-order correlation of original data simultaneously. Specifically, DC-RMEC constructs a hypergraph from BPs to fuse high-level complementary information from different algorithms and incorporates multiple CA-based representations into a low-rank tensor to discover the high-order relevance underlying CA matrices, such that double high-order correlation of multi-view features could be dexterously uncovered. Moreover, a marginalized denoiser is invoked to gain robust view-specific CA matrices. Furthermore, we develop a unified framework to jointly optimize the representation tensor and the result matrix. An effective iterative optimization algorithm is designed to optimize our DC-RMEC model by resorting to the alternating direction method of multipliers. Extensive experiments on seven real-world multi-view datasets have demonstrated the superiority of DC-RMEC compared with several state-of-the-art multi-view ensemble clustering methods.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4385548067",
    "type": "article"
  },
  {
    "title": "Self-Supervised Consistency Based on Joint Learning for Unsupervised Person Re-identification",
    "doi": "https://doi.org/10.1145/3612926",
    "publication_date": "2023-08-19",
    "publication_year": 2023,
    "authors": "Xulei Lou; Tinghui Wu; Haifeng Hu; Dihu Chen",
    "corresponding_authors": "",
    "abstract": "Recently, unsupervised domain adaptive person re-identification (Re-ID) methods have been extensively studied thanks to not requiring annotations, and they have achieved excellent performance. Most of the existing methods aim to train the Re-ID model for learning a discriminative feature representation. However, they usually only consider training the model to learn a global feature of a pedestrian image, but neglecting the local feature, which restricts further improvement of model performance. To address this problem, two local branches are added to the networks, aiming to allow the model to focus on the local feature containing identity information. Furthermore, we propose a self-supervised consistency constraint to further improve robustness of the model. Specifically, the self-supervised consistency constraint uses the basic data augmentation operations without other auxiliary networks, which can improve performance of the model effectively. Then, a learnable memory matrix is designed to store the mapping vectors that maps person features into probability distributions. Finally, extensive experiments are conducted on multiple commonly used person Re-ID datasets to verify the effectiveness of the proposed generative adversarial networks fusing global and local features. Experimental results reveal that our method achieves results comparable to state-of-the-art methods.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4386001436",
    "type": "article"
  },
  {
    "title": "Black-box Attack against Self-supervised Video Object Segmentation Models with Contrastive Loss",
    "doi": "https://doi.org/10.1145/3617502",
    "publication_date": "2023-08-25",
    "publication_year": 2023,
    "authors": "Ying Chen; Rui Yao; Yong Zhou; Jiaqi Zhao; Bing Liu; Abdulmotaleb El Saddik",
    "corresponding_authors": "",
    "abstract": "Deep learning models have been proven to be susceptible to malicious adversarial attacks, which manipulate input images to deceive the model into making erroneous decisions. Consequently, the threat posed to these models serves as a poignant reminder of the necessity to focus on the model security of object segmentation algorithms based on deep learning. However, the current landscape of research on adversarial attacks primarily centers around static images, resulting in a dearth of studies on adversarial attacks targeting Video Object Segmentation (VOS) models. Given that a majority of self-supervised VOS models rely on affinity matrices to learn feature representations of video sequences and achieve robust pixel correspondence, our investigation has delved into the impact of adversarial attacks on self-supervised VOS models. In response, we propose an innovative black-box attack method incorporating contrastive loss. This method induces segmentation errors in the model through perturbations in the feature space and the application of a pixel-level loss function. Diverging from conventional gradient-based attack techniques, we adopt an iterative black-box attack strategy that incorporates contrastive loss across the current frame, any two consecutive frames, and multiple frames. Through extensive experimentation conducted on the DAVIS 2016 and DAVIS 2017 datasets using three self-supervised VOS models and one unsupervised VOS model, we unequivocally demonstrate the potent attack efficiency of the black-box approach. Remarkably, the J&amp;F metric value experiences a significant decline of up to 50.08% post-attack.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4386162746",
    "type": "article"
  },
  {
    "title": "Asymmetric Dual-Decoder U-Net for Joint Rain and Haze Removal",
    "doi": "https://doi.org/10.1145/3628451",
    "publication_date": "2023-10-18",
    "publication_year": 2023,
    "authors": "Yuan Feng; Yaojun Hu; Pengfei Fang; Sheng Liu; Yanhong Yang; Shengyong Chen",
    "corresponding_authors": "",
    "abstract": "This work studies the multi-weather restoration problem. In real-life scenarios, rain and haze, two often co-occurring common weather phenomena, can greatly degrade the clarity and quality of the scene images, leading to a performance drop in the visual applications, such as autonomous driving. However, jointly removing the rain and haze in scene images is ill-posed and challenging, where the existence of haze and rain and the change of atmosphere light, can both degrade the scene information. Current methods focus on the contamination removal part, thus ignoring the restoration of the scene information affected by the change of atmospheric light. We propose a novel deep neural network, named Asymmetric Dual-decoder U-Net (ADU-Net), to address the aforementioned challenge. The ADU-Net produces both the contamination residual and the scene residual to efficiently remove the contamination while preserving the fidelity of the scene information. Extensive experiments show our work outperforms the existing state-of-the-art methods by a considerable margin in both synthetic data and real-world data benchmarks, including RainCityscapes, BID Rain, and SPA-Data. For instance, we improve the state-of-the-art PSNR value by 2.26/4.57 on the RainCityscapes/SPA-Data, respectively. Codes will be made available freely to the research community.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4387735423",
    "type": "article"
  },
  {
    "title": "Hierarchical Synergy-Enhanced Multimodal Relational Network for Video Question Answering",
    "doi": "https://doi.org/10.1145/3630101",
    "publication_date": "2023-10-25",
    "publication_year": 2023,
    "authors": "Min Peng; Xiaohu Shao; Yu Shi; Xiangdong Zhou",
    "corresponding_authors": "",
    "abstract": "Video question answering (VideoQA) is challenging as it requires reasoning about natural language and multimodal interactive relations. Most existing methods apply attention mechanisms to extract interactions between the question and the video or to extract effective spatio-temporal relational representations. However, these methods neglect the implication of relations between intra- and inter-modal interactions for multimodal learning, and they fail to fully exploit the synergistic effect of multiscale semantics in answer reasoning. In this article, we propose a novel hierarchical synergy-enhanced multimodal relational network (HMRNet) to address these issues. Specifically, we devise (i) a compact and unified relation-oriented interaction module that explores the relation between intra- and inter-modal interactions to enable effective multimodal learning; and (ii) a hierarchical synergistic memory unit that leverages a memory-based interaction scheme to complement and fuse multimodal semantics at multiple scales to achieve synergistic enhancement of answer reasoning. With careful design of each component, our HMRNet has fewer parameters and is computationally efficient. Extensive experiments and qualitative analyses demonstrate that the HMRNet is superior to previous state-of-the-art methods on eight benchmark datasets. We also demonstrate the effectiveness of the different components of our method.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4387933309",
    "type": "article"
  },
  {
    "title": "Graph Based Cross-Channel Transform for Color Image Compression",
    "doi": "https://doi.org/10.1145/3631710",
    "publication_date": "2023-11-07",
    "publication_year": 2023,
    "authors": "Lilong Wang; Yunhui Shi; Jin Wang; Shujun Chen; Baocai Yin; Nam Ling",
    "corresponding_authors": "",
    "abstract": "Adaptive transform coding is gaining more and more attention for better mining of image content over fixed transforms such as discrete cosine transform (DCT). As a special case, graph transform learning establishes a novel paradigm for the graph-based transforms. However, there still exists a challenge for graph transform learning-based image codecs design on natural image compression, and graph representation cannot describe regular image samples well over graph-structured data. Therefore, in this article, we propose a cross-channel graph-based transform (CCGBT) for natural color image compression. We observe that neighboring pixels having similar intensities should have similar values in the chroma channels, which means that the prominent structure of the luminance channel is related to the contours of the chrominance channels. A collaborative design of the learned graphs and their corresponding distinctive transforms lies in the assumption that a sufficiently small block can be considered smooth, meanwhile, guaranteeing the compression of the luma and chroma signals at the cost of a small overhead for coding the description of the designed luma graph. In addition, a color image compression framework based on the CCGBT is designed for comparing DCT on the classic JPEG codec. The proposed method benefits from its flexible transform block design on arbitrary sizes to exploit image content better than the fixed transform. The experimental results show that the unified graph-based transform outperforms conventional DCT, while close to discrete wavelet transform on JPEG2000 at high bit-rates.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4388463875",
    "type": "article"
  },
  {
    "title": "Detecting Post Editing of Multimedia Images using Transfer Learning and Fine Tuning",
    "doi": "https://doi.org/10.1145/3633284",
    "publication_date": "2023-11-16",
    "publication_year": 2023,
    "authors": "Simon Lucas Jonker; Malthe Andreas Lejbølle Jelstrup; Weizhi Meng; Brooke Lampe",
    "corresponding_authors": "",
    "abstract": "In the domain of general image forgery detection, a myriad of different classification solutions have been developed to distinguish a “tampered” image from a “pristine” image. In this work, we aim to develop a new method to tackle the problem of binary image forgery detection. Our approach builds upon the extensive training that state-of-the-art image classification models have undergone on regular images from the ImageNet dataset, and transfers that knowledge to the image forgery detection space. By leveraging transfer learning and fine tuning, we can fit state-of-the-art image classification models to the forgery detection task. We train the models on a diverse and evenly distributed image forgery dataset. With five models—EfficientNetB0, VGG16, Xception, ResNet50V2, and NASNet-Large—we transferred and adapted pre-trained knowledge from ImageNet to the forgery detection task. Each model was fitted, fine-tuned, and evaluated according to a set of performance metrics. Our evaluation demonstrated the efficacy of large-scale image classification models—paired with transfer learning and fine tuning—at detecting image forgeries. When pitted against a previously unseen dataset, the best-performing model of EfficientNetB0 could achieve an accuracy rate of nearly 89.7%.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4388730548",
    "type": "article"
  },
  {
    "title": "One-Bit Supervision for Image Classification: Problem, Solution, and Beyond",
    "doi": "https://doi.org/10.1145/3633779",
    "publication_date": "2023-11-24",
    "publication_year": 2023,
    "authors": "Hengtong Hu; Lingxi Xie; Xinyue Huo; Richang Hong; Qi Tian",
    "corresponding_authors": "",
    "abstract": "This article presents one-bit supervision, a novel setting of learning with fewer labels, for image classification. Instead of the training model using the accurate label of each sample, our setting requires the model to interact with the system by predicting the class label of each sample and learn from the answer whether the guess is correct, which provides one bit (yes or no) of information. An intriguing property of the setting is that the burden of annotation largely is alleviated in comparison to offering the accurate label. There are two keys to one-bit supervision: (i) improving the guess accuracy and (ii) making good use of the incorrect guesses. To achieve these goals, we propose a multi-stage training paradigm and incorporate negative label suppression into an off-the-shelf semi-supervised learning algorithm. Theoretical analysis shows that one-bit annotation is more efficient than full-bit annotation in most cases and gives the conditions of combining our approach with active learning. Inspired by this, we further integrate the one-bit supervision framework into the self-supervised learning algorithm, which yields an even more efficient training schedule. Different from training from scratch, when self-supervised learning is used for initialization, both hard example mining and class balance are verified to be effective in boosting the learning performance. However, these two frameworks still need full-bit labels in the initial stage. To cast off this burden, we utilize unsupervised domain adaptation to train the initial model and conduct pure one-bit annotations on the target dataset. In multiple benchmarks, the learning efficiency of the proposed approach surpasses that using full-bit, semi-supervised supervision.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4388977658",
    "type": "article"
  },
  {
    "title": "Secure Low-Complexity Compressive Sensing with Preconditioning Prior Regularization Reconstruction",
    "doi": "https://doi.org/10.1145/3635308",
    "publication_date": "2023-12-02",
    "publication_year": 2023,
    "authors": "Hui Huang; Di Xiao; Jia Liang",
    "corresponding_authors": "",
    "abstract": "Compressive sensing (CS), a breakthrough technology in image processing, provides a privacy-preserving layer and image reconstruction while performing sensing and recovery processes, respectively. Unfortunately, it still faces high-complexity, low-security and low-quality reconstruction challenges during image processing. Therefore, this paper presents a secure low-complexity CS scheme with preconditioning prior regularization reconstruction. More specifically, the original image is compressed by a low-complexity LFSR-based sparse circulant matrix to obtain measurements. It is worth noting that measurements achieve preliminary distribution equalization through the Tanh sequence to acquire processed measurements. Furthermore, the privacy-preserving edge processing for processed measurements can achieve high security. Finally, preconditioning prior regularization CS reconstruction is designed to improve reconstruction performance. Simulation results and analyses demonstrate that the proposed scheme can achieve low-complexity sampling, high-security and superior reconstruction performance.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4389269206",
    "type": "article"
  },
  {
    "title": "CMAF: Cross-Modal Augmentation via Fusion for Underwater Acoustic Image Recognition",
    "doi": "https://doi.org/10.1145/3636427",
    "publication_date": "2023-12-06",
    "publication_year": 2023,
    "authors": "Shih‐Wei Yang; Li-Hsiang Shen; Hong-Han Shuai; Kai‐Ten Feng",
    "corresponding_authors": "",
    "abstract": "Underwater image recognition is crucial for underwater detection applications. Fish classification has been one of the emerging research areas in recent years. Existing image classification models usually classify data collected from terrestrial environments. However, existing image classification models trained with terrestrial data are unsuitable for underwater images, as identifying underwater data is challenging due to their incomplete and noisy features. To address this, we propose a cross-modal augmentation via fusion ( CMAF ) framework for acoustic-based fish image classification. Our approach involves separating the process into two branches: visual modality and sonar signal modality, where the latter provides a complementary character feature. We augment the visual modality, design an attention-based fusion module, and adopt a masking-based training strategy with a mask-based focal loss to improve the learning of local features and address the class imbalance problem. Our proposed method outperforms the state-of-the-art methods. Our source code is available at https://github.com/WilkinsYang/CMAF.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4389386424",
    "type": "article"
  },
  {
    "title": "Learning Offset Probability Distribution for Accurate Object Detection",
    "doi": "https://doi.org/10.1145/3637214",
    "publication_date": "2023-12-13",
    "publication_year": 2023,
    "authors": "Heqian Qiu; Hongliang Li; Qingbo Wu; Hengcan Shi; Lanxiao Wang; Fanman Meng; Linfeng Xu",
    "corresponding_authors": "",
    "abstract": "Object detection combines object classification and object localization problems. Current object detection methods heavily depend on regression networks to locate objects, which are optimized with various regression loss functions to predict offsets between candidate boxes and objects. However, these regression losses are difficult to assign the appropriate penalties for samples with large offset errors, resulting in suboptimal regression networks and inaccurate object offsets. In this paper, we consider object location as offset bin classification problem, and propose a distance-aware offset bin classification network optimized with multiple binary cross entropy losses to learn various offset probability distribution, including single label distribution and distance-aware label distribution. On the one hand, it provides gradient contributions for different samples based on the bounded probability instead of previous incalculable offset error. On the other hand, it explores the distance correlations between discrete offset bins to facilitate network learning. Specifically, we discretize the continuous offset into a number of bins, and predict the probability of each offset bin, in which the probability should be higher for the offset bin closer to the target offsets, and vice versa. Furthermore, we propose an expectation-based offset prediction and a hierarchical focusing method to improve the precision of prediction. We conduct extensive experiments to evaluate the effectiveness of our method. In addition, our method can be conveniently and flexibly inserted into existing object detection methods, which consistently achieves a large gain based on popular anchor-based and anchor-free methods on the PASCAL VOC, MS-COCO, KITTI and CrowdHuman datasets. Code will be released at: https://github.com/QiuHeqian/DBC.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4389675241",
    "type": "article"
  },
  {
    "title": "Context-aware Optimization for Bandwidth-Efficient Image Analytics Offloading",
    "doi": "https://doi.org/10.1145/3638768",
    "publication_date": "2023-12-27",
    "publication_year": 2023,
    "authors": "Bo Chen; Zhisheng Yan; Klara Nahrstedt",
    "corresponding_authors": "",
    "abstract": "Convolutional Neural Networks (CNN) have given rise to numerous visual analytics applications at the edge of the Internet. The image is typically captured by cameras and then live-streamed to edge servers for analytics due to the prohibitive cost of running CNN on computation-constrained end devices. A critical component to ensure low-latency and accurate visual analytics offloading over low bandwidth networks is image compression which minimizes the amount of visual data to offload and maximizes the decoding quality of salient pixels for analytics. Despite the wide adoption, JPEG standards and traditional image compression techniques do not address the accuracy of analytics tasks, leading to ineffective compression for visual analytics offloading. Although recent machine-centric image compression techniques leverage sophisticated neural network models or hardware architecture to support the accuracy-bandwidth tradeoff, they introduce excessive latency in the visual analytics offloading pipeline. This article presents CICO, a Context-aware Image Compression Optimization framework to achieve low-bandwidth and low-latency visual analytics offloading. CICO contextualizes image compression for offloading by employing easily-computable low-level image features to understand the importance of different image regions for a visual analytics task. Accordingly, CICO can optimize the tradeoff between compression size and analytics accuracy. Extensive real-world experiments demonstrate that CICO reduces the bandwidth consumption of existing compression methods by up to 40% under comparable analytics accuracy. Regarding the low-latency support, CICO achieves up to a 2× speedup over state-of-the-art compression techniques.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4390262838",
    "type": "article"
  },
  {
    "title": "Achieving simultaneous distribution control and privacy protection for Internet media delivery",
    "doi": "https://doi.org/10.1145/1352012.1352013",
    "publication_date": "2008-05-01",
    "publication_year": 2008,
    "authors": "Songqing Chen; Shiping Chen; Huiping Guo; Bo Shen; Sushil Jajodia",
    "corresponding_authors": "",
    "abstract": "Massive Internet media distribution demands prolonged continuous consumption of networking and disk bandwidths in large capacity. Many proxy-based Internet media distribution algorithms and systems have been proposed, implemented, and evaluated to address the scalability and performance issue. However, few of them have been used in practice, since two important issues are not satisfactorily addressed. First, existing proxy-based media distribution architectures lack an efficient media distribution control mechanism. Without copyright protection, content providers are hesitant to use proxy-based fast distribution techniques. Second, little has been done to protect client privacy during content accesses on the Internet. Straightforward solutions to address these two issues independently lead to conflicts. For example, to enforce distribution control, only legitimate users should be granted access rights. However, this normally discloses more information (such as which object the client is accessing) other than the client identity, which conflicts with the client's desire for privacy protection. In this article, we propose a unified proxy-based media distribution protocol to effectively address these two problems simultaneously. We further design a set of new algorithms in a cooperative proxy environment where our proposed scheme works efficiently and practically. Simulation-based experiments are conducted to extensively evaluate the proposed system. Preliminary results demonstrate the effectiveness of our proposed strategy.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2101382581",
    "type": "article"
  },
  {
    "title": "ASAP",
    "doi": "https://doi.org/10.1145/3219750",
    "publication_date": "2018-06-27",
    "publication_year": 2018,
    "authors": "Ahmed H. Zahran; Jason J. Quinlan; K. K. Ramakrishnan; Cormac J. Sreenan",
    "corresponding_authors": "",
    "abstract": "The dramatic growth of video traffic represents a practical challenge for cellular network operators in providing a consistent streaming Quality of Experience (QoE) to their users. Satisfying this objective has so-far proved elusive, due to the inherent characteristics of wireless networks and varying channel conditions as well as variability in the video bitrate that can degrade streaming performance. In this article, we propose stall-aware pacing as a novel MPEG DASH video traffic management solution that reduces playback stalls and seeks to maintain a consistent QoE for cellular users, even those with diverse channel conditions. These goals are achieved by leveraging both network and client state information to optimize the pacing of individual video flows. We evaluate the performance of two versions of stall-aware pacing techniques extensively, including stall-aware pacing (SAP) and adaptive stall-aware pacing (ASAP), using real video content and clients, operating over a simulated LTE network. We implement state-of-the-art client adaptation and traffic management strategies for direct comparisons with SAP and ASAP. Our results, using a heavily loaded base station, show that SAP reduces the number of stalls and the average stall duration per session by up to 95%. Additionally, SAP ensures that clients with good channel conditions do not dominate available wireless resources, evidenced by a reduction of up to 40% in the standard deviation of the QoE metric across clients. We also show that ASAP achieves additional performance gains by adaptively pacing video streams based on the application buffer state.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2811174823",
    "type": "article"
  },
  {
    "title": "Data Musicalization",
    "doi": "https://doi.org/10.1145/3184742",
    "publication_date": "2018-04-25",
    "publication_year": 2018,
    "authors": "Aurora Tulilaulu; Matti Nelimarkka; Joonas Paalasmaa; Daniel D. Johnson; Dan Ventura; Petri Myllys; Hannu Toivonen",
    "corresponding_authors": "",
    "abstract": "Data musicalization is the process of automatically composing music based on given data as an approach to perceptualizing information artistically. The aim of data musicalization is to evoke subjective experiences in relation to the information rather than merely to convey unemotional information objectively. This article is written as a tutorial for readers interested in data musicalization. We start by providing a systematic characterization of musicalization approaches, based on their inputs, methods, and outputs. We then illustrate data musicalization techniques with examples from several applications: one that perceptualizes physical sleep data as music, several that artistically compose music inspired by the sleep data, one that musicalizes on-line chat conversations to provide a perceptualization of liveliness of a discussion, and one that uses musicalization in a gamelike mobile application that allows its users to produce music. We additionally provide a number of electronic samples of music produced by the different musicalization applications.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2911524770",
    "type": "article"
  },
  {
    "title": "A <sup>2</sup> CMHNE",
    "doi": "https://doi.org/10.1145/3321506",
    "publication_date": "2019-05-31",
    "publication_year": 2019,
    "authors": "Jun Hu; Shengsheng Qian; Quan Fang; Xueliang Liu; Changsheng Xu",
    "corresponding_authors": "",
    "abstract": "Network representation learning is playing an important role in network analysis due to its effectiveness in a variety of applications. However, most existing network embedding models focus on homogeneous networks and neglect the diverse properties such as different types of network structures and associated multimedia content information. In this article, we learn node representations for multimodal heterogeneous networks, which contain multiple types of nodes and/or links as well as multimodal content such as texts and images. We propose a novel attention-aware collaborative multimodal heterogeneous network embedding method (A 2 CMHNE), where an attention-based collaborative representation learning approach is proposed to promote the collaboration of structure-based embedding and content-based embedding, and generate the robust node representation by introducing an attention mechanism that enables informative embedding integration. In experiments, we compare our model with existing network embedding models on two real-world datasets. Our method leads to dramatic improvements in performance by 5%, and 9% compared with five state-of-the-art embedding methods on one benchmark (M10 Dataset), and on a multi-modal heterogeneous network dataset (WeChat dataset) for node classification, respectively. Experimental results demonstrate the effectiveness of our proposed method on both node classification and link prediction tasks.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2949403962",
    "type": "article"
  },
  {
    "title": "Subtitle Region Selection of S3D Images in Consideration of Visual Discomfort and Viewing Habit",
    "doi": "https://doi.org/10.1145/3325197",
    "publication_date": "2019-08-20",
    "publication_year": 2019,
    "authors": "Guanghui Yue; Chunping Hou; Tianwei Zhou",
    "corresponding_authors": "",
    "abstract": "Subtitles, serving as a linguistic approximation of the visual content, are an essential element in stereoscopic advertisement and the film industry. Due to the vergence accommodation conflict, the stereoscopic 3D (S3D) subtitle inevitably causes visual discomfort. To meet the viewing experience, the subtitle region should be carefully arranged. Unfortunately, very few works have been dedicated to this area. In this article, we propose a method for S3D subtitle region selection in consideration of visual discomfort and viewing habit. First, we divide the disparity map into multiple depth layers according to the disparity value. The preferential processed depth layer is determined by considering the disparity value of the foremost object. Second, the optimal region and coarse disparity value for S3D subtitle insertion are chosen by convolving the selective depth layer with the mean filter. Specifically, the viewing habit is considered during the region selection. Finally, after region selection, the disparity value of the subtitle is further modified by using the just noticeable depth difference (JNDD) model. Given that there is no public database reported for the evaluation of S3D subtitle insertion, we collect 120 S3D images as the test platform. Both objective and subjective experiments are conducted to evaluate the comfort degree of the inserted subtitle. Experimental results demonstrate that the proposed method can obtain promising performance in improving the viewing experience of the inserted subtitle.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2969399605",
    "type": "article"
  },
  {
    "title": "BTDP",
    "doi": "https://doi.org/10.1145/3282469",
    "publication_date": "2019-04-30",
    "publication_year": 2019,
    "authors": "Zhiwei Fang; Jing Liu; Xueliang Liu; Qu Tang; Yong Li; Hanqing Lu",
    "corresponding_authors": "",
    "abstract": "Bilinear models are very powerful in multimodal fusion tasks like Visual Question Answering. The predominant bilinear methods can all be seen as a kind of tensor-based decomposition operation that contains a key kernel called “core tensor.” Current approaches usually focus on reducing the computation complexity by applying low-rank constraint on the core tensor. In this article, we propose a novel bilinear architecture called Block Term Decomposition Pooling (BTDP), which not only maintains the advantages of previous bilinear methods but also conducts sparse bilinear interactions between modalities. Our method is based on Block Term Decompositions theory of tensor, which will result in a sparse and learnable block-diagonal core tensor for multimodal fusion. We prove that using such a block-diagonal core tensor is equivalent to conducting many “tiny” bilinear operations in different feature spaces. Thus, introducing sparsity into the bilinear operation can significantly increase the performance of feature fusion and improve VQA models. What is more, our BTDP is very flexible in design. We develop several variants of BTDP and discuss the effects of the diagonal blocks of core tensor. Extensive experiments on two challenging VQA-v1 and VQA-v2 datasets show that our BTDP method outperforms current bilinear models, achieving state-of-the-art performance.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2969699701",
    "type": "article"
  },
  {
    "title": "Cross Refinement Techniques for Markerless Human&lt;?brk?&gt; Motion Capture",
    "doi": "https://doi.org/10.1145/3372207",
    "publication_date": "2020-02-29",
    "publication_year": 2020,
    "authors": "Miaopeng Li; Zimeng Zhou; Xinguo Liu",
    "corresponding_authors": "",
    "abstract": "This article presents a global 3D human pose estimation method for markerless motion capture. Given two calibrated images of a person, it first obtains the 2D joint locations in the images using a pre-trained 2D Pose CNN, then constructs the 3D pose based on stereo triangulation. To improve the accuracy and the stability of the system, we propose two efficient optimization techniques for the joints. The first one, called cross-view refinement, optimizes the joints based on epipolar geometry. The second one, called cross-joint refinement, optimizes the joints using bone-length constraints. Our method automatically detects and corrects the unreliable joint, and consequently is robust against heavy occlusion, symmetry ambiguity, motion blur, and highly distorted poses. We evaluate our method on a number of benchmark datasets covering indoors and outdoors, which showed that our method is better than or on par with the state-of-the-art methods. As an application, we create a 3D human pose dataset using the proposed motion capture system, which contains about 480K images of both indoor and outdoor scenes, and demonstrate the usefulness of the dataset for human pose estimation.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3009819822",
    "type": "article"
  },
  {
    "title": "Causal Structures of Multidimensional QoE in Haptic-Audiovisual Communications",
    "doi": "https://doi.org/10.1145/3375922",
    "publication_date": "2020-02-17",
    "publication_year": 2020,
    "authors": "Shuji Tasaka",
    "corresponding_authors": "Shuji Tasaka",
    "abstract": "This article proposes a methodology for building and verifying plausible models that can express causation in multidimensional QoE for haptic-audiovisual interactive communications. For the modeling, we utilize subjective experimental data of five-point scores collected in a previous study where a pair of subjects carry out two kinds of interactive tasks (castanets hitting and object movement) in real space (not in virtual space). The multidimensional QoE is composed of 15 measures for the castanets hitting and 14 measures for the object movement. To reduce the dimension, we classify the QoE measures into three groups as indicators of three constructs (latent variables or factors): AVQ (AudioVisual Quality), HQ (Haptic Quality), and UXQ (User eXperience Quality). We then build two models: (1) a structural equation model in which AVQ and HQ correlated with each other give causal effects on UXQ, and (2) a confirmatory factor analysis model in which the three constructs are only correlated with each other. We refer to the former as 3C-SEM and the latter as 3C-CFA. We further introduce a CFA model with a single construct for which all QoE measures are its indicators (1C-CFA). We perform Bayesian analysis of the three models by means of Markov chain Monte Carlo simulation; in each model, the deviance information criterion is obtained for model comparison, and the posterior predictive p -value is calculated for model checking. As a result, we find that 3C-SEM is the most plausible and that HQ has a stronger causal effect on UXQ than AVQ. We also learn that the correlation between AVQ and UXQ is much higher than the direct causal effect and that the increase in the association as correlation is due to the causal effect of HQ on UXQ through the correlation of AVQ with HQ. Thus, it is suggested that improving haptic performance is more effective in enhancement of QoE than improving audiovisual performance.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3010113119",
    "type": "article"
  },
  {
    "title": "Robust Visual Tracking Using Kernel Sparse Coding on Multiple Covariance Descriptors",
    "doi": "https://doi.org/10.1145/3360308",
    "publication_date": "2020-01-31",
    "publication_year": 2020,
    "authors": "Changyong Guo; Zhaoxin Zhang; Jinjiang Li; Xuesong Jiang; Jun Zhang; Lei Zhang",
    "corresponding_authors": "",
    "abstract": "In this article, we aim to improve the performance of visual tracking by combing different features of multiple modalities. The core idea is to use covariance matrices as feature descriptors and then use sparse coding to encode different features. The notion of sparsity has been successfully used in visual tracking. In this context, sparsity is used along appearance models often obtained from intensity/color information. In this work, we step outside this trend and propose to model the target appearance by local covariance descriptors (CovDs) in a pyramid structure. The proposed pyramid structure not only enables us to encode local and spatial information of the target appearance but also inherits useful properties of CovDs such as invariance to affine transforms. Since CovDs lie on a Riemannian manifold, we further propose to perform tracking through sparse coding by embedding the Riemannian manifold into an infinite-dimensional Hilbert space. Embedding the manifold into a Hilbert space allows us to perform sparse coding efficiently using the kernel trick. Our empirical study shows that the proposed tracking framework outperforms the existing state-of-the-art methods in challenging scenarios.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3021642200",
    "type": "article"
  },
  {
    "title": "Learning Visual Elements of Images for Discovery of Brand Posts",
    "doi": "https://doi.org/10.1145/3385413",
    "publication_date": "2020-05-22",
    "publication_year": 2020,
    "authors": "Francesco Gelli; Tiberio Uricchio; Xiangnan He; Alberto Del Bimbo; Tat‐Seng Chua",
    "corresponding_authors": "",
    "abstract": "Online Social Network Sites have become a primary platform for brands and organizations to engage their audience by sharing image and video posts on their timelines. Different from traditional advertising, these posts are not restricted to the products or logo but include visual elements that express more in general the values and attributes of the brand, called brand associations. Since marketers are increasingly spending time in discovering and re-posting user generated posts that reflect the brand attributes, there is an increasing demand for such discovery systems. The goal of these systems is to assist brand experts in filtering through online collections of new user media to discover actionable posts, which match the brand value and have the potential to engage the consumers. Driven by this real-life application, we define and formulate a new task of content discovery for brands and propose a framework that learns to rank posts for brands from their historical timeline. We design a Personalized Content Discovery (PCD) framework to address the three challenges of high inter-brand similarity, sparsity of brand--post interactions, and diversification of timeline. To learn fine-grained brand representation and to generate explanations for the ranking, we automatically learn visual elements of posts from the timeline of brands and from a set of brand attributes in the domain of marketing. To test our framework we use two large-scale Instagram datasets that contain a total of more than 1.5 million image and video posts from the historical timeline of hundreds of brands from multiple verticals such as food and fashion. Extensive experiments indicate that our model can effectively learn fine-grained brand representations and outperform the closest state-of-the-art solutions.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3034980272",
    "type": "article"
  },
  {
    "title": "A New Transfer Function for Volume Visualization of Aortic Stent and Its Application to Virtual Endoscopy",
    "doi": "https://doi.org/10.1145/3373358",
    "publication_date": "2020-04-30",
    "publication_year": 2020,
    "authors": "Chenxi Huang; Yisha Lan; Guokai Zhang; Gaowei Xu; Landu Jiang; Nianyin Zeng; Jen Hong Tan; E. Y. K. Ng; Yongqiang Cheng; Ningzhi Han; Rongrong Ji; Yonghong Peng",
    "corresponding_authors": "",
    "abstract": "Aortic stent has been widely used in restoring vascular stenosis and assisting patients with cardiovascular disease. The effective visualization of aortic stent is considered to be critical to ensure the effectiveness and functions of the aortic stent in clinical practice. Volume rendering with ray casting has been used as an effective approach to enable the effective visualization of aortic stent. The volume rendering relies on the transfer function that converts the medical images into optical attributes including color and transparency. This article proposes a new transfer function, namely, the multi-dimensional transfer function, to provide additional transparency value of a voxel. The proposed approach using the additional transparency value effectively assists the distinguishing of tissues that have the same CT value. The transparency values are simultaneously determined by gray threshold and gray change threshold, which can recognize the unnecessary structures such as bones transparent. A series of experimental results demonstrate that the situation of aorta stent of a patient can be directly observed, and the angle of view can be switched arbitrarily. The proposed method provides a new way for the operation of a virtual endoscopy to reach the place of blood vessels that a traditional endoscopy fails to reach.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3036616607",
    "type": "article"
  },
  {
    "title": "Improving Multiperson Pose Estimation by Mask-aware Deep Reinforcement Learning",
    "doi": "https://doi.org/10.1145/3397340",
    "publication_date": "2020-07-05",
    "publication_year": 2020,
    "authors": "Xun Wang; Yan Tian; Xuran Zhao; Tao Yang; Judith Gelernter; Jialei Wang; Guohua Cheng; Wei Hu",
    "corresponding_authors": "",
    "abstract": "Research on single-person pose estimation based on deep neural networks has recently witnessed progress in both accuracy and execution efficiency. However, multiperson pose estimation is still a challenging topic, partially because the object regions are selected greedily from proposals via class-agnostic nonmaximum suppression (NMS), and the misalignment in the redundant detection yields inaccurate human poses. Therefore, we consider how to obtain the optimal input in human pose estimation under conditions in which intermediate label information is not available. As supervised learning–based alignment does not generalize well to unseen samples in the human pose space, in this article, we present a mask-aware deep reinforcement learning approach to modify the detection result. We use mask information to remove the adverse effects from the cluttered background and to select the optimal action according to the revised reward function. We also propose a new regularization term to punish joints that are outside of the silhouette region in the human pose estimation stage. We evaluate our approach on the MPII Multiperson dataset and the MS-COCO Keypoints Challenge. The results show that our approach yields competing inference results when it is compared to the other state-of-the-art approaches.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3038501011",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1314303",
    "publication_date": "2007-12-01",
    "publication_year": 2007,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4237074217",
    "type": "paratext"
  },
  {
    "title": "Table of Contents",
    "doi": "https://doi.org/10.1145/3264510",
    "publication_date": "2018-08-09",
    "publication_year": 2018,
    "authors": "M Abdallah",
    "corresponding_authors": "M Abdallah",
    "abstract": "No abstract available.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4237690442",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3173554",
    "publication_date": "2018-01-16",
    "publication_year": 2018,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Adaptive Educational Hypermedia (AEH) e-learning models aim to personalize educational content and learning resources based on the needs of an individual learner. The Adaptive Hypermedia Architecture (AHA) is a specific implementation of the AEH model ...",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4256743882",
    "type": "paratext"
  },
  {
    "title": "Sprite generation using sprite fusion",
    "doi": "https://doi.org/10.1145/2168996.2169002",
    "publication_date": "2012-05-01",
    "publication_year": 2012,
    "authors": "Yi Chen; Abhidnya A. Deshpande; Ramazan S. Aygüun",
    "corresponding_authors": "",
    "abstract": "There has been related research for sprite or mosaic generation for over 15 years. In this article, we try to understand the methodologies for sprite generation and identify what has not actually been covered for sprite generation. We first identify issues and focus on the domain of videos for sprite generation. We introduce a novel sprite fusion method that blends two sprites. Sprite fusion method produces good results for tracking videos and does not require object segmentation. We present sample results of our experiments.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W1981367029",
    "type": "article"
  },
  {
    "title": "Identification of scene locations from geotagged images",
    "doi": "https://doi.org/10.1145/2422956.2422961",
    "publication_date": "2013-02-01",
    "publication_year": 2013,
    "authors": "Jong-Seung Park; Ramesh Jain",
    "corresponding_authors": "",
    "abstract": "Due to geotagging capabilities of consumer cameras, it has become easy to capture the exact geometric location where a picture is taken. However, the location is not the whereabouts of the scene taken by the photographer but the whereabouts of the photographer himself. To determine the actual location of an object seen in a photo some sophisticated and tiresome steps are required on a special camera rig, which are generally not available in common digital cameras. This article proposes a novel method to determine the geometric location corresponding to a specific image pixel. A new technique of stereo triangulation is introduced to compute the relative depth of a pixel position. Geographical metadata embedded in images are utilized to convert relative depths to absolute coordinates. When a geographic database is available we can also infer the semantically meaningful description of a scene object from where the specified pixel is projected onto the photo. Experimental results demonstrate the effectiveness of the proposed approach in accurately identifying actual locations.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W1992147809",
    "type": "article"
  },
  {
    "title": "Introduction to special section on 3D mobile multimedia",
    "doi": "https://doi.org/10.1145/2348816.2348820",
    "publication_date": "2012-09-01",
    "publication_year": 2012,
    "authors": "Shervin Shirmohammadi; Mohamed Hefeeda; Wei Tsang Ooi; Romulus Grigoraş",
    "corresponding_authors": "",
    "abstract": "introduction Share on Introduction to special section on 3D mobile multimedia Authors: Shervin Shirmohammadi DISCOVER Lab, Univeristy of Ottawa DISCOVER Lab, Univeristy of OttawaView Profile , Mohamed Hefeeda Network Systems lab, Simon Fraser University Network Systems lab, Simon Fraser UniversityView Profile , Wei Tsang Ooi Department of Computer Science, National University of Singapore Department of Computer Science, National University of SingaporeView Profile , Romulus Grigoras IRIT-University of Toulouse and Deviatics.com IRIT-University of Toulouse and Deviatics.comView Profile Authors Info & Claims ACM Transactions on Multimedia Computing, Communications, and ApplicationsVolume 8Issue 3sSeptember 2012 Article No.: 41pp 1–3https://doi.org/10.1145/2348816.2348820Published:16 October 2012Publication History 3citation142DownloadsMetricsTotal Citations3Total Downloads142Last 12 Months3Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W1993644180",
    "type": "article"
  },
  {
    "title": "Active query sensing",
    "doi": "https://doi.org/10.1145/2348816.2348819",
    "publication_date": "2012-09-01",
    "publication_year": 2012,
    "authors": "Rongrong Ji; Felix X. Yu; Tongtao Zhang; Shih‐Fu Chang",
    "corresponding_authors": "",
    "abstract": "While much exciting progress is being made in mobile visual search, one important question has been left unexplored in all current systems. When searching objects or scenes in the 3D world, which viewing angle is more likely to be successful? More particularly, if the first query fails to find the right target, how should the user control the mobile camera to form the second query? In this article, we propose a novel Active Query Sensing system for mobile location search, which actively suggests the best subsequent query view to recognize the physical location in the mobile environment. The proposed system includes two unique components: (1) an offline process for analyzing the saliencies of different views associated with each geographical location, which predicts the location search precisions of individual views by modeling their self-retrieval score distributions. (2) an online process for estimating the view of an unseen query, and suggesting the best subsequent view change. Specifically, the optimal viewing angle change for the next query can be formulated as an online information theoretic approach. Using a scalable visual search system implemented over a NYC street view dataset (0.3 million images), we show a performance gain by reducing the failure rate of mobile location search to only 12% after the second query. We have also implemented an end-to-end functional system, including user interfaces on iPhones, client-server communication, and a remote search server. This work may open up an exciting new direction for developing interactive mobile media applications through the innovative exploitation of active sensing and query formulation.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W1998508273",
    "type": "article"
  },
  {
    "title": "A reward-and-punishment-based approach for concept detection using adaptive ontology rules",
    "doi": "https://doi.org/10.1145/2457450.2457452",
    "publication_date": "2013-05-01",
    "publication_year": 2013,
    "authors": "Chidansh Bhatt; Pradeep K. Atrey; Mohan Kankanhalli",
    "corresponding_authors": "",
    "abstract": "Despite the fact that performance improvements have been reported in the last years, semantic concept detection in video remains a challenging problem. Existing concept detection techniques, with ontology rules, exploit the static correlations among primitive concepts but not the dynamic spatiotemporal correlations. The proposed method rewards (or punishes) detected primitive concepts using dynamic spatiotemporal correlations of the given ontology rules and updates these ontology rules based on the accuracy of detection. Adaptively learned ontology rules significantly help in improving the overall accuracy of concept detection as shown in the experimental result.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2013792503",
    "type": "article"
  },
  {
    "title": "A personal look back at twenty years of research in multimedia content analysis",
    "doi": "https://doi.org/10.1145/2502434",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Wolfgang Effelsberg",
    "corresponding_authors": "Wolfgang Effelsberg",
    "abstract": "This paper is a personal look back at twenty years of research in multimedia content analysis. It addresses the areas of audio, photo and video analysis for the purpose of indexing and retrieval from the perspective of a multimedia researcher. Whereas a general analysis of content is impossible due to the personal bias of the user, significant progress was made in the recognition of specific objects or events. The paper concludes with a brief outlook on the future.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2014372404",
    "type": "article"
  },
  {
    "title": "Aggregate licenses validation for digital rights violation detection",
    "doi": "https://doi.org/10.1145/2344436.2344443",
    "publication_date": "2012-09-01",
    "publication_year": 2012,
    "authors": "Amit Sachan; Sabu Emmanuel; Mohan Kankanhalli",
    "corresponding_authors": "",
    "abstract": "Digital Rights Management (DRM) is the term associated with the set of technologies to prevent illegal multimedia content distribution and consumption. DRM systems generally involve multiple parties such as owner, distributors, and consumers. The owner issues redistribution licenses to its distributors. The distributors in turn using their received redistribution licenses can generate and issue new redistribution licenses to other distributors and new usage licenses to consumers. As a part of rights violation detection, these newly generated licenses must be validated by a validation authority against the redistribution license used to generate them. The validation of these newly generated licenses becomes quite complex when there exist multiple redistribution licenses for a media with the distributors. In such cases, the validation process requires validation using an exponential number (to the number of redistribution licenses) of validation inequalities and each validation inequality may contain up to an exponential number of summation terms. This makes the validation process computationally intensive and necessitates to do the validation efficiently. To overcome this, we propose validation tree , a prefix-tree-based validation method to do the validation efficiently. Theoretical analysis and experimental results show that our proposed technique reduces the validation time significantly.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2033565377",
    "type": "article"
  },
  {
    "title": "Multimedia systems research",
    "doi": "https://doi.org/10.1145/2490859",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Prashant Shenoy",
    "corresponding_authors": "Prashant Shenoy",
    "abstract": "This retrospective article examines the past two decades of multimedia systems research through the lens of three research topics that were in vogue in the early days of the field and offers perspectives on the evolution of these research topics. We discuss the eventual impact of each line of research and offer lessons for future research in the field.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2037662519",
    "type": "article"
  },
  {
    "title": "Comparison of predictive contract mechanisms from an information theory perspective",
    "doi": "https://doi.org/10.1145/2168996.2168998",
    "publication_date": "2012-05-01",
    "publication_year": 2012,
    "authors": "Xin Zhang; Tomás Ward; Séamus McLoone",
    "corresponding_authors": "",
    "abstract": "Inconsistency arises across a Distributed Virtual Environment due to network latency induced by state changes communications. Predictive Contract Mechanisms (PCMs) combat this problem through reducing the amount of messages transmitted in return for perceptually tolerable inconsistency. To date there are no methods to quantify the efficiency of PCMs in communicating this reduced state information. This article presents an approach derived from concepts in information theory for a deeper understanding of PCMs. Through a comparison of representative PCMs, the worked analysis illustrates interesting aspects of PCMs operation and demonstrates how they can be interpreted as a form of lossy information compression.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2049759546",
    "type": "article"
  },
  {
    "title": "Online video delivery",
    "doi": "https://doi.org/10.1145/2502435",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Kien A. Hua",
    "corresponding_authors": "Kien A. Hua",
    "abstract": "Video streaming is the core technology for online video delivery systems. Initial research on this technology faced many challenges. In this article, lessons learned from beginning trials are discussed; some pioneering works that provided early solutions and inspired subsequent research are presented; and new techniques required for emerging applications are examined.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2068837109",
    "type": "article"
  },
  {
    "title": "Identity verification based on handwritten signatures with haptic information using genetic programming",
    "doi": "https://doi.org/10.1145/2457450.2457453",
    "publication_date": "2013-05-01",
    "publication_year": 2013,
    "authors": "Fawaz A. Alsulaiman; Nizar Sakr; Julio J. Valdés; Abdulmotaleb El Saddik",
    "corresponding_authors": "",
    "abstract": "In this article, haptic-based handwritten signature verification using Genetic Programming (GP) classification is presented. A comparison of GP-based classification with classical classifiers including support vector machine, k -nearest neighbors, naïve Bayes, and random forest is conducted. In addition, the use of GP in discovering small knowledge-preserving subsets of features in high-dimensional datasets of haptic-based signatures is investigated and several approaches are explored. Subsets of features extracted from GP-generated models (analytic functions) are also exploited to determine the importance and relevance of different haptic data types (e.g., force, position, torque, and orientation) in user identity verification. The results revealed that GP classifiers compare favorably with the classical methods and use a much fewer number of attributes (with simple function sets).",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2109589711",
    "type": "article"
  },
  {
    "title": "EDITORIAL The birth of the ACM transactions on multimedia computing, communications and applications (TOMCCAP)",
    "doi": "https://doi.org/10.1145/1047936.1047937",
    "publication_date": "2005-02-01",
    "publication_year": 2005,
    "authors": "Nicolas D. Georganas",
    "corresponding_authors": "Nicolas D. Georganas",
    "abstract": "No abstract available.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2005727908",
    "type": "article"
  },
  {
    "title": "DLRF-Net: A Progressive Deep Latent Low-Rank Fusion Network for Hierarchical Subspace Discovery",
    "doi": "https://doi.org/10.1145/3402030",
    "publication_date": "2021-01-31",
    "publication_year": 2021,
    "authors": "Zhao Zhang; Jiahuan Ren; Haijun Zhang; Zheng Zhang; Guangcan Liu; Shuicheng Yan",
    "corresponding_authors": "",
    "abstract": "Low-rank coding-based representation learning is powerful for discovering and recovering the subspace structures in data, which has obtained an impressive performance; however, it still cannot obtain deep hidden information due to the essence of single-layer structures. In this article, we investigate the deep low-rank representation of images in a progressive way by presenting a novel strategy that can extend existing single-layer latent low-rank models into multiple layers. Technically, we propose a new progressive Deep Latent Low-Rank Fusion Network (DLRF-Net) to uncover deep features and the clustering structures embedded in latent subspaces. The basic idea of DLRF-Net is to progressively refine the principal and salient features in each layer from previous layers by fusing the clustering and projective subspaces, respectively, which can potentially learn more accurate features and subspaces. To obtain deep hidden information, DLRF-Net inputs shallow features from the last layer into subsequent layers. Then, it aims at recovering the hierarchical information and deeper features by respectively congregating the subspaces in each layer of the network. As such, one can also ensure the representation learning of deeper layers to remove the noise and discover the underlying clean subspaces, which will be verified by simulations. It is noteworthy that the framework of our DLRF-Net is general and is applicable to most existing latent low-rank representation models, i.e., existing latent low-rank models can be easily extended to the multilayer scenario using DLRF-Net. Extensive results on real databases show that our framework can deliver enhanced performance over other related techniques.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3151947159",
    "type": "article"
  },
  {
    "title": "3D Tensor Auto-encoder with Application to Video Compression",
    "doi": "https://doi.org/10.1145/3431768",
    "publication_date": "2021-05-11",
    "publication_year": 2021,
    "authors": "Yang Li; Guangcan Liu; Yubao Sun; Qingshan Liu; Shengyong Chen",
    "corresponding_authors": "",
    "abstract": "Auto-encoder has been widely used to compress high-dimensional data such as the images and videos. However, the traditional auto-encoder network needs to store a large number of parameters. Namely, when the input data is of dimension n , the number of parameters in an auto-encoder is in general O ( n ). In this article, we introduce a network structure called 3D Tensor Auto-Encoder (3DTAE). Unlike the traditional auto-encoder, in which a video is represented as a vector, our 3DTAE considers videos as 3D tensors to directly pass tensor objects through the network. The weights of each layer are represented by three small matrices, and thus the number of parameters in 3DTAE is just O ( n 1/3). The compact nature of 3DTAE fits well the needs of video compression. Given an ensemble of high-dimensional videos, we represent them as 3DTAE networks plus some small core tensors, and we further quantize the network parameters and the core tensors to get the final compressed data. Experimental results verify the efficiency of 3DTAE.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3162460290",
    "type": "article"
  },
  {
    "title": "PGNet: Progressive Feature Guide Learning Network for Three-dimensional Shape Recognition",
    "doi": "https://doi.org/10.1145/3443708",
    "publication_date": "2021-07-22",
    "publication_year": 2021,
    "authors": "Jie Nie; Zhiqiang Wei; Weizhi Nie; An-An Liu",
    "corresponding_authors": "",
    "abstract": "Three-dimensional (3D) shape recognition is a popular topic and has potential application value in the field of computer vision. With the recent proliferation of deep learning, various deep learning models have achieved state-of-the-art performance. Among them, multiview-based 3D shape representation has received increased attention in recent years, and related approaches have shown significant improvement in 3D shape recognition. However, these methods focus on feature learning based on the design of the network and ignore the correlation among views. In this article, we propose a novel progressive feature guide learning network (PGNet) that focuses on the correlation among multiple views and integrates multiple modalities for 3D shape recognition. In particular, we propose two information fusion schemes from visual and feature aspects. The visual fusion scheme focuses on the view level and employs the soft-attention model to define the weights of views for visual information fusion. The feature fusion scheme focuses on the feature dimension information and employs the quantified feature as the mask to further optimize the feature. These two schemes jointly construct a PGNet for 3D shape representation. The classic ModelNet40 and ShapeNetCore55 datasets are applied to demonstrate the performance of our approach. The corresponding experiment also demonstrates the superiority of our approach.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3184079008",
    "type": "article"
  },
  {
    "title": "Black-Box Diagnosis and Calibration on GAN Intra-Mode Collapse: A Pilot Study",
    "doi": "https://doi.org/10.1145/3472768",
    "publication_date": "2021-10-31",
    "publication_year": 2021,
    "authors": "Zhenyu Wu; Zhaowen Wang; Ye Yuan; Jianming Zhang; Zhangyang Wang; Hailin Jin",
    "corresponding_authors": "",
    "abstract": "Generative adversarial networks (GANs) nowadays are capable of producing images of incredible realism. Two concerns raised are whether the state-of-the-art GAN’s learned distribution still suffers from mode collapse and what to do if so. Existing diversity tests of samples from GANs are usually conducted qualitatively on a small scale and/or depend on the access to original training data as well as the trained model parameters. This article explores GAN intra-mode collapse and calibrates that in a novel black-box setting: access to neither training data nor the trained model parameters is assumed. The new setting is practically demanded yet rarely explored and significantly more challenging. As a first stab, we devise a set of statistical tools based on sampling that can visualize, quantify, and rectify intra-mode collapse . We demonstrate the effectiveness of our proposed diagnosis and calibration techniques, via extensive simulations and experiments, on unconditional GAN image generation (e.g., face and vehicle). Our study reveals that the intra-mode collapse is still a prevailing problem in state-of-the-art GANs and the mode collapse is diagnosable and calibratable in black-box settings. Our codes are available at https://github.com/VITA-Group/BlackBoxGANCollapse .",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3186861155",
    "type": "article"
  },
  {
    "title": "Local Constraint and Label Embedding Multi-layer Dictionary Learning for Sperm Head Classification",
    "doi": "https://doi.org/10.1145/3458927",
    "publication_date": "2021-10-26",
    "publication_year": 2021,
    "authors": "Tongguang Ni; Ding Yan; Jing Xue; Kaijian Xia; Xiaoqing Gu; Yizhang Jiang",
    "corresponding_authors": "",
    "abstract": "Morphological classification of human sperm heads is a key technology for diagnosing male infertility. Due to its sparse representation and learning capability, dictionary learning has shown remarkable performance in human sperm head classification. To promote the discriminability of the classification model, a novel local constraint and label embedding multi-layer dictionary learning model called LCLM-MDL is proposed in this study. Based on the multi-layer dictionary learning framework, two dictionaries are built on the basis of Laplacian regularized constraint and label embedding term in each layer, and the two dictionaries are approximated to each other as much as possible, so as to well exploit the nonlinear structure and discriminability features of the morphology of human sperm heads. In addition, to promote the robustness of the model, the asymmetric Huber loss is adopted in the last layer of LCLM-MDL, which approximates the misclassification error by using the absolute error function. Finally, the experimental results on HuSHeM dataset demonstrate the validity of the LCLM-MDL.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3210321121",
    "type": "article"
  },
  {
    "title": "Unsupervised Domain Expansion for Visual Categorization",
    "doi": "https://doi.org/10.1145/3448108",
    "publication_date": "2021-11-12",
    "publication_year": 2021,
    "authors": "Jie Wang; Kaibin Tian; Dayong Ding; Gang Yang; Xirong Li",
    "corresponding_authors": "",
    "abstract": "Expanding visual categorization into a novel domain without the need of extra annotation has been a long-term interest for multimedia intelligence. Previously, this challenge has been approached by unsupervised domain adaptation (UDA). Given labeled data from a source domain and unlabeled data from a target domain, UDA seeks for a deep representation that is both discriminative and domain-invariant. While UDA focuses on the target domain, we argue that the performance on both source and target domains matters, as in practice which domain a test example comes from is unknown. In this article, we extend UDA by proposing a new task called unsupervised domain expansion (UDE), which aims to adapt a deep model for the target domain with its unlabeled data, meanwhile maintaining the model’s performance on the source domain. We propose Knowledge Distillation Domain Expansion (KDDE) as a general method for the UDE task. Its domain-adaptation module can be instantiated with any existing model. We develop a knowledge distillation-based learning mechanism, enabling KDDE to optimize a single objective wherein the source and target domains are equally treated. Extensive experiments on two major benchmarks, i.e., Office-Home and DomainNet, show that KDDE compares favorably against four competitive baselines, i.e., DDC, DANN, DAAN, and CDAN, for both UDA and UDE tasks. Our study also reveals that the current UDA models improve their performance on the target domain at the cost of noticeable performance loss on the source domain.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3214160574",
    "type": "article"
  },
  {
    "title": "CAPTAIN: Comprehensive Composition Assistance for Photo Taking",
    "doi": "https://doi.org/10.1145/3462762",
    "publication_date": "2022-01-27",
    "publication_year": 2022,
    "authors": "Farshid Farhat; Mohammad Mahdi Kamani; James Z. Wang",
    "corresponding_authors": "",
    "abstract": "Many people are interested in taking astonishing photos and sharing them with others. Emerging high-tech hardware and software facilitate the ubiquitousness and functionality of digital photography. Because composition matters in photography, researchers have leveraged some common composition techniques, such as the rule of thirds and the perspective-related techniques, in providing photo-taking assistance. However, composition techniques developed by professionals are far more diverse than well-documented techniques can cover. We present a new approach to leverage the underexplored photography ideas, which are virtually unlimited, diverse, and correlated. We propose a comprehensive fork-join framework, named CAPTAIN ( C omposition A ssistance for P hoto Ta k in g), to guide a photographer with a variety of photography ideas. The framework consists of a few components: integrated object detection, photo genre classification, artistic pose clustering, and personalized aesthetics-aware image retrieval. CAPTAIN is backed by a large managed dataset crawled from a Website with ideas from photography enthusiasts and professionals. The work proposes steps to decompose a given amateurish shot into composition ingredients and compose them to bring the photographer a list of useful and related ideas. The work addresses personal preferences for composition by presenting a user-specified preference list of photography ideas. We have conducted many experiments on the newly proposed components and reported findings. A user study demonstrates that the work is useful to those taking photos.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2900153539",
    "type": "article"
  },
  {
    "title": "Contrast-Enhanced Color Visual Cryptography for (k, n) Threshold Schemes",
    "doi": "https://doi.org/10.1145/3508394",
    "publication_date": "2022-02-18",
    "publication_year": 2022,
    "authors": "Zuquan Liu; Guopu Zhu; Feng Ding; Xiangyang Luo; Sam Kwong; Peng Li",
    "corresponding_authors": "",
    "abstract": "In traditional visual cryptography schemes (VCSs), pixel expansion remains to be an unsolved challenge. To alleviate the impact of pixel expansion, several colored-black-and-white VCSs, called CBW-VCSs, were proposed in recent years. Although these methods could ease the effect of pixel expansion, the reconstructed image obtained by these methods may also suffer from low contrasts. To address this issue, we propose a contrast-enhanced (k, n) CBW-VCS based on random grids, named (k,n) RG-CBW-VCS, in this article. By applying color random grids, a binary secret image is encrypted into n color shares that have no pixel expansion. When any k 1 (k 1 &gt; k ) color shares are collected together, the stacked results of them can be identified as the secret image; whereas the superposition of any k 2 ( k 2 &lt; k ) color shares shows nothing. Through theoretical analysis and experimental results, we justify the effectiveness of the proposed (k, n) RG-CBW-VCS. Compared with related methods in feature, contrast, and pixel expansion, the results indicate that the proposed method generally achieves better performance.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4213015127",
    "type": "article"
  },
  {
    "title": "Online Correction of Camera Poses for the Surround-view System: A Sparse Direct Approach",
    "doi": "https://doi.org/10.1145/3505252",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Tianjun Zhang; Hao Deng; Lin Zhang; Shengjie Zhao; Xiao Liu; Yicong Zhou",
    "corresponding_authors": "",
    "abstract": "The surround-view module is an indispensable component of a modern advanced driving assistance system. By calibrating the intrinsics and extrinsics of the surround-view cameras accurately, a top-down surround-view can be generated from raw fisheye images. However, poses of these cameras sometimes may change. At present, how to correct poses of cameras in a surround-view system online without re-calibration is still an open issue. To settle this problem, we introduce the sparse direct framework and propose a novel optimization scheme of a cascade structure. This scheme is actually composed of two levels of optimization and two corresponding photometric error based models are proposed. The model for the first-level optimization is called the ground model, as its photometric errors are measured on the ground plane. For the second level of the optimization, it’s based on the so-called ground-camera model, in which photometric errors are computed on the imaging planes. With these models, the pose correction task is formulated as a nonlinear least-squares problem to minimize photometric errors in overlapping regions of adjacent bird’s-eye-view images. With a cascade structure of these two levels of optimization, an appropriate balance between the speed and the accuracy can be achieved. Experiments show that our method can effectively eliminate the misalignment caused by cameras’ moderate pose changes in the surround-view system. Source code and test cases are available online at https://cslinzhang.github.io/CamPoseCorrection/ .",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4214865768",
    "type": "article"
  },
  {
    "title": "MFGAN: Multi-modal Feature-fusion for CT Metal Artifact Reduction Using GANs",
    "doi": "https://doi.org/10.1145/3528172",
    "publication_date": "2022-03-31",
    "publication_year": 2022,
    "authors": "Liming Xu; Xianhua Zeng; Weisheng Li; Bochuan Zheng",
    "corresponding_authors": "",
    "abstract": "Due to the existence of metallic implants in certain patients, the Computed Tomography (CT) images from these patients are often corrupted by undesirable metal artifacts, which causes severe problem of metal artifact. Although many methods have been proposed to reduce metal artifact, reduction is still challenging and inadequate. Some reduced results are suffering from symptom variance, second artifact, and poor subjective evaluation. To address these, we propose a novel method based on generative adversarial nets (GANs) to reduce metal artifacts. Specifically, we firstly encode interactive information (text) and imaging CT (image) to yield multi-modal feature-fusion representation, which overcomes representative ability limitation of single-modal CT images. The incorporation of interaction information constrains feature generation, which ensures symptom consistency between corrected and target CT. Then, we design an enhancement network to avoid second artifact and enhance edge as well as suppress noise. Besides, three radiology physicians are invited to evaluate the corrected CT image. Experiments show that our method gains significant improvement over other methods. Objectively, ours achieves an average increment of 7.44% PSNR and 6.12% SSIM on two medical image datasets. Subjectively, ours outperforms others in comparison in term of sharpness, resolution, invariance, and acceptability.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4220786120",
    "type": "article"
  },
  {
    "title": "Social Network Analytic-Based Online Counterfeit Seller Detection using User Shared Images",
    "doi": "https://doi.org/10.1145/3524135",
    "publication_date": "2022-03-12",
    "publication_year": 2022,
    "authors": "Ming Cheung; Weiwei Sun; James She; Jiantao Zhou",
    "corresponding_authors": "",
    "abstract": "Selling counterfeit online has become a serious problem, especially with the advancement of social media and mobile technology. Instead of investigating the products directly, one can only check the images, tags annotated by the sellers on the images, or the price to decide if a seller sells counterfeits. One of the ways to detect counterfeit sellers is to investigate their social graphs, in which counterfeit sellers show different behaviour in network measurements, such as those in centrality and EgoNet. However, social graphs are not easily accessible. They may be kept private by the operators, or there are no connections at all. This article proposes a framework to detect counterfeit sellers using their connection graphs discovered from their shared images. Based on 153 K shared images from Taobao, it is proven that counterfeit sellers have different network behaviours. It is observed that the network measurements follow Beta function well. Those distributions are formulated to detect counterfeit sellers by the proposed framework, which is 60% better than approaches using classification.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4220967272",
    "type": "article"
  },
  {
    "title": "AABLSTM: A Novel Multi-task Based CNN-RNN Deep Model for Fashion Analysis",
    "doi": "https://doi.org/10.1145/3519029",
    "publication_date": "2022-03-12",
    "publication_year": 2022,
    "authors": "Xianlin Zhang; Mengling Shen; Xueming Li; Xiaojie Wang",
    "corresponding_authors": "",
    "abstract": "With the rapid growth of online commerce and fashion-related applications, visual clothing analysis and recognition has become a hotspot in computer vision. In this paper, we propose a novel AABLSTM network, which is based on deep CNN-RNN, to solve the visual fashion analysis of clothing category classification, attribute detection, and landmark localization. The designed fashion model is leveraged with the multi-task driven mechanism as follows: firstly, a bidirectional LSTM (Bi-LSTM) branch is proposed for efficiently mining the semantic association between related attributes so as to improve the precision of clothing category classification and attribute detection; then, an imitated hourglass sub-network of “down-up sampling” is constructed for boosting the accuracy of fashion landmark localization; and finally, a specially designed multi-loss function is constructed to better optimize the network training. Extensive experimental results on large-scale fashion datasets demonstrate the superior performance of our approach.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4221114431",
    "type": "article"
  },
  {
    "title": "Cross-User Similarities in Viewing Behavior for 360° Video and Caching Implications",
    "doi": "https://doi.org/10.1145/3507917",
    "publication_date": "2022-04-23",
    "publication_year": 2022,
    "authors": "Niklas Carlsson; Derek L. Eager",
    "corresponding_authors": "",
    "abstract": "The demand and usage of 360$^{\\circ}$ video services are expected to increase. However, despite these services being highly bandwidth intensive, not much is known about the potential value that basic bandwidth saving techniques such as server or edge-network on-demand caching (e.g., in a CDN) could have when used for delivery of such services. This problem is both important and complicated as client-side solutions have been developed that split the full 360$^{\\circ}$ view into multiple tiles, and adapt the quality of the downloaded tiles based on the user's expected viewing direction and bandwidth conditions. This paper presents new trace-based analysis methods that incorporate users' viewports (the area of the full 360$^{\\circ}$ view the user actually sees), a first characterization of the cross-user similarities of the users' viewports, and a trace-based analysis of the potential bandwidth savings that caching-based techniques may offer under different conditions. Our analysis takes into account differences in the time granularity over which viewport overlaps can be beneficial for resource saving techniques, compares and contrasts differences between video categories, and accounts for uncertainties in the network conditions and the prediction of the future viewing direction when prefetching. The results provide substantial insight into the conditions under which overlap can be considerable and caching effective, and inform the design of new caching system policies tailored for 360$^{\\circ}$ video.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4288312810",
    "type": "article"
  },
  {
    "title": "Quality Enhancement of Compressed 360-Degree Videos Using Viewport-based Deep Neural Networks",
    "doi": "https://doi.org/10.1145/3551641",
    "publication_date": "2022-08-05",
    "publication_year": 2022,
    "authors": "Qipu Qin; Cheolkon Jung",
    "corresponding_authors": "",
    "abstract": "360-degree video provides omnidirectional views by a bounding sphere, thus also called omnidirectional video. For omnidirectional video, people can only see specific content in the viewport through head movement, i.e., only a small portion of the 360-degree content is exposed at a given time. Therefore, the viewport quality is of particular importance for 360-degree videos. In this article, we propose a quality enhancement of compressed 360-degree videos using viewport-based deep neural networks, named V-DNN. V-DNN is mainly composed of two modules: viewport prediction network (VPN) and viewport quality enhancement network (VQEN). VPN based on spherical convolution and 2D convolution generates potential viewports for omnidirectional video. VQEN takes the current viewport and its reference viewports as the input and enhances residual for the current viewport based on bidirectional offset prediction and Spatio-temporal deformable convolutions. Compared with HM16.16 baseline at QP = 37 under the Low Delay P (LDP) configuration, experimental results show that V-DNN achieves an average 0.605 dB and 0.0139 gains in viewport-based ΔPSNR and ΔMS-SSIM, respectively, and is 0.379 dB (59.63%) and 0.0073 (110.61%) higher than the multi-frame quality enhancement (MFQE-2.0) scheme at QP = 37, respectively. Moreover, V-DNN consistently outperforms MFQE-1.0, MFQE-2.0, and HM16.16 baseline at the other QPs in terms of ΔPSNR, ΔWS-PSNR, and ΔMS-SSIM.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4289878123",
    "type": "article"
  },
  {
    "title": "Guided Graph Attention Learning for Video-Text Matching",
    "doi": "https://doi.org/10.1145/3538533",
    "publication_date": "2022-06-30",
    "publication_year": 2022,
    "authors": "Kunpeng Li; Chang Liu; Mike Stopa; Jun Amano; Yun Fu",
    "corresponding_authors": "",
    "abstract": "As a bridge between videos and natural languages, video-text matching has been a hot multimedia research topic in recent years. Such cross-modal retrieval is usually achieved by learning a common embedding space where videos and text captions are directly comparable. It is still challenging because existing visual representations cannot exploit semantic correlations within videos well, resulting in a mismatch with semantic concepts that are contained in the corresponding text descriptions. In this article, we propose a new Guided Graph Attention Learning (GGAL) model to enhance video embedding learning by capturing important region-level semantic concepts within the spatiotemporal space. Our model builds connections between object regions and performs hierarchical graph reasoning on both frame-level and whole video–level region graphs. During this process, global context is used to guide attention learning on this hierarchical graph topology so that the learned overall video embedding can focus on essential semantic concepts and can be better aligned with text captions. Experiments on commonly used benchmarks validate that GGAL outperforms many recent video-text retrieval methods with a clear margin. As multimedia data in dynamic environments becomes critically important, we also validate GGAL learned video-text representations that can be generalized well to unseen out-of-domain data via cross-dataset evaluations. To further investigate the interpretability of our model, we visualize attention weights learned by GGAL models. We find that GGAL successfully focuses on key semantic concepts in the video and has complementary attention on the context parts based on different ways of building region graphs.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4295135076",
    "type": "article"
  },
  {
    "title": "Towards Intelligent Attack Detection Using DNA Computing",
    "doi": "https://doi.org/10.1145/3561057",
    "publication_date": "2022-09-08",
    "publication_year": 2022,
    "authors": "Zengri Zeng; Baokang Zhao; Han‐Chieh Chao; Ilsun You; Kuo‐Hui Yeh; Weizhi Meng",
    "corresponding_authors": "",
    "abstract": "In recent years, frequent network attacks have seriously threatened the interests and security of humankind. To address this threat, many detection methods have been studied, some of which have achieved good results. However, with the development of network interconnection technology, massive amounts of network data have been produced, and considerable redundant information has been generated. At the same time, the frequently changing types of cyberattacks result in great difficulty collecting samples, resulting in a serious imbalance in the sample size of each attack type in the dataset. These two problems seriously reduce the robustness of existing detection methods, and existing research methods do not provide a good solution. To address these two problems, we define an unbalanced index and an optimal feature index to directly reflect the performance of a detection method in terms of overall accuracy, feature subset optimization, and detection balance. Inspired by DNA computing, we propose intelligent attack detection based on DNA computing (ADDC). First, we design a set of regular encoding and decoding features based on DNA sequences and obtain a better subset of features through biochemical reactions. Second, nondominated ranking based on reference points is used to select individuals to form a new population to optimize the detection balance. Finally, a large number of experiments are carried out on four datasets to reflect real-world cyberattack situations. Experimental results show that compared with the most recent detection methods, our method can improve the overall accuracy of multiclass classification by up to 10%; the imbalance index decreased by 0.5, and 1.5 more attack types were detected on average; and the optimal index of the feature subset increased by 83.8%.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4296117805",
    "type": "article"
  },
  {
    "title": "Domain Adaptation Problem in Sketch Based Image Retrieval",
    "doi": "https://doi.org/10.1145/3565368",
    "publication_date": "2022-10-08",
    "publication_year": 2022,
    "authors": "Hongchuan Yu; Mengqing Huang; Jianjun Zhang",
    "corresponding_authors": "",
    "abstract": "In this article, we present two algorithms that discover the discriminative structures of sketches, given pairs of sketches and photos in sketch-based image retrieval (SBIR) scenarios. Unlike the existing approaches, we aim at the few-shot and domain adaptation (DA) problems, and set up a module with canonical correlation analysis (CCA) technique in our algorithms to improve retrieval performance. For single source domain settings, our first algorithm can effectively transfer a classifier trained on a known dataset to a new one. For multisource settings, our second algorithm sophisticatedly combines multisource domain data to yield a classifier on the target domain. To the best of our knowledge, these two works are the first research in SBIR field. Experiments on the Sketchy and TU-Berlin sketch benchmark datasets demonstrate the effectiveness of our algorithms and compelling performance. Compared with the state-of-the-art methods, our algorithms do not use text based semantic information, but achieve competitive results. Furthermore, experiments also turn out that feature representation by available trained deep networks has distinct advantages and combination with traditional machine learning methods brings substantial improvements against the state-of-the-art methods on image retrievals. The complete source code of the proposed algorithms will be released on gitHub: [ GitHub- ADAM0912/Domain-Adaptation-Problem-in-Sketch-Based-Image-Retrieval: The code repository for the articleDomain Adaptation Problem in Sketch Based Image Retrieval ] .",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4303647620",
    "type": "article"
  },
  {
    "title": "Continual Recognition with Adaptive Memory Update",
    "doi": "https://doi.org/10.1145/3573202",
    "publication_date": "2022-12-06",
    "publication_year": 2022,
    "authors": "Xuanrong Yao; Xin Wang; Yue Liu; Wenwu Zhu",
    "corresponding_authors": "",
    "abstract": "Class incremental continual learning aims to improve the ability of modern classification models to continually recognize new classes without forgetting the previous ones. Prior art in the field has largely considered using a replay buffer. In this article, we start from an observation that the existing replay-based method would fail when the stored exemplars are not hard enough to get a good decision boundary between a previously learned class and a new class. To prevent this situation, we propose a method from the perspective of remedy after forgetting for the first time. In the proposed method, a set of exemplars is preserved as a working memory, which helps to recognize new classes. When the working memory is insufficient to distinguish between new classes, more discriminating samples would be swapped from a long-term memory, which is built up during the early training process, in an adaptive way. Our continual recognition model with adaptive memory update is capable of overcoming the problem of catastrophic forgetting with various new classes coming in sequence, especially for similar but different classes. Extensive experiments on different real-world datasets demonstrate that the proposed model is superior to existing state-of-the-art algorithms. Moreover, our model can be used as a general plugin for any replay-based continual learning algorithm to further improve their performance.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4311623178",
    "type": "article"
  },
  {
    "title": "Dual-Lens HDR using Guided 3D Exposure CNN and Guided Denoising Transformer",
    "doi": "https://doi.org/10.1145/3579167",
    "publication_date": "2022-09-30",
    "publication_year": 2022,
    "authors": "Weixin Li; Tiantian Cao; Chang Liu; Xue Tian; Ya Li; Xiaojie Wang; Dong Xuan",
    "corresponding_authors": "",
    "abstract": "We study the high dynamic range (HDR) imaging problem in dual-lens systems. Existing methods usually treat the HDR imaging problem as an image fusion problem and the HDR result is estimated by fusing the aligned short exposure image and long exposure image. However, the image fusion pipeline depends highly on the image alignment, which is difficult to be perfect. We propose to transfer the dual-lens HDR imaging problem into the disentangled enhancement of exposure correction and denoising for the short exposure image, guided by the long exposure image. In the guided exposure correction module, we make use of the guidance image and 3D color transformation to propose a guided 3D exposure CNN (GEC) to get the rough HDR result from the short exposure image. Then, in the guided denoising module, we make use of the cross-attention mechanism to propose a guided denoising transformer (GDT) to directly use the long exposure image as guidance to denoise the rough HDR result in a pyramid way. And in both modules, we bypass the difficult image alignment processing. Experimental results demonstrate the superiority of our method over the state-of-the-art ones.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4313649730",
    "type": "article"
  },
  {
    "title": "Real-time video content analysis",
    "doi": "https://doi.org/10.1145/1142020.1142024",
    "publication_date": "2006-05-01",
    "publication_year": 2006,
    "authors": "Viktor S. Wold Eide; Ole‐Christoffer Granmo; Frank Eliassen; Jørgen Andreas Michaelsen",
    "corresponding_authors": "",
    "abstract": "Real-Time content-based access to live video data requires content analysis applications that are able to process video streams in real-time and with an acceptable error rate. Statements such as this express quality of service (QoS) requirements. In general, control of the QoS provided can be achieved by sacrificing application quality in one QoS dimension for better quality in another, or by controlling the allocation of processing resources to the application. However, controlling QoS in video content analysis is particularly difficult, not only because main QoS dimensions like accuracy are nonadditive, but also because both the communication- and the processing-resource requirements are challenging.This article presents techniques for QoS-aware composition of applications for real-time video content analysis, based on dynamic Bayesian networks. The aim of QoS-aware composition is to determine application deployment configurations which satisfy a given set of QoS requirements. Our approach consists of: (1) an algorithm for QoS-aware selection of configurations of feature extractor and classification algorithms which balances requirements for timeliness and accuracy against available processing resources, (2) a distributed content-based publish/subscribe system which provides application scalability at multiple logical levels of distribution, and (3) scalable solutions for video streaming, filtering/transformation, feature extraction, and classification.We evaluate our approach based on experiments with an implementation of a real-time motion vector based object-tracking application. The evaluation shows that the application largely behaves as expected when resource availability and selections of configurations of feature extractor and classification algorithms vary. The evaluation also shows that increasing QoS requirements can be met by allocating additional CPUs for parallel processing, with only minor overhead.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W1979990092",
    "type": "article"
  },
  {
    "title": "Exploring many-to-one speech-to-text correlation for web-based language learning",
    "doi": "https://doi.org/10.1145/1236471.1236472",
    "publication_date": "2007-08-01",
    "publication_year": 2007,
    "authors": "Herng-Yow Chen; Shengwei Li",
    "corresponding_authors": "",
    "abstract": "This article investigates the correlations between multimedia objects (particularly speech and text) involved in language lectures in order to design an effective presentation mechanism for web-based learning. The cross-media correlations are classified into implicit relations (retrieved by computing) and explicit relations (recorded during the preprocessing stage). The implicit temporal correlation between speech and text is primarily to help to negotiate supplementary lecture navigations like tele-pointer movement, lips-sync movement, and content scrolling. We propose a speech-text alignment framework, using an iterative algorithm based on local alignment, to probe many-to-one temporal correlations, and not the one-to-one only. The proposed framework is a more practical method for analyzing general language lectures, and the algorithm's time complexity conforms to the best-possible computation cost, O(nm) , without introducing additional computation. In addition, we have shown the feasibility of creating vivid presentations by exploiting implicit relations and artificially simulating some explicit media. To facilitate the navigation of integrated multimedia documents, we develop several visualization techniques for describing media correlations, including guidelines for speech-text correlations, visible-automatic scrolling, and levels of detail of timeline, to provide intuitive and easy-to-use random access mechanisms. We evaluated the performance of the analysis method and human perceptions of the synchronized presentation. The overall performance of the analysis method is that about 99.5% of the words analyzed are of a temporal error within 0.5 sec and the subjective evaluation result shows that the synchronized presentation is highly acceptable to human beings.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W1983878852",
    "type": "article"
  },
  {
    "title": "The conductor interaction method",
    "doi": "https://doi.org/10.1145/1314303.1314312",
    "publication_date": "2007-12-01",
    "publication_year": 2007,
    "authors": "Dorothy Rachovides; James Walkerdine; Peter Phillips",
    "corresponding_authors": "",
    "abstract": "Computers have increasingly become part of our everyday lives, with many activities either involving their direct use or being supported by one. This has prompted research into developing methods and mechanisms to assist humans in interacting with computers (human-computer interaction, or HCI). A number of HCI techniques have been developed over the years, some of which are quite old but continue to be used, and some more recent and still evolving. Many of these interaction techniques, however, are not natural in their use and typically require the user to learn a new means of interaction. Inconsistencies within these techniques and the restrictions they impose on user creativity can also make such interaction techniques difficult to use, especially for novice users. This article proposes an alternative interaction method, the conductor interaction method (CIM), which aims to provide a more natural and easier-to-learn interaction technique. This novel interaction method extends existing HCI methods by drawing upon techniques found in human-human interaction. It is argued that the use of a two-phased multimodal interaction mechanism, using gaze for selection and gesture for manipulation, incorporated within a metaphor-based environment, can provide a viable alternative for interacting with a computer (especially for novice users). Both the model and an implementation of the CIM within a system are presented in this article. This system formed the basis of a number of user studies that have been performed to assess the effectiveness of the CIM, the findings of which are discussed in this work.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2063004507",
    "type": "article"
  },
  {
    "title": "PMS",
    "doi": "https://doi.org/10.1145/3183515",
    "publication_date": "2018-04-30",
    "publication_year": 2018,
    "authors": "Joachim Bruneau‐Queyreix; Jordi Mongay Batalla; Mathias Lacaud; Daniel Négru",
    "corresponding_authors": "",
    "abstract": "Single-source HTTP adaptive streaming solutions (HAS) have become the de facto solutions to deliver live video over the Internet. By avoiding video stalling events that are mainly caused by the lack of throughput at client or at server side, HAS solutions increase the end users’ quality of experience (QoE). We propose to pragmatically extend HAS with our MS-Stream solution that simultaneously utilizes several servers. MS-Stream aims at offering high QoE for live content delivery by exploiting expanded bandwidth and link diversity in distributed heterogeneous infrastructures. By leveraging end users’ connectivity capacities, we further extend the QoE and scalability capabilities of our proposal by exposing a hybrid P2P/multisource live-streaming solution (P2P/MS-Stream (PMS)), achieving trade-offs between the system’s scale and the end users’ QoE. We propose a distributed quality adaptation algorithm run by every peer, along with a local optimization method of the usage of the server infrastructure made available. Large-scale evaluations conducted with 300 peers located in France permits validating our approach and algorithms over flash crowd events and allow us to conclude that PMS can reach the optimal trade-offs between QoE and system scale.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2804752433",
    "type": "article"
  },
  {
    "title": "Novel Hybrid-Cast Approach to Reduce Bandwidth and Latency for Cloud-Based Virtual Space",
    "doi": "https://doi.org/10.1145/3205864",
    "publication_date": "2018-06-15",
    "publication_year": 2018,
    "authors": "Xueshi Hou; Yao Lu; Sujit Dey",
    "corresponding_authors": "",
    "abstract": "In this article, we explore the possibility of enabling cloud-based virtual space applications for better computational scalability and easy access from any end device, including future lightweight wireless head-mounted displays. In particular, we investigate virtual space applications such as virtual classroom and virtual gallery, in which the scenes and activities are rendered in the cloud, with multiple views captured and streamed to each end device. A key challenge is the high bandwidth requirement to stream all the user views, leading to high operational cost and potential large delay in a bandwidth-restricted wireless network. We propose a novel hybrid-cast approach to save bandwidth in a multi-user streaming scenario. We identify and broadcast the common pixels shared by multiple users, while unicasting the residual pixels for each user. We formulate the problem of minimizing the total bitrate needed to transmit the user views using hybrid-casting and describe our approach. A common view extraction approach and a smart grouping algorithm are proposed and developed to achieve our hybrid-cast approach. Simulation results show that the hybrid-cast approach can significantly reduce total bitrate by up to 55% and avoid congestion-related latency, compared to traditional cloud-based approach of transmitting all the views as individual unicast streams, hence addressing the bandwidth challenges of the cloud, with additional benefits in cost and delay.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2809082161",
    "type": "article"
  },
  {
    "title": "Automatic Data Augmentation from Massive Web Images for Deep Visual Recognition",
    "doi": "https://doi.org/10.1145/3204941",
    "publication_date": "2018-07-24",
    "publication_year": 2018,
    "authors": "Yalong Bai; Kuiyuan Yang; Tao Mei; Wei‐Ying Ma; Tiejun Zhao",
    "corresponding_authors": "",
    "abstract": "Large-scale image datasets and deep convolutional neural networks (DCNNs) are the two primary driving forces for the rapid progress in generic object recognition tasks in recent years. While lots of network architectures have been continuously designed to pursue lower error rates, few efforts are devoted to enlarging existing datasets due to high labeling costs and unfair comparison issues. In this article, we aim to achieve lower error rates by augmenting existing datasets in an automatic manner. Our method leverages both the web and DCNN, where the web provides massive images with rich contextual information, and DCNN replaces humans to automatically label images under the guidance of web contextual information. Experiments show that our method can automatically scale up existing datasets significantly from billions of web pages with high accuracy. The performance on object recognition tasks and transfer learning tasks have been significantly improved by using the automatically augmented datasets, which demonstrates that more supervisory information has been automatically gathered from the web. Both the dataset and models trained on the dataset have been made publicly available.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2883516373",
    "type": "article"
  },
  {
    "title": "Learning Multiple Kernel Metrics for Iterative Person Re-Identification",
    "doi": "https://doi.org/10.1145/3234929",
    "publication_date": "2018-08-09",
    "publication_year": 2018,
    "authors": "Husheng Dong; Ping Lu; Chunping Liu; Yi Ji; Shengrong Gong",
    "corresponding_authors": "",
    "abstract": "In person re-identification most metric learning methods learn from training data only once, and then they are deployed for testing. Although impressive performance has been achieved, the discriminative information from successfully identified test samples are ignored. In this work, we present a novel re-identification framework termed Iterative Multiple Kernel Metric Learning (IMKML). Specifically, there are two main modules in IMKML. In the first module, multiple metrics are learned via a new derived Kernel Marginal Nullspace Learning (KMNL) algorithm. Taking advantage of learning a discriminative nullspace from neighborhood manifold, KMNL can well tackle the Small Sample Size (SSS) problem in re-identification distance metric learning. The second module is to construct a pseudo training set by performing re-identification on the testing set. The pseudo training set, which consists of the test image pairs that are highly probable correct matches, is then inserted into the labeled training set to retrain the metrics. By iteratively alternating between the two modules, many more samples will be involved for training and significant performance gains can be achieved. Experiments on four challenging datasets, including VIPeR, PRID450S, CUHK01, and Market-1501, show that the proposed method performs favorably against the state-of-the-art approaches, especially on the lower ranks.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2886319144",
    "type": "article"
  },
  {
    "title": "Efficient Face Alignment with Fast Normalization and Contour Fitting Loss",
    "doi": "https://doi.org/10.1145/3338842",
    "publication_date": "2019-11-01",
    "publication_year": 2019,
    "authors": "Zhiwei Liu; Xiangyu Zhu; Ming Tang; Zhen Lei; Jinqiao Wang",
    "corresponding_authors": "",
    "abstract": "Face alignment is a key component of numerous face analysis tasks. In recent years, most existing methods have focused on designing high-performance face alignment systems and paid less attention to efficiency. However more face alignment systems are now applied on low-cost devices, such as mobile phones. In this article, we design a common efficient framework that can team with any face alignment regression network and improve the overall performance with nearly no extra computational cost. First, we discover that the maximum regression error exists in the face contour, where landmarks do not have distinct semantic positions, and thus are randomly labeled along the face contours in training data. To address this problem, we propose a novel contour fitting loss that dynamically adjusts the regression target during training so the network can learn more accurate semantic meanings of the contour landmarks and achieve better localization performance. Second, we decouple the complex sample variations in face alignment task and propose a Fast Normalization Module (FNM) to efficiently normalize considerable variations that can be described by geometric transformation. Finally, a new lightweight network architecture named Lightweight Alignment Module (LAM) is also proposed to achieve fast and precise face alignment on mobile devices. Our method achieves competitive performance with state-of-the-arts on 300W and AFLW2000-3D benchmarks. Meanwhile, the speed of our framework is significantly faster than other CNN-based approaches.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2985635826",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on Smart Communications and Networking for Future Video Surveillance",
    "doi": "https://doi.org/10.1145/3398382",
    "publication_date": "2020-04-30",
    "publication_year": 2020,
    "authors": "Honghao Gao; Yudong Zhang",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3034015733",
    "type": "article"
  },
  {
    "title": "ANALYSIS OF OBSTRUCTIVE SLEEP APNEA DISORDER WITH ACCURACY PREDICTION USING SVM FOR SMART ENVIRONMENT",
    "doi": "https://doi.org/10.1145/3382782",
    "publication_date": "2020-05-07",
    "publication_year": 2020,
    "authors": "Madhumitha Ramamurthy; Ilango Krishnamurthi; S Vimal; Suresh Annamalai",
    "corresponding_authors": "",
    "abstract": "One of the most crucial sleep disorders that has a direct hit on the quality of life is sleep apnea disorder. Declined memory and disorders related to personality are some of the consequences of Sleep Apnea Disorder. Identifying the difference between normal and abnormal levels of snoring sound is important for the detection of sleep apnea. Diagnosis of the sleep apnea usually takes place in hospitals under the direct supervision of medical professionals. Frequent visits to hospital for diagnosis tend to be an inconvenience to the elderly. Along with it, usage of body monitors also acts as a parameter in disrupting sleep. To overcome them, the analysis equipment is placed in the usual sleeping environment of the patient and LM393 sound sensor is used to detect the snoring levels. The analysis between the normal and the acquired snoring levels using threshold values and SVM helps confirm the presence or absence of the sleep apnea disorder.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3041296259",
    "type": "article"
  },
  {
    "title": "Multi-View Graph Matching for 3D Model Retrieval",
    "doi": "https://doi.org/10.1145/3387920",
    "publication_date": "2020-07-05",
    "publication_year": 2020,
    "authors": "Yuting Su; Wenhui Li; Weizhi Nie; An-An Liu",
    "corresponding_authors": "",
    "abstract": "3D model retrieval has been widely utilized in numerous domains, such as computer-aided design, digital entertainment, and virtual reality. Recently, many graph-based methods have been proposed to address this task by using multi-view information of 3D models. However, these methods are always constrained by many-to-many graph matching for the similarity measure between pairwise models. In this article, we propose a multi-view graph matching method (MVGM) for 3D model retrieval. The proposed method can decompose the complicated multi-view graph-based similarity measure into multiple single-view graph-based similarity measures and fusion. First, we present the method for single-view graph generation, and we further propose the novel method for the similarity measure in a single-view graph by leveraging both node-wise context and model-wise context. Then, we propose multi-view fusion with diffusion, which can collaboratively integrate multiple single-view similarities w.r.t. different viewpoints and adaptively learn their weights, to compute the multi-view similarity between pairwise models. In this way, the proposed method can avoid the difficulty in the definition and computation of the traditional high-order graph. Moreover, this method is unsupervised and does not require a large-scale 3D dataset for model learning. We conduct evaluations on four popular and challenging datasets. The extensive experiments demonstrate the superiority and effectiveness of the proposed method compared against the state of the art. In particular, this unsupervised method can achieve competitive performances against the most recent supervised and deep learning method.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3043436549",
    "type": "article"
  },
  {
    "title": "Single-stage Instance Segmentation",
    "doi": "https://doi.org/10.1145/3387926",
    "publication_date": "2020-07-05",
    "publication_year": 2020,
    "authors": "Feng Lin; Bin Li; Wengang Zhou; Houqiang Li; Yan Lu",
    "corresponding_authors": "",
    "abstract": "Albeit the highest accuracy of object detection is generally acquired by multi-stage detectors, like R-CNN and its extension approaches, the single-stage object detectors also achieve remarkable performance with faster execution and higher scalability. Inspired by this, we propose a single-stage framework to tackle the instance segmentation task. Building on a single-stage object detection network in hand, our model outputs the detected bounding box of each instance, the semantic segmentation result, and the pixel affinity simultaneously. After that, we generate the final instance masks via a fast post-processing method with the help of the three outputs above. As far as we know, it is the first attempt to segment instances in a single-stage pipeline on challenging datasets. Extensive experiments demonstrate the efficiency of our post-processing method, and the proposed framework obtains competitive results as a single-stage instance segmentation method. We achieve 32.5 box AP and 26.0 mask AP on the COCO validation set with 500 pixels input scale and 22.9 mask AP on the Cityscapes test set.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3048630682",
    "type": "article"
  },
  {
    "title": "Make Full Use of Priors",
    "doi": "https://doi.org/10.1145/3408293",
    "publication_date": "2020-11-30",
    "publication_year": 2020,
    "authors": "Xin He; Qiong Liu; You Yang",
    "corresponding_authors": "",
    "abstract": "Multi-view video plus depth (MVD) is the promising and widely adopted data representation for future 3D visual applications and interactive media. However, compression distortions on depth videos impede the development of such applications, and filters are crucially needed for the quality enhancement at the terminal side. Cross-view priors can intuitively be involved in filter design, but these priors are also distorted in compression and thus the contribution of them can hardly be considered in previous research. In this article, we propose a cross-view optimized filter for depth map quality enhancement by making full use of inner- and cross-view priors. We dedicate to evaluate the contributions of distorted cross-view priors in filtering the current view of depth, and then both inner- and cross-view priors can be involved in the filter design. Thus, distortions of cross-view priors are not barriers again as before. For the purpose of that, mutual information guided cross-view consistency is designed to evaluate the contributions of cross-view priors from compression distortions of MVD. After that, under the framework of global optimization, both inner- and cross-view priors are modeled and taken to minimize the designed energy function where both data accuracy and spatial smoothness are modeled. The experimental results show that the proposed model outperforms state-of-the-art methods, where 3.289 dB and 0.0407 average gains on peak signal-to-noise ratio and structural similarity metrics can be obtained, respectively. For the subjective evaluations, object details and structure information are recovered in the compressed depth video. We also verify our method via several practical applications, including virtual view synthesis for smooth interaction and point cloud for 3D modeling for accuracy evaluation. In these verifications, the ringing and malposition artifacts on object contours are properly handled for interactive video, and discontinuous object surfaces are restored for 3D modeling. All of these results suggest that compression distortions in MVD can be properly filtered by the proposed model, which provides a promising solution for future bandwidth constrained 3D and interactive visual applications.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3110744506",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3190503",
    "publication_date": "2018-04-02",
    "publication_year": 2018,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Computer Vision and Multimedia solutions are now offering an increasing number of applications ready for use by end users in everyday life. Many of these applications are centered for detection, representation, and analysis of face and body. Methods ...",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4249040904",
    "type": "paratext"
  },
  {
    "title": "Up-Fusion",
    "doi": "https://doi.org/10.1145/2611777",
    "publication_date": "2014-08-01",
    "publication_year": 2014,
    "authors": "Xiangyu Wang; Yong Rui; Mohan Kankanhalli",
    "corresponding_authors": "",
    "abstract": "The amount of multimedia data on the Internet has increased exponentially in the past few decades and this trend is likely to continue. Multimedia content inherently has multiple information sources, therefore effective fusion methods are critical for data analysis and understanding. So far, most of the existing fusion methods are static with respect to time, making it difficult for them to handle the evolving multimedia content. To address this issue, in recent years, several evolving fusion methods were proposed, however, their requirements are difficult to meet, making them useful only in limited applications. In this article, we propose a novel evolving fusion method based on the online portfolio selection theory. The proposed method takes into account the correlation among different information sources and evolves the fusion model when new multimedia data is added. It performs effectively on both crisp and soft decisions without requiring additional context information. Extensive experiments on concept detection and human detection tasks over the TRECVID dataset and surveillance data have been conducted and significantly better performance has been obtained.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W1973559818",
    "type": "article"
  },
  {
    "title": "Beyond 1Mbps Global Overlay Live Streaming",
    "doi": "https://doi.org/10.1145/2652485",
    "publication_date": "2015-01-07",
    "publication_year": 2015,
    "authors": "Dongni Ren; Yisheng Xu; S.-H. Gary Chan",
    "corresponding_authors": "",
    "abstract": "In order to provide live streaming over the global Internet, a content provider often deploys an overlay network consisting of distributed proxies placed close to user pools. Streaming of multi-Mbps video over such an overlay is challenging because of bandwidth bottlenecks in paths. To effectively overcome these bottlenecks, we consider employing proxy helpers in the overlay to provide rich path diversity. The helpers do not have any attached users, and hence may forward partial video streams (or not at all) if necessary. In this way, the helpers serve as stepping stones to supply full streams to the servers. The issue is how to involve the helpers in the overlay to achieve low streaming delay meeting a certain high streaming bitrate requirement. To address the issue, we first formulate the problem which captures various delay and bandwidth components, and show that it is NP-hard. We then propose an efficient algorithm called Stepping-Stones (SS) which can be efficiently implemented in a controller. Given the encouraging simulation results, we develop a novel streaming testbed for SS and explore, through sets of Internet experiments, the effectiveness of helpers to achieve high bitrate (multi-Mbps) global live streaming. In our experiments, proxies are deployed with a reasonably wide global footprint. We collect more than a hundred hours of streaming traces with bitrate ranging from 500kbps to a few Mbps. Our experimental data validates that helpers indeed play an important role in achieving high bitrate in today's Internet. Global multi-Mbps streaming is possible due to their multihop and multipath advantages. Our experimental trials and data also provide valuable insights on the design of a global push-based streaming network. There are strong benefits of using proxy helpers to achieve high bitrate and low delay.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W1999907841",
    "type": "article"
  },
  {
    "title": "Efficient computation of queries on feature streams",
    "doi": "https://doi.org/10.1145/2043612.2043616",
    "publication_date": "2011-11-01",
    "publication_year": 2011,
    "authors": "Simone Santini",
    "corresponding_authors": "Simone Santini",
    "abstract": "This article introduces the notion of virtual feature stream , a feature stream defined from a primary data stream, in which at any time only the features that are needed to compute the queries that are currently running in the system are computed. Virtual feature streams are, in general, impossible to determine a priori, but the paper introduces an algorithm that stops the computation of features as soon as it can be proved that they are no longer needed thus generating, albeit in a roundabout and more expensive than the ideal way, a feature stream that is less expensive than the complete one to compute and safe: the queries that accept the virtual feature stream are those (and only those) that would accept the original feature stream.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2001523045",
    "type": "article"
  },
  {
    "title": "OSM",
    "doi": "https://doi.org/10.1145/2543899",
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "Ahsan Arefin; Raoul Rivas; Klara Nahrstedt",
    "corresponding_authors": "",
    "abstract": "Different 3D tele-immersive (3DTI) activities pose different prioritized requirements for application and network-level quality of service (QoS) to ensure a strong quality of experience (QoE) for participants. Some applications put heavy weight on audio quality, some consider higher quality for upper body video streams, and some seek very low end-to-end interactivity delay. In addition, a variation in streaming content may arise in the 3DTI space due to the participants' change in interests (e.g., view change). Therefore, there is a need for an adaptive multistream, multisite 3DTI session management strategy that is not only unobtrusive, but also optimizes prioritized QoS parameters in 3DTI content distribution based on user activity and content variation. To address this next generation session management problem, we revisit the design space of multistream and multisite 3DTI session layer. We present an evolutionary 3DTI session optimization approach using Open Session Management (OSM) architecture that uses a global view of participants and overlays network conditions to optimize QoS parameters. Experimental results with PlanetLab traces show that our optimization process is unobtrusive, and the optimized TI sessions provide higher satisfaction to the participants (in some cases up to 50% higher) compared to the current solutions in the 3DTI space.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2026038487",
    "type": "article"
  },
  {
    "title": "A new data hiding method via revision history records on collaborative writing platforms",
    "doi": "https://doi.org/10.1145/2534408",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "Ya-Lin Lee; Wen‐Hsiang Tsai",
    "corresponding_authors": "",
    "abstract": "A new data hiding method via collaboratively-written articles with forged revision history records on collaborative writing platforms is proposed. The hidden message is camouflaged as a stego-document consisting of a stego-article and a revision history created through a simulated process of collaborative writing. The revisions are forged using a database constructed by mining word sequences used in real cases from an English Wikipedia XML dump. Four characteristics of article revisions are identified and utilized to embed secret messages, including the author of each revision, the number of corrected word sequences, the content of the corrected word sequences, and the word sequences replacing the corrected ones. Related problems arising in utilizing these characteristics for data hiding are identified and solved skillfully, resulting in an effective multiway method for hiding secret messages into the revision history. To create more realistic revisions, Huffman coding based on the word sequence frequencies collected from Wikipedia is applied to encode the word sequences. Good experimental results show the feasibility of the proposed method.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2052652977",
    "type": "article"
  },
  {
    "title": "APRICOD",
    "doi": "https://doi.org/10.1145/2457450.2457457",
    "publication_date": "2013-05-01",
    "publication_year": 2013,
    "authors": "Zhen Zhao; Wei Tsang Ooi",
    "corresponding_authors": "",
    "abstract": "Content discovery is a major source of latency in peer-to-peer (P2P) media streaming systems, especially in the presence of noncontinuous user access, such as random seek in Video-on-Demand (VoD) streaming and teleportation in a Networked Virtual Environment (NVE). After the aforementioned user interactions, streaming systems often need to initiate the content discovery process to identify where to retrieve the requested media objects. Short content lookup latency is demanded to ensure smooth user experience. Existing content discovery systems based on either a Distributed Hash Table (DHT) or gossip mechanism cannot cope with noncontinuous access efficiently due to their long lookup latency. In this work, we propose an access-pattern-driven distributed caching middleware named APRICOD, which caters for fast and scalable content discovery in peer-to-peer media streaming systems, especially when user interactions are present. APRICOD exploits correlations among media objects accessed by users, and adapts to shift in the user access pattern automatically. We first present a general APRICOD design that can be used with any existing content discovery system. We then present an implementation of APRICOD on top of Pastry, which we use to evaluate APRICOD. Our evaluation in a 1024-node system, using a Second Life trace with 5,735 users and a VoD trace with 54 users, shows that APRICOD can effectively resolve all continuous access queries with a single hop deterministically with node failure as an exception, and resolve noncontinuous access queries with a single hop with high probability.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2088465559",
    "type": "article"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/2978431",
    "publication_date": "2016-09-15",
    "publication_year": 2016,
    "authors": "Zheng Yan; Jun Liu; Robert H. Deng; Francisco Herrera",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2519033667",
    "type": "editorial"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2788342",
    "publication_date": "2015-06-02",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "In this article, we take advantage of the user behavior of requesting videos from the top of the related list provided by YouTube to improve the performance of YouTube caches. We recommend that local caches reorder the related lists associated with ...",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4246896450",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2422956",
    "publication_date": "2013-02-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Hundreds of millions of people play computer games every day. For them, game content—from 3D objects to abstract puzzles—plays a major entertainment role. Manual labor has so far ensured that the quality and quantity of game content matched the demands ...",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4247462843",
    "type": "paratext"
  },
  {
    "title": "An information-based dynamic extrapolation model for networked virtual environments",
    "doi": "https://doi.org/10.1145/2240136.2240140",
    "publication_date": "2012-07-01",
    "publication_year": 2012,
    "authors": "Xin Zhang; Tomás Ward; Séamus McLoone",
    "corresponding_authors": "",
    "abstract": "Various Information Management techniques have been developed to help maintain a consistent shared virtual world in a Networked Virtual Environment. However, such techniques have to be carefully adapted to the application state dynamics and the underlying network. This work presents a novel framework that minimizes inconsistency by optimizing bandwidth usage to deliver useful information. This framework measures the state evolution using an information model and dynamically switches extrapolation models and the packet rate to make the most information-efficient usage of the available bandwidth. The results shown demonstrate that this approach can help optimize consistency under constrained and time-varying network conditions.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W1977116216",
    "type": "article"
  },
  {
    "title": "A novel 3D video transcoding scheme for adaptive 3D video transmission to heterogeneous terminals",
    "doi": "https://doi.org/10.1145/2348816.2348822",
    "publication_date": "2012-09-01",
    "publication_year": 2012,
    "authors": "Shujie Liu; Chang Wen Chen",
    "corresponding_authors": "",
    "abstract": "Three-dimensional video (3DV) is attracting many interests with its enhanced viewing experience and more user driven features. 3DV has several unique characteristics different from 2D video: (1) It has a much larger amount of data captured and compressed, and corresponding video compression techniques can be much more complicated in order to explore data redundancy. This will lead to more constraints on users' network access and computational capability, (2) Most users only need part of the 3DV data at any given time, while the users' requirements exhibit large diversity, (3) Only a limited number of views are captured and transmitted for 3DV. View rendering is thus necessary to generate virtual views based on the received 3DV data. However, many terminal devices do not have the functionality to generate virtual views. To enable 3DV experience for the majority of users with limited capabilities, adaptive 3DV transmission is necessary to extract/generate the required data content and represent it with supported formats and bitrates for heterogeneous terminal devices. 3DV transcoding is an emerging and effective technique to achieve desired adaptive 3DV transmission. In this article, we propose the first efficient 3DV transcoding scheme that can obtain any desired view, either an encoded one or a virtual one, and compress it with more universal H.264/AVC. The key idea of the proposed scheme is to appropriately utilize motion information contained in the bitstream to generate candidate motion information. Original information of both the desired view and reference views are used to obtain this candidate information and a proper motion refinement process is carried out for certain blocks. Simulation results show that, compared to the straightforward cascade algorithm, the proposed scheme is able to output compressed bitstream of the required view with significantly reduced complexity while incurring negligible performance loss. Such a 3DV transcoding can be applied to most gateways that usually have constraints on computational complexity and time delay.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W1991403741",
    "type": "article"
  },
  {
    "title": "Word level automatic alignment of music and lyrics using vocal synthesis",
    "doi": "https://doi.org/10.1145/1823746.1823753",
    "publication_date": "2010-08-01",
    "publication_year": 2010,
    "authors": "Namunu C. Maddage; Khe Chai Sim; Haizhou Li",
    "corresponding_authors": "",
    "abstract": "We propose a signal-based approach instead of the commonly used model-based approach, to automatically align vocal music with text lyrics at the word level. In this approach, we use a text-to-speech system to synthesize the singing voice according to the lyrics. In this way, aligning the music signal with the corresponding text lyrics becomes the alignment of two audio signals. This study uses the results of music information modeling and singing voice synthesis. In music information modeling, we study different music representation strategies for music segmentation, music region indexing and region content descriptions; in singing voice synthesis, we generate singing voice by making use of music knowledge to approximate the target vocal line in terms of tempo. The experimental results on a 20-song database show 26.3% and 36.1% word level alignment error rates at eighth note and sixteenth note alignment tolerances respectively. The proposed approach presents an alternative and effective solution to music-lyrics alignment which may require less training dataset.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2028766658",
    "type": "article"
  },
  {
    "title": "Algorithms for stochastic optimization of multicast content delivery with network coding",
    "doi": "https://doi.org/10.1145/2379790.2379798",
    "publication_date": "2012-11-01",
    "publication_year": 2012,
    "authors": "Ajay Gopinathan; Zongpeng Li",
    "corresponding_authors": "",
    "abstract": "The usage of network resources by content providers is commonly governed by Service-Level Agreements (SLA) between the content provider and the network service provider. Resource usage exceeding the limits specified in the SLA incurs the content provider additional charges, usually at a higher cost. Hence, the content provider's goal is to provision adequate resources in the SLA based on forecasts of future demand. We study capacity purchasing strategies when the content provider employs network coded multicast as the media delivery mechanism, with uncertainty in its future customer set explicitly taken into consideration. The latter requires the content provider to make capacity provisioning decisions based on market predictions and historical customer usage patterns. The probabilistic element suggests a stochastic optimization approach. We model this problem as a two-stage stochastic optimization problem with recourse. Such optimizations are #P-hard to solve directly, and we design two approximation algorithms for them. The first is a heuristic algorithm that exploits properties unique to network coding, so that only polynomial-time operations are needed. It performs well in general scenarios, but the gap from the optimal solution is not bounded by any constant in the worst case. This motivates our second approach, a sampling algorithm partly inspired from the work of Gupta et al. [2004a]. We employ techniques from duality theory in linear optimization to prove that the sampling algorithm provides a 3-approximation to the stochastic multicast problem. We conduct extensive simulations to illustrate the efficacy of both algorithms, and show that the performance of both is usually within 10% of the optimal solution in practice.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2050313358",
    "type": "article"
  },
  {
    "title": "Interactive television news",
    "doi": "https://doi.org/10.1145/2168996.2168999",
    "publication_date": "2012-05-01",
    "publication_year": 2012,
    "authors": "Dan R. Olsen; Derek W. Bunn; Trent Boulter; Robert Walz",
    "corresponding_authors": "",
    "abstract": "A new interactive television experience has been created for watching television news. The goal is to create a news experience that is similar to the way people watch television in their living rooms while giving viewers the power to make choices about what they see. We partnered with existing news organizations to create tools consistent with current news production practices. The viewer experience allows selection of the order of news content, skipping unwanted content and exploring stories in more depth. These tools were used to produce seven days of interactive commercial news that were viewed in ten homes.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2050417000",
    "type": "article"
  },
  {
    "title": "Efficient delivery of on-demand video streams to heterogeneous receivers",
    "doi": "https://doi.org/10.1145/1823746.1823754",
    "publication_date": "2010-08-01",
    "publication_year": 2010,
    "authors": "Bashar Qudah; Nabil J. Sarhan",
    "corresponding_authors": "",
    "abstract": "The number of video streams that can be serviced concurrently is highly constrained by the required real-time and high-rate transfers of multimedia data. Resource sharing techniques, such as Batching, Patching, and Earliest Reachable Merge Target (ERMT), can be used to address this problem by utilizing the multicast facility, which allows multiple requests to share the same set of server and network resources. They assume, however, that all clients have the same available download bandwidth and buffer space. We study how to efficiently support clients with varying available download bandwidth and buffer space, while delivering data in a client-pull fashion using enhanced resource sharing. In particular, we propose three hybrid solutions to address the variability in the download bandwidth among clients: Simple Hybrid Solution (SHS), Adaptive Hybrid Solution (AHS), and Enhanced Hybrid Solution (EHS). SHS simply combines Batching with either Patching or ERMT, leading to two alternatives: SHS-P and SHS-E , respectively. Batching is used for clients with bandwidth lower than double the video playback rate, and Patching/ERMT is used for the rest. In contrast, AHS and EHS classify clients into multiple bandwidth classes and service them accordingly. AHS employs a new stream type, called adaptive stream , and EHS employs an enhanced adaptive stream type to serve clients with bandwidth capacities ranging between the video playback rate and double that rate. AHS and EHS employ adaptive streams or enhanced adaptive streams in conjunction with Batching and Patching or ERMT, leading to four possible schemes: AHS-P, AHS-E, EHS-P, and EHS-E. Moreover, we consider the variability of the available buffer space among clients. Furthermore, we study how the waiting playback requests for different videos can be scheduled for service in the heterogeneous environment, capturing the variations in both the client bandwidth and buffer space. We evaluate the effectiveness of the proposed solutions and analyze various scheduling policies through extensive simulation.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2068500093",
    "type": "article"
  },
  {
    "title": "Automated Orchestration of Online Educational Collaboration in Cloud-based Environments",
    "doi": "https://doi.org/10.1145/3412381",
    "publication_date": "2021-02-28",
    "publication_year": 2021,
    "authors": "Łukasz Czekierda; Krzysztof Zieliński; Sławomir Zieliński",
    "corresponding_authors": "",
    "abstract": "Integrated collaboration environments (ICEs) are widely used by corporations to increase productivity by fostering groupwide and interpersonal collaboration. In this article, we discuss the enhancements of such environment needed to build an educational ICE (E-ICE) that addresses the specific needs of educational users. The motivation for the research was the Małopolska Educational Cloud (MEC) project conducted by AGH University and its partners. The E-ICE developed by MEC project fosters collaboration between universities and high schools by creating an immersive virtual collaboration space. MEC is a unique project due to its scale and usage domain. Multiple online collaboration events are organized weekly between over 150 geographically scattered institutions. Such events, aside from videoconferencing, require various services. The MEC E-ICE is a complex composition of a significant number of services and various terminals that require very specific configuration and management. In this article, we focus on a model-driven approach to automating the organization of online meetings in their preparation, execution, and conclusion phases. We present a conceptual model of E-ICE-supported educational courses, introduce a taxonomy of online educational services, identify planes and modes of their operation, as well as discuss the most common collaboration patterns. The MEC E-ICE, which we present as a case study, is built in accordance with the presented, model-driven approach. MEC educational services are described in a way that allows for converting the declarative specification of E-ICE application models into platform-independent models, platform-specific models, and, finally, working sets of orchestrated service instances. Such approach both reduces the level of technical knowledge required from the end-users and considerably speeds up the construction of online educational collaboration environments.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3155208984",
    "type": "article"
  },
  {
    "title": "Affinity Derivation for Accurate Instance Segmentation",
    "doi": "https://doi.org/10.1145/3407090",
    "publication_date": "2021-02-28",
    "publication_year": 2021,
    "authors": "Yiding Liu; Siyu Yang; Bin Li; Wengang Zhou; Jizheng Xu; Houqiang Li; Yan Lu",
    "corresponding_authors": "",
    "abstract": "Affinity, which represents whether two pixels belong to a same instance, is an equivalent representation to the instance segmentation labels. Conventional works do not make an explicit exploration on the affinity. In this article, we present two instance segmentation schemes based on pixel affinity information and show the effectiveness of affinity in both aspects. For proposal-free method, we predict pixel affinity for each image and then propose a simple yet effective graph merge algorithm to cluster pixels into instances. It shows that the affinity is powerful as an instance-relevant information to guide the clustering procedure in proposal-free instance segmentation. For proposal-based methods, we extend conventional framework with affinity head and introduce affinity as attached supervision in training phase. Without any additional inference cost, we can improve the performance of existing proposal-based instance segmentation methods, which shows that the affinity can also be applied as an auxiliary loss and training with such extra loss is beneficial to the training progress. Experimental results show that our schemes achieve comparable performance to other state-of-the-art instance segmentation methods. With Cityscapes training data, the proposed proposal-free method achieves 28.8 AP and the proposal-based method gets 27.2 AP both on test sets.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3155754834",
    "type": "article"
  },
  {
    "title": "Semantic Correspondence with Geometric Structure Analysis",
    "doi": "https://doi.org/10.1145/3441576",
    "publication_date": "2021-07-22",
    "publication_year": 2021,
    "authors": "Rui Wang; Dong Liang; Xiaochun Cao; Yuanfang Guo",
    "corresponding_authors": "",
    "abstract": "This article studies the correspondence problem for semantically similar images, which is challenging due to the joint visual and geometric deformations. We introduce the Flip-aware Distance Ratio method (FDR) to solve this problem from the perspective of geometric structure analysis. First, a distance ratio constraint is introduced to enforce the geometric consistencies between images with large visual variations, whereas local geometric jitters are tolerated via a smoothness term. For challenging cases with symmetric structures, our proposed method exploits Curl to suppress the mismatches. Subsequently, image correspondence is formulated as a permutation problem, for which we propose a Gradient Guided Simulated Annealing (GGSA) algorithm to perform a robust discrete optimization. Experiments on simulated and real-world datasets, where both visual and geometric deformations are present, indicate that our method significantly improves the baselines for both visually and semantically similar images.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3184092807",
    "type": "article"
  },
  {
    "title": "Payoff-based Dynamic Segment Replication and Graph Classification Method with Attribute Vectors Adapted to Urban VANET",
    "doi": "https://doi.org/10.1145/3440018",
    "publication_date": "2021-08-16",
    "publication_year": 2021,
    "authors": "Bechir Alaya",
    "corresponding_authors": "Bechir Alaya",
    "abstract": "Due to the number of constraints and the dynamic nature of vehicular ad hoc networks (VANET), effective video broadcasting always remains a difficult task. In this work, we proposed a quality of video visualization guarantee model based on a feedback loop and an efficient algorithm for segmenting and replicating video segments using the Payoff-based Dynamic Segment Replication Policy (P-DSR). In the urban VANET environment, P-DSR is defined by taking into account the position of the vehicles, the speed, the direction, the number of neighboring vehicles, and the reputation of each node to stabilize the urban VANET topology. However, the management of various load control parameters between the different components of the urban VANET network remains a problem to be studied. This work uses a multi-objective problem that takes the parameters of our algorithm based on the Graph Classification Method with Attribute Vectors (GCMAV) as input. This algorithm aims to provide an improved class lifetime, an improved video segment delivery rate, a reduced inter-class overload, and an optimization of a global criterion. A scalable algorithm is used to optimize the parameters of the GCMAV. The simulations were carried out using the NetSim simulator and Multi-Objective Evolutionary Algorithms framework to optimize parameters. Experiments were carried out with realistic maps of Open Street Maps and its results were compared with other algorithms such as Seamless and Authorized Multimedia Streaming and P-DSR. The survey suggests that the proposed methodology works well concerning the average lifetime of the inter-classes and the delivery rate of video segments.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3196079534",
    "type": "article"
  },
  {
    "title": "Dissimilarity-Based Regularized Learning of Charts",
    "doi": "https://doi.org/10.1145/3458884",
    "publication_date": "2021-11-12",
    "publication_year": 2021,
    "authors": "Prerna Mishra; Santosh Kumar; Mithilesh Kumar Chaube",
    "corresponding_authors": "",
    "abstract": "Chart images exhibit significant variabilities that make each image different from others even though they belong to the same class or categories. Classification of charts is a major challenge because each chart class has variations in features, structure, and noises. However, due to the lack of affiliation between the dissimilar features and the structure of the chart, it is a challenging task to model these variations for automatic chart recognition. In this article, we present a novel dissimilarity-based learning model for similar structured but diverse chart classification. Our approach jointly learns the features of both dissimilar and similar regions. The model is trained by an improved loss function, which is fused by a structural variation-aware dissimilarity index and incorporated with regularization parameters, making the model more prone toward dissimilar regions. The dissimilarity index enhances the discriminative power of the learned features not only from dissimilar regions but also from similar regions. Extensive comparative evaluations demonstrate that our approach significantly outperforms other benchmark methods, including both traditional and deep learning models, over publicly available datasets.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3212505973",
    "type": "article"
  },
  {
    "title": "Semantic Explanation for Deep Neural Networks Using Feature Interactions",
    "doi": "https://doi.org/10.1145/3474557",
    "publication_date": "2021-10-31",
    "publication_year": 2021,
    "authors": "Bohui Xia; Xueting Wang; Toshihiko Yamasaki",
    "corresponding_authors": "",
    "abstract": "Given the promising results obtained by deep-learning techniques in multimedia analysis, the explainability of predictions made by networks has become important in practical applications. We present a method to generate semantic and quantitative explanations that are easily interpretable by humans. The previous work to obtain such explanations has focused on the contributions of each feature, taking their sum to be the prediction result for a target variable; the lack of discriminative power due to this simple additive formulation led to low explanatory performance. Our method considers not only individual features but also their interactions, for a more detailed interpretation of the decisions made by networks. The algorithm is based on the factorization machine, a prediction method that calculates factor vectors for each feature. We conducted experiments on multiple datasets with different models to validate our method, achieving higher performance than the previous work. We show that including interactions not only generates explanations but also makes them richer and is able to convey more information. We show examples of produced explanations in a simple visual format and verify that they are easily interpretable and plausible.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3212573592",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on Explainable AI on Multimedia Computing",
    "doi": "https://doi.org/10.1145/3489522",
    "publication_date": "2021-10-31",
    "publication_year": 2021,
    "authors": "Wen-Huang Cheng; Jiaying Liu; Nicu Sebe; Junsong Yuan; Hong-Han Shuai",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3213028131",
    "type": "article"
  },
  {
    "title": "Offering data confidentiality for multimedia overlay multicast",
    "doi": "https://doi.org/10.1145/1413862.1413866",
    "publication_date": "2008-11-01",
    "publication_year": 2008,
    "authors": "Wai-Pun Ken Yiu; S.-H. Gary Chan",
    "corresponding_authors": "",
    "abstract": "Application layer multicast (ALM) has been proposed to overcome current limitations in IP multicast for large-group multimedia communication. We address offering data confidentiality tailored for ALM. To achieve confidentiality, a node may need to continuously re-encrypt packets before forwarding them downstream. Furthermore, keys have to be changed whenever there is a membership change, leading to rekey processing overhead at the nodes. For a large and dynamic group, these reencryption and rekeying operations incur high processing overhead at the nodes. We propose and analyze a scalable scheme called Secure Overlay Multicast (SOM) which clusters ALM peers so as to localize rekeying within a cluster and to limit re-encryption at cluster boundaries, thereby minimizing the total nodal processing overhead. We describe the operations of SOM and compare its nodal processing overhead with two other basic approaches, namely, host-to-host encryption and whole group encryption. We also present a simplified analytic model for SOM and show that there exists an optimal cluster size to minimize the total nodal processing overhead. By comparing with a recently proposed ALM scheme (DT protocol), SOM achieves a substantial reduction in nodal processing overhead with similar network performance in terms of network stress and delay.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W1999399362",
    "type": "article"
  },
  {
    "title": "Data placement and prefetching with accurate bit rate control for interactive media server",
    "doi": "https://doi.org/10.1145/1386109.1386114",
    "publication_date": "2008-08-01",
    "publication_year": 2008,
    "authors": "Seung‐Ho Lim; Yo-Won Jeong; Kyu Ho Park",
    "corresponding_authors": "",
    "abstract": "An interactive Media Server should support unrestricted control to viewers with their service level agreements. It is important to manage video data effectively to facilitate efficient retrieval. In this paper, we propose an efficient placement algorithm as part of an effective retrieval scheme to increase the number of clients who can be provided with interactive service. The proposed management schemes are incorporated with a bit count control method that is based on repeated tuning of quantization parameters to adjust the actual bit count to the target bit count. The encoder using this method can generate coded frames whose sizes are synchronized with the RAID stripe size, so that when various fast-forward levels are accessed we can reduce the seek and rotational latency and enhance the disk throughput of each disk in the RAID system. Experimental results demonstrate that the proposed schemes can significantly improve the average service time and guarantee more users service of quality, and the interactive media server can thereby efficiently service a large number of clients.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2070910100",
    "type": "article"
  },
  {
    "title": "Securing Speech Noise Reduction in Outsourced Environment",
    "doi": "https://doi.org/10.1145/3105970",
    "publication_date": "2017-08-12",
    "publication_year": 2017,
    "authors": "Abukari Mohammed Yakubu; Namunu C. Maddage; Pradeep K. Atrey",
    "corresponding_authors": "",
    "abstract": "Cloud data centers (CDCs) are becoming a cost-effective method for processing and storage of multimedia data including images, video, and audio. Since CDCs are physically located in different jurisdictions, and are managed by external parties, data security is a growing concern. Data encryption at CDCs is commonly practiced to improve data security. However, to process the data at CDCs, data must often be decrypted, which raises issues in security. Thus, there is a growing demand for data processing techniques in encrypted domain in such an outsourced environment. In this article, we analyze encrypted domain speech content processing techniques for noise reduction. Noise contaminates speech during transmission or during the acquisition process by recording. As a result, the quality of the speech content is degraded. We apply Shamir’s secret sharing as the cryptosystem to encrypt speech data before uploading it to a CDC. We then propose finite impulse response digital filters to reduce white and wind noise in the speech in the encrypted domain. We prove that our proposed schemes meet the security requirements of efficiency, accuracy, and checkability for both semi-honest and malicious adversarial models. Experimental results show that our proposed filtering techniques for speech noise reduction in the encrypted domain produce similar results when compared to plaintext domain processing.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2745390760",
    "type": "article"
  },
  {
    "title": "This is the Table of Contents for the most recent online-only supplemental issue TOMM 13(3s). Please find this supplemental issue in the ACM Digital Library and enjoy reading them!",
    "doi": "https://doi.org/10.1145/3143786",
    "publication_date": "2017-10-09",
    "publication_year": 2017,
    "authors": "Minh Son Dao",
    "corresponding_authors": "Minh Son Dao",
    "abstract": "No abstract available.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2763066343",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Section on Multimedia Computing and Applications of Socio-Affective Behaviors in the Wild",
    "doi": "https://doi.org/10.1145/3181711",
    "publication_date": "2018-03-26",
    "publication_year": 2018,
    "authors": "Fabien Ringeval; Björn W. Schuller; Michel Valstar; Jonathan Gratch; Roddy Cowie; Maja Pantić",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2795043468",
    "type": "article"
  },
  {
    "title": "Properties and Design of Variable-to-Variable Length Codes",
    "doi": "https://doi.org/10.1145/3230653",
    "publication_date": "2018-07-24",
    "publication_year": 2018,
    "authors": "Heiner Kirchhoffer; Detlev Marpe; Heiko Schwarz; Thomas Wiegand",
    "corresponding_authors": "",
    "abstract": "For the entropy coding of independent and identically distributed (i.i.d.) binary sources, variable-to-variable length (V2V) codes are an interesting alternative to arithmetic coding. Such a V2V code translates variable length words of the source into variable length code words by employing two prefix-free codes. In this article, several properties of V2V codes are studied, and new concepts are developed. In particular, it is shown that the redundancy of a V2V code cannot be zero for a binary i.i.d. source {X} with 0 &lt; p X (1) &lt; 0.5. Furthermore, the concept of prime and composite V2V codes is proposed, and it is shown why composite V2V codes can be disregarded in the search for particular classes of minimum redundancy codes. Moreover, a canonical representation for V2V codes is proposed, which identifies V2V codes that have the same average code length function. It is shown how these concepts can be employed to greatly reduce the complexity of a search for minimum redundancy (size-limited) V2V codes.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2884076773",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3058792",
    "publication_date": "2017-05-05",
    "publication_year": 2017,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "In this article, we introduce a method to overcome one of the main challenges of person reidentification in multicamera networks, namely cross-view appearance changes. The proposed solution addresses the extreme variability of person appearance in ...",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4234349554",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1412196",
    "publication_date": "2008-10-01",
    "publication_year": 2008,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The research area of interactive digital TV is in the midst of a significant revival. Unlike the first generation of digital TV, which focused on producer concerns that effectively limited (re)distribution, the current generation of research is closely ...",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4240677288",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3129737",
    "publication_date": "2017-10-26",
    "publication_year": 2017,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Label imbalance and the insufficiency of labeled training samples are major obstacles in most methods for counting people in images or videos. In this work, a sparse representation-based semi-supervised regression method is proposed to count people in ...",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4243192814",
    "type": "paratext"
  },
  {
    "title": "Game of Streaming Players",
    "doi": "https://doi.org/10.1145/3336496",
    "publication_date": "2019-04-30",
    "publication_year": 2019,
    "authors": "Abdelhak Bentaleb; Ali C. Begen; Saad Harous; Roger Zimmermann",
    "corresponding_authors": "",
    "abstract": "The dramatic growth of HTTP adaptive streaming (HAS) traffic represents a practical challenge for service providers in satisfying the demand from their customers. Achieving this in a network where multiple players share the network capacity has so far proved hard because of the bandwidth competition among the HAS players. This competition is exacerbated by the bandwidth overestimation that is introduced due to the isolated and selfish behavior of the HAS players. Each player strives individually to select the maximum bitrate without considering the co-existing players or network resource dynamics. As a result, the HAS players suffer from video quality instability, quality unfairness, and network underutilization or oversubscription, and the players observe a poor quality of experience (QoE). To address this issue, we propose a fully distributed game theory and consensus-based collaborative adaptive bitrate solution for shared network environments, termed Game Theory and consensus-based Approach for Cooperative HAS delivery systems (GTAC). Our solution consists of two-stage games that run in parallel during a streaming session. We extensively evaluate GTAC on a broad set of trace-driven and real-world experiments. Results show that GTAC enhances the viewer QoE by up to 22%, presentation quality stability by up to 24%, fairness by at least 31%, and network utilization by 28% compared to the well-known schemes.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2964262904",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on the Cross-Media Analysis for Visual Question Answering",
    "doi": "https://doi.org/10.1145/3337985",
    "publication_date": "2019-04-30",
    "publication_year": 2019,
    "authors": "Richang Hong; Yahong Han; Tat‐Seng Chua",
    "corresponding_authors": "",
    "abstract": "editorial Free Access Share on Introduction to the Special Issue on the Cross-Media Analysis for Visual Question Answering Editors: Richang Hong Hefei University of Technology, China Hefei University of Technology, ChinaView Profile , Yahong Han Tianjin University, China Tianjin University, ChinaView Profile , Tat-Seng Chua National University of Singapore, Singapore National University of Singapore, SingaporeView Profile Authors Info & Claims ACM Transactions on Multimedia Computing, Communications, and ApplicationsVolume 15Issue 2sApril 2019 Article No.: 48pp 1–3https://doi.org/10.1145/3337985Published:03 July 2019Publication History 1citation368DownloadsMetricsTotal Citations1Total Downloads368Last 12 Months72Last 6 weeks9 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteView all FormatsPDF",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2965950070",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on Big Data, Machine Learning, and AI Technologies for Art and Design",
    "doi": "https://doi.org/10.1145/3338002",
    "publication_date": "2019-04-30",
    "publication_year": 2019,
    "authors": "James She",
    "corresponding_authors": "James She",
    "abstract": "editorial Free Access Share on Introduction to the Special Issue on Big Data, Machine Learning, and AI Technologies for Art and Design Editor: James She Social Media Lab., Hong Kong Uni. of Science 8 Technology, H.K. Social Media Lab., Hong Kong Uni. of Science 8 Technology, H.K.View Profile Authors Info & Claims ACM Transactions on Multimedia Computing, Communications, and ApplicationsVolume 15Issue 2sApril 2019 Article No.: 57pp 1–3https://doi.org/10.1145/3338002Published:19 July 2019Publication History 1citation653DownloadsMetricsTotal Citations1Total Downloads653Last 12 Months169Last 6 weeks25 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteView all FormatsPDF",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2967747063",
    "type": "article"
  },
  {
    "title": "A Pseudo-likelihood Approach for Geo-localization of Events from Crowd-sourced Sensor-Metadata",
    "doi": "https://doi.org/10.1145/3321701",
    "publication_date": "2019-08-20",
    "publication_year": 2019,
    "authors": "Amit More; Subhasis Chaudhuri",
    "corresponding_authors": "",
    "abstract": "Events such as live concerts, protest marches, and exhibitions are often video recorded by many people at the same time, typically using smartphone devices. In this work, we address the problem of geo-localizing such events from crowd-generated data. Traditional approaches for solving such a problem using multiple video sequences of the event would require highly complex computer vision (CV) methods, which are computation intensive and are not robust under the environment where visual data are collected through crowd-sourced medium. In the present work, we approach the problem in a probabilistic framework using only the sensor metadata obtained from smartphones. We model the event location and camera locations and orientations (camera parameters) as the hidden states in a Hidden Markov Model. The sensor metadata from GPS and the digital compass from user smartphones are used as the observations associated with the hidden states of the model. We have used a suitable potential function to capture the complex interaction between the hidden states (i.e., event location and camera parameters). The non-Gaussian densities involved in the model, such as the potential function involving hidden states, make the maximum-likelihood estimation intractable. We propose a pseudo-likelihood-based approach to maximize the approximate-likelihood, which provides a tractable solution to the problem. The experimental results on the simulated as well as real data show correct event geo-localization using the proposed method. When compared with several baselines the proposed method shows a superior performance. The overall computation time required is much smaller, since only the sensor metadata are used instead of visual data.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2969790139",
    "type": "article"
  },
  {
    "title": "Random Playlists Smoothly Commuting Between Styles",
    "doi": "https://doi.org/10.1145/3361742",
    "publication_date": "2019-11-30",
    "publication_year": 2019,
    "authors": "Marcos Alves de Almeida; Carolina Coimbra Vieira; Pedro O. S. Vaz de Melo; Renato Assunção",
    "corresponding_authors": "",
    "abstract": "Someone enjoys listening to playlists while commuting. He wants a different playlist of n songs each day, but always starting from Locked Out of Heaven , a Bruno Mars song. The list should progress in smooth transitions between successive and randomly selected songs until it ends up at Stairway to Heaven , a Led Zeppelin song. The challenge of automatically generating random and heterogeneous playlists is to find the appropriate balance among several conflicting goals. We propose two methods for solving this problem. One is called ROPE , and it depends on a representation of the songs in a Euclidean space. It generates a random path through a Brownian Bridge that connects any two songs selected by the user in this music space. The second is STRAW , which constructs a graph representation of the music space where the nodes are songs and edges connect similar songs. STRAW creates a playlist by traversing the graph through a steering random walk that starts on a selected song and is directed toward a target song also selected by the user. When compared with the state-of-the-art algorithms, our algorithms are the only ones that satisfy the following quality constraints: heterogeneity , smooth transitions , novelty , scalability , and usability . We demonstrate the usefulness of our proposed algorithms by applying them to a large collection of songs and make available a prototype.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2995121024",
    "type": "article"
  },
  {
    "title": "RCE-HIL",
    "doi": "https://doi.org/10.1145/3365003",
    "publication_date": "2020-02-17",
    "publication_year": 2020,
    "authors": "Xin Huang; Yuxin Peng; Wen Zhang",
    "corresponding_authors": "",
    "abstract": "Entailment recognition is an important paradigm of reasoning that judges if a hypothesis can be inferred from given premises. However, previous efforts mainly concentrate on text-based reasoning as recognizing textual entailment (RTE) , where the hypotheses and premises are both textual. In fact, humans’ reasoning process has the characteristic of cross-media reasoning . It is naturally based on the joint inference with different sensory organs, which represent complementary reasoning cues from unique perspectives as language, vision, and audition. How to realize cross-media reasoning has been a significant challenge to achieve the breakthrough for width and depth of entailment recognition. Therefore, this article extends RTE to a novel reasoning paradigm: recognizing cross-media entailment (RCE) , and proposes heterogeneous interactive learning (HIL) approach. Specifically, HIL recognizes entailment relationships via cross-media joint inference, from image-text premises to text hypotheses. It is an end-to-end architecture with two parts: (1) Cross-media hybrid embedding is proposed to perform cross embedding of premises and hypotheses for generating their fine-grained representations. It aims to achieve the alignment of cross-media inference cues via image-text and text-text interactive attention. (2) Heterogeneous joint inference is proposed to construct a heterogeneous interaction tensor space and extract semantic features for entailment recognition. It aims to simultaneously capture the interaction between cross-media premises and hypotheses and distinguish their entailment relationships. Experimental results on widely used Stanford natural language inference (SNLI) dataset with image premises from Flickr30K dataset verify the effectiveness of HIL and the intrinsic inter-media complementarity in reasoning.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3009710195",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on Multimodal Machine Learning for Human Behavior Analysis",
    "doi": "https://doi.org/10.1145/3381917",
    "publication_date": "2020-01-31",
    "publication_year": 2020,
    "authors": "Shengping Zhang; Huiyu Zhou; Dong Xu; M. Emre Celebi; Thierry Bouwmans",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3012361713",
    "type": "article"
  },
  {
    "title": "Multi Agent Based Intrusion Detection System using Artificial Immune System for Distributed Network",
    "doi": "https://doi.org/10.1145/3378544",
    "publication_date": "2020-05-07",
    "publication_year": 2020,
    "authors": "Aumreesh Kumar Saxena; Sitesh Kumar Sinha; Piyush Kumar Shukla; Prashant Kumar Shukla; Manish Maheshwari; Ratnesh Kumar Dubey; K. Sadhna",
    "corresponding_authors": "",
    "abstract": "To enhance the ability of intrusion detection system (IDS) with detection accuracy and low false positive rate, artificial immune system (AIS) based multi agent IDS is proposed that is inspired by human immune system (HIS). Presented IDS is designed four new algorithms these are mobile agent cloning algorithm, attribute selection algorithm based on danger theory which is filtering unnecessary attributes and giving most appropriate attributes for intrusion detection, detection algorithm based on negative selection technique is the prime algorithm which is detecting intrusions and rule algorithm is the simple and initial level algorithm to find normality or abnormality in the captured packet. Proposed IDS is the combination of misuse and anomaly IDS where architecture is the client server cum distributed IDS architecture. Proposed IDS reducing work load of network administrator and overcome network latency through automation and dynamic adaptation of mobile agent which is implement clonal selection concept of AIS. Proposed IDS is increasing efficiency and detection accuracy by using negative selection concept of AIS to design rule agent and informative agent. Another concept of AIS is danger theory is used to design packet capture &amp; extract packet agent which is support to reduce execution time and improve efficiency. Database agent is used to maintain or update database (training, testing) dynamically during novel information identified. Finally alert agent responded to client host after intrusion detected for further action. These entire agents are the primary and most influential performer of the proposed IDS. Presented IDS identify and focused on most appropriate 13 attributes out of 41 of a packet to detect intrusion and proved that intrusion detection accuracy and intrusion detection rate increased with 0.2% and 0.1% respectively and reduce false positive rate with 0.5%.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3041092044",
    "type": "article"
  },
  {
    "title": "Modeling Long-Term Dependencies from Videos Using Deep Multiplicative Neural Networks",
    "doi": "https://doi.org/10.1145/3357797",
    "publication_date": "2020-04-30",
    "publication_year": 2020,
    "authors": "Wen Si; Cong Liu; Zhongqin Bi; Meijing Shan",
    "corresponding_authors": "",
    "abstract": "Understanding temporal dependencies of videos is fundamental for vision problems, but deep learning–based models are still insufficient in this field. In this article, we propose a novel deep multiplicative neural network (DMNN) for learning hierarchical long-term representations from video. The DMNN is built upon the multiplicative block that remembers the pairwise transformations between consecutive frames using multiplicative interactions rather than the regular weighted-sum ones. The block is slided over the timesteps to update the memory of the networks on the frame pairs. Deep architecture can be implemented by stacking multiple layers of the sliding blocks. The multiplicative interactions lead to exact, rather than approximate, modeling of temporal dependencies. The memory mechanism can remember the temporal dependencies for an arbitrary length of time. The multiple layers output multiple-level representations that reflect the multi-timescale structure of video. Moreover, to address the difficulty of training DMNNs, we derive a theoretically sound convergent method, which leads to a fast and stable convergence. We demonstrate a new state-of-the-art classification performance with proposed networks on the UCF101 dataset and the effectiveness of capturing complicate temporal dependencies on a variety of synthetic datasets.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3044780020",
    "type": "article"
  },
  {
    "title": "Evaluation of Information Comprehension in Concurrent Speech-based Designs",
    "doi": "https://doi.org/10.1145/3409463",
    "publication_date": "2020-11-30",
    "publication_year": 2020,
    "authors": "Muhammad Abu ul Fazal; Sam Ferguson; Andrew Johnston",
    "corresponding_authors": "",
    "abstract": "In human-computer interaction, particularly in multimedia delivery, information is communicated to users sequentially, whereas users are capable of receiving information from multiple sources concurrently. This mismatch indicates that a sequential mode of communication does not utilise human perception capabilities as efficiently as possible. This article reports an experiment that investigated various speech-based (audio) concurrent designs and evaluated the comprehension depth of information by comparing comprehension performance across several different formats of questions (main/detailed, implied/stated). The results showed that users, besides answering the main questions, were also successful in answering the implied questions, as well as the questions that required detailed information, and that the pattern of comprehension depth remained similar to that seen to a baseline condition, where only one speech source was presented. However, the participants answered more questions correctly that were drawn from the main information, and performance remained low where the questions were drawn from detailed information. The results are encouraging to explore the concurrent methods further for communicating multiple information streams efficiently in human-computer interaction, including multimedia.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3112195279",
    "type": "article"
  },
  {
    "title": "Matching Faces and Attributes Between the Artistic and the Real Domain: the PersonArt Approach",
    "doi": "https://doi.org/10.1145/3490033",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Marcella Cornia; Matteo Tomei; Lorenzo Baraldi; Rita Cucchiara",
    "corresponding_authors": "",
    "abstract": "In this article, we present an approach for retrieving similar faces between the artistic and the real domain. The application we refer to is an interactive exhibition inside a museum, in which a visitor can take a photo of himself and search for a lookalike in the collection of paintings. The task requires not only to identify faces but also to extract discriminative features from artistic and photo-realistic images, tackling a significant domain shift. Our method integrates feature extraction networks which account for the aesthetic similarity of two faces and their correspondences in terms of semantic attributes. Also, it addresses the domain shift between realistic images and paintings by translating photo-realistic images into the artistic domain. Noticeably, by exploiting the same technique, our model does not need to rely on annotated data in the artistic domain. Experimental results are conducted on different paired datasets to show the effectiveness of the proposed solution in terms of identity and attribute preservation. The approach is also evaluated on unpaired settings and in combination with an interactive relevance feedback strategy. Finally, we show how the proposed algorithm has been implemented in a real showcase at the Gallerie Estensi museum in Italy, with the participation of more than 1,100 visitors in just three days.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3201982149",
    "type": "article"
  },
  {
    "title": "Learning Hierarchical Video Graph Networks for One-Stop Video Delivery",
    "doi": "https://doi.org/10.1145/3466886",
    "publication_date": "2022-01-27",
    "publication_year": 2022,
    "authors": "Yaguang Song; Junyu Gao; Xiaoshan Yang; Changsheng Xu",
    "corresponding_authors": "",
    "abstract": "The explosive growth of video data has brought great challenges to video retrieval, which aims to find out related videos from a video collection. Most users are usually not interested in all the content of retrieved videos but have a more fine-grained need. In the meantime, most existing methods can only return a ranked list of retrieved videos lacking a proper way to present the video content. In this paper, we introduce a distinctively new task, namely One-Stop Video Delivery (OSVD) aiming to realize a comprehensive retrieval system with the following merits: it not only retrieves the relevant videos but also filters out irrelevant information and presents compact video content to users, given a natural language query and video collection. To solve this task, we propose an end-to-end Hierarchical Video Graph Reasoning framework (HVGR) , which considers relations of different video levels and jointly accomplishes the one-stop delivery task. Specifically, we decompose the video into three levels, namely the video-level, moment-level, and the clip-level in a coarse-to-fine manner, and apply Graph Neural Networks (GNNs) on the hierarchical graph to model the relations. Furthermore, a pairwise ranking loss named Progressively Refined Loss is proposed based on prior knowledge that there is a relative order of the similarity of query-video, query-moment, and query-clip due to the different granularity of matched information. Extensive experimental results on benchmark datasets demonstrate that the proposed method achieves superior performance compared with baseline methods.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4210416956",
    "type": "article"
  },
  {
    "title": "Animating Still Natural Images Using Warping",
    "doi": "https://doi.org/10.1145/3511894",
    "publication_date": "2022-02-18",
    "publication_year": 2022,
    "authors": "Thi-Ngoc-Hanh Le; Chih-Kuo Yeh; Ying-Chi Lin; Tong‐Yee Lee",
    "corresponding_authors": "",
    "abstract": "From a single still image, a looping video could be generated by imparting subtle motion to objects in the image. The results are a hybrid of photography and video. They contain gentle motion in some objects, while the rest of the image remains still. Existing techniques are successful in animating such images. However, there are still some drawbacks that need to be investigated, such as too-large computation time necessary to retrieve the matched videos or the challenges of controlling the desired motion not only in terms of a single region but also in terms of consistency in regions. In this work, we address these issues by proposing an interactive system with a novel warping method. The key idea of our approach is to utilize user’s annotations to impart motion to certain objects. With two proposed phases in terms of preserve-curve-warping and cycle warping, a looping video is generated. We demonstrate the effectiveness of our method via various experimental challenging results and evaluations. We show that with a simple and lightweight method, our system is able to deal with animating a still image’s problems and results in realistic motion and appealing videos. In addition, using our proposed system, it is easy to create plausible animation using simple user annotations without referencing the video database or machine learning models and allows ordinary users with minimal expertise to produce compelling results.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4213028419",
    "type": "article"
  },
  {
    "title": "Toward a Holistic Approach to the Socio-historical Analysis of Vernacular Photos",
    "doi": "https://doi.org/10.1145/3507918",
    "publication_date": "2022-02-18",
    "publication_year": 2022,
    "authors": "Lorenzo Stacchio; Alessia Angeli; Giuseppe Lisanti; Daniela Calanca; Gustavo Marfia",
    "corresponding_authors": "",
    "abstract": "Although one of the most popular practices in photography since the end of the 19th century, an increase in scholarly interest in family photo albums dates back to the early 1980s. Such collections of photos may reveal sociological and historical insights regarding specific cultures and times. They are, however, in most cases scattered among private homes and only available on paper or photographic film, thus making their analysis by academics such as historians, social-cultural anthropologists and cultural theorists very cumbersome. In this paper, we analyze the IMAGO dataset including photos belonging to family albums assembled at the University of Bologna's Rimini campus since 2004. Following a deep learning-based approach, the IMAGO dataset has offered the opportunity of experimenting with photos taken between year 1845 and year 2009, with the goals of assessing the dates and the socio-historical contexts of the images, without use of any other sources of information. Exceeding our initial expectations, such analysis has revealed its merit not only in terms of the performance of the approach adopted in this work, but also in terms of the foreseeable implications and use for the benefit of socio-historical research. To the best of our knowledge, this is the first work that moves along this path in literature.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4213132893",
    "type": "article"
  },
  {
    "title": "GraSP: Local Grassmannian Spatio-Temporal Patterns for Unsupervised Pose Sequence Recognition",
    "doi": "https://doi.org/10.1145/3491227",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Himanshu Buckchash; Balasubramanian Raman",
    "corresponding_authors": "",
    "abstract": "Many applications of action recognition, especially broad domains like surveillance or anomaly-detection, favor unsupervised methods considering that exhaustive labeling of actions is not possible. However, very limited work has happened in this domain. Moreover, the existing self-supervised approaches suffer from their dependency upon labeled data for finetuning. To this end, this paper puts forward a manifold based unsupervised pose-sequence recognition approach that leverages only the natural biases present in the data. It works by clustering the projections of temporal derivatives of the fragmented data on the Grassmann manifold. Temporal derivatives are formed by the inter-frame gradients with local and global metrics. To commensurate with this, a dynamic view-invariant pose representation is proposed. Additionally, a variable aggregation step is introduced for better feature vector quantization. Extensive empirical evaluation and ablations on several challenging datasets under three categories confirm the superiority of the proposed approach in contrast to current methods.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4214836257",
    "type": "article"
  },
  {
    "title": "Recognizing Gaits Across Walking and Running Speeds",
    "doi": "https://doi.org/10.1145/3488715",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Lingxiang Yao; Worapan Kusakunniran; Qiang Wu; Jingsong Xu; Jian Zhang",
    "corresponding_authors": "",
    "abstract": "For decades, very few methods were proposed for cross-mode (i.e., walking vs. running) gait recognition. Thus, it remains largely unexplored regarding how to recognize persons by the way they walk and run. Existing cross-mode methods handle the walking-versus-running problem in two ways, either by exploring the generic mapping relation between walking and running modes or by extracting gait features which are non-/less vulnerable to the changes across these two modes. However, for the first approach, a mapping relation fit for one person may not be applicable to another person. There is no generic mapping relation given that walking and running are two highly self-related motions. The second approach does not give more attention to the disparity between walking and running modes, since mode labels are not involved in their feature learning processes. Distinct from these existing cross-mode methods, in our method, mode labels are used in the feature learning process, and a mode-invariant gait descriptor is hybridized for cross-mode gait recognition to handle this walking-versus-running problem. Further research is organized in this article to investigate the disparity between walking and running. Running is different from walking not only in the speed variances but also, more significantly, in prominent gesture/motion changes. According to these rationales, in our proposed method, we give more attention to the differences between walking and running modes, and a robust gait descriptor is developed to hybridize the mode-invariant spatial and temporal features. Two multi-task learning-based networks are proposed in this method to explore these mode-invariant features. Spatial features describe the body parts non-/less affected by mode changes, and temporal features depict the instinct motion relation of each person. Mode labels are also adopted in the training phase to guide the network to give more attention to the disparity across walking and running modes. In addition, relevant experiments on OU-ISIR Treadmill Dataset A have affirmed the effectiveness and feasibility of the proposed method. A state-of-the-art result can be achieved by our proposed method on this dataset.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4214850011",
    "type": "article"
  },
  {
    "title": "CRAR: Accelerating Stereo Matching with Cascaded Residual Regression and Adaptive Refinement",
    "doi": "https://doi.org/10.1145/3488719",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Linghua Zeng; Xinmei Tian",
    "corresponding_authors": "",
    "abstract": "Dense stereo matching estimates the depth for each pixel of the referenced images. Recently, deep learning algorithms have dramatically promoted the development of stereo matching. The state-of-the-art result is achieved by models adopting deep convolutional neural networks. However, a considerable computational burden is also introduced, which slows the inference. To solve this problem, previous works down-sampled the input images to decrease the spatial size. However, down-sampling increases the error rate and its lower bound. In this article, we accelerate stereo matching algorithms through the improvement of network structure. Inspired by network compression, we conduct decomposition and sparsification to squeeze the computationally expensive cost optimization network. It is sparsified and then decomposed into smaller networks, which are designed and trained in a cascaded manner to reach the nearest possible performance of the larger network. Previous methods have utilized numerous refinement methods to adjust the coarse disparity. We integrate refinement methods to create an unified algorithm to utilize parallelism for running devices to further accelerate the inference. The extensive experiments on Kitti2015, Kitti2012, and Middlebury datasets demonstrate the efficiency of our method.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4214922651",
    "type": "article"
  },
  {
    "title": "An Efficient and Accurate GPU-based Deep Learning Model for Multimedia Recommendation",
    "doi": "https://doi.org/10.1145/3524022",
    "publication_date": "2022-03-12",
    "publication_year": 2022,
    "authors": "Youcef Djenouri; Asma Belhadi; Gautam Srivastava; Jerry Chun‐Wei Lin",
    "corresponding_authors": "",
    "abstract": "This article proposes the use of deep learning in human-computer interaction and presents a new explainable hybrid framework for recommending relevant hashtags on a set of orpheline tweets, which are tweets with hashtags. The approach starts by determining the set of batches used in the convolution neural network based on frequent pattern mining solutions. The convolutional neural network is then applied to the set of batches of tweets to learn the hashtags of the tweets. An optimization strategy has been proposed to accurately perform the learning process by reducing the number of frequent patterns. Moreover, eXplainable AI is introduced for hashtag recommendations by analyzing the user preferences and understanding the different weights of the deep learning model used in the learning process. This is performed by learning the hyper-parameters of the deep architecture using the genetic algorithm. GPU computing is also investigated to achieve high speed and enable the execution of the overall framework in real time. Extensive experimental analysis has been performed to show that our methodology is useful on different collections of tweets. The experimental results clearly show the efficiency of our proposed approach compared to baseline approaches in terms of both runtime and accuracy. Thus, the proposed solution achieves an accuracy of 90% when analyzing complex Wikipedia data while the other algorithms did not achieve 85% when processing the same amount of data.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4220870126",
    "type": "article"
  },
  {
    "title": "Mask-Guided Deformation Adaptive Network for Human Parsing",
    "doi": "https://doi.org/10.1145/3467889",
    "publication_date": "2022-01-31",
    "publication_year": 2022,
    "authors": "Aihua Mao; Yuan Liang; Jianbo Jiao; Yongtuo Liu; Shengfeng He",
    "corresponding_authors": "",
    "abstract": "Due to the challenges of densely compacted body parts, nonrigid clothing items, and severe overlap in crowd scenes, human parsing needs to focus more on multilevel feature representations compared to general scene parsing tasks. Based on this observation, we propose to introduce the auxiliary task of human mask and edge detection to facilitate human parsing. Different from human parsing, which exploits the discriminative features of each category, human mask and edge detection emphasizes the boundaries of semantic parsing regions and the difference between foreground humans and background clutter, which benefits the parsing predictions of crowd scenes and small human parts. Specifically, we extract human mask and edge labels from the human parsing annotations and train a shared encoder with three independent decoders for the three mutually beneficial tasks. Furthermore, the decoder feature maps of the human mask prediction branch are further exploited as attention maps, indicating human regions to facilitate the decoding process of human parsing and human edge detection. In addition to these auxiliary tasks, we further alleviate the problem of deformed clothing items under various human poses by tracking the deformation patterns with the deformable convolution. Extensive experiments show that the proposed method can achieve superior performance against state-of-the-art methods on both single and multiple human parsing datasets. Codes and trained models are available https://github.com/ViktorLiang/MGDAN .",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4221037265",
    "type": "article"
  },
  {
    "title": "Table of Contents",
    "doi": "https://doi.org/10.1145/3311969",
    "publication_date": "2019-02-25",
    "publication_year": 2019,
    "authors": "Wei Zhan",
    "corresponding_authors": "Wei Zhan",
    "abstract": "No abstract available.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4248581394",
    "type": "article"
  },
  {
    "title": "LANBIQUE: LANguage-based Blind Image QUality Evaluation",
    "doi": "https://doi.org/10.1145/3538649",
    "publication_date": "2022-05-20",
    "publication_year": 2022,
    "authors": "Leonardo Galteri; Lorenzo Seidenari; Pietro Bongini; Marco Bertini; Alberto Del Bimbo",
    "corresponding_authors": "",
    "abstract": "Image quality assessment is often performed with deep networks that are fine-tuned to regress a human provided quality score of a given image. Usually, this approach may lack generalization capabilities and, while being highly precise on similar image distribution, it may yield lower correlation on unseen distortions. In particular, they show poor performances, whereas images corrupted by noise, blur, or compression have been restored by generative models. As a matter of fact, evaluation of these generative models is often performed providing anecdotal results to the reader. In the case of image enhancement and restoration, reference images are usually available. Nevertheless, using signal based metrics often leads to counterintuitive results: Highly natural crisp images may obtain worse scores than blurry ones. However, blind reference image assessment may rank images reconstructed with GANs higher than the original undistorted images. To avoid time-consuming human-based image assessment, semantic computer vision tasks may be exploited instead. In this article, we advocate the use of language generation tasks to evaluate the quality of restored images. We refer to our assessment approach as LANguage-based Blind Image QUality Evaluation (LANBIQUE). We show experimentally that image captioning, used as a downstream task, may serve as a method to score image quality, independently of the distortion process that affects the data. Captioning scores are better aligned with human rankings with respect to classic signal based or No-reference image quality metrics. We show insights on how the corruption, by artefacts, of local image structure may steer image captions in the wrong direction.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4281297448",
    "type": "article"
  },
  {
    "title": "A Normalized Slicing-assigned Virtualization Method for 6G-based Wireless Communication Systems",
    "doi": "https://doi.org/10.1145/3546077",
    "publication_date": "2022-07-18",
    "publication_year": 2022,
    "authors": "Abdullah Alharbi; Mohammed Aljebreen; Amr Tolba; Konstantinos A. Lizos; Saied M. Abd El‐atty; Farid Shawki",
    "corresponding_authors": "",
    "abstract": "The next generation of wireless communication systems will rely on advantageous sixth-generation wireless network (6G) features and sophisticated edge Internet-of-Things technology to provide continuous service delegation and resource allocation. Network slicing and virtualization are common in these scenarios to meet user demands and application services. This article introduces a Normalized Slicing-assigned Virtualization Method for satisfying the 6G features in future generation systems. The proposed method relies on available resource roots and time intervals for replications. Based on the availability and Accessibility, the resource virtualization and network slicing processes are forwarded. The proposed method exploits federated learning for determining availability and accessibility models in detecting slicing, virtualization, or both the requirements throughout the resource sharing process. This improves the resource sharing rate, with less latency and high processing despite the user and application demands. The learning models are trained to balance replication and network slicing for confining complexity across different resources. The proposed method's performance is validated using the above metrics for varying users and intervals.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4285727772",
    "type": "article"
  },
  {
    "title": "Boosting Relationship Detection in Images with Multi-Granular Self-Supervised Learning",
    "doi": "https://doi.org/10.1145/3556978",
    "publication_date": "2022-08-18",
    "publication_year": 2022,
    "authors": "Xuewei Ding; Yingwei Pan; Yehao Li; Ting Yao; Dan Zeng; Tao Mei",
    "corresponding_authors": "",
    "abstract": "Visual and spatial relationship detection in images has been a fast-developing research topic in the multimedia field, which learns to recognize the semantic/spatial interactions between objects in an image, aiming to compose a structured semantic understanding of the scene. Most of the existing techniques directly encapsulate the holistic image feature plus the semantic and spatial features of the given two objects for predicting the relationship, but leave the inherent supervision derived from such structured and thorough image understanding under-exploited. Specifically, the inherent supervision among objects or relations within an image can span different granularities in this hierarchy including, from simple to comprehensive, (1) the object-based supervision that captures the interaction between the semantic and spatial features of each individual object, (2) the inter-object supervision that characterizes the dependency within the relationship triplet ( &lt;subject-predicate-object&gt; ), and (3) the inter-relation supervision that exploits contextual information among all relationship triplets in an image. These inherent multi-granular supervisions offer a fertile ground for building self-supervised proxy tasks. In this article, we compose a trilogy of exploring the multi-granular supervision in the sequence from object-based, inter-object, and inter-relation perspectives. We integrate the standard relationship detection objective with a series of proposed self-supervised proxy tasks, which is named as Multi-Granular Self-Supervised learning (MGS). Our MGS is appealing in view that it is pluggable to any neural relationship detection models by simply including the proxy tasks during training, without increasing the computational cost at inference. Through extensive experiments conducted on the SpatialSense and VRD datasets, we demonstrate the superiority of MGS for both spatial and visual relationship detection tasks.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4293067332",
    "type": "article"
  },
  {
    "title": "CL <sup>2</sup> R: Compatible Lifelong Learning Representations",
    "doi": "https://doi.org/10.1145/3564786",
    "publication_date": "2022-06-30",
    "publication_year": 2022,
    "authors": "Niccolò Biondi; Federico Pernici; Matteo Bruni; Daniele Mugnai; Alberto Del Bimbo",
    "corresponding_authors": "",
    "abstract": "In this article, we propose a method to partially mimic natural intelligence for the problem of lifelong learning representations that are compatible. We take the perspective of a learning agent that is interested in recognizing object instances in an open dynamic universe in a way in which any update to its internal feature representation does not render the features in the gallery unusable for visual search. We refer to this learning problem as Compatible Lifelong Learning Representations (CL 2 R), as it considers compatible representation learning within the lifelong learning paradigm. We identify stationarity as the property that the feature representation is required to hold to achieve compatibility and propose a novel training procedure that encourages local and global stationarity on the learned representation. Due to stationarity, the statistical properties of the learned features do not change over time, making them interoperable with previously learned features. Extensive experiments on standard benchmark datasets show that our CL 2 R training procedure outperforms alternative baselines and state-of-the-art methods. We also provide novel metrics to specifically evaluate compatible representation learning under catastrophic forgetting in various sequential learning tasks. Code is available at https://github.com/NiccoBiondi/CompatibleLifelongRepresentation .",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4297999826",
    "type": "article"
  },
  {
    "title": "Fake and Dishonest Participant Immune Secret Image Sharing",
    "doi": "https://doi.org/10.1145/3572842",
    "publication_date": "2022-11-24",
    "publication_year": 2022,
    "authors": "Xuehu Yan; Longlong Li; Lei Sun; Jia Chen; Shudong Wang",
    "corresponding_authors": "",
    "abstract": "Secret image sharing (SIS) has received increased attention from the research community because of its usefulness in multiparty secure computing, access control, blockchain distributive storage and other security-oriented applications. Prevention of fake and dishonest participants is a key issue that has spurred interest in practical applications of SIS. Unfortunately, most previous SIS schemes failed to detect and locate fake or dishonest participants. In this article, an SIS for a ( k,n )-threshold without pixel expansion is presented, which can detect and locate both fake and dishonest participants. Using a screening operation, the proposed approach fuses the benefits of polynomial-based SIS, visual cryptographic scheme (VCS) , and hash functions to authenticate separate participants both with and without a dealer. In addition, the proposed approach achieves lossless rebuilding of the secret image. Analyses and experiments are conducted in this study to establish the effectiveness of the presented approach.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4309857258",
    "type": "article"
  },
  {
    "title": "Improving Face Anti-spoofing via Advanced Multi-perspective Feature Learning",
    "doi": "https://doi.org/10.1145/3575660",
    "publication_date": "2022-12-08",
    "publication_year": 2022,
    "authors": "Zhuming Wang; Yaowen Xu; Lifang Wu; Hu Han; Yukun Ma; Zun Li",
    "corresponding_authors": "",
    "abstract": "Face anti-spoofing (FAS) plays a vital role in securing face recognition systems. Previous approaches usually learn spoofing features from a single perspective, in which only universal cues shared by all attack types are explored. However, such single-perspective-based approaches ignore the differences among various attacks and commonness between certain attacks and bona fides, thus tending to neglect some non-universal cues that contain strong discernibility against certain types. As a result, when dealing with multiple types of attacks, the above approaches may suffer from the uncomprehensive representation of bona fides and spoof faces. In this work, we propose a novel Advanced Multi-Perspective Feature Learning network (AMPFL), in which multiple perspectives are adopted to learn discriminative features, to improve the performance of FAS. Specifically, the proposed network first learns universal cues and several perspective-specific cues from multiple perspectives, then aggregates the above features and further enhances them to perform face anti-spoofing. In this way, AMPFL obtains features that are difficult to be captured by single-perspective-based methods and provides more comprehensive information on bona fides and spoof faces, thus achieving better performance for FAS. Experimental results show that our AMPFL achieves promising results in public databases, and it effectively solves the issues of single-perspective-based approaches.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4311938558",
    "type": "article"
  },
  {
    "title": "Introduction to",
    "doi": "https://doi.org/10.1145/2820398",
    "publication_date": "2015-10-21",
    "publication_year": 2015,
    "authors": "James She; Alvin Chin; Feng Xia; Jon Crowcroft",
    "corresponding_authors": "",
    "abstract": "editorial Free Access Share on Introduction to: Special Issue on Smartphone-Based Interactive Technologies, Systems, and Applications Editors: James She Hong Kong University of Science & Technology, Hong Kong Hong Kong University of Science & Technology, Hong KongView Profile , Alvin Chin BMW Group, United States BMW Group, United StatesView Profile , Feng Xia Dalian University of Technology, China Dalian University of Technology, ChinaView Profile , Jon Crowcroft University of Cambridge, UK University of Cambridge, UKView Profile Authors Info & Claims ACM Transactions on Multimedia Computing, Communications, and ApplicationsVolume 12Issue 1sArticle No.: 11pp 1–4https://doi.org/10.1145/2820398Published:21 October 2015Publication History 4citation201DownloadsMetricsTotal Citations4Total Downloads201Last 12 Months18Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W1984562085",
    "type": "article"
  },
  {
    "title": "Introduction to",
    "doi": "https://doi.org/10.1145/2820400",
    "publication_date": "2015-10-21",
    "publication_year": 2015,
    "authors": "Hayley Hung; George Toderici",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2063741257",
    "type": "article"
  },
  {
    "title": "Octree-Based 3D Logic and Computation of Spatial Relationships in Live Video Query Processing",
    "doi": "https://doi.org/10.1145/2645864",
    "publication_date": "2015-01-07",
    "publication_year": 2015,
    "authors": "Jun Ye; Kien A. Hua",
    "corresponding_authors": "",
    "abstract": "Live video computing (LVC) on distributed smart cameras has many important applications; and a database approach based on a Live Video DataBase Management System (LVDBMS) has shown to be effective for general LVC application development. The performance of such a database system relies on accurate interpretation of spatial relationships among objects in the live video. With the popularity of affordable depth cameras, 3D spatial computation techniques have been applied. However, the 3D object models currently used are expensive to compute, and offer limited scalability. We address this drawback in this article by proposing an octree-based 3D spatial logic and presenting algorithms for computing 3D spatial relationships using depth cameras. To support continuous query processing on live video streams, we also develop a GPU-based implementation of the proposed technique to further enhance scalability for real-time applications. Extensive performance studies based on a public RGB-D dataset as well as the LVDBMS prototype demonstrates the correctness and efficiency of our techniques.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2096999828",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue Best Papers of ACM Multimedia 2013",
    "doi": "https://doi.org/10.1145/2661331",
    "publication_date": "2014-10-01",
    "publication_year": 2014,
    "authors": "Zheng-Jun Zha; Lei Zhang; Max Mühlhäuser; Alan F. Smeaton",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2118465554",
    "type": "article"
  },
  {
    "title": "Introduction to Special Issue on Multimedia Big Data",
    "doi": "https://doi.org/10.1145/2989214",
    "publication_date": "2016-09-21",
    "publication_year": 2016,
    "authors": "Mianxiong Dong; Vincenzo Piuri; S.-H. Gary Chan; Ramesh Jain",
    "corresponding_authors": "",
    "abstract": "editorial Free Access Share on Introduction to Special Issue on Multimedia Big Data: Networking Editors: Mianxiong Dong Muroran Institute of Technology, Japan Muroran Institute of Technology, JapanView Profile , Vincenzo Piuri Università degli Studi di Milano, Italy Università degli Studi di Milano, ItalyView Profile , Shueng-Han Gary Chan The Hong Kong University of Science and Technology, Hong Kong The Hong Kong University of Science and Technology, Hong KongView Profile , Ramesh Jain University of California, Irvine, CA University of California, Irvine, CAView Profile Authors Info & Claims ACM Transactions on Multimedia Computing, Communications, and ApplicationsVolume 12Issue 5sDecember 2016 Article No.: 70pp 1–3https://doi.org/10.1145/2989214Published:21 September 2016Publication History 1citation336DownloadsMetricsTotal Citations1Total Downloads336Last 12 Months17Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2522947614",
    "type": "article"
  },
  {
    "title": "Audiovisual Tool for Solfège Assessment",
    "doi": "https://doi.org/10.1145/3007194",
    "publication_date": "2016-12-16",
    "publication_year": 2016,
    "authors": "Rodrigo Schramm; Helena de Souza Nunes; Cláudio R. Jung",
    "corresponding_authors": "",
    "abstract": "Solfège is a general technique used in the music learning process that involves the vocal performance of melodies, regarding the time and duration of musical sounds as specified in the music score, properly associated with the meter-mimicking performed by hand movement. This article presents an audiovisual approach for automatic assessment of this relevant musical study practice. The proposed system combines the gesture of meter-mimicking (video information) with the melodic transcription (audio information), where hand movement works as a metronome, controlling the time flow (tempo) of the musical piece. Thus, meter-mimicking is used to align the music score (ground truth) with the sung melody, allowing assessment even in time-dynamic scenarios. Audio analysis is applied to achieve the melodic transcription of the sung notes and the solfège performances are evaluated by a set of Bayesian classifiers that were generated from real evaluations done by experts listeners.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2562850355",
    "type": "article"
  },
  {
    "title": "Table of Contents",
    "doi": "https://doi.org/10.1145/2602969",
    "publication_date": "2014-04-01",
    "publication_year": 2014,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "research-article Table of Contents: Online Supplement Volume 10, Number 1s Share on ACM Transactions on Multimedia Computing, Communications, and ApplicationsVolume 10Issue 3April 2014 pp 1https://doi.org/10.1145/2602969Online:17 April 2014Publication History 0citation93DownloadsMetricsTotal Citations0Total Downloads93Last 12 Months1Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4213274786",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2576908",
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We present the first open source cloud gaming system, called GamingAnywhere. In addition to its openness, we have designed, GamingAnywhere for high extensibility, portability, and reconfigurability. We implemented it on Windows, Linux, OS X, and ...",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4230937331",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2716635",
    "publication_date": "2015-01-07",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "In recent years, with the rapid development of camera technology and portable devices, we have witnessed a flourish of user generated videos, which are gradually reshaping the traditional professional video oriented media market. The volume of user ...",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4234876595",
    "type": "paratext"
  },
  {
    "title": "Self-contained Entity Discovery from Captioned Videos",
    "doi": "https://doi.org/10.1145/3583138",
    "publication_date": "2023-02-07",
    "publication_year": 2023,
    "authors": "Melika Ayoughi; Pascal Mettes; Paul Groth",
    "corresponding_authors": "",
    "abstract": "This article introduces the task of visual named entity discovery in videos without the need for task-specific supervision or task-specific external knowledge sources. Assigning specific names to entities (e.g., faces, scenes, or objects) in video frames is a long-standing challenge. Commonly, this problem is addressed as a supervised learning objective by manually annotating entities with labels. To bypass the annotation burden of this setup, several works have investigated the problem by utilizing external knowledge sources such as movie databases. While effective, such approaches do not work when task-specific knowledge sources are not provided and can only be applied to movies and TV series. In this work, we take the problem a step further and propose to discover entities in videos from videos and corresponding captions or subtitles. We introduce a three-stage method where we (i) create bipartite entity-name graphs from frame–caption pairs, (ii) find visual entity agreements, and (iii) refine the entity assignment through entity-level prototype construction. To tackle this new problem, we outline two new benchmarks, SC-Friends and SC-BBT , based on the Friends and Big Bang Theory TV series. Experiments on the benchmarks demonstrate the ability of our approach to discover which named entity belongs to which face or scene, with an accuracy close to a supervised oracle, just from the multimodal information present in videos. Additionally, our qualitative examples show the potential challenges of self-contained discovery of any visual entity for future work. The code and the data are available on GitHub. 1",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4319348862",
    "type": "article"
  },
  {
    "title": "Tensor-Empowered LSTM for Communication-Efficient and Privacy-Enhanced Cognitive Federated Learning in Intelligent Transportation Systems",
    "doi": "https://doi.org/10.1145/3575661",
    "publication_date": "2023-02-10",
    "publication_year": 2023,
    "authors": "Ruonan Zhao; Laurence T. Yang; Debin Liu; Wanli Lu; Chenlu Zhu; Yiheng Ruan",
    "corresponding_authors": "",
    "abstract": "Multimedia cognitive computing as a revolutionary emerging concept of artificial intelligence emulating the reasoning process like human brains can facilitate the evolution of intelligent transportation systems (ITS) to be smarter, safer, and more efficient. Massive multimedia traffic big data is an important prerequisite for the success of cognitive computing in ITS. However, traditional data-centralized artificial intelligence approaches often face the problems of data islands and data famine due to concerns about data privacy and security. To this end, we propose the concept of cognitive federated learning leveraging federated learning as the learning paradigm for cognitive computing, which solves the preceding concerns by sharing updated models rather than raw data. Nevertheless, the exchange of numerous model parameters not only generates significant communication overhead but also suffers from the risk of privacy leakage due to inference attacks. This article aims to design a novel lightweight and privacy-enhanced cognitive federated learning architecture to facilitate the development of ITS. First, a privacy-enhanced model protection scheme with homomorphic encryption as the underlying technology is proposed to simultaneously defend against the inference attacks launched by external malicious attackers, honest-but-curious cognitive platforms, and internal participants. Furthermore, a novel tensor ring-block decomposition and its corresponding deep computation model converting the weight tensor into a set of matrices and third-order core tensors are proposed, which could reduce the communication overhead and storage requirements without compromising model performance. Experimental results on real-world datasets show that the proposed approach performs well.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4319966069",
    "type": "article"
  },
  {
    "title": "A Geometrical Approach to Evaluate the Adversarial Robustness of Deep Neural Networks",
    "doi": "https://doi.org/10.1145/3587936",
    "publication_date": "2023-03-15",
    "publication_year": 2023,
    "authors": "Yang Wang; Bo Dong; Ke Xu; Haiyin Piao; Yufei Ding; Baocai Yin; Xin Yang",
    "corresponding_authors": "",
    "abstract": "Deep Neural Networks (DNNs) are widely used for computer vision tasks. However, it has been shown that deep models are vulnerable to adversarial attacks, i.e., their performances drop when imperceptible perturbations are made to the original inputs, which may further degrade the following visual tasks or introduce new problems such as data and privacy security. Hence, metrics for evaluating the robustness of deep models against adversarial attacks are desired. However, previous metrics are mainly proposed for evaluating the adversarial robustness of shallow networks on the small-scale datasets. Although the Cross Lipschitz Extreme Value for nEtwork Robustness (CLEVER) metric has been proposed for large-scale datasets (e.g., the ImageNet dataset), it is computationally expensive and its performance relies on a tractable number of samples. In this paper, we propose the Adversarial Converging Time Score (ACTS), an attack-dependent metric that quantifies the adversarial robustness of a DNN on a specific input. Our key observation is that local neighborhoods on a DNN's output surface would have different shapes given different inputs. Hence, given different inputs, it requires different time for converging to an adversarial sample. Based on this geometry meaning, ACTS measures the converging time as an adversarial robustness metric. We validate the effectiveness and generalization of the proposed ACTS metric against different adversarial attacks on the large-scale ImageNet dataset using state-of-the-art deep networks. Extensive experiments show that our ACTS metric is an efficient and effective adversarial metric over the previous CLEVER metric.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4324359915",
    "type": "article"
  },
  {
    "title": "Unsupervised Discovery and Manipulation of Continuous Disentangled Factors of Variation",
    "doi": "https://doi.org/10.1145/3591358",
    "publication_date": "2023-04-06",
    "publication_year": 2023,
    "authors": "Tomaso Fontanini; Luca Lupària Donati; Massimo Bertozzi; Andrea Prati",
    "corresponding_authors": "",
    "abstract": "Learning a disentangled representation of a distribution in a completely unsupervised way is a challenging task that has drawn attention recently. In particular, much focus has been put in separating factors of variation (i.e., attributes) within the latent code of a Generative Adversarial Network (GAN). Achieving that permits control of the presence or absence of those factors in the generated samples by simply editing a small portion of the latent code. Nevertheless, existing methods that perform very well in a noise-to-image setting often fail when dealing with a real data distribution, i.e., when the discovered attributes need to be applied to real images. However, some methods are able to extract and apply a style to a sample but struggle to maintain its content and identity, while others are not able to locally apply attributes and end up achieving only a global manipulation of the original image. In this article, we propose a completely (i.e., truly ) unsupervised method that is able to extract a disentangled set of attributes from a data distribution and apply them to new samples from the same distribution by preserving their content. This is achieved by using an image-to-image GAN that maps an image and a random set of continuous attributes to a new image that includes those attributes. Indeed, these attributes are initially unknown and they are discovered during training by maximizing the mutual information between the generated samples and the attributes’ vector. Finally, the obtained disentangled set of continuous attributes can be used to freely manipulate the input samples. We prove the effectiveness of our method over a series of datasets and show its application on various tasks, such as attribute editing, data augmentation, and style transfer.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4362666964",
    "type": "article"
  },
  {
    "title": "Instance-Based Continual Learning: A Real-World Dataset and Baseline for Fresh Recognition",
    "doi": "https://doi.org/10.1145/3591209",
    "publication_date": "2023-04-25",
    "publication_year": 2023,
    "authors": "Zhenbo Xu; Hai‐Miao Hu; Liu Liu; Dongping Zhang; Shifeng Zhang; Wenming Tan",
    "corresponding_authors": "",
    "abstract": "Real-time learning on real-world data streams with temporal relations is essential for intelligent agents. However, current online Continual Learning (CL) benchmarks adopt the mini-batch setting and are composed of temporally unrelated and disjoint tasks as well as pre-set class boundaries. In this paper, we delve into a real-world CL scenario for fresh recognition where algorithms are required to recognize a huge variety of products to facilitate the checkout speed. Products mainly consists of packaged cereals, seasonal fruits, and vegetables from local farms or shipped from overseas. Since algorithms process instance streams consisting of sequential images, we name this real-world CL problem as Instance-Based Continual Learning (IBCL) . Different from the current online CL setting, algorithms are required to perform instant testing and learning upon each incoming instance. Moreover, IBCL has no task boundaries or class boundaries and allows the evolution and the forgetting of old samples within each class. To promote the researches on real CL challenges, we propose the first real-world CL dataset coined the Continual Fresh Recognition (CFR) dataset, which consists of fresh recognition data streams (766 K labelled images in total) collected from 30 supermarkets. Based on the CFR dataset, we extensively evaluate the performance of current online CL methods under various settings and find that current prominent online CL methods operate at high latency and demand significant memory consumption to cache old samples for replaying. Therefore, we make the first attempt to design an efficient and effective Instant Training-Free Learning (ITFL) framework for IBCL. ITFL consists of feature extractors trained in the metric learning manner and reformulates CL as a temporal classification problem among several most similar classes. Unlike current online CL methods that cache image samples (150 KB per image) and rely on training to learn new knowledge, our framework only caches features (2 KB per image) and is free of training in deployment. Extensive evaluations across three datasets demonstrate that our method achieves comparable recognition accuracy to current methods with lower latency and less resource consumption. Our codes and datasets will be publicly available at https://github.com/detectRecog/IBCL .",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4366985339",
    "type": "article"
  },
  {
    "title": "Enhancing Adversarial Embedding based Image Steganography via Clustering Modification Directions",
    "doi": "https://doi.org/10.1145/3603377",
    "publication_date": "2023-06-03",
    "publication_year": 2023,
    "authors": "Dewang Wang; Gaobo Yang; Zhiqing Guo; Jiyou Chen",
    "corresponding_authors": "",
    "abstract": "Image steganography is a technique used to conceal secret information within cover images without being detected. However, the advent of convolutional neural networks (CNNs) has threatened the security of image steganography. Due to the inherent properties of adversarial examples, adding perturbations to stego images can mislead the CNN-based image steganalysis, but it also easily leads to some errors when extracting secret information. Recently, some adversarial embedding methods have been proposed for improving image steganography security. In this work, we aim at furthering enhance the security of adversarial embedding-based image steganography by exploiting the strong correlation between adjacent pixels. Specifically, we divide the cover image into four non-overlapping parts for four-stage information embedding. During the adversarial embedding process, we cluster the modification directions of adjacent pixels and select only those with relatively larger amplitudes of gradients and smaller embedding costs to update their original embedding costs. Experimental results demonstrate that our proposed method can effectively fool targeted steganalyzers and outperform state-of-the-art techniques under different scenarios.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4379209355",
    "type": "article"
  },
  {
    "title": "A Visual Sensitivity Aware ABR Algorithm for DASH via Deep Reinforcement Learning",
    "doi": "https://doi.org/10.1145/3591108",
    "publication_date": "2023-06-29",
    "publication_year": 2023,
    "authors": "Ye Jin; Dan Meng; Wenchao Jiang",
    "corresponding_authors": "",
    "abstract": "In order to cope with the fluctuation of network bandwidth and provide smooth video services, adaptive video streaming technology is proposed. In particular, the adaptive bitrate (ABR) algorithm is widely used in dynamic adaptive streaming over HTTP (DASH) to improve quality of experience (QoE). However, existing ABR algorithms still ignore the inherent visual sensitivity of human visual system (HVS). As the final receiver of video, HVS has different sensitivity to the quality distortion of different video content, and video content with high visual sensitivity needs to allocate more bitrate resources. Therefore, existing ABR algorithms still have limitations in reasonably allocating bitrate and maximizing QoE. To solve this problem, this paper designs an adaptive bitrate strategy from the perspective of user vision, studies the modeling of visual sensitivity, and proposes a visual sensitivity aware ABR algorithm. We extract a set of content features and attribute features from the video, and consider the simulation of HVS to establish a total masking effect model that reflects the visual sensitivity more accurately. Further, the network status, buffer occupancy, and visual sensitivity are comprehensively considered under a deep reinforcement learning framework to select the appropriate bitrate for maximizing QoE. We implement the proposed algorithm over a realistic trace-driven evaluation and compare its performance with several latest algorithms. Experimental results show that our algorithm can align ABR strategy with visual sensitivity to achieve better QoE in high visual sensitivity content, and improves the average perceptual video quality and overall user QoE by 18.3% and 22.8%, respectively. Additionally, we prove the feasibility of our algorithm through subjective evaluation in the real environment.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4382540848",
    "type": "article"
  },
  {
    "title": "An Iterative Semi-supervised Approach with Pixel-wise Contrastive Loss for Road Extraction in Aerial Images",
    "doi": "https://doi.org/10.1145/3606374",
    "publication_date": "2023-07-22",
    "publication_year": 2023,
    "authors": "Huijie Zhang; Pu Li; Xiaobai Liu; Xianfeng Yang; Li An",
    "corresponding_authors": "",
    "abstract": "Extracting roads in aerial images has numerous applications in artificial intelligence and multimedia computing, including traffic pattern analysis and parking space planning. Learning deep neural networks, though very successful, demand vast amounts of high-quality annotations, of which acquisition is time-consuming and expensive. In this work, we propose a semi-supervised approach for image-based road extraction in which only a small set of labeled images are available for training to address this challenge. We design a pixel-wise contrastive loss to self-supervise the network training to utilize the large corpus of unlabeled images. The key idea is to identify pairs of overlapping image regions (positive) or non-overlapping image regions (negative) and encourage the network to make similar outputs for positive pairs or dissimilar outputs for negative pairs. We also develop a negative sampling strategy to filter false-negative samples during the process. An iterative procedure is introduced to apply the network over raw images to generate pseudo-labels, filter and select high-quality labels with the proposed contrastive loss, and retrain the network with the enlarged training dataset. We repeat these iterative steps until convergence. We validate the effectiveness of the proposed methods by performing extensive experiments on the public SpaceNet3 and DeepGlobe Road datasets. Results show that our proposed method achieves state-of-the-art results on public image segmentation benchmarks and significantly outperforms other semi-supervised methods.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4385073983",
    "type": "article"
  },
  {
    "title": "Semi-supervised Video Object Segmentation Via an Edge Attention Gated Graph Convolutional Network",
    "doi": "https://doi.org/10.1145/3611389",
    "publication_date": "2023-08-01",
    "publication_year": 2023,
    "authors": "Yuqing Zhang; Yong Zhang; Shaofan Wang; Yun Liang; Baocai Yin",
    "corresponding_authors": "",
    "abstract": "Video object segmentation (VOS) exhibits heavy occlusions, large deformation, and severe motion blur. While many remarkable convolutional neural networks are devoted to the VOS task, they often mis-identify background noise as the target or output coarse object boundaries, due to the failure of mining detail information and high-order correlations of pixels within the whole video. In this work, we propose an edge attention gated graph convolutional network (GCN) for VOS. The seed point initialization and graph construction stages construct a spatio-temporal graph of the video by exploring the spatial intra-frame correlation and the temporal inter-frame correlation of superpixels. The node classification stage identifies foreground superpixels by using an edge attention gated GCN which mines higher-order correlations between superpixels and propagates features among different nodes. The segmentation optimization stage optimizes the classification of foreground superpixels and reduces segmentation errors by using a global appearance model which captures the long-term stable feature of objects. In summary, the key contribution of our framework is twofold: (a) the spatio-temporal graph representation can propagate the seed points of the first frame to subsequent frames and facilitate our framework for the semi-supervised VOS task; and (b) the edge attention gated GCN can learn the importance of each node with respect to both the neighboring nodes and the whole task with a small number of layers. Experiments on Davis 2016 and Davis 2017 datasets show that our framework achieves the excellent performance with only small training samples (45 video sequences).",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4385445430",
    "type": "article"
  },
  {
    "title": "Usefulness of QoS in Multidimensional QoE Prediction for Haptic–Audiovisual Communications",
    "doi": "https://doi.org/10.1145/3613246",
    "publication_date": "2023-08-04",
    "publication_year": 2023,
    "authors": "Shuji Tasaka",
    "corresponding_authors": "Shuji Tasaka",
    "abstract": "This article investigates prediction of Quality of Experience (QoE) by comparing borrowing-from-neighbor situations and isolated ones. We demonstrate that joint utilization of multiple QoE measures enhances the accuracy of QoE prediction compared to that by a collection of individual QoE measures each regressed on Quality of Service (QoS) parameters, while the accuracy improvement with additional usage of QoS information in the former is limited. As an example of system that needs multidimensional QoE representation, the article gives haptic audiovisual interactive communications. We employ QoE and QoS data taken previously in an experiment, where 13 QoE measures (a five-point score each) and 12 QoS parameters (nonnegative continuous values each) are available at three average rates of load traffic. We build two kinds of Bayesian models for QoE prediction; one is a logistic regression model of a single QoE measure as the response variable and QoS parameters as predictors, which is a typical traditional method of discrete QoE prediction and isolated in a sense. The other is a structural equation model (SEM) with latent constructs (i.e., factors) of audiovisual quality, haptic quality, and user experience quality; the original SEM, which contains only QoE indicators of three constructs (audiovisual quality (AVQ), haptic quality (HQ), and user experience quality (UXQ)), was proposed in one of the author’s previous studies. This article extends the SEM to accommodate QoS parameters. We develop two kinds of new SEMs with QoS parameters: One has three extended constructs referred to as eAVQ, eHQ, and eUXQ, each of which has both QoE and QoS indicators, and the other has separate constructs for QoE and QoS, which lead to totally six constructs (AVqoe, Hqoe, UXqoe, AVqos, Hqos, and UXqos). We performed Markov chain Monte Carlo simulation of the Bayesian models with the JAGS software in an R environment. For comparison of QoE prediction accuracy, we adopt the 10-fold cross validation method and in part widely applicable information criterion. We then found that the three-construct models outperform the logistic regression models with respect to all subjective QoE measures and that the two kinds of the models are comparable as for the objective measure. The six-construct model exhibits almost the same accuracy as that of the three-construct one unless the number of QoE measures ( n qoe ) in the model is small. When the number n qoe is small, single-construct models may be a better choice. We have thus learned that multiple QoE measures should be utilized jointly (i.e., borrowing from neighbor) in QoE prediction rather than resorting to QoS information only.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4385584189",
    "type": "article"
  },
  {
    "title": "Self-Adaptive Clothing Mapping Based Virtual Try-on",
    "doi": "https://doi.org/10.1145/3613453",
    "publication_date": "2023-08-08",
    "publication_year": 2023,
    "authors": "Chengji Shen; Zhenjiang Liu; Xin Gao; Zunlei Feng; Mingli Song",
    "corresponding_authors": "",
    "abstract": "VTON (Virtual Try-ON), as an innovative visual application in e-commerce scenarios with great commercial value, has been widely studied in recent years. Due to its better robustness and realistic effect, deformation-synthesize-based VTON has become the dominant approach in this field. Existing clothing deformation techniques optimize the mapping relations between the original clothing image and the ground truth (GT) image of the worn clothing. However, there are color differences between the original and GT clothing images caused by lighting, warping, and occlusion. The color differences may lead to misaligned clothing mapping by only minimizing the cost of pixel value difference. Another drawback is that taking the parsing prediction as GT will bring alignment remnant, rooting in the processing order of parsing and deformation. Aiming above two drawbacks, we put forward SAME-VTON (Self-Adaptive clothing Mapping basEd Virtual Try-ON) for achieving realistic virtual try-on results. The core of SAME-VTON is the self-adaptive clothing mapping technique, composed of two parts: a color-adaptive clothing mapping module and a parsing-adaptive prediction process. In the color-adaptive clothing mapping module, we map each pixel of the target clothing with a combination of multiple pixel values from the original clothing image, which considers both the position and color changes. Furthermore, different combination weights are learned to increase the diversity of color mapping. In the parsing-adaptive prediction process, the color-adaptive clothing mapping module is adopted to deform clothing first, then the human parsing result is predicted under the reference of the deformed clothing, which can avoid alignment remnant. Extensive experiments demonstrate that the proposed SAME-VTON with the self-adaptive clothing mapping technique can achieve optimal mapping in the case of large color differences and obtain superior results compared with existing deformation-synthesize-based VTON.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4385665219",
    "type": "article"
  },
  {
    "title": "Explaining Cross-domain Recognition with Interpretable Deep Classifier",
    "doi": "https://doi.org/10.1145/3623399",
    "publication_date": "2023-09-08",
    "publication_year": 2023,
    "authors": "Yiheng Zhang; Ting Yao; Zhaofan Qiu; Tao Mei",
    "corresponding_authors": "",
    "abstract": "The recent advances in deep learning predominantly construct models in their internal representations, and it is opaque to explain the rationale behind and decisions to human users. Such explainability is especially essential for domain adaptation, whose challenges require developing more adaptive models across different domains. In this article, we ask the question: How much does each sample in the source domain contribute to the network’s prediction on the samples from the target domain? To address this, we devise a novel Interpretable Deep Classifier (IDC) that learns the nearest source samples of a target sample as evidence upon which the classifier makes the decision. Technically, IDC maintains a differentiable memory bank for each category, and the memory slot derives a form of key–value pair. The key records the features of discriminative source samples, and the value stores the corresponding properties, e.g., representative scores of the features for describing the category. IDC computes the loss between the output of IDC and the labels of source samples to back-propagate to adjust the representative scores and update the memory banks. Extensive experiments on Office-Home and VisDA-2017 datasets demonstrate that our IDC leads to a more explainable model with almost no accuracy degradation and effectively calibrates classification for optimum reject options. More remarkably, when taking IDC as a prior interpreter, capitalizing on 0.1% source training data selected by IDC still yields superior results than that uses full training set on VisDA-2017 for unsupervised domain adaptation.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4386546977",
    "type": "article"
  },
  {
    "title": "VisActive: Visual-concept-based Active Learning for Image Classification under Class Imbalance",
    "doi": "https://doi.org/10.1145/3617999",
    "publication_date": "2023-09-15",
    "publication_year": 2023,
    "authors": "Mohammed Khaleel; Azeez Idris; Wallapak Tavanapong; Jacob Pratt; JungHwan Oh; Piet C. de Groen",
    "corresponding_authors": "",
    "abstract": "Active learning methods recommend the most informative images from a large unlabeled dataset for manual labeling. These methods improve the performance of an image classifier while minimizing manual labeling efforts. We propose VisActive, a visual-concept-based active learning method for image classification under class imbalance. VisActive learns a visual concept, a generalized representation that holds the most important image characteristics for class prediction, and then recommends for each class four sets of unlabeled images with different visual concepts to increase the diversity and enlarge the training dataset. Experimental results on four datasets show that VisActive outperforms the state-of-the-art deep active learning methods.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4386772932",
    "type": "article"
  },
  {
    "title": "Generating Robust Adversarial Examples against Online Social Networks (OSNs)",
    "doi": "https://doi.org/10.1145/3632528",
    "publication_date": "2023-11-11",
    "publication_year": 2023,
    "authors": "Jun Liu; Jiantao Zhou; Haiwei Wu; Weiwei Sun; Jinyu Tian",
    "corresponding_authors": "",
    "abstract": "Online Social Networks (OSNs) have blossomed into prevailing transmission channels for images in the modern era. Adversarial examples (AEs) deliberately designed to mislead deep neural networks (DNNs) are found to be fragile against the inevitable lossy operations conducted by OSNs. As a result, the AEs would lose their attack capabilities after being transmitted over OSNs. In this work, we aim to design a new framework for generating robust AEs that can survive the OSN transmission; namely, the AEs before and after the OSN transmission both possess strong attack capabilities. To this end, we first propose a differentiable network termed SImulated OSN (SIO) to simulate the various operations conducted by an OSN. Specifically, the SIO network consists of two modules: (1) a differentiable JPEG layer for approximating the ubiquitous JPEG compression and (2) an encoder-decoder subnetwork for mimicking the remaining operations. Based upon the SIO network, we then formulate an optimization framework to generate robust AEs by enforcing model outputs with and without passing through the SIO to be both misled. Extensive experiments conducted over Facebook, WeChat and QQ demonstrate that our attack methods produce more robust AEs than existing approaches, especially under small distortion constraints; the performance gain in terms of Attack Success Rate (ASR) could be more than 60%. Furthermore, we build a public dataset containing more than 10,000 pairs of AEs processed by Facebook, WeChat or QQ, facilitating future research in the robust AEs generation. The dataset and code are available at https://github.com/csjunjun/RobustOSNAttack.git .",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4388594563",
    "type": "article"
  },
  {
    "title": "Towards Egocentric Compositional Action Anticipation with Adaptive Semantic Debiasing",
    "doi": "https://doi.org/10.1145/3633333",
    "publication_date": "2023-12-04",
    "publication_year": 2023,
    "authors": "Tianyu Zhang; Weiqing Min; Tao Liu; Shuqiang Jiang; Yong Rui",
    "corresponding_authors": "",
    "abstract": "Predicting the unknown from the first-person perspective is expected as a necessary step towards machine intelligence, which is essential for practical applications including autonomous driving and robotics. As a human-level task, egocentric action anticipation aims at predicting an unknown action seconds before it is performed from the first-person viewpoint. While egocentric actions are usually provided as verb-noun pairs, predicting the unknown action may be trapped in insufficient training data for all possible combinations. Therefore, it is crucial for intelligent systems to use limited known verb-noun pairs to predict new combinations of actions that have never appeared, which is known as compositional generalization. In this paper, we are the first to explore the egocentric compositional action anticipation problem, which is more in line with real-world settings but neglected by existing studies. While prediction results are prone to suffer from semantic bias considering the distinct difference between training and test distributions, we further introduce a general and flexible adaptive semantic debiasing (ASD) framework that is compatible with different deep neural networks. To capture and mitigate semantic bias, we can imagine one counterfactual situation where no visual representations have been observed and only semantic patterns of observation are used to predict the next action. Instead of the traditional counterfactual analysis scheme that reduces semantic bias in a mindless way, we devise a novel counterfactual analysis scheme to adaptively amplify or penalize the effect of semantic experience by considering the discrepancy both among categories and among examples. We also demonstrate that the traditional counterfactual analysis scheme is a special case of the devised adaptive counterfactual analysis scheme. We conduct experiments on three large-scale egocentric video datasets. Experimental results verify the superiority and effectiveness of our proposed solution.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4389299511",
    "type": "article"
  },
  {
    "title": "Auxiliary Information Guided Self-Attention for Image Quality Assessment",
    "doi": "https://doi.org/10.1145/3635716",
    "publication_date": "2023-12-04",
    "publication_year": 2023,
    "authors": "Jifan Yang; Zhongyuan Wang; Guangcheng Wang; Baojin Huang; Yuhong Yang; Weiping Tu",
    "corresponding_authors": "",
    "abstract": "Image quality assessment (IQA) is an important problem in computer vision with many applications. We propose a transformer-based multi-task learning framework for the IQA task. Two subtasks: constructing an auxiliary information error map and completing image quality prediction, are jointly optimized using a shared feature extractor. We use visual transformers (ViT) as a feature extractor for feature extraction and guide ViT to focus on image quality-related features by building auxiliary information error map subtask. In particular, we propose a fusion network that includes a channel focus module. Unlike the fusion methods commonly used in previous IQA methods, we use the fusion network, including the channel attention module, to fuse the auxiliary information error map features with the image features, which facilitates the model to mine the image quality features for more accurate image quality assessment. And by jointly optimizing the two subtasks, ViT focuses more on extracting image quality features and building a more precise mapping from feature representation to quality score. With slight adjustments to the model, our approach can be used in both no-reference (NR) and full-reference (FR) IQA environments. We evaluate the proposed method in multiple IQA databases, showing better performance than state-of-the-art FR and NR IQA methods.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4389311899",
    "type": "article"
  },
  {
    "title": "Arbitrary Virtual Try-on Network: Characteristics Preservation and Tradeoff between Body and Clothing",
    "doi": "https://doi.org/10.1145/3636426",
    "publication_date": "2023-12-09",
    "publication_year": 2023,
    "authors": "Yu Liu; Mingbo Zhao; Zhao Zhang; Yuping Liu; Shuicheng Yan",
    "corresponding_authors": "",
    "abstract": "Deep learning based virtual try-on system has achieved some encouraging progress recently, but there still remain several big challenges that need to be solved, such as trying on arbitrary clothes of all types, trying on the clothes from one category to another and generating image-realistic results with few artifacts. To handle this issue, we in this article first collect a new dataset with all types of clothes, i.e., tops, bottoms, and whole clothes, each one has multiple categories with rich information of clothing characteristics such as patterns, logos, and other details. Based on this dataset, we then propose the Arbitrary Virtual Try-On Network (AVTON) that is utilized for all-type clothes, which can synthesize realistic try-on images by preserving and trading off characteristics of the target clothes and the reference person. Our approach includes three modules: (1) Limbs Prediction Module, which is utilized for predicting the human body parts by preserving the characteristics of the reference person. This is especially good for handling cross-category try-on task (e.g., long sleeves ↔ short sleeves or long pants ↔ skirts), where the exposed arms or legs with the skin colors and details can be reasonably predicted; (2) Improved Geometric Matching Module, which is designed to warp clothes according to the geometry of the target person. We improve the TPS based warping method with a compactly supported radial function (Wendland’s Ψ-function); (3) Trade-Off Fusion Module, which is to tradeoff the characteristics of the warped clothes and the reference person. This module is to make the generated try-on images look more natural and realistic based on a fine-tune symmetry of the network structure. Extensive simulations are conducted and our approach can achieve better performance compared with the state-of-the-art virtual try-on methods.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4389506867",
    "type": "article"
  },
  {
    "title": "Two-Stage Perceptual Quality Oriented Rate Control Algorithm for HEVC",
    "doi": "https://doi.org/10.1145/3636510",
    "publication_date": "2023-12-13",
    "publication_year": 2023,
    "authors": "Yunyao Yan; Guoqing Xiang; Huizhu Jia; Jie Chen; Xiaofeng Huang; Xiaodong Xie",
    "corresponding_authors": "",
    "abstract": "As a practical technique in mainstream video coding applications, rate control dominates important to ensure compression quality with limited bitrates constraints. However, most rate control methods mainly focus on objective quality while ignoring the perceptual quality improvement for human eyes. In this paper, we propose a two-stage rate control algorithm to optimize the perceptual quality at the frame encoding stage and the coding tree unit (CTU) encoding stage for high efficiency video coding (HEVC), respectively. Firstly, for the frame encoding stage, with inter-frame distortion dependency consideration, a frame-level rate control method is presented by adjusting the frame-level Lagrange multiplier adaptively with a preprocessing method. Secondly, for the CTU encoding stage, we propose a saliency-based CTU-level perceptual quality rate control algorithm, which employs CTU-level saliency weight to adjust the perceptual rate-distortion (R-D) model. We conduct the CTU-level rate control by an optimized Lagrange multiplier and quantization parameter (QP) to achieve perceptual quality optimization. Extensive experimental results reveal that, compared with state-of-the-art rate control methods on HEVC, our algorithm achieves significant perceptual coding performance with improved subjective visual quality.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4389675199",
    "type": "article"
  },
  {
    "title": "A Bitcoin-based Secure Outsourcing Scheme for Optimization Problem in Multimedia Internet of Things",
    "doi": "https://doi.org/10.1145/3637489",
    "publication_date": "2023-12-19",
    "publication_year": 2023,
    "authors": "Wenyuan Yang; Shaocong Wu; Jianwei Fei; X. Zeng; Yuemin Ding; Zhihua Xia",
    "corresponding_authors": "",
    "abstract": "With the development of the Internet of Things (IoT) and cloud computing, various multimedia data such as audio, video, and images have experienced explosive growth, ushering in the era of big data. Large-scale computing tasks in the Multimedia Internet of Things (M-IoT), such as mathematical optimization problems, have begun to be outsourced from IoT devices with limited computing power to cloud servers for execution. However, outsourcing computation brings security concerns, because the behaviors of clouds are invisible to users. The leakage of privacy data in outsourced optimization problems leads to immeasurable losses. The mutual distrust between clouds and users causes that the correctness of the optimal decisions and the fairness of the payment activities are not guaranteed. Blockchain technology has the characteristic of immutability and has become a new security paradigm for eliminating multi-party trust concerns. In this article, we propose a Bitcoin-based secure outsourcing scheme to address the aforementioned security concerns. To prevent confidential data leakage, the proposed scheme designs a computable privacy-preserving method for the outsourced optimization problems. To judge the correctness of the optimal decision and reduce verification costs, the proposed scheme designs a low-cost two-layer verification mechanism based on dual theory and blockchain technology. Blockchain nodes reach a consensus on the problem solutions and trigger an automatic fair payment protocol-based Bitcoin. Security analysis and experimental results demonstrate that our scheme guarantees privacy, fairness, and computational efficiency.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4389937856",
    "type": "article"
  },
  {
    "title": "TinyPredNet: A Lightweight Framework for Satellite Image Sequence Prediction",
    "doi": "https://doi.org/10.1145/3638773",
    "publication_date": "2023-12-28",
    "publication_year": 2023,
    "authors": "Kuai Dai; Xutao Li; Huiwei Lin; Yin Jiang; Xunlai Chen; Yunming Ye; Di Xian",
    "corresponding_authors": "",
    "abstract": "Satellite image sequence prediction aims to precisely infer future satellite image frames with historical observations, which is a significant and challenging dense prediction task. Though existing deep learning models deliver promising performance for satellite image sequence prediction, the methods suffer from quite expensive training costs, especially in training time and GPU memory demand, due to the inefficiently modeling for temporal variations. This issue seriously limits the lightweight application in satellites such as space-borne forecast models. In this article, we propose a lightweight prediction framework TinyPredNet for satellite image sequence prediction, in which a spatial encoder and decoder model the intra-frame appearance features and a temporal translator captures inter-frame motion patterns. To efficiently model the temporal evolution of satellite image sequences, we carefully design a multi-scale temporal-cascaded structure and a channel attention-gated structure in the temporal translator. Comprehensive experiments are conducted on FengYun-4A (FY-4A) satellite dataset, which show that the proposed framework achieves very competitive performance with much lower computation cost compared to state-of-the-art methods. In addition, corresponding interpretability experiments are conducted to show how our designed structures work. We believe the proposed method can serve as a solid lightweight baseline for satellite image sequence prediction.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4390317758",
    "type": "article"
  },
  {
    "title": "Introduction to ACM multimedia 2010 best paper candidates",
    "doi": "https://doi.org/10.1145/2037676.2037677",
    "publication_date": "2011-10-01",
    "publication_year": 2011,
    "authors": "Shervin Shirmohammadi; Jiebo Luo; Jie Yang; Abdulmotaleb El Saddik",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W1966869280",
    "type": "article"
  },
  {
    "title": "Domical cooperative caching for streaming media in wireless home networks",
    "doi": "https://doi.org/10.1145/2043612.2043618",
    "publication_date": "2011-11-01",
    "publication_year": 2011,
    "authors": "Shahram Ghandeharizadeh; Shahin Shayandeh",
    "corresponding_authors": "",
    "abstract": "Wireless home networks are widely deployed due to their low cost, ease of installation, and plug-and-play capabilities with consumer electronic devices. A challenge of these environments is how to manage data across devices. This is specially true for continuous media (audio and video clips) which are large in size and delay sensitive. Caching of clips across wireless devices may improve user experience, measurable by different Quality of Service (QoS) metrics such as throughput and startup latency. Moreover, caching at the edge of the network reduces the demand for the infrastructure outside the home. In this study, we present Domical, a novel cooperative caching technique designed for streaming media in wireless home networks consisting of a handful of devices. Domical is novel because it considers both asymmetry of the available wireless link bandwidth and heterogeneity of available cache space. We provide a comprehensive description of Domical, presenting its key knobs, and the behavior of the algorithm with different granularity of data caching (block versus clip).",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W1982901288",
    "type": "article"
  },
  {
    "title": "A self-similarity approach to repairing large dropouts of streamed music",
    "doi": "https://doi.org/10.1145/2487268.2487273",
    "publication_date": "2013-06-01",
    "publication_year": 2013,
    "authors": "Jonathan Doherty; Kevin Curran; Paul McKevitt",
    "corresponding_authors": "",
    "abstract": "Enjoyment of audio has now become about flexibility and personal freedom. Digital audio content can be acquired from many sources and wireless networking allows digital media devices and associated peripherals to be unencumbered by wires. However, despite recent improvements in capacity and quality of service, wireless networks are inherently unreliable communications channels for the streaming of audio, being susceptible to the effects of range, interference, and occlusion. This time-varying reliability of wireless audio transfer introduces data corruption and loss, with unpleasant audible effects that can be profound and prolonged in duration. Traditional communications techniques for error mitigation perform poorly and in a bandwidth inefficient manner in the presence of such large-scale defects in a digital audio stream. A novel solution that can complement existing techniques takes account of the semantics and natural repetition of music. Through the use of self-similarity metadata, missing or damaged audio segments can be seamlessly replaced with similar undamaged segments that have already been successfully received. We propose a technology to generate relevant self-similarity metadata for arbitrary audio material and to utilize this metadata within a wireless audio receiver to provide sophisticated and real-time correction of large-scale errors. The primary objectives are to match the current section of a song being received with previous sections while identifying incomplete sections and determining replacements based on previously received portions of the song. This article outlines our approach to Forward Error Correction (FEC) technology that is used to “repair” a bursty dropout when listening to time-dependent media on a wireless network. Using self-similarity analysis on a music file, we can “automatically” repair the dropout with a similar portion of the music already received thereby minimizing a listener's discomfort.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W1990143371",
    "type": "article"
  },
  {
    "title": "Statistical multiplexing of variable-bit-rate videos streamed to mobile devices",
    "doi": "https://doi.org/10.1145/1925101.1925107",
    "publication_date": "2011-02-01",
    "publication_year": 2011,
    "authors": "Cheng-Hsin Hsu; Mohamed Hefeeda",
    "corresponding_authors": "",
    "abstract": "We address the problem of broadcasting multiple video streams over a broadcast network to many mobile devices, so that: (i) streaming quality of mobile devices is maximized, (ii) energy consumption of mobile devices is minimized, and (iii) goodput in the network is maximized. We consider two types of broadcast networks: closed-loop networks, in which all video streams are jointly encoded to ensure their total bit rate does not exceed the broadcast network bandwidth, and open-loop networks, in which videos are encoded using standalone coders, and thus must be carefully broadcast to avoid playout glitches. We first show that the problem of optimally broadcasting multiple videos is NP-complete. We then propose an approximation algorithm to construct burst schedules for multiple VBR (Variable-Bit-Rate) streams. The proposed algorithm frees network operators from the manual and error-prone bandwidth reservation process which is currently used in practice. We prove that the proposed algorithm achieves optimal goodput and near-optimal energy saving. We show that it produces glitch-free schedules in closed-loop networks, and it minimizes number of glitches in open-loop networks. We implement the proposed algorithm in a trace-driven simulator, and conduct extensive simulations for both open- and closed-loop networks. The simulation results show that the proposed algorithm outperforms the existing algorithms in many aspects, including number of late frames, number of concurrently broadcast video streams, and energy saving of mobile devices. To show the practicality and efficiency of the proposed algorithm, we also implement it in a real mobile TV testbed as a proof of concept. The results from the testbed confirm that the proposed algorithm: (i) does not result in playout glitches, (ii) achieves high energy saving, and (iii) runs in real time.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2013211237",
    "type": "article"
  },
  {
    "title": "Care and scale",
    "doi": "https://doi.org/10.1145/2492703",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Brian Whitman",
    "corresponding_authors": "Brian Whitman",
    "abstract": "The co-founder of The Echo Nest, a music intelligence company that now powers recommendation and discovery for most music services, discusses the notion of care and scale, cultural analysis of music, a brief history of music retrieval, and how and why The Echo Nest got started.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2049125430",
    "type": "article"
  },
  {
    "title": "Navigating the worldwide community of photos",
    "doi": "https://doi.org/10.1145/2492208",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Richard Szeliski; Noah Snavely; Steven M. Seitz",
    "corresponding_authors": "",
    "abstract": "The last decade has seen an explosion in the number of photographs available on the Internet. The sheer volume of interesting photos makes it a challenge to explore this space. Various Web and social media sites, along with search and indexing techniques, have been developed in response. One natural way to navigate these images in a 3D geo-located context. In this article, we reflect on our work in this area, with a focus on techniques that build partial 3D scene models to help find and navigate interesting photographs in an interactive, immersive 3D setting. We also discuss how finding such relationships among photographs opens up exciting new possibilities for multimedia authoring, visualization, and editing.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2085817938",
    "type": "article"
  },
  {
    "title": "Introduction to special issue on multimedia security",
    "doi": "https://doi.org/10.1145/2344436.2344437",
    "publication_date": "2012-09-01",
    "publication_year": 2012,
    "authors": "Mohan Kankanhalli",
    "corresponding_authors": "Mohan Kankanhalli",
    "abstract": "No abstract available.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2103699198",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2559928",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "To facilitate users to access news quickly and comprehensively, we design a news search and browsing system named GeoVisNews, in which the news elements of “Where”, “Who”, “What” and “When” are enhanced via news geo-localization, image enrichment and ...",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4232724543",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2487268",
    "publication_date": "2013-06-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The ever increasing volume of video content on the Web has created profound challenges for developing efficient indexing and search techniques to manage video data. Conventional techniques such as video compression and summarization strive for the two ...",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4248235928",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2523001",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "For over two decades, video streaming over the Internet has received a substantial amount of attention from both academia and industry. Starting from the design of transport protocols for streaming video, research interests have later shifted to the ...",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4248904370",
    "type": "paratext"
  },
  {
    "title": "Multi-Class Latent Concept Pooling for Computer-Aided Endoscopy Diagnosis",
    "doi": "https://doi.org/10.1145/3051481",
    "publication_date": "2017-03-21",
    "publication_year": 2017,
    "authors": "Shuai Wang; Yang Cong; Huijie Fan; Baojie Fan; Lianqing Liu; Yunsheng Yang; Yandong Tang; Huaici Zhao; Haibin Yu",
    "corresponding_authors": "",
    "abstract": "Successful computer-aided diagnosis systems typically rely on training datasets containing sufficient and richly annotated images. However, detailed image annotation is often time consuming and subjective, especially for medical images, which becomes the bottleneck for the collection of large datasets and then building computer-aided diagnosis systems. In this article, we design a novel computer-aided endoscopy diagnosis system to deal with the multi-classification problem of electronic endoscopy medical records (EEMRs) containing sets of frames, while labels of EEMRs can be mined from the corresponding text records using an automatic text-matching strategy without human special labeling. With unambiguous EEMR labels and ambiguous frame labels, we propose a simple but effective pooling scheme called Multi-class Latent Concept Pooling, which learns a codebook from EEMRs with different classes step by step and encodes EEMRs based on a soft weighting strategy. In our method, a computer-aided diagnosis system can be extended to new unseen classes with ease and applied to the standard single-instance classification problem even though detailed annotated images are unavailable. In order to validate our system, we collect 1,889 EEMRs with more than 59K frames and successfully mine labels for 348 of them. The experimental results show that our proposed system significantly outperforms the state-of-the-art methods. Moreover, we apply the learned latent concept codebook to detect the abnormalities in endoscopy images and compare it with a supervised learning classifier, and the evaluation shows that our codebook learning method can effectively extract the true prototypes related to different classes from the ambiguous data.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2597972910",
    "type": "article"
  },
  {
    "title": "Distributed Rate Allocation in Switch-Based Multiparty Videoconferencing System",
    "doi": "https://doi.org/10.1145/3092835",
    "publication_date": "2017-06-28",
    "publication_year": 2017,
    "authors": "Stefano D’Aronco; Sergio Mena; Pascal Frossard",
    "corresponding_authors": "",
    "abstract": "Multiparty videoconferences, or more generally multiparty video calls, are gaining a lot of popularity as they offer a rich communication experience. These applications have, however, large requirements in terms of both network and computational resources and have to deal with sets of heterogenous clients. The multiparty videoconferencing systems are usually either based on expensive central nodes, called Multipoint Control Units (MCU), with transcoding capabilities, or on a peer-to-peer architecture where users cooperate to distribute more efficiently the different video streams. Whereas the first class of systems requires an expensive central hardware, the second one depends completely on the redistribution capacity of the users, which sometimes might neither provide sufficient bandwidth nor be reliable enough. In this work, we propose an alternative solution where we use a central node to distribute the video streams, but at the same time we maintain the hardware complexity and the computational requirements of this node as low as possible, for example, it has no video decoding capabilities. We formulate the rate allocation problem as an optimization problem that aims at maximizing the Quality of Service (QoS) of the videoconference. We propose two different distributed algorithms for solving the optimization problem: the first algorithm is able to find an approximate solution of the problem in a one-shot execution, whereas the second algorithm, based on Lagrangian relaxation, performs iterative updates of the optimization variables in order to gradually increase the value of the objective function. The two algorithms, though being disjointed, nicely complement each other. If executed in sequence, they allow us to achieve both a quick approximate rate reallocation, in case of a sudden change of the system conditions, and a precise refinement of the variables, which avoids problems caused by possible faulty approximate solutions. We have further implemented our solution in a network simulator where we show that our rate allocation algorithm is able to properly optimize users’ QoS. We also illustrate the benefits of our solution in terms of network usage and overall utility when compared to a baseline heuristic method operating on the same system architecture.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2613774252",
    "type": "article"
  },
  {
    "title": "Introduction to Special Issue on Deep Learning for Mobile Multimedia",
    "doi": "https://doi.org/10.1145/3088340",
    "publication_date": "2017-06-28",
    "publication_year": 2017,
    "authors": "Kaoru Ota; Minh Son Dao; Vasileios Mezaris; Francesco G. B. De Natale",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2729268012",
    "type": "article"
  },
  {
    "title": "Caching Online Video",
    "doi": "https://doi.org/10.1145/3106157",
    "publication_date": "2017-08-12",
    "publication_year": 2017,
    "authors": "Shahid Akhtar; André T. Beck; Ivica Rimac",
    "corresponding_authors": "",
    "abstract": "Online video presents new challenges to traditional caching with over a thousand-fold increase in number of assets, rapidly changing popularity of assets and much higher throughput requirements. We propose a new hierarchical filtering algorithm for caching online video HiFi. Our algorithm is designed to optimize hit rate, replacement rate and cache throughput. It has an associated implementation complexity comparable to that of LRU. Our results show that, under typical operator conditions, HiFi can increase edge cache byte hit rate by 5%--24% over an LRU policy, but more importantly can increase the RAM or memory byte hit rate by 80% to 200% and reduce the replacement rate by more than 100 times! These two factors combined can dramatically increase throughput for most caches. If SSDs are used for storage, the much lower replacement rate may also allow substitution of lower-cost MLC-based SSDs instead of SLC-based SSDs. We extend previous multi-tier analytical models for LRU caches to caches with filtering. We analytically show how HiFi can approach the performance of an optimal caching policy and how to tune HiFi to reach as close to optimal performance as the traffic conditions allow. We develop a realistic simulation environment for online video using statistics from operator traces. We show that HiFi performs within a few percentage points from the optimal solution which was simulated by Belady's MIN algorithm under typical operator conditions",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2749407930",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1671962",
    "publication_date": "2010-03-01",
    "publication_year": 2010,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Three-dimensional tele-immersive (3DTI) environments have great potential to promote collaborative work among geographically distributed users. However, most existing 3DTI systems only work with two sites due to the huge demand of resources and the lack ...",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4235990922",
    "type": "paratext"
  },
  {
    "title": "RICA-MD: A Refined ICA Algorithm for Motion Detection",
    "doi": "https://doi.org/10.1145/3416492",
    "publication_date": "2021-01-31",
    "publication_year": 2021,
    "authors": "Chao Zhang; Xiaopei Wu; Jianchao Lu; Xi Zheng; Alireza Jolfaei; Quan Z. Sheng; Dongjin Yu",
    "corresponding_authors": "",
    "abstract": "With the rapid development of various computing technologies, the constraints of data processing capabilities gradually disappeared, and more data can be simultaneously processed to obtain better performance compared to conventional methods. As a standard statistical analysis method that has been widely used in many fields, Independent Component Analysis (ICA) provides a new way for motion detection by extracting the foreground without precisely modeling the background. However, most existing ICA-based motion detection algorithms use only two-channel data for source separation and simply generate the observation vectors by decomposing and reconstructing the images by row, hence they cannot obtain an integrated and accurate shape of the moving objects in complex scenes. In this article, we propose a refined ICA algorithm for motion detection (RICA-MD), which fuses a larger number of channels than conventional ICA-based motion detection algorithms to provide more effective information for foreground extraction. Meanwhile, we propose four novel methods for generating observation vectors to further cover the diverse motion styles of the moving objects. These improvements enable RICA-MD to effectively deal with slowly moving objects, which are difficult to detect using conventional methods. Our quantitative evaluation in multiple scenes shows that our proposed method is able to achieve a better performance at an acceptable cost of false alarms.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3144208039",
    "type": "article"
  },
  {
    "title": "High-quality Frame Recurrent Video De-raining with Multi-contextual Adversarial Network",
    "doi": "https://doi.org/10.1145/3444974",
    "publication_date": "2021-05-11",
    "publication_year": 2021,
    "authors": "Prasen Kumar Sharma; Sujoy Ghosh; Arijit Sur",
    "corresponding_authors": "",
    "abstract": "In this article, we address the problem of rain-streak removal in the videos. Unlike the image, challenges in video restoration comprise temporal consistency besides spatial enhancement. The researchers across the world have proposed several effective methods for estimating the de-noised videos with outstanding temporal consistency. However, such methods also amplify the computational cost due to their larger size. By way of analysis, incorporating separate modules for spatial and temporal enhancement may require more computational resources. It motivates us to propose a unified architecture that directly estimates the de-rained frame with maximal visual quality and minimal computational cost. To this end, we present a deep learning-based Frame-recurrent Multi-contextual Adversarial Network for rain-streak removal in videos. The proposed model is built upon a Conditional Generative Adversarial Network (CGAN)-based framework where the generator model directly estimates the de-rained frame from the previously estimated one with the help of its multi-contextual adversary. To optimize the proposed model, we have incorporated the Perceptual loss function in addition to the conventional Euclidean distance. Also, instead of traditional entropy loss from the adversary, we propose to use the Euclidean distance between the features of de-rained and clean frames, extracted from the discriminator model as a cost function for video de-raining. Various experimental observations across 11 test sets, with over 10 state-of-the-art methods, using 14 image-quality metrics, prove the efficacy of the proposed work, both visually and computationally.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3160161700",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on Advanced Approaches for Multiple Instance Learning on Multimedia Applications",
    "doi": "https://doi.org/10.1145/3459603",
    "publication_date": "2021-05-18",
    "publication_year": 2021,
    "authors": "Pourya Shamsolmoali; Ruili Wang; Abdul Sadka",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3162811570",
    "type": "article"
  },
  {
    "title": "IRTS: An Intelligent and Reliable Transmission Scheme for Screen Updates Delivery in DaaS",
    "doi": "https://doi.org/10.1145/3440035",
    "publication_date": "2021-07-22",
    "publication_year": 2021,
    "authors": "Hongdi Zheng; Junfeng Wang; Jianping Zhang; Ruirui Li",
    "corresponding_authors": "",
    "abstract": "Desktop-as-a-service (DaaS) has been recognized as an elastic and economical solution that enables users to access personal desktops from anywhere at any time. During the interaction process of DaaS, users rely on screen updates to perceive execution results remotely, and thus the reliability and timeliness of screen updates transmission have a great influence on users’ quality of experience (QoE). However, the efficient transmission of screen updates in DaaS is facing severe challenges: most transmission schemes applied in DaaS determine sending strategies in terms of pre-set rules, lacking the intelligence to utilize bandwidth rationally and fit new network scenarios. Meanwhile, they tend to focus on reliability or timeliness and perform unsatisfactorily in ensuring reliability and timeliness simultaneously, leading to lower transmission efficiency of screen updates and users’ QoE when network conditions turn unfavorable. In this article, an intelligent and reliable end-to-end transmission scheme (IRTS) is proposed to cope with the preceding issues. IRTS draws support from reinforcement learning by adopting SARSA, an online learning method based on the temporal difference update rule, to grasp the optimal mapping between network states and sending actions, which extricates IRTS from the reliance on pre-set rules and augments its adaptability to different network conditions. Moreover, IRTS guarantees reliability and timeliness via an adaptive loss recovery method, which intends to recover lost screen updates data automatically with fountain code while controlling the number of redundant packets generated. Extensive performance evaluations are conducted, and numerical results show that IRTS outperforms the reference schemes in display quality, end-to-end delay/delay jitter, and fairness when transferring screen updates under various network conditions, proving that IRTS can enhance the transmission efficiency of screen updates and users’ QoE in DaaS.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3186504016",
    "type": "article"
  },
  {
    "title": "Task-independent Recognition of Communication Skills in Group Interaction Using Time-series Modeling",
    "doi": "https://doi.org/10.1145/3450283",
    "publication_date": "2021-11-12",
    "publication_year": 2021,
    "authors": "Candy Olivia Mawalim; Shogo Okada; Yukiko Nakano",
    "corresponding_authors": "",
    "abstract": "Case studies of group discussions are considered an effective way to assess communication skills (CS). This method can help researchers evaluate participants’ engagement with each other in a specific realistic context. In this article, multimodal analysis was performed to estimate CS indices using a three-task-type group discussion dataset, the MATRICS corpus. The current research investigated the effectiveness of engaging both static and time-series modeling, especially in task-independent settings. This investigation aimed to understand three main points: first, the effectiveness of time-series modeling compared to nonsequential modeling; second, multimodal analysis in a task-independent setting; and third, important differences to consider when dealing with task-dependent and task-independent settings, specifically in terms of modalities and prediction models. Several modalities were extracted (e.g., acoustics, speaking turns, linguistic-related movement, dialog tags, head motions, and face feature sets) for inferring the CS indices as a regression task. Three predictive models, including support vector regression (SVR), long short-term memory (LSTM), and an enhanced time-series model (an LSTM model with a combination of static and time-series features), were taken into account in this study. Our evaluation was conducted by using the R 2 score in a cross-validation scheme. The experimental results suggested that time-series modeling can improve the performance of multimodal analysis significantly in the task-dependent setting (with the best R 2 = 0.797 for the total CS index), with word2vec being the most prominent feature. Unfortunately, highly context-related features did not fit well with the task-independent setting. Thus, we propose an enhanced LSTM model for dealing with task-independent settings, and we successfully obtained better performance with the enhanced model than with the conventional SVR and LSTM models (the best R 2 = 0.602 for the total CS index). In other words, our study shows that a particular time-series modeling can outperform traditional nonsequential modeling for automatically estimating the CS indices of a participant in a group discussion with regard to task dependency.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3212609835",
    "type": "article"
  },
  {
    "title": "A New Foreground-Background based Method for Behavior-Oriented Social Media Image Classification",
    "doi": "https://doi.org/10.1145/3458051",
    "publication_date": "2021-11-12",
    "publication_year": 2021,
    "authors": "Lokesh Nandanwar; Palaiahnakote Shivakumara; Divya Krishnani; Raghavendra Ramachandra; Tong Lu; Umapada Pal; Mohan Kankanhalli",
    "corresponding_authors": "",
    "abstract": "Due to various applications, research on personal traits using information on social media has become an important area. In this paper, a new method for the classification of behavior-oriented social images uploaded on various social media platforms is presented. The proposed method introduces a multimodality concept using skin of different parts of human body and background information, such as indoor and outdoor environments. For each image, the proposed method detects skin candidate components based on R, G, B color spaces and entropy features. The iterative mutual nearest neighbor approach is proposed to detect accurate skin candidate components, which result in foreground components. Next, the proposed method detects the remaining part (other than skin components) as background components based on structure tensor of R, G, B color spaces, and Maximally Stable Extremal Regions (MSER ) concept in the wavelet domain. We then explore Hanman Transform for extracting context features from foreground and background components through clustering and fusion operation. These features are then fed to an SVM classifier for the classification of behavior-oriented images. Comprehensive experiments on 10-class datasets of Normal Behavior-Oriented Social media Image (NBSI) and Abnormal Behavior-Oriented Social media Image (ABSI) show that the proposed method is effective and outperforms the existing methods in terms of average classification rate. Also, the results on the benchmark dataset of five classes of personality traits and two classes of emotions of different facial expressions (FERPlus dataset) demonstrated the robustness of the proposed method over the existing methods.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3214012628",
    "type": "article"
  },
  {
    "title": "Partial query resolution for animation authoring",
    "doi": "https://doi.org/10.1145/1324287.1324291",
    "publication_date": "2008-01-01",
    "publication_year": 2008,
    "authors": "Phani S. Kotharu; Balakrishnan Prabhakaran",
    "corresponding_authors": "",
    "abstract": "Animations are a part of multimedia and techniques such as motion mapping and inverse kinematics aid in reusing models and motion sequences to create new animations. This reuse approach is facilitated by the use of content-based retrieval techniques that often require fuzzy query resolution. Most fuzzy query resolution approaches work on all the attributes of the query to minimize the database access cost thus resulting in an unsatisfactory result set. It turns out that the query resolution can be carried out in a partial manner to achieve user satisfactory results and aid in easy authoring. In this article, we present two partial fuzzy query resolution approaches, one that results in high-quality animations and the other that produces results with decreasing number of satisfied conditions in the query.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2065868040",
    "type": "article"
  },
  {
    "title": "Special Section on Multimodal Understanding of Social, Affective, and Subjective Attributes",
    "doi": "https://doi.org/10.1145/3292061",
    "publication_date": "2019-01-24",
    "publication_year": 2019,
    "authors": "Xavier Alameda-Pineda; Мириам Реди; Mohammad Soleymani; Nicu Sebe; Shih-Fu Chang; Samuel D. Gosling",
    "corresponding_authors": "",
    "abstract": "Multimedia scientists have largely focused their research on the recognition of tangible properties of data such as objects and scenes. Recently, the field has started evolving toward the modeling of more complex properties. For example, the understanding of social, affective, and subjective attributes of visual data has attracted the attention of many research teams at the crossroads of computer vision, multimedia, and social sciences. These intangible attributes include, for example, visual beauty, video popularity, or user behavior. Multiple, diverse challenges arise when modeling such properties from multimedia data. The sections concern technical aspects such as reliable groundtruth collection, the effective learning of subjective properties, or the impact of context in subjective perception; see Refs. [2] and [3].",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2914421969",
    "type": "article"
  },
  {
    "title": "Introduction to the Best Papers of the ACM Multimedia Systems (MMSys) Conference 2018 and the ACM Workshop on Network and Operating System Support for Digital Audio and Video (NOSSDAV) 2018 and the International Workshop on Mixed and Virtual Environment Systems (MMVE) 2018",
    "doi": "https://doi.org/10.1145/3339846",
    "publication_date": "2019-04-30",
    "publication_year": 2019,
    "authors": "Pablo César; Michael Zink; Niall Murray",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2968145035",
    "type": "article"
  },
  {
    "title": "Rethinking the Combined and Individual Orders of Derivative of States for Differential Recurrent Neural Networks",
    "doi": "https://doi.org/10.1145/3337928",
    "publication_date": "2019-08-31",
    "publication_year": 2019,
    "authors": "Naifan Zhuang; Guo-Jun Qi; The Duc Kieu; Kien A. Hua",
    "corresponding_authors": "",
    "abstract": "Due to their special gating schemes, Long Short-Term Memory (LSTM) has shown greater potential to process complex sequential information than the traditional Recurrent Neural Network (RNN). The conventional LSTM, however, fails to take into consideration the impact of salient spatio-temporal dynamics present in the sequential input data. This problem was first addressed by the differential Recurrent Neural Network (dRNN), which uses a differential gating scheme known as Derivative of States (DoS). DoS uses higher orders of internal state derivatives to analyze the change in information gain originated from the salient motions between the successive frames. The weighted combination of several orders of DoS is then used to modulate the gates in dRNN. While each individual order of DoS is good at modeling a certain level of salient spatio-temporal sequences, the sum of all the orders of DoS could distort the detected motion patterns. To address this problem, we propose to control the LSTM gates via individual orders of DoS. To fully utilize the different orders of DoS, we further propose to stack multiple levels of LSTM cells in an increasing order of state derivatives. The proposed model progressively builds up the ability of the LSTM gates to detect salient dynamical patterns in deeper stacked layers modeling higher orders of DoS; thus, the proposed LSTM model is termed deep differential Recurrent Neural Network ( d 2 RNN). The effectiveness of the proposed model is demonstrated on three publicly available human activity datasets: NUS-HGA, Violent-Flows, and UCF101. The proposed model outperforms both LSTM and non-LSTM based state-of-the-art algorithms.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2998596875",
    "type": "article"
  },
  {
    "title": "Table of Contents",
    "doi": "https://doi.org/10.1145/3359668",
    "publication_date": "2019-08-31",
    "publication_year": 2019,
    "authors": "Richang Hong",
    "corresponding_authors": "Richang Hong",
    "abstract": "editorial Free AccessTable of Contents: Online Supplement Volume 15, Number 2s Share on Author: Richang Hong View Profile Authors Info & Claims ACM Transactions on Multimedia Computing, Communications, and ApplicationsVolume 15Issue 3September 2019 Article No.: 69epp 1–3https://doi.org/10.1145/3359668Published:31 August 2019 1citation93DownloadsMetricsTotal Citations1Total Downloads93Last 12 Months9Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteView all FormatsPDF",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4246361817",
    "type": "article"
  },
  {
    "title": "Human Selective Matting",
    "doi": "https://doi.org/10.1145/3640017",
    "publication_date": "2024-01-15",
    "publication_year": 2024,
    "authors": "Qinglin Liu; Quanling Meng; Xiaoqian Lv; Zonglin Li; Wei Yu; Shengping Zhang",
    "corresponding_authors": "",
    "abstract": "Existing human matting methods are incapable of accurately estimating the alpha mattes of arbitrarily selected humans from a group photo. An alternative solution is to apply them to the corresponding cropped image patches. However, this option obtains an inaccurate alpha estimation due to the interference of the body parts of the neighboring humans. In addition, these methods are only trained on finely annotated synthetic data, which causes poor performance in real-world scenarios due to the domain shift. To address these problems, we propose human selective matting (HSMatt), which performs matting for arbitrarily selected humans from a group photo given only a simple bounding box as guidance. Specifically, we design a global–local context network to extract both local and global semantic context features. A human-aware trimap network is then proposed to generate human-aware trimaps for the selected humans, which adopts stacked bidirectional inference modules with intermediate supervision to progressively refine the estimated trimap. Finally, a partially supervised matting network is introduced to estimate the alpha matte, which uses a sample-varying loss to train the network on both the finely annotated synthetic data and coarsely annotated real-world data, resulting in high accuracy and good generalization. To evaluate the proposed HSMatt, we construct the first human selective matting dataset, named HSM-200K, which contains over 200,000 human images with instance-level alpha matte annotations. Experimental results demonstrate that the proposed HSMatt outperforms state-of-the-art methods.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4390878761",
    "type": "article"
  },
  {
    "title": "A Reconfigurable Framework for Neural Network Based Video In-Loop Filtering",
    "doi": "https://doi.org/10.1145/3640467",
    "publication_date": "2024-01-16",
    "publication_year": 2024,
    "authors": "Yichi Zhang; Dandan Ding; Zhan Ma; Zhu Li",
    "corresponding_authors": "",
    "abstract": "This article proposes a reconfigurable framework for neural network based video in-loop filtering to guide large-scale models for content-aware processing. Specifically, the backbone neural model is decomposed into several convolutional groups and the encoder systematically traverses all candidate configurations combined by these groups to find the best one. The selected configuration index is then encapsulated as side information and passed to the decoder, enabling dynamic model reconfiguration during the decoding stage. The preceding reconfiguration process is only deployed in the inference stage on top of a pre-trained backbone model. Furthermore, we devise WMSPFormer , a wavelet multi-scale Poolformer, as the backbone network structure. WMSPFormer utilizes a wavelet-based multi-scale structure to losslessly decompose the input into multiple scales for spatial-spectral features aggregation. Moreover, it uses multi-scale pooling operations ( MSPoolformer ) instead of complicated matrix calculations to substitute the attention process. We also extend MSPoolformer to a large-scale version using more parameters, referred to as MSPoolformerExt . Extensive experiments demonstrate that the proposed WMSPFormer+Reconfig. and WMSPFormerExt+Reconfig. achieve a remarkable 7.13% and 7.92% BD-Rate reduction over the anchor H.266/VVC, outperforming most existing methods evaluated under the same training and testing conditions. In addition, the low-complexity nature of the WMSPFormer series makes it attractive for practical applications.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4390918564",
    "type": "article"
  },
  {
    "title": "Deep Variational Learning for 360° Adaptive Streaming",
    "doi": "https://doi.org/10.1145/3643031",
    "publication_date": "2024-01-25",
    "publication_year": 2024,
    "authors": "Quentin Guimard; Lucile Sassatelli; Francesco Marchetti; Federico Becattini; Lorenzo Seidenari; Alberto Del Bimbo",
    "corresponding_authors": "",
    "abstract": "Prediction of head movements in immersive media is key to designing efficient streaming systems able to focus the bandwidth budget on visible areas of the content. However, most of the numerous proposals made to predict user head motion in 360° images and videos do not explicitly consider a prominent characteristic of the head motion data: its intrinsic uncertainty. In this article, we present an approach to generate multiple plausible futures of head motion in 360° videos, given a common past trajectory. To our knowledge, this is the first work that considers the problem of multiple head motion prediction for 360° video streaming. We introduce our discrete variational multiple sequence (DVMS) learning framework, which builds on deep latent variable models. We design a training procedure to obtain a flexible, lightweight stochastic prediction model compatible with sequence-to-sequence neural architectures. Experimental results on four different datasets show that DVMS outperforms competitors adapted from the self-driving domain by up to 41% on prediction horizons up to 5 s, at lower computational and memory costs. To understand how the learned features account for the motion uncertainty, we analyze the structure of the learned latent space and connect it with the physical properties of the trajectories. We also introduce a method to estimate the likelihood of each generated trajectory, enabling the integration of DVMS in a streaming system. We hence deploy an extensive evaluation of the interest of our DVMS proposal for a streaming system. To do so, we first introduce a new Python-based 360° streaming simulator that we make available to the community. On real-world user, video, and networking data, we show that predicting multiple trajectories yields higher fairness between the traces, the gains for 20–30% of the users reaching up to 10% in visual quality for the best number K of trajectories to generate.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4391222154",
    "type": "article"
  },
  {
    "title": "Multimodal Visual-Semantic Representations Learning for Scene Text Recognition",
    "doi": "https://doi.org/10.1145/3646551",
    "publication_date": "2024-02-19",
    "publication_year": 2024,
    "authors": "Xinjian Gao; Ye Pang; Yuyu Liu; Maokun Han; Jun Yu; Wei Wang; Yuanxu Chen",
    "corresponding_authors": "",
    "abstract": "Scene Text Recognition (STR), the critical step in OCR systems, has attracted much attention in computer vision. Recent research on modeling textual semantics with Language Model (LM) has witnessed remarkable progress. However, LM only optimizes the joint probability of the estimated characters generated from the Vision Model (VM) in a single language modality, ignoring the visual-semantic relations in different modalities. Thus, LM-based methods can hardly generalize well to some challenging conditions, in which the text has weak or multiple semantics, arbitrary shape, and so on. To migrate the above issue, in this paper, we propose Multimodal Visual-Semantic Representations Learning for Text Recognition Network (MVSTRN) to reason and combine the multimodal visual-semantic information for accurate Scene Text Recognition. Specifically, our MVSTRN builds a bridge between vision and language through its unified architecture and has the ability to reason visual semantics by guiding the network to reconstruct the original image from the latent text representation, breaking the structural gap between vision and language. Finally, the tailored multimodal Fusion (MMF) module is motivated to combine the multimodal visual and textual semantics from VM and LM to make the final predictions. Extensive experiments demonstrate our MVSTRN achieves state-of-the-art performance on several benchmarks.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4391931509",
    "type": "article"
  },
  {
    "title": "Recipe Generation from Unsegmented Cooking Videos",
    "doi": "https://doi.org/10.1145/3649137",
    "publication_date": "2024-02-21",
    "publication_year": 2024,
    "authors": "Taichi Nishimura; Atsushi Hashimoto; Yoshitaka Ushiku; Hirotaka Kameko; Shinsuke Mori",
    "corresponding_authors": "",
    "abstract": "This paper tackles recipe generation from unsegmented cooking videos, a task that requires agents to (1) extract key events in completing the dish and (2) generate sentences for the extracted events. Our task is similar to dense video captioning (DVC), which aims at detecting events thoroughly and generating sentences for them. However, unlike DVC, in recipe generation, recipe story awareness is crucial, and a model should extract an appropriate number of events in the correct order and generate accurate sentences based on them. We analyze the output of the DVC model and confirm that although (1) several events are adoptable as a recipe story, (2) the generated sentences for such events are not grounded in the visual content. Based on this, we set our goal to obtain correct recipes by selecting oracle events from the output events and re-generating sentences for them. To achieve this, we propose a transformer-based multimodal recurrent approach of training an event selector and sentence generator for selecting oracle events from the DVC’s events and generating sentences for them. In addition, we extend the model by including ingredients to generate more accurate recipes. The experimental results show that the proposed method outperforms state-of-the-art DVC models. We also confirm that, by modeling the recipe in a story-aware manner, the proposed model outputs the appropriate number of events in the correct order.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4392018329",
    "type": "article"
  },
  {
    "title": "Tensorial Evolutionary Optimization for Natural Image Matting",
    "doi": "https://doi.org/10.1145/3649138",
    "publication_date": "2024-02-23",
    "publication_year": 2024,
    "authors": "Si-Chao Lei; Yue‐Jiao Gong; Xiaolin Xiao; Yicong Zhou; Jun Zhang",
    "corresponding_authors": "",
    "abstract": "Natural image matting has garnered increasing attention in various computer vision applications. The matting problem aims to find the optimal foreground/background (F/B) color pair for each unknown pixel and thus obtain an alpha matte indicating the opacity of the foreground object. This problem is typically modeled as a large-scale pixel pair combinatorial optimization (PPCO) problem. Heuristic optimization is widely employed to tackle the PPCO problem owing to its gradient-free property and promising search ability. However, traditional heuristic methods often encode F/B solutions to a one-dimensional (1D) representation and then evolve the solutions in a 1D manner. This 1D representation destroys the intrinsic two-dimensional (2D) structure of images, where the significant spatial correlations among pixels are ignored. Moreover, the 1D representation also brings operation inefficiency. To address the above issues, this article develops a spatial-aware tensorial evolutionary image matting (TEIM) method. Specifically, the matting problem is modeled as a 2D Spatial-PPCO (S-PPCO) problem, and a global tensorial evolutionary optimizer is proposed to tackle the S-PPCO problem. The entire population is represented as a whole by a third-order tensor, in which individuals are classified into two types: F and B individuals for denoting the 2D F/B solutions, respectively. The evolution process, consisting of three tensorial evolutionary operators, is implemented based on pure tensor computation for efficiently seeking F/B solutions. The local spatial smoothness of images is also integrated into the evaluation process for obtaining a high-quality alpha matte. Experimental results compared with state-of-the-art methods validate the effectiveness of TEIM.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4392121153",
    "type": "article"
  },
  {
    "title": "Action Segmentation Through Self-Supervised Video Features and Positional-Encoded Embeddings",
    "doi": "https://doi.org/10.1145/3649465",
    "publication_date": "2024-08-16",
    "publication_year": 2024,
    "authors": "Guilherme de A. P. Marques; José Boaro; Antonio José G. Busson; Álan L. V. Guedes; Júlio César Duarte; Sérgio Colcher",
    "corresponding_authors": "",
    "abstract": "Action segmentation consists of temporally segmenting a video and labeling each segmented interval with a specific action label. In this work, we propose a novel action segmentation method that requires no initial video analysis and no annotated data. Our proposal involves extracting features from videos using several pre-trained deep-learning models, including spatiotemporal and self-supervised methods. Data is then transformed using a positional encoder, and finally, a clustering algorithm is applied, where each produced cluster presumably corresponds to a different single and distinguishable action. For self-supervised features, we explored DINO, and for spatiotemporal features, we investigated I3D and SlowFast methods. Moreover, two different clustering algorithms (FINCH and KMeans) were investigated, and we also explored how varying the length of video snippets that generate the feature vectors affected the quality of the segmentation. Experiments show that our method produces competitive results on the Breakfast and INRIA Instructional Videos dataset benchmarks. Our best result was produced using a composition of self-supervised features generated by DINO, FINCH clustering, and positional encoding.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4392131568",
    "type": "article"
  },
  {
    "title": "Perceptual Quality-Oriented Rate Allocation via Distillation from End-to-End Image Compression",
    "doi": "https://doi.org/10.1145/3650034",
    "publication_date": "2024-02-29",
    "publication_year": 2024,
    "authors": "Runyu Yang; Dong Liu; Siwei Ma; Feng Wu; Wen Gao",
    "corresponding_authors": "",
    "abstract": "Mainstream image/video coding standards, exemplified by the state-of-the-art H.266/VVC, AVS3, and AV1, follow the block-based hybrid coding framework. Due to the block-based framework, encoders designed for these standards are easily optimized for peak signal-to-noise ratio (PSNR) but have difficulties optimizing for the metrics more aligned to perceptual quality, e.g., multi-scale structural similarity (MS-SSIM), since these metrics cannot be accurately evaluated at the small block level. We address this problem by leveraging inspiration from the end-to-end image compression built on deep networks, which is easily optimized through network training for any metric as long as the metric is differentiable. We compared the trained models using the same network structure but different metrics and observed that the models allocate rates in different ratios. We then propose a distillation method to obtain the rate allocation rule from end-to-end image compression models with different metrics and to utilize such a rule in the block-based encoders. We implement the proposed method on the VVC reference software—VTM and the AVS3 reference software—HPM, focusing on intraframe coding. Experimental results show that the proposed method on top of VTM achieves more than 10% BD-rate reduction than the anchor when evaluated with MS-SSIM or LPIPS, which leads to concrete perceptual quality improvement.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4392295192",
    "type": "article"
  },
  {
    "title": "Realizing Efficient On-Device Language-based Image Retrieval",
    "doi": "https://doi.org/10.1145/3649896",
    "publication_date": "2024-03-15",
    "publication_year": 2024,
    "authors": "Zhiming Hu; Mete Kemertas; Lan Xiao; Caleb Phillips; Iqbal Mohomed; Afsaneh Fazly",
    "corresponding_authors": "",
    "abstract": "Advances in deep learning have enabled accurate language-based search and retrieval (e.g., over user photos) in the cloud. Many users prefer to store their photos in the home due to privacy concerns. As such, a need arises for models that can perform cross-modal search on resource-limited devices. State-of-the-art (SOTA) cross-modal retrieval models achieve high accuracy through learning entangled representations that enable fine-grained similarity calculation between a language query and an image, but at the expense of having a prohibitively high retrieval latency. Alternatively, there is a new class of methods that exhibits good performance with low latency but requires a lot more computational resources and an order of magnitude more training data (i.e., large web-scraped datasets consisting of millions of image–caption pairs), making them infeasible to use in a commercial context. From a pragmatic perspective, none of the existing methods are suitable for developing commercial applications for low-latency cross-modal retrieval on low-resource devices. We propose CrispSearch, a cascaded approach that greatly reduces the retrieval latency with minimal loss in ranking accuracy for on-device language-based image retrieval. The idea behind our approach is to combine a light-weight and runtime-efficient coarse model with a fine re-ranking stage. Given a language query, the coarse model effectively filters out many of the irrelevant image candidates. After this filtering, only a handful of strong candidates will be selected and sent to a fine model for re-ranking. Extensive experimental results with two SOTA models for the fine re-ranking stage on standard benchmark datasets show that CrispSearch results in a speedup of up to 38 times over the SOTA fine methods with negligible performance degradation. Moreover, our method does not require millions of training instances, making it a pragmatic solution to on-device search and retrieval.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4392864334",
    "type": "article"
  },
  {
    "title": "A Novel Framework for Joint Learning of City Region Partition and Representation",
    "doi": "https://doi.org/10.1145/3652857",
    "publication_date": "2024-03-17",
    "publication_year": 2024,
    "authors": "Mingyu Deng; Wanyi Zhang; Jie Zhao; Zhu Wang; Mingliang Zhou; Jun Luo; Chao Chen",
    "corresponding_authors": "",
    "abstract": "The proliferation of multimodal big data in cities provides unprecedented opportunities for modeling and forecasting urban problems, such as crime prediction and house price prediction, through data-driven approaches. A fundamental and critical issue in modeling and forecasting urban problems lies in identifying suitable spatial analysis units, also known as city region partition. Existing works rely on subjective domain knowledge for static partitions, which is general and universal for all tasks. In fact, different tasks may need different city region partitions. To address this issue, we propose JLPR , a task-oriented framework for J oint L earning of region P artition and R epresentation. To make partitions fit tasks, JLPR integrates the region partition into the representation model training and learns region partitions using the supervision signal from the downstream task. We evaluate the framework on two prediction tasks (i.e., crime prediction and housing price prediction) in Chicago. Experiments show that JLPR consistently outperforms state-of-the-art partitioning methods in both tasks, which achieves above 25% and 70% performance improvements in terms of mean absolute error for crime prediction and house price prediction tasks, respectively. Additionally, we meticulously undertake three visualization case studies, which yield profound and illuminating findings from diverse perspectives, demonstrating the remarkable effectiveness and superiority of our approach.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4392895023",
    "type": "article"
  },
  {
    "title": "Real-time Attentive Dilated U-Net for Extremely Dark Image Enhancement",
    "doi": "https://doi.org/10.1145/3654668",
    "publication_date": "2024-03-27",
    "publication_year": 2024,
    "authors": "Junjian Huang; Hao Ren; Shulin Liu; Yong Liu; chuantao Lv; Jiawen Lu; Changyong Xie; Hong Lu",
    "corresponding_authors": "",
    "abstract": "Images taken under low-light conditions suffer from poor visibility, color distortion, and graininess, all of which degrade the image quality and hamper the performance of downstream vision tasks, such as object detection and instance segmentation in the field of autonomous driving, making low-light enhancement an indispensable basic component of high-level visual tasks. Low-light enhancement aims to mitigate these issues, and has garnered extensive attention and research over several decades. The primary challenge in low-light image enhancement arises from the low signal-to-noise ratio caused by insufficient lighting. This challenge becomes even more pronounced in near-zero lux conditions, where noise overwhelms the available image information. Both traditional image signal processing pipeline and conventional low-light image enhancement methods struggle in such scenarios. Recently, deep neural networks have been used to address this challenge. These networks take unmodified RAW images as input and produce the enhanced sRGB images, forming a deep learning based image signal processing pipeline. However, most of these networks are computationally expensive and thus far from practical use. In this article, we propose a lightweight model called attentive dilated U-Net (ADU-Net) to tackle this issue. Our model incorporates several innovative designs, including an asymmetric U-shape architecture, dilated residual modules for feature extraction, and attentive fusion modules for feature fusion. The dilated residual modules provide strong representative capability, whereas the attentive fusion modules effectively leverage low-level texture information and high-level semantic information within the network. Both modules employ a lightweight design but offer significant performance gains. Extensive experiments demonstrate that our method is highly effective, achieving an excellent balance between image quality and computational complexity—that is, taking less than 4ms for a high-definition 4K image on a single GTX 1080Ti GPU and yet maintaining competitive visual quality. Furthermore, our method exhibits pleasing scalability and generalizability, highlighting its potential for widespread applicability.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4393224628",
    "type": "article"
  },
  {
    "title": "A Unified Framework for Jointly Compressing Visual and Semantic Data",
    "doi": "https://doi.org/10.1145/3654800",
    "publication_date": "2024-03-28",
    "publication_year": 2024,
    "authors": "Shizhan Liu; Weiyao Lin; Yihang Chen; Yufeng Zhang; Wenrui Dai; John See; Hongkai Xiong",
    "corresponding_authors": "",
    "abstract": "The rapid advancement of multimedia and imaging technologies has resulted in increasingly diverse visual and semantic data. A large range of applications such as remote-assisted driving requires the amalgamated storage and transmission of various visual and semantic data. However, existing works suffer from the limitation of insufficiently exploiting the redundancy between different types of data. In this article, we propose a unified framework to jointly compress a diverse spectrum of visual and semantic data, including images, point clouds, segmentation maps, object attributes, and relations. We develop a unifying process that embeds the representations of these data into a joint embedding graph according to their categories, which enables flexible handling of joint compression tasks for various visual and semantic data. To fully leverage the redundancy between different data types, we further introduce an embedding-based adaptive joint encoding process and a Semantic Adaptation Module to efficiently encode diverse data based on the learned embeddings in the joint embedding graph. Experiments on the Cityscapes, MSCOCO, and KITTI datasets demonstrate the superiority of our framework, highlighting promising steps toward scalable multimedia processing.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4393259594",
    "type": "article"
  },
  {
    "title": "Universal Relocalizer for Weakly Supervised Referring Expression Grounding",
    "doi": "https://doi.org/10.1145/3656045",
    "publication_date": "2024-04-04",
    "publication_year": 2024,
    "authors": "Panpan Zhang; Meng Liu; Xuemeng Song; Da Cao; Zan Gao; Liqiang Nie",
    "corresponding_authors": "",
    "abstract": "This article introduces the Universal Relocalizer, a novel approach designed for weakly supervised referring expression grounding. Our method strives to pinpoint a target proposal that corresponds to a specific query, eliminating the need for region-level annotations during training. To bolster the localization precision and enrich the semantic understanding of the target proposal, we devise three key modules: the category module, the color module, and the spatial relationship module. The category and color modules assign respective category and color labels to region proposals, enabling the computation of category and color scores. Simultaneously, the spatial relationship module integrates spatial cues, yielding a spatial score for each proposal to enhance localization accuracy further. By adeptly amalgamating the category, color, and spatial scores, we derive a refined grounding score for every proposal. Comprehensive evaluations on the RefCOCO, RefCOCO+, and RefCOCOg datasets manifest the prowess of the Universal Relocalizer, showcasing its formidable performance across the board.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4393934405",
    "type": "article"
  },
  {
    "title": "C2: ABR Streaming in Cognizant of Consumption Context for Improved QoE and Resource Usage Tradeoffs",
    "doi": "https://doi.org/10.1145/3652517",
    "publication_date": "2024-08-16",
    "publication_year": 2024,
    "authors": "Cheonjin Park; Chinmaey Shende; Subhabrata Sen; Bing Wang",
    "corresponding_authors": "",
    "abstract": "Smartphones have emerged as ubiquitous platforms for people to consume content in a wide range of consumption contexts (C2) , e.g., over cellular or WiFi, playing back audio and video directly on phone or through peripheral devices such as external screens or speakers. In this article, we argue that a user’s specific C2 is an important factor to consider in Adaptive Bitrate (ABR) streaming. We examine the current practices of using C2 in five popular ABR players, and identify various limitations in existing treatments that have a detrimental impact on network resource usage and user experience. We then formulate C2-cognizant ABR streaming as an optimization problem and develop practical best-practice guidelines to realize it. Instantiating these guidelines, we develop a proof-of-concept implementation in the widely used state-of-the-art ExoPlayer platform and demonstrate that it leads to significantly better tradeoffs in terms of user experience and resource usage. Last, we show that the guidelines also benefit dash.js player that uses an ABR logic significantly different from that of ExoPlayer.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4394931316",
    "type": "article"
  },
  {
    "title": "Recurrent Appearance Flow for Occlusion-Free Virtual Try-On",
    "doi": "https://doi.org/10.1145/3659581",
    "publication_date": "2024-04-23",
    "publication_year": 2024,
    "authors": "Xiaoling Gu; Junkai Zhu; Yongkang Wong; Zizhao Wu; Jun Yu; Jianping Fan; Mohan Kankanhalli",
    "corresponding_authors": "",
    "abstract": "Image-based virtual try-on aims at transferring a target in-shop garment onto a reference person, and has garnered significant attention from the research communities recently. However, previous methods have faced severe challenges in handling occlusion problems. To address this limitation, we classify occlusion problems into three types based on the reference person’s arm postures: single-arm occlusion , two-arm non-crossed occlusion , and two-arm crossed occlusion . Specifically, we propose a novel Occlusion-Free Virtual Try-On Network (OF-VTON) that effectively overcomes these occlusion challenges. The OF-VTON framework consists of two core components: (i) a new Recurrent Appearance Flow based Deformation (RAFD) model that robustly aligns the in-shop garment to the reference person by adopting a multi-task learning strategy . This model jointly produces the dense appearance flow to warp the garment and predicts a human segmentation map to provide semantic guidance for the subsequent image synthesis model. (ii) a powerful Multi-mask Image SynthesiS (MISS) model that generates photo-realistic try-on results by introducing a new mask generation and selection mechanism . Experimental results demonstrate that our proposed OF-VTON significantly outperforms existing state-of-the-art methods by mitigating the impact of occlusion problems. Our code is available at https://github.com/gxl-groups/OF-VTON .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4395038457",
    "type": "article"
  },
  {
    "title": "Bridging the Domain Gap in Scene Flow Estimation via Hierarchical Smoothness Refinement",
    "doi": "https://doi.org/10.1145/3661823",
    "publication_date": "2024-04-27",
    "publication_year": 2024,
    "authors": "Dejun Zhang; Mian Zhang; Xuefeng Tan; Jun Liu",
    "corresponding_authors": "",
    "abstract": "This article introduces SmoothFlowNet3D, an innovative encoder-decoder architecture specifically designed for bridging the domain gap in scene flow estimation. To achieve this goal, SmoothFlowNet3D divides the scene flow estimation task into two stages: initial scene flow estimation and smoothness refinement. Specifically, SmoothFlowNet3D comprises a hierarchical encoder that extracts multi-scale point cloud features from two consecutive frames, along with a hierarchical decoder responsible for predicting the initial scene flow and further refining it to achieve smoother estimation. To generate the initial scene flow, a cross-frame nearest-neighbor search operation is performed between the features extracted from two consecutive frames, resulting in forward and backward flow embeddings. These embeddings are then combined to form the bidirectional flow embedding, serving as input for predicting the initial scene flow. Additionally, a flow smoothing module based on the self-attention mechanism is proposed to predict the smoothing error and facilitate the refinement of the initial scene flow for more accurate and smoother estimation results. Extensive experiments demonstrate that the proposed SmoothFlowNet3D approach achieves state-of-the-art performance on both synthetic datasets and real LiDAR point clouds, confirming its effectiveness in enhancing scene flow smoothness.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4395691052",
    "type": "article"
  },
  {
    "title": "InteractNet: Social Interaction Recognition for Semantic-rich Videos",
    "doi": "https://doi.org/10.1145/3663668",
    "publication_date": "2024-05-03",
    "publication_year": 2024,
    "authors": "Yuanjie Lyu; Penggang Qin; Tong Xu; Chen Zhu; Enhong Chen",
    "corresponding_authors": "",
    "abstract": "The overwhelming surge of online video platforms has raised an urgent need for social interaction recognition techniques. Compared with simple short-term actions, long-term social interactions in semantic-rich videos could reflect more complicated semantics such as character relationships or emotions, which will better support various downstream applications, e.g., story summarization and fine-grained clip retrieval. However, considering the longer duration of social interactions with severe mutual overlap, involving multiple characters, dynamic scenes, and multi-modal cues, among other factors, traditional solutions for short-term action recognition may probably fail in this task. To address these challenges, in this article, we propose a hierarchical graph-based system, named InteractNet, to recognize social interactions in a multi-modal perspective. Specifically, our approach first generates a semantic graph for each sampled frame with integrating multi-modal cues and then learns the node representations as short-term interaction patterns via an adapted GCN module. Along this line, global interaction representations are accumulated through a sub-clip identification module, effectively filtering out irrelevant information and resolving temporal overlaps between interactions. In the end, the association among simultaneous interactions will be captured and modelled by constructing a global-level character-pair graph to predict the final social interactions. Comprehensive experiments on publicly available datasets demonstrate the effectiveness of our approach compared with state-of-the-art baseline methods.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4396612938",
    "type": "article"
  },
  {
    "title": "Exploiting Instance-level Relationships in Weakly Supervised Text-to-Video Retrieval",
    "doi": "https://doi.org/10.1145/3663571",
    "publication_date": "2024-05-03",
    "publication_year": 2024,
    "authors": "Shukang Yin; Sirui Zhao; Hao Wang; Tong Xu; Enhong Chen",
    "corresponding_authors": "",
    "abstract": "Text-to-Video Retrieval is a typical cross-modal retrieval task that has been studied extensively under a conventional supervised setting. Recently, some works have sought to extend the problem to a weakly supervised formulation, which can be more consistent with real-life scenarios and more efficient in annotation cost. In this context, a new task called Partially Relevant Video Retrieval (PRVR) is proposed, which aims to retrieve videos that are partially relevant to a given textual query, i.e., the videos containing at least one semantically relevant moment. Formulating the task as a Multiple Instance Learning (MIL) ranking problem, prior arts rely on heuristics algorithms such as a simple greedy search strategy and deal with each query independently. Although these early explorations have achieved decent performance, they may not fully utilize the bag-level label and only consider the local optimum, which could result in suboptimal solutions and inferior final retrieval performance. To address this problem, in this paper, we propose to exploit the relationships between instances to boost retrieval performance. Based on this idea, we creatively put forward: (1) a new matching scheme for pairing queries and their related moments in the video; and (2) a new loss function to facilitate cross-modal alignment between two views of an instance. Extensive validations on three publicly available datasets have demonstrated the effectiveness of our solution and verified our hypothesis that modeling instance-level relationships is beneficial in the MIL ranking setting. Our code will be publicly available at https://github.com/xjtupanda/BGM-Net .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4396612943",
    "type": "article"
  },
  {
    "title": "Exploration of Speech and Music Information for Movie Genre Classification",
    "doi": "https://doi.org/10.1145/3664197",
    "publication_date": "2024-05-07",
    "publication_year": 2024,
    "authors": "Mrinmoy Bhattacharjee; S. R. Mahadeva Prasanna; Prithwijit Guha",
    "corresponding_authors": "",
    "abstract": "Movie genre prediction from trailers is mostly attempted in a multi-modal manner. However, the characteristics of movie trailer audio indicate that this modality alone might be highly effective in genre prediction. Movie trailer audio predominantly consists of speech and music signals in isolation or overlapping conditions. This work hypothesizes that the genre labels of movie trailers might relate to the composition of their audio component. In this regard, speech-music confidence sequences for the trailer audio are used as a feature. In addition, two other features previously proposed for discriminating speech-music are also adopted in the current task. This work proposes a time and channel Attention Convolutional Neural Network (ACNN) classifier for the genre classification task. The convolutional layers in ACNN learn the spatial relationships in the input features. The time and channel attention layers learn to focus on crucial timesteps and CNN kernel outputs, respectively. The Moviescope dataset is used to perform the experiments, and two audio-based baseline methods are employed to benchmark this work. The proposed feature set with the ACNN classifier improves the genre classification performance over the baselines. Moreover, decent generalization performance is obtained for genre prediction of movies with different cultural influences (EmoGDB).",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4396695496",
    "type": "article"
  },
  {
    "title": "IoT Video Delivery Optimization Through Machine Learning-Based Frame Resolution Adjustment",
    "doi": "https://doi.org/10.1145/3665929",
    "publication_date": "2024-05-24",
    "publication_year": 2024,
    "authors": "Yoanes Bandung; Mokhamad Arfan Wicaksono; Sean Pribadi; Armein Z. R. Langi; Dion Tanjung",
    "corresponding_authors": "",
    "abstract": "Providing acceptable video quality in the Internet of Things (IoT) implementation poses a significant challenge, mainly when the application is performed on low-cost and low-power devices. This research focuses on developing a frame resolution adjustment system that maintains the frame rate value of video delivery in wireless IoT environments with resource-constrained devices. Consistent frame rates prevent motion lag and data loss, improving user experience. The system works by predicting the upcoming throughput values using machine learning methods to adjust the sensing parameter, which is the resolution of the video frame to be captured by camera nodes. Hence, the proposed system is equipped with a file size estimator to estimate the size of the next video frame and then adjust the resolution in accordance with the throughput prediction. In this research, we conducted extensive experiments to evaluate the accuracy of the file size estimator and the throughput prediction. The experiment generated a dataset to evaluate throughput prediction and file size estimator model. The evaluation results for the file size estimator showed a mean absolute percentage error (MAPE) of 6.73% in the experiment using 317 frames with video resolutions between 72p and 720p. Experiments were also conducted to compare several machine learning methods for predicting throughput values. Compared to long short-term memory (LSTM) and autoregressive integrated moving average (ARIMA), simple exponential smoothing (SES) outperforms the others with the lowest root mean squared error (RMSE) and mean absolute error (MAE) values. Building upon these findings, we implemented the frame resolution adjustment system using SES as the method for predicting the upcoming throughput values. Finally, we demonstrated that the proposed system can maintain the frame rate according to the threshold set by the system while the resolution is being maximized, thereby addressing the challenges of maintaining video quality in resource-constrained IoT environments.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4398776566",
    "type": "article"
  },
  {
    "title": "Cascaded Adaptive Graph Representation Learning for Image Copy-Move Forgery Detection",
    "doi": "https://doi.org/10.1145/3669905",
    "publication_date": "2024-05-29",
    "publication_year": 2024,
    "authors": "Yuanman Li; Lanhao Ye; Haokun Cao; Wei Wang; Zhongyun Hua",
    "corresponding_authors": "",
    "abstract": "In the realm of image security, there has been a burgeoning interest in harnessing deep learning techniques for the detection of digital image copy-move forgeries, resulting in promising outcomes. The generation process of such forgeries results in a distinctive topological structure among patches, and collaborative modeling based on these underlying topologies proves instrumental in enhancing the discrimination of ambiguous pixels. Despite the attention received, existing deep learning models predominantly rely on convolutional neural networks (CNNs), falling short in adequately capturing correlations among distant patches. This limitation impedes the seamless propagation of information and collaborative learning across related patches. To address this gap, our work introduces an innovative framework for image copy-move forensics rooted in graph representation learning. Initially, we introduce an adaptive graph learning approach to foster collaboration among related patches, dynamically learning the inherent topology of patches. The devised approach excels in promoting efficient information flow among related patches, encompassing both short-range and long-range correlations. Additionally, we formulate a cascaded graph learning framework, progressively refining patch representations and disseminating information to broader correlated patches based on their updated topologies. Finally, we propose a hierarchical cross-attention mechanism facilitating the exchange of information between the cascaded graph learning branch and a dedicated forgery detection branch. This equips our method with the capability to jointly grasp the homology of copy-move correspondences and identify inconsistencies between the target region and the background. Comprehensive experimental results validate the superiority of our proposed scheme, providing a robust solution to security challenges posed by digital image manipulations.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4399125861",
    "type": "article"
  },
  {
    "title": "ANAGL: A Noise-resistant and Anti-sparse Graph Learning for micro-video recommendation",
    "doi": "https://doi.org/10.1145/3670407",
    "publication_date": "2024-06-03",
    "publication_year": 2024,
    "authors": "Jingwei Ma; Kangkang Bian; Yang Xu; Lei Zhu",
    "corresponding_authors": "",
    "abstract": "In recent years, Graph Convolutional Networks (GCNs) have seen widespread utilization within micro-video recommendation systems, facilitating the understanding of user preferences through interactions with micro-videos. Despite the commendable performance exhibited by GCN-based methodologies, several persistent issues demand further scrutiny. Primarily, most user-micro-video interactions involve implicit behaviors, such as clicks or abstentions, which may inadvertently capture irrelevant micro-video content, thereby introducing significant noise (false touches, low watch-ratio, low ratings) into users’ histories. Consequently, this noise undermines the efficacy of micro-video recommendations. Moreover, the abundance of micro-videos has resulted in fewer interactions between users and micro-video content. To tackle these challenges, we propose a noise-resistant and anti-sparse graph learning framework for micro-video recommendation. Initially, we construct a denoiser that leverages implicit multi-attribute information (e.g., watch-ratio, timestamp, ratings, etc.) to filter noisy data from user interaction histories. This process yields high-fidelity micro-video information, enabling a more precise modeling of users’ feature preferences. Subsequently, we employ a multi-view reconstruction approach and utilize cross-view self-supervised learning to gain insights into user and micro-video features. This strategic approach effectively mitigates the issue of data sparsity. Extensive experiments conducted on two publicly available micro-video recommendation datasets validate the effectiveness of our proposed method. For in-depth details and access to the code, please refer to our repository at “ https://github.com/kbk12/ANAGL.git .”",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4399301894",
    "type": "article"
  },
  {
    "title": "Compressed Point Cloud Quality Index by Combining Global Appearance and Local Details",
    "doi": "https://doi.org/10.1145/3672567",
    "publication_date": "2024-06-15",
    "publication_year": 2024,
    "authors": "Yiling Xu; Yujie Zhang; Qi Yang; Xiaozhong Xu; Shan Liu",
    "corresponding_authors": "",
    "abstract": "In recent years, many standardized algorithms for point cloud compression (PCC) has been developed and achieved remarkable compression ratios. To provide guidance for rate-distortion optimization and codec evaluation, point cloud quality assessment (PCQA) has become a critical problem for PCC. Therefore, in order to achieve a more consistent correlation with human visual perception of a compressed point cloud, we propose a full-reference PCQA algorithm tailored for static point clouds in this paper, which can jointly measure geometry and attribute deformations. Specifically, we assume that the quality decision of compressed point clouds is determined by both global appearance (e.g., density, contrast, complexity) and local details (e.g., gradient, hole). Motivated by the nature of compression distortions and the properties of the human visual system, we derive perceptually effective features for the above two categories, such as content complexity, luminance/ geometry gradient, and hole probability. Through systematically incorporating measurements of variations in the local and global characteristics, we derive an effective quality index for the input compressed point clouds. Extensive experiments and analyses conducted on popular PCQA databases show the superiority of the proposed method in evaluating compression distortions. Subsequent investigations validate the efficacy of different components within the model design.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4399702955",
    "type": "article"
  },
  {
    "title": "Text-driven Video Prediction",
    "doi": "https://doi.org/10.1145/3675171",
    "publication_date": "2024-06-27",
    "publication_year": 2024,
    "authors": "Xue Song; Jingjing Chen; Bin Zhu; Yu‐Gang Jiang",
    "corresponding_authors": "",
    "abstract": "Current video generation models usually convert signals indicating appearance and motion received from inputs (e.g., image and text) or latent spaces (e.g., noise vectors) into consecutive frames, fulfilling a stochastic generation process for the uncertainty introduced by latent code sampling. However, this generation pattern lacks deterministic constraints for both appearance and motion, leading to uncontrollable and undesirable outcomes. To this end, we propose a new task called Text-driven Video Prediction (TVP). Taking the first frame and text caption as inputs, this task aims to synthesize the following frames. Specifically, appearance and motion components are provided by the image and caption separately. The key to addressing the TVP task depends on fully exploring the underlying motion information in text descriptions, thus facilitating plausible video generation. In fact, this task is intrinsically a cause-and-effect problem, as the text content directly influences the motion changes of frames. To investigate the capability of text in causal inference for progressive motion information, our TVP framework contains a Text Inference Module (TIM), producing step-wise embeddings to regulate motion inference for subsequent frames. In particular, a refinement mechanism incorporating global motion semantics guarantees coherent generation. Extensive experiments are conducted on Something-Something V2 and Single Moving MNIST datasets. Experimental results demonstrate that our model achieves better results over other baselines, verifying the effectiveness of the proposed framework.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4400075376",
    "type": "article"
  },
  {
    "title": "Artificial Intelligence Empowered Digital Twins for ECG Monitoring in a Smart Home",
    "doi": "https://doi.org/10.1145/3672564",
    "publication_date": "2024-07-04",
    "publication_year": 2024,
    "authors": "Junxin Chen; Z X Wang; Tongyue He; Bo Fang; Chen Li; Mikael Fridenfalk; Zhihan Lyu",
    "corresponding_authors": "",
    "abstract": "Recent years have witnessed the increasing prevalence of smart home applications, where digital twin (DT) is popularly employed for creating virtual models that interact with physical devices in real time. Empowered by artificial intelligence (AI), these DT-created virtual models have more intelligent decision-making capabilities to ensure reliable performance of a smart home system. In this paper, a DT based smart home framework is investigated. It is capable of achieving intelligent control, healthcare prediction and graphical monitoring. First, the human body and device are individually modeled, and then assembled into a DT system, and the corresponding model interfaces are provided for visual monitoring. Then, an intelligent algorithm fusing VGG, LSTM and attention mechanism is developed for healthcare monitoring, i.e., the screening out of the irregular ECG rhythms. The system results are provided, including various high-fidelity interactive DT interfaces as well as the effectiveness and advantages of the intelligent algorithms for arrhythmia detection.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4400322080",
    "type": "article"
  },
  {
    "title": "Noise-Tolerant Hybrid Prototypical Learning with Noisy Web Data",
    "doi": "https://doi.org/10.1145/3672396",
    "publication_date": "2024-07-08",
    "publication_year": 2024,
    "authors": "Chao Liang; Linchao Zhu; Zongxin Yang; Wei Chen; Yi Yang",
    "corresponding_authors": "",
    "abstract": "We focus on the challenging problem of learning an unbiased classifier from a large number of potentially relevant but noisily labeled web images given only a few clean labeled images. This problem is particularly practical because it reduces the expensive annotation costs by utilizing freely accessible web images with noisy labels. Typically, prototypes are representative images or features used to classify or identify other images. However, in the few clean and many noisy scenarios, the class prototype can be severely biased due to the presence of irrelevant noisy images. The resulting prototypes are less compact and discriminative, as previous methods do not take into account the diverse range of images in the noisy web image collections. On the other hand, the relation modeling between noisy and clean images is not learned for the class prototype generation in an end-to-end manner, which results in a suboptimal class prototype. In this article, we introduce a similarity maximization loss named SimNoiPro. Our SimNoiPro first generates noise-tolerant hybrid prototypes composed of clean and noise-tolerant prototypes and then pulls them closer to each other. Our approach considers the diversity of noisy images by explicit division and overcomes the optimization discrepancy issue. This enables better relation modeling between clean and noisy images and helps extract judicious information from the noisy image set. The evaluation results on two extended few-shot classification benchmarks confirm that our SimNoiPro outperforms prior methods in measuring image relations and cleaning noisy data.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4400419662",
    "type": "article"
  },
  {
    "title": "Digging into Depth and Color Spaces: A Mapping Constraint Network for Depth Super-Resolution",
    "doi": "https://doi.org/10.1145/3677123",
    "publication_date": "2024-07-10",
    "publication_year": 2024,
    "authors": "Baoli Sun; Yanjun Guo; Tiantian Yan; Xinchen Ye; Zhihui Wang; Haojie Li; Zhiyong Wang",
    "corresponding_authors": "",
    "abstract": "Scene depth super-resolution (DSR) poses an inherently ill-posed problem due to the extremely large space of one-to-many mapping functions from a given low-resolution (LR) depth map, which possesses limited depth information, to multiple plausible high-resolution (HR) depth maps. This characteristic renders the task highly challenging, as identifying an optimal solution becomes significantly intricate amidst this multitude of potential mappings. While simplistic constraints have been proposed to address the DSR task, the relationship between LR and HR depth maps and the color image has not been thoroughly investigated. In this paper, we introduce a novel mapping constraint network (MCNet) that incorporates additional constraints derived from both LR depth maps and color images. This integration aims to optimize the space of mapping functions and enhance the performance of DSR. Specifically, alongside the primary DSR network (DSRNet) dedicated to learning LR-to-HR mapping, we have developed an auxiliary degradation network (ADNet) that operates in reverse, generating the LR depth map from the reconstructed HR depth map to obtain depth features in LR space. To enhance the learning process of DSRNet in LR-to-HR mapping, we introduce two mapping constraints in LR space: (1) the cycle-consistent constraint, which offers additional supervision by establishing a closed loop between LR-to-HR and HR-to-LR mappings, and (2) the region-level contrastive constraint, aimed at reinforcing region-specific HR representations by explicitly modeling the consistency between LR and HR spaces. To leverage the color image effectively, we introduce a feature screening module to adaptively fuse color features at different layers, which can simultaneously maintain strong structural context and suppress texture distraction through subspace generation and image projection. Comprehensive experimental results across synthetic and real-world benchmark datasets unequivocally demonstrate the superiority of our proposed method over state-of-the-art DSR methods. Our MCNet achieves an average MAD reduction of 3.7% and 7.5% over state-of-the-art DSR method for ×8 and ×16 cases on Milddleburry dataset, respectively, without incurring additional costs during inference.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4400493221",
    "type": "article"
  },
  {
    "title": "Boosting Semi-Supervised Video Captioning via Learning Candidates Adjusters",
    "doi": "https://doi.org/10.1145/3652838",
    "publication_date": "2024-07-11",
    "publication_year": 2024,
    "authors": "Wanru Xu; Zhenjiang Miao; Jian Yu; Yigang Cen; Yi Tian; Lili Wan; Yanli Wan; Qiang Ji",
    "corresponding_authors": "",
    "abstract": "Video captioning is a multimodal task on both CV and NLP, whose goal is to automatically obtain the description of video content with natural language statements. Although there are amounts of video data, their annotations with description sentences are very limited. In this paper, we define the semi-supervised video captioning (SSVC) problem in order to improve performance with limited annotations by leveraging the semantic knowledge from both well-annotated samples and no-annotated samples. To address the problem, we introduce a LCA-boosted model (LCABM) for boosting SSVC, where it is first to explore a learnable candidates adjuster to adjust the caption candidates and then treat these adjusted captions as pesudo labels to train the SSVC model with no-annotated samples in reverse. In particular, the model learning is considered as a bi-level optimization problem and solved by an EM-like multi-stage training algorithm. The experiments show the effectiveness of our proposed LCABM, whose performance is comparable and even better than those state-of-the-art fully-supervised methods even with less annotations.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4400550467",
    "type": "article"
  },
  {
    "title": "Transcoding V-PCC Point Cloud Streams in Real-time",
    "doi": "https://doi.org/10.1145/3682062",
    "publication_date": "2024-08-01",
    "publication_year": 2024,
    "authors": "Michael Rudolph; Stefan Schneegaß; Amr Rizk",
    "corresponding_authors": "",
    "abstract": "Dynamic Point Clouds are a representation for 3D immersive media that allows users to freely navigate a scene while consuming the content. However, this comes at the cost of substantial data size, requiring efficient compression techniques to make point cloud videos accessible. Addressing this, Video-based Point Cloud Compression (V-PCC) projects points into 2D patches to compress video frames, leveraging the high compression efficiency of legacy video codecs and exploiting temporal correlations in the 2D images. However, clustering and projecting points into meaningful 2D patches is computationally intensive, leading to high encoding latency in V-PCC. Applying adaptive streaming techniques, originating from traditional video streaming, multiplies the computational effort as multiple encodings of the same content are required. In this light, transcoding a compressed representation into lower qualities for dynamic adaptation to user requirements is gaining popularity. To address the high latency when employing the full decoder-encoder stack of V-PCC during transcoding, we propose RABBIT, a novel technique that only re-encodes the underlying video sub-streams. This is in contrast to slow V-PCC transcoding that reconstructs and re-encodes the raw point cloud at a new quality setting. By eliminating expensive overhead resulting from calculations based on the 3D space representation, the latency of RABBIT is bounded by the latency of transcoding the underlying video streams, allowing optimized video codec implementations to be used to meet the real time requirements of adaptive streaming systems. Our evaluations of RABBIT, using various optimized video codec implementations, shows on-par quality with the baseline V-PCC transcoding given a high-quality representation. Given unicast or multicast distribution of a point cloud stream and in-network or edge transcoders, our evaluations show the tradeoff between rate-distortion performance and the required network bandwidth.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4401216610",
    "type": "article"
  },
  {
    "title": "Knowledge Guided Transformer Network for Compositional Zero-shot Learning",
    "doi": "https://doi.org/10.1145/3687129",
    "publication_date": "2024-08-09",
    "publication_year": 2024,
    "authors": "Aditya Panda; Dipti Prasad Mukherjee",
    "corresponding_authors": "",
    "abstract": "Compositional Zero-shot Learning (CZSL) attempts to recognise images of new compositions of states and objects when images of only a subset of state-object compositions are available as training data. An example of CZSL is to recognise images of peeled apple by a model when it is trained using images of peeled orange , ripe apple and ripe orange . There are two major challenges in solving CZSL. First, the visual features of a state vary depending on the context of a state-object composition. For example state like ripe produces distinct visual properties in the compositions ripe orange and ripe banana . Hence, understanding the context dependency of state features is a necessary requirement to solve CZSL. Second, the extent of association between the features of a state and an object varies significantly in different images of same composition. For example, in different images of peeled oranges , the oranges may be peeled to different extents. As a consequence, the visual features of images of the class peeled orange may vary. Hence, there exists a significant amount of intra-class variability among the visual features of different images of a composition. Existing approaches merely look for the existence or absence of features of particular state or object in a composition. Our approach not only looks for the existence of a particular state features or object features but also the extent of association of state features and object features to better tackle the intra-class variability in visual features of compositional images. The proposed architecture is constructed using a novel Knowledge Guided Transformer . The transformer-based framework is utilised for processing larger context dependency between the state and object. Extensive experiments on C-GQA, MIT-States and UT-Zappos50k datasets demonstrate the superiority of the proposed approach in comparison with the state-of-the-art in both open-world and closed-world CZSL settings.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4401454109",
    "type": "article"
  },
  {
    "title": "TPTE: Text-guided Patch Token Exploitation for Unsupervised Fine-Grained Representation Learning",
    "doi": "https://doi.org/10.1145/3673657",
    "publication_date": "2024-08-09",
    "publication_year": 2024,
    "authors": "Shunan Mao; Hao Chen; Yaowei Wang; Wei Zeng; Shiliang Zhang",
    "corresponding_authors": "",
    "abstract": "Recent advances in pre-trained vision-language models have successfully boosted the performance of unsupervised image representation in many vision tasks. Most of existing works focus on learning global visual features with Transformers and neglect detailed local cues, leading to suboptimal performance in fine-grained vision tasks. In this article, we propose a text-guided patch token exploitation framework to enhance the discriminative power of unsupervised representation by exploiting more detailed local features. Our text-guided decoder extracts local features with the guidance of texts or learned prompts describing discriminative object parts. We hence introduce a local-global relation distillation loss to promote the joint optimization of local and global features. The proposed method allows to flexibly extract either global or global-local features as the image representation. It significantly outperforms previous methods in fine-grained image retrieval and base-to-new fine-grained classification tasks. For instance, our Recall@1 metric surpasses the recent unsupervised retrieval method STML by 6.0% on the SOP dataset. The code is publicly available at https://github.com/maosnhehe/TPTE .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4401454333",
    "type": "article"
  },
  {
    "title": "Hypercube Pooling for Visual Semantic Embedding",
    "doi": "https://doi.org/10.1145/3689637",
    "publication_date": "2024-08-23",
    "publication_year": 2024,
    "authors": "Hongbin Wang; Rui Tang; Fan Li",
    "corresponding_authors": "",
    "abstract": "Visual Semantic Embedding ( VSE ) is a primary model for cross-modal retrieval, wherein the global feature aggregator is a crucial component of the VSE model. In recent research, the General Pooling Operator ( GPO ) aggregator, which weighs the features reconstructed from the local feature set to aggregate, facilitates the related models to achieve good retrieval performance. However, the reason for the effectiveness remains to be explored. To enhance the rationality of aggregator designs, we analyze the reason from the perspective of feature space. Indeed, for each data, the local feature set forms a hypercube containing abundant data information, and the feature learned by GPO measures the hypercube, thereby representing the data. The geometric structure of the hypercube implies that the set containing all points within the hypercube is a convex set, so the feature learned by weighted aggregation is an interior point of the hypercube. However, using the interior point to measure the hypercube leads to some problems in feature representation and model optimization, as well as the reduction of retrieval efficiency caused by weight computation. For example, the related pair’s features may be far, while the unrelated ones may be close. To measure the hypercube more clearly and alleviate the problems mentioned above, we propose Hypercube Pooling ( HCP ) aggregator. Specifically, HCP concatenates the Max and Min Pooling features as the global features. This aggregation method has multiple advantages, e.g., the learned global feature represents all hyperplanes of the hypercube that contain critical information and hypercube geometric structure. Moreover, HCP adds normalization-before-concatenation and reduces the usual setting of margin in the loss function by half to avoid gradient loss caused by the difference in the feature value and dimensionality doubling. The experimental results on the Flickr30K and MSCOCO datasets show that the HCP model has excellent performance with high efficiency, confirming the correctness of the spatial analysis and the effectiveness of the HCP aggregator.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4401817900",
    "type": "article"
  },
  {
    "title": "Depth Matters: Spatial Proximity-based Gaze Cone Generation for Gaze Following in Wild",
    "doi": "https://doi.org/10.1145/3689643",
    "publication_date": "2024-08-26",
    "publication_year": 2024,
    "authors": "Feiyang Liu; Kun Li; Zhun Zhong; Wei Jia; Bin Hu; Xun Yang; Meng Wang; Dan Guo",
    "corresponding_authors": "",
    "abstract": "Gaze following aims to predict where a person is looking in a scene. Existing methods tend to prioritize traditional 2D RGB visual cues or require burdensome prior knowledge and extra expensive datasets annotated in 3D coordinate systems to train specialized modules to enhance scene modeling. In this work, we introduce a novel framework deployed on a simple ResNet backbone, which exclusively uses image and depth maps to mimic human visual preferences and realize 3D-like depth perception. We first leverage depth maps to formulate spatial-based proximity information regarding the objects with the target person. This process sharpens the focus of the gaze cone on the specific region of interest pertaining to the target while diminishing the impact of surrounding distractions. To capture the diverse dependence of scene context on the saliency gaze cone, we then introduce a learnable grid-level regularized attention that anticipates coarse-grained regions of interest, thereby refining the mapping of the saliency feature to pixel-level heatmaps. This allows our model to better account for individual differences when predicting others’ gaze locations. Finally, we employ the KL-divergence loss to super the grid-level regularized attention, which combines the gaze direction, heatmap regression, and in/out classification losses, providing comprehensive supervision for model optimization. Experimental results on two publicly available datasets demonstrate the comparable performance of our model with less help of modal information. Quantitative visualization results further validate the interpretability of our method. The source code will be available at https://github.com/VUT-HFUT/DepthMatters .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4401887908",
    "type": "article"
  },
  {
    "title": "Assessing Usefulness, Ease of Use and Recognition Performance of Semi-Automatic Mulsemedia Authoring",
    "doi": "https://doi.org/10.1145/3689640",
    "publication_date": "2024-08-26",
    "publication_year": 2024,
    "authors": "Raphael Abreu; Joel dos Santos; Gheorghiță Ghinea; Débora C. Muchaluat-Saade",
    "corresponding_authors": "",
    "abstract": "Mulsemedia (Multiple Sensorial Media) authoring poses a considerable challenge as authors navigate the intricate task of identifying moments to activate sensory effects within multimedia content. A novel proposal is to integrate content recognition algorithms that use machine learning (ML) into authoring tools to alleviate the authoring effort. As author subjectivity is very important, it is imperative to allow users to define which sensory effects should be automatically extracted. This paper conducts a twofold evaluation of the proposed semi-automatic authoring. The first is from a user perspective within the STEVE 2.0 mulsemedia authoring tool, employing the Goal-Question-Metric (GQM) methodology and a user feedback questionnaire. Our user evaluation indicates that users perceive the semi-automatic authoring approach as a positive enhancement to the authoring process. The second evaluation targets sensory effect recognition using two different content recognition modules, quantifying their automatic recognition capabilities against manual authoring. Metrics such as precision, recall, and F1 scores provide insights into the strengths and nuances of each module. Differences in label assignments underscore the need for ML module result combination methodologies. These evaluations contribute to a comprehensive understanding of the effectiveness of sensory effect recognition modules in enhancing mulsemedia content authoring.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4401888088",
    "type": "article"
  },
  {
    "title": "XDTEncoder: A Deep Explainable Arrhythmia Classification Framework for Smart Healthcare",
    "doi": "https://doi.org/10.1145/3689947",
    "publication_date": "2024-08-27",
    "publication_year": 2024,
    "authors": "Jilong Wang; Jianhui Lv; Rui Li; Yinyin Gong; Yijie Chen; B. D. Parameshachari; Adam Słowik; Wei Wei",
    "corresponding_authors": "",
    "abstract": "In the context of smart healthcare, the integration of multimedia and digital twin technologies has driven significant advances in telemedicine. Electrocardiogram (ECG) signals, as a part of multimedia healthcare data, provide crucial digital information about the electrical activity of the heart, which is essential for diagnosing arrhythmias and ensuring a healthy life. Arrhythmia classification is a fundamental step in analyzing ECG signals and a critical problem for diagnosing heart diseases. One key challenge in arrhythmia classification is the lack of high accuracy for classifying arrhythmia heartbeats, and another key challenge is the lack of interpretability of decision-making models. This study aims to develop a novel approach to improve the performance of arrhythmia classification while providing explainable diagnostic decision paths. We propose XDTEncoder, an explainable arrhythmia classification framework that leverages multi-level features to classify arrhythmia heartbeats while offering an explainable diagnostic decision path. XDTEncoder is novel in three aspects. (1) It constructs a human-machine collaborative knowledge representation based on the Encoder-Decoder paradigm, which allows our model to classify arrhythmias while producing decision paths for cardiologists. (2) XDTEncoder compares two encoding methods (the binary tree encoding method and the Huffman encoding method) to embed the diagnostic decision tree into the arrhythmia classification framework. (3) XDTEncoder fuses multi-level features to improve the performance of arrhythmias classification. Evaluation on 5 types of arrhythmias in the MIT-BIH database demonstrates that our new approach outperforms state-of-the-art classifiers while providing interpretability.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4401910812",
    "type": "article"
  },
  {
    "title": "SwinShadow: Shifted Window for Ambiguous Adjacent Shadow Detection",
    "doi": "https://doi.org/10.1145/3688803",
    "publication_date": "2024-08-27",
    "publication_year": 2024,
    "authors": "Yonghui Wang; Shaokai Liu; Li Li; Wengang Zhou; Houqiang Li",
    "corresponding_authors": "",
    "abstract": "Shadow detection is a fundamental and challenging task in many computer vision applications. Intuitively, most shadows come from the occlusion of light by the object itself, resulting in the object and its shadow being contiguous (referred to as the adjacent shadow in this article). In this case, when the color of the object is similar to that of the shadow, existing methods struggle to achieve accurate detection. To address this problem, we present SwinShadow, a transformer-based architecture that fully utilizes the powerful shifted window mechanism for detecting adjacent shadows. The mechanism operates in two steps. Initially, it applies local self-attention within a single window, enabling the network to focus on local details. Subsequently, it shifts the attention windows to facilitate inter-window attention, enabling the capture of a broader range of adjacent information. These combined steps significantly improve the network’s capacity to distinguish shadows from nearby objects. And the whole process can be divided into three parts: encoder, decoder, and feature integration. During encoding, we adopt Swin Transformer to acquire hierarchical features. Then during decoding, for shallow layers, we propose a deep supervision (DS) module to suppress the false positives and boost the representation capability of shadow features for subsequent processing, while for deep layers, we leverage a double attention (DA) module to integrate local and shifted window in one stage to achieve a larger receptive field and enhance the continuity of information. Ultimately, a new multi-level aggregation (MLA) mechanism is applied to fuse the decoded features for mask prediction. Extensive experiments on three shadow detection benchmark datasets, SBU, UCF, and ISTD, demonstrate that our network achieves good performance in terms of balance error rate (BER). The source code and results are now publicly available at https://github.com/harrytea/SwinShadow .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4401912111",
    "type": "article"
  },
  {
    "title": "Upsampling algorithm for V-PCC-coded 3D point clouds",
    "doi": "https://doi.org/10.1145/3690641",
    "publication_date": "2024-08-31",
    "publication_year": 2024,
    "authors": "Ting‐Lan Lin; Bing-Wei Su; P. X. Shen; D M Chen; Chi-Fu Liang; Yan-Cheng Chen; Yangming Wen; Mohammad Shahid",
    "corresponding_authors": "",
    "abstract": "Point cloud (PC) compression is crucial to immersive visual applications such as autonomous vehicles to classify objects on the roads. The MPEG standardization group has achieved a notable compression efficiency, called video-based point-cloud compression (V-PCC), which consists of an encoder-decoder. The V-PCC encoder takes original 3D PC data and projects them onto multiple 2D planes to generate several 2D feature images. These images are then compressed using the well-established High-Efficiency Video Coding (HEVC) method. The V-PCC decoder uses compressed information and decoding techniques to reconstruct the 3D point cloud. However, the point clouds produced by V-PCC are often sparse, non-uniform, and contain artifacts. In many practical applications, it is necessary to recover complete point clouds from partial ones in real time. This paper presents a method for enhancing decoded point clouds as a post-processing step in the V-PCC with reduced computational time. Our approach involves a 2D upsampling for the V-PCC occupancy image, which increases the density of the point cloud, and a 2D high-resolution auxiliary information modification algorithm for the 2D-3D conversion of high-resolution 3D point clouds, which improves the uniformity and reduces the noise in the point cloud. The 3D high-resolution point cloud has been further enhanced using the developed 3D outlier removal and point regeneration algorithm. Our proposed work can significantly simplify the state-of-the-art superresolution methods for point clouds and reduce the time complexity of \\(61\\%\\sim 75\\%,\\) while maintaining a high level of quality in point clouds.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4402091902",
    "type": "article"
  },
  {
    "title": "Generation and Editing of 2D Shapes using a Branched Representation",
    "doi": "https://doi.org/10.1145/3691635",
    "publication_date": "2024-09-03",
    "publication_year": 2024,
    "authors": "Luis Álvarez; Agustín Trujillo; Nelson Monzón; Jean‐Michel Morel",
    "corresponding_authors": "",
    "abstract": "In this paper we propose a new planar shape representation, the medial branch graph representation (MBGR) which allows to easily generate, vary and edit all-new sorts of parametric shapes. Each MBGR shape is described by a collection of connected control circles organized as a graph. Each pair of connected circles generates a branch of the shape. The boundaries of the branches are designed using cubic Bézier curves which enable shape representation in a compact SVG format. Each circle is associated with a regularity parameter locally controlling the smoothness of the branch connections. In this way, we can manage shapes having both sharp corners and smooth boundaries. To illustrate the potential of the MBGR representation we have created an online facility ( https://mbgrg.github.io/mbgrg/ ) where the user can create/edit MBGR shapes automatically or interactively.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4402169107",
    "type": "article"
  },
  {
    "title": "Pivot: Panoramic-image-based VR User Authentication against Side-Channel Attacks",
    "doi": "https://doi.org/10.1145/3694975",
    "publication_date": "2024-09-09",
    "publication_year": 2024,
    "authors": "Gui Xiao; Zhen Ling; Qunqun Fan; Xiangyu Xu; Wenjia Wu; Ding Ding; Chen Chen; Xinwen Fu",
    "corresponding_authors": "",
    "abstract": "With metaverse attracting increasing attention from both academic and industry, the application of virtual reality (VR) has extended beyond 3D immersive viewing/gaming to a broader range of areas, such as banking, shopping, tourism, education, etc., which involves a growing amount of sensitive and private user data into VR systems. However, with current password-based user authentication schemes in mainstream VR devices, studies demonstrate that side-channel attacks can pose a severe threat to VR user privacy. To mitigate the threat, we propose a novel Panoramic-image-based VR user authentication system, i.e., Pivot , to defend against such attacks, yet maintain high usability. Specifically, in Pivot , we design an image-random-pivoting-based user interaction mechanism to assist users in quickly and securely selecting memorable points of interest in a panoramic image. Then an image region segmentation algorithm is designed to automatically scatter the points to regions to form the customized graphic password for the user, which could ensure a sufficiently large password space and also reduce the near-region point misclicks. Afterward, the region indexes are used to generate the hashed password for authentication. Both theoretical security analysis and extensive user studies demonstrate that Pivot is secure and user-friendly in practice.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4402354741",
    "type": "article"
  },
  {
    "title": "Diverse Image Captioning via Panoptic Segmentation and Sequential Conditional Variational Transformer",
    "doi": "https://doi.org/10.1145/3695878",
    "publication_date": "2024-09-17",
    "publication_year": 2024,
    "authors": "Bing Liu; Jinfu Lu; Mingming Liu; Hao Liu; Yong Zhou; Dongping Yang",
    "corresponding_authors": "",
    "abstract": "Recently, transformer-based image captioning models have achieved significant performance improvement. However, due to the limitations of region visual features and deterministic projections between image space and caption space, existing methods still suffer from disentangled visual features and rigid sentences. To address these issues, we first introduce panoptic segmentation to extract the segmentation region features, which can effectively alleviate the visual confusion caused by the widely-adopted region visual features. Then, we propose a panoptic segmentation based sequential conditional variational transformer (PS-SCVT) framework for diverse image captioning, which not only accurately extracts the image visual representations by fusing the segmentation region features and object detection features, but has the ability of learning one-to-many mappings from image space to caption space. The experimental results demonstrate that our approach achieves better interpretability and generalization performance compared with the state-of-the-art diverse image captioning models.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4402572275",
    "type": "article"
  },
  {
    "title": "EdgeSyn: Privacy-preserving Data Publishing on Edge Network over Infinite Multimedia Data Stream",
    "doi": "https://doi.org/10.1145/3696421",
    "publication_date": "2024-09-19",
    "publication_year": 2024,
    "authors": "Chang Tan; Zhewei Liu; Zhengdao Li; Jingyu Jia; Siyi Lv; Tong Li; Zheli Liu",
    "corresponding_authors": "",
    "abstract": "To privately publish sensitive multimedia data in an edge network with fog devices, one of the best privacy-preserving solutions is to use differential privacy (DP) mechanisms. However, existing DP data publication mechanisms for the infinite data stream of edge networks mainly focus on publishing data with specific types of data or a set of predetermined queries. This approach is not suitable for multimedia data with numerous features that require a more flexible data publishing mechanism. In this paper, we propose EdgeSyn, a novel mechanism for accurately publishing multimedia data over infinite data streams in an edge network. It allocates privacy budgets with a sliding window, adopting data synthesis mechanisms to support dynamic publishing without loss of accuracy. In more detail, EdgeSyn addresses the limitations associated with data types in prior data stream publishing approaches and introduces a privacy budget management strategy that optimally allocates budgets for the implementation of data synthesis mechanisms over an infinite data stream. The experimental results show that EdgeSyn performs well under different privacy budgets and various lengths of active windows.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4402647019",
    "type": "article"
  },
  {
    "title": "Prompt-Based Modality Bridging for Unified Text-to-Face Generation and Manipulation",
    "doi": "https://doi.org/10.1145/3694974",
    "publication_date": "2024-09-24",
    "publication_year": 2024,
    "authors": "Yiyang Ma; Haowei Kuang; H. X. Yang; Jianlong Fu; Jiaying Liu",
    "corresponding_authors": "",
    "abstract": "Text-driven face image generation and manipulation are significant tasks. However, such tasks are quite challenging due to the gap between text and image modalities. It is difficult to utilize current methods to deal with both of the two problems because these methods are usually designed for one certain task, limiting their application in real scenarios. To address the two problems in one framework, we propose a U nified P rompt-based C ross- M odal Frame work (UPCM-Frame) to bridge the gap between the text modality and image modality with CLIP and StyleGAN, which are two large-scale pre-trained models. The proposed framework is combined with two main modules: a Text Embedding-to-Image Embedding projection module based on a special prompt embedding pair, and a projection module mapping Image Embeddings to semantically aligned StyleGAN Embeddings which can be used in both image generation and manipulation. The proposed framework is able to handle complicated descriptions and generate impressive results with high quality due to the utilization of large-scale pre-trained models. In order to demonstrate the effectiveness of the proposed method in the two tasks, we conduct experiments to evaluate the results of our method both quantitatively and qualitatively.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4402760804",
    "type": "article"
  },
  {
    "title": "Scalable Frame-based Construction of Sociocultural NormBases for Socially-Aware Dialogues",
    "doi": "https://doi.org/10.1145/3697838",
    "publication_date": "2024-10-04",
    "publication_year": 2024,
    "authors": "Shilin Qu; Weiqing Wang; Xin Zhou; Haolan Zhan; Zhuang Li; Lizhen Qu; Linhao Luo; Yuan-Fang Li; Gholamreza Haffari",
    "corresponding_authors": "",
    "abstract": "Sociocultural norms serve as guiding principles for personal conduct in social interactions, emphasizing respect, cooperation, and appropriate behavior, which is able to benefit tasks including conversational information retrieval, contextual information retrieval and retrieval-enhanced machine learning. We propose a scalable approach for constructing a Sociocultural Norm ( Scn ) Base using Large Language Models (LLMs) for socially aware dialogues. We construct a comprehensive and publicly accessible Chinese Sociocultural NormBase ( ChineseNormBase ). Our approach utilizes socially-aware dialogues, enriched with contextual frames, as the primary data source to constrain the generating process and reduce the hallucinations. This enables extracting of high-quality and nuanced natural-language norm statements, leveraging the pragmatic implications of utterances with respect to the situation. As real dialogue annotated with gold frames are not readily available, we propose using synthetic data. Our empirical results show: (i) the quality of the Scn s derived from synthetic data is comparable to that from real dialogues annotated with gold frames, and (ii) the quality of the Scn s extracted from real data, annotated with either silver (predicted) or gold frames, surpasses that without the frame annotations. We further show the effectiveness of the extracted Scn s in a RAG-based (Retrieval-Augmented Generation) model to reason about multiple downstream dialogue tasks.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403131948",
    "type": "article"
  },
  {
    "title": "ViCoFace: Learning Disentangled Latent Motion Representations for Visual-Consistent Face Reenactment",
    "doi": "https://doi.org/10.1145/3698769",
    "publication_date": "2024-10-04",
    "publication_year": 2024,
    "authors": "Jun Ling; Han Xue; Anni Tang; Rong Xie; Li Song",
    "corresponding_authors": "",
    "abstract": "Unsupervised face reenactment aims to animate a source image to imitate the motions of a target image while retaining the source portrait’s attributes like facial geometry, identity, hair texture, and background. While prior methods can extract the motion from the target image via compact representations (e.g., key-points or latent motion bases [50]), they are not robust in predicting motions that are disentangled with portrait attributes, thus failing to preserve portrait attributes in the cross-subject reenactment. In this work, we propose an effective and cost-efficient face reenactment approach to address this issue. Our approach is highlighted by two major strengths. First, based on the theory of latent-motion bases, we disentangle the full-head motion into two parts: the transferable motion and preservable motion and then compose the full motion representation using latent motions from the source image and the target image. Second, to optimize and learn disentangled motions, we introduce an efficient training framework, which features two training strategies 1) a mixture training strategy that encompasses self-reenactment training and cross-subject training for better motion disentanglement; and 2) a multi-path training strategy that improves the visual consistency of portrait attributes. Extensive experiments on widely used benchmarks demonstrate that our method exhibits a remarkable generalization ability compared to state-of-the-art baselines. Project and demos are available at https://junleen.github.io/projects/vicoface .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403132043",
    "type": "article"
  },
  {
    "title": "Correlation-aware Cross-modal Attention Network for Fashion Compatibility Modeling in UGC Systems",
    "doi": "https://doi.org/10.1145/3698772",
    "publication_date": "2024-10-05",
    "publication_year": 2024,
    "authors": "Kai Cui; Shenghao Liu; Wei Feng; Xianjun Deng; Liangbin Gao; Minmin Cheng; Hongwei Lu; Laurence T. Yang",
    "corresponding_authors": "",
    "abstract": "Empowered by the continuous integration of social multimedia and artificial intelligence, the application scenarios of information retrieval (IR) progressively tend to be diversified and personalized. Currently, User-Generated Content (UGC) systems have great potential to handle the interactions between large-scale users and massive media contents. As an emerging multimedia IR, Fashion Compatibility Modeling (FCM) aims to predict the matching degree of each given outfit and provide complementary item recommendation for user queries. Although existing studies attempt to explore the FCM task from a multimodal perspective with promising progress, they still fail to fully leverage the interactions between multimodal information or ignore the item-item contextual connectivities of intra-outfit. In this paper, a novel fashion compatibility modeling scheme is proposed based on Correlation-aware Cross-modal Attention Network. To better tackle these issues, our work mainly focuses on enhancing comprehensive multimodal representations of fashion items by integrating the cross-modal collaborative contents and uncovering the contextual correlations. Since the multimodal information of fashion items can deliver various semantic clues from multiple aspects, a modality-driven collaborative learning module is presented to explicitly model the interactions of modal consistency and complementarity via a co-attention mechanism. Considering the rich connections among numerous items in each outfit as contextual cues, a correlation-aware information aggregation module is further designed to adaptively capture significant intra-correlations of item-item for characterizing the content-aware outfit representations. Experiments conducted on two real-world fashion datasets demonstrate the superiority of our approach over state-of-the-art methods.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403155096",
    "type": "article"
  },
  {
    "title": "Contrastive Learning based Speech Spoofing Detection for Multimedia Security in Edge Intelligence",
    "doi": "https://doi.org/10.1145/3698773",
    "publication_date": "2024-10-07",
    "publication_year": 2024,
    "authors": "Jiaqi Sun; Xianjun Deng; Shenghao Liu; Xiaoxuan Fan; Yongling Huang; Yuanyuan He; Celimuge Wu; Jong Hyuk Park",
    "corresponding_authors": "",
    "abstract": "Artificial intelligence (AI) empowered edge computing has given rise to a new paradigm and effectively facilitated the promotion and development of multimedia applications. The speech assistant is one of the significant services provided by multimedia applications, which aims to offer intelligent interactive experiences between humans and machines. However, malicious attackers may exploit spoofed speeches to deceive speech assistants, posing great challenges to the security of multimedia applications. The limited resources of multimedia terminal devices hinder their ability to effectively load speech spoofing detection models. Furthermore, processing and analyzing speech in the cloud can result in poor real-time performance and potential privacy risks. Existing speech spoofing detection methods rely heavily on annotated data and exhibit poor generalization capabilities for unseen spoofed speeches. To address these challenges, this paper first proposes the Coordinate Attention Network (CA2Net) that consists of coordinate attention blocks and Res2Net blocks. CA2Net can simultaneously extract temporal and spectral speech feature information and represent multi-scale speech features at a granularity level. Besides, a contrastive learning-based speech spoofing detection framework named GEMINI is proposed. GEMINI can be effectively deployed on edge nodes and autonomously learn speech features with strong generalization capabilities. GEMINI first performs data augmentation on speech signals and extracts conventional acoustic features to enhance the feature robustness. Subsequently, GEMINI utilizes the proposed CA2Net to further explore the discriminative speech features. Then, a tensor-based multi-attention comparison model is employed to maximize the consistency between speech contexts. GEMINI continuously updates CA2Net with contrastive learning, which enables CA2Net to effectively represent speech signals and accurately detect spoofed speeches. Extensive experiments on the ASVspoof2019 dataset show that GEMINI reduces the Equal Error Rate and tandem Detection Cost Function by up to 96.75% and 96.35% in the physical access scenario, and by up to 86.62% and 87.71% in the logical access scenario compared to peer methods.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403184910",
    "type": "article"
  },
  {
    "title": "Dual-path Imbalanced Feature Compensation Network for Visible-Infrared Person Re-identification",
    "doi": "https://doi.org/10.1145/3700135",
    "publication_date": "2024-10-11",
    "publication_year": 2024,
    "authors": "Xu Cheng; Zichun Wang; Yan Jiang; Xingyu Liu; Hao Yu; Jingang Shi; Zitong Yu",
    "corresponding_authors": "",
    "abstract": "Visible-Infrared Person Re-identification (VI-ReID) presents significant challenges on account of the substantial cross-modality gap and intra-class variations. Most existing methods primarily concentrate on aligning cross-modality at the feature or image levels and training with an equal number of samples from different modalities. However, in the real world, there exists an issue of modality imbalance between visible and infrared data. Besides, imbalanced samples between train and test impact the robustness and generalization of the VI-ReID. To alleviate this problem, we propose a dual-path imbalanced feature compensation network (DICNet) for VI-ReID, which provides equal opportunities for each modality to learn inconsistent information from different identities of others, enhancing identity discrimination performance and generalization. First, a modality consistency perception (MCP) module is designed to assist the backbone focus on spatial and channel information, extracting diverse and salient features to enhance feature representation. Second, we propose a cross-modality features reassignment strategy to simulate modality imbalance by grouping and reorganizing the cross-modality features. Third, we perform bidirectional heterogeneous cooperative compensation with cross-modality imbalanced feature interaction modules (CIFIMs), allowing our network to explore the identity-aware patterns from imbalanced features of multiple groups for cross-modality interaction and fusion. Further, we design a feature reconstruction difference loss to reduce cross-modality discrepancy and enrich feature diversity within each modality. Extensive experiments on three mainstream datasets show the superiority of the DICNet. Additionally, competitive results in corrupted scenarios verify its generalization and robustness.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403319676",
    "type": "article"
  },
  {
    "title": "Multimodal Consistency Suppression Factor for Fake News Detection",
    "doi": "https://doi.org/10.1145/3699959",
    "publication_date": "2024-10-14",
    "publication_year": 2024,
    "authors": "Zhulin Tao; Runze Zhao; Xin Shi; Xingyu Gao; Xi Wang; Xianglin Huang",
    "corresponding_authors": "",
    "abstract": "Recent multi-modal fake news detection methods often use the consistency between textual and visual contents to determine the truth or fake of a news information. Higher levels of textual-visual consistency typically lead to a greater likelihood of classifying a news item as real. However, a critical observation reveals that creators of most fake news intentionally select images that align with the textual content, thereby enhancing the credibility of the news. Consequently, high consistency between textual and visual contents alone cannot guarantee the authenticity of the information. To address this problem, we introduce a novel approach termed Multimodal Consistency-based Suppression Factor to modulate the significance of textual-visual consistency in information assessment. When the textual-visual matching is high, this suppression factor reduces the influence of consistency during the judgment process. Moreover, we use Contrastive Language-Image Pre-training (CLIP) model to extract features and measure the consistency level between modalities to guide multimodal fusion. In addition, we also use a method of compressing and fusing modal information based on Variational Autoencoder (VAE) to reconstruct CLIP features, learning the shared representation of different modal information of CLIP. Finally, extensive experiments were conducted on three publicly datasets, Weibo, Twitter and Weibo21, and the results confirmed that our method outperformed the state-of-the-art methods in the field, and had 0.8%, 2.6% and 4.1% effect improvement on the accuracy rate.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403381080",
    "type": "article"
  },
  {
    "title": "Temporal-enhanced Radar and Camera Fusion for Object Detection",
    "doi": "https://doi.org/10.1145/3700442",
    "publication_date": "2024-10-14",
    "publication_year": 2024,
    "authors": "Linhua Kong; Yiming Wang; Dongxia Chang; Yao Zhao",
    "corresponding_authors": "",
    "abstract": "Recently, object detection methods based on multimodal fusion have gained widespread adoption in autonomous driving, proving to be valuable for detecting objects in dynamic environments. Among them, millimetre wave (mmWave) radar is commonly utilized as an effective complement to cameras, as it is almost unaffected by harsh weather conditions. However, current approaches that fuse mmWave radar and camera often overlook the correlation between the two modalities, failing to fully exploit their complementary features. To address this, we propose a temporal-enhanced radar and camera fusion network to explore the correlation between these two modalities and learn a comprehensive representation for object detection. In our model, a temporal fusion model is introduced to fuse mmWave radar features from different moments, thus mitigating the problem of mmWave radar point-object mismatch due to object movement. Moreover, a new correlation-based fusion strategy using the dedicated mask cross attention is proposed to fuse mmWave radar and vision features more effectively. Finally, we design a gate feature pyramid network that selects shallow texture information based on deep semantic information to obtain more representative features. The experimental results on the nuScenes benchmark demonstrate the effectiveness of our proposed method.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403395898",
    "type": "article"
  },
  {
    "title": "AMVFNet: Attentive Multi-View Fusion Network for 3D Object Detection",
    "doi": "https://doi.org/10.1145/3689639",
    "publication_date": "2024-10-15",
    "publication_year": 2024,
    "authors": "Yuxiao Huang; Zhicong Huang; Jingwen Zhao; Haifeng Hu; Dihu Chen",
    "corresponding_authors": "",
    "abstract": "Pillar-based method is significant in the field of LiDAR-based 3D object detection which could directly make use of efficient 2D backbone and save computational resources during reference. Existing methods usually sequentially project the original point clouds into the cylindrical view or the bird-eye view for feature extraction. However, the former suffers from obscured problems and the scales of instances vary greatly with distance, while the latter leads to considerable confusion problems due to the loss of semantic information caused by the sparsity of the projected point cloud. In this paper, we present a novel and efficient two-stage point-pillar hybrid architecture named Attentive Multi-View Fusion Network (AMVFNet), in which we abstract features from all cylindrical view, bird-eye view, and raw point clouds. Rather than designing more complex modules to solve the problems inherent in the single-view approach, our multi-view fusion architecture effectively combines the strengths of multiple perspectives to improve performance at a more fundamental level. Besides, to compensate for quantization distortion caused by projection operations, we propose attentive feature enhancement layers to further improve the capability of contextual information capturing. Extensive experiments on the KITTI detection benchmark illustrate that our proposed AMVFNet achieves competitive performance compared with other SOTA 3D object detectors.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403428104",
    "type": "article"
  },
  {
    "title": "Optimal Illumination Distance Metrics for Person Re-identification in Complex Lighting Conditions",
    "doi": "https://doi.org/10.1145/3700771",
    "publication_date": "2024-10-15",
    "publication_year": 2024,
    "authors": "Chao Wang; Zhongyuan Wang; Ruimin Hu; Xiaochen Wang; Wen Zhou",
    "corresponding_authors": "",
    "abstract": "Person Re-identification is extensively applied in public security and surveillance. However, environmental factors like time and location often lead to varying lighting conditions in captured pedestrian images, significantly impacting identification accuracy. Current approaches mitigate this issue through lighting transformation techniques, aiming to normalize images to a standard lighting condition for consistent person re-identification results. Yet, these methods overlook the fact that different content may hold distinct identification values under diverse lighting conditions. To address this, we conducted an analysis on the identification distance between images of the same or different pedestrians under predefined lighting conditions. From this analysis, we introduce the concept of optimal lighting: a condition where the distance between image pairs is minimized compared to other lighting scenarios. We propose utilizing this optimal lighting distance in the image retrieval process for final ranking. Our study, validated on synthetic datasets Market-IA and Duke-IA, demonstrates that optimal lighting is independent of image texture information. Each image pair exhibits a unique optimal lighting, yet consistently shows a minimum distance value.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403428165",
    "type": "article"
  },
  {
    "title": "Text-and-Image Learning Transformer for Cross-modal Person Re-identification",
    "doi": "https://doi.org/10.1145/3686160",
    "publication_date": "2024-10-15",
    "publication_year": 2024,
    "authors": "Tinghui Wu; Shuhe Zhang; Dihu Chen; Haifeng Hu",
    "corresponding_authors": "",
    "abstract": "Text-based person re-identification aims to find the target person from a large pedestrian gallery with the given natural language description. Previous works mainly focus on embedding salient textual and visual representations in a common latent space by utilizing the dual-path structure or parameter-shared network. However, they still lack the ability to effectively extract fine-grained unimodal features as well as fuse the cross-modal data, leading to the increase of misaligned cases. To settle these issues, we propose a text-and-image implicit learning Transformer (TILT) to eliminate textual anisotropy and enhance the cross-modal alignment from both domains based on the bi-direction multimodal encoders. Specifically, we apply the pre-trained multimodal embedding module to overcome the unimodal anisotropy problem with contrastive learning, and map fine-grained features with dual encoder in bidirectional masking. Then, we design the cross-modal interaction encoder to comprehensively mine implicit cross-modal relations by reconstructing masked tokens, and fuse rich multi-modal knowledge in a common space. In addition, the cross-modal similarity matching module is proposed to optimize the intra-domain classification and decrease the inter-domain divergence. Extensive experiments are conducted on three public benchmarks CUHK-PEDES, ICFG-PEDES and RSTPReid to verify the effectiveness of our proposed framework. And results prove that our model outperforms state-of-the-art methods on all metrics.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403428260",
    "type": "article"
  },
  {
    "title": "ExpAvatar: High-Fidelity Avatar Generation of Unseen Expressions with 3D Face Priors",
    "doi": "https://doi.org/10.1145/3700770",
    "publication_date": "2024-10-15",
    "publication_year": 2024,
    "authors": "Yuan Gan; Ruijie Quan; Yawei Luo",
    "corresponding_authors": "",
    "abstract": "The reconstruction of dynamic head avatars has gained increasing significance, giving rise to various downstream applications such as visual dubbing and digital human creation. Despite recent advancements, generating novel, unseen expressions for a given identity remains challenging in concurrently achieving 1) accurate expression and consistent appearance and 2) high-quality and realistic faces. This paper introduces ExpAvatar, a novel approach crafted to address these challenges. ExpAvatar elaborately leverages the appearance consistency capabilities inherent in 3DMMs-based models along with the robust generalization ability of DDPMs-based models to alleviate appearance drift issues and enhance the generation of unseen expressions. Specifically, ExpAvatar introduces a Face Priors-conditioned Diffusion model (FPDiff) to inject 3D face priors into generation models through fine-tuning. Furthermore, a Face Priors-conditioned Catalyst (FPCatalyst) is employed to enhance the inference efficiency and generation quality. Moreover, we propose a unique confidence-based regularizer function to mitigate the effect of imperfect face-tracking estimates, thereby improving the quality of dynamic neural head avatars. Experimental results demonstrate that ExpAvatar surpasses current state-of-the-art solutions in generating unseen expressions, marking an advancement in the realm of dynamic head avatar synthesis. Code: https://github.com/yuangan/ExpAvatar .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403428274",
    "type": "article"
  },
  {
    "title": "KeYric: Unsupervised Keywords Extraction and Expansion from Music for Coherent Lyrics Generation",
    "doi": "https://doi.org/10.1145/3699717",
    "publication_date": "2024-10-17",
    "publication_year": 2024,
    "authors": "Xichu Ma; V.S. R. K. Sharma; Min‐Yen Kan; Wee Sun Lee; Ye Wang",
    "corresponding_authors": "",
    "abstract": "We address the challenge of enhancing coherence in generated lyrics from symbolic music, particularly for creating singing-based language learning materials. Coherence, defined as the quality of being logical and consistent, forming a unified whole, is crucial for lyrics at multiple levels—word, sentence, and full-text. Additionally, it involves lyrics’ musicality—matching of style and sentiment of the music. To tackle this, we introduce KeYric, a novel system that leverages keyword skeletons to strengthen both coherence and musicality in lyrics generation. KeYric employs an innovative approach with an unsupervised keyword skeleton extractor and a graph-based skeleton expander, designed to produce a style-appropriate keyword skeleton from input music. This framework integrates the skeleton with the input music via a three-layer coherence mechanism, significantly enhancing lyric coherence by 5% in objective evaluations. Subjective assessments confirm that KeYric-generated lyrics are perceived as 19% more coherent and suitable for language learning through singing compared to existing models. Our analyses indicate that integrating genre-relevant elements, such as pitch, into music encoding is crucial, as musical genres significantly affect lyric coherence.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403491013",
    "type": "article"
  },
  {
    "title": "CTRNet++: Dual-path Learning with Local-global Context Modeling for Scene Text Removal",
    "doi": "https://doi.org/10.1145/3697837",
    "publication_date": "2024-10-24",
    "publication_year": 2024,
    "authors": "Chongyu Liu; Dezhi Peng; Yuliang Liu; Lianwen Jin",
    "corresponding_authors": "",
    "abstract": "Recent advances in scene text removal have attracted growing research interest due to its applications on privacy protection, document restoration, and text editing. While deep learning and generative adversarial network have shown significant progress, existing methods often struggle to generate consistent and plausible textures when erasing texts on complex backgrounds. To address this challenge, we propose a Contextual-guided Text Removal Network (CTRNet). CTRNet utilizes Low-level/High-level Contextual Guidance blocks (LCG, HCG) to explore both low-level structure and high-level discriminative context features from existing data to guide the text erasure and background restoration process. We further extend CTRNet to CTRNet++ by incorporate an Auto-Encoder architecture as a novel and effective HCG block, which serves as an additional image inpainting branch, providing more accurate texture and context clues with the assiatance of a large volume of natural images. Then we introduce a Context Embedding and Content Feature Modeling (CECFM) block that combines Depth-wise CNN and Transformer layers to capture local features and establish long-term relationships among pixels globally. In addition, an efficient Progressive Feature Fusion Module (PFFM) is proposed to fully utilize multi-scale features from different branches. Experiments on benchmark datasets, SCUT-EnsText and SCUT-Syn, demonstrate that CTRNet++ significantly outperforms existing state-of-the-art methods and exhibits a stronger ability for complex background reconstruction. The code is available at https://github.com/lcy0604/CTRNet-plus .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403729448",
    "type": "article"
  },
  {
    "title": "Discard Significant Bits of Compressed Sensing: A Robust Image Coding for Resource-Limited Contexts",
    "doi": "https://doi.org/10.1145/3701732",
    "publication_date": "2024-10-24",
    "publication_year": 2024,
    "authors": "Zan Chen; Tao Wang; Jun Li; Wenlong Guo; Yuanjing Feng; Xueming Qian; Xingsong Hou",
    "corresponding_authors": "",
    "abstract": "Compressive sampling (CS) provides a robust and simple framework for compressing images in resource-constrained environments. However, CS-based image coding schemes often have poor rate-distortion (R-D) performance, particularly due to the quantization process. Our research indicates that leveraging the image prior enables the estimation of most significant bits (MSBs) from least significant bits (LSBs), which provides a quantization strategy to improve R-D performance without increasing coding complexity. That is discarding MSBs of measurements, and only transmitting LSBs to the decoder side. At the decoder side, we reconstruct images by solving an inverse-quantization set-constrained CS optimization problem. Our approach further employs a tailored designed deep denoiser as the proximal operator to enhance the reconstructed image quality.Extensive experimental results demonstrate that the proposed scheme achieves satisfactory performance, with promising R-D results (PSNR gains over 1.71 dB than JPEG at 0.50 bpp compression ratio), and robust bit error and loss resilience (reconstructed 29.98 dB even with 50% bit loss at 0.50 bpp compression ratio), meanwhile having lower encoding complexity (less than half encoding time of CCSDS-IDC).",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403729663",
    "type": "article"
  },
  {
    "title": "A Clustering Approach to Unveil User Similarities in 6-DoF Extended Reality Applications",
    "doi": "https://doi.org/10.1145/3701734",
    "publication_date": "2024-10-25",
    "publication_year": 2024,
    "authors": "Silvia Rossi; Irene Viola; Laura Toni; Pablo César",
    "corresponding_authors": "",
    "abstract": "The advent in our daily life of Extended Reality (XR) technologies, such as Virtual and Augmented Reality, has led to the rise of user-centric systems, offering higher level of interaction and presence in virtual environments. In this context, understanding the actual interactivity of users is still an open challenge and a key step to enabling user-centric system. In this work, our goal is to construct an efficient clustering tool for 6 Degree-of-Freedom (DoF) navigation trajectories by extending the applicability of existing behavioural tool. Specifically, we first compare the navigation in 6-DoF with its 3-DoF counterpart, highlighting the main differences and novelties. Then, we investigate new metrics aimed at better modelling behavioural similarities between users in a 6-DoF system. More concretely, we define and compare 11 similarity metrics which are based on different distance features ( i.e. , user positions in the 3D space, user viewing directions) and distance measurements ( i.e. , Euclidean, Geodesic, angular distance). Our solutions are validated and tested on real navigation paths of users interacting with dynamic volumetric media in both 6-DoF Virtual Reality and Augmented Reality conditions. Results show that metrics based on both user position and viewing direction better perform in detecting user similarity while navigating in a 6-DoF system. Such easy-to-use but robust metrics allow us to answer a fundamental question for user-centric systems: “how do we detect if users look at the same content in 6-DoF?”, opening the gate to new solutions based on users interactivity, such as viewport prediction, live streaming services optimised based on users behaviour but also for user-based quality assessment methods.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403763692",
    "type": "article"
  },
  {
    "title": "WTDUN: Wavelet Tree-Structured Sampling and Deep Unfolding Network for Image Compressed Sensing",
    "doi": "https://doi.org/10.1145/3701731",
    "publication_date": "2024-10-26",
    "publication_year": 2024,
    "authors": "Kai Han; Jin Wang; Yunhui Shi; HanQin Cai; Nam Ling; Baocai Yin",
    "corresponding_authors": "",
    "abstract": "Deep unfolding networks have gained increasing attention in the field of compressed sensing (CS) owing to their theoretical interpretability and superior reconstruction performance. However, most existing deep unfolding methods often face the following issues: 1) they learn directly from single-channel images, leading to a simple feature representation that does not fully capture complex features; and 2) they treat various image components uniformly, ignoring the characteristics of different components. To address these issues, we propose a novel wavelet-domain deep unfolding framework named WTDUN, which operates directly on the multi-scale wavelet subbands. Our method utilizes the intrinsic sparsity and multi-scale structure of wavelet coefficients to achieve a tree-structured sampling and reconstruction, effectively capturing and highlighting the most important features within images. Specifically, the design of tree-structured reconstruction aims to capture the inter-dependencies among the multi-scale subbands, enabling the identification of both fine and coarse features, which can lead to a marked improvement in reconstruction quality. Furthermore, a wavelet domain adaptive sampling method is proposed to greatly improve the sampling capability, which is realized by assigning measurements to each wavelet subband based on its importance. Unlike pure deep learning methods that treat all components uniformly, our method introduces a targeted focus on important subbands, considering their energy and sparsity. This targeted strategy lets us capture key information more efficiently while discarding less important information, resulting in a more effective and detailed reconstruction. Extensive experimental results on various datasets validate the superior performance of our proposed method.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403784164",
    "type": "article"
  },
  {
    "title": "Reference-based In-loop Filter with Robust Neural Feature Transfer for Video Coding",
    "doi": "https://doi.org/10.1145/3702643",
    "publication_date": "2024-11-01",
    "publication_year": 2024,
    "authors": "Nayoung Kim; Jung-Kyung Lee; Je‐Won Kang",
    "corresponding_authors": "",
    "abstract": "In this paper, we propose an efficient reference-based deep in-loop filtering method for video coding. Existing reference-based in-loop filters often face challenges in improving coding efficiency due to the difficulty in capturing relevant textures from the reference frames. Our method accurately predicts the texture of a reference block and uses this information to restore the current block. To achieve this, we develop a reference-to-current feature estimation module that conveys high-quality information from previously coded frames in the feature domain, thereby preventing loss of detail due to inaccurate prediction. Although a neural network is trained to restore a coded video frame to be similar to the current frame, their performance can significantly degrade when operating with various quantization parameters (QPs) and managing different levels of distortion. This problem becomes further severe in the reference-to-current feature estimation, in which QP values are applied differently to video frames. We address this problem by developing a QP-aware convolution layer with a small number of learnable parameters to generate reliable features and adapt to fine-grained adaptive QPs among consecutive frames. The proposed method is implemented into the Versatile Video Coding (VVC) reference software, VTM version 10.0. Experimental results demonstrate that the proposed method improves coding performance significantly in VVC.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403986556",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on Security and Privacy of Avatar in Metaverse",
    "doi": "https://doi.org/10.1145/3702485",
    "publication_date": "2024-11-02",
    "publication_year": 2024,
    "authors": "Yushu Zhang; William Puech; Anderson Rocha; Rongxing Lu; Stefano Cresci; Roberto Di Pietro",
    "corresponding_authors": "",
    "abstract": "The metaverse overcomes the time and space constraints of the real world and expands the physical world based on the Internet of Things, virtual reality, and digital twins. For this reason, interest in metaverse applications has recently increased because ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403997011",
    "type": "article"
  },
  {
    "title": "Interactive Garment Recommendation with User in the Loop",
    "doi": "https://doi.org/10.1145/3702327",
    "publication_date": "2024-11-02",
    "publication_year": 2024,
    "authors": "Federico Becattini; Xiaolin Chen; Andrea Puccia; Haokun Wen; Xuemeng Song; Liqiang Nie; Alberto Del Bimbo",
    "corresponding_authors": "",
    "abstract": "Recommending fashion items often leverages rich user profiles and makes targeted suggestions based on past history and previous purchases. In this paper, we work under the assumption that no prior knowledge is given about a user. We propose to build a user profile on the fly by integrating user reactions as we recommend complementary items to compose an outfit. We present a reinforcement learning agent capable of suggesting appropriate garments and ingesting user feedback so to improve its recommendations and maximize user satisfaction. To train such a model, we resort to a proxy model to be able to simulate having user feedback in the training loop. We experiment on the IQON3000 fashion dataset and we find that a reinforcement learning-based agent becomes capable of improving its recommendations by taking into account personal preferences. Furthermore, such task demonstrated to be hard for non-reinforcement models, that cannot exploit exploration during training.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403997018",
    "type": "article"
  },
  {
    "title": "Towards Oriented Fisheye Object Detection: Dataset and Baseline",
    "doi": "https://doi.org/10.1145/3702640",
    "publication_date": "2024-11-02",
    "publication_year": 2024,
    "authors": "Jialin Yang; Chunyu Lin; Lang Nie; Zisen Kong; Jiapeng Wang; Yao Zhao",
    "corresponding_authors": "",
    "abstract": "Fisheye object detection is challenging due to the fisheye distortion, which inclines objects to different extents and pushes extensive irrelevant pixels into the predicted horizontal bounding box (HBB). To address the problems above, we establish a new fisheye object detection dataset (named FishOBB) with compact oriented bounding box (OBB) annotations, as well as an OBB-customized mosaic augmentation technology. To our knowledge, there are very few fisheye datasets labeled by OBB, especially the open-source forword view dataset like ours. Besides, we provide a fisheye object detection baseline (named FDA-YOLO) with two fisheye adaption units. Concretely, we first design a distortion orientation aggregation (DOA) unit guided by polar sampling to capture distortion-aware fisheye features. On the other hand, to transfer HBB-based detection models to OBB-based counterparts, we propose an oriented anchor attention unit. It automatically weights the unbalanced positive/negative samples and facilitates convergence for multi-anchor models. Finally, we demonstrate that the two adaption units can be easily integrated into various anchor-based YOLO methods, e.g., ScaledYOLOv4 and YOLOv7, contributing to superior performance to existing state-of-the-art (SoTA) solutions in the proposed dataset. Meanwhile, our method has also achieved SoTA performance on other popular datasets like WEPDTOF. The dataset and code are released at https://github.com/lukanightfever/FishOBB .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403997026",
    "type": "article"
  },
  {
    "title": "Diversity-Representativeness Replay and Knowledge Alignment for Lifelong Vehicle Re-identification",
    "doi": "https://doi.org/10.1145/3702998",
    "publication_date": "2024-11-05",
    "publication_year": 2024,
    "authors": "Anqi Cao; Zhijing Wan; Xiao Wang; Wei Liu; Wei Wang; Zheng Wang; Xin Xu",
    "corresponding_authors": "",
    "abstract": "Lifelong vehicle re-identification (LVReID) aims to match a target vehicle across multiple cameras, considering non-stationary and continuous data streams, which fits the needs of the practical application better than traditional vehicle re-identification. Nonetheless, this area has received relatively little attention. Recently, methods for lifelong person re-identification (LPReID) have been emerging, with replay-based methods achieving the best results by storing a small number of instances from previous tasks for retraining, thus effectively reducing catastrophic forgetting. However, these methods cannot be directly applied to LVReID because they fail to simultaneously consider the diversity and representativeness of replayed data, resulting in biases between the subset stored in the memory buffer and the original data. They randomly sample classes, which may not adequately represent the distribution of the original data. Additionally, these methods fail to consider the rich variation in instances of the same vehicle class due to factors such as vehicle orientation and lighting conditions. Therefore, preserving more informative classes and instances for replay helps maintain information from previous tasks and may mitigate the model’s forgetting of old knowledge. In view of this, we propose a novel Diversity-Representativeness Dual-Stage Sampling Replay (DDSR) strategy for LVReID that constructs an effective memory buffer through two stages, i.e. , Cluster-Centric Class Selection and Diverse Instance Mining. Specifically, we first perform class-level sampling based on density in the clustered class-centered feature space and then further mine the diverse, high-quality instances within the selected classes. In addition, we introduce Maximum Mean Discrepancy loss to align the feature distribution between replay data and the new arrivals and apply L2 regularisation in the parameter space to facilitate knowledge transfer, thus enhancing the model’s generalization ability to new tasks. Extensive experiments demonstrate effective improvements of our method compared to current state-of-the-art lifelong ReID methods on the VeRi-776, VehicleID, and VERI-Wild datasets.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404059286",
    "type": "article"
  },
  {
    "title": "Joint Mixing Data Augmentation for Skeleton-based Action Recognition",
    "doi": "https://doi.org/10.1145/3700878",
    "publication_date": "2024-11-05",
    "publication_year": 2024,
    "authors": "Linhua Xiang; Zengfu Wang",
    "corresponding_authors": "",
    "abstract": "Skeleton-based action recognition is beneficial for understanding human behavior in videos, and thus has received much attention in recent years as an important research area in action recognition. Current research focuses on designing more advanced algorithms to better extract spatio-temporal information from skeleton data. However, due to the small amount of data in the existing skeleton dataset and the lack of effective data augmentation methods, it is easy to lead to overfitting in model training. To address this challenge, we propose a mix-based data augmentation method, Joint Mixing Data Augmentation (JMDA), which can generally improve the effectiveness and robustness of various skeleton-based action recognition algorithms. In terms of spatial information, we introduce SpatialMix (SM), a method that projects the original 3D skeleton discrete information into a 2D space. Then, SM mixes the projected spatial information between two random samples during the training process to achieve the spatial-based mixing data augmentation. Concerning temporal information, we propose TemporalMix (TM). Leveraging the temporal continuity in skeleton data, we perform a temporal resize operation on the original skeleton data, and then merge two random samples during training to achieve the temporal-based mixed data augmentation. Additionally, we analyze the Feature Mismatch (FM) problem caused by introducing mix-based data augmentation into skeleton data. Then we propose a new data preprocessing method called Feature Alignment (FA) to effectively address this problem and improve model performance. Moreover, we propose a novel training pipeline, Joint Training Strategy (JTS), which combines multiple mix-based data augmentation methods for further improvement of model performance. Specifically, our proposed JMDA is plug-and-play and widely applicable to skeleton-based action recognition models. At the same time, the application of JMDA does not increase the model parameters and there is almost no additional training cost. We conduct extensive experiments on NTU RGB+D 60 and NTU RGB+D 120 datasets to demonstrate the effectiveness and robustness of the proposed JMDA on several mainstream skeleton-based action recognition algorithms.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404059287",
    "type": "article"
  },
  {
    "title": "RGB-D Data Compression via Bi-Directional Cross-Modal Prior Transfer and Enhanced Entropy Modeling",
    "doi": "https://doi.org/10.1145/3702997",
    "publication_date": "2024-11-05",
    "publication_year": 2024,
    "authors": "Yuyu Xu; Pingping Zhang; Minghui Chen; Qiudan Zhang; Wenhui Wu; Yun Zhang; Xu Wang",
    "corresponding_authors": "",
    "abstract": "RGB-D data, being homogeneous cross-modal data, demonstrates significant correlations among data elements. However, current research focuses only on a unidirectional pattern of cross-modal contextual information, neglecting the exploration of bidirectional relationships in the compression field. Thus, we propose a joint RGB-D compression scheme, which is combined with Bi-directional Cross-modal Prior Transfer (Bi-CPT) modules and a Bi-directional Cross-modal Enhanced Entropy (Bi-CEE) model. The Bi-CPT module is designed for compact representations of cross-modal features, effectively eliminating spatial and modality redundancies at different granularity levels. In contrast to the traditional entropy models, our proposed Bi-CEE model not only achieves spatial-channel contextual adaptation through partitioning RGB and depth features but also incorporates information from other modalities as prior to enhance the accuracy of probability estimation for latent variables. Furthermore, this model enables parallel multi-stage processing to accelerate coding. Experimental results demonstrate the superiority of our proposed framework over the current compression scheme, outperforming both rate-distortion performance and downstream tasks, including surface reconstruction and semantic segmentation. The source code will be available at https://github.com/xyy7/Learning-based-RGB-D-Image-Compression .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404059299",
    "type": "article"
  },
  {
    "title": "SAMControl: Controlling Pose and Object for Image Editing with Soft Attention Mask",
    "doi": "https://doi.org/10.1145/3702999",
    "publication_date": "2024-11-05",
    "publication_year": 2024,
    "authors": "Yue Zhang; Chao Wang; Feifei Fang; Yunzhi Zhuge; Hehe Fan; Xiaojun Chang; Cheng Deng; Yi Yang",
    "corresponding_authors": "",
    "abstract": "To achieve content-consistent results in text-conditioned image editing, existing methods typically employ a reconstruction branch to capture the source image details via diffusion inversion and a generation branch to synthesize the target image based on the given textual prompt and the masked source image details. However, accurately segmenting source details is challenging with the current fixed-threshold mask strategy. Additionally, the inadequacies in the inversion process can lead to insufficient retention of source details. In this paper, we propose a method called SAMControl ( S oft A ttention M ask) to adaptively control the pose and object details for image editing. SAMControl dynamically learns flexible attention masks for different images at various diffusion steps. Furthermore, in the reconstruction branch, we utilize a direct inversion technique to ensure the fidelity of source details within SAM. Extensive qualitative and quantitative results demonstrate the effectiveness of the proposed method.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404059342",
    "type": "article"
  },
  {
    "title": "Demonstrative Learning for Human-Agent Knowledge Transfer",
    "doi": "https://doi.org/10.1145/3703838",
    "publication_date": "2024-11-08",
    "publication_year": 2024,
    "authors": "Xiaonuo Dongye; Haiyan Jiang; Dongdong Weng; Zhenliang Zhang",
    "corresponding_authors": "",
    "abstract": "Demonstrative learning in virtual reality (VR) is a pivotal learning strategy for knowledge transfer for embodied agents. While existing studies have extensively explored agents’ knowledge transfer through self-demonstrative learning (SDL) or teacher-demonstrative learning (TDL), there has been limited focus on a system that integrates both paradigms. This paper proposes a comprehensive system that combines the SDL paradigm with the TDL paradigm in VR from a top-down perspective. The system involves using directed probabilistic graphs (DPGs) for knowledge representation, constructing detectors and actuators based on object fluents and atomic actions, representing knowledge acquired from both learning paradigms on the DPGs, and incorporating knowledge integration and visualization. Through system evaluation, we show the advantages of integrating two demonstrative learning paradigms, including increased learning efficiency, mitigating demonstrator's deficiencies, and more logical task execution. The study also reveals that a dynamically decreasing fusion factor with the learning progresses, performs well across different correct percentages of teacher demonstrations. Additionally, we show a more decentralized demonstration in the middle of the agent's learning progress maximizing learning efficiency when demonstrating only a few atomic actions. Finally, we assess the system's generalizability and delineate its boundaries. With the ongoing development and the increasing availability of human data in VR, we anticipate that our system can be applied to future scenarios of efficient human-agent knowledge transfer in human-agent symbiosis.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404184174",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on Realistic Synthetic Data: Generation, Learning, Evaluation",
    "doi": "https://doi.org/10.1145/3703593",
    "publication_date": "2024-11-09",
    "publication_year": 2024,
    "authors": "Bogdan Ionescu; Ioannis Patras; Henning Müller; Alberto Del Bimbo",
    "corresponding_authors": "",
    "abstract": "This is the foreword of our special issue volume on Realistic Synthetic Data: Generation, Learning, Evaluation organized with the ACM Transactions on Multimedia Computing, Communications, and Applications. It presents the target of the special issue that relates to synthetic data for various modalities, e.g., signals, images, volumes, audio, etc., controllable generation for learning from synthetic data, transfer learning and generalization of models, causality in data generation, addressing bias, limitations and trustworthiness in data generation, evaluation measures/protocols and benchmarks to assess quality of synthetic content, open synthetic datasets and software tools, and ethical aspects of synthetic data. The call for papers received a record number of 40 submissions out of which 15 were finally accepted for publication. This introduction provides an overview of the topics of each of the articles.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404198422",
    "type": "article"
  },
  {
    "title": "Person in Uniforms Re-Identification",
    "doi": "https://doi.org/10.1145/3703839",
    "publication_date": "2024-11-11",
    "publication_year": 2024,
    "authors": "Chong-Yang Xiang; Xiao Wu; Jun-Yan He; Zhaoquan Yuan; Tingquan He",
    "corresponding_authors": "",
    "abstract": "Person in Uniforms Re-identification (PU-ReID) is an emerging computer vision task for various intelligent video surveillance applications. PU-ReID is much understudied due to the absence of large-scale annotated datasets, also this task is extremely challenging because many individuals captured in surveillance videos wear same clothing, introducing significant interference for retrieval tasks owing to the high visual similarity of outfits and subtle differences among individuals. This research initiates the exploration of person in uniform re-identification, a novel and challenging task tailored for real industrial scenarios. To address these issues, a novel framework is proposed for PU-ReID, which aims to reduce the visual impact of similar uniforms and learn the unique cues derived from human parts and detailed visual features. Specifically, several novel techniques are built in this study: first, a uniform feature separation method with orthogonal constraints is proposed to extract non-uniform features. Second, multi-view subspace feature alignment is introduced to integrate soft-biometrics including optics-related visual features, contextual information of human parts, and cloth-invariant biometric features. In addition, to close the gap between academic research and real world settings, a new person in uniforms ReID dataset named PU-151 is constructed, which consists of 151 gas station employees in uniforms from 1,488 videos. At last, extensive experiments conducted on five datasets demonstrate that the proposed approach significantly outperforms the state-of-the-art methods. This advancement can drive further developments in re-identification and person search technologies.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404241143",
    "type": "article"
  },
  {
    "title": "SD-Meta: The Software-Defined Network of Human-Centric Metaverse for Multi-Lead or Multi-Media Data in Spread Spectrum Communications",
    "doi": "https://doi.org/10.1145/3703913",
    "publication_date": "2024-11-12",
    "publication_year": 2024,
    "authors": "YePing Zhou; Mingyang Li; Tong Jingze; Linlin li; Zhiwei Yang",
    "corresponding_authors": "",
    "abstract": "In this paper, the novel adjacent two-end link or network multi-end link concept of software-defined network is presented to support the data transmission of electroencephalogram, audio and video streaming in spread spectrum communications. Instead of solving the problem of software-defined prioritization scheme in networking calculation and communication, the southbound-northbound structure of existing neural or information network is found in control and perceptual interactions. The proposed framework of multi-lead and multi-media structures allow the human-centric Metaverse to execute different operations for local and global planning schemes with the high-speed lead or media data transmission of spread spectrum streaming, depending on the kind of brain-computer interaction, and similar to the system of multi-modal perception in the routing or switching. Firstly, this research designed the filed programmable gate array for conventional routing nodes in the computation of different neural networks. Secondly, this study demonstrates its enhanced applicability with software core for multiple routers in the computing power network of the Internet of brains in different brain-computer smart terminals. Experiment results also show the benefits of proposed model and structure, and exposing the different running indicators in the simulation.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404298126",
    "type": "article"
  },
  {
    "title": "Adaptive Prediction Structure for Learned Video Compression",
    "doi": "https://doi.org/10.1145/3703914",
    "publication_date": "2024-11-12",
    "publication_year": 2024,
    "authors": "Jiayu Yang; Yongqi Zhai; Wei Jiang; Chunhui Yang; Feng Gao; Ronggang Wang",
    "corresponding_authors": "",
    "abstract": "Learned video compression has developed rapidly and shown competitive rate-distortion performance compared with the latest traditional video coding standard H.266 (VVC). However, existing works were restricted to fixed prediction direction and GoP size. The inflexibility on prediction structure hinders learned video compression towards optimal compression efficiency in diverse motion scenarios. In this paper, we propose to advance learned video compression with adaptive prediction structure decision. Specifically, we propose a unified compression framework that supports both forward prediction and bi-directional prediction. The framework can flexibly switch to different prediction direction to achieve better prediction performance. Meanwhile, we propose a low-complexity prediction structure decision algorithm, where prediction direction and GoP size are adaptively determined based on motion complexity to achieve optimal compression efficiency. Experimental results demonstrate that the proposed unified framework with adaptive decision algorithm improves compression efficiency of pure forward prediction-based or bi-directional prediction-based framework with neglectable ( \\(0.9\\%\\) ) encoding time increment. Meanwhile, it achieves comparable compression performance with VVC and recent learned video coding methods.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404301003",
    "type": "article"
  },
  {
    "title": "Multi-scale Consistency Deep Lifelong Cross-modal Hashing",
    "doi": "https://doi.org/10.1145/3704636",
    "publication_date": "2024-11-14",
    "publication_year": 2024,
    "authors": "Liming Xu; Hanqi Li; Jie Shao; Xianhua Zeng; Weisheng Li",
    "corresponding_authors": "",
    "abstract": "Deep cross-modal hashing methods provide effective and efficient solutions for large-scale cross-modal retrieval. However, existing cross-modal hashing methods fail to capture the dynamic changes of real-world data, and suffer from serious performance degradation when retrieving streaming data. In this paper, we propose a novel hashing method to achieve accurate cross-modal retrieval under continuous and streaming scenarios. Specifically, regularization-based lifelong learning module is introduced to balance plasticity for learning new knowledge and stability for maintaining old knowledge, and update incremental hash codes without retraining cumulative data. Then, multi-scale consistency network which employs multi-scale feature fusion module to extract fine-grained features among multi-scale modalities is introduced to learn multi-level semantic representations with consistency. Additionally, modality alignment with variational information bottleneck is designed to remove irrelevant information and obtain unified representation, which can be proved to be effective to yield high-quality hash code with new and old knowledge. Extensive experiments show that ours gains the advanced performance and the better adaptability to continuous and streaming environments.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404374447",
    "type": "article"
  },
  {
    "title": "GSyncCode: Geometry Synchronous Hidden Code for One-step Photography Decoding",
    "doi": "https://doi.org/10.1145/3706060",
    "publication_date": "2024-11-27",
    "publication_year": 2024,
    "authors": "Chengxin Zhao; Hefei Ling; Jialie Shen; Han Fang; Yaokun Fang; Sijing Xie; Zongyi Li; Ping Li",
    "corresponding_authors": "",
    "abstract": "Invisible hyperlinks and hidden barcodes have recently emerged as a hot topic in offline-to-online messaging, where an invisible message or barcode is embedded in an image and can be decoded via camera shooting. Current schemes involve a two-step decoding process: starting with vertex localization of the embedded region to correct the perspective distortion introduced by shooting, followed by decoding the message from the corrected region. However, vertex localization can be complex and time-consuming, which affects the efficiency and accuracy of message decoding. To address this issue, this paper proposes a geometry synchronous decoding scheme called GSyncCode, allowing for one-step extraction of a Data Matrix code from the photograph. Instead of correction before decoding, GSyncCode directly decodes a geometry-transformed Data Matrix that is synchronized with the embedded region. A barcode scanner is then used to efficiently retrieve messages. We design a Haar-transform based encoder HaarUNet and a HaarLoss visual function to select the key component of the Data Matrix for embedding. They improve the visual quality of the embedded image by reducing redundant embedding signals. Extensive simulated and real-world experiments demonstrate the superiority of GSyncCode in both decoding efficiency and accuracy. Our codes are published at: https://github.com/zcx-language/GSyncCode.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404767404",
    "type": "article"
  },
  {
    "title": "Implications of Privacy Regulations on Video Surveillance Systems",
    "doi": "https://doi.org/10.1145/3706108",
    "publication_date": "2024-11-28",
    "publication_year": 2024,
    "authors": "Kajal Kansal; Yongkang Wong; Mohan Kankanhalli",
    "corresponding_authors": "",
    "abstract": "Advanced video surveillance systems (VSS), which collect information of every individual who passes through a surveilled area, have become ubiquitous due to its utility for security. However, such proactive monitoring threatens the individual's privacy due to the public's lack of control over personal data. Additionally, individuals or organizations may unethically misuse VSS for other purposes (e.g. individual profiling and unwarranted monitoring). To safeguard individual privacy, various governments have introduced mandatory information privacy regulations (e.g. GDPR, PDPA, and CCPA) to provide extensive guidelines for the purpose of achieving identity confidentiality. Currently, there is a gap between the information privacy regulations and VSS. This paper aims to bridge this gap through four contributions. First, this paper conceptualizes VSS as comprising various data stages based on the idea of data life cycle and studies the implications of existing regulations on VSS. Second, we conducted a survey in ASEAN and European regions to understand the public perception of data risks at each data stage. Third, we review existing privacy-enhancing technologies and its relation to each data stage. Finally, we discuss open research problems in order to realize privacy-aware VSS.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404813608",
    "type": "article"
  },
  {
    "title": "<i>Pr-Ge-Ne</i> : Efficient Encoding of Pervasive Video Sensing Streams by Pruned Generative Networks",
    "doi": "https://doi.org/10.1145/3706109",
    "publication_date": "2024-12-02",
    "publication_year": 2024,
    "authors": "J. L. Wu; Kasun Gamlath; Archan Misra",
    "corresponding_authors": "",
    "abstract": "While video sensing, performed by resource-constrained pervasive devices, is a key enabler of many machine intelligence applications, the high energy and bandwidth overheads of streaming video transmission continue to present formidable deployment challenges. Motivated by the recent advancements in deep learning models, this article proposes the usage of a Generative Network-based technique for resource-efficient streaming video compression and transmission. However, we empirically show that while such generative network-based models offer superior compression gains compared to H.265, additional DNN optimization mechanisms are needed to substantially reduce their encoder complexity. Our proposed optimized system, dubbed Pr-Ge-Ne , adopts a carefully pruned encoder-decoder DNN, on the pervasive device, to efficiently encode a latent vector representation of intra-frame relative motion, and then uses a generator network at the decoder to reconstruct the frames by overlaying such motion information to ”animate” an initial reference frame. By evaluating three representative streaming video datasets, we show that Pr-Ge-Ne achieves around \\(6\\) – \\(10\\) -fold reduction in video transmission rates (with negligible impact on the accuracy of machine perception tasks) compared to H.265, while simultaneously reducing latency and energy overheads on a pervasive device by \\(\\sim\\) 90% and 15–50%, respectively.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404919706",
    "type": "article"
  },
  {
    "title": "Model can be subtle: Two important mechanisms for Social Media Popularity Prediction",
    "doi": "https://doi.org/10.1145/3705319",
    "publication_date": "2024-12-03",
    "publication_year": 2024,
    "authors": "Ning Xu; X Wang; Jing Liu; Lanjun Wang; Xuanya Li; Mengxiao Zhu; Yongdong Zhang; An-An Liu",
    "corresponding_authors": "",
    "abstract": "Social media popularity prediction is an important channel to explore content sharing and communication on social networks. It aims to capture informative cues by analyzing multi-type data (such as user profile, image, and text) to decide the popularity of a specified post. In this paper, we divide social network users into two categories (i.e., active and inactive users) and find a dilemma in existing models: If an active user publishes the low-popularity post, the model will habitually predict the high score. On the contrary, if an inactive user provides the high-popularity post, the model still gives the low score incorrectly. Therefore, how to make the model more subtle to users is important. Comparing to existing methods that directly leverage multi-modal features for regression training, this paper stresses more on two novel mechanisms. The first method aims to prevent the over-fitting on user IDs. We propose the attribute-sensitive interactive mechanism (M1) by incorporating explicit user-attribute and post-attribute interaction. It can analyze which type of features a user cares the most and weaken the model’s dependence on user IDs. The second method aims to strengthen the influence of post content. We propose the knowledge embedding mechanism (M2) to revise the popularity scores in existing models by fusing the statistical frequency over multi-type data. Note that both mechanisms are model-agnostic, which can be applicable in any popularity prediction model. Extensive experiments conducted on the Social Media Prediction Dataset further validate the effectiveness.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404961990",
    "type": "article"
  },
  {
    "title": "Beyond Songs: Analyzing User Sentiment Through Music Playlists and Multimodal Data",
    "doi": "https://doi.org/10.1145/3708346",
    "publication_date": "2024-12-12",
    "publication_year": 2024,
    "authors": "Y. Chen; H.-M. Yuan; Baojun Ma; Limin Wang; Yu Qian",
    "corresponding_authors": "",
    "abstract": "The automatic recognition of user sentiments through their music listening behavior is an important research task in cognitive studies. Whereas prior studies were conducted to identify the sentiment conveyed (or evoked) by a song that a user listens to at a particular time, we argue that a more effective method would be to identify the user’s induced sentiment based on the comprehensive list of songs they have listened to (e.g., the sequence of music being played). However, recognizing the sentiment information induced by a playlist using machine learning techniques is much more challenging than identifying the sentiment induced by a single song, as it is difficult to obtain accurately labeled training samples for playlists. In this study, we developed the List–Song Relationship Factorization (LSRF) model with the objective of efficiently identifying sentiments induced by playlists. This model employs two side information constraints: the sentiment similarity between songs, based on multimodal information, and the co-occurrence of songs in playlists. These constraints enable the simultaneous co-clustering of songs and playlists. The experimental results demonstrate that the proposed model efficiently and consistently identifies sentiment information evoked by either playlists or individual songs.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4405330806",
    "type": "article"
  },
  {
    "title": "Neural Image Compression with Regional Decoding",
    "doi": "https://doi.org/10.1145/3708347",
    "publication_date": "2024-12-13",
    "publication_year": 2024,
    "authors": "Yili Jin; Jiahao Li; Bin Li; Yan Lu",
    "corresponding_authors": "",
    "abstract": "As advancements are made in technology such as AR/VR and high-resolution photography, there is a growing need for a function in image compression named regional decoding . This function lets an image be encoded as a whole, but allows for an arbitrary region to be decoded using only a small part of the bitstream. However, existing neural image compression methods lack support for this crucial functionality. In this paper, we propose a novel approach called the slicing en/decoder , which addresses the need for regional decoding while maintaining performance on par with state-of-the-art methods. Our approach is based on the insight that, during the compression process, local information within pixels holds greater importance than global information. By leveraging this understanding, we divide the image into different bitstreams according to cross-boundary patterns. Consequently, for a selected region, our method can intelligently choose specific portions of the bitstreams to decode only that particular region of interest. Furthermore, we extend the application of our method to 360° image compression, allowing for efficient encoding and decoding of immersive visual content. Moreover, our proposed technique offers the capability to decode regions identically, which paves the way for future advancements in regional video decoding. Our experimental results demonstrate that our method maintains performance on par with state-of-the-art methods while providing the functionality of regional decoding . In conclusion, this paper presents a significant step forward in image compression technology, offering enhanced flexibility and efficiency for emerging applications in digital media.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4405376223",
    "type": "article"
  },
  {
    "title": "EVCS-DAS: Evolving Visual Cryptography Schemes for Dynamic Access Structures",
    "doi": "https://doi.org/10.1145/3708547",
    "publication_date": "2024-12-14",
    "publication_year": 2024,
    "authors": "Xiaotian Wu; Xinjie Feng; Bing Chen; Ching‐Nung Yang; Q. Y. Peng; Weiqi Yan",
    "corresponding_authors": "",
    "abstract": "A systematic investigation of evolving visual cryptography scheme (EVCS) is carried out in this paper. The evolving scheme, denoted as \\((k,\\infty)\\) , differs from the \\((k,n)\\) threshold in that it permits an arbitrary and perhaps unlimited number of participants. More importantly, the access structure can be updated dynamically by adding new users. First of all, a preliminary implementation strategy for the \\((2,\\infty)\\) EVCS is introduced. Then, by employing the \\((2,2)\\) VCS recursively with the \\((2,\\infty)\\) EVCS, a \\((k,\\infty)\\) EVCS is created. In order to enhance the performance, an improved scheme is constructed based on the multi-secret VCS (MVCS) and a series of EVCS schemes with thresholds of \\((1,\\infty)\\) , \\(\\cdots\\) , \\((k-1,\\infty)\\) . Moreover, Boolean XOR operation is adopted for secret recovery to further improve the visual quality. To facilitate the XOR decryption, a novel access structure partition algorithm is presented. Additionally, the proposed partition method can successfully solve the security issue in existing multi-secret XOR-based VCS (MXVCS). By integrating the more secure MXVCS into the improved scheme, XOR decryption is provided. The two proposed methods are shown to be effective and advantageous through extensive experiments and comparisons.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4405397101",
    "type": "article"
  },
  {
    "title": "A Survey on Securing Image-Centric Edge Intelligence",
    "doi": "https://doi.org/10.1145/3700792",
    "publication_date": "2024-12-18",
    "publication_year": 2024,
    "authors": "Tang Li; Haibo Hu; Moncef Gabbouj; Qingqing Ye; Yang Xiang; Jin Li; Lang Li",
    "corresponding_authors": "",
    "abstract": "Facing enormous data generated at the network edge, Edge Intelligence (EI) emerges as the fusion of Edge Computing and Artificial Intelligence, revolutionizing edge data processing and intelligent decision-making. Nonetheless, this emergent mode presents a complex array of security challenges, particularly prominent in image-centric applications due to the sheer volume of visual data and its direct connection to user privacy. These challenges include safeguarding model/image privacy and ensuring model integrity against various security threats, such as model poisoning. Essentially, those threats originate from data attacks, suggesting data protection as a promising solution. Although data protection measures are well-established in other domains, image-centric EI necessitates focused research. This survey examines the security issues inherent to image-centric EI and outlines the protection efforts, providing a comprehensive overview of the landscape. We begin by introducing EI, detailing its operational mechanics and associated security issues. We then explore the technologies facilitating security enhancement (e.g., differential privacy) and edge intelligence (e.g., compact networks and distributed learning frameworks). Next, we categorize security strategies by their application in data preparation, training, and inference, with a focus on image-based contexts. Despite these efforts on security, our investigation identifies research gaps. We also outline promising research directions to bridge these gaps, bolstering security frameworks in image-centric EI applications.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4405541728",
    "type": "article"
  },
  {
    "title": "Scene Adaptive Context Modeling and Balanced Relation Prediction for Scene Graph Generation",
    "doi": "https://doi.org/10.1145/3708350",
    "publication_date": "2024-12-18",
    "publication_year": 2024,
    "authors": "Kai Xu; Lichun Wang; Shuang Li; Tong Gao; Baocai Yin",
    "corresponding_authors": "",
    "abstract": "Scene graph generation (SGG) aims to perceive objects and their relations in images, which can bridge the gap between upstream detection tasks and downstream high-level visual understanding tasks. For SGG models, over-fitting head predicates can lead to bias in the generated scene graph, which has become a consensus. A series of debiasing methods have been proposed to solve the problem. However, some existing debiasing SGG methods have a tendency to over-fit tail predicates, which is another type of bias. In order to eliminate the one-way over-fitting of head or tail predicates, this article proposes a balanced relation prediction (BRP) module which is model-agnostic and compatible with existing re-balancing methods. Moreover, because the relation prediction is based on object feature representation, this article proposes a scene adaptive context fusion (SACF) module to refine the object feature representation. Specifically, SACF models the context based on a chain structure, where the order of objects in the chain structure is adaptively arranged according to the scene content, achieving visual information fusion that adapts to the scene where the objects are located. Experiments on VG and GQA datasets show that the proposed method achieves competitive results on the comprehensive metric of R@K and mR@K.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4405541992",
    "type": "article"
  },
  {
    "title": "CVLP-NaVD: Contrastive Visual-Language Pre-training Models for Non-annotated Visual Description",
    "doi": "https://doi.org/10.1145/3708348",
    "publication_date": "2024-12-20",
    "publication_year": 2024,
    "authors": "Haoran Li; Yanbin Hao; Jiarui Yu; Bin Zhu; Shuo Wang; Tong Xu",
    "corresponding_authors": "",
    "abstract": "Non-annotated visual description (NaVD) aims to describe generic visuals without human-annotated pairwise data. The generic visuals refer to images and videos. Existing works mainly focus on one specific visual modality, i.e. , image or video. In this paper, we propose a new framework for this task, which can directly be applied to both image and video with the pipeline unchanged. Essentially, it is a unified framework that flexibly adapts to images and videos. Recently, contrastive visual-language pre-training models (CVLPs) have experienced rapid development, demonstrating powerful abilities to align vision and language. To continuously leverage advanced CVLPs, our framework is designed to work well with general CVLPs. It can easily use image-language CVLPs for image input and switch to video-language CVLPs for video input. Specifically, we propose a CVLP-based framework for NaVD, named CVLP-NaVD. It follows the paradigm of adversarial learning, containing a generator and a discriminator. The generator takes an image or a video as input and produces a corresponding language description, while the discriminator evaluates the generated sentence for its naturalness in human-like language. Apart from the naturalness, CVLPs play a crucial role in enhancing the alignment between visual and language signals during generation. Particularly, we explore three rewarding strategies to compute the alignment score, including directly calculating cosine similarity ( i.e. , VL-cross), projecting visual embeddings into the textual domain ( i.e. , VL-project) and their combination ( i.e. , VL-mix). The three strategies are fully examined in different scenarios. Finally, we conduct extensive experiments with various unpaired and unsupervised setups in both image and video captioning tasks. The experimental results demonstrate that our CVLP-NaVD outperforms the state-of-the-art methods significantly.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4405645745",
    "type": "article"
  },
  {
    "title": "Guest editorial",
    "doi": "https://doi.org/10.1145/1047936.1047939",
    "publication_date": "2005-02-01",
    "publication_year": 2005,
    "authors": "Ramesh Jain; Thomas Plagemann; Ralf Steinmetz",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2038830658",
    "type": "editorial"
  },
  {
    "title": "Flexible cross-domain event delivery for quality-managed multimedia applications",
    "doi": "https://doi.org/10.1145/1083314.1083316",
    "publication_date": "2005-08-01",
    "publication_year": 2005,
    "authors": "Christian Poellabauer; Karsten Schwan",
    "corresponding_authors": "",
    "abstract": "To meet end users' quality-of-service (QoS) requirements, online quality management for multimedia applications must include appropriate allocation of the underlying computing platform's resources. Previous work has developed novel operating system (OS) functionality for dynamic QoS management, including multimedia or real-time CPU schedulers and OS extensions for online performance monitoring and for adaptations, as well as QoS-aware applications that adapt their behavior to gain additional benefits from such functionality. This article describes a general OS mechanism that may be used to implement a wide variety of online quality management functions. ECalls is a communication mechanism that implements multiple cross-domain calling conventions that can be customized to the quality management needs of applications. The ECalls mechanism is based on the notions of events, event channels, and event handlers. Using events, applications can share relevant QoS attributes with OS services, and OS-level resource management services can efficiently provide monitoring data to target applications or application managers. Dynamically generated event handlers can be used to customize event delivery to meet diverse application needs, for example, to achieve high scalability for Web servers or small jitter for real-time data delivery.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2054650386",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on Computational Intelligence for Biomedical Data and Imaging",
    "doi": "https://doi.org/10.1145/3381919",
    "publication_date": "2020-01-31",
    "publication_year": 2020,
    "authors": "M. Tanveer; Pritee Khanna; Mukesh Prasad; Chin‐Teng Lin",
    "corresponding_authors": "",
    "abstract": "editorial Free Access Share on Introduction to the Special Issue on Computational Intelligence for Biomedical Data and Imaging Editors: M. Tanveer Indian Institute of Technology Indore, India Indian Institute of Technology Indore, India 0000-0002-5727-3697View Profile , P. Khanna Indian Institute of Information Technology, Design 8 Manufacturing, Jabalpur, India Indian Institute of Information Technology, Design 8 Manufacturing, Jabalpur, IndiaView Profile , M. Prasad University of Technology Sydney, Australia University of Technology Sydney, AustraliaView Profile , C. T. Lin University of Technology Sydney, Australia University of Technology Sydney, AustraliaView Profile Authors Info & Claims ACM Transactions on Multimedia Computing, Communications, and ApplicationsVolume 16Issue 1sJanuary 2020 Article No.: 29pp 1–4https://doi.org/10.1145/3381919Published:17 April 2020Publication History 13citation405DownloadsMetricsTotal Citations13Total Downloads405Last 12 Months100Last 6 weeks4 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteView all FormatsPDF",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3017363410",
    "type": "article"
  },
  {
    "title": "Design, Analysis, and Implementation of Efficient Framework for Image Annotation",
    "doi": "https://doi.org/10.1145/3386249",
    "publication_date": "2020-07-05",
    "publication_year": 2020,
    "authors": "Gargi Srivastava; Rajeev Srivastava",
    "corresponding_authors": "",
    "abstract": "In this article, a general framework of image annotation is proposed by involving salient object detection (SOD), feature extraction, feature selection, and multi-label classification. For SOD, Augmented-Gradient Vector Flow (A-GVF) is proposed, which fuses benefits of GVF and Minimum Directional Contrast. The article also proposes to control the background information to be included for annotation. This article brings about a comprehensive study of all major feature selection methods for a study on four publicly available datasets. The study concludes with the proposition of using Fisher’s method for reducing the dimension of features. Moreover, this article also proposes a set of features that are found to be strong discriminants by most of the methods. This reduced set for image annotation gives 3--4% better accuracy across all the four datasets. This article also proposes an improved multi-label classification algorithm C-MLFE.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3039401858",
    "type": "article"
  },
  {
    "title": "Vertical Retargeting for Stereoscopic Images via Stereo Seam Carving",
    "doi": "https://doi.org/10.1145/3408295",
    "publication_date": "2020-11-30",
    "publication_year": 2020,
    "authors": "Kun Zeng; Jiangchuan Hu; Yongyi Gong; Kanoksak Wattanachote; Runpeng Yu; Xiaonan Luo",
    "corresponding_authors": "",
    "abstract": "Vertical retargeting for stereoscopic images using seam manipulation-based approaches has remained an open challenge over the years. Even though horizontal retargeting had attracted a huge amount of interest, its seam coupling strategies were not capable to construct valid seam pairs for vertical retargeting. In this article, we propose two seam coupling strategies for vertical retargeting, namely, real mapping and virtual mapping. Our proposed mapping strategies were implemented to address the problems of multiple assignments and missing assignments, which are able to occur in the straightforward generalization from horizontal retargeting to vertical retargeting. On the basis of our proposed method, stereo seams were allowed to lay across occluded regions and occluding regions in stereo images. We maintained the geometric consistency by removing occluded pixels and corresponding occluding pixels in both stereo images. As a result, our method guarantees valid and geometrically consistent stereo seam pairs to be found in the horizontal direction. We generate vertically retargeted stereo images by removing or adding horizontal seam pairs iteratively. We conducted experiments on a number of indoor and outdoor scenes. Experimental results demonstrated that our method overcomes the limitations of vertical retargeting and is effective in preserving the geometric consistency.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3112613423",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on Privacy and Security in Evolving Internet of Multimedia Things",
    "doi": "https://doi.org/10.1145/3423955",
    "publication_date": "2020-10-31",
    "publication_year": 2020,
    "authors": "Suraj Sharma; Xuyun Zhang; Hesham El‐Sayed; Zhiyuan Tan",
    "corresponding_authors": "",
    "abstract": "introduction Introduction to the Special Issue on Privacy and Security in Evolving Internet of Multimedia Things Share on Editors: Suraj Sharma View Profile , Xuyun Zhang View Profile , Hesham El-Sayed View Profile , Zhiyuan Tan View Profile Authors Info & Claims ACM Transactions on Multimedia Computing, Communications, and ApplicationsVolume 16Issue 3sOctober 2020 Article No.: 93pp 1–3https://doi.org/10.1145/3423955Online:17 December 2020Publication History 0citation77DownloadsMetricsTotal Citations0Total Downloads77Last 12 Months38Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3135281472",
    "type": "article"
  },
  {
    "title": "Table of Contents",
    "doi": "https://doi.org/10.1145/3409367",
    "publication_date": "2020-07-14",
    "publication_year": 2020,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "other Table of Contents: Online Supplement Volume 16, Number 1s Share on ACM Transactions on Multimedia Computing, Communications, and ApplicationsVolume 16Issue 3August 2020 Article No.: 74pp 1–5https://doi.org/10.1145/3409367Online:14 July 2020Publication History 1citation50DownloadsMetricsTotal Citations1Total Downloads50Last 12 Months19Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4205250093",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on Fine-Grained Visual Recognition and Re-Identification",
    "doi": "https://doi.org/10.1145/3505280",
    "publication_date": "2022-01-25",
    "publication_year": 2022,
    "authors": "Shiliang Zhang; Guorong Li; Weigang Zhang; Qingming Huang; Tiejun Huang; Mubarak Shah; Nicu Sebe",
    "corresponding_authors": "",
    "abstract": "introduction Share on Introduction to the Special Issue on Fine-Grained Visual Recognition and Re-Identification Authors: Shiliang Zhang Peking University Peking UniversityView Profile , Guorong Li University of Chinese Academy of Sciences University of Chinese Academy of SciencesView Profile , Weigang Zhang Harbin Institute of Technology Harbin Institute of TechnologyView Profile , Qingming Huang University of Chinese Academy of Sciences University of Chinese Academy of SciencesView Profile , Tiejun Huang Peking University Peking UniversityView Profile , Mubarak Shah University of Central Florida University of Central FloridaView Profile , Nicu Sebe University of Trento University of TrentoView Profile Authors Info & Claims ACM Transactions on Multimedia Computing, Communications, and ApplicationsVolume 18Issue 1sFebruary 2022 Article No.: 24pp 1–3https://doi.org/10.1145/3505280Online:25 January 2022Publication History 0citation169DownloadsMetricsTotal Citations0Total Downloads169Last 12 Months169Last 6 weeks22 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4207082786",
    "type": "article"
  },
  {
    "title": "TTV Regularized LRTA Technique for the Estimation of Haze Model Parameters in Video Dehazing",
    "doi": "https://doi.org/10.1145/3465454",
    "publication_date": "2022-01-27",
    "publication_year": 2022,
    "authors": "P. S. Baiju; Sudhish N. George",
    "corresponding_authors": "",
    "abstract": "Nowadays, intelligent transport systems have a major role in providing a safe and secure traffic society for passengers, pedestrians, and vehicles. However, some bad weather conditions such as haze or fog may affect the visual clarity of video footage captured by the camera. This will cause a malfunction in further video processing algorithms performed by such automated systems. This article proposes an efficient technique for estimating the atmospheric light and the transmission map in the haze model entirely in tensor domain for video dehazing. In this work, the atmospheric light is appraised using the Mie scattering principle of visible light and the temporal coherency among the frames is achieved by means of tensor algebra. Furthermore, the transmission map is computed using Low Rank Tensor Approximation (LRTA) based on Weighted Tensor Nuclear Norm (WTNN) minimization and Tensor Total Variation (TTV) regularization. WTNN minimization is used to smooth the coarse transmission map, and TTV regularization is employed to maintain spatio-temporal continuity by preserving the details of salient structures and edges. The novelty of the proposed model is confined in the efficient formulation of a unified optimization model for the estimation of transmission map and atmospheric light in the tensor domain with fine-tuned regularization terms, which is not reported till now in the direction of video dehazing. Extensive experiments show that the proposed method outperforms state-of-the-art methods in video dehazing.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4210323949",
    "type": "article"
  },
  {
    "title": "When Did It Happen? Duration-informed Temporal Localization of Narrated Actions in Vlogs",
    "doi": "https://doi.org/10.1145/3495211",
    "publication_date": "2022-02-18",
    "publication_year": 2022,
    "authors": "Oana Ignat; Santiago Castro; Yuhang Zhou; Jiajun Bao; Dandan Shan; Rada Mihalcea",
    "corresponding_authors": "",
    "abstract": "We consider the task of temporal human action localization in lifestyle vlogs. We introduce a novel dataset consisting of manual annotations of temporal localization for 13,000 narrated actions in 1,200 video clips. We present an extensive analysis of this data, which allows us to better understand how the language and visual modalities interact throughout the videos. We propose a simple yet effective method to localize the narrated actions based on their expected duration. Through several experiments and analyses, we show that our method brings complementary information with respect to previous methods, and leads to improvements over previous work for the task of temporal action localization.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4213340648",
    "type": "article"
  },
  {
    "title": "Densely Enhanced Semantic Network for Conversation System in Social Media",
    "doi": "https://doi.org/10.1145/3501799",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Yongrui Li; Zengfu Wang; Jun Yu",
    "corresponding_authors": "",
    "abstract": "The human–computer conversation system is a significant application in the field of multimedia. To select an appropriate response, retrieval-based systems model the matching between the dialogue history and response candidates. However, most of the existing methods cannot fully capture and utilize varied matching patterns, which may degrade the performance of the systems. To address the issue, a densely enhanced semantic network (DESN) is proposed in our work. Given a multi-turn dialogue history and a response candidate, DESN first constructs the semantic representations of sentences from the word perspective, the sentence perspective, and the dialogue perspective. In particular, the dialogue perspective is a novel one introduced in our work. The dependencies between a single sentence and the whole dialogue are modeled from the dialogue perspective. Then, the response candidate and each utterance in the dialogue history are made to interact with each other. The varied matching patterns are captured for each utterance–response pair by using a dense matching module. The matching patterns of all the utterance–response pairs are accumulated in chronological order to calculate the matching degree between the dialogue history and the response. The responses in the candidate pool are ranked with the matching degree, thereby returning the most appropriate candidate. Our model is evaluated on the benchmark datasets. The experimental results prove that our model achieves significant and consistent improvement when compared with other baselines.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4214896178",
    "type": "article"
  },
  {
    "title": "Pansharpening Scheme Using Bi-dimensional Empirical Mode Decomposition and Neural Network",
    "doi": "https://doi.org/10.1145/3506709",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Nidhi Saxena; Balasubramanian Raman",
    "corresponding_authors": "",
    "abstract": "The pansharpening is a combination of multispectral (MS) and panchromatic (PAN) images that produce a high-spatial-spectral-resolution MS images. In multiresolution analysis–based pansharpening schemes, some spatial and spectral distortions are found. It can be reduced by adding spatial detail images of the PAN image into MS images. In the convolution neural network– (CNN) based method, the lowpass filter image extracted by the CNN model when MS and PAN images are directly applied into the input. The feature values are very high and reduce the conversion efficiency. In the proposed scheme, bi-dimensional empirical mode decomposition is used to extract the spatial detail information of the PAN image to reduce the feature values of the input. This extracted PAN image information is applied to the CNN to produce the non-linear changes in the image pixels and transformed into the perfect spatial detail image. It identifies the spatial and spectral detail quantity for the proposed scheme and it also varies with the different datasets automatically of the same satellite images. Simulation results in the context of qualitative and quantitative analysis demonstrate the effectiveness of proposed scheme applied on datasets collected by different satellites.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4214927157",
    "type": "article"
  },
  {
    "title": "Joint Source-Channel Decoding of Polar Codes for HEVC-Based Video Streaming",
    "doi": "https://doi.org/10.1145/3502208",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Jinzhi Lin; Yun Zhang; Na Li; Hongling Jiang",
    "corresponding_authors": "",
    "abstract": "Ultra High-Definition (UHD) and Virtual Reality (VR) video streaming over 5G networks are emerging, in which High-Efficiency Video Coding (HEVC) is used as source coding to compress videos more efficiently and polar code is used as channel coding to transmit bitstream reliably over an error-prone channel. In this article, a novel Joint Source-Channel Decoding (JSCD) of polar codes for HEVC-based video streaming is presented to improve the streaming reliability and visual quality. Firstly, a Kernel Density Estimation (KDE) fitting approach is proposed to estimate the positions of error channel decoded bits. Secondly, a modified polar decoder called R-SCFlip is designed to improve the channel decoding accuracy. Finally, to combine the KDE estimator and the R-SCFlip decoder together, the JSCD scheme is implemented in an iterative process. Extensive experimental results reveal that, compared to the conventional methods without JSCD, the error data-frame correction ratios are increased. Averagely, 1.07% and 1.11% Frame Error Ratio (FER) improvements have been achieved for Additive White Gaussian Noise (AWGN) and Rayleigh fading channels, respectively. Meanwhile, the qualities of the recovered videos are significantly improved. For the 2D videos, the average Peak Signal-to-Noise Ratio (PSNR) and Structural SIMilarity (SSIM) gains reach 14% and 34%, respectively. For the 360֯ videos, the average improvements in terms of Weighted-to-Spherically-uniform PSNR (WS-PSNR) and Voronoi-based Video Multimethod Assessment Fusion (VI-VMAF) reach 21% and 7%, respectively.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4214928899",
    "type": "article"
  },
  {
    "title": "On Teaching Mode of MTI Translation Workshop Based on IPT Corpus for Tibetan Areas of China",
    "doi": "https://doi.org/10.1145/3527173",
    "publication_date": "2022-03-30",
    "publication_year": 2022,
    "authors": "Xin Huang",
    "corresponding_authors": "Xin Huang",
    "abstract": "With the technological turn of applied research in translation, increasing attention has been paid to the teaching of translation technology. This article addresses two important questions in this regard: how to independently develop Master of Translation and Interpreting (MTI) translation teaching resources with ethnic minority characteristics and how to use information technology to carry out Tibet-related computer-assisted translation (CAT) teaching. This article discusses the background, structure, and functions of the International Publicity Translation Corpus (IPT Corpus) for Tibetan Areas of China through empirical research, combining theory with practice, and validates the translation teaching mode through case study to better train translators and interpreters working on content related to Tibetan culture. Through teaching practice since 2017, the MTI translation workshop based on the IPT Corpus has proven to be an effective teaching mode that is worthy of further improvement and extension.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4220842321",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1126004",
    "publication_date": "2006-02-01",
    "publication_year": 2006,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4229566379",
    "type": "paratext"
  },
  {
    "title": "Evaluation of an Intervention Program Based on Mobile Apps to Learn Sexism Prevention in Teenagers",
    "doi": "https://doi.org/10.1145/3471139",
    "publication_date": "2022-02-16",
    "publication_year": 2022,
    "authors": "Pedro Morillo; José Javier Navarro Pérez; Juan M. Orduña; Marcos Fernández",
    "corresponding_authors": "",
    "abstract": "The fight against sexism is nowadays one of the flagship social movements in western countries. Adolescence is a crucial period, and some empirical studies have focused on the socialization of teenagers, proving that the socialization with the surrounding environment prevent sexist practices. In a previous work, we developed and tested the effectiveness of a mobile app, called Liad@s , with the goals of helping teenagers to prevent sexism and build healthy couple relationships. In this article, we carry out a study where (using a real situation) we compare the effectiveness of the Liad@s app in front of traditional interventions like a workshop about sexism for teenagers. Also, we evaluate the usability of the app and the user satisfaction with this application. In this study, our primary hypothesis is that the effectiveness of using our mobile application, in terms of knowledge acquired about sexism, would be at least as good as attending the workshop. Our secondary hypothesis is that the user satisfaction with the mobile application would be higher than the one with the workshop, causing a preference for the app. The results of this study show significant differences in learning appeared between gender and between the two different procedures when separately evaluating the data collected from both hostile sexism (HS) and benevolent sexism (BS) questionnaires. These results validate our primary hypothesis. Also, most of the population under study preferred the mobile app in front of the traditional workshop, validating also our secondary hypothesis.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4283399293",
    "type": "article"
  },
  {
    "title": "From False-Free to Privacy-Oriented Communitarian Microblogging Social Networks",
    "doi": "https://doi.org/10.1145/3555354",
    "publication_date": "2022-08-27",
    "publication_year": 2022,
    "authors": "Joan Manuel Marquès Puig; Helena Rifà-Pous; Samia Oukemeni",
    "corresponding_authors": "",
    "abstract": "Online Social Networks (OSNs) have gained enormous popularity in recent years. They provide a dynamic platform for sharing content (text messages or multimedia) and for facilitating communication between friends and acquaintances. Microblogging services are a popular form of OSNs. They allow sending small messages in a one-to-many messaging model so that users can communicate with their favorite celebrity, brand, politician, or other regular users without the obligation of a pre-existing social relationship. A chain of privacy-related scandals linked to questionable data handling practices in microblogging services has arisen in the last past few years. Most current microblogging service providers offer centralized services and their business model is based on monitoring, analyzing, and selling users’ activity and patterns. In the end, the personal information shared by the users to benefit from the free-of-charge services is used for the underlying payment in such systems. In this paper, we present Garlanet, a privacy-aware censorship-resistant microblogging social network that does not rely on a centralized service provider as all data is hosted in computers voluntarily contributed by the users of the system. Garlanet provides microblogging functionalities while protecting privacy and preserving the confidentiality and integrity of users and data. It ensures that users’ identities and their social graphs are hidden from the system and adversaries and it provides availability and scalability of the services. We also evaluate the privacy level of Garlanet and we compare it with the privacy level of eight other microblogging systems.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4293368634",
    "type": "article"
  },
  {
    "title": "DNA Computing-Based Multi-Source Data Storage Model in Digital Twins",
    "doi": "https://doi.org/10.1145/3561823",
    "publication_date": "2022-09-13",
    "publication_year": 2022,
    "authors": "Jinxia Wang; Rui Chen; Zhihan Lv",
    "corresponding_authors": "",
    "abstract": "The work aims to study the application of Deoxyribonucleic Acid (DNA) multi-source data storage in Digital Twins (DT) . Through the investigation of the research status of DT and DNA computing, the work puts forward the concept of DNA multi-source data storage for DT. Raptor code is improved from the design direction of degree distribution function, and six degree function distribution schemes are proposed in turn in the process of describing the research method. Additionally, a quaternary dynamic Huffman coding method is applied in DNA data storage, combined with the improved concatenated code as the error correction code. Considering the content of cytosine deoxynucleotide (C) and guanine deoxynucleotide Guanine (G) and the distribution of homopolymer in DNA storage, the work proposes and verifies an improved concatenated code algorithm Deoxyribonucleic Acid-Improved Concatenated code (DNA-ICC) . The results show that while the Signal-to-Noise Ratio (SNR) increases, the Bit Error Rate (BER) decreases gradually and the trend is similar. But the anti-interference ability of the degree distribution function optimized by the probability transfer method is better. The BER of DNA-ICC scheme decreases with the decrease of error probability, which is stronger than other error correction codes. Compared with the original concatenated code, it saves at least 1.65 s, and has a good control effect on homopolymer. When the size of homopolymer exceeds 4 nt, the probability of homopolymer is only 0.44%. The proposed Quaternary dynamic Huffman code and concatenated error correction code have excellent performance.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4296592198",
    "type": "article"
  },
  {
    "title": "Temporal Dynamic Concept Modeling Network for Explainable Video Event Recognition",
    "doi": "https://doi.org/10.1145/3568312",
    "publication_date": "2022-10-25",
    "publication_year": 2022,
    "authors": "Weigang Zhang; Zhaobo Qi; Shuhui Wang; Chi Su; Li Su; Qingming Huang",
    "corresponding_authors": "",
    "abstract": "Recently, with the vigorous development of deep learning and multimedia technology, intelligent urban computing has received more and more extensive attention from academia and industry. Unfortunately, most of the related technologies are black-box paradigms that lack interpretability. Among them, video event recognition is a basic technology. Event contains multiple concepts and their rich interactions, which can assist us to construct explainable event recognition methods. However, the crucial concepts needed to recognize events have various temporal existing patterns, and the relationship between events and the temporal characteristics of concepts has not been fully exploited. This brings great challenges for concept-based event categorization. To address the above issues, we introduce the temporal concept receptive field, which is the length of the temporal window size required to capture key concepts for concept-based event recognition methods. Accordingly, we introduce the temporal dynamic convolution (TDC) to model the temporal concept receptive field dynamically according to different events. Its core idea is to combine the results of multiple convolution layers with the learned coefficients from two complementary perspectives. These convolution layers contain a variety of kernel sizes, which can provide temporal concept receptive fields of different lengths. Similarly, we also propose the cross-domain temporal dynamic convolution (CrTDC) with the help of the rich relationship between different concepts. Different coefficients can help us to capture suitable temporal concept receptive field sizes and highlight crucial concepts to obtain accurate and complete concept representations for event analysis. Based on the TDC and CrTDC, we introduce the temporal dynamic concept modeling network (TDCMN) for explainable video event recognition. We evaluate TDCMN on large-scale and challenging datasets FCVID, ActivityNet, and CCV. Experimental results show that TDCMN significantly improves the event recognition performance of concept-based methods, and the explainability of our method inspires us to construct more explainable models from the perspective of the temporal concept receptive field.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4307380267",
    "type": "article"
  },
  {
    "title": "Fine-Grained Text-to-Video Temporal Grounding from Coarse Boundary",
    "doi": "https://doi.org/10.1145/3579825",
    "publication_date": "2022-09-30",
    "publication_year": 2022,
    "authors": "Jiachang Hao; Haifeng Sun; Pengfei Ren; Yiming Zhong; Jingyu Wang; Qi Qi; Jianxin Liao",
    "corresponding_authors": "",
    "abstract": "Text-to-video temporal grounding aims to locate a target video moment that semantically corresponds to the given sentence query in an untrimmed video. In this task, fully supervised works require text descriptions for each event along with its temporal segment coordinate for training, which is labor-consuming. Existing weakly supervised works require only video-sentence pairs but cannot achieve satisfactory performance. However, many available annotations in the form of coarse temporal boundaries for sentences are ignored and unexploited. These coarse boundaries are common in streaming media platform and can be collected in a mechanical manner. We propose a novel approach to perform fine-grained text-to-video temporal grounding from these coarse boundaries. We take dense video captioning as base task and leverage the trained captioning model to identify the relevance of each video frame to the sentence query according to the frame participation in event captioning. To quantify the frame participation in event captioning, we propose event activation sequence , a simple method that highlights the temporal regions which have high correlations to the text modality in videos. Experiments on modified ActivityNet Captions and a use case demonstrate the promising fine-grained performance of our approach.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4315705014",
    "type": "article"
  },
  {
    "title": "A Multi-Level Consistency Network for High-Fidelity Virtual Try-On",
    "doi": "https://doi.org/10.1145/3580500",
    "publication_date": "2022-09-30",
    "publication_year": 2022,
    "authors": "Hao Wei; Rui Chen",
    "corresponding_authors": "",
    "abstract": "The 2D virtual try-on task aims to transfer a target clothing image to the corresponding region of a person image. Although an extensive amount of research has been conducted due to its immense applications, this task still remains a great challenge to handle some complicated issues (e.g., non-rigid shapes, large occlusions and arbitrary poses). To this end, we propose a novel network with structural and textural consistency-preserving mechanism for producing high-fidelity try-on images. Specifically, we first generate the semantic layout of a clothing-agnostic person to obtain the segmentation map, which is used as the transforming conditions of the target clothes. Based on a recurrent network structure, the transform lookup is performed to iteratively update a dense flow. Then, we adopt a thin-plate-spline-based warping method to estimate the coarse offset flow for all key-point positions. Guided by this sparse flow, a multi-scale deformable convolution module is designed to further iteratively predict the fine offsets for densely sampled positions, by which the clothing item and person shape can be accurately aligned. Finally, we develop a refinement module to effectively fuse the global and local features, which can render accurate geometric structures of the body parts and maintain texture sharpness of the clothes. Extensive experiments on benchmark datasets demonstrate that our method outperforms other state-of-the-art methods in terms of quantitative and qualitative try-on results. The code is available on: https://github.com/TJU-WEIHAO/MLCN .",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4317434496",
    "type": "article"
  },
  {
    "title": "Robust Video Stabilization based on Motion Decomposition",
    "doi": "https://doi.org/10.1145/3580498",
    "publication_date": "2022-09-30",
    "publication_year": 2022,
    "authors": "Jian Wang; Qiang Ling; Peiyan Li",
    "corresponding_authors": "",
    "abstract": "Video stabilization aims to eliminate camera jitter and improve the visual experience of shaky videos. Video stabilization methods often ignore the active movement of the foreground objects and the camera, and may result in distortion and over-smoothing problems. To resolve these issues, this paper proposes a novel video stabilization method based on motion decomposition. Since the inter-frame movement of foreground objects is different from that of the background, we separate foreground feature points from background feature points by modifying the classic density based spatial clustering method of applications with noise (DBSCAN). The movement of background feature points is consistent with the movement of the camera, which can be decomposed into the camera jitter and the active movement of the camera. And the movement of foreground feature points can be decomposed into the movement of the camera and the active movement of foreground objects. Based on motion decomposition, we design first-order and second-order trajectory smoothing constraints to eliminate the high-frequency and low-frequency components of the camera jitter. To reduce content distortion, shape-preserving constraints, and regularization constraints are taken to generate stabilized views of all feature points. Experimental results demonstrate the effectiveness and robustness of the proposed video stabilization method on a variety of challenging videos.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4318477362",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on 6G Enabled Interactive Multimedia Communication Systems",
    "doi": "https://doi.org/10.1145/3567835",
    "publication_date": "2022-10-31",
    "publication_year": 2022,
    "authors": "Carlos Montenegro; R. Dinesh Jackson Samuel; N. Gunasekaran",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4320073115",
    "type": "article"
  },
  {
    "title": "Audio-Visual Contrastive Pre-train for Face Forgery Detection",
    "doi": "https://doi.org/10.1145/3651311",
    "publication_date": "2024-03-13",
    "publication_year": 2024,
    "authors": "Hanqing Zhao; Wenbo Zhou; Dongdong Chen; Weiming Zhang; Ying Guo; Zhen Cheng; Pengfei Yan; Nenghai Yu",
    "corresponding_authors": "",
    "abstract": "The highly realistic avatar in the metaverse may lead to severe leakage of facial privacy. Malicious users can more easily obtain the 3D structure of faces, thus using Deepfake technology to create counterfeit videos with higher realism. To automatically discern facial videos forged with the advancing generation techniques, deepfake detectors need to achieve stronger generalization abilities. Inspired by transfer learning, neural networks pre-trained on other large-scale face-related tasks would provide fundamental features for deepfake detection. We propose a video-level deepfake detection method based on a temporal transformer with a self-supervised audio-visual contrastive learning approach for pre-training the deepfake detector. The proposed method learns motion representations in the mouth region by encouraging the paired video and audio representations to be close while unpaired ones to be diverse. The deepfake detector adopts the pre-trained weights and partially fine-tunes on deepfake datasets. Extensive experiments show that our self-supervised pre-training method can effectively improve the accuracy and robustness of our deepfake detection model without extra human efforts. Compared with existing deepfake detection methods, our proposed method achieves better generalization ability in cross-dataset evaluations.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4392761692",
    "type": "article"
  },
  {
    "title": "Multiple Image Distortion DNN Modeling Individual Subject Quality Assessment",
    "doi": "https://doi.org/10.1145/3664198",
    "publication_date": "2024-05-21",
    "publication_year": 2024,
    "authors": "Lohic Fotio Tiotsop; Antonio Servetti; Peter Počta; Glenn Van Wallendael; Marcus Barkowsky; Enrico Masala",
    "corresponding_authors": "",
    "abstract": "A recent research direction is focused on training Deep Neural Networks (DNNs) to replicate individual subject assessments of media quality. These DNNs are referred to as Artificial Intelligence-based Observers (AIOs). An AIO is designed to simulate, in real-time, the quality ratings of a specific individual, enabling an automatic quality assessment that accounts for subjects characteristics and preferences. Training AIOs is a promising but challenging research area due to the greater noise in individual raw opinion scores compared to the Mean Opinion Score. Effective learning from noisy labels necessitates the training of complex models on large-scale datasets. Unfortunately, this is challenging for AIOs as the media quality assessment community lacks extensive datasets that include individual opinion scores. To address the complexity of the task, we first created a dataset comprising two million samples, with synthetic labels derived from human annotation. We then trained a customized network for image quality assessment, named Multi-Distortion ResNet50 (MDResNet50), on this dataset. The weights of the MDResNet50 were subsequently utilized to initialize the learning process of each AIO, thereby avoiding the need to train a complex model from scratch on a small-scale dataset with raw individual opinion scores. Computational experiments show that our approach significantly advances the state-of-the-art in the AIO research. In particular: (i) we demonstrate through a simulation the ability of AIOs to mimic two well-known behavioral characteristics of a subject, i.e., bias and inconsistency, when scoring the media quality; (ii) we train and release DNN-based AIOs that, compared to the state-of-the-art, exhibit a higher performance with a statistical significance in assessing multiple image distortions; (iii) we train AIOs that more accurately mimic the sensitivity of real subjects to noise and color saturation and also better predict the opinion score distribution compared to the state-of-the-art AIOs.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4398169429",
    "type": "article"
  },
  {
    "title": "Hidden Barcode in Sub-Images with Invisible Locating Marker",
    "doi": "https://doi.org/10.1145/3674976",
    "publication_date": "2024-06-28",
    "publication_year": 2024,
    "authors": "Jun Jia; Zhongpai Gao; Yiwei Yang; Wei Sun; Dandan Zhu; Xiaohong Liu; Xiongkuo Min; Guangtao Zhai",
    "corresponding_authors": "",
    "abstract": "The prevalence of the Internet of Things has led to the widespread adoption of 2D barcodes as a means of offline-to-online communication. Whereas, 2D barcodes are not ideal for publicity materials due to their space-consuming nature. Recent works have proposed 2D image barcodes that contain invisible codes or hyperlinks to transmit hidden information from offline to online. However, these methods undermine the purpose of the codes being invisible due to the the requirement of markers to locate them. The conference version of this work presented a novel imperceptible information embedding framework for display or print-camera scenarios, which includes not only hiding and recovery but also locating and correcting. With the assistance of learned invisible markers, hidden codes can be rendered truly imperceptible. A highly effective multi-stage training scheme is proposed to achieve high visual fidelity and retrieval resiliency, wherein information is concealed in a sub-region rather than the entire image. However, our conference version does not address the optimal sub-region for hiding, which is crucial when dealing with local region concealment problems. In this article extension, we consider human perceptual characteristics and introduce an optimal hiding region recommendation algorithm that comprehensively incorporates Just Noticeable Difference and visual saliency factors into consideration. Extensive experiments demonstrate superior visual quality and robustness compared to state-of-the-art methods. With the assistance of our proposed hiding region recommendation algorithm, concealed information becomes even less visible than the results of our conference version without compromising robustness.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4400117926",
    "type": "article"
  },
  {
    "title": "Gated multi-modal edge refinement network for light field salient object detection",
    "doi": "https://doi.org/10.1145/3674836",
    "publication_date": "2024-06-28",
    "publication_year": 2024,
    "authors": "Yefan Li; Fuqing Duan; Ke Lü",
    "corresponding_authors": "",
    "abstract": "Light field can be decoded into multiple representations and provides valuable focus and depth information. This breakthrough overcomes the limitations of traditional 2D and 3D saliency detection methods, opening up new possibilities for more accurate and comprehensive analysis of visual scenes. To tackle the challenges of inaccurate edge prediction and effectively leverage the rich multi-modal light field information, we propose a gated multi-modal edge refinement network (GMERNet). It first obtains the preliminary position and structure information of the salient object and then gradually refines the object edge. This involves two modules: gated multi-modal feature complement (GMFC) module and progressive edge refinement (PER) module. The GMFC module captures dependencies across the all-in-focus image and its corresponding focal stack and depth map, effectively aggregating multiple features through gate mechanisms. The PER module progressively refines edges by combining salient object features with edge features through a cascaded structure. Experimental results demonstrate that GMERNet achieves state-of-the-art performance on five benchmark datasets and shows significant advantages in extracting salient objects with complex edges.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4400118046",
    "type": "article"
  },
  {
    "title": "Introduction to Special Issue on “Recent trends in Multimedia Forensics”",
    "doi": "https://doi.org/10.1145/3678473",
    "publication_date": "2024-08-02",
    "publication_year": 2024,
    "authors": "Ritesh Vyas; Michele Nappi; Alberto Del Bimbo; Sambit Bakshi",
    "corresponding_authors": "",
    "abstract": "Multimedia forensics is a subject area which is the need of the hour in this modern era of media-manipulation and generation of fake images/videos assisted with artificial intelligence (AI) models. With the ubiquitous expansion of internet enabled devices, there is a humungous amount of data available to the perusal of forensic experts. This data comprises of audio, video, images, text or a mix of those. Hence multimedia forensics, which involves a set of scientific techniques to collect, scrutinize and analyze this digital content, becomes highly imperative. The increasing threat of compelling media manipulations through machine learning-based technologies is making the situation more alarming. The most common instances are generative adversarial networks (GANs) (to generate artificial yet realistic images/videos) and DeepFake algorithms (to swap faces and expressions in videos). Furthermore, the ease of getting these manipulations done has lowered the skill required from the attacker's end, which has intensified the problem manifold. This special issue captures a few recent outstanding works beyond trivial research results in order to push the border of the state-of-the-art and record the developments on this subject of research.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4401249194",
    "type": "article"
  },
  {
    "title": "ST-360: Spatial–Temporal Filtering-Based Low-Latency 360-Degree Video Analytics Framework",
    "doi": "https://doi.org/10.1145/3694685",
    "publication_date": "2024-09-04",
    "publication_year": 2024,
    "authors": "Jiaxi Li; Jingwei Liao; Bo Chen; Anh Nguyen; Aditi Tiwari; Qian Zhou; Zhisheng Yan; Klara Nahrstedt",
    "corresponding_authors": "",
    "abstract": "Recent advances in computer vision algorithms and video streaming technologies have facilitated the development of edge-server-based video analytics systems, enabling them to process sophisticated real-world tasks, such as traffic surveillance and workspace monitoring. Meanwhile, due to their omnidirectional recording capability, 360-degree cameras have been proposed to replace traditional cameras in video analytics systems to offer enhanced situational awareness. Yet, we found that providing an efficient 360-degree video analytics framework is a non-trivial task. Due to the higher resolution and geometric distortion in 360-degree videos, existing video analytics pipelines fail to meet the performance requirements for end-to-end latency and query accuracy. To address these challenges, we introduce the innovative ST-360 framework specifically designed for 360-degree video analytics. This framework features a spatial-temporal filtering algorithm that optimizes both data transmission and computational workloads. Evaluation of the ST-360 framework on a unique dataset of 360-degree first-responders videos reveals that it yields accurate query results with a 50% reduction in end-to-end latency compared to state-of-the-art methods.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4402227027",
    "type": "article"
  },
  {
    "title": "Mutually-Guided Hierarchical Multi-Modal Feature Learning for Referring Image Segmentation",
    "doi": "https://doi.org/10.1145/3698771",
    "publication_date": "2024-10-05",
    "publication_year": 2024,
    "authors": "Jiachen Li; Qing Xie; Xiaojun Chang; Jinyu Xu; Yongjian Liu",
    "corresponding_authors": "",
    "abstract": "Referring image segmentation aims to locate and segment the target region based on a given textual expression query. The primary challenge is to understand semantics from visual and textual modalities and achieve alignment and matching. Prior works have attempted to address this challenge by leveraging separately pretrained unimodal models to extract global visual and textual features and perform straightforward fusion to establish cross-modal semantic associations. However, these methods often concentrate solely on the global semantics, disregarding the hierarchical semantics of expression and image and struggling with complex and open real scenarios, thus failing to capture critical cross-modal information. To address these limitations, this article introduces an innovative mutually-guided hierarchical multi-modal feature learning scheme. By leveraging the guidance of global visual features, the model mines hierarchical text features from different stages of the text encoder. Simultaneously, the guidance of global textual features is leveraged to aggregate multi-scale visual features. This mutually guided hierarchical feature learning effectively addresses the semantically inaccurate cause by free-form text and naturally occurring scale variations. Furthermore, a Segment Detail Refinement (SDR) module is designed to enhance the model’s spatial detail awareness through attention mapping of low-level visual features and cross-modal features. To evaluate the effectiveness of the proposed approach, extensive experiments are conducted on three widely used referring image object segmentation datasets. The results demonstrate the superiority of the presented method in accurately locating and segmenting objects in images.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403155129",
    "type": "article"
  },
  {
    "title": "A Hierarchically Discriminative Loss with Group Regularization for Fine-Grained Image Classification",
    "doi": "https://doi.org/10.1145/3698398",
    "publication_date": "2024-10-07",
    "publication_year": 2024,
    "authors": "Hung-Jen Chen; Chu-Jun Peng; Hong-Han Shuai; Wen-Huang Cheng",
    "corresponding_authors": "",
    "abstract": "Fine-grained visual classification targets the discrimination of subordinate categories within broader classes, such as avian species, aerial crafts, and notably, in medical diagnostics like breast cancer. In the domain of breast cancer classification, there has been an emergence of numerous fine-grained models leveraging pathological images. Elevating the interpretability of the model’s discriminative processes among these nuanced categories holds promise in enhancing the transparency of AI decision-making. However, the effective utilization of hierarchical label structures within medical data remains a crucial consideration. In this work, we explore how the label hierarchy can be used to better learn subtle feature embeddings. We observe that the semantic relationships between fine-grained categories can help to analyze the misclassified samples. To this end, with the label hierarchy, we introduce two novel losses to cultivate subtle feature representations, coordinate with feature learning, and align with the principles of Explainable AI (XAI). First, we propose hierarchically discriminative loss to enhance the interactions between fine- and coarse-level features, which can help reinforce the discriminability of fine-grained features for reducing false predictions belonging to the out-group relation. Second, we introduce the in-group regularization loss to establish the interactions between the target class and those confusing classes belonging to the in-group relation. Treating another confusing class as a distraction can regularize feature learning of the target class. Thus, it allows the network to discover more discriminative features and reduces in-group false predictions. Five commonly used datasets for fine-grained classification are extensively evaluated. Our experimental results validate the effectiveness of our proposed novel losses when compared to state-of-the-art methods that utilize hierarchical multi-granularity labels.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403184950",
    "type": "article"
  },
  {
    "title": "FaceDefend: Copyright Protection to Prevent Face Embezzle",
    "doi": "https://doi.org/10.1145/3699718",
    "publication_date": "2024-10-08",
    "publication_year": 2024,
    "authors": "Rui Zhai; Rongrong Ni; Yang Yu; Yao Zhao",
    "corresponding_authors": "",
    "abstract": "With the rapid evolution of deep learning and the advent of artificial intelligence, the metaverse has emerged as a significant technology. Within the metaverse, diverse elements such as rich applications and realistic digital avatars provide users with immersive experiences, but it poses a series of security problems. Current research predominantly focuses on the data storage and transmission processes from the perspective of blockchain and the Internet of Things to achieve the protection of the metaverse. However, there exists a gap in security research on the digital avatar generation process. Given that digital avatars are the primary entities engaging in social activities within the metaverse and are crafted based on real face images, the virtual character can be generated easily by stealing the user’s face image and controlled to interact with others. In order to deal with the above problems, we propose a novel method to prevent the misuse of faces, which maintains the security of the metaverse by protecting facial data and thus preventing its misuse. We explore the common architecture of generative models and propose a defense method based on copyright protection to prevent face embezzling. Firstly, we utilize the copyright protection module to obtain copyright protection information. Secondly, we utilized the defense control module to ensure the representation of the protected images occurs errors in the latent space of the generation model. Therefore the subsequent generation task output fails, which effectively protects the face data and prevents the generation of digital avatars. Furthermore, the results on public datasets and across multiple generative models present unnatural outputs, indicating the excellence of our defense and transfer capabilities.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403222517",
    "type": "article"
  },
  {
    "title": "Content-Aware Selective Encryption for H.265/HEVC using Deep Hashing Network and Steganography",
    "doi": "https://doi.org/10.1145/3698400",
    "publication_date": "2024-10-08",
    "publication_year": 2024,
    "authors": "Qingxin Sheng; Chong Fu; Zhaonan Lin; Junxin Chen; Xingwei Wang; Chiu‐Wing Sham",
    "corresponding_authors": "",
    "abstract": "Existing selective encryption schemes for High Efficiency Video Coding (HEVC) only focus on the encoding characteristics of syntax elements in entropy coding and lack an understanding of the video content. Consequently, a large amount of unnecessary encryption operations are utilized to protect insensitive video frames, resulting in low encryption efficiency. In this paper, we propose a content-aware selective encryption scheme for H.265/HEVC, which encrypts only the groups of pictures (GOPs) containing sensitive content and thus offers high efficiency. In our scheme, a deep hashing network is first adopted to retrieve video frames to determine the content-sensitive GOPs. Then, multiple prediction and residual syntax elements in sensitive GOPs are encrypted using a keystream sequence generated by the hyperchaotic Lorenz system. In addition, the direct current coefficient of each \\(4\\times 4\\) transform block is exchanged with a pseudo-randomly selected non-zero alternating current coefficient to further offer stronger visual distortion. Finally, the sign bits used for marking each GOP-type are reversibly embedded into the encrypted syntax elements to facilitate the decoder to distinguish the GOPs that need to be decrypted. Experimental results indicate that the proposed content-aware selective encryption scheme can efficiently protect sensitive content and is robust against all common attacks. Furthermore, it outperforms other state-of-the-art HEVC selective encryption algorithms in terms of security performance.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403222537",
    "type": "article"
  },
  {
    "title": "Audio-visual Saliency Prediction Model with Implicit Neural Representation",
    "doi": "https://doi.org/10.1145/3698881",
    "publication_date": "2024-10-11",
    "publication_year": 2024,
    "authors": "Nana Zhang; Min Xiong; Dandan Zhu; Kun Zhu; Guangtao Zhai; Xiaokang Yang",
    "corresponding_authors": "",
    "abstract": "With the remarkable advancement of deep learning techniques and the wide availability of large-scale datasets, the performance of audio-visual saliency prediction has been drastically improved. Actually, audio-visual saliency prediction is still at an early exploration stage due to the spatial-temporal signal complexity and dynamic continuity of video content. To our knowledge, most existing audio-visual saliency prediction approaches usually represent videos as 3D grid of RGB values using discrete convolution neural networks (CNNs), which inevitably incurs video content-agnostic and ignores the dynamic continuity issues. This paper proposes a novel parametric audio-visual saliency (PAVS) model with implicit neural representation (INR) to address the aforementioned problems. Specifically, by using the proposed parametric neural network, we can effectively encode the space-time coordinates of video frames into corresponding saliency values, which can significantly enhance the compact feature representation ability. Meanwhile, a parametric feature fusion method is developed to achieve intrinsic interactions between audio and visual information streams, which can adaptively fuse audio and visual features to obtain competitive performance. Notably, without resorting to any specific audio-visual feature fusion strategy, the proposed PAVS model outperforms other state-of-the-art saliency methods by a large margin.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403319665",
    "type": "article"
  },
  {
    "title": "Backpropagation-Free Multi-modal On-Device Model Adaptation via Cloud-Device Collaboration",
    "doi": "https://doi.org/10.1145/3706422",
    "publication_date": "2024-12-02",
    "publication_year": 2024,
    "authors": "Wei Ji; Li Li; Zheqi Lv; Wenqiao Zhang; Mengze Li; Zhen Wan; Wenqiang Lei; Roger Zimmermann",
    "corresponding_authors": "",
    "abstract": "In our increasingly interconnected world, where intelligent devices continually amass copious personalized multi-modal data, a pressing need arises to deliver high-quality, personalized device-aware services. However, this endeavor presents a multifaceted challenge to prevailing artificial intelligence (AI) systems primarily rooted in the cloud. As these systems grapple with shifting data distributions between the cloud and devices, the traditional approach of fine-tuning-based adaptation (FTA) exists the following issues: the costly and time-consuming data annotation required by FTA and the looming risk of model overfitting. To surmount these challenges, we introduce a Universal On-Device Multi-modal Model Adaptation Framework, revolutionizing on-device model adaptation by striking a balance between efficiency and effectiveness. The framework features the Fast Domain Adaptor (FDA) hosted in the cloud, providing tailored parameters for the Lightweight Multi-modal Model on devices. To enhance adaptability across multi-modal tasks, the AnchorFrame Distribution Reasoner (ADR) minimizes communication costs. Our contributions, encapsulated in the Cloud-Device Collaboration Multi-modal Parameter Generation (CDC-MMPG) framework, represent a pioneering solution for on-Device Multi-modal Model Adaptation (DMMA). Extensive experiments validate the efficiency and effectiveness of our method, particularly in video question answering and retrieval tasks, driving forward the integration of intelligent devices into our daily lives.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404923611",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on MMSys 2014 and NOSSDAV 2014",
    "doi": "https://doi.org/10.1145/2717509",
    "publication_date": "2015-02-24",
    "publication_year": 2015,
    "authors": "Kuan-Ta Chen; Songqing Chen; Wei Tsang Ooi",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W1983462846",
    "type": "article"
  },
  {
    "title": "Introduction to the special issue of best papers of ACM MMSys 2013 and ACM NOSSDAV 2013",
    "doi": "https://doi.org/10.1145/2557424",
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "Roger Zimmermann; László Böszörményi; Pål Halvorsen",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2045420500",
    "type": "article"
  },
  {
    "title": "Automated Link Generation for Sensor-Enriched Smartphone Images",
    "doi": "https://doi.org/10.1145/2808209",
    "publication_date": "2015-10-21",
    "publication_year": 2015,
    "authors": "Seshadri Padmanabha Venkatagiri; Mun Choon Chan; Wei Tsang Ooi",
    "corresponding_authors": "",
    "abstract": "The ubiquity of the smartphones makes them ideal platforms for generating in-situ content. In well-attended events, photos captured by attendees have diverse views that could be subjected to occlusion and abnormal lighting effects that could obscure the view. Such unstructured photo collections also have significant redundancy. Thus, a scene that is partially occluded or has bad contrast in one photo may be captured in another photo, possibly with higher details. We propose an application called Autolink that automatically establishes content-based links between sensor-annotated photos in unstructured photo collections captured using smartphones, such that users could navigate between high-context and high-detail images. This hierarchically structured image collection facilitates the design of applications for navigation and discovery, analytics about user photography patterns, user taste, and content/event popularity. Autolink includes a framework that constructs this hierarchy efficiently and with little content-specific training data by combining photo content processing with associated sensor logs obtained from multiple participants. We evaluated the performance of Autolink on two real-world sensor tagged photo datasets. The result shows that Autolink is able to efficiently cluster photos at 20 times faster than candidate algorithms, into the appropriate hierarchy with at least 70% precision and 37% better recall than candidate algorithms.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2082022286",
    "type": "article"
  },
  {
    "title": "Errata for",
    "doi": "https://doi.org/10.1145/2661298",
    "publication_date": "2014-10-01",
    "publication_year": 2014,
    "authors": "Bogdan Cărbunar; Rahul Potharaju; Michael Pearce; Venugopal Vasudevan; Michael Needham",
    "corresponding_authors": "",
    "abstract": "Some errors were introduced while preparing the final source files for the original article published in August 2013 in the 9,4 issue of TOMM. The errata are summarized here below together with attached revised pages showing the corrected elements indicated in red. The full CVoR (Corrected Version of Record) can be accessed in the ACM Digital Library, DOI=http://dx.doi.org/10.1145/2501643.2501652 —Page 8: New Figure 6(a) —Page 16: New Figures 8(a), 8(b), and 9(a) —Page 17: New Figure 10(b) —Page 18: New Figures 11 and 12; corrected text reference —Page 19: Final sentence deleted",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2120247170",
    "type": "article"
  },
  {
    "title": "Collaborative Annotation of Videos Relying on Weak Consistency",
    "doi": "https://doi.org/10.1145/2907983",
    "publication_date": "2016-05-24",
    "publication_year": 2016,
    "authors": "Stefan Wilk; Stephan Kopf; Wolfgang Effelsberg",
    "corresponding_authors": "",
    "abstract": "This work discusses a distributed interactive video system that supports video annotation using simultaneous hyperlinking by multiple users. The users mark and annotate objects within the video with links to other media such as text, images, websites, or other videos. Annotations are visualized on the client user interface as an overlay close to the objects. Our system is intuitive to use; for example, it contains automatic object-tracking functionality that correctly positions the annotations, even when the form or location of an object changes. Thus, our first contribution discusses the adaptive object-tracking algorithm used for this repositioning. It shows improved precision and reliability in comparison to nonadaptive algorithms. A second key issue is to keep the system responsive when the number of concurrent annotators increases. Thus, we rely on the concept of eventual consistency between different network entities. While this weak form of consistency allows temporary inconsistencies, it ensures that a consistent state can be reached. Thus, the second contribution is the design and evaluation of our distributed interactive video system, which relies on the weak consistency paradigm.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2403292523",
    "type": "article"
  },
  {
    "title": "Introduction to Special Issue MMSys/NOSSDAV 2015",
    "doi": "https://doi.org/10.1145/3003439",
    "publication_date": "2016-11-02",
    "publication_year": 2016,
    "authors": "Feng Liu; Wu‐chi Feng; Michael Zink",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2547787630",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2816987",
    "publication_date": "2015-08-24",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "This article proposes to use the relative distances between adjacent envelope peaks detected in stereo audio as fingerprints for copy identification. The matching algorithm used is the rough longest common subsequence (RLCS) algorithm. The experimental ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4232357793",
    "type": "paratext"
  },
  {
    "title": "Table of Contents",
    "doi": "https://doi.org/10.1145/2782781",
    "publication_date": "2015-06-02",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "other Share on Table of Contents: Online Supplement Volume 11, Issue 2sACM Transactions on Multimedia Computing, Communications, and ApplicationsVolume 11Issue 4April 2015 Article No.: 47pp 1https://doi.org/10.1145/2782781Published:02 June 2015Publication History 0citation71DownloadsMetricsTotal Citations0Total Downloads71Last 12 Months1Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4232634786",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2675060",
    "publication_date": "2014-10-01",
    "publication_year": 2014,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We present a hand-and-foot-based multimodal interaction approach for handheld devices. Our method combines input modalities (i.e., hand and foot) and provides a coordinated output to both modalities along with audio and video. Human foot gesture is ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4235418005",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2733235",
    "publication_date": "2015-02-05",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The motion vector similarity between neighboring blocks is widely used in motion estimation algorithms. However, for nonneighboring blocks, they may also have similar motions due to close depths or belonging to the same object inside the scene. ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4236077231",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2997658",
    "publication_date": "2016-11-18",
    "publication_year": 2016,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "User-shared images are shared on social media about a user’s life and interests that are widely accessible to others due to their sharing nature. Unlike for online profiles and social graphs, most users are unaware of the privacy risks relating to ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4237125234",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2579228",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Distributed virtual environments (DVEs) are attracting a lot of attention in recent years, due to the increasing popularity of online gaming and social networks. As the number of concurrent users of a DVE increases, a critical problem is on how the ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4241032802",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2901366",
    "publication_date": "2016-06-15",
    "publication_year": 2016,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "With the popularity of mobile devices, photo retargeting has become a useful technique that adapts a high-resolution photo onto a low-resolution screen. Conventional approaches are limited in two aspects. The first factor is the de-emphasized role of ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4242505643",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2602979",
    "publication_date": "2014-04-01",
    "publication_year": 2014,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "IPTV services deployed nowadays often consist of both live TV and Video-on-Demand (VoD), offered by the same service provider to the same pool of users over the same managed network. Understanding user behaviors in such a setting is hence an important ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4243776717",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2837041",
    "publication_date": "2016-03-03",
    "publication_year": 2016,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Three-dimensional (3D) sensing and printing technologies have reshaped our world in recent years. In this article, a comprehensive overview of techniques related to the pipeline from 3D sensing to printing is provided. We compare the latest 3D sensors ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4245460934",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2837676",
    "publication_date": "2015-10-21",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "While visualization has been widely used as a data presentation tool in both desktop and mobile devices, the rapid visualization of information from images is still underexplored. In this work, we present a smartphone image acquisition and visualization ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4247044371",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2739966",
    "publication_date": "2015-02-24",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "To support the development of any system that includes the generation and evaluation of camcorder copies, as well as to provide a common benchmark for robustness against camcorder copies, we present a tool to simulate digital video re-acquisition using ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4251203301",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3001754",
    "publication_date": "2016-12-12",
    "publication_year": 2016,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Gaming on demand is an emerging service that has recently started to garner prominence in the gaming industry. Cloud-based video games provide affordable, flexible, and high-performance solutions for end-users with constrained computing resources and ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4252168207",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2665935",
    "publication_date": "2014-08-01",
    "publication_year": 2014,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "With the rapid growth of video resources, techniques for efficient organization of video clips are becoming appealing in the multimedia domain. In this article, a sketch-based approach is proposed to intuitively organize video clips by: (1) enhancing ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4252531065",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2983297",
    "publication_date": "2016-08-24",
    "publication_year": 2016,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Online gaming franchises such as World of Tanks, Defense of the Ancients, and StarCraft have attracted hundreds of millions of users who, apart from playing the game, also socialize with each other through gaming and viewing gamecasts. As a form of User ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4253724293",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2656131",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "With the massive growth of events on the Internet, efficient organization and monitoring of events becomes a practical challenge. To deal with this problem, we propose a novel CO-PMHT (CO-Probabilistic Multi-Hypothesis Tracking) algorithm for cross-...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4253921380",
    "type": "paratext"
  },
  {
    "title": "Editorial Note",
    "doi": "https://doi.org/10.1145/2634234",
    "publication_date": "2014-08-01",
    "publication_year": 2014,
    "authors": "Ralf Steinmetz",
    "corresponding_authors": "Ralf Steinmetz",
    "abstract": "editorial Free Access Share on Editorial Note Editor: Ralf Steinmetz Multimedia Communications Lab, Technische Universität Darmstadt, Germany Multimedia Communications Lab, Technische Universität Darmstadt, GermanyView Profile Authors Info & Claims ACM Transactions on Multimedia Computing, Communications, and ApplicationsVolume 11Issue 1Article No.: 1pp 1–2https://doi.org/10.1145/2634234Published:04 September 2014Publication History 0citation340DownloadsMetricsTotal Citations0Total Downloads340Last 12 Months8Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4256627852",
    "type": "editorial"
  },
  {
    "title": "Introduction to the Special Issue on Fine-grained Visual Computing",
    "doi": "https://doi.org/10.1145/3447532",
    "publication_date": "2021-01-31",
    "publication_year": 2021,
    "authors": "Shaohua Wan; Zan Gao; Hanwang Zhang; Xiaojun Chang",
    "corresponding_authors": "",
    "abstract": "introduction Share on Introduction to the Special Issue on Fine-grained Visual Computing Editors: Shaohua Wan Zhongnan University of Economics and Law Zhongnan University of Economics and LawView Profile , Zan Gao Qilu University of Technology Qilu University of TechnologyView Profile , Hanwang Zhang Nanyang Technological University Nanyang Technological UniversityView Profile , Xiaojun Chang Monash University Monash UniversityView Profile Authors Info & Claims ACM Transactions on Multimedia Computing, Communications, and ApplicationsVolume 17Issue 1sJanuary 2021 Article No.: 11pp 1–3https://doi.org/10.1145/3447532Online:31 March 2021Publication History 0citation70DownloadsMetricsTotal Citations0Total Downloads70Last 12 Months24Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3142903815",
    "type": "article"
  },
  {
    "title": "Robust Ordinal Regression: User Credit Grading with Triplet Loss-Based Sampling",
    "doi": "https://doi.org/10.1145/3408303",
    "publication_date": "2021-01-31",
    "publication_year": 2021,
    "authors": "Jing Zhang; Jiaqi Guo; Yonggong Ren",
    "corresponding_authors": "",
    "abstract": "With the development of social media sites, user credit grading, which served as an important and fashionable problem, has attracted substantial attention from a slew of developers and operators of mobile applications. In particular, multi-grades of user credit aimed to achieve (1) anomaly detection and risk early warning and (2) personalized information and service recommendation for privileged users. The above two goals still remained as up-to-date challenges. To these ends, in this article, we propose a novel regression-based method. Technically speaking, we define three natural ordered categories including BlockList , GeneralList , and AllowList according to users’ registration and behavior information, which preserve both the global hierarchical relationship of user credit and the local coincident features of users, and hence formulate user credit grading as the ordinal regression problem. Our method is inspired by KDLOR ( kernel discriminant learning for ordinal regression ), which is an effective and efficient model to solve ordinal regression by mapping high-dimension samples to the discriminant region with supervised conditions. However, the performance of KDLOR is fragile to the extreme imbalanced distribution of users. To address this problem, we propose a robust sampling model to balance distribution and avoid overfit or underfit learning, which induces the triplet metric constraint to obtain hard negative samples that well represent the latent ordered class information. A step further, another salient problem lies in ambiguous samples that are noises or located in the classification boundary to impede optimized mapping and embedding. To this problem, we improve sampling by identifying and evading noises in triplets to obtain hard negative samples to enhance robustness and effectiveness for ordinal regression. We organized training and testing datasets for user credit grading by selecting limited items from real-life huge tables of users in the mobile application, which are used in similar problems; moreover, we theoretically and empirically demonstrate the advantages of the proposed model over established datasets.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3146881866",
    "type": "article"
  },
  {
    "title": "Introduction to Big Multimodal Multimedia Data with Deep Analytics",
    "doi": "https://doi.org/10.1145/3447530",
    "publication_date": "2021-01-31",
    "publication_year": 2021,
    "authors": "Yang Wang; Meng Fang; Joey Tianyi Zhou; Tingting Mu; Dacheng Tao",
    "corresponding_authors": "",
    "abstract": "introduction Share on Introduction to Big Multimodal Multimedia Data with Deep Analytics Authors: Yang Wang Hefei University of Technology Hefei University of TechnologyView Profile , Meng Fang Tecent AI Tecent AIView Profile , Joey Tianyi Zhou A-Star A-StarView Profile , Tingting Mu The University of Manchester The University of ManchesterView Profile , Dacheng Tao The UBTECH Sydney Artificial Intelligence Centre, the University of Sydney, Australia The UBTECH Sydney Artificial Intelligence Centre, the University of Sydney, AustraliaView Profile Authors Info & Claims ACM Transactions on Multimedia Computing, Communications, and ApplicationsVolume 17Issue 1sJanuary 2021 Article No.: 1pp 1–3https://doi.org/10.1145/3447530Online:31 March 2021Publication History 0citation212DownloadsMetricsTotal Citations0Total Downloads212Last 12 Months93Last 6 weeks4 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3152361319",
    "type": "article"
  },
  {
    "title": "Exploring Uncertainty Measures for Image-caption Embedding-and-retrieval Task",
    "doi": "https://doi.org/10.1145/3425663",
    "publication_date": "2021-04-21",
    "publication_year": 2021,
    "authors": "Kenta Hama; Takashi Matsubara; Kuniaki Uehara; Jianfei Cai",
    "corresponding_authors": "",
    "abstract": "With the significant development of black-box machine learning algorithms, particularly deep neural networks, the practical demand for reliability assessment is rapidly increasing. On the basis of the concept that “Bayesian deep learning knows what it does not know,” the uncertainty of deep neural network outputs has been investigated as a reliability measure for classification and regression tasks. By considering an embedding task as a regression task, several existing studies have quantified the uncertainty of embedded features and improved the retrieval performance of cutting-edge models by model averaging. However, in image-caption embedding-and-retrieval tasks, well-known samples are not always easy to retrieve. This study shows that the existing method has poor performance in reliability assessment and investigates another aspect of image-caption embedding-and-retrieval tasks. We propose posterior uncertainty by considering the retrieval task as a classification task, which can accurately assess the reliability of retrieval results. The consistent performance of the two uncertainty measures is observed with different datasets (MS-COCO and Flickr30k), different deep-learning architectures (dropout and batch normalization), and different similarity functions. To the best of our knowledge, this is the first study to perform a reliability assessment on image-caption embedding-and-retrieval tasks.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3157539197",
    "type": "article"
  },
  {
    "title": "Deep-based Self-refined Face-top Coordination",
    "doi": "https://doi.org/10.1145/3446970",
    "publication_date": "2021-07-22",
    "publication_year": 2021,
    "authors": "Honglin Li; Xiaoyang Mao; Mengdi Xu; Xiaogang Jin",
    "corresponding_authors": "",
    "abstract": "Face-top coordination, which exists in most clothes-fitting scenarios, is challenging due to varieties of attributes, implicit correlations, and tradeoffs between general preferences and individual preferences. We present a Deep-Based Self-Refined (DBSR) system to simulate face-top coordination based on intuition evaluation. To this end, we first establish a well-coordinated face-top (WCFT) dataset from fashion databases and communities. Then, we use a jointly trained CNN Deep Canonical Correlation Analysis (DCCA) method to bridge the semantic face-top gap based on the WCFT dataset to deal with general preferences. Subsequently, an irrelevance-based Optimum-path Forest (OPF) method is developed to adapt the results to individual preferences iteratively. Experimental results and user study demonstrate the effectiveness of our method.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3183553453",
    "type": "article"
  },
  {
    "title": "Cross-Domain Object Representation via Robust Low-Rank Correlation Analysis",
    "doi": "https://doi.org/10.1145/3458825",
    "publication_date": "2021-11-12",
    "publication_year": 2021,
    "authors": "Xiang‐Jun Shen; Jinghui Zhou; Zhongchen Ma; Bing‐Kun Bao; Zheng-Jun Zha",
    "corresponding_authors": "",
    "abstract": "Cross-domain data has become very popular recently since various viewpoints and different sensors tend to facilitate better data representation. In this article, we propose a novel cross-domain object representation algorithm (RLRCA) which not only explores the complexity of multiple relationships of variables by canonical correlation analysis (CCA) but also uses a low rank model to decrease the effect of noisy data. To the best of our knowledge, this is the first try to smoothly integrate CCA and a low-rank model to uncover correlated components across different domains and to suppress the effect of noisy or corrupted data. In order to improve the flexibility of the algorithm to address various cross-domain object representation problems, two instantiation methods of RLRCA are proposed from feature and sample space, respectively. In this way, a better cross-domain object representation can be achieved through effectively learning the intrinsic CCA features and taking full advantage of cross-domain object alignment information while pursuing low rank representations. Extensive experimental results on CMU PIE, Office-Caltech, Pascal VOC 2007, and NUS-WIDE-Object datasets, demonstrate that our designed models have superior performance over several state-of-the-art cross-domain low rank methods in image clustering and classification tasks with various corruption levels.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3213122001",
    "type": "article"
  },
  {
    "title": "Beat space segmentation and octave scale cepstral feature for sung language recognition in pop music",
    "doi": "https://doi.org/10.1145/2043612.2043615",
    "publication_date": "2011-11-01",
    "publication_year": 2011,
    "authors": "Namunu C. Maddage; Haizhou Li",
    "corresponding_authors": "",
    "abstract": "Sung language recognition relies on both effective feature extraction and acoustic modeling. In this paper, we study rhythm based music segmentation with the frame size being the duration of the smallest note in the music, as opposed to fixed length segmentation in spoken language recognition. It is found that acoustic features extracted from the rhythm based segmentation scheme outperform those from fixed length segmentation. We also study the effectiveness of a musically motivated acoustic feature. Octave scale cepstral coefficients (OSCCs) by comparing with the other acoustic features: Log frequency cepstral coefficients, Linear prediction coefficients (LPC) and LPC-derived cepstral coefficients. Finally, we examine the modeling capabilities of Gaussian mixture models and support vector machines in sung language recognition experiments. Experiments conducted on a corpus of 400 popular songs sung in English, Chinese, German, and Indonesian, showed that the OSCC feature outperforms other features. A sung language recognition accuracy of 64.9% was achieved when Gaussian mixture models were trained on shifted-delta-OSCC acoustic features, extracted via rhythm based music segmentation.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W1965073280",
    "type": "article"
  },
  {
    "title": "Introduction to the special section on the 20 <sup>th</sup> anniversary of the ACM international conference on multimedia",
    "doi": "https://doi.org/10.1145/2523001.2523003",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Klara Nahrstedt; Rainer Lienhart; Malcolm Slaney",
    "corresponding_authors": "",
    "abstract": "introduction Introduction to the special section on the 20th anniversary of the ACM international conference on multimedia Authors: Klara Nahrstedt View Profile , Rainer Lienhart View Profile , Malcolm Slaney View Profile Authors Info & Claims ACM Transactions on Multimedia Computing, Communications, and ApplicationsVolume 9Issue 1sOctober 2013 Article No.: 32pp 1–3https://doi.org/10.1145/2523001.2523003Published:17 October 2013Publication History 0citation112DownloadsMetricsTotal Citations0Total Downloads112Last 12 Months0Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2053706612",
    "type": "article"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/2501643.2501644",
    "publication_date": "2013-08-01",
    "publication_year": 2013,
    "authors": "Professor Dr.-Ing. Ralf Steinmetz",
    "corresponding_authors": "Professor Dr.-Ing. Ralf Steinmetz",
    "abstract": "editorial Free Access Share on Editorial: Reviewers Editor: Professor Dr.-Ing. Ralf Steinmetz View Profile Authors Info & Claims ACM Transactions on Multimedia Computing, Communications, and ApplicationsVolume 9Issue 4August 2013 Article No.: 22pp 1–3https://doi.org/10.1145/2501643.2501644Online:19 August 2013Publication History 0citation90DownloadsMetricsTotal Citations0Total Downloads90Last 12 Months5Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2295028572",
    "type": "editorial"
  },
  {
    "title": "Editorial notice",
    "doi": "https://doi.org/10.1145/2000486.2000487",
    "publication_date": "2011-08-01",
    "publication_year": 2011,
    "authors": "Ralf Steinmetz",
    "corresponding_authors": "Ralf Steinmetz",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2296332792",
    "type": "article"
  },
  {
    "title": "Introduction to the special section of best papers of ACM multimedia 2012",
    "doi": "https://doi.org/10.1145/2523001.2523004",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Ioannis Kompatsiaris; Wenjun Zeng; Gang Hua; Liangliang Cao",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2915925221",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2043612",
    "publication_date": "2011-11-01",
    "publication_year": 2011,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The explosive growth of Web videos brings out the challenge of how to efficiently browse hundreds or even thousands of videos at a glance. Given an event-driven query, social media Web sites usually return a large number of videos that are diverse and ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4232016171",
    "type": "paratext"
  },
  {
    "title": "Table of Contents",
    "doi": "https://doi.org/10.1145/2043612.2043620",
    "publication_date": "2011-11-01",
    "publication_year": 2011,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "research-article Share on Table of Contents: Online Supplement Volume 7S, Number 1ACM Transactions on Multimedia Computing, Communications, and ApplicationsVolume 7Issue 4November 2011 Article No.: 34pp 1https://doi.org/10.1145/2043612.2043620Online:02 December 2011Publication History 0citation109DownloadsMetricsTotal Citations0Total Downloads109Last 12 Months0Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4232882586",
    "type": "article"
  },
  {
    "title": "Call for papers",
    "doi": "https://doi.org/10.1145/2487268.2500818",
    "publication_date": "2013-06-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "research-article Share on Call for papers: Multiple sensorial (MulSeMedia) multi-modal media: Advances and applicationsACM Transactions on Multimedia Computing, Communications, and ApplicationsVolume 9Issue 3June 2013 Article No.: 15pp 1https://doi.org/10.1145/2487268.2500818Published:03 July 2013Publication History 0citation86DownloadsMetricsTotal Citations0Total Downloads86Last 12 Months0Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4237721338",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1870121",
    "publication_date": "2011-01-01",
    "publication_year": 2011,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "This article investigates a scenario of reuse in which existing learning resources serve as preliminary products for the creation of new learning resources. Authors should be able to reuse learning resources and also parts of them at different levels of ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4237958760",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2037676",
    "publication_date": "2011-10-01",
    "publication_year": 2011,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "This article presents an interactive application that enables users to improve the visual aesthetics of their digital photographs using several novel spatial recompositing techniques. This work differs from earlier efforts in two important aspects: (1) ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4243256573",
    "type": "paratext"
  },
  {
    "title": "Call for papers",
    "doi": "https://doi.org/10.1145/2487268.2499523",
    "publication_date": "2013-06-01",
    "publication_year": 2013,
    "authors": "NULL AUTHOR_ID",
    "corresponding_authors": "NULL AUTHOR_ID",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4244560130",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1925101",
    "publication_date": "2011-02-01",
    "publication_year": 2011,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Recent advances in network coding research dramatically changed the underlying structure of optimal multicast routing algorithms and made them efficiently computable. While most such algorithm design assumes a single file/layer being multicast, layered ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4249928342",
    "type": "paratext"
  },
  {
    "title": "Editorial note",
    "doi": "https://doi.org/10.1145/2523001.2523002",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Ralf Steinmetz",
    "corresponding_authors": "Ralf Steinmetz",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4250906834",
    "type": "editorial"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2501643",
    "publication_date": "2013-08-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "At present, mobile devices are prevalent with end users and continuous media streaming services in mobile ad-hoc networks (MANETs) support popular applications. It is required for applications that stream isochronous media that the network link be ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4251086932",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2457450",
    "publication_date": "2013-05-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The purpose of this research is to assess the sensitivity of humans to perceive asynchrony among media signals coming from a computer application. Particularly we examine haptic-to-video and haptic-to-audio skew. For this purpose we have designed an ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4252922135",
    "type": "paratext"
  },
  {
    "title": "Call for papers:",
    "doi": "https://doi.org/10.1145/2043612.2043619",
    "publication_date": "2011-11-01",
    "publication_year": 2011,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "research-article Share on Call for papers:: Special issue on 3D mobile multimediaACM Transactions on Multimedia Computing, Communications, and ApplicationsVolume 7Issue 4November 2011 Article No.: 41pp 1https://doi.org/10.1145/2043612.2043619Published:02 December 2011Publication History 0citation131DownloadsMetricsTotal Citations0Total Downloads131Last 12 Months0Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4255991712",
    "type": "paratext"
  },
  {
    "title": "Obituary to our dear friend professor Dr. Nicolas D. Georganas, PhD.",
    "doi": "https://doi.org/10.1145/1865106.1865107",
    "publication_date": "2010-11-01",
    "publication_year": 2010,
    "authors": "Ralf Steinmetz",
    "corresponding_authors": "Ralf Steinmetz",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W1967305136",
    "type": "article"
  },
  {
    "title": "Introduction to the special section of best papers of ACM multimedia 2011",
    "doi": "https://doi.org/10.1145/2348816.2348817",
    "publication_date": "2012-09-01",
    "publication_year": 2012,
    "authors": "Daniel Gática-Pérez; Gang Hua; Wei Tsang Ooi; Pål Halvorsen",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W1976925902",
    "type": "article"
  },
  {
    "title": "Call for papers",
    "doi": "https://doi.org/10.1145/1823746.1837254",
    "publication_date": "2010-08-01",
    "publication_year": 2010,
    "authors": "Prof. Susanne Boll; Jiebo Luo; Prof. Ramesh Jain; Prof. Dong Xu",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2008303679",
    "type": "paratext"
  },
  {
    "title": "Introduction to the best papers of ACM multimedia 2009",
    "doi": "https://doi.org/10.1145/1823746.1830482",
    "publication_date": "2010-08-01",
    "publication_year": 2010,
    "authors": "Changsheng Xu; Eckehard Steinbach; Abdulmotaleb El Saddik; Michelle X. Zhou",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2048755180",
    "type": "article"
  },
  {
    "title": "Introduction to the special section for the best papers of ACM multimedia 2008",
    "doi": "https://doi.org/10.1145/1556134.1556135",
    "publication_date": "2009-08-01",
    "publication_year": 2009,
    "authors": "K. Selçuk Candan; Alberto Del Bimbo; Carsten Griwodz; Alejandro Jaimes",
    "corresponding_authors": "",
    "abstract": "introduction Share on Introduction to the special section for the best papers of ACM multimedia 2008 Authors: K. Selçuk Candan Arizona State University, USA Arizona State University, USAView Profile , Alberto Del Bimbo Università degli Studi di Firenze, Italy Università degli Studi di Firenze, ItalyView Profile , Carsten Griwodz Simula Research Laboratory, Norway Simula Research Laboratory, NorwayView Profile , Alejandro Jaimes Telefonica Research, Spain Telefonica Research, SpainView Profile Authors Info & Claims ACM Transactions on Multimedia Computing, Communications, and ApplicationsVolume 5Issue 3August 2009 Article No.: 18pp 1–3https://doi.org/10.1145/1556134.1556135Published:14 August 2009Publication History 0citation329DownloadsMetricsTotal Citations0Total Downloads329Last 12 Months0Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2061216100",
    "type": "article"
  },
  {
    "title": "Foreword to the special issue on multimedia sensor fusion",
    "doi": "https://doi.org/10.1145/1865106.1865108",
    "publication_date": "2010-11-01",
    "publication_year": 2010,
    "authors": "Thomas Haenselmann",
    "corresponding_authors": "Thomas Haenselmann",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2083949537",
    "type": "article"
  },
  {
    "title": "Table of contents",
    "doi": "https://doi.org/10.1145/2379790.2382432",
    "publication_date": "2012-11-01",
    "publication_year": 2012,
    "authors": "TOMCCAP-STAFF",
    "corresponding_authors": "TOMCCAP-STAFF",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4205810092",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2348816",
    "publication_date": "2012-09-01",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "This article presents a psychophysical study that measures the perceptual thresholds of a new factor called Color-plus-Depth Level-of-Details (CZLoD) peculiar to polygon-based 3D tele-immersive video. The results demonstrate the existence of Just ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4232044100",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2071396",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Olfaction, or smell, is one of the last challenges which multimedia applications have to conquer. As far as computerized smell is concerned, there are several difficulties to overcome, particularly those associated with the ambient nature of smell. In ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4235991850",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1823746",
    "publication_date": "2010-08-01",
    "publication_year": 2010,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Query suggestion is an effective approach to bridge the Intention Gap between the users' search intents and queries. Most existing search engines are able to automatically suggest a list of textual query terms based on users' current query input, which ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4237806290",
    "type": "paratext"
  },
  {
    "title": "Table of Contents",
    "doi": "https://doi.org/10.1145/2168996.2169004",
    "publication_date": "2012-05-01",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "research-article Share on Table of Contents: Online Supplement Volume 8S, Number 1ACM Transactions on Multimedia Computing, Communications, and ApplicationsVolume 8Issue 2May 2012 Article No.: 16pp 1https://doi.org/10.1145/2168996.2169004Online:22 May 2012Publication History 0citation124DownloadsMetricsTotal Citations0Total Downloads124Last 12 Months1Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4238385622",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1556134",
    "publication_date": "2009-08-01",
    "publication_year": 2009,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The migration of media consumption to personal computers retains distributed social viewing, but only via nonsocial, strictly personal interfaces. This article presents an architecture, and implementation for media sharing that allows for enhanced ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4239728091",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2168996",
    "publication_date": "2012-05-01",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "This article addresses the problem of registering high-resolution, small field-of-view images with low-resolution panoramic images provided by a panoramic catadioptric video sensor. Such systems may find application in surveillance and telepresence ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4240918795",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2379790",
    "publication_date": "2012-11-01",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "In this work, we investigate how to reassign the fully annotated labels at image level to those contextually derived semantic regions, namely Label-to-Region (L2R), in a collective manner. Given a set of input images with label annotations, the basic ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4241584965",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1671954",
    "publication_date": "2010-02-01",
    "publication_year": 2010,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4241915457",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1865106",
    "publication_date": "2010-11-01",
    "publication_year": 2010,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The multimodal data usually contain complementary, correlated and redundant information. Thus, multimodal fusion is useful for many multisensor applications. Here, a novel multimodal fusion algorithm is proposed, which is referred to as “MultiFusion.” ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4244053703",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2240136",
    "publication_date": "2012-07-01",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "A lot of people around the world commute using public transportation and would like to spend this time viewing streamed video content such as news or sports updates. However, mobile wireless networks typically suffer from severe bandwidth fluctuations, ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4245040978",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2344436",
    "publication_date": "2012-09-01",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Image hatching (or nonphotorealistic line-art) is a technique widely used in the printing or engraving of currency. Diverse styles of brush strokes have previously been adopted for different areas of an image to create aesthetically pleasing textures ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4245172106",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2089085",
    "publication_date": "2012-02-01",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "With the increasing deployment of Internet P2P/overlay streaming systems, more and more clients use mobile devices, such as smart phones and PDAs, to access these Internet streaming services. Compared to wired desktops, mobile devices normally have a ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4245708852",
    "type": "paratext"
  },
  {
    "title": "Editorial note",
    "doi": "https://doi.org/10.1145/2089085.2089086",
    "publication_date": "2012-02-01",
    "publication_year": 2012,
    "authors": "Ralf Steinmetz",
    "corresponding_authors": "Ralf Steinmetz",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4247318566",
    "type": "editorial"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1596990",
    "publication_date": "2009-10-01",
    "publication_year": 2009,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Music-driven character animation extracts musical features from a song and uses them to create an animation. This article presents a system that builds a new animation directly from musical attributes, rather than simply synchronizing it to the music ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4250706439",
    "type": "paratext"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/2379790.2379791",
    "publication_date": "2012-11-01",
    "publication_year": 2012,
    "authors": "Ralf Steinmetz",
    "corresponding_authors": "Ralf Steinmetz",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4252584101",
    "type": "editorial"
  },
  {
    "title": "Guest Editorial: Special Issue on Delay-Sensitive Video Computing in the Cloud",
    "doi": null,
    "publication_date": "2017-01-01",
    "publication_year": 2017,
    "authors": "Maha Abdallah; Kuan‐Ta Chen; Carsten Griwodz; Cheng-Hsin Hsu",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2528919589",
    "type": "editorial"
  },
  {
    "title": "A Dual-Domain Perceptual Framework for Generating Visual Inconspicuous Counterparts",
    "doi": "https://doi.org/10.1145/3068427",
    "publication_date": "2017-04-26",
    "publication_year": 2017,
    "authors": "Zhuo Su; Kun Zeng; Hanhui Li; Xiaonan Luo",
    "corresponding_authors": "",
    "abstract": "For a given image, it is a challenging task to generate its corresponding counterpart with visual inconspicuous modification. The complexity of this problem reasons from the high correlativity between the editing operations and vision perception. Essentially, a significant requirement that should be emphasized is how to make the object modifications hard to be found visually in the generative counterparts. In this article, we propose a novel dual-domain perceptual framework to generate visual inconspicuous counterparts, which applies the perceptual bidirectional similarity metric (PBSM) and appearance similarity metric (ASM) to create the dual-domain perception error minimization model. The candidate targets are yielded by the well-known PatchMatch model with the strokes-based interactions and selective object library. By the dual-perceptual evaluation index, all candidate targets are sorted to select out the best result. For demonstration, a series of objective and subjective measurements are used to evaluate the performance of our framework.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2608870172",
    "type": "article"
  },
  {
    "title": "Best Papers of the 2016 ACM Multimedia Systems (MMSys) Conference and Workshop on Network and Operating System Support for Digital Audio and Video (NOSSDAV) 2016",
    "doi": "https://doi.org/10.1145/3084539",
    "publication_date": "2017-06-28",
    "publication_year": 2017,
    "authors": "Christian Timmerer; Ali C. Begen",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2726153774",
    "type": "article"
  },
  {
    "title": "A Distributed Streaming Framework for Connection Discovery Using Shared Videos",
    "doi": "https://doi.org/10.1145/3120996",
    "publication_date": "2017-09-18",
    "publication_year": 2017,
    "authors": "Xiaopeng Li; Ming Cheung; James She",
    "corresponding_authors": "",
    "abstract": "With the advances in mobile devices and the popularity of social networks, users can share multimedia content anytime, anywhere. One of the most important types of emerging content is video, which is commonly shared on platforms such as Instagram and Facebook. User connections, which indicate whether two users are follower/followee or have the same interests, are essential to improve services and information relevant to users for many social media applications. But they are normally hidden due to users’ privacy concerns or are kept confidential by social media sites. Using user-shared content is an alternative way to discover user connections. This article proposes to use user-shared videos for connection discovery with the Bag of Feature Tagging method and proposes a distributed streaming computation framework to facilitate the analytics. Exploiting the uniqueness of shared videos, the proposed framework is divided into Streaming processing and Online and Offline Computation. With experiments using a dataset from Twitter, it has been proved that the proposed method using user-shared videos for connection discovery is feasible. And the proposed computation framework significantly accelerates the analytics, reducing the processing time to only 32% for follower/followee recommendation. It has also been proved that comparable performance can be achieved with only partial data for each video and leads to more efficient computation.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2754248511",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1404880",
    "publication_date": "2008-10-01",
    "publication_year": 2008,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4231189537",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1386109",
    "publication_date": "2008-08-01",
    "publication_year": 2008,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The access of multimedia computing in wireless networks is concerned with the performance of handoff because of the irretrievable property of real-time data delivery. To lessen throughput degradation incurred by unnecessary handoffs or handoff latencies ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4233683335",
    "type": "paratext"
  },
  {
    "title": "Table of Contents",
    "doi": "https://doi.org/10.1145/3026796",
    "publication_date": "2017-01-17",
    "publication_year": 2017,
    "authors": "Zheng Yan",
    "corresponding_authors": "Zheng Yan",
    "abstract": "editorial Free Access Share on Table of Contents Author: Zheng Yan View Profile Authors Info & Claims ACM Transactions on Multimedia Computing, Communications, and ApplicationsVolume 13Issue 1February 2017 Article No.: 1epp 1–3https://doi.org/10.1145/3026796Published:17 January 2017Publication History 0citation70DownloadsMetricsTotal Citations0Total Downloads70Last 12 Months7Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4233743564",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1230812",
    "publication_date": "2007-05-01",
    "publication_year": 2007,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Video key frame extraction is one of the most important research problems for video summarization, indexing, and retrieval. For a variety of applications such as ubiquitous media access and video streaming, the temporal boundaries between video key ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4235262907",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1413862",
    "publication_date": "2008-11-01",
    "publication_year": 2008,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Surveillance applications in private environments such as smart houses require a privacy management policy if such systems are to be accepted by the occupants of the environment. This is due to the invasive nature of surveillance, and the private nature ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4235734859",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1198302",
    "publication_date": "2007-02-01",
    "publication_year": 2007,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4238581229",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1236471",
    "publication_date": "2007-08-01",
    "publication_year": 2007,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "This article investigates the correlations between multimedia objects (particularly speech and text) involved in language lectures in order to design an effective presentation mechanism for web-based learning. The cross-media correlations are classified ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4244773056",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1352012",
    "publication_date": "2008-05-01",
    "publication_year": 2008,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4249162119",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3104033",
    "publication_date": "2017-08-08",
    "publication_year": 2017,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The benefits of high-end computation infrastructure facilities provided by cloud-based multimedia systems are attracting people all around the globe. However, such cloud-based systems possess security issues as third party servers become involved in ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4249996789",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3119899",
    "publication_date": "2017-08-10",
    "publication_year": 2017,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Deep Learning (DL) has become a crucial technology for multimedia computing. It offers a powerful instrument to automatically produce high-level abstractions of complex multimedia data, which can be exploited in a number of applications, including ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4252324628",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1324287",
    "publication_date": "2008-01-01",
    "publication_year": 2008,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4253774303",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3012406",
    "publication_date": "2017-01-17",
    "publication_year": 2017,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Feature representation for visual content is the key to the progress of many fundamental applications such as annotation and cross-modal retrieval. Although recent advances in deep feature learning offer a promising route towards these tasks, they are ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4256276321",
    "type": "paratext"
  },
  {
    "title": "QuickCSGModeling: Quick CSG Operations Based on Fusing Signed Distance Fields for VR Modeling",
    "doi": "https://doi.org/10.1145/3599729",
    "publication_date": "2023-05-24",
    "publication_year": 2023,
    "authors": "Shuangmin Chen; Rui Xu; Jian Xu; Shiqing Xin; Changhe Tu; Chenglei Yang; Lin Lü",
    "corresponding_authors": "",
    "abstract": "The latest advancements in Virtual Reality (VR) enable the creation of 3D models within a holographic immersive simulation environment. In this article, we create QuickCSGModeling , a user-friendly mid-air interactive modeling system. We first prepare a dataset consisting of diverse components and precompute the discrete signed distance function (SDF) for each component. During the modeling phase, users can freely design complicated shapes with a pair of VR controllers. Based on the discrete SDF representation, any CSG-like operation (union, intersection, and subtraction) can be performed voxel-wisely. Also, we maintain a single dynamic SDF for the whole scene, whose zero-level set surface exactly encodes the most recent constructed shape. Both SDF fusion and surface extraction are implemented via GPU for a smooth user experience. A total of 34 volunteers were asked to create their favorite models using QuickCSGModeling. With a simple training, most of them can create a fascinating shape or even a descriptive scene quickly. We also discuss how to extend our system to create articulated models with hinges, where an adaptive cube subdivision has to be enforced to improve the reconstruction accuracy around the hinge part, followed by a Dual Contouring-based surface extraction. 1",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4377989138",
    "type": "article"
  },
  {
    "title": "Learning from the Past: Fast NAS for Tasks and Datasets",
    "doi": "https://doi.org/10.1145/3618000",
    "publication_date": "2023-09-01",
    "publication_year": 2023,
    "authors": "Ming Cheung",
    "corresponding_authors": "Ming Cheung",
    "abstract": "Nowadays, with the advancement of technology, many retail companies require in-house data scientist teams to build machine learning tasks, such as user segmentation and item price prediction. These teams typically use a trial-and-error process to obtain a good model for a given dataset and machine learning task, which is time-consuming and requires expertise. However, the team may have built models for other tasks on different datasets. This article proposes a framework to obtain a model architecture using the previous solved machine learning tasks and datasets. By analyzing real datasets with over 70,000 images from 11 online retail e-commerce websites, it is demonstrated that the performance of a model is related to the similarity among datasets, models, and machine learning tasks. A framework is hence proposed to obtain the model using the similarities among them. It was proven that the model was 26.6% better in accuracy, and using only 20% of the runtime while comparing to an auto network architecture search library, Auto-Keras, in predicting the attributes of fashion images. To the best of our knowledge, this is the first article to obtain the best model based on the similarity among machine learning tasks, models, and datasets.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4386396474",
    "type": "article"
  },
  {
    "title": "Boosting Diversity in Visual Search with Pareto Non-Dominated Re-Ranking",
    "doi": "https://doi.org/10.1145/3625296",
    "publication_date": "2023-09-27",
    "publication_year": 2023,
    "authors": "Si-Chao Lei; Yue‐Jiao Gong; Xiaolin Xiao; Yicong Zhou; Jun Zhang",
    "corresponding_authors": "",
    "abstract": "The field of visual search has gained significant attention recently, particularly in the context of web search engines and e-commerce product search platforms. However, the abundance of web images presents a challenge for modern image retrieval systems, as they need to find both relevant and diverse images that maximize users’ satisfaction. In response to this challenge, we propose a non-dominated visual diversity re-ranking (NDVDR) method based on the concept of Pareto optimality. To begin with, we employ a fast binary hashing method as a coarse-grained retrieval procedure. This allows us to efficiently obtain a subset of candidate images for subsequent re-ranking. Fed with this initial retrieved image results, the NDVDR performs a fine-grained re-ranking procedure for boosting both relevance and visual diversity among the top-ranked images. Recognizing the inherent conflict nature between the objectives of relevance and diversity, the re-ranking procedure is simulated as the analytical stage of a multi-criteria decision-making process, seeking the optimal tradeoff between the two conflicting objectives within the initial retrieved images. In particular, a non-dominated sorting mechanism is devised that produces Pareto non-dominated hierarchies among images based on the Pareto dominance relation. Additionally, two novel measures are introduced for the effective characterization of the relevance and diversity scores among different images. We conduct experiments on three popular real-world image datasets and compare our re-ranking method with several state-of-the-art image search re-ranking methods. The experimental results validate that our re-ranking approach guarantees retrieval accuracy while simultaneously boosting diversity among the top-ranked images.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4387105804",
    "type": "article"
  },
  {
    "title": "Editorial to Special Issue on Multimedia Cognitive Computing for Intelligent Transportation System",
    "doi": "https://doi.org/10.1145/3604938",
    "publication_date": "2023-09-27",
    "publication_year": 2023,
    "authors": "Shaohua Wan; Yi Jin; Guandong Xu; Michele Nappi",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4387106348",
    "type": "article"
  },
  {
    "title": "Sparsity-guided Discriminative Feature Encoding for Robust Keypoint Detection",
    "doi": "https://doi.org/10.1145/3628432",
    "publication_date": "2023-10-17",
    "publication_year": 2023,
    "authors": "Yurui Xie; Ling Guan",
    "corresponding_authors": "",
    "abstract": "Existing handcrafted keypoint detectors typically focus on designing specific local structures manually while ignoring whether they have enough flexibility to explore diverse visual patterns in an image. Despite the advancement of learning-based approaches in the past few years, most of them still rely on the availability of the outputs of handcrafted detectors as a part of training. In fact, such dependence limits their ability to discover various visual information. Recently, semi-handcrafted methods based on sparse coding have emerged as a promising paradigm to alleviate the above issue. However, the visual relationships between feature points have not been considered in the encoding stage, which may weaken the discriminative capability of feature representations for keypoint recognition. To tackle this problem, we propose a novel sparsity-guided discriminative feature representation (SDFR) method that attempts to explore the intrinsic correlations of keypoint candidates, thus ensuring the validity of characterizing distinctive and diverse structural information. Specifically, we first incorporate an affinity constraint into the feature representation objective, which jointly encodes all the patches in an image while highlighting the similarities and differences between them. Meanwhile, a smoother sparsity regularization with the Frobenius norm is leveraged to further preserve the similarity relationships of patch representations. Due to the differentiable property of this sparsity, SDFR is computationally feasible and effective for representing dense patches. Finally, we treat the SDFR model as multiple optimization sub-problems and introduce an iterative solver. During comprehensive evaluations on five challenging benchmarks, the proposed method achieves favorable performances compared with the state of the art in the literature.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4387698524",
    "type": "article"
  },
  {
    "title": "Contrastive Learning of View-invariant Representations for Facial Expressions Recognition",
    "doi": "https://doi.org/10.1145/3632960",
    "publication_date": "2023-11-14",
    "publication_year": 2023,
    "authors": "Shuvendu Roy; Ali Etemad",
    "corresponding_authors": "",
    "abstract": "Although there has been much progress in the area of facial expression recognition (FER), most existing methods suffer when presented with images that have been captured from viewing angles that are non-frontal and substantially different from those used in the training process. In this article, we propose ViewFX, a novel view-invariant FER framework based on contrastive learning, capable of accurately classifying facial expressions regardless of the input viewing angles during inference. ViewFX learns view-invariant features of expression using a proposed self-supervised contrastive loss, which brings together different views of the same subject with a particular expression in the embedding space. We also introduce a supervised contrastive loss to push the learned view-invariant features of each expression away from other expressions. Since facial expressions are often distinguished with very subtle differences in the learned feature space, we incorporate the Barlow twins loss to reduce the redundancy and correlations of the representations in the learned representations. The proposed method is a substantial extension of our previously proposed CL-MEx, which only had a self-supervised loss. We test the proposed framework on two public multi-view facial expression recognition datasets, KDEF and DDCF. The experiments demonstrate that our approach outperforms previous works in the area and sets a new state-of-the-art for both datasets while showing considerably less sensitivity to challenging angles and the number of output labels used for training. We also perform detailed sensitivity and ablation experiments to evaluate the impact of different components of our model as well as its sensitivity to different parameters.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4388657569",
    "type": "article"
  },
  {
    "title": "Exploring Neighbor Correspondence Matching for Multiple-Hypotheses Video Frame Synthesis",
    "doi": "https://doi.org/10.1145/3633780",
    "publication_date": "2023-11-23",
    "publication_year": 2023,
    "authors": "Zhaoyang Jia; Yan Lu; Houqiang Li",
    "corresponding_authors": "",
    "abstract": "Video frame synthesis, which consists of interpolation and extrapolation , is an essential video processing technique that can be applied to various scenarios. However, most existing methods cannot handle small objects or large motion well, especially in high-resolution videos such as 4K videos. To eliminate such limitations, we introduce a neighbor correspondence matching (NCM) algorithm for flow-based frame synthesis. Since the current frame is not available in video frame synthesis, NCM is performed in a current-frame-agnostic fashion to establish multi-scale correspondences in the spatial-temporal neighborhoods of each pixel. Based on the powerful motion representation capability of NCM, we propose a heterogeneous coarse-to-fine scheme for intermediate flow estimation. The coarse-scale and fine-scale modules are trained progressively, making NCM computationally efficient and robust to large motions. We further explore the mechanism of NCM and find that neighbor correspondence is powerful, since it provides multiple-hypotheses motion information for synthesis. Based on this analysis, we introduce a multiple-hypotheses estimation process for video frame extrapolation, resulting in a more robust framework, NCM-MH. Experimental results show that NCM and NCM-MH achieve 31.63 and 28.08 dB for interpolation and extrapolation on the most challenging X4K1000FPS benchmark, outperforming all the other state-of-the-art methods that use two reference frames as input.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4388941901",
    "type": "article"
  },
  {
    "title": "Introduction to special issue on the use of context in multimedia information systems",
    "doi": "https://doi.org/10.1145/1152149.1152150",
    "publication_date": "2006-08-01",
    "publication_year": 2006,
    "authors": "K. Selçuk Candan; Augusto Celentano; Wolfgang Klas",
    "corresponding_authors": "",
    "abstract": "introduction Share on Introduction to special issue on the use of context in multimedia information systems Authors: K. Selçuk Candan Arizona State University Arizona State UniversityView Profile , Augusto Celentano Ca' Foscari University of Venice Ca' Foscari University of VeniceView Profile , Wolfgang Klas University of Vienna University of ViennaView Profile Authors Info & Claims ACM Transactions on Multimedia Computing, Communications, and ApplicationsVolume 2Issue 3August 06 pp 173–176https://doi.org/10.1145/1152149.1152150Published:01 August 2006Publication History 0citation826DownloadsMetricsTotal Citations0Total Downloads826Last 12 Months2Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2055715480",
    "type": "article"
  },
  {
    "title": "Selected papers from the ACM multimedia conference 2003",
    "doi": "https://doi.org/10.1145/1062253.1062254",
    "publication_date": "2005-05-01",
    "publication_year": 2005,
    "authors": "Thomas Plagemann; Prashant Shenoy; John R. Smith",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2143760963",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on Representation, Analysis, and Recognition of 3D Humans",
    "doi": "https://doi.org/10.1145/3181709",
    "publication_date": "2018-03-06",
    "publication_year": 2018,
    "authors": "Stefano Berretti; Mohamed Daoudi; Pavan Turaga; Anup Basu",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2794190844",
    "type": "article"
  },
  {
    "title": "Guest Editorial",
    "doi": "https://doi.org/10.1145/3192334",
    "publication_date": "2018-04-25",
    "publication_year": 2018,
    "authors": "Yan Yan; Liqiang Nie; Rita Cucchiara",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2806640328",
    "type": "editorial"
  },
  {
    "title": "Guest Editorial",
    "doi": "https://doi.org/10.1145/3192332",
    "publication_date": "2018-04-25",
    "publication_year": 2018,
    "authors": "Lea Skorin‐Kapov; Martı́n Varela; Tobias Hoßfeld; Kuan-Ta Chen",
    "corresponding_authors": "",
    "abstract": "editorial Free Access Share on Guest Editorial: Special Issue on “QoE Management for Multimedia Services” Editors: Lea Skorin-Kapov University of Zagreb, Faculty of Electrical Engineering and Computing, Croatia University of Zagreb, Faculty of Electrical Engineering and Computing, CroatiaView Profile , Martín Varela callstats.io, Finland callstats.io, FinlandView Profile , Tobias Hoßfeld Chair of Communication Networks, Computer Science, University of Würzburg, Germany Chair of Communication Networks, Computer Science, University of Würzburg, GermanyView Profile , Kuan-Ta Chen Academia Sinica, Taiwan Academia Sinica, TaiwanView Profile Authors Info & Claims ACM Transactions on Multimedia Computing, Communications, and ApplicationsVolume 14Issue 2sApril 2018 Article No.: 28pp 1–3https://doi.org/10.1145/3192332Published:25 April 2018Publication History 5citation269DownloadsMetricsTotal Citations5Total Downloads269Last 12 Months29Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2809584482",
    "type": "editorial"
  },
  {
    "title": "Introduction to the Special Issue on Delay-Sensitive Video Computing in the Cloud",
    "doi": "https://doi.org/10.1145/3214698",
    "publication_date": "2018-06-15",
    "publication_year": 2018,
    "authors": "Maha Abdallah; Kuan‐Ta Chen; Carsten Griwodz; Cheng-Hsin Hsu",
    "corresponding_authors": "",
    "abstract": "editorial Free Access Share on Introduction to the Special Issue on Delay-Sensitive Video Computing in the Cloud Editors: Maha Abdallah Sorbonne Université, CNRS, LIP6 Sorbonne Université, CNRS, LIP6View Profile , Kuan-Ta Chen Academia Sinica Academia SinicaView Profile , Carsten Griwodz University of Oslo University of OsloView Profile , Cheng-Hsin Hsu National Tsing Hua University National Tsing Hua UniversityView Profile Authors Info & Claims ACM Transactions on Multimedia Computing, Communications, and ApplicationsVolume 14Issue 3sJune 2018 Article No.: 53pp 1–3https://doi.org/10.1145/3214698Published:15 June 2018Publication History 0citation117DownloadsMetricsTotal Citations0Total Downloads117Last 12 Months5Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2887355315",
    "type": "article"
  },
  {
    "title": "Best Papers of the ACM Multimedia Systems (MMSys) Conference 2017 and the ACM Workshop on Network and Operating System Support for Digital Audio and Video (NOSSDAV) 2017",
    "doi": "https://doi.org/10.1145/3214700",
    "publication_date": "2018-06-15",
    "publication_year": 2018,
    "authors": "Pablo César; Cheng-Hsin Hsu; Chun‐Ying Huang; Pan Hui",
    "corresponding_authors": "",
    "abstract": "editorial Free Access Share on Best Papers of the ACM Multimedia Systems (MMSys) Conference 2017 and the ACM Workshop on Network and Operating System Support for Digital Audio and Video (NOSSDAV) 2017 Editors: Pablo Cesar Centrum Wiskunde 8 Informatica Centrum Wiskunde 8 InformaticaView Profile , Cheng-Hsin Hsu National Tsing Hua University National Tsing Hua UniversityView Profile , Chun-Ying Huang National Chiao Tung University National Chiao Tung UniversityView Profile , Pan Hui University of Helsinki and Hong Kong University of Science and Technology University of Helsinki and Hong Kong University of Science and TechnologyView Profile Authors Info & Claims ACM Transactions on Multimedia Computing, Communications, and ApplicationsVolume 14Issue 3sJune 2018 Article No.: 60pp 1–3https://doi.org/10.1145/3214700Published:15 June 2018Publication History 0citation359DownloadsMetricsTotal Citations0Total Downloads359Last 12 Months59Last 6 weeks4 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2893959230",
    "type": "article"
  },
  {
    "title": "Editorial to Special Issue on Deep Learning for Intelligent Multimedia Analytics",
    "doi": "https://doi.org/10.1145/3292059",
    "publication_date": "2019-01-24",
    "publication_year": 2019,
    "authors": "Wei Zhang; Ting Yao; Shiai Zhu; Abdulmotaleb El Saddik",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2928394911",
    "type": "article"
  },
  {
    "title": "Harvesting Visual Objects from Internet Images via Deep-Learning-Based Objectness Assessment",
    "doi": "https://doi.org/10.1145/3318463",
    "publication_date": "2019-08-08",
    "publication_year": 2019,
    "authors": "Kan Wu; Guanbin Li; Haofeng Li; Jianjun Zhang; Yizhou Yu",
    "corresponding_authors": "",
    "abstract": "The collection of internet images has been growing in an astonishing speed. It is undoubted that these images contain rich visual information that can be useful in many applications, such as visual media creation and data-driven image synthesis. In this article, we focus on the methodologies for building a visual object database from a collection of internet images. Such database is built to contain a large number of high-quality visual objects that can help with various data-driven image applications. Our method is based on dense proposal generation and objectness-based re-ranking. A novel deep convolutional neural network is designed for the inference of proposal objectness , the probability of a proposal containing optimally located foreground object. In our work, the objectness is quantitatively measured in regard of completeness and fullness , reflecting two complementary features of an optimal proposal: a complete foreground and relatively small background. Our experiments indicate that object proposals re-ranked according to the output of our network generally achieve higher performance than those produced by other state-of-the-art methods. As a concrete example, a database of over 1.2 million visual objects has been built using the proposed method, and has been successfully used in various data-driven image applications.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2967144810",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on Face Analysis Applications",
    "doi": "https://doi.org/10.1145/3359624",
    "publication_date": "2019-10-15",
    "publication_year": 2019,
    "authors": "Pietro Pala; Liming Chen; Di Huang; Xiaoming Liu; Stefanos Zafeiriou",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2998311786",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on Affective Computing for Large-scale Heterogeneous Multimedia Data",
    "doi": "https://doi.org/10.1145/3365845",
    "publication_date": "2019-11-15",
    "publication_year": 2019,
    "authors": "Sicheng Zhao; Dhiraj Joshi; Mohammad Soleymani; Qiang Ji",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3007537689",
    "type": "article"
  },
  {
    "title": "大規模芸術ベンチマーク【JST・京大機械翻訳】",
    "doi": null,
    "publication_date": "2018-01-01",
    "publication_year": 2018,
    "authors": "Strezoski Gjorgji; Worring Marcel",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3081629089",
    "type": "article"
  },
  {
    "title": "音楽検索におけるオーディオとLyricsのための深いクロスモーダル相関学習【JST・京大機械翻訳】",
    "doi": null,
    "publication_date": "2019-01-01",
    "publication_year": 2019,
    "authors": "Yu Yi; Suhua Tang; Raposo Francisco; Chen Lei",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3181899832",
    "type": "article"
  },
  {
    "title": "Table of Contents",
    "doi": "https://doi.org/10.1145/3226039",
    "publication_date": "2018-05-22",
    "publication_year": 2018,
    "authors": "Stefano Berretti",
    "corresponding_authors": "Stefano Berretti",
    "abstract": "editorial Free Access Share on Table of Contents: Online Supplement Volume 14, Number 1s Author: Stefano Berretti 0000-0003-1219-4386View Profile Authors Info & Claims ACM Transactions on Multimedia Computing, Communications, and ApplicationsVolume 14Issue 2May 2018 Article No.: 42pp 1–4https://doi.org/10.1145/3226039Published:22 May 2018Publication History 5citation206DownloadsMetricsTotal Citations5Total Downloads206Last 12 Months26Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4229552824",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3339884",
    "publication_date": "2019-06-14",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Presentation has been an effective method for delivering information to an audience for many years. Over the past few decades, technological advancements have revolutionized the way humans deliver presentation. Conventionally, the quality of a ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4231397012",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1142020",
    "publication_date": "2006-05-01",
    "publication_year": 2006,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We propose a generic and robust framework for news video indexing which we founded on a broadcast news production model. We identify within this model four production phases, each providing useful metadata for annotation. In contrast to semiautomatic ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4231851107",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3241977",
    "publication_date": "2018-08-31",
    "publication_year": 2018,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4234955021",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3233173",
    "publication_date": "2018-08-09",
    "publication_year": 2018,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "While cloud servers provide a tremendous amount of resources for networked video applications, most successful stories of cloud-assisted video applications are presentational video services, such as YouTube and NetFlix. This article surveys the recent ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4237769990",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3210458",
    "publication_date": "2018-05-22",
    "publication_year": 2018,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Digital multimedia steganalysis has attracted wide attention over the past decade. Currently, there are many algorithms for detecting image steganography. However, little research has been devoted to audio steganalysis. Since the statistical properties ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4238072131",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3352586",
    "publication_date": "2019-09-18",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Stereo image retargeting plays a significant role in the field of image processing, which aims at making major objects as prominent as possible when the resolution of an image is changed, including maintaining disparity and depth information at the same ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4238264070",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3210485",
    "publication_date": "2018-05-22",
    "publication_year": 2018,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Quality of Experience (QoE) has received much attention over the past years and has become a prominent issue for delivering services and applications. A significant amount of research has been devoted to understanding, measuring, and modelling QoE for a ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4241293325",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3343360",
    "publication_date": "2019-08-12",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Visual Question Answering (VQA) is a hot-spot in the intersection of computer vision and natural language processing research and its progress has enabled many in high-level applications. This work aims to describe a novel VQA model based on semantic ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4244094529",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3368027",
    "publication_date": "2019-12-07",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Appearance variations result in many difficulties in face image analysis. To deal with this challenge, we present a Unified Tensor-based Active Appearance Model (UT-AAM) for jointly modelling the geometry and texture information of 2D faces. For each ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4244244790",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3282485",
    "publication_date": "2018-11-26",
    "publication_year": 2018,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "In this article, a two-stage refinement network is proposed for facial landmarks detection on unconstrained conditions. Our model can be divided into two modules, namely the Head Attribude Classifier (HAC) module and the Domain-Specific Refinement (DSR) ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4246601461",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1152149",
    "publication_date": "2006-08-01",
    "publication_year": 2006,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4247630250",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3309769",
    "publication_date": "2019-02-23",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The multimedia community has witnessed the rise of deep learning–based techniques in analyzing multimedia content more effectively. In the past decade, the convergence of deep-learning and multimedia analytics has boosted the performance of several ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4252084417",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1201730",
    "publication_date": "2006-11-01",
    "publication_year": 2006,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4253788260",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3376119",
    "publication_date": "2019-12-26",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "In this article, we propose a novel method to address the two-dimensional (2D) image-based 3D object retrieval problem. First, we extract a set of virtual views to represent each 3D object. Then, a soft-attention model is utilized to find the weight of ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4253954865",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3309717",
    "publication_date": "2019-02-25",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Dynamic Adaptive Streaming over HTTP (DASH) is a popular over-the-top video content distribution technique that adapts the streaming session according to the user's network condition typically in terms of downlink bandwidth. This video quality ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4254840933",
    "type": "paratext"
  },
  {
    "title": "How avatar gender may influence users intention to use Second Life environment: An empirical study",
    "doi": "https://doi.org/10.1145/3374755",
    "publication_date": "2020-05-07",
    "publication_year": 2020,
    "authors": "Zaid Bassfar",
    "corresponding_authors": "Zaid Bassfar",
    "abstract": "The use of virtual environments to transform individuals? use of modern technology has gained a special attention lately. The role of gender in the virtual space has also been viewed as a contributor to individuals? use of modern technology. This study investigated the impact of avatar gender on users? intention to use the Second Life (SL) environment in a university context. Two avatars of male and female characteristics were designed and used in the SL environment. A total of 74 SL users were involved in two learning sessions (with male female avatars). A questionnaire was used to capture users? perceptions of ease of use, usefulness, attitude, and behavioral intention to use the SL space. SL users had positive intentions to use the SL environment for various learning purposes when they are provided with the preferred gender appearance. Offering opposite gender characteristics can help stimulate users? interaction with the avatar, thus facilitating the learning process and building the sense of technology effectiveness.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3034627021",
    "type": "article"
  },
  {
    "title": "Physiological Monitoring by Fail Proof Lifetime Enhancement Algorithm for Smart Digital Environments",
    "doi": "https://doi.org/10.1145/3381087",
    "publication_date": "2020-05-07",
    "publication_year": 2020,
    "authors": "S. Malathy; Basetty Mallikarjuna; R. Maheswar; Kanagachidambaresan GR; Sundararajan. TVP; A Sampathkumar; Vigneswaran Dhasarathan",
    "corresponding_authors": "",
    "abstract": "Physiological signals monitoring in smart digital environment became ease with miniaturized embedded systems. The node loses its energy due to continuous monitoring and becomes unavailable during critical conditions. The implanted node dissipated heat and causes tissue damage when overloaded. The problem is addressed through Fail Proof Lifetime Enhancement (FPLE) Algorithm by optimally scheduling the nodes in digital environment based on the energy level and through Threshold T* policy model proposed. The objective is achieved by classifying packets based on their status and packets are transmitted towards sink once it meets a threshold value T*. A part of the energy in the sensor node is utilized during emergencies to ensure the availability of monitoring the subject during critical conditions. The subject status is realized as a Finite State Machine and evaluated through the Markov model and threshold T* based framework is adopted to enhance the lifetime of the network. The FPLE algorithm performs better than the SingleHop, MultiHop and ATTEMPT routing scheme in terms of network lifetime and throughput. The proposed FPLE algorithm provides 1.91 times lifetime and 1.1 times throughput when compared with the ATTEMPT communication protocol. The false data generation is identified and alarmed through this approach, and the FPLE algorithm also ensures monitoring during critical conditions. The implanted node is loaded least to avoid thermal dissipation and tissue damage of the subject. The FPLE algorithm is tested in real-time digital environment for its novel working, provides better results when compared to ATTEMPT, SingleHop, and MultiHop protocol.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3041295572",
    "type": "article"
  },
  {
    "title": "USER LOCATION PREDICTION USING HYPERGRAPH IMPACT FACTOR IN TWITTER WITH GLOBAL DATA COMMUNICATION",
    "doi": "https://doi.org/10.1145/3385911",
    "publication_date": "2020-05-07",
    "publication_year": 2020,
    "authors": "S. Pradeepa; K. R. Manjula",
    "corresponding_authors": "",
    "abstract": "Twitter is one of the most prominent online media that acts as a global network for sharing sensitive real-time information like earthquake alerts, political news, product review, personality identification, criminal detection etc. along with regular usage, which is why knowing the location of a user in twitter gets at most important even though they do not tend to disclose it. In this paper, we propose a technique to detect the name of the locations for the twitter users. This technique involves a hypergraph-based map-reduce concept to represent the user tweets with their locations. The Helly property of the hypergraph was used to remove less potential words and the Impact Factor measure (IF) was introduced to calculate the score of each location for a particular user. The algorithm (HIF) was implemented in a big data environment provided by Hadoop and found to give an average accuracy of 78% which is well ahead of the existing methodologies. This method gives appreciable results, with high values of precision and recall for all locations.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3042050919",
    "type": "article"
  },
  {
    "title": "Introduction to the Best Papers from the ACM Multimedia Systems (MMSys) 2019 and Co-Located Workshops",
    "doi": "https://doi.org/10.1145/3398384",
    "publication_date": "2020-04-30",
    "publication_year": 2020,
    "authors": "Michael Zink; L B De Toni; Ali C. Begen",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3044973600",
    "type": "article"
  },
  {
    "title": "マルチビュー深さ強調のための事前クロスビュー最適化フィルタの完全使用【JST・京大機械翻訳】",
    "doi": null,
    "publication_date": "2020-01-01",
    "publication_year": 2020,
    "authors": "He Xin; Qiong Liu; You Yang",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3181369342",
    "type": "article"
  },
  {
    "title": "暗号化されたYouTubeトラヒックのためのRequet実時間QoEメトリック検出【JST・京大機械翻訳】",
    "doi": null,
    "publication_date": "2020-01-01",
    "publication_year": 2020,
    "authors": "Gutterman Craig; Guo Katherine; Arora Sarthak; Gilliland Trey; Xiaoyang. Wang; Wu Les; Katz-Bassett Ethan; Gil Zussman",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3203520753",
    "type": "article"
  },
  {
    "title": "Table of Contents",
    "doi": "https://doi.org/10.1145/3386289",
    "publication_date": "2020-02-29",
    "publication_year": 2020,
    "authors": "Pietro Pala",
    "corresponding_authors": "Pietro Pala",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4255245675",
    "type": "article"
  },
  {
    "title": "Table of Contents",
    "doi": "https://doi.org/10.1145/3446798",
    "publication_date": "2020-11-30",
    "publication_year": 2020,
    "authors": "Suraj Sharma",
    "corresponding_authors": "Suraj Sharma",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4255408052",
    "type": "article"
  },
  {
    "title": "A Novel Multi-Modal Network-Based Dynamic Scene Understanding",
    "doi": "https://doi.org/10.1145/3462218",
    "publication_date": "2022-01-27",
    "publication_year": 2022,
    "authors": "Md Azher Uddin; Joolekha Bibi Joolee; Young-Koo Lee; Kyung-Ah Sohn",
    "corresponding_authors": "",
    "abstract": "In recent years, dynamic scene understanding has gained attention from researchers because of its widespread applications. The main important factor in successfully understanding the dynamic scenes lies in jointly representing the appearance and motion features to obtain an informative description. Numerous methods have been introduced to solve dynamic scene recognition problem, nevertheless, a few concerns still need to be investigated. In this article, we introduce a novel multi-modal network for dynamic scene understanding from video data, which captures both spatial appearance and temporal dynamics effectively. Furthermore, two-level joint tuning layers are proposed to integrate the global and local spatial features as well as spatial and temporal stream deep features. In order to extract the temporal information, we present a novel dynamic descriptor, namely, Volume Symmetric Gradient Local Graph Structure ( VSGLGS ), which generates temporal feature maps similar to optical flow maps. However, this approach overcomes the issues of optical flow maps. Additionally, Volume Local Directional Transition Pattern ( VLDTP ) based handcrafted spatiotemporal feature descriptor is also introduced, which extracts the directional information through exploiting edge responses. Lastly, a stacked Bidirectional Long Short-Term Memory ( Bi-LSTM ) network along with a temporal mixed pooling scheme is designed to achieve the dynamic information without noise interference. The extensive experimental investigation proves that the proposed multi-modal network outperforms most of the state-of-the-art approaches for dynamic scene understanding.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4210489963",
    "type": "article"
  },
  {
    "title": "Diversely-Supervised Visual Product Search",
    "doi": "https://doi.org/10.1145/3461646",
    "publication_date": "2022-01-27",
    "publication_year": 2022,
    "authors": "William Thong; Cees G. M. Snoek",
    "corresponding_authors": "",
    "abstract": "This article strives for a diversely supervised visual product search, where queries specify a diverse set of labels to search for. Where previous works have focused on representing attribute, instance, or category labels individually, we consider them together to create a diverse set of labels for visually describing products. We learn an embedding from the supervisory signal provided by every label to encode their interrelationships. Once trained, every label has a corresponding visual representation in the embedding space, which is an aggregation of selected items from the training set. At search time, composite query representations retrieve images that match a specific set of diverse labels. We form composite query representations by averaging over the aggregated representations of each diverse label in the specific set. For evaluation, we extend existing product datasets of cars and clothes with a diverse set of labels. Experiments show the benefits of our embedding for diversely supervised visual product search in seen and unseen product combinations and for discovering product design styles.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4210785040",
    "type": "article"
  },
  {
    "title": "Enhanced 3D Shape Reconstruction With Knowledge Graph of Category Concept",
    "doi": "https://doi.org/10.1145/3491224",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Guofei Sun; Yongkang Wong; Mohan Kankanhalli; Xiangdong Li; Weidong Geng",
    "corresponding_authors": "",
    "abstract": "Reconstructing three-dimensional (3D) objects from images has attracted increasing attention due to its wide applications in computer vision and robotic tasks. Despite the promising progress of recent deep learning–based approaches, which directly reconstruct the full 3D shape without considering the conceptual knowledge of the object categories, existing models have limited usage and usually create unrealistic shapes. 3D objects have multiple forms of representation, such as 3D volume, conceptual knowledge, and so on. In this work, we show that the conceptual knowledge for a category of objects, which represents objects as prototype volumes and is structured by graph, can enhance the 3D reconstruction pipeline. We propose a novel multimodal framework that explicitly combines graph-based conceptual knowledge with deep neural networks for 3D shape reconstruction from a single RGB image. Our approach represents conceptual knowledge of a specific category as a structure-based knowledge graph. Specifically, conceptual knowledge acts as visual priors and spatial relationships to assist the 3D reconstruction framework to create realistic 3D shapes with enhanced details. Our 3D reconstruction framework takes an image as input. It first predicts the conceptual knowledge of the object in the image, then generates a 3D object based on the input image and the predicted conceptual knowledge. The generated 3D object satisfies the following requirements: (1) it is consistent with the predicted graph in concept, and (2) consistent with the input image in geometry. Extensive experiments on public datasets (i.e., ShapeNet, Pix3D, and Pascal3D+) with 13 object categories show that (1) our method outperforms the state-of-the-art methods, (2) our prototype volume-based conceptual knowledge representation is more effective, and (3) our pipeline-agnostic approach can enhance the reconstruction quality of various 3D shape reconstruction pipelines.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4214906900",
    "type": "article"
  },
  {
    "title": "When Pairs Meet Triplets: Improving Low-Resource Captioning via Multi-Objective Optimization",
    "doi": "https://doi.org/10.1145/3492325",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Yike Wu; Shiwan Zhao; Ying Zhang; Xiaojie Yuan; Zhong Su",
    "corresponding_authors": "",
    "abstract": "Image captioning for low-resource languages has attracted much attention recently. Researchers propose to augment the low-resource caption dataset into (image, rich-resource language, and low-resource language) triplets and develop the dual attention mechanism to exploit the existence of triplets in training to improve the performance. However, datasets in triplet form are usually small due to their high collecting cost. On the other hand, there are already many large-scale datasets, which contain one pair from the triplet, such as caption datasets in the rich-resource language and translation datasets from the rich-resource language to the low-resource language. In this article, we revisit the caption-translation pipeline of the translation-based approach to utilize not only the triplet dataset but also large-scale paired datasets in training. The caption-translation pipeline is composed of two models, one caption model of the rich-resource language and one translation model from the rich-resource language to the low-resource language. Unfortunately, it is not trivial to fully benefit from incorporating both the triplet dataset and paired datasets into the pipeline, due to the gap between the training and testing phases and the instability in the training process. We propose to jointly optimize the two models of the pipeline in an end-to-end manner to bridge the training and testing gap, and introduce two auxiliary training objectives to stabilize the training process. Experimental results show that the proposed method improves significantly over the state-of-the-art methods.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4214934429",
    "type": "article"
  },
  {
    "title": "DIPS: A Dyadic Impression Prediction System for Group Interaction Videos",
    "doi": "https://doi.org/10.1145/3532865",
    "publication_date": "2022-05-06",
    "publication_year": 2022,
    "authors": "Chongyang Bai; Maksim Bolonkin; Viney Regunath; V. S. Subrahmanian",
    "corresponding_authors": "",
    "abstract": "We consider the problem of predicting the impression that one subject has of another in a video clip showing a group of interacting people. Our novel Dyadic Impression Prediction System ( DIPS ) contains two major innovations. First, we develop a novel method to align the facial expressions of subjects p i and p j as well as account for the temporal delay that might be involved in p i reacting to p j ’s facial expressions. Second, we propose the concept of a multilayered stochastic network for impression prediction on top of which we build a novel Temporal Delayed Network graph neural network architecture. Our overall DIPS architecture predicts six dependent variables relating to the impression p i has of p j . Our experiments show that DIPS beats eight baselines from the literature, yielding statistically significant improvements of 19.9% to 30.8% in AUC and 12.6% to 47.2% in F1-score. We further conduct ablation studies showing that our novel features contribute to the overall quality of the predictions made by DIPS .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4229014675",
    "type": "article"
  },
  {
    "title": "Modified 2D-Ghost-Free Stereoscopic Display with Depth-of-Field Effects",
    "doi": "https://doi.org/10.1145/3534964",
    "publication_date": "2022-05-12",
    "publication_year": 2022,
    "authors": "Jiazhi Liu; Feng Liu",
    "corresponding_authors": "",
    "abstract": "Backward-compatible stereoscopic display, a novel display technique that can simultaneously present satisfying 3D effects to viewers with stereo glasses and clear 2D contents to viewers without, aims at helping the people who are unsuitable for watching 3D movies for a long time. In this article, we introduce two versions of backward-compatible stereoscopic display: the simpler version is far simpler than Hidden Stereo (the state-of-the-art method) while preserving competitive 2D–3D effects; the advanced version, which we call 2D-Ghost-Free Stereoscopic Display, overcomes the limitation that Hidden Stereo and the simpler version are both confined to small absolute disparity. 2D-Ghost-Free Stereoscopic Display improves tolerable disparity range by adding depth-of-field in the regions with large disparity, so that it can be applied to more scenes of 3D movies. User experiments and theoretical analysis both demonstrate the superiority of the 2D-Ghost-Free Stereoscopic Display over the state-of-the-art method and our simpler version. In addition, to make the user experiments double-blind and automatic, we developed a user study system that can automatically present 3D images and videos in NVIDIA 3D Vision 2 and collect corresponding votes of subjects on stimuli, whereas the previous researchers did not state that their user experiments were double-blind.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4280570839",
    "type": "article"
  },
  {
    "title": "Context-aware Pseudo-true Video Interpolation at 6G Edge",
    "doi": "https://doi.org/10.1145/3555313",
    "publication_date": "2022-08-12",
    "publication_year": 2022,
    "authors": "Ran Li; Wei Wei; Peinan Hao; Jian Su; Fengyuan Sun",
    "corresponding_authors": "",
    "abstract": "In the 6G network, lots of edge devices facilitate the low-latency transmission of video. However, with limited processing and storage capabilities, the edge devices cannot afford to reconstruct the vast amount of video data. On the condition of edge computing in the 6G network, this article fuses a self-similarity-based context feature into Frame Rate Up-Conversion (FRUC) to generate the pseudo-true video sequences at high frame rate, and its core is the extraction of the context layer for each video frame. First, we extract the patch centered at each pixel and use the self-similarity descriptor to generate the correlation surface. Then, the expectation or skewness of the correlation surface in statistics is computed to represent its context feature. By attaching an expectation or a skewness to each pixel, the context layer is constructed and added to the video frame as a new channel. According to the context layer, we predict the motion vector field of the absent frame by using the bidirectional context match and finally produce the interpolated frame. From the experimental results, it can be seen that by deploying the proposed FRUC algorithm on edge devices, the output pseudo-true video sequences have satisfying objective and subjective qualities.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4291142487",
    "type": "article"
  },
  {
    "title": "Joint Augmented and Compressed Dictionaries for Robust Image Classification",
    "doi": "https://doi.org/10.1145/3572910",
    "publication_date": "2022-12-01",
    "publication_year": 2022,
    "authors": "Shaoning Zeng; Yunbo Rao; Bob Zhang; Yong Xu",
    "corresponding_authors": "",
    "abstract": "Dictionary-based Classification (DC) has been a promising learning theory in multimedia computing. Previous studies focused on learning a discriminative dictionary as well as the sparsest representation based on the dictionary, to cope with the complex conditions in real-world applications. However, robustness by learning only one single dictionary is far from the optimal level. What is worse, it cannot take advantage of the available techniques proven in modern machine learning, like data augmentation, to mitigate the same problem. In this work, we propose a novel method that utilizes joint Augmented and Compressed Dictionaries for Robust Dictionary-based Classification (ACD-RDC). For optimization under the noise model introduced by real-world conditions, the objective function of ACD-RDC incorporates only two simple, but well-designed constraints, including one enhanced sparsity constraint by the general data augmentation, which requires less case-by-case and sophisticated tuning, and another discriminative constraint solved by a jointly learned dictionary. The optimization of the objective function is then deduced theoretically to an approximate linear problem. The sparsity and discrimination enhanced by data augmentation guarantees the robustness for image classification under various conditions, which constructs the first positive case using data augmentation to obtain robust dictionary-based classification. Numerous experiments have been conducted on popular facial and object image datasets. The results demonstrate that ACD-RDC obtains more promising classification on diversely collected images than the current dictionary-based classification methods. ACD-RDC is also confirmed to be a state-of-the-art classification method when using deep features as inputs.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4311057953",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Section on Learning Representations, Similarity, and Associations in Dynamic Multimedia Environments",
    "doi": "https://doi.org/10.1145/3569952",
    "publication_date": "2022-06-30",
    "publication_year": 2022,
    "authors": "Xun Yang; Liang Zheng; Elisa Ricci; Meng Wang",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4313652019",
    "type": "article"
  },
  {
    "title": "Assessment of Machine Learning-Based Audiovisual Quality Predictors",
    "doi": "https://doi.org/10.1145/3430376",
    "publication_date": "2021-04-21",
    "publication_year": 2021,
    "authors": "K. Mythili; Manish Narwaria",
    "corresponding_authors": "",
    "abstract": "Quality assessment of audiovisual (AV) signals is important from the perspective of system design, optimization, and management of a modern multimedia communication system. However, automatic prediction of AV quality via the use of computational models remains challenging. In this context, machine learning (ML) appears to be an attractive alternative to the traditional approaches. This is especially when such assessment needs to be made in no-reference (i.e., the original signal is unavailable) fashion. While development of ML-based quality predictors is desirable, we argue that proper assessment and validation of such predictors is also crucial before they can be deployed in practice. To this end, we raise some fundamental questions about the current approach of ML-based model development for AV quality assessment and signal processing for multimedia communication in general. We also identify specific limitations associated with the current validation strategy which have implications on analysis and comparison of ML-based quality predictors. These include a lack of consideration of: (a) data uncertainty, (b) domain knowledge, (c) explicit learning ability of the trained model, and (d) interpretability of the resultant model. Therefore, the primary goal of this article is to shed some light into mentioned factors. Our analysis and proposed recommendations are of particular importance in the light of significant interests in ML methods for multimedia signal processing (specifically in cases where human-labeled data is used), and a lack of discussion of mentioned issues in existing literature.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3157261369",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on Explainable Deep Learning for Medical Image Computing",
    "doi": "https://doi.org/10.1145/3485046",
    "publication_date": "2021-10-26",
    "publication_year": 2021,
    "authors": "Yudong Zhang; J. M. Górriz; Zhengchao Dong",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3211114259",
    "type": "article"
  },
  {
    "title": "Bayesian Covariance Representation with Global Informative Prior for 3D Action Recognition",
    "doi": "https://doi.org/10.1145/3460235",
    "publication_date": "2021-11-12",
    "publication_year": 2021,
    "authors": "Jianhai Zhang; Zhiyong Feng; Yong Su; Meng Xing",
    "corresponding_authors": "",
    "abstract": "For the merits of high-order statistics and Riemannian geometry, covariance matrix has become a generic feature representation for action recognition. An independent action can be represented by an empirical statistics over all of its pose samples. Two major problems of covariance include the following: (1) it is prone to be singular so that actions fail to be represented properly, and (2) it is short of global action/pose-aware information so that expressive and discriminative power is limited. In this article, we propose a novel Bayesian covariance representation by a prior regularization method to solve the preceding problems. Specifically, covariance is viewed as a parametric maximum likelihood estimate of Gaussian distribution over local poses from an independent action. Then, a Global Informative Prior (GIP) is generated over global poses with sufficient statistics to regularize covariance. In this way, (1) singularity is greatly relieved due to sufficient statistics, (2) global pose information of GIP makes Bayesian covariance theoretically equivalent to a saliency weighting covariance over global action poses so that discriminative characteristics of actions can be represented more clearly. Experimental results show that our Bayesian covariance with GIP efficiently improves the performance of action recognition. In some databases, it outperforms the state-of-the-art variant methods that are based on kernels, temporal-order structures, and saliency weighting attentions, among others.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3212601227",
    "type": "article"
  },
  {
    "title": "Table of Contents: Online Supplement Volume 17, Number 2s-3s",
    "doi": "https://doi.org/10.1145/3507468",
    "publication_date": "2021-11-30",
    "publication_year": 2021,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "introduction Table of Contents: Online Supplement Volume 17, Number 2s-3sACM Transactions on Multimedia Computing, Communications, and ApplicationsVolume 17Issue 4November 2021 Article No.: 117epp 1–5https://doi.org/10.1145/3507468Published:13 January 2022Publication History 0citation56DownloadsMetricsTotal Citations0Total Downloads56Last 12 Months27Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4205906499",
    "type": "article"
  },
  {
    "title": "Table of Contents: Online Supplement Volume 17, Number 1s",
    "doi": "https://doi.org/10.1145/3457372",
    "publication_date": "2021-02-28",
    "publication_year": 2021,
    "authors": "Yan Wang",
    "corresponding_authors": "Yan Wang",
    "abstract": "editorial Free Access Share on Table of Contents: Online Supplement Volume 17, Number 1s Author: Yang Wang View Profile Authors Info & Claims ACM Transactions on Multimedia Computing, Communications, and ApplicationsVolume 17Issue 1February 2021 Article No.: 21epp 1–3https://doi.org/10.1145/3457372Published:16 April 2021Publication History 1citation115DownloadsMetricsTotal Citations1Total Downloads115Last 12 Months72Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteView all FormatsPDF",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4232591956",
    "type": "article"
  }
]