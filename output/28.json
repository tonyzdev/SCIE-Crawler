[
  {
    "title": "Temperature-aware microarchitecture",
    "doi": "https://doi.org/10.1145/980152.980157",
    "publication_date": "2004-03-01",
    "publication_year": 2004,
    "authors": "Kevin Skadron; Mircea R. Stan; Karthik Sankaranarayanan; Wei Huang; Sivakumar Velusamy; David Tarjan",
    "corresponding_authors": "",
    "abstract": "With cooling costs rising exponentially, designing cooling solutions for worst-case power dissipation is prohibitively expensive. Chips that can autonomously modify their execution and power-dissipation characteristics permit the use of lower-cost cooling solutions while still guaranteeing safe temperature regulation. Evaluating techniques for this dynamic thermal management (DTM), however, requires a thermal model that is practical for architectural studies.This paper describes HotSpot , an accurate yet fast and practical model based on an equivalent circuit of thermal resistances and capacitances that correspond to microarchitecture blocks and essential aspects of the thermal package. Validation was performed using finite-element simulation. The paper also introduces several effective methods for DTM: \"temperature-tracking\" frequency scaling, \"migrating computation\" to spare hardware units, and a \"hybrid\" policy that combines fetch gating with dynamic voltage scaling. The latter two achieve their performance advantage by exploiting instruction-level parallelism, showing the importance of microarchitecture research in helping control the growth of cooling costs.Modeling temperature at the microarchitecture level also shows that power metrics are poor predictors of temperature, that sensor imprecision has a substantial impact on the performance of DTM, and that the inclusion of lateral resistances for thermal diffusion is important for accuracy.",
    "cited_by_count": 775,
    "openalex_id": "https://openalex.org/W2152165066",
    "type": "article"
  },
  {
    "title": "CACTI 7",
    "doi": "https://doi.org/10.1145/3085572",
    "publication_date": "2017-06-28",
    "publication_year": 2017,
    "authors": "Rajeev Balasubramonian; Andrew B. Kahng; Naveen Muralimanohar; Ali Shafiee; Vaishnav Srinivas",
    "corresponding_authors": "",
    "abstract": "Historically, server designers have opted for simple memory systems by picking one of a few commoditized DDR memory products. We are already witnessing a major upheaval in the off-chip memory hierarchy, with the introduction of many new memory products—buffer-on-board, LRDIMM, HMC, HBM, and NVMs, to name a few. Given the plethora of choices, it is expected that different vendors will adopt different strategies for their high-capacity memory systems, often deviating from DDR standards and/or integrating new functionality within memory systems. These strategies will likely differ in their choice of interconnect and topology, with a significant fraction of memory energy being dissipated in I/O and data movement. To make the case for memory interconnect specialization, this paper makes three contributions. First, we design a tool that carefully models I/O power in the memory system, explores the design space, and gives the user the ability to define new types of memory interconnects/topologies. The tool is validated against SPICE models, and is integrated into version 7 of the popular CACTI package. Our analysis with the tool shows that several design parameters have a significant impact on I/O power. We then use the tool to help craft novel specialized memory system channels. We introduce a new relay-on-board chip that partitions a DDR channel into multiple cascaded channels. We show that this simple change to the channel topology can improve performance by 22% for DDR DRAM and lower cost by up to 65% for DDR DRAM. This new architecture does not require any changes to DIMMs, and it efficiently supports hybrid DRAM/NVM systems. Finally, as an example of a more disruptive architecture, we design a custom DIMM and parallel bus that moves away from the DDR3/DDR4 standards. To reduce energy and improve performance, the baseline data channel is split into three narrow parallel channels and the on-DIMM interconnects are operated at a lower frequency. In addition, this allows us to design a two-tier error protection strategy that reduces data transfers on the interconnect. This architecture yields a performance improvement of 18% and a memory power reduction of 23%. The cascaded channel and narrow channel architectures serve as case studies for the new tool and show the potential for benefit from re-organizing basic memory interconnects.",
    "cited_by_count": 368,
    "openalex_id": "https://openalex.org/W2725159389",
    "type": "article"
  },
  {
    "title": "Polyhedral parallel code generation for CUDA",
    "doi": "https://doi.org/10.1145/2400682.2400713",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Sven Verdoolaege; Juan Carlos Juega; Albert Cohen; J.I. Gomez; Christian Tenllado; Francky Catthoor",
    "corresponding_authors": "",
    "abstract": "This article addresses the compilation of a sequential program for parallel execution on a modern GPU. To this end, we present a novel source-to-source compiler called PPCG. PPCG singles out for its ability to accelerate computations from any static control loop nest, generating multiple CUDA kernels when necessary. We introduce a multilevel tiling strategy and a code generation scheme for the parallelization and locality optimization of imperfectly nested loops, managing memory and exposing concurrency according to the constraints of modern GPUs. We evaluate our algorithms and tool on the entire PolyBench suite.",
    "cited_by_count": 358,
    "openalex_id": "https://openalex.org/W2077143534",
    "type": "article"
  },
  {
    "title": "An Evaluation of High-Level Mechanistic Core Models",
    "doi": "https://doi.org/10.1145/2629677",
    "publication_date": "2014-08-25",
    "publication_year": 2014,
    "authors": "Trevor E. Carlson; Wim Heirman; Stijn Eyerman; Ibrahim Hur; Lieven Eeckhout",
    "corresponding_authors": "",
    "abstract": "Large core counts and complex cache hierarchies are increasing the burden placed on commonly used simulation and modeling techniques. Although analytical models provide fast results, they do not apply to complex, many-core shared-memory systems. In contrast, detailed cycle-level simulation can be accurate but also tends to be slow, which limits the number of configurations that can be evaluated. A middle ground is needed that provides for fast simulation of complex many-core processors while still providing accurate results. In this article, we explore, analyze, and compare the accuracy and simulation speed of high-abstraction core models as a potential solution to slow cycle-level simulation. We describe a number of enhancements to interval simulation to improve its accuracy while maintaining simulation speed. In addition, we introduce the instruction-window centric (IW-centric) core model, a new mechanistic core model that bridges the gap between interval simulation and cycle-accurate simulation by enabling high-speed simulations with higher levels of detail. We also show that using accurate core models like these are important for memory subsystem studies, and that simple, naive models, like a one-IPC core model, can lead to misleading and incorrect results and conclusions in practical design studies. Validation against real hardware shows good accuracy, with an average single-core error of 11.1% and a maximum of 18.8% for the IW-centric model with a 1.5× slowdown compared to interval simulation.",
    "cited_by_count": 300,
    "openalex_id": "https://openalex.org/W2154001575",
    "type": "article"
  },
  {
    "title": "The McPAT Framework for Multicore and Manycore Architectures",
    "doi": "https://doi.org/10.1145/2445572.2445577",
    "publication_date": "2013-04-01",
    "publication_year": 2013,
    "authors": "Sheng Li; Jung Ho Ahn; Richard Strong; Jay Brockman; Dean M. Tullsen; Norman P. Jouppi",
    "corresponding_authors": "",
    "abstract": "This article introduces McPAT, an integrated power, area, and timing modeling framework that supports comprehensive design space exploration for multicore and manycore processor configurations ranging from 90nm to 22nm and beyond. At microarchitectural level, McPAT includes models for the fundamental components of a complete chip multiprocessor, including in-order and out-of-order processor cores, networks-on-chip, shared caches, and integrated system components such as memory controllers and Ethernet controllers. At circuit level, McPAT supports detailed modeling of critical-path timing, area, and power. At technology level, McPAT models timing, area, and power for the device types forecast in the ITRS roadmap. McPAT has a flexible XML interface to facilitate its use with many performance simulators. Combined with a performance simulator, McPAT enables architects to accurately quantify the cost of new ideas and assess trade-offs of different architectures using new metrics such as Energy-Delay-Area2 Product (EDA2P) and Energy-Delay-Area Product (EDAP). This article explores the interconnect options of future manycore processors by varying the degree of clustering over generations of process technologies. Clustering will bring interesting trade-offs between area and performance because the interconnects needed to group cores into clusters incur area overhead, but many applications can make good use of them due to synergies from cache sharing. Combining power, area, and timing results of McPAT with performance simulation of PARSEC benchmarks for manycore designs at the 22nm technology shows that 8-core clustering gives the best energy-delay product, whereas when die area is taken into account, 4-core clustering gives the best EDA2P and EDAP.",
    "cited_by_count": 202,
    "openalex_id": "https://openalex.org/W2027829345",
    "type": "article"
  },
  {
    "title": "Non-monopolizable caches",
    "doi": "https://doi.org/10.1145/2086696.2086714",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Leonid Domnitser; Aamer Jaleel; Jason Loew; Nael Abu‐Ghazaleh; Dmitry Ponomarev",
    "corresponding_authors": "",
    "abstract": "We propose a flexibly-partitioned cache design that either drastically weakens or completely eliminates cache-based side channel attacks. The proposed Non-Monopolizable (NoMo) cache dynamically reserves cache lines for active threads and prevents other co-executing threads from evicting reserved lines. Unreserved lines remain available for dynamic sharing among threads. NoMo requires only simple modifications to the cache replacement logic, making it straightforward to adopt. It requires no software support enabling it to automatically protect pre-existing binaries. NoMo results in performance degradation of about 1% on average. We demonstrate that NoMo can provide strong security guarantees for the AES and Blowfish encryption algorithms.",
    "cited_by_count": 190,
    "openalex_id": "https://openalex.org/W2025539306",
    "type": "article"
  },
  {
    "title": "Design of the Java HotSpot™ client compiler for Java 6",
    "doi": "https://doi.org/10.1145/1369396.1370017",
    "publication_date": "2008-05-01",
    "publication_year": 2008,
    "authors": "Thomas Kotzmann; Christian Wimmer; Hanspeter Mössenböck; Thomas Rodriguez; K.D. Russell; David Cox",
    "corresponding_authors": "",
    "abstract": "Version 6 of Sun Microsystems' Java HotSpot™ VM ships with a redesigned version of the client just-in-time compiler that includes several research results of the last years. The client compiler is at the heart of the VM configuration used by default for interactive desktop applications. For such applications, low startup and pause times are more important than peak performance. This paper outlines the new architecture of the client compiler and shows how it interacts with the VM. It presents the intermediate representation that now uses static single-assignment (SSA) form and the linear scan algorithm for global register allocation. Efficient support for exception handling and deoptimization fulfills the demands that are imposed by the dynamic features of the Java programming language. The evaluation shows that the new client compiler generates better code in less time. The popular SPECjvm98 benchmark suite is executed 45% faster, while the compilation speed is also up to 40% better. This indicates that a carefully selected set of global optimizations can also be integrated in just-in-time compilers that focus on compilation speed and not on peak performance. In addition, the paper presents the impact of several optimizations on execution and compilation speed. As the source code is freely available, the Java HotSpot™ VM and the client compiler are the ideal basis for experiments with new feedback-directed optimizations in a production-level Java just-in-time compiler. The paper outlines research projects that add fast algorithms for escape analysis, automatic object inlining, and array bounds check elimination.",
    "cited_by_count": 174,
    "openalex_id": "https://openalex.org/W2057651724",
    "type": "article"
  },
  {
    "title": "When Prefetching Works, When It Doesn’t, and Why",
    "doi": "https://doi.org/10.1145/2133382.2133384",
    "publication_date": "2012-03-01",
    "publication_year": 2012,
    "authors": "Jaekyu Lee; Hyesoon Kim; Richard Vuduc",
    "corresponding_authors": "",
    "abstract": "In emerging and future high-end processor systems, tolerating increasing cache miss latency and properly managing memory bandwidth will be critical to achieving high performance. Prefetching, in both hardware and software, is among our most important available techniques for doing so; yet, we claim that prefetching is perhaps also the least well-understood. Thus, the goal of this study is to develop a novel, foundational understanding of both the benefits and limitations of hardware and software prefetching. Our study includes: source code-level analysis, to help in understanding the practical strengths and weaknesses of compiler- and software-based prefetching; a study of the synergistic and antagonistic effects between software and hardware prefetching; and an evaluation of hardware prefetching training policies in the presence of software prefetching requests. We use both simulation and measurement on real systems. We find, for instance, that although there are many opportunities for compilers to prefetch much more aggressively than they currently do, there is also a tangible risk of interference with training existing hardware prefetching mechanisms. Taken together, our observations suggest new research directions for cooperative hardware/software prefetching.",
    "cited_by_count": 142,
    "openalex_id": "https://openalex.org/W2036162037",
    "type": "article"
  },
  {
    "title": "Simultaneous Multi-Layer Access",
    "doi": "https://doi.org/10.1145/2832911",
    "publication_date": "2016-01-06",
    "publication_year": 2016,
    "authors": "Donghyuk Lee; Saugata Ghose; Gennady Pekhimenko; Samira Khan; Onur Mutlu",
    "corresponding_authors": "",
    "abstract": "3D-stacked DRAM alleviates the limited memory bandwidth bottleneck that exists in modern systems by leveraging through silicon vias (TSVs) to deliver higher external memory channel bandwidth. Today’s systems, however, cannot fully utilize the higher bandwidth offered by TSVs, due to the limited internal bandwidth within each layer of the 3D-stacked DRAM. We identify that the bottleneck to enabling higher bandwidth in 3D-stacked DRAM is now the global bitline interface , the connection between the DRAM row buffer and the peripheral IO circuits. The global bitline interface consists of a limited and expensive set of wires and structures, called global bitlines and global sense amplifiers , whose high cost makes it difficult to simply scale up the bandwidth of the interface within a single DRAM layer in the 3D stack. We alleviate this bandwidth bottleneck by exploiting the observation that several global bitline interfaces already exist across the multiple DRAM layers in current 3D-stacked designs, but only a fraction of them are enabled at the same time. We propose a new 3D-stacked DRAM architecture, called Simultaneous Multi-Layer Access (SMLA), which increases the internal DRAM bandwidth by accessing multiple DRAM layers concurrently, thus making much greater use of the bandwidth that the TSVs offer. To avoid channel contention, the DRAM layers must coordinate with each other when simultaneously transferring data. We propose two approaches to coordination, both of which deliver four times the bandwidth for a four-layer DRAM, over a baseline that accesses only one layer at a time. Our first approach, Dedicated-IO, statically partitions the TSVs by assigning each layer to a dedicated set of TSVs that operate at a higher frequency. Unfortunately, Dedicated-IO requires a nonuniform design for each layer (increasing manufacturing costs), and its DRAM energy consumption scales linearly with the number of layers. Our second approach, Cascaded-IO, solves both issues by instead time multiplexing all of the TSVs across layers. Cascaded-IO reduces DRAM energy consumption by lowering the operating frequency of higher layers. Our evaluations show that SMLA provides significant performance improvement and energy reduction across a variety of workloads (55%/18% on average for multiprogrammed workloads, respectively) over a baseline 3D-stacked DRAM, with low overhead.",
    "cited_by_count": 138,
    "openalex_id": "https://openalex.org/W2250217037",
    "type": "article"
  },
  {
    "title": "Fine-grained DVFS using on-chip regulators",
    "doi": "https://doi.org/10.1145/1952998.1952999",
    "publication_date": "2011-02-05",
    "publication_year": 2011,
    "authors": "Stijn Eyerman; Lieven Eeckhout",
    "corresponding_authors": "",
    "abstract": "Limit studies on Dynamic Voltage and Frequency Scaling (DVFS) provide apparently contradictory conclusions. On the one hand early limit studies report that DVFS is effective at large timescales (on the order of million(s) of cycles) with large scaling overheads (on the order of tens of microseconds), and they conclude that there is no need for small overhead DVFS at small timescales. Recent work on the other hand—motivated by the surge of on-chip voltage regulator research—explores the potential of fine-grained DVFS and reports substantial energy savings at timescales of hundreds of cycles (while assuming no scaling overhead). This article unifies these apparently contradictory conclusions through a DVFS limit study that simultaneously explores timescale and scaling speed. We find that coarse-grained DVFS is unaffected by timescale and scaling speed, however, fine-grained DVFS may lead to substantial energy savings for memory-intensive workloads. Inspired by these insights, we subsequently propose a fine-grained microarchitecture-driven DVFS mechanism that scales down voltage and frequency upon individual off-chip memory accesses using on-chip regulators. Fine-grained DVFS reduces energy consumption by 12% on average and up to 23% over a collection of memory-intensive workloads for an aggressively clock-gated processor, while incurring an average 0.08% performance degradation (and at most 0.14%). We also demonstrate that the proposed fine-grained DVFS mechanism is orthogonal to existing coarse-grained DVFS policies, and further reduces energy by 6% on average and up to 11% for memory-intensive applications with limited performance impact (at most 0.7%).",
    "cited_by_count": 119,
    "openalex_id": "https://openalex.org/W2048085362",
    "type": "article"
  },
  {
    "title": "Programming Heterogeneous Systems from an Image Processing DSL",
    "doi": "https://doi.org/10.1145/3107953",
    "publication_date": "2017-08-16",
    "publication_year": 2017,
    "authors": "Jing Pu; Steven Bell; Xuan Yang; Jeff Setter; Stephen Richardson; Jonathan Ragan‐Kelley; Mark Horowitz",
    "corresponding_authors": "",
    "abstract": "Specialized image processing accelerators are necessary to deliver the performance and energy efficiency required by important applications in computer vision, computational photography, and augmented reality. But creating, “programming,” and integrating this hardware into a hardware/software system is difficult. We address this problem by extending the image processing language Halide so users can specify which portions of their applications should become hardware accelerators, and then we provide a compiler that uses this code to automatically create the accelerator along with the “glue” code needed for the user’s application to access this hardware. Starting with Halide not only provides a very high-level functional description of the hardware but also allows our compiler to generate a complete software application, which accesses the hardware for acceleration when appropriate. Our system also provides high-level semantics to explore different mappings of applications to a heterogeneous system, including the flexibility of being able to change the throughput rate of the generated hardware. We demonstrate our approach by mapping applications to a commercial Xilinx Zynq system. Using its FPGA with two low-power ARM cores, our design achieves up to 6× higher performance and 38× lower energy compared to the quad-core ARM CPU on an NVIDIA Tegra K1, and 3.5× higher performance with 12× lower energy compared to the K1’s 192-core GPU.",
    "cited_by_count": 110,
    "openalex_id": "https://openalex.org/W2544002786",
    "type": "article"
  },
  {
    "title": "OpenStream",
    "doi": "https://doi.org/10.1145/2400682.2400712",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Antoniu Pop; Albert Cohen",
    "corresponding_authors": "",
    "abstract": "We present OpenStream, a data-flow extension of OpenMP to express dynamic dependent tasks. The language supports nested task creation, modular composition, variable and unbounded sets of producers/consumers, and first-class streams. These features, enabled by our original compilation flow, allow translating high-level parallel programming patterns, like dependences arising from StarSs' array regions, or universal low-level primitives like futures. In particular, these dynamic features can be embedded efficiently and naturally into an unmanaged imperative language, avoiding the complexity and overhead of a concurrent garbage collector. We demonstrate the performance advantages of a data-flow execution model compared to more restricted task and barrier models. We also demonstrate the efficiency of our compilation and runtime algorithms for the support of complex dependence patterns arising from StarSs benchmarks.",
    "cited_by_count": 105,
    "openalex_id": "https://openalex.org/W2009964586",
    "type": "article"
  },
  {
    "title": "DASH",
    "doi": "https://doi.org/10.1145/2847255",
    "publication_date": "2016-01-04",
    "publication_year": 2016,
    "authors": "Hiroyuki Usui; Lavanya Subramanian; Kevin K. Chang; Onur Mutlu",
    "corresponding_authors": "",
    "abstract": "Modern SoCs integrate multiple CPU cores and hardware accelerators (HWAs) that share the same main memory system, causing interference among memory requests from different agents. The result of this interference, if it is not controlled well, is missed deadlines for HWAs and low CPU performance. Few previous works have tackled this problem. State-of-the-art mechanisms designed for CPU-GPU systems strive to meet a target frame rate for GPUs by prioritizing the GPU close to the time when it has to complete a frame. We observe two major problems when such an approach is adapted to a heterogeneous CPU-HWA system. First, HWAs miss deadlines because they are prioritized only when close to their deadlines. Second, such an approach does not consider the diverse memory access characteristics of different applications running on CPUs and HWAs, leading to low performance for latency-sensitive CPU applications and deadline misses for some HWAs, including GPUs. In this article, we propose a Deadline-Aware memory Scheduler for Heterogeneous systems (DASH), which overcomes these problems using three key ideas, with the goal of meeting HWAs’ deadlines while providing high CPU performance. First, DASH prioritizes an HWA when it is not on track to meet its deadline any time during a deadline period, instead of prioritizing it only when close to a deadline. Second, DASH prioritizes HWAs over memory-intensive CPU applications based on the observation that memory-intensive applications’ performance is not sensitive to memory latency. Third, DASH treats short-deadline HWAs differently as they are more likely to miss their deadlines and schedules their requests based on worst-case memory access time estimates. Extensive evaluations across a wide variety of different workloads and systems show that DASH achieves significantly better CPU performance than the best previous scheduler while always meeting the deadlines for all HWAs, including GPUs, thereby largely improving frame rates.",
    "cited_by_count": 104,
    "openalex_id": "https://openalex.org/W2238307037",
    "type": "article"
  },
  {
    "title": "Software-controlled fault tolerance",
    "doi": "https://doi.org/10.1145/1113841.1113843",
    "publication_date": "2005-12-01",
    "publication_year": 2005,
    "authors": "George A. Reis; Jonathan Chang; Neil Vachharajani; Ram Rangan; David I. August; Shubhendu S. Mukherjee",
    "corresponding_authors": "",
    "abstract": "Traditional fault-tolerance techniques typically utilize resources ineffectively because they cannot adapt to the changing reliability and performance demands of a system. This paper proposes software-controlled fault tolerance, a concept allowing designers and users to tailor their performance and reliability for each situation. Several software-controllable fault-detection techniques are then presented: SWIFT, a software-only technique, and CRAFT, a suite of hybrid hardware/software techniques. Finally, the paper introduces PROFiT, a technique which adjusts the level of protection and performance at fine granularities through software control. When coupled with software-controllable techniques like SWIFT and CRAFT, PROFiT offers attractive and novel reliability options.",
    "cited_by_count": 160,
    "openalex_id": "https://openalex.org/W2007925061",
    "type": "article"
  },
  {
    "title": "Chlorofluoromethanes and the Stratosphere",
    "doi": null,
    "publication_date": "1977-08-01",
    "publication_year": 1977,
    "authors": "R. D. Hudson",
    "corresponding_authors": "R. D. Hudson",
    "abstract": "The conclusions of a workshop held by the National Aeronautics and Space Administration to assess the current knowledge of the impact of chlorofluoromethane release in the troposphere on stratospheric ozone concentrations. The following topics are discussed; (1) Laboratory measurements; (2) Ozone measurements and trends; (3) Minor species and aerosol measurements; (4) One dimensional modeling; and (5) Multidimensional modeling.",
    "cited_by_count": 123,
    "openalex_id": "https://openalex.org/W1630931613",
    "type": "article"
  },
  {
    "title": "An analysis of on-chip interconnection networks for large-scale chip multiprocessors",
    "doi": "https://doi.org/10.1145/1736065.1736069",
    "publication_date": "2010-04-01",
    "publication_year": 2010,
    "authors": "Daniel Sánchez; George Michelogiannakis; Christos Kozyrakis",
    "corresponding_authors": "",
    "abstract": "With the number of cores of chip multiprocessors (CMPs) rapidly growing as technology scales down, connecting the different components of a CMP in a scalable and efficient way becomes increasingly challenging. In this article, we explore the architectural-level implications of interconnection network design for CMPs with up to 128 fine-grain multithreaded cores. We evaluate and compare different network topologies using accurate simulation of the full chip, including the memory hierarchy and interconnect, and using a diverse set of scientific and engineering workloads. We find that the interconnect has a large impact on performance, as it is responsible for 60% to 75% of the miss latency. Latency, and not bandwidth, is the primary performance constraint, since, even with many threads per core and workloads with high miss rates, networks with enough bandwidth can be efficiently implemented for the system scales we consider. From the topologies we study, the flattened butterfly consistently outperforms the mesh and fat tree on all workloads, leading to performance advantages of up to 22%. We also show that considering interconnect and memory hierarchy together when designing large-scale CMPs is crucial, and neglecting either of the two can lead to incorrect conclusions. Finally, the effect of the interconnect on overall performance becomes more important as the number of cores increases, making interconnection choices especially critical when scaling up.",
    "cited_by_count": 119,
    "openalex_id": "https://openalex.org/W2029990606",
    "type": "article"
  },
  {
    "title": "Virtual machine showdown",
    "doi": "https://doi.org/10.1145/1328195.1328197",
    "publication_date": "2008-01-01",
    "publication_year": 2008,
    "authors": "Yunhe Shi; Kevin Casey; M. Anton Ertl; David Gregg",
    "corresponding_authors": "",
    "abstract": "Virtual machines (VMs) enable the distribution of programs in an architecture-neutral format, which can easily be interpreted or compiled. A long-running question in the design of VMs is whether a stack architecture or register architecture can be implemented more efficiently with an interpreter. We extend existing work on comparing virtual stack and virtual register architectures in three ways. First, our translation from stack to register code and optimization are much more sophisticated. The result is that we eliminate an average of more than 46% of executed VM instructions, with the bytecode size of the register machine being only 26% larger than that of the corresponding stack one. Second, we present a fully functional virtual-register implementation of the Java virtual machine (JVM), which supports Intel, AMD64, PowerPC and Alpha processors. This register VM supports inline-threaded, direct-threaded, token-threaded, and switch dispatch. Third, we present experimental results on a range of additional optimizations such as register allocation and elimination of redundant heap loads. On the AMD64 architecture the register machine using switch dispatch achieves an average speedup of 1.48 over the corresponding stack machine. Even using the more efficient inline-threaded dispatch, the register VM achieves a speedup of 1.15 over the equivalent stack-based VM.",
    "cited_by_count": 107,
    "openalex_id": "https://openalex.org/W2010167524",
    "type": "article"
  },
  {
    "title": "Efficient Data Mapping and Buffering Techniques for Multilevel Cell Phase-Change Memories",
    "doi": "https://doi.org/10.1145/2669365",
    "publication_date": "2014-12-08",
    "publication_year": 2014,
    "authors": "HanBin Yoon; Justin Meza; Naveen Muralimanohar; Norman P. Jouppi; Onur Mutlu",
    "corresponding_authors": "",
    "abstract": "New phase-change memory (PCM) devices have low-access latencies (like DRAM) and high capacities (i.e., low cost per bit, like Flash). In addition to being able to scale to smaller cell sizes than DRAM, a PCM cell can also store multiple bits per cell (referred to as multilevel cell, or MLC), enabling even greater capacity per bit. However, reading and writing the different bits of data from and to an MLC PCM cell requires different amounts of time: one bit is read or written first, followed by another. Due to this asymmetric access process, the bits in an MLC PCM cell have different access latency and energy depending on which bit in the cell is being read or written. We leverage this observation to design a new way to store and buffer data in MLC PCM devices. While traditional devices couple the bits in each cell next to one another in the address space, our key idea is to logically decouple the bits in each cell into two separate regions depending on their read/write characteristics: fast-read/slow-write bits and slow-read/fast-write bits. We propose a low-overhead hardware/software technique to predict and map data that would benefit from being in each region at runtime. In addition, we show how MLC bit decoupling provides more flexibility in the way data is buffered in the device, enabling more efficient use of existing device buffer space. Our evaluations for a multicore system show that MLC bit decoupling improves system performance by 19.2%, memory energy efficiency by 14.4%, and thread fairness by 19.3% over a state-of-the-art MLC PCM system that couples the bits in its cells. We show that our results are consistent across a variety of workloads and system configurations.",
    "cited_by_count": 94,
    "openalex_id": "https://openalex.org/W2027435840",
    "type": "article"
  },
  {
    "title": "A dynamic self-scheduling scheme for heterogeneous multiprocessor architectures",
    "doi": "https://doi.org/10.1145/2400682.2400716",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Mehmet E. Belviranlı; Laxmi N. Bhuyan; Rajiv Gupta",
    "corresponding_authors": "",
    "abstract": "Today's heterogeneous architectures bring together multiple general-purpose CPUs and multiple domain-specific GPUs and FPGAs to provide dramatic speedup for many applications. However, the challenge lies in utilizing these heterogeneous processors to optimize overall application performance by minimizing workload completion time. Operating system and application development for these systems is in their infancy. In this article, we propose a new scheduling and workload balancing scheme, HDSS, for execution of loops having dependent or independent iterations on heterogeneous multiprocessor systems. The new algorithm dynamically learns the computational power of each processor during an adaptive phase and then schedules the remainder of the workload using a weighted self-scheduling scheme during the completion phase. Different from previous studies, our scheme uniquely considers the runtime effects of block sizes on the performance for heterogeneous multiprocessors. It finds the right trade-off between large and small block sizes to maintain balanced workload while keeping the accelerator utilization at maximum. Our algorithm does not require offline training or architecture-specific parameters. We have evaluated our scheme on two different heterogeneous architectures: AMD 64-core Bulldozer system with nVidia Fermi C2050 GPU and Intel Xeon 32-core SGI Altix 4700 supercomputer with Xilinx Virtex 4 FPGAs. The experimental results show that our new scheduling algorithm can achieve performance improvements up to over 200% when compared to the closest existing load balancing scheme. Our algorithm also achieves full processor utilization with all processors completing at nearly the same time which is significantly better than alternative current approaches.",
    "cited_by_count": 89,
    "openalex_id": "https://openalex.org/W1993338334",
    "type": "article"
  },
  {
    "title": "Maxine",
    "doi": "https://doi.org/10.1145/2400682.2400689",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Christian Wimmer; Michael Haupt; Michael L. Van De Vanter; Mick Jordan; Laurent Daynès; Douglas N. Simon",
    "corresponding_authors": "",
    "abstract": "A highly productive platform accelerates the production of research results. The design of a Virtual Machine (VM) written in the Java™ programming language can be simplified through exploitation of interfaces, type and memory safety, automated memory management (garbage collection), exception handling, and reflection. Moreover, modern Java IDEs offer time-saving features such as refactoring, auto-completion, and code navigation. Finally, Java annotations enable compiler extensions for low-level “systems programming” while retaining IDE compatibility. These techniques collectively make complex system software more “approachable” than has been typical in the past. The Maxine VM, a metacircular Java VM implementation, has aggressively used these features since its inception. A co-designed companion tool, the Maxine Inspector, offers integrated debugging and visualization of all aspects of the VM's runtime state. The Inspector's implementation exploits advanced Java language features, embodies intimate knowledge of the VM's design, and even reuses a significant amount of VM code directly. These characteristics make Maxine a highly approachable VM research platform and a productive basis for research and teaching.",
    "cited_by_count": 86,
    "openalex_id": "https://openalex.org/W2021246880",
    "type": "article"
  },
  {
    "title": "On the evaluation of the impact of shared resources in multithreaded COTS processors in time-critical environments",
    "doi": "https://doi.org/10.1145/2086696.2086713",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Petar Radojković; Sylvain Girbal; Arnaud Grasset; Eduardo Quiñones; Sami Yehia; Francisco J. Cazorla",
    "corresponding_authors": "",
    "abstract": "Commercial Off-The-Shelf (COTS) processors are now commonly used in real-time embedded systems. The characteristics of these processors fulfill system requirements in terms of time-to-market, low cost, and high performance-per-watt ratio. However, multithreaded (MT) processors are still not widely used in real-time systems because the timing analysis is too complex. In MT processors, simultaneously-running tasks share and compete for processor resources, so the timing analysis has to estimate the possible impact that the inter-task interferences have on the execution time of the applications. In this paper, we propose a method that quantifies the slowdown that simultaneously-running tasks may experience due to collision in shared processor resources. To that end, we designed benchmarks that stress specific processor resources and we used them to (1) estimate the upper limit of a slowdown that simultaneously-running tasks may experience because of collision in different shared processor resources, and (2) quantify the sensitivity of time-critical applications to collision in these resources. We used the presented method to determine if a given MT processor is a good candidate for systems with timing requirements. We also present a case study in which the method is used to analyze three multithreaded architectures exhibiting different configurations of resource sharing. Finally, we show that measuring the slowdown that real applications experience when simultaneously-running with resource-stressing benchmarks is an important step in measurement-based timing analysis. This information is a base for incremental verification of MT COTS architectures.",
    "cited_by_count": 84,
    "openalex_id": "https://openalex.org/W2156678748",
    "type": "article"
  },
  {
    "title": "The accelerator store",
    "doi": "https://doi.org/10.1145/2086696.2086727",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Michael J. Lyons; Mark Hempstead; Gu-Yeon Wei; David Brooks",
    "corresponding_authors": "",
    "abstract": "This paper presents the many-accelerator architecture, a design approach combining the scalability of homogeneous multi-core architectures and system-on-chip's high performance and power-efficient hardware accelerators. In preparation for systems containing tens or hundreds of accelerators, we characterize a diverse pool of accelerators and find each contains significant amounts of SRAM memory (up to 90% of their area). We take advantage of this discovery and introduce the accelerator store, a scalable architectural component to minimize accelerator area by sharing its memories between accelerators. We evaluate the accelerator store for two applications and find significant system area reductions (30%) in exchange for small overheads (2% performance, 0%--8% energy). The paper also identifies new research directions enabled by the accelerator store and the many-accelerator architecture.",
    "cited_by_count": 83,
    "openalex_id": "https://openalex.org/W2047128179",
    "type": "article"
  },
  {
    "title": "MiCOMP",
    "doi": "https://doi.org/10.1145/3124452",
    "publication_date": "2017-09-06",
    "publication_year": 2017,
    "authors": "Amir H. Ashouri; Andrea Bignoli; Gianluca Palermo; Cristina Silvano; Sameer G. Kulkarni; John Cavazos",
    "corresponding_authors": "",
    "abstract": "Recent compilers offer a vast number of multilayered optimizations targeting different code segments of an application. Choosing among these optimizations can significantly impact the performance of the code being optimized. The selection of the right set of compiler optimizations for a particular code segment is a very hard problem, but finding the best ordering of these optimizations adds further complexity. Finding the best ordering represents a long standing problem in compilation research, named the phase-ordering problem. The traditional approach of constructing compiler heuristics to solve this problem simply cannot cope with the enormous complexity of choosing the right ordering of optimizations for every code segment in an application. This article proposes an automatic optimization framework we call MiCOMP, which &lt;u&gt;Mi&lt;/u&gt;tigates the &lt;u&gt;Com&lt;/u&gt;piler &lt;u&gt;P&lt;/u&gt;hase-ordering problem. We perform phase ordering of the optimizations in LLVM’s highest optimization level using optimization sub-sequences and machine learning. The idea is to cluster the optimization passes of LLVM’s O3 setting into different clusters to predict the speedup of a complete sequence of all the optimization clusters instead of having to deal with the ordering of more than 60 different individual optimizations. The predictive model uses (1) dynamic features, (2) an encoded version of the compiler sequence, and (3) an exploration heuristic to tackle the problem. Experimental results using the LLVM compiler framework and the Cbench suite show the effectiveness of the proposed clustering and encoding techniques to application-based reordering of passes, while using a number of predictive models. We perform statistical analysis on the results and compare against (1) random iterative compilation, (2) standard optimization levels, and (3) two recent prediction approaches. We show that MiCOMP’s iterative compilation using its sub-sequences can reach an average performance speedup of 1.31 (up to 1.51). Additionally, we demonstrate that MiCOMP’s prediction model outperforms the -O1, -O2, and -O3 optimization levels within using just a few predictions and reduces the prediction error rate down to only 5%. Overall, it achieves 90% of the available speedup by exploring less than 0.001% of the optimization space.",
    "cited_by_count": 81,
    "openalex_id": "https://openalex.org/W2751901133",
    "type": "article"
  },
  {
    "title": "COBAYN",
    "doi": "https://doi.org/10.1145/2928270",
    "publication_date": "2016-06-14",
    "publication_year": 2016,
    "authors": "Amir H. Ashouri; Giovanni Mariani; Gianluca Palermo; Eun Park; John Cavazos; Cristina Silvano",
    "corresponding_authors": "",
    "abstract": "The variety of today’s architectures forces programmers to spend a great deal of time porting and tuning application codes across different platforms. Compilers themselves need additional tuning, which has considerable complexity as the standard optimization levels, usually designed for the average case and the specific target architecture, often fail to bring the best results. This article proposes COBAYN : Compiler autotuning framework using BAYesian Networks, an approach for a compiler autotuning methodology using machine learning to speed up application performance and to reduce the cost of the compiler optimization phases. The proposed framework is based on the application characterization done dynamically by using independent microarchitecture features and Bayesian networks. The article also presents an evaluation based on using static analysis and hybrid feature collection approaches. In addition, the article compares Bayesian networks with respect to several state-of-the-art machine-learning models. Experiments were carried out on an ARM embedded platform and GCC compiler by considering two benchmark suites with 39 applications. The set of compiler configurations, selected by the model (less than 7% of the search space), demonstrated an application performance speedup of up to 4.6 × on Polybench (1.85 × on average) and 3.1 × on cBench (1.54 × on average) with respect to standard optimization levels. Moreover, the comparison of the proposed technique with (i) random iterative compilation, (ii) machine learning--based iterative compilation, and (iii) noniterative predictive modeling techniques shows, on average, 1.2 × , 1.37 × , and 1.48 × speedup, respectively. Finally, the proposed method demonstrates 4 × and 3 × speedup, respectively, on cBench and Polybench in terms of exploration efficiency given the same quality of the solutions generated by the random iterative compilation model.",
    "cited_by_count": 80,
    "openalex_id": "https://openalex.org/W2441512324",
    "type": "article"
  },
  {
    "title": "TLB Improvements for Chip Multiprocessors",
    "doi": "https://doi.org/10.1145/2445572.2445574",
    "publication_date": "2013-04-01",
    "publication_year": 2013,
    "authors": "Daniel Lustig; Abhishek Bhattacharjee; Margaret Martonosi",
    "corresponding_authors": "",
    "abstract": "Translation Lookaside Buffers (TLBs) are critical to overall system performance. Much past research has addressed uniprocessor TLBs, lowering access times and miss rates. However, as Chip MultiProcessors (CMPs) become ubiquitous, TLB design and performance must be reevaluated. Our article begins by performing a thorough TLB performance evaluation of sequential and parallel benchmarks running on a real-world, modern CMP system using hardware performance counters. This analysis demonstrates the need for further improvement of TLB hit rates for both classes of application, and it also points out that the data TLB has a significantly higher miss rate than the instruction TLB in both cases. In response to the characterization data, we propose and evaluate both Inter-Core Cooperative (ICC) TLB prefetchers and Shared Last-Level (SLL) TLBs as alternatives to the commercial norm of private, per-core L2 TLBs. ICC prefetchers eliminate 19% to 90% of Data TLB (D-TLB) misses across parallel workloads while requiring only modest changes in hardware. SLL TLBs eliminate 7% to 79% of D-TLB misses for parallel workloads and 35% to 95% of D-TLB misses for multiprogrammed sequential workloads. This corresponds to 27% and 21% increases in hit rates as compared to private, per-core L2 TLBs, respectively, and is achieved this using even more modest hardware requirements. Because of their benefits for parallel applications, their applicability to sequential workloads, and their readily implementable hardware, SLL TLBs and ICC TLB prefetchers hold great promise for CMPs.",
    "cited_by_count": 75,
    "openalex_id": "https://openalex.org/W2156703074",
    "type": "article"
  },
  {
    "title": "Understanding and Mitigating Covert Channels Through Branch Predictors",
    "doi": "https://doi.org/10.1145/2870636",
    "publication_date": "2016-03-07",
    "publication_year": 2016,
    "authors": "Dmitry Evtyushkin; Dmitry Ponomarev; Nael Abu‐Ghazaleh",
    "corresponding_authors": "",
    "abstract": "Covert channels through shared processor resources provide secret communication between two malicious processes: the trojan and the spy. In this article, we classify, analyze, and compare covert channels through dynamic branch prediction units in modern processors. Through experiments on a real hardware platform, we compare contention-based channel and the channel that is based on exploiting the branch predictor’s residual state. We analyze these channels in SMT and single-threaded environments under both clean and noisy conditions. Our results show that the residual state-based channel provides a cleaner signal and is effective even in noisy execution environments with another application sharing the same physical core with the trojan and the spy. We also estimate the capacity of the branch predictor covert channels and describe a software-only mitigation technique that is based on randomizing the state of the predictor tables on context switches. We show that this protection eliminates all covert channels through the branch prediction unit with minimal impact on performance.",
    "cited_by_count": 75,
    "openalex_id": "https://openalex.org/W2299561166",
    "type": "article"
  },
  {
    "title": "Integrating profile-driven parallelism detection and machine-learning-based mapping",
    "doi": "https://doi.org/10.1145/2579561",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "Zheng Wang; Georgios Tournavitis; Björn Franke; Michael O’Boyle",
    "corresponding_authors": "",
    "abstract": "Compiler-based auto-parallelization is a much-studied area but has yet to find widespread application. This is largely due to the poor identification and exploitation of application parallelism, resulting in disappointing performance far below that which a skilled expert programmer could achieve. We have identified two weaknesses in traditional parallelizing compilers and propose a novel, integrated approach resulting in significant performance improvements of the generated parallel code. Using profile-driven parallelism detection, we overcome the limitations of static analysis, enabling the identification of more application parallelism, and only rely on the user for final approval. We then replace the traditional target-specific and inflexible mapping heuristics with a machine-learning-based prediction mechanism, resulting in better mapping decisions while automating adaptation to different target architectures. We have evaluated our parallelization strategy on the NAS and SPEC CPU2000 benchmarks and two different multicore platforms (dual quad-core Intel Xeon SMP and dual-socket QS20 Cell blade). We demonstrate that our approach not only yields significant improvements when compared with state-of-the-art parallelizing compilers but also comes close to and sometimes exceeds the performance of manually parallelized codes. On average, our methodology achieves 96% of the performance of the hand-tuned OpenMP NAS and SPEC parallel benchmarks on the Intel Xeon platform and gains a significant speedup for the IBM Cell platform, demonstrating the potential of profile-guided and machine-learning- based parallelization for complex multicore platforms.",
    "cited_by_count": 71,
    "openalex_id": "https://openalex.org/W1987564528",
    "type": "article"
  },
  {
    "title": "RFVP",
    "doi": "https://doi.org/10.1145/2836168",
    "publication_date": "2016-01-06",
    "publication_year": 2016,
    "authors": "Amir Yazdanbakhsh; Gennady Pekhimenko; Bradley Thwaites; Hadi Esmaeilzadeh; Onur Mutlu; Todd C. Mowry",
    "corresponding_authors": "",
    "abstract": "This article aims to tackle two fundamental memory bottlenecks: limited off-chip bandwidth (bandwidth wall) and long access latency (memory wall). To achieve this goal, our approach exploits the inherent error resilience of a wide range of applications. We introduce an approximation technique, called Rollback-Free Value Prediction (RFVP). When certain safe-to-approximate load operations miss in the cache, RFVP predicts the requested values. However, RFVP does not check for or recover from load-value mispredictions, hence, avoiding the high cost of pipeline flushes and re-executions. RFVP mitigates the memory wall by enabling the execution to continue without stalling for long-latency memory accesses. To mitigate the bandwidth wall, RFVP drops a fraction of load requests that miss in the cache after predicting their values. Dropping requests reduces memory bandwidth contention by removing them from the system. The drop rate is a knob to control the trade-off between performance/energy efficiency and output quality. Our extensive evaluations show that RFVP, when used in GPUs, yields significant performance improvement and energy reduction for a wide range of quality-loss levels. We also evaluate RFVP’s latency benefits for a single core CPU. The results show performance improvement and energy reduction for a wide variety of applications with less than 1% loss in quality.",
    "cited_by_count": 70,
    "openalex_id": "https://openalex.org/W2100799353",
    "type": "article"
  },
  {
    "title": "IR2V <scp>EC</scp>",
    "doi": "https://doi.org/10.1145/3418463",
    "publication_date": "2020-12-22",
    "publication_year": 2020,
    "authors": "S. VenkataKeerthy; Rohit Aggarwal; Shalini Jain; Maunendra Sankar Desarkar; Ramakrishna Upadrasta; Y. N. Srikant",
    "corresponding_authors": "",
    "abstract": "We propose IR2V EC , a Concise and Scalable encoding infrastructure to represent programs as a distributed embedding in continuous space. This distributed embedding is obtained by combining representation learning methods with flow information to capture the syntax as well as the semantics of the input programs. As our infrastructure is based on the Intermediate Representation (IR) of the source code, obtained embeddings are both language and machine independent. The entities of the IR are modeled as relationships, and their representations are learned to form a seed embedding vocabulary . Using this infrastructure, we propose two incremental encodings: Symbolic and Flow-Aware . Symbolic encodings are obtained from the seed embedding vocabulary , and Flow-Aware encodings are obtained by augmenting the Symbolic encodings with the flow information. We show the effectiveness of our methodology on two optimization tasks (Heterogeneous device mapping and Thread coarsening). Our way of representing the programs enables us to use non-sequential models resulting in orders of magnitude of faster training time. Both the encodings generated by IR2V EC outperform the existing methods in both the tasks, even while using simple machine learning models. In particular, our results improve or match the state-of-the-art speedup in 11/14 benchmark-suites in the device mapping task across two platforms and 53/68 benchmarks in the thread coarsening task across four different platforms. When compared to the other methods, our embeddings are more scalable , is non-data-hungry , and has better Out-Of-Vocabulary (OOV) characteristics .",
    "cited_by_count": 67,
    "openalex_id": "https://openalex.org/W3116350821",
    "type": "article"
  },
  {
    "title": "A Reconfiguration Algorithm for Power-Aware Parallel Applications",
    "doi": "https://doi.org/10.1145/3004054",
    "publication_date": "2016-12-02",
    "publication_year": 2016,
    "authors": "Daniele De Sensi; Massimo Torquati; Marco Danelutto",
    "corresponding_authors": "",
    "abstract": "In current computing systems, many applications require guarantees on their maximum power consumption to not exceed the available power budget. On the other hand, for some applications, it could be possible to decrease their performance, yet maintain an acceptable level, in order to reduce their power consumption. To provide such guarantees, a possible solution consists in changing the number of cores assigned to the application, their clock frequency, and the placement of application threads over the cores. However, power consumption and performance have different trends depending on the application considered and on its input. Finding a configuration of resources satisfying user requirements is, in the general case, a challenging task. In this article, we propose N ornir , an algorithm to automatically derive, without relying on historical data about previous executions, performance and power consumption models of an application in different configurations. By using these models, we are able to select a close-to-optimal configuration for the given user requirement, either performance or power consumption. The configuration of the application will be changed on-the-fly throughout the execution to adapt to workload fluctuations, external interferences, and/or application’s phase changes. We validate the algorithm by simulating it over the applications of the P arsec benchmark suit. Then, we implement our algorithm and we analyse its accuracy and overhead over some of these applications on a real execution environment. Eventually, we compare the quality of our proposal with that of the optimal algorithm and of some state-of-the-art solutions.",
    "cited_by_count": 66,
    "openalex_id": "https://openalex.org/W2559048697",
    "type": "article"
  },
  {
    "title": "Clustering-Based Selection for the Exploration of Compiler Optimization Sequences",
    "doi": "https://doi.org/10.1145/2883614",
    "publication_date": "2016-03-28",
    "publication_year": 2016,
    "authors": "Luiz G. A. Martins; Ricardo Nobre; João M. P. Cardoso; Alexandre C. B. Delbem; Eduardo Marques",
    "corresponding_authors": "",
    "abstract": "A large number of compiler optimizations are nowadays available to users. These optimizations interact with each other and with the input code in several and complex ways. The sequence of application of optimization passes can have a significant impact on the performance achieved. The effect of the optimizations is both platform and application dependent. The exhaustive exploration of all viable sequences of compiler optimizations for a given code fragment is not feasible. As this exploration is a complex and time-consuming task, several researchers have focused on Design Space Exploration (DSE) strategies both to select optimization sequences to improve the performance of each function of the application and to reduce the exploration time. In this article, we present a DSE scheme based on a clustering approach for grouping functions with similarities and exploration of a reduced search space resulting from the combination of optimizations previously suggested for the functions in each group. The identification of similarities between functions uses a data mining method that is applied to a symbolic code representation. The data mining process combines three algorithms to generate clusters: the Normalized Compression Distance, the Neighbor Joining, and a new ambiguity-based clustering algorithm. Our experiments for evaluating the effectiveness of the proposed approach address the exploration of optimization sequences in the context of the ReflectC compiler, considering 49 compilation passes while targeting a Xilinx MicroBlaze processor, and aiming at performance improvements for 51 functions and four applications. Experimental results reveal that the use of our clustering-based DSE approach achieves a significant reduction in the total exploration time of the search space (20× over a Genetic Algorithm approach) at the same time that considerable performance speedups (41% over the baseline) were obtained using the optimized codes. Additional experiments were performed considering the LLVM compiler, considering 124 compilation passes, and targeting a LEON3 processor. The results show that our approach achieved geometric mean speedups of 1.49 × , 1.32 × , and 1.24 × for the best 10, 20, and 30 functions, respectively, and a global improvement of 7% over the performance obtained when compiling with -O2.",
    "cited_by_count": 64,
    "openalex_id": "https://openalex.org/W2330621167",
    "type": "article"
  },
  {
    "title": "Hardware Performance Counter-Based Malware Identification and Detection with Adaptive Compressive Sensing",
    "doi": "https://doi.org/10.1145/2857055",
    "publication_date": "2016-03-28",
    "publication_year": 2016,
    "authors": "Xueyang Wang; Sek Chai; Michael Isnardi; Sehoon Lim; Ramesh Karri",
    "corresponding_authors": "",
    "abstract": "Hardware Performance Counter-based (HPC) runtime checking is an effective way to identify malicious behaviors of malware and detect malicious modifications to a legitimate program’s control flow. To reduce the overhead in the monitored system which has limited storage and computing resources, we present a “sample-locally-analyze-remotely” technique. The sampled HPC data are sent to a remote server for further analysis. To minimize the I/O bandwidth required for transmission, the fine-grained HPC profiles are compressed into much smaller vectors with Compressive Sensing. The experimental results demonstrate an 80% I/O bandwidth reduction after applying Compressive Sensing, without compromising the detection and identification capabilities.",
    "cited_by_count": 63,
    "openalex_id": "https://openalex.org/W2319159802",
    "type": "article"
  },
  {
    "title": "SMAUG",
    "doi": "https://doi.org/10.1145/3424669",
    "publication_date": "2020-11-10",
    "publication_year": 2020,
    "authors": "Sam Likun Xi; Yuan Yao; Kshitij Bhardwaj; Paul N. Whatmough; Gu-Yeon Wei; David Brooks",
    "corresponding_authors": "",
    "abstract": "In recent years, there has been tremendous advances in hardware acceleration of deep neural networks. However, most of the research has focused on optimizing accelerator microarchitecture for higher performance and energy efficiency on a per-layer basis. We find that for overall single-batch inference latency, the accelerator may only make up 25–40%, with the rest spent on data movement and in the deep learning software framework. Thus far, it has been very difficult to study end-to-end DNN performance during early stage design (before RTL is available), because there are no existing DNN frameworks that support end-to-end simulation with easy custom hardware accelerator integration. To address this gap in research infrastructure, we present SMAUG, the first DNN framework that is purpose-built for simulation of end-to-end deep learning applications. SMAUG offers researchers a wide range of capabilities for evaluating DNN workloads, from diverse network topologies to easy accelerator modeling and SoC integration. To demonstrate the power and value of SMAUG, we present case studies that show how we can optimize overall performance and energy efficiency for up to 1.8×–5× speedup over a baseline system, without changing any part of the accelerator microarchitecture, as well as show how SMAUG can tune an SoC for a camera-powered deep learning pipeline.",
    "cited_by_count": 54,
    "openalex_id": "https://openalex.org/W3102175148",
    "type": "article"
  },
  {
    "title": "Domain-Specific Multi-Level IR Rewriting for GPU",
    "doi": "https://doi.org/10.1145/3469030",
    "publication_date": "2021-09-03",
    "publication_year": 2021,
    "authors": "Tobias Gysi; Christoph Müller; Oleksandr Zinenko; Stephan Herhut; Eddie C. Davis; Tobias Wicky; Oliver Fuhrer; Torsten Hoefler; Tobias Grosser",
    "corresponding_authors": "",
    "abstract": "Most compilers have a single core intermediate representation (IR) (e.g., LLVM) sometimes complemented with vaguely defined IR-like data structures. This IR is commonly low-level and close to machine instructions. As a result, optimizations relying on domain-specific information are either not possible or require complex analysis to recover the missing information. In contrast, multi-level rewriting instantiates a hierarchy of dialects (IRs), lowers programs level-by-level, and performs code transformations at the most suitable level. We demonstrate the effectiveness of this approach for the weather and climate domain. In particular, we develop a prototype compiler and design stencil- and GPU-specific dialects based on a set of newly introduced design principles. We find that two domain-specific optimizations (500 lines of code) realized on top of LLVM’s extensible MLIR compiler infrastructure suffice to outperform state-of-the-art solutions. In essence, multi-level rewriting promises to herald the age of specialized compilers composed from domain- and target-specific dialects implemented on top of a shared infrastructure.",
    "cited_by_count": 48,
    "openalex_id": "https://openalex.org/W3196320218",
    "type": "article"
  },
  {
    "title": "Vitruvius+: An Area-Efficient RISC-V Decoupled Vector Coprocessor for High Performance Computing Applications",
    "doi": "https://doi.org/10.1145/3575861",
    "publication_date": "2022-12-09",
    "publication_year": 2022,
    "authors": "Francesco Minervini; Oscar Palomar; Osman Ünsal; Enrico Reggiani; Josue V. Quiroga; Joan Marimon; Carlos Rojas; Roger Figueras; Abraham Ruiz; Alberto González; Jonnatan Mendoza; Ivan Vargas Valdivieso; César Alejandro Hernández; Joan Cabré; Lina Khoirunisya; Mustapha Bouhali; Julián Pavón; Francesc Moll; Mauro Olivieri; Mario Kovač; Mate Kovač; Leon Dragić; Mateo Valero; Adrián Cristal",
    "corresponding_authors": "",
    "abstract": "The maturity level of RISC-V and the availability of domain-specific instruction set extensions, like vector processing, make RISC-V a good candidate for supporting the integration of specialized hardware in processor cores for the High Performance Computing (HPC) application domain. In this article, 1 we present Vitruvius+, the vector processing acceleration engine that represents the core of vector instruction execution in the HPC challenge that comes within the EuroHPC initiative. It implements the RISC-V vector extension (RVV) 0.7.1 and can be easily connected to a scalar core using the Open Vector Interface standard. Vitruvius+ natively supports long vectors: 256 double precision floating-point elements in a single vector register. It is composed of a set of identical vector pipelines (lanes), each containing a slice of the Vector Register File and functional units (one integer, one floating point). The vector instruction execution scheme is hybrid in-order/out-of-order and is supported by register renaming and arithmetic/memory instruction decoupling. On a stand-alone synthesis, Vitruvius+ reaches a maximum frequency of 1.4 GHz in typical conditions (TT/0.80V/25°C) using GlobalFoundries 22FDX FD-SOI. The silicon implementation has a total area of 1.3 mm 2 and maximum estimated power of ∼920 mW for one instance of Vitruvius+ equipped with eight vector lanes.",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W4311995762",
    "type": "article"
  },
  {
    "title": "Compiler Support for Sparse Tensor Computations in MLIR",
    "doi": "https://doi.org/10.1145/3544559",
    "publication_date": "2022-08-08",
    "publication_year": 2022,
    "authors": "Aart J. C. Bik; Penporn Koanantakool; Tatiana Shpeisman; Nicolas Vasilache; Bixia Zheng; Fredrik Kjølstad",
    "corresponding_authors": "",
    "abstract": "Sparse tensors arise in problems in science, engineering, machine learning, and data analytics. Programs that operate on such tensors can exploit sparsity to reduce storage requirements and computational time. Developing and maintaining sparse software by hand, however, is a complex and error-prone task. Therefore, we propose treating sparsity as a property of tensors, not a tedious implementation task, and letting a sparse compiler generate sparse code automatically from a sparsity-agnostic definition of the computation. This paper discusses integrating this idea into MLIR.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W4290648346",
    "type": "article"
  },
  {
    "title": "ShuffleInfer: Disaggregate LLM Inference for Mixed Downstream Workloads",
    "doi": "https://doi.org/10.1145/3732941",
    "publication_date": "2025-04-30",
    "publication_year": 2025,
    "authors": "C.-C. Hu; Haoyang Huang; Liangliang Xu; Xusheng Chen; Chenxi Wang; Xu Jiang; Shuang Chen; Hao Feng; Sa Wang; Yungang Bao; Ninghui Sun; Yizhou Shan",
    "corresponding_authors": "",
    "abstract": "Transformer-based large language model (LLM) inference serving is now the backbone of many cloud services. LLM inference consists of a prefill phase and a decode phase. However, existing LLM deployment practices often overlook the distinct characteristics of these phases, leading to significant interference. To mitigate interference, our insight is to carefully schedule and group inference requests based on their characteristics. We realize this idea in ShuffleInfer through three pillars. First, it partitions prompts into fixed-size chunks so that the accelerator always runs close to its computation-saturated limit. Second, it disaggregates prefill and decode instances so each can run independently. Finally, it uses a smart two-level scheduling algorithm augmented with predicted resource usage to avoid decode scheduling hotspots. Results show that ShuffleInfer improves time-to-first-token (TTFT), job completion time (JCT), and inference efficiency in turns of performance per dollar by a large margin, e.g., it uses 38% less resources all the while lowering average TTFT and average JCT by 97% and 47%, respectively.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4409963643",
    "type": "article"
  },
  {
    "title": "IATAC: a smart predictor to turn-off L2 cache lines",
    "doi": "https://doi.org/10.1145/1061267.1061271",
    "publication_date": "2005-03-01",
    "publication_year": 2005,
    "authors": "Jaume Abella; Antonio González; Xavier Vera; Michael O’Boyle",
    "corresponding_authors": "",
    "abstract": "As technology evolves, power dissipation increases and cooling systems become more complex and expensive. There are two main sources of power dissipation in a processor: dynamic power and leakage. Dynamic power has been the most significant factor, but leakage will become increasingly significant in future. It is predicted that leakage will shortly be the most significant cost as it grows at about a 5× rate per generation. Thus, reducing leakage is essential for future processor design. Since large caches occupy most of the area, they are one of the leakiest structures in the chip and hence, a main source of energy consumption for future processors.This paper introduces IATAC (inter-access time per access count), a new hardware technique to reduce cache leakage for L2 caches. IATAC dynamically adapts the cache size to the program requirements turning off cache lines whose content is not likely to be reused. Our evaluation shows that this approach outperforms all previous state-of-the-art techniques. IATAC turns off 65% of the cache lines across different L2 cache configurations with a very small performance degradation of around 2%.",
    "cited_by_count": 97,
    "openalex_id": "https://openalex.org/W2045110768",
    "type": "article"
  },
  {
    "title": "Thermal monitoring mechanisms for chip multiprocessors",
    "doi": "https://doi.org/10.1145/1400112.1400114",
    "publication_date": "2008-08-01",
    "publication_year": 2008,
    "authors": "Jieyi Long; Seda Öǧrenci Memik; Gokhan Memik; Rajarshi Mukherjee",
    "corresponding_authors": "",
    "abstract": "With large-scale integration and increasing power densities, thermal management has become an important tool to maintain performance and reliability in modern process technologies. In the core of dynamic thermal management schemes lies accurate reading of on-die temperatures. Therefore, careful planning and embedding of thermal monitoring mechanisms into high-performance systems becomes crucial. In this paper, we propose three techniques to create sensor infrastructures for monitoring the maximum temperature on a multicore system. Initially, we extend a nonuniform sensor placement methodology proposed in the literature to handle chip multiprocessors (CMPs) and show its limitations. We then analyze a grid-based approach where the sensors are placed on a static grid covering each core and show that the sensor readings can differ from the actual maximum core temperature by as much as 12.6°C when using 16 sensors per core. Also, as large as 10.6% of the thermal emergencies are not captured using the same number of sensors. Based on this observation, we first develop an interpolation scheme, which estimates the maximum core temperature through interpolation of the readings collected at the static grid points. We show that the interpolation scheme improves the measurement accuracy and emergency coverage compared to grid-based placement when using the same number of sensors. Second, we present a dynamic scheme where only a subset of the sensor readings is collected to predict the maximum temperature of each core. Our results indicate that, we can reduce the number of active sensors by as much as 50%, while maintaining similar measurement accuracy and emergency coverage compared to the case where the entire sensor set on the grid is sampled at all times.",
    "cited_by_count": 86,
    "openalex_id": "https://openalex.org/W2043174713",
    "type": "article"
  },
  {
    "title": "Dynamic warp formation",
    "doi": "https://doi.org/10.1145/1543753.1543756",
    "publication_date": "2009-06-01",
    "publication_year": 2009,
    "authors": "Wilson Wai Lun Fung; Ivan Sham; George L. Yuan; Tor M. Aamodt",
    "corresponding_authors": "",
    "abstract": "Recent advances in graphics processing units (GPUs) have resulted in massively parallel hardware that is easily programmable and widely available in today's desktop and notebook computer systems. GPUs typically use single-instruction, multiple-data (SIMD) pipelines to achieve high performance with minimal overhead for control hardware. Scalar threads running the same computing kernel are grouped together into SIMD batches, sometimes referred to as warps. While SIMD is ideally suited for simple programs, recent GPUs include control flow instructions in the GPU instruction set architecture and programs using these instructions may experience reduced performance due to the way branch execution is supported in hardware. One solution is to add a stack to allow different SIMD processing elements to execute distinct program paths after a branch instruction. The occurrence of diverging branch outcomes for different processing elements significantly degrades performance using this approach. In this article, we propose dynamic warp formation and scheduling, a mechanism for more efficient SIMD branch execution on GPUs. It dynamically regroups threads into new warps on the fly following the occurrence of diverging branch outcomes. We show that a realistic hardware implementation of this mechanism improves performance by 13%, on average, with 256 threads per core, 24% with 512 threads, and 47% with 768 threads for an estimated area increase of 8%.",
    "cited_by_count": 72,
    "openalex_id": "https://openalex.org/W2081373884",
    "type": "article"
  },
  {
    "title": "Writeback-aware partitioning and replacement for last-level caches in phase change main memory systems",
    "doi": "https://doi.org/10.1145/2086696.2086732",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Miao Zhou; Yu Du; Bruce R. Childers; Rami Melhem; Daniel Mossé",
    "corresponding_authors": "",
    "abstract": "Phase-Change Memory (PCM) has emerged as a promising low-power main memory candidate to replace DRAM. The main problems of PCM are that writes are much slower and more power hungry than reads, write bandwidth is much lower than read bandwidth, and limited write endurance. Adding an extra layer of cache, which is logically the last-level cache (LLC), can mitigate the drawbacks of PCM. However, writebacks from the LLC might (a) overwhelm the limited PCM write bandwidth and stall the application, (b) shorten lifetime, and (c) increase energy consumption. Cache partitioning and replacement schemes are important to achieve high throughput for multi-core systems. However, we noted that no existing partitioning and replacement policy takes into account the writeback information. This paper proposes two writeback-aware schemes to manage the LLC for PCM main memory systems. Writeback-aware Cache Partitioning (WCP) is a runtime mechanism that partitions a shared LLC among multiple applications. Unlike past partitioning schemes, our scheme considers the reduction in cache misses as well as writebacks. Write Queue Balancing (WQB) replacement policy manages the cache partition of each application intelligently so that the writebacks are distributed evenly among PCM write queues. In this way, applications rarely stall due to unbalanced PCM write traffic among write queues. Our evaluation shows that WCP and WQB result in, on average, 21% improvement in throughput, 49% reduction in PCM writes, and 14% reduction in energy over a state-of-the-art cache partitioning scheme.",
    "cited_by_count": 69,
    "openalex_id": "https://openalex.org/W2037548600",
    "type": "article"
  },
  {
    "title": "PARTANS",
    "doi": "https://doi.org/10.1145/2400682.2400718",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Thibaut Lutz; Christian Fensch; Murray Cole",
    "corresponding_authors": "",
    "abstract": "GPGPUs are a powerful and energy-efficient solution for many problems. For higher performance or larger problems, it is necessary to distribute the problem across multiple GPUs, increasing the already high programming complexity. In this article, we focus on abstracting the complexity of multi-GPU programming for stencil computation. We show that the best strategy depends not only on the stencil operator, problem size, and GPU, but also on the PCI express layout. This adds nonuniform characteristics to a seemingly homogeneous setup, causing up to 23% performance loss. We address this issue with an autotuner that optimizes the distribution across multiple GPUs.",
    "cited_by_count": 65,
    "openalex_id": "https://openalex.org/W2001258868",
    "type": "article"
  },
  {
    "title": "Compiler mitigations for time attacks on modern x86 processors",
    "doi": "https://doi.org/10.1145/2086696.2086702",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Jeroen Van Cleemput; Bart Coppens; Bjorn De Sutter",
    "corresponding_authors": "",
    "abstract": "This paper studies and evaluates the extent to which automated compiler techniques can defend against timing-based side channel attacks on modern x86 processors. We study how modern x86 processors can leak timing information through side channels that relate to data flow. We study the efficiency, effectiveness, portability, predictability and sensitivity of several mitigating code transformations that eliminate or minimize key-dependent execution time variations. Furthermore, we discuss the extent to which compiler backends are a suitable tool to provide automated support for the proposed mitigations.",
    "cited_by_count": 63,
    "openalex_id": "https://openalex.org/W2062463502",
    "type": "article"
  },
  {
    "title": "Cross-Loop Optimization of Arithmetic Intensity for Finite Element Local Assembly",
    "doi": "https://doi.org/10.1145/2687415",
    "publication_date": "2015-01-09",
    "publication_year": 2015,
    "authors": "Fabio Luporini; Ana Lucia Vărbănescu; Florian Rathgeber; Gheorghe-Teodor Bercea; J. Ramanujam; David A. Ham; Paul H. J. Kelly",
    "corresponding_authors": "",
    "abstract": "We study and systematically evaluate a class of composable code transformations that improve arithmetic intensity in local assembly operations, which represent a significant fraction of the execution time in finite element methods. Their performance optimization is indeed a challenging issue. Even though affine loop nests are generally present, the short trip counts and the complexity of mathematical expressions, which vary among different problems, make it hard to determine an optimal sequence of successful transformations. Our investigation has resulted in the implementation of a compiler (called COFFEE) for local assembly kernels, fully integrated with a framework for developing finite element methods. The compiler manipulates abstract syntax trees generated from a domain-specific language by introducing domain-aware optimizations for instruction-level parallelism and register locality. Eventually, it produces C code including vector SIMD intrinsics. Experiments using a range of real-world finite element problems of increasing complexity show that significant performance improvement is achieved. The generality of the approach and the applicability of the proposed code transformations to other domains is also discussed.",
    "cited_by_count": 63,
    "openalex_id": "https://openalex.org/W3100990548",
    "type": "article"
  },
  {
    "title": "A-DFA",
    "doi": "https://doi.org/10.1145/2445572.2445576",
    "publication_date": "2013-04-01",
    "publication_year": 2013,
    "authors": "Michela Becchi; Patrick Crowley",
    "corresponding_authors": "",
    "abstract": "Modern network intrusion detection systems need to perform regular expression matching at line rate in order to detect the occurrence of critical patterns in packet payloads. While Deterministic Finite Automata (DFAs) allow this operation to be performed in linear time, they may exhibit prohibitive memory requirements. Kumar et al. [2006a] have proposed Delayed Input DFAs (D2FAs), which provide a trade-off between the memory requirements of the compressed DFA and the number of states visited for each character processed, which in turn affects the memory bandwidth required to evaluate regular expressions. In this article we introduce Amortized time − bandwidth overhead DFAs ( A − DFAs ), a general compression technique that results in at most N ( k + 1)/ k state traversals when processing a string of length N , k being a positive integer. In comparison to the D2FA approach, our technique achieves comparable levels of compression with lower provable bounds on memory bandwidth (or greater compression for a given bandwidth bound). Moreover, the A-DFA algorithm has lower complexity, can be applied during DFA creation, and is suitable for scenarios where a compressed DFA needs to be dynamically built or updated. Finally, we show how to combine A-DFA with alphabet reduction and multistride DFAs, two techniques aimed at reducing the memory space and bandwidth requirement of DFAs, and discuss memory encoding schemes suitable for A-DFAs.",
    "cited_by_count": 61,
    "openalex_id": "https://openalex.org/W2115345034",
    "type": "article"
  },
  {
    "title": "Finding good optimization sequences covering program space",
    "doi": "https://doi.org/10.1145/2400682.2400715",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Suresh Purini; Lakshya Jain",
    "corresponding_authors": "",
    "abstract": "The compiler optimizations we enable and the order in which we apply them on a program have a substantial impact on the program execution time. Compilers provide default optimization sequences which can give good program speedup. As the default sequences have to optimize programs with different characteristics, they embed in them multiple subsequences which can optimize different classes of programs. These multiple subsequences may falsely interact with each other and affect the potential program speedup achievable. Instead of searching for a single universally optimal sequence, we can construct a small set of good sequences such that for every program class there exists a near-optimal optimization sequence in the good sequences set. If we can construct such a good sequences set which covers all the program classes in the program space, then we can choose the best sequence for a program by trying all the sequences in the good sequences set. This approach completely circumvents the need to solve the program classification problem. Using a sequence set size of around 10 we got an average speedup up to 14% on PolyBench programs and up to 12% on MiBench programs. Our approach is quite different from either the iterative compilation or machine-learning-based prediction modeling techniques proposed in the literature so far. We use different training and test datasets for cross-validation as against the Leave-One-Out cross-validation technique.",
    "cited_by_count": 60,
    "openalex_id": "https://openalex.org/W1984845936",
    "type": "article"
  },
  {
    "title": "VGRIS: Virtualized GPU Resource Isolation and Scheduling in Cloud Gaming",
    "doi": "https://doi.org/10.1145/2632216",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "Zhengwei Qi; Jianguo Yao; Chao Zhang; Miao Yu; Zhizhou Yang; Haibing Guan",
    "corresponding_authors": "",
    "abstract": "To achieve efficient resource management on a graphics processing unit (GPU), there is a demand to develop a framework for scheduling virtualized resources in cloud gaming. In this article, we propose VGRIS, a resource management framework for virtualized GPU resource isolation and scheduling in cloud gaming. A set of application programming interfaces (APIs) is provided so that a variety of scheduling algorithms can be implemented within the framework without modifying the framework itself. Three scheduling algorithms are implemented by the APIs within VGRIS. Experimental results show that VGRIS can effectively schedule GPU resources among various workloads.",
    "cited_by_count": 59,
    "openalex_id": "https://openalex.org/W2015518316",
    "type": "article"
  },
  {
    "title": "On the simulation of large-scale architectures using multiple application abstraction levels",
    "doi": "https://doi.org/10.1145/2086696.2086715",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Alejandro Rico; Felipe Cabarcas; Carlos Villavieja; Milan Pavlović; Augusto Vega; Yoav Etsion; Alex Ramírez; Mateo Valero",
    "corresponding_authors": "",
    "abstract": "Simulation is a key tool for computer architecture research. In particular, cycle-accurate simulators are extremely important for microarchitecture exploration and detailed design decisions, but they are slow and, so, not suitable for simulating large-scale architectures, nor are they meant for this. Moreover, microarchitecture design decisions are irrelevant, or even misleading, for early processor design stages and high-level explorations. This allows one to raise the abstraction level of the simulated architecture, and also the application abstraction level, as it does not necessarily have to be represented as an instruction stream. In this paper we introduce a definition of different application abstraction levels, and how these are employed in TaskSim, a multi-core architecture simulator, to provide several architecture modeling abstractions, and simulate large-scale architectures with hundreds of cores. We compare the simulation speed of these abstraction levels to the ones in existing simulation tools, and also evaluate their utility and accuracy. Our simulations show that a very high-level abstraction, which may be even faster than native execution, is useful for scalability studies on parallel applications; and that just simulating explicit memory transfers, we achieve accurate simulations for architectures using non-coherent scratchpad memories, with just a 25x slowdown compared to native execution. Furthermore, we revisit trace memory simulation techniques, that are more abstract than instruction-by-instruction simulations and provide an 18x simulation speedup.",
    "cited_by_count": 59,
    "openalex_id": "https://openalex.org/W2095685483",
    "type": "article"
  },
  {
    "title": "A System-Level Simulator for RRAM-Based Neuromorphic Computing Chips",
    "doi": "https://doi.org/10.1145/3291054",
    "publication_date": "2018-12-31",
    "publication_year": 2018,
    "authors": "Matthew Kay Fei Lee; Yingnan Cui; Thannirmalai Somu; Tao Luo; Jun Zhou; Wai Teng Tang; Weng‐Fai Wong; Rick Siow Mong Goh",
    "corresponding_authors": "",
    "abstract": "Advances in non-volatile resistive switching random access memory (RRAM) have made it a promising memory technology with potential applications in low-power and embedded in-memory computing devices owing to a number of advantages such as low-energy consumption, low area cost and good scaling. There have been proposals to employ RRAM in architecting chips for neuromorphic computing and artificial neural networks where matrix-vector multiplication can be computed in the analog domain in a single timestep. However, it is challenging to employ RRAM devices in neuromorphic chips owing to the non-ideal behavior of RRAM. In this article, we propose a cycle-accurate and scalable system-level simulator that can be used to study the effects of using RRAM devices in neuromorphic computing chips. The simulator models a spatial neuromorphic chip architecture containing many neural cores with RRAM crossbars connected via a Network-on-Chip (NoC). We focus on system-level simulation and demonstrate the effectiveness of our simulator in understanding how non-linear RRAM effects such as stuck-at-faults (SAFs), write variability, and random telegraph noise (RTN) can impact an application’s behavior. By using our simulator, we show that RTN and write variability can have adverse effects on an application. Nevertheless, we show that these effects can be mitigated through proper design choices and the implementation of a write-verify scheme.",
    "cited_by_count": 58,
    "openalex_id": "https://openalex.org/W2909633668",
    "type": "article"
  },
  {
    "title": "A script-based autotuning compiler system to generate high-performance CUDA code",
    "doi": "https://doi.org/10.1145/2400682.2400690",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Malik M. Khan; Protonu Basu; Gabe Rudy; Mary Hall; Chun Chen; Jacqueline Chame",
    "corresponding_authors": "",
    "abstract": "This article presents a novel compiler framework for CUDA code generation. The compiler structure is designed to support autotuning , which employs empirical techniques to evaluate a set of alternative mappings of computation kernels and select the mapping that obtains the best performance. This article introduces a Transformation Strategy Generator , a meta-optimizer that generates a set of transformation recipes , which are descriptions of the mapping of the sequential code to parallel CUDA code. These recipes comprise a search space of possible implementations. This system achieves performance comparable and sometimes better than manually tuned libraries and exceeds the performance of a state-of-the-art GPU compiler.",
    "cited_by_count": 57,
    "openalex_id": "https://openalex.org/W1975001341",
    "type": "article"
  },
  {
    "title": "Mitigating Prefetcher-Caused Pollution Using Informed Caching Policies for Prefetched Blocks",
    "doi": "https://doi.org/10.1145/2677956",
    "publication_date": "2015-01-09",
    "publication_year": 2015,
    "authors": "Vivek Seshadri; Samihan Yedkar; Hongyi Xin; Onur Mutlu; Phillip B. Gibbons; Michael A. Kozuch; Todd C. Mowry",
    "corresponding_authors": "",
    "abstract": "Many modern high-performance processors prefetch blocks into the on-chip cache. Prefetched blocks can potentially pollute the cache by evicting more useful blocks. In this work, we observe that both accurate and inaccurate prefetches lead to cache pollution, and propose a comprehensive mechanism to mitigate prefetcher-caused cache pollution. First, we observe that over 95% of useful prefetches in a wide variety of applications are not reused after the first demand hit (in secondary caches). Based on this observation, our first mechanism simply demotes a prefetched block to the lowest priority on a demand hit. Second, to address pollution caused by inaccurate prefetches, we propose a self-tuning prefetch accuracy predictor to predict if a prefetch is accurate or inaccurate. Only predicted-accurate prefetches are inserted into the cache with a high priority. Evaluations show that our final mechanism, which combines these two ideas, significantly improves performance compared to both the baseline LRU policy and two state-of-the-art approaches to mitigating prefetcher-caused cache pollution (up to 49%, and 6% on average for 157 two-core multiprogrammed workloads). The performance improvement is consistent across a wide variety of system configurations.",
    "cited_by_count": 55,
    "openalex_id": "https://openalex.org/W2067510835",
    "type": "article"
  },
  {
    "title": "CAIRO",
    "doi": "https://doi.org/10.1145/3155287",
    "publication_date": "2017-12-20",
    "publication_year": 2017,
    "authors": "Ramyad Hadidi; Lifeng Nai; Hyojong Kim; Hyesoon Kim",
    "corresponding_authors": "",
    "abstract": "Three-dimensional (3D)-stacking technology and the memory-wall problem have popularized processing-in-memory (PIM) concepts again, which offers the benefits of bandwidth and energy savings by offloading computations to functional units inside the memory. Several memory vendors have also started to integrate computation logics into the memory, such as Hybrid Memory Cube (HMC), the latest version of which supports up to 18 in-memory atomic instructions. Although industry prototypes have motivated studies for investigating efficient methods and architectures for PIM, researchers have not proposed a systematic way for identifying the benefits of instruction-level PIM offloading . As a result, compiler support for recognizing offloading candidates and utilizing instruction-level PIM offloading is unavailable. In this article, we analyze the advantages of instruction-level PIM offloading in the context of HMC-atomic instructions for graph-computing applications and propose CAIRO , a compiler-assisted technique and decision model for enabling instruction-level offloading of PIM without any burden on programmers. To develop CAIRO, we analyzed how instruction offloading enables performance gain in both CPU and GPU workloads. Our studies show that performance gain from bandwidth savings, the ratio of number of cache misses to total cache accesses, and the overhead of host atomic instructions are the key factors in selecting an offloading candidate. Based on our analytical models, we characterize the properties of beneficial and nonbeneficial candidates for offloading. We evaluate CAIRO with 27 multithreaded CPU and 36 GPU benchmarks. In our evaluation, CAIRO not only doubles the speedup for a set of PIM-beneficial workloads by exploiting HMC-atomic instructions but also prevents slowdown caused by incorrect offloading decisions for other workloads.",
    "cited_by_count": 52,
    "openalex_id": "https://openalex.org/W2776052384",
    "type": "article"
  },
  {
    "title": "Synergistic Analysis of Evolving Graphs",
    "doi": "https://doi.org/10.1145/2992784",
    "publication_date": "2016-10-25",
    "publication_year": 2016,
    "authors": "Keval Vora; Rajiv Gupta; Guoqing Xu",
    "corresponding_authors": "",
    "abstract": "Evolving graph processing involves repeating analyses, which are often iterative, over multiple snapshots of the graph corresponding to different points in time. Since the snapshots of an evolving graph share a great number of vertices and edges, traditional approaches that process these snapshots one at a time without exploiting this overlap contain much wasted effort on both data loading and computation, making them extremely inefficient. In this article, we identify major sources of inefficiencies and present two optimization techniques to address them. First, we propose a technique for amortizing the fetch cost by merging fetching of values for different snapshots of the same vertex. Second, we propose a technique for amortizing the processing cost by feeding values computed by earlier snapshots into later snapshots. We have implemented these optimizations in two distributed graph processing systems, namely, GraphLab and ASPIRE. Our experiments with multiple real evolving graphs and algorithms show that, on average fetch amortization speeds up execution of GraphLab and ASPIRE by 5.2× and 4.1× , respectively. Amortizing the processing cost yields additional average speedups of 2× and 7.9×, respectively.",
    "cited_by_count": 50,
    "openalex_id": "https://openalex.org/W2534918852",
    "type": "article"
  },
  {
    "title": "A Case For Intra-rack Resource Disaggregation in HPC",
    "doi": "https://doi.org/10.1145/3514245",
    "publication_date": "2022-02-02",
    "publication_year": 2022,
    "authors": "George Michelogiannakis; Benjamin Klenk; Brandon Cook; Min Yee Teh; Madeleine Glick; Larry Dennison; Keren Bergman; John Shalf",
    "corresponding_authors": "",
    "abstract": "The expected halt of traditional technology scaling is motivating increased heterogeneity in high-performance computing (HPC) systems with the emergence of numerous specialized accelerators. As heterogeneity increases, so does the risk of underutilizing expensive hardware resources if we preserve today’s rigid node configuration and reservation strategies. This has sparked interest in resource disaggregation to enable finer-grain allocation of hardware resources to applications. However, there is currently no data-driven study of what range of disaggregation is appropriate in HPC. To that end, we perform a detailed analysis of key metrics sampled in NERSC’s Cori, a production HPC system that executes a diverse open-science HPC workload. In addition, we profile a variety of deep-learning applications to represent an emerging workload. We show that for a rack (cabinet) configuration and applications similar to Cori, a central processing unit with intra-rack disaggregation has a 99.5% probability to find all resources it requires inside its rack. In addition, ideal intra-rack resource disaggregation in Cori could reduce memory and NIC resources by 5.36% to 69.01% and still satisfy the worst-case average rack utilization.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W4210363011",
    "type": "article"
  },
  {
    "title": "Assessing the Impact of Compiler Optimizations on GPUs Reliability",
    "doi": "https://doi.org/10.1145/3638249",
    "publication_date": "2024-01-12",
    "publication_year": 2024,
    "authors": "Fernando Fernandes dos Santos; Luigi Carro; Flavio Vella; Paolo Rech",
    "corresponding_authors": "",
    "abstract": "Graphics Processing Units (GPUs) compilers have evolved in order to support general-purpose programming languages for multiple architectures. NVIDIA CUDA Compiler (NVCC) has many compilation levels before generating the machine code and applies complex optimizations to improve performance. These optimizations modify how the software is mapped in the underlying hardware; thus, as we show in this article, they can also affect GPU reliability. We evaluate the effects on the GPU error rate of the optimization flags applied at the NVCC Parallel Thread Execution (PTX) compiling phase by analyzing two NVIDIA GPU architectures (Kepler and Volta) and two compiler versions (NVCC 10.2 and 11.3). We compare and combine fault propagation analysis based on software fault injection, hardware utilization distribution obtained with application-level profiling, and machine instructions radiation-induced error rate measured with beam experiments. We consider eight different workloads and 144 combinations of compilation flags, and we show that optimizations can impact the GPUs’ error rate of up to an order of magnitude. Additionally, through accelerated neutron beam experiments on a NVIDIA Kepler GPU, we show that the error rate of the unoptimized GEMM (-O0 flag) is lower than the optimized GEMM’s (-O3 flag) error rate. When the performance is evaluated together with the error rate, we show that the most optimized versions (-O1 and -O3) always produce a higher amount of correct data than the unoptimized code (-O0).",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4390814681",
    "type": "article"
  },
  {
    "title": "Unveiling and Evaluating Vulnerabilities in Branch Predictors via a Three-Step Modeling Methodology",
    "doi": "https://doi.org/10.1145/3711923",
    "publication_date": "2025-01-13",
    "publication_year": 2025,
    "authors": "Quancheng Wang; Ming Tang; Ke Xu; Han Wang",
    "corresponding_authors": "",
    "abstract": "With the emergence and proliferation of microarchitectural attacks targeting branch predictors, the once-established security boundary in computer systems and architectures is facing unprecedented challenges. This paper introduces an innovative branch predictor modeling methodology that abstractly characterizes 19 states and 53 operations of branch predictors, aiming to assist hardware designers in addressing overlooked security concerns during the microarchitecture design phase. Building upon this modeling discipline, we develop a symbolic execution-based framework to analyze and derive potential vulnerabilities in branch predictors. This framework finally yields 156 valid three-step attack patterns against branch predictors, including 89 novel variants not discovered in previous work. Subsequently, we extend the framework to automatically generate a benchmark suite for assessing the practical feasibility of derived attacks in real-world scenarios. Evaluation across five commercial Intel processors underscores the substantial threat posed by branch predictor attacks, with 130 of the 156 derived attacks proving viable on at least one processor. Finally, we theoretically model and evaluate 12 secure designs related to branch predictors. The evaluation results demonstrate that existing secure branch predictors can offer better security guarantees than secure speculation schemes, indicating that secure branch predictor designs are promising solutions to maintain the confidentiality and integrity of computer systems.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4406302341",
    "type": "article"
  },
  {
    "title": "BridgeGC: An Efficient Cross-Level Garbage Collector for Big Data Frameworks",
    "doi": "https://doi.org/10.1145/3722110",
    "publication_date": "2025-03-07",
    "publication_year": 2025,
    "authors": "Yicheng Wang; Lijie Xu; Tian Guo; Wensheng Dou; Hongbin Zeng; Wei Wang; Jun Wei; Huang Tao",
    "corresponding_authors": "",
    "abstract": "Popular big data frameworks commonly run atop Java Virtual Machine (JVM), and rely on garbage collection (GC) mechanism to automatically allocate/reclaim in-memory objects. Existing garbage collectors are designed based on the hypothesis that most objects are short-lived. However, big data frameworks usually generate many long-lived data objects, which can cause heavy GC overhead. Recent approaches have reduced GC overhead in big data frameworks but still suffer from heavy human efforts, additional runtime overhead, or suboptimal GC efficiency. This paper describes the design of BridgeGC , a big-data-friendly garbage collector that significantly reduces GC overhead introduced by long-lived data objects. BridgeGC follows a cross-level co-design. At the big data framework level, BridgeGC provides two annotations for framework developers to denote the creation and release of data objects. Based on the annotations, BridgeGC tracks the life cycles of annotated data objects and optimizes their allocation/reclamation at the GC level. At the GC level, we design a label-based allocator that stores data objects separately from other objects and balances their memory usage in the same JVM, leading to fewer GC cycles. We further design an efficient collector to eliminate unnecessary marking and copying of data objects during GC cycles, lowering the GC time. We have integrated BridgeGC into OpenJDK ZGC. The extensive evaluation, using two popular big data frameworks (Flink and Spark) and a key-value database (Cassandra), shows that BridgeGC achieves 31%-82% GC time reduction compared to the baseline ZGC. BridgeGC also outperforms other traditional and academic garbage collectors in end-to-end performance.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4408169552",
    "type": "article"
  },
  {
    "title": "A way-halting cache for low-energy high-performance systems",
    "doi": "https://doi.org/10.1145/1061267.1061270",
    "publication_date": "2005-03-01",
    "publication_year": 2005,
    "authors": "Chuanjun Zhang; Frank Vahid; Jun Yang; Walid Najjar",
    "corresponding_authors": "",
    "abstract": "Caches contribute to much of a microprocessor system's power and energy consumption. Numerous new cache architectures, such as phased, pseudo-set-associative, way predicting, reactive-associative, way-shutdown, way-concatenating, and highly-associative, are intended to reduce power and/or energy, but they all impose some performance overhead. We have developed a new cache architecture, called a way-halting cache, that reduces energy further than previously mentioned architectures, while imposing no performance overhead. Our way-halting cache is a four-way set-associative cache that stores the four lowest-order bits of all ways' tags into a fully associative memory, which we call the halt tag array. The lookup in the halt tag array is done in parallel with, and is no slower than, the set-index decoding. The halt tag array predetermines which tags cannot match due to their low-order 4 bits mismatching. Further accesses to ways with known mismatching tags are then halted, thus saving power. Our halt tag array has an additional feature of using static logic only, rather than dynamic logic used in highly associative caches, making our cache simpler to design with existing tools. We provide data from experiments on 29 benchmarks drawn from Powerstone, Mediabench, and Spec 2000, based on our layouts in 0.18 micron CMOS technology. On average, we obtained 55% savings of memory-access related energy over a conventional four-way set-associative cache. We show that savings are greater than previous methods, and nearly twice that of highly associative caches, while imposing no performance overhead and only 2% cache area overhead.",
    "cited_by_count": 87,
    "openalex_id": "https://openalex.org/W2007395720",
    "type": "article"
  },
  {
    "title": "Bit-split string-matching engines for intrusion detection and prevention",
    "doi": "https://doi.org/10.1145/1132462.1132464",
    "publication_date": "2006-03-01",
    "publication_year": 2006,
    "authors": "Lin Tan; Brett Brotherton; Timothy Sherwood",
    "corresponding_authors": "",
    "abstract": "Network Intrusion Detection and Prevention Systems have emerged as one of the most effective ways of providing security to those connected to the network and at the heart of almost every modern intrusion detection system is a string-matching algorithm. String matching is one of the most critical elements because it allows for the system to make decisions based not just on the headers, but the actual content flowing through the network. Unfortunately, checking every byte of every packet to see if it matches one of a set of thousands of strings becomes a computationally intensive task as network speeds grow into the tens, and eventually hundreds, of gigabits/second. To keep up with these speeds, a specialized device is required, one that can maintain tight bounds on worst-case performance, that can be updated with new rules without interrupting operation, and one that is efficient enough that it could be included on-chip with existing network chips or even into wireless devices. We have developed an approach that relies on a special purpose architecture that executes novel string matching algorithms specially optimized for implementation in our design. We show how the problem can be solved by converting the large database of strings into many tiny state machines, each of which searches for a portion of the rules and a portion of the bits of each rule. Through the careful codesign and optimization of our architecture with a new string-matching algorithm, we show that it is possible to build a system that is 10 times more efficient than the currently best known approaches.",
    "cited_by_count": 84,
    "openalex_id": "https://openalex.org/W2145492393",
    "type": "article"
  },
  {
    "title": "TRIPS",
    "doi": "https://doi.org/10.1145/980152.980156",
    "publication_date": "2004-03-01",
    "publication_year": 2004,
    "authors": "Karthikeyan Sankaralingam; Ramadass Nagarajan; Haiming Liu; Changkyu Kim; Jaehyuk Huh; Nitya Ranganathan; Doug Burger; Stephen W. Keckler; Robert McDonald; Charles R. Moore",
    "corresponding_authors": "",
    "abstract": "This paper describes the polymorphous TRIPS architecture that can be configured for different granularities and types of parallelism. The TRIPS architecture is the first in a class of post-RISC, dataflow-like instruction sets called explicit data-graph execution (EDGE). This EDGE ISA is coupled with hardware mechanisms that enable the processing cores and the on-chip memory system to be configured and combined in different modes for instruction, data, or thread-level parallelism. To adapt to small and large-grain concurrency, the TRIPS architecture prototype contains two out-of-order, 16-wide-issue grid processor cores, which can be partitioned when easily extractable fine-grained parallelism exists. This approach to polymorphism provides better performance across a wide range of application types than an approach in which many small processors are aggregated to run workloads with irregular parallelism. Our results show that high performance can be obtained in each of the three modes---ILP, TLP, and DLP---demonstrating the viability of the polymorphous coarse-grained approach for future microprocessors.",
    "cited_by_count": 83,
    "openalex_id": "https://openalex.org/W2089984858",
    "type": "article"
  },
  {
    "title": "Power-performance considerations of parallel computing on chip multiprocessors",
    "doi": "https://doi.org/10.1145/1113841.1113844",
    "publication_date": "2005-12-01",
    "publication_year": 2005,
    "authors": "Jian Li; José F. Martínez",
    "corresponding_authors": "",
    "abstract": "This paper looks at the power-performance implications of running parallel applications on chip multiprocessors (CMPs). First, we develop an analytical model that, for the first time, puts together parallel efficiency, granularity of parallelism, and voltage/frequency scaling, to establish a formal connection with the power consumption and performance of a parallel code running on a CMP. We then conduct detailed simulations of parallel applications running on a detailed power-performance CMP model to confirm the analytical results and provide further insights. Both analytical and experimental models show that parallel computing can bring significant power savings and still meet a given performance target by choosing granularity and voltage/frequency levels judiciously. The particular choice, however, is dependent on the application's parallel efficiency curve and the process technology utilized, which our model captures. Likewise, analytical model and experiments show the effect of a limited power budget on the application's scalability curve. In particular, we show that a limited power budget can cause a rapid performance degradation beyond a number of cores, even in the case of applications with excellent scalability properties. On the other hand, our experiments show that, when a limited power budget is in place, power-thrifty memory-bound applications may actually enjoy better scalability than more compute-intensive codes, even if the latter would exhibit higher scalability in a power-unconstrained scenario.",
    "cited_by_count": 81,
    "openalex_id": "https://openalex.org/W2091913208",
    "type": "article"
  },
  {
    "title": "Toward kilo-instruction processors",
    "doi": "https://doi.org/10.1145/1044823.1044825",
    "publication_date": "2004-12-01",
    "publication_year": 2004,
    "authors": "Adrián Cristal; Oliverio J. Santana; Mateo Valero; José F. Martínez",
    "corresponding_authors": "",
    "abstract": "The continuously increasing gap between processor and memory speeds is a serious limitation to the performance achievable by future microprocessors. Currently, processors tolerate long-latency memory operations largely by maintaining a high number of in-flight instructions. In the future, this may require supporting many hundreds, or even thousands, of in-flight instructions. Unfortunately, the traditional approach of scaling up critical processor structures to provide such support is impractical at these levels, due to area, power, and cycle time constraints.In this paper we show that, in order to overcome this resource-scalability problem, the way in which critical processor resources are managed must be changed. Instead of simply upsizing the processor structures, we propose a smarter use of the available resources, supported by a selective checkpointing mechanism. This mechanism allows instructions to commit out of order, and makes a reorder buffer unnecessary. We present a set of techniques such as multilevel instruction queues, late allocation and early release of registers, and early release of load/store queue entries. All together, these techniques constitute what we call a kilo-instruction processor , an architecture that can support thousands of in-flight instructions, and thus may achieve high performance even in the presence of large memory access latencies.",
    "cited_by_count": 80,
    "openalex_id": "https://openalex.org/W2148004594",
    "type": "article"
  },
  {
    "title": "Minos",
    "doi": "https://doi.org/10.1145/1187976.1187977",
    "publication_date": "2006-12-01",
    "publication_year": 2006,
    "authors": "Jedidiah R. Crandall; S. Felix Wu; Frederic T. Chong",
    "corresponding_authors": "",
    "abstract": "We present Minos, a microarchitecture that implements Biba's low water-mark integrity policy on individual words of data. Minos stops attacks that corrupt control data to hijack program control flow, but is orthogonal to the memory model. Control data is any data that is loaded into the program counter on control-flow transfer, or any data used to calculate such data. The key is that Minos tracks the integrity of all data, but protects control flow by checking this integrity when a program uses the data for control transfer. Existing policies, in contrast, need to differentiate between control and noncontrol data a priori , a task made impossible by coercions between pointers and other data types, such as integers in the C language. Our implementation of Minos for Red Hat Linux 6.2 on a Pentium-based emulator is a stable, usable Linux system on the network on which we are currently running a web server (http://minos.cs.ucdavis.edu). Our emulated Minos systems running Linux and Windows have stopped ten actual attacks. Extensive full-system testing and real-world attacks have given us a unique perspective on the policy tradeoffs that must be made in any system, such as Minos; this paper details and discusses these. We also present a microarchitectural implementation of Minos that achieves negligible impact on cycle time with a small investment in die area, as well as and minor changes to the Linux kernel to handle the tag bits and perform virtual memory swapping.",
    "cited_by_count": 69,
    "openalex_id": "https://openalex.org/W2117470854",
    "type": "article"
  },
  {
    "title": "Efficient architectural design space exploration via predictive modeling",
    "doi": "https://doi.org/10.1145/1328195.1328196",
    "publication_date": "2008-01-01",
    "publication_year": 2008,
    "authors": "Engin İpek; Sally A. McKee; Karan Singh; Rich Caruana; Bronis R. de Supinski; Martin Schulz",
    "corresponding_authors": "",
    "abstract": "Efficiently exploring exponential-size architectural design spaces with many interacting parameters remains an open problem: the sheer number of experiments required renders detailed simulation intractable. We attack this via an automated approach that builds accurate predictive models. We simulate sampled points, using results to teach our models the function describing relationships among design parameters. The models can be queried and are very fast, enabling efficient design tradeoff discovery. We validate our approach via two uniprocessor sensitivity studies, predicting IPC with only 1--2% error. In an experimental study using the approach, training on 1% of a 250-K-point CMP design space allows our models to predict performance with only 4--5% error. Our predictive modeling combines well with techniques that reduce the time taken by each simulation experiment, achieving net time savings of three-four orders of magnitude.",
    "cited_by_count": 64,
    "openalex_id": "https://openalex.org/W1984288647",
    "type": "article"
  },
  {
    "title": "Hybrid checkpointing using emerging nonvolatile memories for future exascale systems",
    "doi": "https://doi.org/10.1145/1970386.1970387",
    "publication_date": "2011-06-21",
    "publication_year": 2011,
    "authors": "Xiangyu Dong; Yuan Xie; Naveen Muralimanohar; Norman P. Jouppi",
    "corresponding_authors": "",
    "abstract": "The scalability of future Massively Parallel Processing (MPP) systems is being severely challenged by high failure rates. Current centralized Hard Disk Drive (HDD) checkpointing results in overhead of 25% or more at petascale. Since systems become more vulnerable as the node count keeps increasing, novel techniques that enable fast and frequent checkpointing are critical to the future exascale system implementation. In this work, we first introduce one of the emerging nonvolatile memory technologies, Phase-Change Random Access Memory (PCRAM), as a proper candidate of the fast checkpointing device. After a thorough analysis of MPP systems, failure rates and failure sources, we propose a PCRAM-based hybrid local/global checkpointing mechanism which not only provides a faster checkpoint storage, but also boosts the effectiveness of other orthogonal techniques such as incremental checkpointing and background checkpointing. Three variant implementations of the PCRAM-based hybrid checkpointing are designed to be adopted at different stages and to offer a smooth transition from the conventional in-disk checkpointing to the instant in-memory approach. Analyzing the overhead by using a hybrid checkpointing performance model, we show the proposed approach only incurs less than 3% performance overhead on a projected exascale system.",
    "cited_by_count": 57,
    "openalex_id": "https://openalex.org/W1970958679",
    "type": "article"
  },
  {
    "title": "TACOMA",
    "doi": "https://doi.org/10.1145/2207222.2207227",
    "publication_date": "2012-06-01",
    "publication_year": 2012,
    "authors": "Zahra Abbasi; Georgios Varsamopoulos; Sandeep K. S. Gupta",
    "corresponding_authors": "",
    "abstract": "A two-tier Internet data center management scheme, TACOMA, with thermal-aware server provisioning (TASP) in one tier, and thermal-aware workload distribution (TAWD) in the other is proposed. TASP and TAWD coordinate to maximize the energy savings by leveraging the workload dynamics, at coarse and fine time scale, respectively. TACOMA is aware of the QoS constraints, the energy proportionality of servers, and the potential trade-off between cooling and computing power. The obtained energy savings are a combination of suspending idle servers, using servers at their peak efficiency, and avoiding heat recirculation.",
    "cited_by_count": 54,
    "openalex_id": "https://openalex.org/W2024446827",
    "type": "article"
  },
  {
    "title": "Optimal DPM and DVFS for frame-based real-time systems",
    "doi": "https://doi.org/10.1145/2400682.2400700",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Marco E. T. Gerards; Jan Kuper",
    "corresponding_authors": "",
    "abstract": "Dynamic Power Management (DPM) and Dynamic Voltage and Frequency Scaling (DVFS) are popular techniques for reducing energy consumption. Algorithms for optimal DVFS exist, but optimal DPM and the optimal combination of DVFS and DPM are not yet solved. In this article we use well-established models of DPM and DVFS for frame-based systems. We show that it is not sufficient—as some authors argue—to consider only individual invocations of a task. We define a schedule that also takes interactions between invocations into account and prove—in a theoretical fashion—that this schedule is optimal.",
    "cited_by_count": 53,
    "openalex_id": "https://openalex.org/W2071164455",
    "type": "article"
  },
  {
    "title": "Automatic and Portable Mapping of Data Parallel Programs to OpenCL for GPU-Based Heterogeneous Systems",
    "doi": "https://doi.org/10.1145/2677036",
    "publication_date": "2014-12-08",
    "publication_year": 2014,
    "authors": "Zheng Wang; Dominik Grewe; Michael O’Boyle",
    "corresponding_authors": "",
    "abstract": "General-purpose GPU-based systems are highly attractive, as they give potentially massive performance at little cost. Realizing such potential is challenging due to the complexity of programming. This article presents a compiler-based approach to automatically generate optimized OpenCL code from data parallel OpenMP programs for GPUs. A key feature of our scheme is that it leverages existing transformations, especially data transformations, to improve performance on GPU architectures and uses automatic machine learning to build a predictive model to determine if it is worthwhile running the OpenCL code on the GPU or OpenMP code on the multicore host. We applied our approach to the entire NAS parallel benchmark suite and evaluated it on distinct GPU-based systems. We achieved average (up to) speedups of 4.51× and 4.20× (143× and 67×) on Core i7/NVIDIA GeForce GTX580 and Core i7/AMD Radeon 7970 platforms, respectively, over a sequential baseline. Our approach achieves, on average, greater than 10× speedups over two state-of-the-art automatic GPU code generators.",
    "cited_by_count": 51,
    "openalex_id": "https://openalex.org/W2070897293",
    "type": "article"
  },
  {
    "title": "Using machine learning to improve automatic vectorization",
    "doi": "https://doi.org/10.1145/2086696.2086729",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Kevin Stock; Louis-Noël Pouchet; P. Sadayappan",
    "corresponding_authors": "",
    "abstract": "Automatic vectorization is critical to enhancing performance of compute-intensive programs on modern processors. However, there is much room for improvement over the auto-vectorization capabilities of current production compilers through careful vector-code synthesis that utilizes a variety of loop transformations (e.g., unroll-and-jam, interchange, etc.). As the set of transformations considered is increased, the selection of the most effective combination of transformations becomes a significant challenge: Currently used cost models in vectorizing compilers are often unable to identify the best choices. In this paper, we address this problem using machine learning models to predict the performance of SIMD codes. In contrast to existing approaches that have used high-level features of the program, we develop machine learning models based on features extracted from the generated assembly code. The models are trained offline on a number of benchmarks and used at compile-time to discriminate between numerous possible vectorized variants generated from the input code. We demonstrate the effectiveness of the machine learning model by using it to guide automatic vectorization on a variety of tensor contraction kernels, with improvements ranging from 2× to 8× over Intel ICC's auto-vectorized code. We also evaluate the effectiveness of the model on a number of stencil computations and show good improvement over auto-vectorized code.",
    "cited_by_count": 50,
    "openalex_id": "https://openalex.org/W2064441385",
    "type": "article"
  },
  {
    "title": "WADE",
    "doi": "https://doi.org/10.1145/2541228.2555307",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Zhe Wang; Shuchang Shan; Ting Cao; Junli Gu; Yi Xu; Shuai Mu; Yuan Xie; Daniel A. Jiménez",
    "corresponding_authors": "",
    "abstract": "Emerging Non-Volatile Memory (NVM) technologies are explored as potential alternatives to traditional SRAM/DRAM-based memory architecture in future microprocessor design. One of the major disadvantages for NVM is the latency and energy overhead associated with write operations. Mitigation techniques to minimize the write overhead for NVM-based main memory architecture have been studied extensively. However, most prior work focuses on optimization techniques for NVM-based main memory itself, with little attention paid to cache management policies for the Last-Level Cache (LLC). In this article, we propose a Writeback-Aware Dynamic CachE (WADE) management technique to help mitigate the write overhead in NVM-based memory.&lt;sup;&gt;1&lt;/sup;&gt; The proposal is based on the observation that, when dirty cache blocks are evicted from the LLC and written into NVM-based memory (with PCM as an example), the long latency and high energy associated with write operations to NVM-based memory can cause system performance/power degradation. Thus, reducing the number of writeback requests from the LLC is critical. The proposed WADE cache management technique tries to keep highly reused dirty cache blocks in the LLC. The technique predicts blocks that are frequently written back in the LLC. The LLC sets are dynamically partitioned into a frequent writeback list and a nonfrequent writeback list. It keeps a best size of each list in the LLC. Our evaluation shows that the technique can reduce the number of writeback requests by 16.5% for memory-intensive single-threaded benchmarks and 10.8% for multicore workloads. It yields a geometric mean speedup of 5.1% for single-thread applications and 7.6% for multicore workloads. Due to the reduced number of writeback requests to main memory, the technique reduces the energy consumption by 8.1% for single-thread applications and 7.6% for multicore workloads.",
    "cited_by_count": 48,
    "openalex_id": "https://openalex.org/W2116394526",
    "type": "article"
  },
  {
    "title": "Improving Multibank Memory Access Parallelism with Lattice-Based Partitioning",
    "doi": "https://doi.org/10.1145/2675359",
    "publication_date": "2015-01-09",
    "publication_year": 2015,
    "authors": "Alessandro Cilardo; Luca Gallo",
    "corresponding_authors": "",
    "abstract": "Emerging architectures, such as reconfigurable hardware platforms, provide the unprecedented opportunity of customizing the memory infrastructure based on application access patterns. This work addresses the problem of automated memory partitioning for such architectures, taking into account potentially parallel data accesses to physically independent banks. Targeted at affine static control parts (SCoPs), the technique relies on the Z-polyhedral model for program analysis and adopts a partitioning scheme based on integer lattices. The approach enables the definition of a solution space including previous works as particular cases. The problem of minimizing the total amount of memory required across the partitioned banks, referred to as storage minimization throughout the article, is tackled by an optimal approach yielding asymptotically zero memory waste or, as an alternative, an efficient approach ensuring arbitrarily small waste. The article also presents a prototype toolchain and a detailed step-by-step case study demonstrating the impact of the proposed technique along with extensive comparisons with alternative approaches in the literature.",
    "cited_by_count": 46,
    "openalex_id": "https://openalex.org/W2000031176",
    "type": "article"
  },
  {
    "title": "Enabling GPGPU Low-Level Hardware Explorations with MIAOW",
    "doi": "https://doi.org/10.1145/2764908",
    "publication_date": "2015-06-24",
    "publication_year": 2015,
    "authors": "Raghuraman Balasubramanian; Vinay Gangadhar; Ziliang Guo; Chen-Han Ho; Cherin Joseph; J. Menon; Mario Drumond; Robin Paul; Sharath Prasad; Pradip Valathol; Karthikeyan Sankaralingam",
    "corresponding_authors": "",
    "abstract": "Graphic processing unit (GPU)-based general-purpose computing is developing as a viable alternative to CPU-based computing in many domains. Today’s tools for GPU analysis include simulators like GPGPU-Sim, Multi2Sim, and Barra. While useful for modeling first-order effects, these tools do not provide a detailed view of GPU microarchitecture and physical design. Further, as GPGPU research evolves, design ideas and modifications demand detailed estimates of impact on overall area and power. Fueled by this need, we introduce MIAOW (Many-core Integrated Accelerator Of Wisconsin), an open-source RTL implementation of the AMD Southern Islands GPGPU ISA, capable of running unmodified OpenCL-based applications. We present our design motivated by our goals to create a realistic, flexible, OpenCL-compatible GPGPU, capable of emulating a full system. We first explore if MIAOW is realistic and then use four case studies to show that MIAOW enables the following: physical design perspective to “traditional” microarchitecture, new types of research exploration, and validation/calibration of simulator-based characterization of hardware. The findings and ideas are contributions in their own right, in addition to MIAOW’s utility as a tool for others’ research.",
    "cited_by_count": 45,
    "openalex_id": "https://openalex.org/W799594534",
    "type": "article"
  },
  {
    "title": "GP-SIMD Processing-in-Memory",
    "doi": "https://doi.org/10.1145/2686875",
    "publication_date": "2015-01-09",
    "publication_year": 2015,
    "authors": "Amir Morad; Leonid Yavits; Ran Ginosar",
    "corresponding_authors": "",
    "abstract": "GP-SIMD, a novel hybrid general-purpose SIMD computer architecture, resolves the issue of data synchronization by in-memory computing through combining data storage and massively parallel processing. GP-SIMD employs a two-dimensional access memory with modified SRAM storage cells and a bit-serial processing unit per each memory row. An analytic performance model of the GP-SIMD architecture is presented, comparing it to associative processor and to conventional SIMD architectures. Cycle-accurate simulation of four workloads supports the analytical comparison. Assuming a moderate die area, GP-SIMD architecture outperforms both the associative processor and conventional SIMD coprocessor architectures by almost an order of magnitude while consuming less power.",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W2056118148",
    "type": "article"
  },
  {
    "title": "Static and Dynamic Frequency Scaling on Multicore CPUs",
    "doi": "https://doi.org/10.1145/3011017",
    "publication_date": "2016-12-28",
    "publication_year": 2016,
    "authors": "Wenlei Bao; Changwan Hong; Sudheer Chunduri; Sriram Krishnamoorthy; Louis-Noël Pouchet; Fabrice Rastello; P. Sadayappan",
    "corresponding_authors": "",
    "abstract": "Dynamic Voltage and Frequency Scaling (DVFS) typically adapts CPU power consumption by modifying a processor’s operating frequency (and the associated voltage). Typical DVFS approaches include using default strategies such as running at the lowest or the highest frequency or reacting to the CPU’s runtime load to reduce or increase frequency based on the CPU usage. In this article, we argue that a compile-time approach to CPU frequency selection is achievable for affine program regions and can significantly outperform runtime-based approaches. We first propose a lightweight runtime approach that can exploit the properties of the power profile specific to a processor, outperforming classical Linux governors such as powersave or on-demand for computational kernels. We then demonstrate that, for affine kernels in the application, a purely compile-time approach to CPU frequency and core count selection is achievable, providing significant additional benefits over the runtime approach. Our framework relies on a one-time profiling of the target CPU, along with a compile-time categorization of loop-based code segments in the application. These are combined to determine at compile-time the frequency and the number of cores to use to execute each affine region to optimize energy or energy-delay product. Extensive evaluation on 60 benchmarks and 5 multi-core CPUs show that our approach systematically outperforms the powersave Linux governor while also improving overall performance.",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W2567319156",
    "type": "article"
  },
  {
    "title": "A Neural Network Prefetcher for Arbitrary Memory Access Patterns",
    "doi": "https://doi.org/10.1145/3345000",
    "publication_date": "2019-10-15",
    "publication_year": 2019,
    "authors": "Leeor Peled; Uri Weiser; Yoav Etsion",
    "corresponding_authors": "",
    "abstract": "Accurate memory prefetching is paramount for processor performance, and modern processors employ various techniques to identify and prefetch different memory access patterns. While most modern prefetchers target spatio-temporal patterns by matching memory addresses that are accessed in close proximity (either in space or time), the recently proposed concept of semantic locality views locality as an artifact of the algorithmic level and searches for correlations between memory accesses and program state. While this approach was shown to be effective, capturing semantic locality requires significant associative learning capabilities. In this paper we utilize neural networks for this task. We leverage recent advances in machine learning to propose a neural network prefetcher. We show that by observing program context, this prefetcher can learn distinct memory access patterns that cannot be covered by other state-of-the-art prefetchers. We evaluate the neural network prefetcher over SPEC2006, Graph500, and several microbenchmarks. We show that the prefetcher can deliver an average speedup of 30% for SPEC2006 (up to 2.7x) and up to 4.6x over kernels. We also present a high-level design of our prefetcher, explore the power, energy and area limitations, and propose several optimizations for feasibility. We believe that this line of research can further improve the efficiency of such neural networks and allow harnessing them for additional micro-architectural predictions.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W3124943311",
    "type": "article"
  },
  {
    "title": "HRF-Relaxed",
    "doi": "https://doi.org/10.1145/2701618",
    "publication_date": "2015-04-16",
    "publication_year": 2015,
    "authors": "Benedict R. Gaster; Derek R. Hower; Lee Howes",
    "corresponding_authors": "",
    "abstract": "Memory consistency models, or memory models, allow both programmers and program language implementers to reason about concurrent accesses to one or more memory locations. Memory model specifications balance the often conflicting needs for precise semantics, implementation flexibility, and ease of understanding. Toward that end, popular programming languages like Java, C, and C++ have adopted memory models built on the conceptual foundation of Sequential Consistency for Data-Race-Free programs (SC for DRF). These SC for DRF languages were created with general-purpose homogeneous CPU systems in mind, and all assume a single, global memory address space. Such a uniform address space is usually power and performance prohibitive in heterogeneous Systems on Chips (SoCs), and for that reason most heterogeneous languages have adopted split address spaces and operations with nonglobal visibility. There have recently been two attempts to bridge the disconnect between the CPU-centric assumptions of the SC for DRF framework and the realities of heterogeneous SoC architectures. Hower et al. proposed a class of Heterogeneous-Race-Free (HRF) memory models that provide a foundation for understanding many of the issues in heterogeneous memory models. At the same time, the Khronos Group developed the OpenCL 2.0 memory model that builds on the C++ memory model. The OpenCL 2.0 model includes features not addressed by HRF: primarily support for relaxed atomics and a property referred to as scope inclusion. In this article, we generalize HRF to allow formalization of and reasoning about more complicated models using OpenCL 2.0 as a point of reference. With that generalization, we (1) make the OpenCL 2.0 memory model more accessible by introducing a platform for feature comparisons to other models, (2) consider a number of shortcomings in the current OpenCL 2.0 model, and (3) propose changes that could be adopted by future OpenCL 2.0 revisions or by other, related, models.",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W2083780331",
    "type": "article"
  },
  {
    "title": "A Compiler Approach for Exploiting Partial SIMD Parallelism",
    "doi": "https://doi.org/10.1145/2886101",
    "publication_date": "2016-03-28",
    "publication_year": 2016,
    "authors": "Hao Zhou; Jingling Xue",
    "corresponding_authors": "",
    "abstract": "Existing vectorization techniques are ineffective for loops that exhibit little loop-level parallelism but some limited superword-level parallelism (SLP). We show that effectively vectorizing such loops requires partial vector operations to be executed correctly and efficiently, where the degree of partial SIMD parallelism is smaller than the SIMD datapath width. We present a simple yet effective SLP compiler technique called P aver (PArtial VEctorizeR), formulated and implemented in LLVM as a generalization of the traditional SLP algorithm, to optimize such partially vectorizable loops. The key idea is to maximize SIMD utilization by widening vector instructions used while minimizing the overheads caused by memory access, packing/unpacking, and/or masking operations, without introducing new memory errors or new numeric exceptions. For a set of 9 C/C++/Fortran applications with partial SIMD parallelism, P aver achieves significantly better kernel and whole-program speedups than LLVM on both Intel’s AVX and ARM’s NEON.",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W2333659671",
    "type": "article"
  },
  {
    "title": "A RISC-V Simulator and Benchmark Suite for Designing and Evaluating Vector Architectures",
    "doi": "https://doi.org/10.1145/3422667",
    "publication_date": "2020-11-10",
    "publication_year": 2020,
    "authors": "Marco A. Ramírez; César Alejandro Hernández; Oscar Palomar; Osman Ünsal; Marco Antonio Ramírez; Adrián Cristal",
    "corresponding_authors": "",
    "abstract": "Vector architectures lack tools for research. Consider the gem5 simulator, which is possibly the leading platform for computer-system architecture research. Unfortunately, gem5 does not have an available distribution that includes a flexible and customizable vector architecture model. In consequence, researchers have to develop their own simulation platform to test their ideas, which consume much research time. However, once the base simulator platform is developed, another question is the following: Which applications should be tested to perform the experiments? The lack of Vectorized Benchmark Suites is another limitation. To face these problems, this work presents a set of tools for designing and evaluating vector architectures. First, the gem5 simulator was extended to support the execution of RISC-V Vector instructions by adding a parameterizable Vector Architecture model for designers to evaluate different approaches according to the target they pursue. Second, a novel Vectorized Benchmark Suite is presented: a collection composed of seven data-parallel applications from different domains that can be classified according to the modules that are stressed in the vector architecture. Finally, a study of the Vectorized Benchmark Suite executing on the gem5-based Vector Architecture model is highlighted. This suite is the first in its category that covers the different possible usage scenarios that may occur within different vector architecture designs such as embedded systems, mainly focused on short vectors, or High-Performance-Computing (HPC), usually designed for large vectors.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W3102724434",
    "type": "article"
  },
  {
    "title": "The Next 700 Accelerated Layers",
    "doi": "https://doi.org/10.1145/3355606",
    "publication_date": "2019-10-11",
    "publication_year": 2019,
    "authors": "Nicolas Vasilache; Oleksandr Zinenko; Theodoros Theodoridis; Priya Goyal; Zachary DeVito; William S. Moses; Sven Verdoolaege; Andrew Adams; Albert Cohen",
    "corresponding_authors": "",
    "abstract": "Deep learning frameworks automate the deployment, distribution, synchronization, memory allocation, and hardware acceleration of models represented as graphs of computational operators. These operators wrap high-performance libraries such as cuDNN or NNPACK. When the computation does not match any predefined library call, custom operators must be implemented, often at high engineering cost and performance penalty, limiting the pace of innovation. To address this productivity gap, we propose and evaluate: (1) a domain-specific language with a tensor notation close to the mathematics of deep learning; (2) a Just-In-Time optimizing compiler based on the polyhedral framework; (3) carefully coordinated linear optimization and evolutionary algorithms to synthesize high-performance CUDA kernels; (4) the transparent integration of our flow into PyTorch and Caffe2, providing the fully automatic synthesis of high-performance GPU kernels from simple tensor algebra. The performance is comparable to, and often exceeds the performance of, highly tuned libraries.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W2979365412",
    "type": "article"
  },
  {
    "title": "SLO-Aware Inference Scheduler for Heterogeneous Processors in Edge Platforms",
    "doi": "https://doi.org/10.1145/3460352",
    "publication_date": "2021-07-17",
    "publication_year": 2021,
    "authors": "Wonik Seo; Sang-Hoon Cha; Yeonjae Kim; Jaehyuk Huh; Jongse Park",
    "corresponding_authors": "",
    "abstract": "With the proliferation of applications with machine learning (ML), the importance of edge platforms has been growing to process streaming sensor, data locally without resorting to remote servers. Such edge platforms are commonly equipped with heterogeneous computing processors such as GPU, DSP, and other accelerators, but their computational and energy budget are severely constrained compared to the data center servers. However, as an edge platform must perform the processing of multiple machine learning models concurrently for multimodal sensor data, its scheduling problem poses a new challenge to map heterogeneous machine learning computation to heterogeneous computing processors. Furthermore, processing of each input must provide a certain level of bounded response latency, making the scheduling decision critical for the edge platform. This article proposes a set of new heterogeneity-aware ML inference scheduling policies for edge platforms. Based on the regularity of computation in common ML tasks, the scheduler uses the pre-profiled behavior of each ML model and routes requests to the most appropriate processors. It also aims to satisfy the service-level objective (SLO) requirement while reducing the energy consumption for each request. For such SLO supports, the challenge of ML computation on GPUs and DSP is its inflexible preemption capability. To avoid the delay caused by a long task, the proposed scheduler decomposes a large ML task to sub-tasks by its layer in the DNN model.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W3186289964",
    "type": "article"
  },
  {
    "title": "PiDRAM: A Holistic End-to-end FPGA-based Framework for Processing-in-DRAM",
    "doi": "https://doi.org/10.1145/3563697",
    "publication_date": "2022-09-14",
    "publication_year": 2022,
    "authors": "Ataberk Olgun; Juan Gómez-Luna; Konstantinos Kanellopoulos; Behzad Salami; Hasan Hassan; Oğuz Ergin; Onur Mutlu",
    "corresponding_authors": "",
    "abstract": "Commodity DRAM-based processing-using-memory (PuM) techniques that are supported by off-the-shelf DRAM chips present an opportunity for alleviating the data movement bottleneck at low cost. However, system integration of these techniques imposes non-trivial challenges that are yet to be solve d . Potential solutions to the integration challenges require appropriate tools to develop any necessary hardware and software components. Unfortunately, current proprietary computing systems, specialized DRAM-testing platforms, or system simulators do not provide the flexibility and/or the holistic system view that is necessary to properly evaluate and deal with the integration challenges of commodity DRAM-based PuM techniques. We design and develop Processing-in-DRAM (PiDRAM), the first flexible end-to-end framework that enables system integration studies and evaluation of real, commodity DRAM-based PuM techniques. PiDRAM provides software and hardware components to rapidly integrate PuM techniques across the whole system software and hardware stack. We implement PiDRAM on an FPGA-based RISC-V system. To demonstrate the flexibility and ease of use of PiDRAM, we implement and evaluate two state-of-the-art commodity DRAM-based PuM techniques: (i) in-DRAM copy and initialization (RowClone) and (ii) in-DRAM true random number generation (D-RaNGe) . We describe how we solve key integration challenges to make such techniques work and be effective on a real-system prototype, including memory allocation, alignment, and coherence. We observe that end-to-end RowClone speeds up bulk copy and initialization operations by 14.6× and 12.6×, respectively, over conventional CPU copy, even when coherence is supported with inefficient cache flush operations. Over PiDRAM’s extensible codebase, integrating both RowClone and D-RaNGe end-to-end on a real RISC-V system prototype takes only 388 lines of Verilog code and 643 lines of C++ code.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W4295679041",
    "type": "article"
  },
  {
    "title": "SplitZNS: Towards an Efficient LSM-Tree on Zoned Namespace SSDs",
    "doi": "https://doi.org/10.1145/3608476",
    "publication_date": "2023-07-10",
    "publication_year": 2023,
    "authors": "Dong Huang; Dan Feng; Qiankun Liu; Bo Ding; Wei Zhao; Xueliang Wei; Wei Tong",
    "corresponding_authors": "",
    "abstract": "The Zoned Namespace (ZNS) Solid State Drive (SSD) is a nascent form of storage device that offers novel prospects for the Log Structured Merge Tree (LSM-tree). ZNS exposes erase blocks in SSD as append-only zones, enabling the LSM-tree to gain awareness of the physical layout of data. Nevertheless, LSM-tree on ZNS SSDs necessitates Garbage Collection (GC) owing to the mismatch between the gigantic zones and relatively small Sorted String Tables (SSTables). Through extensive experiments, we observe that a smaller zone size can reduce data migration in GC at the cost of a significant performance decline owing to inadequate parallelism exploitation. In this article, we present SplitZNS, which introduces small zones by tweaking the zone-to-chip mapping to maximize GC efficiency for LSM-tree on ZNS SSDs. Following the multi-level peculiarity of LSM-tree and the inherent parallel architecture of ZNS SSDs, we propose a number of techniques to leverage and accelerate small zones to alleviate the performance impact due to underutilized parallelism. (1) First, we use small zones selectively to prevent exacerbating write slowdowns and stalls due to their suboptimal performance. (2) Second, to enhance parallelism utilization, we propose SubZone Ring, which employs a per-chip FIFO buffer to imitate a large zone writing style; (3) Read Prefetcher, which prefetches data concurrently through multiple chips during compactions; (4) and Read Scheduler, which assigns query requests the highest priority. We build a prototype integrated with SplitZNS to validate its efficiency and efficacy. Experimental results demonstrate that SplitZNS achieves up to 2.77× performance and reduces data migration considerably compared to the lifetime-based data placement. 1",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W4383816991",
    "type": "article"
  },
  {
    "title": "A study of thread migration in temperature-constrained multicores",
    "doi": "https://doi.org/10.1145/1250727.1250729",
    "publication_date": "2007-06-01",
    "publication_year": 2007,
    "authors": "Pierre Michaud; André Seznec; Damien Fétis; Yiannakis Sazeides; Theofanis Constantinou",
    "corresponding_authors": "",
    "abstract": "Temperature has become an important constraint in high-performance processors, especially multicores. Thread migration will be essential to exploit the full potential of future thermally constrained multicores. We propose and study a thread migration method that maximizes performance under a temperature constraint, while minimizing the number of migrations and ensuring fairness between threads. We show that thread migration brings important performance gains and that it is most effective during the first tens of seconds following a decrease of the number of running threads.",
    "cited_by_count": 57,
    "openalex_id": "https://openalex.org/W1973789144",
    "type": "article"
  },
  {
    "title": "Practical exhaustive optimization phase order exploration and evaluation",
    "doi": "https://doi.org/10.1145/1509864.1509865",
    "publication_date": "2009-03-30",
    "publication_year": 2009,
    "authors": "Prasad A. Kulkarni; David Whalley; Gary Tyson; Jack W. Davidson",
    "corresponding_authors": "",
    "abstract": "Choosing the most appropriate optimization phase ordering has been a long-standing problem in compiler optimizations. Exhaustive evaluation of all possible orderings of optimization phases for each function is generally dismissed as infeasible for production-quality compilers targeting accepted benchmarks. In this article, we show that it is possible to exhaustively evaluate the optimization phase order space for each function in a reasonable amount of time for most of the functions in our benchmark suite. To achieve this goal, we used various techniques to significantly prune the optimization phase order search space so that it can be inexpensively enumerated in most cases and reduce the number of program simulations required to evaluate program performance for each distinct phase ordering. The techniques described are applicable to other compilers in which it is desirable to find the best phase ordering for most functions in a reasonable amount of time. We also describe some interesting properties of the optimization phase order space, which will prove useful for further studies of related problems in compilers.",
    "cited_by_count": 55,
    "openalex_id": "https://openalex.org/W2077115703",
    "type": "article"
  },
  {
    "title": "Efficient hardware code generation for FPGAs",
    "doi": "https://doi.org/10.1145/1369396.1369402",
    "publication_date": "2008-05-01",
    "publication_year": 2008,
    "authors": "Zhi Guo; Walid Najjar; Betul Buyukkurt",
    "corresponding_authors": "",
    "abstract": "The wider acceptance of FPGAs as a computing device requires a higher level of programming abstraction. ROCCC is an optimizing C to HDL compiler. We describe the code generation approach in ROCCC. The smart buffer is a component that reuses input data between adjacent iterations. It significantly improves the performance of the circuit and simplifies loop control. The ROCCC-generated datapath can execute one loop iteration per clock cycle when there is no loop dependency or there is only scalar recurrence variable dependency. ROCCC's approach to supporting while-loops operating on scalars makes the compiler able to move scalar iterative computation into hardware.",
    "cited_by_count": 55,
    "openalex_id": "https://openalex.org/W2084069479",
    "type": "article"
  },
  {
    "title": "A memory-efficient pipelined implementation of the aho-corasick string-matching algorithm",
    "doi": "https://doi.org/10.1145/1839667.1839672",
    "publication_date": "2010-09-01",
    "publication_year": 2010,
    "authors": "Derek Pao; Wei Lin; Bin Liu",
    "corresponding_authors": "",
    "abstract": "With rapid advancement in Internet technology and usages, some emerging applications in data communications and network security require matching of huge volume of data against large signature sets with thousands of strings in real time. In this article, we present a memory-efficient hardware implementation of the well-known Aho-Corasick (AC) string-matching algorithm using a pipelining approach called P-AC. An attractive feature of the AC algorithm is that it can solve the string-matching problem in time linearly proportional to the length of the input stream, and the computation time is independent of the number of strings in the signature set. A major disadvantage of the AC algorithm is the high memory cost required to store the transition rules of the underlying deterministic finite automaton. By incorporating pipelined processing, the state graph is reduced to a character trie that only contains forward edges. Together with an intelligent implementation of look-up tables, the memory cost of P-AC is only about 18 bits per character for a signature set containing 6,166 strings extracted from Snort. The control structure of P-AC is simple and elegant. The cost of the control logic is very low. With the availability of dual-port memories in FPGA devices, we can double the system throughput by duplicating the control logic such that the system can process two data streams concurrently. Since our method is memory-based, incremental changes to the signature set can be accommodated by updating the look-up tables without reconfiguring the FPGA circuitry.",
    "cited_by_count": 48,
    "openalex_id": "https://openalex.org/W1982420044",
    "type": "article"
  },
  {
    "title": "Power gating strategies on GPUs",
    "doi": "https://doi.org/10.1145/2019608.2019612",
    "publication_date": "2011-10-01",
    "publication_year": 2011,
    "authors": "Po-Han Wang; Chia-Lin Yang; Yen‐Ming Chen; Yu-Jung Cheng",
    "corresponding_authors": "",
    "abstract": "As technology continues to shrink, reducing leakage is critical to achieving energy efficiency. Previous studies on low-power GPUs (Graphics Processing Units) focused on techniques for dynamic power reduction, such as DVFS (Dynamic Voltage and Frequency Scaling) and clock gating. In this paper, we explore the potential of adopting architecture-level power gating techniques for leakage reduction on GPUs. We propose three strategies for applying power gating on different modules in GPUs. The Predictive Shader Shutdown technique exploits workload variation across frames to eliminate leakage in shader clusters. Deferred Geometry Pipeline seeks to minimize leakage in fixed-function geometry units by utilizing an imbalance between geometry and fragment computation across batches. Finally, the simple time-out power gating method is applied to nonshader execution units to exploit a finer granularity of the idle time. Our results indicate that Predictive Shader Shutdown eliminates up to 60% of the leakage in shader clusters, Deferred Geometry Pipeline removes up to 57% of the leakage in the fixed-function geometry units, and the simple time-out power gating mechanism eliminates 83.3% of the leakage in nonshader execution units on average. All three schemes incur negligible performance degradation, less than 1%.",
    "cited_by_count": 47,
    "openalex_id": "https://openalex.org/W2024001079",
    "type": "article"
  },
  {
    "title": "Deconstructing iterative optimization",
    "doi": "https://doi.org/10.1145/2355585.2355594",
    "publication_date": "2012-09-01",
    "publication_year": 2012,
    "authors": "Yang Chen; Shuangde Fang; Yuanjie Huang; Lieven Eeckhout; Grigori Fursin; Olivier Temam; Chengyong Wu",
    "corresponding_authors": "",
    "abstract": "Iterative optimization is a popular compiler optimization approach that has been studied extensively over the past decade. In this article, we deconstruct iterative optimization by evaluating whether it works across datasets and by analyzing why it works. Up to now, most iterative optimization studies are based on a premise which was never truly evaluated: that it is possible to learn the best compiler optimizations across datasets. In this article, we evaluate this question for the first time with a very large number of datasets. We therefore compose KDataSets, a dataset suite with 1000 datasets for 32 programs, which we release to the public. We characterize the diversity of KDataSets, and subsequently use it to evaluate iterative optimization. For all 32 programs, we find that there exists at least one combination of compiler optimizations that achieves at least 83% or more of the best possible speedup across all datasets on two widely used compilers (Intel's ICC and GNU's GCC). This optimal combination is program-specific and yields speedups up to 3.75× (averaged across datasets of a program) over the highest optimization level of the compilers (-O3 for GCC and -fast for ICC). This finding suggests that optimizing programs across datasets might be much easier than previously anticipated. In addition, we evaluate the idea of introducing compiler choice as part of iterative optimization. We find that it can further improve the performance of iterative optimization because different programs favor different compilers. We also investigate why iterative optimization works by analyzing the optimal combinations. We find that only a handful optimizations yield most of the speedup. Finally, we show that optimizations interact in a complex and sometimes counterintuitive way through two case studies, which confirms that iterative optimization is an irreplaceable and important compiler strategy.",
    "cited_by_count": 47,
    "openalex_id": "https://openalex.org/W2031706008",
    "type": "article"
  },
  {
    "title": "A Massively Parallel, Energy Efficient Programmable Accelerator for Learning and Classification",
    "doi": "https://doi.org/10.1145/2133382.2133388",
    "publication_date": "2012-03-01",
    "publication_year": 2012,
    "authors": "Abhinandan Majumdar; Srihari Cadambi; Michela Becchi; Srimat Chakradhar; Hans Peter Graf",
    "corresponding_authors": "",
    "abstract": "Applications that use learning and classification algorithms operate on large amounts of unstructured data, and have stringent performance constraints. For such applications, the performance of general purpose processors scales poorly with data size because of their limited support for fine-grained parallelism and absence of software-managed caches. The large intermediate data in these applications also limits achievable performance on many-core processors such as GPUs. To accelerate such learning applications, we present a programmable accelerator that can execute multiple learning and classification algorithms. To architect such an accelerator, we profile five representative workloads, and find that their computationally intensive portions can be formulated as matrix or vector operations generating large amounts of intermediate data, which are then reduced by a secondary operation such as array ranking, finding max/min and aggregation. Our proposed accelerator, called MAPLE, has hundreds of simple processing elements (PEs) laid out in a two-dimensional grid, with two key features. First, it uses dynamic in-memory processing where on-chip memory blocks perform the secondary reduction operations. Second, MAPLE uses banked off-chip memory, and organizes its PEs into independent groups each with its own off-chip memory bank. These two features allow MAPLE to scale its performance with data size. We also present an Atom based energy-efficient heterogeneous system with MAPLE as the accelerator that satisfies the application’s performance requirements at a lower system power. This article describes the MAPLE architecture, explores its design space with a simulator, illustrates how to automatically map application kernels to the hardware, and presents its performance improvement and energy benefits over classic server-based implementations. We implement a 512-PE FPGA prototype of MAPLE and find that it is 1.5-10x faster than a 2.5 GHz quad-core Xeon processor despite running at a modest 125 MHz clock rate. With MAPLE connected to a 1.6GHz dual-core Atom, we show an energy improvement of 38-84% over the Xeon server coupled to a 1.3 GHz 240 core Tesla GPU.",
    "cited_by_count": 45,
    "openalex_id": "https://openalex.org/W2043607059",
    "type": "article"
  },
  {
    "title": "Using machine learning to partition streaming programs",
    "doi": "https://doi.org/10.1145/2512436",
    "publication_date": "2013-09-16",
    "publication_year": 2013,
    "authors": "Zheng Wang; Michael O’Boyle",
    "corresponding_authors": "",
    "abstract": "Stream-based parallel languages are a popular way to express parallelism in modern applications. The efficient mapping of streaming parallelism to today's multicore systems is, however, highly dependent on the program and underlying architecture. We address this by developing a portable and automatic compiler-based approach to partitioning streaming programs using machine learning. Our technique predicts the ideal partition structure for a given streaming application using prior knowledge learned offline. Using the predictor we rapidly search the program space (without executing any code) to generate and select a good partition. We applied this technique to standard StreamIt applications and compared against existing approaches. On a 4-core platform, our approach achieves 60% of the best performance found by iteratively compiling and executing over 3000 different partitions per program. We obtain, on average, a 1.90× speedup over the already tuned partitioning scheme of the StreamIt compiler. When compared against a state-of-the-art analytical, model-based approach, we achieve, on average, a 1.77× performance improvement. By porting our approach to an 8-core platform, we are able to obtain 1.8× improvement over the StreamIt default scheme, demonstrating the portability of our approach.",
    "cited_by_count": 45,
    "openalex_id": "https://openalex.org/W2099680095",
    "type": "article"
  },
  {
    "title": "HC-CART",
    "doi": "https://doi.org/10.1145/2400682.2400706",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Grigorios Chrysos; Panagiotis Dagritzikos; Ioannis Papaefstathiou; Apostolos Dollas",
    "corresponding_authors": "",
    "abstract": "Data mining is a new field of computer science with a wide range of applications. Its goal is to extract knowledge from massive datasets in a human-understandable structure, for example, the decision trees. In this article we present an innovative, high-performance, system-level architecture for the Classification And Regression Tree (CART) algorithm, one of the most important and widely used algorithms in the data mining area. Our proposed architecture exploits parallelism at the decision variable level, and was fully implemented and evaluated on a modern high-performance reconfigurable platform, the Convey HC-1 server, that features four FPGAs and a multicore processor. Our FPGA-based implementation was integrated with the widely used “rpart” software library of the R project in order to provide the first fully functional reconfigurable system that can handle real-world large databases. The proposed system, named HC-CART system, achieves a performance speedup of up to two orders of magnitude compared to well-known single-threaded data mining software platforms, such as WEKA and the R platform. It also outperforms similar hardware systems which implement parts of the complete application by an order of magnitude. Finally, we show that the HC-CART system offers higher performance speedup than some other proposed parallel software implementations of decision tree construction algorithms.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W1970322815",
    "type": "article"
  },
  {
    "title": "Topological Characterization of Hamming and Dragonfly Networks and Its Implications on Routing",
    "doi": "https://doi.org/10.1145/2677038",
    "publication_date": "2014-12-08",
    "publication_year": 2014,
    "authors": "Cristóbal Camarero; Enrique Vallejo; Ramón Beivide",
    "corresponding_authors": "",
    "abstract": "Current High-Performance Computing (HPC) and data center networks rely on large-radix routers. Hamming graphs (Cartesian products of complete graphs) and dragonflies (two-level direct networks with nodes organized in groups) are some direct topologies proposed for such networks. The original definition of the dragonfly topology is very loose, with several degrees of freedom, such as the inter- and intragroup topology, the specific global connectivity, and the number of parallel links between groups (or trunking level). This work provides a comprehensive analysis of the topological properties of the dragonfly network, providing balancing conditions for network dimensioning, as well as introducing and classifying several alternatives for the global connectivity and trunking level. From a topological study of the network, it is noted that a Hamming graph can be seen as a canonical dragonfly topology with a high level of trunking. Based on this observation and by carefully selecting the global connectivity, the Dimension Order Routing (DOR) mechanism safely used in Hamming graphs is adapted to dragonfly networks with trunking. The resulting routing algorithms approximate the performance of minimal, nonminimal, and adaptive routings typically used in dragonflies but without requiring virtual channels to avoid packet deadlock, thus allowing for lower cost router implementations. This is obtained by properly selecting the link to route between groups based on a graph coloring of network routers. Evaluations show that the proposed mechanisms are competitive with traditional solutions when using the same number of virtual channels and enable for simpler implementations with lower cost. Finally, multilevel dragonflies are discussed, considering how the proposed mechanisms could be adapted to them.",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W1979056866",
    "type": "article"
  },
  {
    "title": "PARSECSs",
    "doi": "https://doi.org/10.1145/2829952",
    "publication_date": "2015-12-22",
    "publication_year": 2015,
    "authors": "Dimitrios Chasapis; Marc Casas; Miquel Moretó; Raul Vidal; Eduard Ayguadé; Jesús Labarta; Mateo Valero",
    "corresponding_authors": "",
    "abstract": "In this work, we show how parallel applications can be implemented efficiently using task parallelism. We also evaluate the benefits of such parallel paradigm with respect to other approaches. We use the PARSEC benchmark suite as our test bed, which includes applications representative of a wide range of domains from HPC to desktop and server applications. We adopt different parallelization techniques, tailored to the needs of each application, to fully exploit the task-based model. Our evaluation shows that task parallelism achieves better performance than thread-based parallelization models, such as Pthreads. Our experimental results show that we can obtain scalability improvements up to 42% on a 16-core system and code size reductions up to 81%. Such reductions are achieved by removing from the source code application specific schedulers or thread pooling systems and transferring these responsibilities to the runtime system software.",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W2259595552",
    "type": "article"
  },
  {
    "title": "Bahurupi",
    "doi": "https://doi.org/10.1145/2086696.2086701",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Mihai Pricopi; Tulika Mitra",
    "corresponding_authors": "",
    "abstract": "Computing systems have made an irreversible transition towards parallel architectures with the emergence of multi-cores. Moreover, power and thermal limits in embedded systems mandate the deployment of many simpler cores rather than a few complex cores on chip. Consumer electronic devices, on the other hand, need to support an ever-changing set of diverse applications with varying performance demands. While some applications can benefit from thread-level parallelism offered by multi-core solutions, there still exist a large number of applications with substantial amount of sequential code. The sequential programs suffer from limited exploitation of instruction-level parallelism in simple cores. We propose a reconfigurable multi-core architecture, called Bahurupi, that can successfully reconcile the conflicting demands of instruction-level and thread-level parallelism. Bahurupi can accelerate the performance of serial code by dynamically forming coalition of two or more simple cores to offer increased instruction-level parallelism. In particular, Bahurupi can efficiently merge 2-4 simple 2-way out-of-order cores to reach or even surpass the performance of more complex and power-hungry 4-way or 8-way out-of-order core. Compared to baseline 2-way core, quad-core Bahurupi achieves up to 5.61 speedup (average 4.08 speedup) for embedded workloads. On an average, quad-core Bahurupi achieves 17% performance improvement and 43% improvement in energy consumption compared to 8-way out-of-order baseline core on a diverse set of embedded benchmark applications.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W2066789793",
    "type": "article"
  },
  {
    "title": "MAGIC",
    "doi": "https://doi.org/10.1145/2724718",
    "publication_date": "2015-04-02",
    "publication_year": 2015,
    "authors": "Naghmeh Karimi; Arun K. Kanuparthi; Xueyang Wang; Ozgur Sinanoglu; Ramesh Karri",
    "corresponding_authors": "",
    "abstract": "The performance of an IC degrades over its lifetime, ultimately resulting in IC failure. In this article, we present a hardware attack (called MAGIC) to maliciously accelerate NBTI aging effects in cores. In this attack, we identify the input patterns that maliciously age the pipestages of a core. We then craft a program that generates these patterns at the inputs of the targeted pipestage. We demonstrate the MAGIC-based attack on the OpenSPARC processor. Executing this program dramatically accelerates the aging process and degrades the processor’s performance by 10.92% in 1 month, bypassing existing aging mitigation and timing-error correction schemes. We also present two low-cost techniques to thwart the proposed attack.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W2106075707",
    "type": "article"
  },
  {
    "title": "CERE",
    "doi": "https://doi.org/10.1145/2724717",
    "publication_date": "2015-04-16",
    "publication_year": 2015,
    "authors": "Pablo de Oliveira Castro; Chadi Akel; Eric Petit; Mihail Popov; William Jalby",
    "corresponding_authors": "",
    "abstract": "This article presents Codelet Extractor and REplayer (CERE), an open-source framework for code isolation. CERE finds and extracts the hotspots of an application as isolated fragments of code, called codelets . Codelets can be modified, compiled, run, and measured independently from the original application. Code isolation reduces benchmarking cost and allows piecewise optimization of an application. Unlike previous approaches, CERE isolates codes at the compiler Intermediate Representation (IR) level. Therefore CERE is language agnostic and supports many input languages such as C, C++, Fortran, and D. CERE automatically detects codelets invocations that have the same performance behavior. Then, it selects a reduced set of representative codelets and invocations, much faster to replay, which still captures accurately the original application. In addition, CERE supports recompiling and retargeting the extracted codelets. Therefore, CERE can be used for cross-architecture performance prediction or piecewise code optimization. On the SPEC 2006 FP benchmarks, CERE codelets cover 90.9% and accurately replay 66.3% of the execution time. We use CERE codelets in a realistic study to evaluate three different architectures on the NAS benchmarks. CERE accurately estimates each architecture performance and is 7.3 × to 46.6 × cheaper than running the full benchmark.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W2114669797",
    "type": "article"
  },
  {
    "title": "Fine-Grain Power Breakdown of Modern Out-of-Order Cores and Its Implications on Skylake-Based Systems",
    "doi": "https://doi.org/10.1145/3018112",
    "publication_date": "2016-12-16",
    "publication_year": 2016,
    "authors": "Jawad Haj-Yihia; Ahmad Yasin; Yosi Ben Asher; Avi Mendelson",
    "corresponding_authors": "",
    "abstract": "A detailed analysis of power consumption at low system levels becomes important as a means for reducing the overall power consumption of a system and its thermal hot spots. This work presents a new power estimation method that allows understanding the power breakdown of an application when running on modern processor architecture such as the newly released Intel Skylake processor. This work also provides a detailed power and performance characterization report for the SPEC CPU2006 benchmarks, analysis of the data using side-by-side power and performance breakdowns, as well as few interesting case studies.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W2561036834",
    "type": "article"
  },
  {
    "title": "Topology-Aware and Dependence-Aware Scheduling and Memory Allocation for Task-Parallel Languages",
    "doi": "https://doi.org/10.1145/2641764",
    "publication_date": "2014-08-25",
    "publication_year": 2014,
    "authors": "Andi Drebes; Karine Heydemann; Nathalie Drach; Antoniu Pop; Albert Cohen",
    "corresponding_authors": "",
    "abstract": "We present a joint scheduling and memory allocation algorithm for efficient execution of task-parallel programs on non-uniform memory architecture (NUMA) systems. Task and data placement decisions are based on a static description of the memory hierarchy and on runtime information about intertask communication. Existing locality-aware scheduling strategies for fine-grained tasks have strong limitations: they are specific to some class of machines or applications, they do not handle task dependences, they require manual program annotations, or they rely on fragile profiling schemes. By contrast, our solution makes no assumption on the structure of programs or on the layout of data in memory. Experimental results, based on the OpenStream language, show that locality of accesses to main memory of scientific applications can be increased significantly on a 64-core machine, resulting in a speedup of up to 1.63× compared to a state-of-the-art work-stealing scheduler.",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W2159184943",
    "type": "article"
  },
  {
    "title": "Resistive GP-SIMD Processing-In-Memory",
    "doi": "https://doi.org/10.1145/2845084",
    "publication_date": "2016-01-06",
    "publication_year": 2016,
    "authors": "Amir Morad; Leonid Yavits; Shahar Kvatinsky; Ran Ginosar",
    "corresponding_authors": "",
    "abstract": "GP-SIMD, a novel hybrid general-purpose SIMD architecture, addresses the challenge of data synchronization by in-memory computing, through combining data storage and massive parallel processing. In this article, we explore a resistive implementation of the GP-SIMD architecture. In resistive GP-SIMD, a novel resistive row and column addressable 4F 2 crossbar is utilized, replacing the modified CMOS 190F 2 SRAM storage previously proposed for GP-SIMD architecture. The use of the resistive crossbar allows scaling the GP-SIMD from few millions to few hundred millions of processing units on a single silicon die. The performance, power consumption and power efficiency of a resistive GP-SIMD are compared with the CMOS version. We find that PiM architectures and, specifically, GP-SIMD benefit more than other many-core architectures from using resistive memory. A framework for in-place arithmetic operation on a single multivalued resistive cell is explored, demonstrating a potential to become a building block for next-generation PiM architectures.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W2250636351",
    "type": "article"
  },
  {
    "title": "Building Heterogeneous Unified Virtual Memories (UVMs) without the Overhead",
    "doi": "https://doi.org/10.1145/2889488",
    "publication_date": "2016-03-28",
    "publication_year": 2016,
    "authors": "Konstantinos Koukos; Alberto Ros; Erik Hägersten; Stefanos Kaxiras",
    "corresponding_authors": "",
    "abstract": "This work proposes a novel scheme to facilitate heterogeneous systems with unified virtual memory. Research proposals implement coherence protocols for sequential consistency (SC) between central processing unit (CPU) cores and between devices. Such mechanisms introduce severe bottlenecks in the system; therefore, we adopt the heterogeneous-race-free (HRF) memory model. The use of HRF simplifies the coherency protocol and the graphics processing unit (GPU) memory management unit (MMU). Our protocol optimizes CPU and GPU demands separately, with the GPU part being simpler while the CPU is more elaborate and latency aware. We achieve an average 45% speedup and 45% energy-delay product reduction (20% energy) over the corresponding SC implementation.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W2319071579",
    "type": "article"
  },
  {
    "title": "Compiler-Assisted Loop Hardening Against Fault Attacks",
    "doi": "https://doi.org/10.1145/3141234",
    "publication_date": "2017-12-05",
    "publication_year": 2017,
    "authors": "Julien Proy; Karine Heydemann; Alexandre Berzati; Albert Cohen",
    "corresponding_authors": "",
    "abstract": "Secure elements widely used in smartphones, digital consumer electronics, and payment systems are subject to fault attacks. To thwart such attacks, software protections are manually inserted requiring experts and time. The explosion of the Internet of Things (IoT) in home, business, and public spaces motivates the hardening of a wider class of applications and the need to offer security solutions to non-experts. This article addresses the automated protection of loops at compilation time, covering the widest range of control- and data-flow patterns, in both shape and complexity. The security property we consider is that a sensitive loop must always perform the expected number of iterations; otherwise, an attack must be reported. We propose a generic compile-time loop hardening scheme based on the duplication of termination conditions and of the computations involved in the evaluation of such conditions. We also investigate how to preserve the security property along the compilation flow while enabling aggressive optimizations. We implemented this algorithm in LLVM 4.0 at the Intermediate Representation (IR) level in the backend. On average, the compiler automatically hardens 95% of the sensitive loops of typical security benchmarks, and 98% of these loops are shown to be robust to simulated faults. Performance and code size overhead remain quite affordable, at 12.5% and 14%, respectively.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W2773049795",
    "type": "article"
  },
  {
    "title": "Yet Another Compressed Cache",
    "doi": "https://doi.org/10.1145/2976740",
    "publication_date": "2016-09-17",
    "publication_year": 2016,
    "authors": "Somayeh Sardashti; André Seznec; David A. Wood",
    "corresponding_authors": "",
    "abstract": "Cache memories play a critical role in bridging the latency, bandwidth, and energy gaps between cores and off-chip memory. However, caches frequently consume a significant fraction of a multicore chip's area and thus account for a significant fraction of its cost. Compression has the potential to improve the effective capacity of a cache, providing the performance and energy benefits of a larger cache while using less area. The design of a compressed cache must address two important issues: (i) a low-latency, low-overhead compression algorithm that can represent a fixed-size cache block using fewer bits and (ii) a cache organization that can efficiently store the resulting variable-size compressed blocks. This article focuses on the latter issue. Here, we propose Yet Another Compressed Cache (YACC), a new compressed cache design that targets improving effective cache capacity with a simple design. YACC uses super-blocks to reduce tag overheads while packing variable-size compressed blocks to reduce internal fragmentation. YACC achieves the benefits of two state-of-the art compressed caches—Decoupled Compressed Cache (DCC) [Sardashti and Wood 2013a, 2013b] and Skewed Compressed Cache (SCC) [Sardashti et al. 2014]—with a more practical and simpler design. YACC's cache layout is similar to conventional caches, with a largely unmodified tag array and unmodified data array. Compared to DCC and SCC, YACC requires neither the significant extra metadata (i.e., back pointers) needed by DCC to track blocks nor the complexity and overhead of skewed associativity (i.e., indexing ways differently) needed by SCC. An additional advantage over previous work is that YACC enables modern replacement mechanisms, such as RRIP. For our benchmark set, compared to a conventional uncompressed 8MB LLC, YACC improves performance by 8% on average and up to 26%, and reduces total energy by 6% on average and up to 20%. An 8MB YACC achieves approximately the same performance and energy improvements as a 16MB conventional cache at a much smaller silicon footprint, with only 1.6% greater area than an 8MB conventional cache. YACC performs comparably to DCC and SCC but is much simpler to implement.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W2521480511",
    "type": "article"
  },
  {
    "title": "A Black-box Monitoring Approach to Measure Microservices Runtime Performance",
    "doi": "https://doi.org/10.1145/3418899",
    "publication_date": "2020-11-10",
    "publication_year": 2020,
    "authors": "Rolando Brondolin; Marco D. Santambrogio",
    "corresponding_authors": "",
    "abstract": "Microservices changed cloud computing by moving the applications’ complexity from one monolithic executable to thousands of network interactions between small components. Given the increasing deployment sizes, the architectural exploitation challenges, and the impact on data-centers’ power consumption, we need to efficiently track this complexity. Within this article, we propose a black-box monitoring approach to track microservices at scale, focusing on architectural metrics, power consumption, application performance, and network performance. The proposed approach is transparent w.r.t. the monitored applications, generates less overhead w.r.t. black-box approaches available in the state-of-the-art, and provides fine-grain accurate metrics.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W3100676516",
    "type": "article"
  },
  {
    "title": "Grus",
    "doi": "https://doi.org/10.1145/3444844",
    "publication_date": "2021-02-09",
    "publication_year": 2021,
    "authors": "Pengyu Wang; Jing Wang; Chao Li; Jianzong Wang; Haojin Zhu; Minyi Guo",
    "corresponding_authors": "",
    "abstract": "Today’s GPU graph processing frameworks face scalability and efficiency issues as the graph size exceeds GPU-dedicated memory limit. Although recent GPUs can over-subscribe memory with Unified Memory (UM), they incur significant overhead when handling graph-structured data. In addition, many popular processing frameworks suffer sub-optimal efficiency due to heavy atomic operations when tracking the active vertices. This article presents Grus, a novel system framework that allows GPU graph processing to stay competitive with the ever-growing graph complexity. Grus improves space efficiency through a UM trimming scheme tailored to the data access behaviors of graph workloads. It also uses a lightweight frontier structure to further reduce atomic operations. With easy-to-use interface that abstracts the above details, Grus shows up to 6.4× average speedup over the state-of-the-art in-memory GPU graph processing framework. It allows one to process large graphs of 5.5 billion edges in seconds with a single GPU.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W3126386565",
    "type": "article"
  },
  {
    "title": "Efficient Auto-Tuning of Parallel Programs with Interdependent Tuning Parameters via Auto-Tuning Framework (ATF)",
    "doi": "https://doi.org/10.1145/3427093",
    "publication_date": "2021-01-20",
    "publication_year": 2021,
    "authors": "Ari Rasch; Richard Schulze; Michel Steuwer; Sergei Gorlatch",
    "corresponding_authors": "",
    "abstract": "Auto-tuning is a popular approach to program optimization: it automatically finds good configurations of a program’s so-called tuning parameters whose values are crucial for achieving high performance for a particular parallel architecture and characteristics of input/output data. We present three new contributions of the Auto-Tuning Framework (ATF), which enable a key advantage in general-purpose auto-tuning : efficiently optimizing programs whose tuning parameters have interdependencies among them. We make the following contributions to the three main phases of general-purpose auto-tuning: (1) ATF generates the search space of interdependent tuning parameters with high performance by efficiently exploiting parameter constraints; (2) ATF stores such search spaces efficiently in memory, based on a novel chain-of-trees search space structure; (3) ATF explores these search spaces faster, by employing a multi-dimensional search strategy on its chain-of-trees search space representation. Our experiments demonstrate that, compared to the state-of-the-art, general-purpose auto-tuning frameworks, ATF substantially improves generating, storing, and exploring the search space of interdependent tuning parameters, thereby enabling an efficient overall auto-tuning process for important applications from popular domains, including stencil computations, linear algebra routines, quantum chemistry computations, and data mining algorithms.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W3122888791",
    "type": "article"
  },
  {
    "title": "PERI",
    "doi": "https://doi.org/10.1145/3446210",
    "publication_date": "2021-04-14",
    "publication_year": 2021,
    "authors": "Sugandha Tiwari; Neel Gala; Chester Rebeiro; V. Kamakoti",
    "corresponding_authors": "",
    "abstract": "Owing to the failure of Dennard’s scaling, the past decade has seen a steep growth of prominent new paradigms leveraging opportunities in computer architecture. Two technologies of interest are Posit and RISC-V. Posit was introduced in mid-2017 as a viable alternative to IEEE-754, and RISC-V provides a commercial-grade open source Instruction Set Architecture (ISA). In this article, we bring these two technologies together and propose a Configurable Posit Enabled RISC-V Core called PERI. The article provides insights on how the Single-Precision Floating Point (“F”) extension of RISC-V can be leveraged to support posit arithmetic. We also present the implementation details of a parameterized and feature-complete posit Floating Point Unit (FPU). The configurability and the parameterization features of this unit generate optimal hardware, which caters to the accuracy and energy/area tradeoffs imposed by the applications, a feature not possible with IEEE-754 implementation. The posit FPU has been integrated with the RISC-V compliant SHAKTI C-class core as an execution unit. To further leverage the potential of posit , we enhance our posit FPU to support two different exponent sizes (with posit-size being 32-bits), thereby enabling multiple-precision at runtime. To enable the compilation and execution of C programs on PERI, we have made minimal modifications to the GNU C Compiler (GCC), targeting the “F” extension of the RISC-V. We compare posit with IEEE-754 in terms of hardware area, application accuracy, and runtime. We also present an alternate methodology of integrating the posit FPU with the RISC-V core as an accelerator using the custom opcode space of RISC-V.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W3152828401",
    "type": "article"
  },
  {
    "title": "Configurable Multi-directional Systolic Array Architecture for Convolutional Neural Networks",
    "doi": "https://doi.org/10.1145/3460776",
    "publication_date": "2021-07-17",
    "publication_year": 2021,
    "authors": "Rui Xu; Sheng Ma; Yaohua Wang; Xinhai Chen; Yang Guo",
    "corresponding_authors": "",
    "abstract": "The systolic array architecture is one of the most popular choices for convolutional neural network hardware accelerators. The biggest advantage of the systolic array architecture is its simple and efficient design principle. Without complicated control and dataflow, hardware accelerators with the systolic array can calculate traditional convolution very efficiently. However, this advantage also brings new challenges to the systolic array. When computing special types of convolution, such as the small-scale convolution or depthwise convolution, the processing element (PE) utilization rate of the array decreases sharply. The main reason is that the simple architecture design limits the flexibility of the systolic array. In this article, we design a configurable multi-directional systolic array (CMSA) to address these issues. First, we added a data path to the systolic array. It allows users to split the systolic array through configuration to speed up the calculation of small-scale convolution. Second, we redesigned the PE unit so that the array has multiple data transmission modes and dataflow strategies. This allows users to switch the dataflow of the PE array to speed up the calculation of depthwise convolution. In addition, unlike other works, we only make a few changes and modifications to the existing systolic array architecture. It avoids additional hardware overheads and can be easily deployed in application scenarios that require small systolic arrays such as mobile terminals. Based on our evaluation, CMSA can increase the PE utilization rate by up to 1.6 times compared to the typical systolic array when running the last layers of ResNet-18. When running depthwise convolution in MobileNet, CMSA can increase the utilization rate by up to 14.8 times. At the same time, CMSA and the traditional systolic arrays are similar in area and energy consumption.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W3184376546",
    "type": "article"
  },
  {
    "title": "WA-Zone: Wear-Aware Zone Management Optimization for LSM-Tree on ZNS SSDs",
    "doi": "https://doi.org/10.1145/3637488",
    "publication_date": "2023-12-13",
    "publication_year": 2023,
    "authors": "Linbo Long; Shuiyong He; Jingcheng Shen; Renping Liu; Zhenhua Tan; Congming Gao; Duo Liu; Kan Zhong; Yi Jiang",
    "corresponding_authors": "",
    "abstract": "ZNS SSDs divide the storage space into sequential-write zones, reducing costs of DRAM utilization, garbage collection, and over-provisioning. The sequential-write feature of zones is well-suited for LSM-based databases, where random writes are organized into sequential writes to improve performance. However, the current compaction mechanism of LSM-tree results in widely varying access frequencies (i.e., hotness) of data and thus incurs an extreme imbalance in the distribution of erasure counts across zones. The imbalance significantly limits the lifetime of SSDs. Moreover, the current zone-reset method involves a large number of unnecessary erase operations on unused blocks, further shortening the SSD lifetime. Considering the access pattern of LSM-tree, this article proposes a wear-aware zone-management technique, termed WA-Zone , to effectively balance inter- and intra-zone wear in ZNS SSDs. In WA-Zone, a wear-aware zone allocator is first proposed to dynamically allocate data with different hotness to zones with corresponding lifetimes, enabling an even distribution of the erasure counts across zones. Then, a partial-erase-based zone-reset method is presented to avoid unnecessary erase operations. Furthermore, because the novel zone-reset method might lead to an unbalanced distribution of erasure counts across blocks in a zone, a wear-aware block allocator is proposed. Experimental results based on the FEMU emulator demonstrate the proposed WA-Zone enhances the ZNS-SSD lifetime by 5.23×, compared with the baseline scheme.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4389675214",
    "type": "article"
  },
  {
    "title": "Cost-aware Service Placement and Scheduling in the Edge-Cloud Continuum",
    "doi": "https://doi.org/10.1145/3640823",
    "publication_date": "2024-01-16",
    "publication_year": 2024,
    "authors": "Samuel Rac; Mats Brorsson",
    "corresponding_authors": "",
    "abstract": "The edge to data center computing continuum is the aggregation of computing resources located anywhere between the network edge (e.g., close to 5G antennas), and servers in traditional data centers. Kubernetes is the de facto standard for the orchestration of services in data center environments, where it is very efficient. It, however, fails to give the same performance when including edge resources. At the edge, resources are more limited, and networking conditions are changing over time. In this article, we present a methodology that lowers the costs of running applications in the edge-to-cloud computing continuum. This methodology can adapt to changing environments, e.g., moving end-users. We are also monitoring some Key Performance Indicators of the applications to ensure that cost optimizations do not negatively impact their Quality of Service. In addition, to ensure that performances are optimal even when users are moving, we introduce a background process that periodically checks if a better location is available for the service and, if so, moves the service. To demonstrate the performance of our scheduling approach, we evaluate it using a vehicle cooperative perception use case, a representative 5G application. With this use case, we can demonstrate that our scheduling approach can robustly lower the cost in different scenarios, while other approaches that are already available fail in either being adaptive to changing environments or will have poor cost-effectiveness in some scenarios.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4390911999",
    "type": "article"
  },
  {
    "title": "Rcmp: Reconstructing RDMA-Based Memory Disaggregation via CXL",
    "doi": "https://doi.org/10.1145/3634916",
    "publication_date": "2024-01-19",
    "publication_year": 2024,
    "authors": "Zhonghua Wang; Y. M. Guo; Kai Lü; Jiguang Wan; Daohui Wang; Ting Yao; Huatao Wu",
    "corresponding_authors": "",
    "abstract": "Memory disaggregation is a promising architecture for modern datacenters that separates compute and memory resources into independent pools connected by ultra-fast networks, which can improve memory utilization, reduce cost, and enable elastic scaling of compute and memory resources. However, existing memory disaggregation solutions based on remote direct memory access (RDMA) suffer from high latency and additional overheads including page faults and code refactoring. Emerging cache-coherent interconnects such as CXL offer opportunities to reconstruct high-performance memory disaggregation. However, existing CXL-based approaches have physical distance limitation and cannot be deployed across racks. In this article, we propose Rcmp, a novel low-latency and highly scalable memory disaggregation system based on RDMA and CXL. The significant feature is that Rcmp improves the performance of RDMA-based systems via CXL, and leverages RDMA to overcome CXL’s distance limitation. To address the challenges of the mismatch between RDMA and CXL in terms of granularity, communication, and performance, Rcmp (1) provides a global page-based memory space management and enables fine-grained data access, (2) designs an efficient communication mechanism to avoid communication blocking issues, (3) proposes a hot-page identification and swapping strategy to reduce RDMA communications, and (4) designs an RDMA-optimized RPC framework to accelerate RDMA transfers. We implement a prototype of Rcmp and evaluate its performance by using micro-benchmarks and running a key-value store with YCSB benchmarks. The results show that Rcmp can achieve 5.2× lower latency and 3.8× higher throughput than RDMA-based systems. We also demonstrate that Rcmp can scale well with the increasing number of nodes without compromising performance.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4391034987",
    "type": "article"
  },
  {
    "title": "Sectored DRAM: A Practical Energy-Efficient and High-Performance Fine-Grained DRAM Architecture",
    "doi": "https://doi.org/10.1145/3673653",
    "publication_date": "2024-06-14",
    "publication_year": 2024,
    "authors": "Ataberk Olgun; F. Nisa Bostancı; Geraldo F. Oliveira; Yahya Can Tuğrul; Rahul Bera; A. Giray Yağlıkçı; Hasan Hassan; Oğuz Ergin; Onur Mutlu",
    "corresponding_authors": "",
    "abstract": "Modern computing systems access data in main memory at coarse granularity (e.g., at 512-bit cache block granularity). Coarse-grained access leads to wasted energy because the system does not use all individually accessed small portions (e.g., words , each of which typically is 64 bits) of a cache block. In modern DRAM-based computing systems, two key coarse-grained access mechanisms lead to wasted energy: large and fixed-size (i) data transfers between DRAM and the memory controller and (ii) DRAM row activations. We propose Sectored DRAM, a new, low-overhead DRAM substrate that reduces wasted energy by enabling fine-grained DRAM data transfer and DRAM row activation. To retrieve only useful data from DRAM, Sectored DRAM exploits the observation that many cache blocks are not fully utilized in many workloads due to poor spatial locality. Sectored DRAM predicts the words in a cache block that will likely be accessed during the cache block’s residency in cache and (i) transfers only the predicted words on the memory channel by dynamically tailoring the DRAM data transfer size for the workload and (ii) activates a smaller set of cells that contain the predicted words by carefully operating physically isolated portions of DRAM rows (i.e., mats). Activating a smaller set of cells on each access relaxes DRAM power delivery constraints and allows the memory controller to schedule DRAM accesses faster. We evaluate Sectored DRAM using 41 workloads from widely used benchmark suites. Compared to a system with coarse-grained DRAM, Sectored DRAM reduces the DRAM energy consumption of highly memory intensive workloads by up to (on average) 33% (20%) while improving their performance by up to (on average) 36% (17%). Sectored DRAM’s DRAM energy savings, combined with its system performance improvement, allows system-wide energy savings of up to 23%. Sectored DRAM’s DRAM chip area overhead is 1.7% of the area of a modern DDR4 chip. Compared to state-of-the-art fine-grained DRAM architectures, Sectored DRAM greatly reduces DRAM energy consumption, does not reduce DRAM bandwidth, and can be implemented with low hardware cost. Sectored DRAM provides 89% of the performance benefits of, consumes 12% less DRAM energy than, and takes up 34% less DRAM chip area than a high-performance state-of-the-art fine-grained DRAM architecture (Half-DRAM). It is our hope and belief that Sectored DRAM’s ideas and results will help to enable more efficient and high-performance memory systems. To this end, we open source Sectored DRAM at https://github.com/CMU-SAFARI/Sectored-DRAM.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4399677071",
    "type": "article"
  },
  {
    "title": "gCom: Fine-grained Compressors in Graphics Memory of Mobile GPU",
    "doi": "https://doi.org/10.1145/3711819",
    "publication_date": "2025-01-08",
    "publication_year": 2025,
    "authors": "Dongjie Tang; Zheng Yi Wu; Yun Wang; Yicheng Gu; Fangxin Liu; Zhengwei Qi",
    "corresponding_authors": "",
    "abstract": "Nowadays, GPUs significantly boost rendering performance. However, the high memory requirements limit their use, especially on low-end mobile platforms. Compression techniques have been widely adopted to reduce memory consumption but face two primary issues when applied to mobile GPUs: 1) low repetition ratio caused by small raw data sizes and concurrency, and 2) low locality caused by unpredictable rendering behaviors. These two limitations result in a low compression ratio when compressors are applied to low-end mobile devices. This paper introduces gCom , a fine-grained rendering compressor accelerated by GPUs. To improve the compression ratio, gCom incorporates the following innovations: 1) Unlike other compression techniques that use frames or tiles as basic processing units, gCom is the first to employ a fine-grained processing unit (i.e., the color channel), enhancing repetition amplification without increasing raw data. 2) gCom introduces two key features, hierarchical delta , and channel decorrelator , which maximize the locality of adjacent channels and reduce raw data size. 3) To maintain the original GPU throughput, gCom revolutionizes the Golomb-Rice algorithm and proposes a new compression approach, the Parallel-Oriented Golomb-Rice (POGR) algorithm, enabling parallel execution of both decompression and compression processes. The entire design of gCom utilizes only idle resources and existing commands on mobile GPUs, thus keeping purchasing costs low. To date, gCom has improved the channel locality by nearly 50%. The best compression achievement received by gCom has reached around 20%.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4406166810",
    "type": "article"
  },
  {
    "title": "Accelerating Nearest Neighbor Search in 3D Point Cloud Registration on GPUs",
    "doi": "https://doi.org/10.1145/3716875",
    "publication_date": "2025-02-10",
    "publication_year": 2025,
    "authors": "Qiong Chang; Weimin Wang; Jun Miyazaki",
    "corresponding_authors": "",
    "abstract": "The Iterative Closest Points (ICP) algorithm is the most widely used method for estimating rigid transformation in 3D point cloud registration. However, the ICP relies on repeatedly performing computationally intensive nearest neighbor searches (NNS) within 3D space. This dependency becomes a significant bottleneck when processing large datasets, thereby hindering the practical deployment of point cloud technologies in real-world applications. To address this issue, we propose two approximate nearest neighbor search (ANNS) acceleration strategies for efficient improvement of the processing speed of the NNS. Our strategies first voxelize target cloud points and then fill voxels in the 3D coordinate space around the source point cloud in two different ways, which can convert the global nearest neighbor search to a local search. Both the proposed methods are suited to be parallelized on GPUs with a low computational load. Extensive experiments show that our methods significantly accelerate NNS processing while maintaining high accuracy, outperforming most of the currently known approaches.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4407319506",
    "type": "article"
  },
  {
    "title": "VersaTile: Flexible Tiled Architectures via Associative Processors",
    "doi": "https://doi.org/10.1145/3716873",
    "publication_date": "2025-02-10",
    "publication_year": 2025,
    "authors": "Kailin Yang; José F. Martínez",
    "corresponding_authors": "",
    "abstract": "As modern applications demand more data, processing-in-memory (PIM) architectures have emerged to address the challenges of data movement and parallelism. In this article, we propose VersaTile, a heterogeneous, fully CMOS-based tiled architecture that combines conventional out-of-order (OoO) superscalar CPUs and associative processors (APs), a type of CAM-based PIM core. Both CPUs and APs leverage the RISC-V ISA and its standard RVV vector extension. VersaTile fosters collaboration between multiple low-latency CPUs and high-throughput APs by sharing the same software stack and adopting a common CPU programming and compilation frontend. Moreover, we introduce tile stitching, a mechanism enabling the aggregation of multiple APs into a single vector super-unit with modest hardware support and no programming effort. Tile stitching allows us to configure an architecture for optimal performance across a wide range of applications. We provide a detailed case study, including a scalable floorplan example, as well as a comprehensive evaluation of various design points. Our experiments show that, when using only AP tiles, VersaTile can achieve, on average across the Phoenix benchmark suite and 3D convolution, a \\(5.7\\times\\) speedup with respect to area-equivalent OoO CPU cores with SIMD ALUs (up to \\(23\\times\\) ), and \\(4.6\\times\\) with respect to an equivalent-sized monolithic AP baseline (up to \\(29\\times\\) ). For applications with both DLP (vector) and ILP (scalar) regions, VersaTile can use APs and OoO cores collaboratively to achieve better performance than using either one of them only, up to \\(4.4\\times\\) .",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4407319894",
    "type": "article"
  },
  {
    "title": "Deep Learning Workload Mapping Optimization on Jetson Platforms",
    "doi": "https://doi.org/10.1145/3736175",
    "publication_date": "2025-05-20",
    "publication_year": 2025,
    "authors": "Farui Wang; Meng Hao; Siyu Yang; Weizhe Zhang",
    "corresponding_authors": "",
    "abstract": "To improve the performance and energy efficiency of deep learning (DL) applications, recent edge computing platforms have built-in heterogeneous accelerators, such as general-purpose graphics processing units (GPUs) and neural processing units (NPUs). For example, widely used NVIDIA Jetson platforms contain CPU, GPU, and deep learning accelerator (DLA), a type of NPU. It is non-trivial to map DL workloads to suitable accelerators to improve performance, energy efficiency, or even both. This paper presents JDIMO, a Jetson-aware deep-learning inference workload mapping optimization framework, to simultaneously improve energy efficiency and performance. JDIMO first measures energy-performance data of the fundamental nodes and the sub-networks with energy-efficiency improvement potential according to the topology structure of a DL network. Then, under the guidance of an analytical energy-performance model, the framework exploits an algorithm based on the variable-length sliding window to find the optimal mapping configuration and the optimal number of CUDA streams. We evaluate JDIMO by applying it to seven DL applications on a Jetson Orin NX (16GB) platform. JDIMO saves \\(47.5\\% \\) EDP (energy delay product) and \\(22.6\\% \\) energy and improves \\(138.3\\% \\) QPS (queries per second) on average compared to the DLA-possible configuration. JDIMO saves \\(22.5\\% \\) EDP and \\(12.6\\% \\) energy and improves \\(13.5\\% \\) QPS on average compared to JEDI, the most similar work to ours. Meanwhile, JDIMO also reduces \\(93.8\\% \\) optimization time on average compared to JEDI.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4410518888",
    "type": "article"
  },
  {
    "title": "Merging path and gshare indexing in perceptron branch prediction",
    "doi": "https://doi.org/10.1145/1089008.1089011",
    "publication_date": "2005-09-01",
    "publication_year": 2005,
    "authors": "David Tarjan; Kevin Skadron",
    "corresponding_authors": "",
    "abstract": "We introduce the hashed perceptron predictor, which merges the concepts behind the gshare, path-based and perceptron branch predictors. This predictor can achieve superior accuracy to a path-based and a global perceptron predictor, previously the most accurate dynamic branch predictors known in the literature. We also show how such a predictor can be ahead pipelined to yield one cycle effective latency. On the SPECint2000 set of benchmarks, the hashed perceptron predictor improves accuracy by up to 15.6% over a MAC-RHSP and 27.2% over a path-based neural predictor.",
    "cited_by_count": 58,
    "openalex_id": "https://openalex.org/W2052647915",
    "type": "article"
  },
  {
    "title": "Exploring the limits of leakage power reduction in caches",
    "doi": "https://doi.org/10.1145/1089008.1089009",
    "publication_date": "2005-09-01",
    "publication_year": 2005,
    "authors": "Meng Yan; Timothy Sherwood; Ryan Kastner",
    "corresponding_authors": "",
    "abstract": "If current technology scaling trends hold, leakage power dissipation will soon become the dominant source of power consumption. Caches, because of the fact that they account for the largest fraction of on-chip transistors in most modern processors, are a primary candidate for attacking the leakage problem. While there has been a flurry of research in this area over the last several years, a major question remains unanswered---What is the total potential of existing architectural and circuit techniques to address this important design concern? In this paper, we explore the limits in which existing circuit and architecture technologies may address this growing problem. We first formally propose a parameterized model that can determine the optimal leakage savings based on the perfect knowledge of the address trace. By carefully applying the sleep and drowsy modes, we find that the total leakage power from the L1 instruction cache, data cache, and a unified L2 cache may be reduced to mere 3.6, 0.9, and 2.3%, respectively, of the unoptimized case. We further study how such a model can be extended to obtain the optimal leakage power savings for different cache configurations.",
    "cited_by_count": 51,
    "openalex_id": "https://openalex.org/W2062798759",
    "type": "article"
  },
  {
    "title": "Cross-component energy management",
    "doi": "https://doi.org/10.1145/1275937.1275938",
    "publication_date": "2007-09-01",
    "publication_year": 2007,
    "authors": "Xiaodong Li; Ritu Gupta; Sarita V. Adve; Yuanyuan Zhou",
    "corresponding_authors": "",
    "abstract": "Researchers have proposed the use of adaptation to reduce the energy consumption of different hardware components, such as the processor, memory, disk, and display for general-purpose applications. Previous algorithms to control these adaptations, however, have focused on a single component. This work takes the first step toward developing algorithms that can jointly control adaptations in multiple interacting components for general-purpose applications, with the goal of minimizing the total energy consumed within a specified performance loss. Specifically, we develop a joint-adaptation algorithm for processor and memory adaptations. We identify two properties that enable per-component algorithms to be easily used in a cross-component context---the algorithms' performance impact must be guaranteed and composable. We then modify a current processor and a memory algorithm to obey these properties. This allows the cross-component problem to be reduced to determine an appropriate (energy-optimal) allocation of the target performance loss (slack) between the two components. We develop such an optimal slack allocation algorithm that exploits the above properties. The result is an efficient cross-component adaptation framework that minimizes the total energy of the processor and memory without exceeding the target performance loss, while substantially leveraging current per-component algorithms. Our experiments show that joint processor and memory adaptation provides significantly more energy savings than adapting either component alone; intelligent slack distribution is specifically effective for highly compute- or memory-intensive applications; and the performance slowdown never exceeds the specification.",
    "cited_by_count": 48,
    "openalex_id": "https://openalex.org/W2163573474",
    "type": "article"
  },
  {
    "title": "Compiler-directed scratchpad memory management via graph coloring",
    "doi": "https://doi.org/10.1145/1582710.1582711",
    "publication_date": "2009-09-01",
    "publication_year": 2009,
    "authors": "Lian Li; Hui Feng; Jingling Xue",
    "corresponding_authors": "",
    "abstract": "Scratchpad memory (SPM), a fast on-chip SRAM managed by software, is widely used in embedded systems. This article introduces a general-purpose compiler approach, called memory coloring, to assign static data aggregates, such as arrays and structs, in a program to an SPM. The novelty of this approach lies in partitioning the SPM into a pseudo--register file (with interchangeable and aliased registers), splitting the live ranges of data aggregates to create potential data transfer statements between SPM and off-chip memory, and finally, adapting an existing graph coloring algorithm for register allocation to assign the data aggregates to the pseudo--register file. Our experimental results using a set of 10 C benchmarks from MediaBench and MiBench show that our methodology is capable of managing SPMs efficiently and effectively for large embedded applications. In addition, our SPM allocator can obtain close to optimal solutions when evaluated and compared against an existing heuristics-based SPM allocator and an ILP-based SPM allocator.",
    "cited_by_count": 46,
    "openalex_id": "https://openalex.org/W2104009155",
    "type": "article"
  },
  {
    "title": "Collective optimization",
    "doi": "https://doi.org/10.1145/1880043.1880047",
    "publication_date": "2010-12-01",
    "publication_year": 2010,
    "authors": "Grigori Fursin; Olivier Temam",
    "corresponding_authors": "",
    "abstract": "Iterative optimization is a popular and efficient research approach to optimize programs using feedback-directed compilation. However, one of the key limitations that prevented widespread use in production compilers and day-to-day practice is the necessity to perform a large number of program runs with the same dataset and environment (architecture, OS, compiler) to test many different combinations of optimizations. In this article, we propose to overcome such a practical obstacle using collective optimization , where the task of optimizing a program or tuning default compiler optimization heuristic leverages the experience of many other users continuously, rather than being performed in isolation, and often redundantly, by each user. During this unobtrusive approach, performance information is sent to a central database after each run and statistically combined with the data from all users to suggest most profitable optimizations for a given program and an architecture, or to gradually improve default optimization level of a compiler for a given architecture. In this article, we address two key challenges of collective optimization. We show that it is possible to simultaneously learn and improve performance while avoiding long training phases. We also demonstrate how to use our approach with static compilers to learn optimizations across multiple datasets and architectures without even a reference run normally needed to compute speedups over the baseline optimization by using static function cloning and dynamic adaptation. We present a novel probabilistic approach based on competition among pairs of optimizations (program reaction to optimizations) to enable optimization knowledge reuse and achieve nearly the best possible iterative optimization performance. We implemented our technique in GCC (widespread production open-source compiler that supports multiple architectures) and connected it to a public collective optimization database at cTuning.org to gather profile and optimization data continuously and transparently in realistic environments ranging from desktop PCs and mobile systems to supercomputers and data centers.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W1988659054",
    "type": "article"
  },
  {
    "title": "A power-aware mapping approach to map IP cores onto NoCs under bandwidth and latency constraints",
    "doi": "https://doi.org/10.1145/1736065.1736066",
    "publication_date": "2010-04-01",
    "publication_year": 2010,
    "authors": "Xiaohang Wang; Mei Yang; Yingtao Jiang; Peng Liu",
    "corresponding_authors": "",
    "abstract": "In this article, we investigate the Intellectual Property (IP) mapping problem that maps a given set of IP cores onto the tiles of a mesh-based Network-on-Chip (NoC) architecture such that the power consumption due to intercore communications is minimized. This IP mapping problem is considered under both bandwidth and latency constraints as imposed by the applications and the on-chip network infrastructure. By examining various applications' communication characteristics extracted from their respective communication trace graphs, two distinguishable connectivity templates are realized: the graphs with tightly coupled vertices and those with distributed vertices. These two templates are formally defined in this article, and different mapping heuristics are subsequently developed to map them. In general, tightly coupled vertices are mapped onto tiles that are physically close to each other while the distributed vertices are mapped following a graph partition scheme. Experimental results on both random and multimedia benchmarks have confirmed that the proposed template-based mapping algorithm achieves an average of 15% power savings as compared with MOCA, a fast greedy-based mapping algorithm. Compared with a branch-and-bound--based mapping algorithm, which produces near optimal results but incurs an extremely high computation cost, the proposed algorithm, due to its polynomial runtime complexity, can generate the results of almost the same quality with much less CPU time. As the on-chip network size increases, the superiority of the proposed algorithm becomes more evident.",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W2054291601",
    "type": "article"
  },
  {
    "title": "Exploiting compression opportunities to improve SpMxV performance on shared memory systems",
    "doi": "https://doi.org/10.1145/1880037.1880041",
    "publication_date": "2010-12-01",
    "publication_year": 2010,
    "authors": "Kornilios Kourtis; Georgios Goumas; Nectarios Koziris",
    "corresponding_authors": "",
    "abstract": "The Sparse Matrix-Vector Multiplication (SpMxV) kernel exhibits poor scaling on shared memory systems, due to the streaming nature of its data access pattern. To decrease memory contention and improve kernel performance we propose two compression schemes: CSR-DU, that targets the reduction of the matrix structural data by applying coarse-grained delta-encoding, and CSR-VI, that targets the reduction of the values using indirect indexing, applicable to matrices with a small number of unique values. Thorough experimental evaluation of the proposed methods and their combination, on two modern shared memory systems, demonstrated that they can significantly improve multithreaded SpMxV performance upon standard and state-of-the-art approaches.",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W2084296319",
    "type": "article"
  },
  {
    "title": "Power-Efficient Predication Techniques for Acceleration of Control Flow Execution on CGRA",
    "doi": "https://doi.org/10.1145/2459316.2459319",
    "publication_date": "2013-05-01",
    "publication_year": 2013,
    "authors": "Kyuseung Han; Junwhan Ahn; Ki‐Young Choi",
    "corresponding_authors": "",
    "abstract": "Coarse-grained reconfigurable architecture typically has an array of processing elements which are controlled by a centralized unit. This makes it difficult to execute programs having control divergence among PEs without predication. However, conventional predication techniques have a negative impact on both performance and power consumption due to longer instruction words and unnecessary instruction-fetching decoding nullifying steps. This article reveals performance and power issues in predicated execution which have not been well-addressed yet. Furthermore, it proposes fast and power-efficient predication mechanisms. Experiments conducted through gate-level simulation show that our mechanism improves energy-delay product by 11.9% to 23.8% on average.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W1997981962",
    "type": "article"
  },
  {
    "title": "An architecture-independent instruction shuffler to protect against side-channel attacks",
    "doi": "https://doi.org/10.1145/2086696.2086699",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Ali Galip Bayrak; Nikola Veličković; Paolo Ienne; Wayne Burleson",
    "corresponding_authors": "",
    "abstract": "Embedded cryptographic systems, such as smart cards, require secure implementations that are robust to a variety of low-level attacks. Side-Channel Attacks (SCA) exploit the information such as power consumption, electromagnetic radiation and acoustic leaking through the device to uncover the secret information. Attackers can mount successful attacks with very modest resources in a short time period. Therefore, many methods have been proposed to increase the security against SCA. Randomizing the execution order of the instructions that are independent, i.e., random shuffling , is one of the most popular among them. Implementing instruction shuffling in software is either implementation specific or has a significant performance or code size overhead. To overcome these problems, we propose in this work a generic custom hardware unit to implement random instruction shuffling as an extension to existing processors. The unit operates between the CPU and the instruction cache (or memory, if no cache exists), without any modification to these components. Both true and pseudo random number generators are used to dynamically and locally provide the shuffling sequence. The unit is mainly designed for in-order processors, since the embedded devices subject to these kind of attacks use simple in-order processors. More advanced processors (e.g., superscalar, VLIW or EPIC processors) are already more resistant to these attacks because of their built-in ILP and wide word size. Our experiments on two different soft in-order processor cores, i.e., OpenRISC and MicroBlaze, implemented on FPGA show that the proposed unit could increase the security drastically with very modest resource overhead. With around 2% area, 1.5% power and no performance overhead, the shuffler increases the effort to mount a successful power analysis attack on AES software implementation over 360 times.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W2055303876",
    "type": "article"
  },
  {
    "title": "Continuous learning of compiler heuristics",
    "doi": "https://doi.org/10.1145/2400682.2400705",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Michele Tartara; Stefano Crespi Reghizzi",
    "corresponding_authors": "",
    "abstract": "Optimizing programs to exploit the underlying hardware architecture is an important task. Much research has been done on enabling compilers to find the best set of code optimizations that can build the fastest and less resource-hungry executable for a given program. A common approach is iterative compilation, sometimes enriched by machine learning techniques. This provides good results, but requires extremely long compilation times and an initial training phase lasting even for days or weeks. We present long-term learning, a new algorithm that allows the compiler user to improve the performance of compiled programs with reduced compilation times with respect to iterative compilation, and without an initial training phase. Our algorithm does not just build good programs: it acquires knowledge every time a program is compiled and it uses such knowledge to learn compiler heuristics, without the need for an expert to manually define them. The heuristics are evolved during every compilation, by evaluating their effect on the generated programs. We present implementations of long-term learning on top of two different compilers, and experimental data gathered on multiple hardware configurations showing its effectiveness.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W2139634727",
    "type": "article"
  },
  {
    "title": "A transactional memory with automatic performance tuning",
    "doi": "https://doi.org/10.1145/2086696.2086733",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Qingping Wang; Sameer G. Kulkarni; John Cavazos; Michael Spear",
    "corresponding_authors": "",
    "abstract": "A significant obstacle to the acceptance of transactional memory (TM) in real-world parallel programs is the abundance of substantially different TM algorithms. Each TM algorithm appears well-suited to certain workload characteristics, but the best choice of algorithm is sensitive to program inputs, available cores, and program phases. Furthermore, operating system and hardware characteristics can affect which algorithm is best, with tradeoffs changing across iterations of a single ISA. This paper introduces methods for constructing policies to dynamically select the most appropriate TM algorithm based on static and dynamic information. We leverage intraprocedural static analysis to create a static profile of the application. We also introduce a low-overhead framework for dynamic profiling of a running transactional application. Armed with these complementary descriptions of a program's behavior, we present novel expert adaptivity policies as well as machine learning policies that are trained off-line using simple microbenchmarks. In our evaluation, we find that both the expert and learned policies provide better performance than any single TM algorithm across the entire STAMP benchmark suite. In addition, policies that combine expert and learned policies offer the best combination of performance, maintainability, and flexibility.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W2012893167",
    "type": "article"
  },
  {
    "title": "A unified optimizing compiler framework for different GPGPU architectures",
    "doi": "https://doi.org/10.1145/2207222.2207225",
    "publication_date": "2012-06-01",
    "publication_year": 2012,
    "authors": "Yi Yang; Ping Xiang; Jingfei Kong; Mike Mantor; Huiyang Zhou",
    "corresponding_authors": "",
    "abstract": "This article presents a novel optimizing compiler for general purpose computation on graphics processing units (GPGPU). It addresses two major challenges of developing high performance GPGPU programs: effective utilization of GPU memory hierarchy and judicious management of parallelism. The input to our compiler is a naïve GPU kernel function, which is functionally correct but without any consideration for performance optimization. The compiler generates two kernels, one optimized for global memories and the other for texture memories. The proposed compilation process is effective for both AMD/ATI and NVIDIA GPUs. The experiments show that our optimized code achieves very high performance, either superior or very close to highly fine-tuned libraries.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W2016563136",
    "type": "article"
  },
  {
    "title": "A performance and energy comparison of convolution on GPUs, FPGAs, and multicore processors",
    "doi": "https://doi.org/10.1145/2400682.2400684",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Jeremy Fowers; G.R. Brown; John Wernsing; Greg Stitt",
    "corresponding_authors": "",
    "abstract": "Recent architectural trends have focused on increased parallelism via multicore processors and increased heterogeneity via accelerator devices (e.g., graphics-processing units, field-programmable gate arrays). Although these architectures have significant performance and energy potential, application designers face many device-specific challenges when choosing an appropriate accelerator or when customizing an algorithm for an accelerator. To help address this problem, in this article we thoroughly evaluate convolution, one of the most common operations in digital-signal processing, on multicores, graphics-processing units, and field-programmable gate arrays. Whereas many previous application studies evaluate a specific usage of an application, this article assists designers with design space exploration for numerous use cases by analyzing effects of different input sizes, different algorithms, and different devices, while also determining Pareto-optimal trade-offs between performance and energy.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W2094786337",
    "type": "article"
  },
  {
    "title": "Improving System Energy Efficiency with Memory Rank Subsetting",
    "doi": "https://doi.org/10.1145/2133382.2133386",
    "publication_date": "2012-03-01",
    "publication_year": 2012,
    "authors": "Jung Ho Ahn; Norman P. Jouppi; Christos Kozyrakis; Jacob Leverich; Robert Schreiber",
    "corresponding_authors": "",
    "abstract": "VLSI process technology scaling has enabled dramatic improvements in the capacity and peak bandwidth of DRAM devices. However, current standard DDR x DIMM memory interfaces are not well tailored to achieve high energy efficiency and performance in modern chip-multiprocessor-based computer systems. Their suboptimal performance and energy inefficiency can have a significant impact on system-wide efficiency since much of the system power dissipation is due to memory power. New memory interfaces, better suited for future many-core systems, are needed. In response, there are recent proposals to enhance the energy efficiency of main-memory systems by dividing a memory rank into subsets, and making a subset rather than a whole rank serve a memory request. We holistically assess the effectiveness of rank subsetting from system-wide performance, energy-efficiency, and reliability perspectives. We identify the impact of rank subsetting on memory power and processor performance analytically, compare two promising rank-subsetting proposals, Multicore DIMM and mini-rank, and verify our analysis by simulating a chip-multiprocessor system using multithreaded and consolidated workloads. We extend the design of Multicore DIMM for high-reliability systems and show that compared with conventional chipkill approaches, rank subsetting can lead to much higher system-level energy efficiency and performance at the cost of additional DRAM devices. This holistic assessment shows that rank subsetting offers compelling alternatives to existing processor-memory interfaces for future DDR systems.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W1991271856",
    "type": "article"
  },
  {
    "title": "Thread Tranquilizer",
    "doi": "https://doi.org/10.1145/2086696.2086725",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Kishore Kumar Pusukuri; Rajiv Gupta; Laxmi N. Bhuyan",
    "corresponding_authors": "",
    "abstract": "To realize the performance potential of multicore systems, we must effectively manage the interactions between memory reference behavior and the operating system policies for thread scheduling and migration decisions. We observe that these interactions lead to significant variations in the performance of a given application, from one execution to the next, even when the program input remains unchanged and no other applications are being run on the system. Our experiments with multithreaded programs, including the TATP database application, SPECjbb2005, and a subset of PARSEC and SPEC OMP programs, on a 24-core Dell PowerEdge R905 server running OpenSolaris confirms the above observation. In this work we develop Thread Tranquilizer, an automatic technique for simultaneously reducing performance variation and improving performance by dynamically choosing appropriate memory allocation and process scheduling policies. Thread Tranquilizer uses simple utilities available on modern Operating Systems for monitoring cache misses and thread context-switches and then utilizes the collected information to dynamically select appropriate memory allocation and scheduling policies. In our experiments, Thread Tranquilizer yields up to 98% (average 68%) reduction in performance variation and up to 43% (average 15%) improvement in performance over default policies of OpenSolaris. We also demonstrate that Thread Tranquilizer simultaneously reduces performance variation and improves performance of the programs on Linux. Thread Tranquilizer is easy to use as it does not require any changes to the application source code or the OS kernel.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2168155345",
    "type": "article"
  },
  {
    "title": "Intercepting Functions for Memoization",
    "doi": "https://doi.org/10.1145/2751559",
    "publication_date": "2015-06-24",
    "publication_year": 2015,
    "authors": "Arjun Suresh; Bharath Narasimha Swamy; Erven Rohou; André Seznec",
    "corresponding_authors": "",
    "abstract": "Memoization is the technique of saving the results of executions so that future executions can be omitted when the input set repeats. Memoization has been proposed in previous literature at the instruction, basic block, and function levels using hardware, as well as pure software--level approaches including changes to programming language. In this article, we focus on software memoization for procedural languages such as C and Fortran at the granularity of a function. We propose a simple linker-based technique for enabling software memoization of any dynamically linked pure function by function interception and illustrate our framework using a set of computationally expensive pure functions—the transcendental functions. Transcendental functions are those that cannot be expressed in terms of a finite sequence of algebraic operations (trigonometric functions, exponential functions, etc.) and hence are computationally expensive. Our technique does not need the availability of source code and thus can even be applied to commercial applications, as well as applications with legacy codes. As far as users are concerned, enabling memoization is as simple as setting an environment variable. Our framework does not make any specific assumptions about the underlying architecture or compiler toolchains and can work with a variety of current architectures. We present experimental results for a x86-64 platform using both gcc and icc compiler toolchains, and an ARM Cortex-A9 platform using gcc. Our experiments include a mix of real-world programs and standard benchmark suites: SPEC and Splash2x. On standard benchmark applications that extensively call the transcendental functions, we report memoization benefits of up to 50% on Intel Ivy Bridge and up to 10% on ARM Cortex-A9. Memoization was able to regain a performance loss of 76% in bwaves due to a known performance bug in the GNU implementation of the pow function. The same benchmark on ARM Cortex-A9 benefited by more than 200%.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2206208003",
    "type": "article"
  },
  {
    "title": "DawnCC",
    "doi": "https://doi.org/10.1145/3084540",
    "publication_date": "2017-05-26",
    "publication_year": 2017,
    "authors": "Gleison Souza Diniz Mendonça; Breno Campos Ferreira Guimarães; Péricles Alves; Márcio Pereira; Guido Araújo; Fernando Magno Quintão Pereira",
    "corresponding_authors": "",
    "abstract": "Directive-based programming models, such as OpenACC and OpenMP, allow developers to convert a sequential program into a parallel one with minimum human intervention. However, inserting pragmas into production code is a difficult and error-prone task, often requiring familiarity with the target program. This difficulty restricts the ability of developers to annotate code that they have not written themselves. This article provides a suite of compiler-related methods to mitigate this problem. Such techniques rely on symbolic range analysis, a well-known static technique, to achieve two purposes: populate source code with data transfer primitives and to disambiguate pointers that could hinder automatic parallelization due to aliasing. We have materialized our ideas into a tool, DawnCC, which can be used stand-alone or through an online interface. To demonstrate its effectiveness, we show how DawnCC can annotate the programs available in PolyBench without any intervention from users. Such annotations lead to speedups of over 100× in an Nvidia architecture and over 50× in an ARM architecture.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2619305731",
    "type": "article"
  },
  {
    "title": "Optimizing GPU energy efficiency with 3D die-stacking graphics memory and reconfigurable memory interface",
    "doi": "https://doi.org/10.1145/2541228.2541231",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Jishen Zhao; Guangyu Sun; Gabriel H. Loh; Yuan Xie",
    "corresponding_authors": "",
    "abstract": "The performance of graphics processing unit (GPU) systems is improving rapidly to accommodate the increasing demands of graphics and high-performance computing applications. With such a performance improvement, however, power consumption of GPU systems is dramatically increased. Up to 30% of the total power of a GPU system is consumed by the graphic memory itself. Therefore, reducing graphics memory power consumption is critical to mitigate the power challenge. In this article, we propose an energy-efficient reconfigurable 3D die-stacking graphics memory design that integrates wide-interface graphics DRAMs side-by-side with a GPU processor on a silicon interposer. The proposed architecture is a “3D+2.5D” system, where the DRAM memory itself is 3D stacked memory with through-silicon via (TSV), whereas the integration of DRAM and the GPU processor is through the interposer solution (2.5D). Since GPU computing units, memory controllers, and memory are all integrated in the same package, the number of memory I/Os is no longer constrained by the package’s pin count. We can reduce the memory power consumption by scaling down the supply voltage and frequency of memory interface while maintaining the same or even higher peak memory bandwidth. In addition, we design a reconfigurable memory interface that can dynamically adapt to the requirements of various applications. We propose two reconfiguration mechanisms to optimize the GPU system energy efficiency and throughput, respectively, and thus benefit both memory-intensive and compute-intensive applications. The experimental results show that the proposed GPU memory architecture can effectively improve GPU system energy efficiency by 21%, without reconfiguration. The reconfigurable memory interface can further improve the system energy efficiency by 26%, and system throughput by 31% under a capped system power budget of 240W.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W1990962327",
    "type": "article"
  },
  {
    "title": "Understanding fundamental design choices in single-ISA heterogeneous multicore architectures",
    "doi": "https://doi.org/10.1145/2400682.2400691",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Kenzo Van Craeynest; Lieven Eeckhout",
    "corresponding_authors": "",
    "abstract": "Single-ISA heterogeneous multicore processors have gained substantial interest over the past few years because of their power efficiency, as they offer the potential for high overall chip throughput within a given power budget. Prior work in heterogeneous architectures has mainly focused on how heterogeneity can improve overall system throughput. To what extent heterogeneity affects per-program performance has remained largely unanswered. In this article, we aim at understanding how heterogeneity affects both chip throughput and per-program performance; how heterogeneous architectures compare to homogeneous architectures under both performance metrics; and how fundamental design choices, such as core type, cache size, and off-chip bandwidth, affect performance. We use analytical modeling to explore a large space of single-ISA heterogeneous architectures. The analytical model has linear-time complexity in the number of core types and programs of interest, and offers a unique opportunity for exploring the large space of both homogeneous and heterogeneous multicore processors in limited time. Our analysis provides several interesting insights: While it is true that heterogeneity can improve system throughput, it fundamentally trades per-program performance for chip throughput; although some heterogeneous configurations yield better throughput and per-program performance than homogeneous designs, some homogeneous configurations are optimal for particular throughput versus per-program performance trade-offs. Two core types provide most of the benefits from heterogeneity and a larger number of core types does not contribute much; job-to-core mapping is both important and challenging for heterogeneous multicore processors to achieve optimum performance. Limited off-chip bandwidth does alter some of the fundamental design choices in heterogeneous multicore architectures, such as the need for large on-chip caches for achieving high throughput, and per-program performance degrading more relative to throughput under constrained off-chip bandwidth.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2054548832",
    "type": "article"
  },
  {
    "title": "Main Memory in HPC",
    "doi": "https://doi.org/10.1145/3023362",
    "publication_date": "2017-03-06",
    "publication_year": 2017,
    "authors": "Darko Živanovič; Milan Pavlović; Milan Radulovic; Hyunsung Shin; Jong-Pil Son; Sally A. McKee; Paul Carpenter; Petar Radojković; Eduard Ayguadé",
    "corresponding_authors": "",
    "abstract": "An important aspect of High-Performance Computing (HPC) system design is the choice of main memory capacity. This choice becomes increasingly important now that 3D-stacked memories are entering the market. Compared with conventional Dual In-line Memory Modules (DIMMs), 3D memory chiplets provide better performance and energy efficiency but lower memory capacities. Therefore, the adoption of 3D-stacked memories in the HPC domain depends on whether we can find use cases that require much less memory than is available now. This study analyzes the memory capacity requirements of important HPC benchmarks and applications. We find that the High-Performance Conjugate Gradients (HPCG) benchmark could be an important success story for 3D-stacked memories in HPC, but High-Performance Linpack (HPL) is likely to be constrained by 3D memory capacity. The study also emphasizes that the analysis of memory footprints of production HPC applications is complex and that it requires an understanding of application scalability and target category, i.e., whether the users target capability or capacity computing. The results show that most of the HPC applications under study have per-core memory footprints in the range of hundreds of megabytes, but we also detect applications and use cases that require gigabytes per core. Overall, the study identifies the HPC applications and use cases with memory footprints that could be provided by 3D-stacked memory chiplets, making a first step toward adoption of this novel technology in the HPC domain.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2592263880",
    "type": "article"
  },
  {
    "title": "Low Complexity Multiply-Accumulate Units for Convolutional Neural Networks with Weight-Sharing",
    "doi": "https://doi.org/10.1145/3233300",
    "publication_date": "2018-09-04",
    "publication_year": 2018,
    "authors": "J. W. Garland; David Gregg",
    "corresponding_authors": "",
    "abstract": "Convolutional neural networks (CNNs) are one of the most successful machine-learning techniques for image, voice, and video processing. CNNs require large amounts of processing capacity and memory bandwidth. Hardware accelerators have been proposed for CNNs that typically contain large numbers of multiply-accumulate (MAC) units, the multipliers of which are large in integrated circuit (IC) gate count and power consumption. “Weight-sharing” accelerators have been proposed where the full range of weight values in a trained CNN are compressed and put into bins, and the bin index is used to access the weight-shared value. We reduce power and area of the CNN by implementing parallel accumulate shared MAC (PASM) in a weight-shared CNN. PASM re-architects the MAC to instead count the frequency of each weight and place it in a bin. The accumulated value is computed in a subsequent multiply phase, significantly reducing gate count and power consumption of the CNN. In this article, we implement PASM in a weight-shared CNN convolution hardware accelerator and analyze its effectiveness. Experiments show that for a clock speed 1GHz implemented on a 45nm ASIC process our approach results in fewer gates, smaller logic, and reduced power with only a slight increase in latency. We also show that the same weight-shared-with-PASM CNN accelerator can be implemented in resource-constrained FPGAs, where the FPGA has limited numbers of digital signal processor (DSP) units to accelerate the MAC operations.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2963287959",
    "type": "article"
  },
  {
    "title": "Revisiting the Complexity of Hardware Cache Coherence and Some Implications",
    "doi": "https://doi.org/10.1145/2663345",
    "publication_date": "2014-12-08",
    "publication_year": 2014,
    "authors": "Rakesh Komuravelli; Sarita V. Adve; Ching-Tsun Chou",
    "corresponding_authors": "",
    "abstract": "Cache coherence is an integral part of shared-memory systems but is also widely considered to be one of the most complex parts of such systems. Much prior work has addressed this complexity and the verification techniques to prove the correctness of hardware coherence. Given the new multicore era with increasing number of cores, there is a renewed debate about whether the complexity of hardware coherence has been tamed or whether it should be abandoned in favor of software coherence. This article revisits the complexity of hardware cache coherence by verifying a publicly available, state-of-the-art implementation of the widely used MESI protocol, using the Murφ model checking tool. To our surprise, we found six bugs in this protocol, most of which were hard to analyze and took several days to fix. To compare the complexity, we also verified the recently proposed DeNovo protocol, which exploits disciplined software programming models. We found three relatively easy to fix bugs in this less mature protocol. After fixing these bugs, our verification experiments showed that, compared to DeNovo, MESI had 15X more reachable states leading to a 20X increase in verification (model checking) time. Although we were eventually successful in verifying the protocols, the tool required making several simplifying assumptions (e.g., two cores, one address). Our results have several implications: (1) they indicate that hardware coherence protocols remain complex; (2) they reinforce the need for protocol designers to embrace formal verification tools to demonstrate correctness of new protocols and extensions; (3) they reinforce the need for formal verification tools that are both scalable and usable by non-expert; and (4) they show that a system based on hardware-software co-design can offer a simpler approach for cache coherence, thus reducing the overall verification effort and allowing verification of more detailed models and protocol extensions that are otherwise limited by computing resources.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W2095319311",
    "type": "article"
  },
  {
    "title": "On How to Accelerate Iterative Stencil Loops",
    "doi": "https://doi.org/10.1145/2842615",
    "publication_date": "2015-12-08",
    "publication_year": 2015,
    "authors": "Riccardo Cattaneo; Giuseppe Natale; Carlo Sicignano; Donatella Sciuto; Marco D. Santambrogio",
    "corresponding_authors": "",
    "abstract": "In high-performance systems, stencil computations play a crucial role as they appear in a variety of different fields of application, ranging from partial differential equation solving, to computer simulation of particles’ interaction, to image processing and computer vision. The computationally intensive nature of those algorithms created the need for solutions to efficiently implement them in order to save both execution time and energy. This, in combination with their regular structure, has justified their widespread study and the proposal of largely different approaches to their optimization. However, most of these works are focused on aggressive compile time optimization, cache locality optimization, and parallelism extraction for the multicore/multiprocessor domain, while fewer works are focused on the exploitation of custom architectures to further exploit the regular structure of Iterative Stencil Loops (ISLs), specifically with the goal of improving power efficiency. This work introduces a methodology to systematically design power-efficient hardware accelerators for the optimal execution of ISL algorithms on Field-programmable Gate Arrays (FPGAs). As part of the methodology, we introduce the notion of Streaming Stencil Time-step (SST), a streaming-based architecture capable of achieving both low resource usage and efficient data reuse thanks to an optimal data buffering strategy, and we introduce a technique called SSTs queuing that is capable of delivering a pseudolinear execution time speedup with constant bandwidth. The methodology has been validated on significant benchmarks on a Virtex-7 FPGA using the Xilinx Vivado suite. Results demonstrate how the efficient usage of the on-chip memory resources realized by an SST allows one to treat problem sizes whose implementation would otherwise not be possible via direct synthesis of the original, unmanipulated code via High-Level Synthesis (HLS). We also show how the SSTs queuing effectively ensures a pseudolinear throughput speedup while consuming constant off-chip bandwidth.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2221279259",
    "type": "article"
  },
  {
    "title": "PIMBALL",
    "doi": "https://doi.org/10.1145/3357250",
    "publication_date": "2019-10-11",
    "publication_year": 2019,
    "authors": "Salonik Resch; S. Karen Khatamifard; Zamshed I. Chowdhury; Masoud Zabihi; Zhengyang Zhao; Jian‐Ping Wang; Sachin S. Sapatnekar; Ulya R. Karpuzcu",
    "corresponding_authors": "",
    "abstract": "Neural networks span a wide range of applications of industrial and commercial significance. Binary neural networks (BNN) are particularly effective in trading accuracy for performance, energy efficiency, or hardware/software complexity. Here, we introduce a spintronic, re-configurable in-memory BNN accelerator, PIMBALL: P rocessing I n M emory B NN A cce L(L) erator, which allows for massively parallel and energy efficient computation. PIMBALL is capable of being used as a standard spintronic memory (STT-MRAM) array and a computational substrate simultaneously. We evaluate PIMBALL using multiple image classifiers and a genomics kernel. Our simulation results show that PIMBALL is more energy efficient than alternative CPU-, GPU-, and FPGA-based implementations while delivering higher throughput.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2980249334",
    "type": "article"
  },
  {
    "title": "Data-driven Mixed Precision Sparse Matrix Vector Multiplication for GPUs",
    "doi": "https://doi.org/10.1145/3371275",
    "publication_date": "2019-12-17",
    "publication_year": 2019,
    "authors": "Khalid Ahmad; Hari Sundar; Mary Hall",
    "corresponding_authors": "",
    "abstract": "We optimize Sparse Matrix Vector multiplication (SpMV) using a mixed precision strategy (MpSpMV) for Nvidia V100 GPUs. The approach has three benefits: (1) It reduces computation time, (2) it reduces the size of the input matrix and therefore reduces data movement, and (3) it provides an opportunity for increased parallelism. MpSpMV’s decision to lower to single precision is data driven , based on individual nonzero values of the sparse matrix. On all real-valued matrices from the Sparse Matrix Collection, we obtain a maximum speedup of 2.61× and average speedup of 1.06× over double precision, while maintaining higher accuracy compared to single precision.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2996522450",
    "type": "article"
  },
  {
    "title": "Fast Crown Scheduling Heuristics for Energy-Efficient Mapping and Scaling of Moldable Streaming Tasks on Manycore Systems",
    "doi": "https://doi.org/10.1145/2687653",
    "publication_date": "2015-01-09",
    "publication_year": 2015,
    "authors": "Nicolas Melot; Christoph Keßler; Jörg Keller; Patrick Eitschberger",
    "corresponding_authors": "",
    "abstract": "Exploiting effectively massively parallel architectures is a major challenge that stream programming can help facilitate. We investigate the problem of generating energy-optimal code for a collection of streaming tasks that include parallelizable or moldable tasks on a generic manycore processor with dynamic discrete frequency scaling. Streaming task collections differ from classical task sets in that all tasks are running concurrently, so that cores typically run several tasks that are scheduled round-robin at user level in a data-driven way. A stream of data flows through the tasks and intermediate results may be forwarded to other tasks, as in a pipelined task graph. In this article, we consider crown scheduling , a novel technique for the combined optimization of resource allocation, mapping, and discrete voltage/frequency scaling for moldable streaming task collections in order to optimize energy efficiency given a throughput constraint. We first present optimal offline algorithms for separate and integrated crown scheduling based on integer linear programming (ILP). We make no restricting assumption about speedup behavior. We introduce the fast heuristic Longest Task, Lowest Group (LTLG) as a generalization of the Longest Processing Time (LPT) algorithm to achieve a load-balanced mapping of parallel tasks, and the Height heuristic for crown frequency scaling. We use them in feedback loop heuristics based on binary search and simulated annealing to optimize crown allocation. Our experimental evaluation of the ILP models for a generic manycore architecture shows that at least for small and medium-sized streaming task collections even the integrated variant of crown scheduling can be solved to optimality by a state-of-the-art ILP solver within a few seconds. Our heuristics produce makespan and energy consumption close to optimality within the limits of the phase-separated crown scheduling technique and the crown structure. Their optimization time is longer than the one of other algorithms we test, but our heuristics consistently produce better solutions.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W3016676131",
    "type": "article"
  },
  {
    "title": "Performance Optimization of the HPCG Benchmark on the Sunway TaihuLight Supercomputer",
    "doi": "https://doi.org/10.1145/3182177",
    "publication_date": "2018-03-22",
    "publication_year": 2018,
    "authors": "Yulong Ao; Chao Yang; Fangfang Liu; Wanwang Yin; Lijuan Jiang; Qiao Sun",
    "corresponding_authors": "",
    "abstract": "In this article, we present some key techniques for optimizing HPCG on Sunway TaihuLight and demonstrate how to achieve high performance in memory-bound applications by exploiting specific characteristics of the hardware architecture. In particular, we utilize a block multicoloring approach for parallelization and propose methods such as requirement-based data mapping and customized gather collective to enhance the effective memory bandwidth. Experiments indicate that the optimized HPCG code can sustain 77% of the theoretical memory bandwidth and scale to the full system of more than 10 million cores, with an aggregated performance of 480.8 Tflop/s and a weak scaling efficiency of 87.3%.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2794424798",
    "type": "article"
  },
  {
    "title": "High-Performance Generalized Tensor Operations",
    "doi": "https://doi.org/10.1145/3235029",
    "publication_date": "2018-09-04",
    "publication_year": 2018,
    "authors": "Roman A. Gareev; Tobias Grosser; Michael Kruse",
    "corresponding_authors": "",
    "abstract": "The efficiency of tensor contraction is of great importance. Compilers cannot optimize it well enough to come close to the performance of expert-tuned implementations. All existing approaches that provide competitive performance require optimized external code. We introduce a compiler optimization that reaches the performance of optimized BLAS libraries without the need for an external implementation or automatic tuning. Our approach provides competitive performance across hardware architectures and can be generalized to deliver the same benefits for algebraic path problems. By making fast linear algebra kernels available to everyone, we expect productivity increases when optimized libraries are not available.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2891477368",
    "type": "article"
  },
  {
    "title": "ShiftsReduce",
    "doi": "https://doi.org/10.1145/3372489",
    "publication_date": "2019-12-26",
    "publication_year": 2019,
    "authors": "Asif Ali Khan; Fazal Hameed; Robin Bläsing; S. Parkin; Jeronimo Castrillon",
    "corresponding_authors": "",
    "abstract": "Racetrack memories (RMs) have significantly evolved since their conception in 2008, making them a serious contender in the field of emerging memory technologies. Despite key technological advancements, the access latency and energy consumption of an RM-based system are still highly influenced by the number of shift operations. These operations are required to move bits to the right positions in the racetracks. This paper presents data placement techniques for RMs that maximize the likelihood that consecutive references access nearby memory locations at runtime thereby minimizing the number of shifts. We present an integer linear programming (ILP) formulation for optimal data placement in RMs, and revisit existing offset assignment heuristics, originally proposed for random-access memories. We introduce a novel heuristic tailored to a realistic RM and combine it with a genetic search to further improve the solution. We show a reduction in the number of shifts of up to 52.5%, outperforming the state of the art by up to 16.1%.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W4288083819",
    "type": "article"
  },
  {
    "title": "DNNTune",
    "doi": "https://doi.org/10.1145/3368305",
    "publication_date": "2019-12-26",
    "publication_year": 2019,
    "authors": "Chunwei Xia; Jiacheng Zhao; Huimin Cui; Xiaobing Feng; Jingling Xue",
    "corresponding_authors": "",
    "abstract": "Deep Neural Networks (DNNs) are now increasingly adopted in a variety of Artificial Intelligence (AI) applications. Meantime, more and more DNNs are moving from cloud to the mobile devices, as emerging AI chips are integrated into mobiles. Therefore, the DNN models can be deployed in the cloud, on the mobile devices, or even mobile-cloud coordinate processing, making it a big challenge to select an optimal deployment strategy under specific objectives. This article proposes a DNN tuning framework, i.e., DNNTune, that can provide layer-wise behavior analysis across a number of platforms. Using DNNTune, this article further selects 13 representative DNN models, including CNN, LSTM, and MLP, and three mobile devices ranging from low-end to high-end, and two AI accelerator chips to characterize the DNN models on these devices to further assist users finding opportunities for mobile-cloud coordinate computing. Our experimental results demonstrate that DNNTune can find a coordinated deployment achieving up to 1.66× speedup and 15× energy saving comparing with mobile-only and cloud-only deployment.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2997215450",
    "type": "article"
  },
  {
    "title": "ArmorAll",
    "doi": "https://doi.org/10.1145/3382132",
    "publication_date": "2020-05-29",
    "publication_year": 2020,
    "authors": "Charu Kalra; Fritz Previlon; Norm Rubin; David Kaeli",
    "corresponding_authors": "",
    "abstract": "The vulnerability of GPUs to soft errors has become a first-class design concern as they are increasingly being used in accuracy-sensitive and safety-critical domains. Existing solutions used to enhance the reliability of GPUs come with significant overhead in terms of area, power, and/or performance. In this article, we propose ArmorAll, a light-weight, adaptive, selective, and portable software solution to protect GPUs against soft errors. ArmorAll consists of a set of purely compiler-based redundancy schemes designed to optimize instruction duplication on GPUs, thereby enabling much more reliable execution. The choice of the scheme determines the subset of instructions that must be duplicated in an application, allowing adaptable fault coverage for different applications. ArmorAll can intelligently select a redundancy scheme that provides the best coverage to an application with an accuracy of 91.7%. The high coverage provided by ArmorAll comes at an average improvement of 64.5% in runtime when using the selected redundancy scheme as compared to the state-of-the-art.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W3031001329",
    "type": "article"
  },
  {
    "title": "PAVER",
    "doi": "https://doi.org/10.1145/3451164",
    "publication_date": "2021-06-08",
    "publication_year": 2021,
    "authors": "Devashree Tripathy; AmirAli Abdolrashidi; Laxmi N. Bhuyan; Liang Zhou; Daniel Wong",
    "corresponding_authors": "",
    "abstract": "The massive parallelism present in GPUs comes at the cost of reduced L1 and L2 cache sizes per thread, leading to serious cache contention problems such as thrashing. Hence, the data access locality of an application should be considered during thread scheduling to improve execution time and energy consumption. Recent works have tried to use the locality behavior of regular and structured applications in thread scheduling, but the difficult case of irregular and unstructured parallel applications remains to be explored. We present PAVER , a P riority- A ware V ertex schedul ER , which takes a graph-theoretic approach toward thread scheduling. We analyze the cache locality behavior among thread blocks ( TBs ) through a just-in-time compilation, and represent the problem using a graph representing the TBs and the locality among them. This graph is then partitioned to TB groups that display maximum data sharing, which are then assigned to the same streaming multiprocessor by the locality-aware TB scheduler. Through exhaustive simulation in Fermi, Pascal, and Volta architectures using a number of scheduling techniques, we show that PAVER reduces L2 accesses by 43.3%, 48.5%, and 40.21% and increases the average performance benefit by 29%, 49.1%, and 41.2% for the benchmarks with high inter-TB locality.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W3168173119",
    "type": "article"
  },
  {
    "title": "Marvel: A Data-Centric Approach for Mapping Deep Learning Operators on Spatial Accelerators",
    "doi": "https://doi.org/10.1145/3485137",
    "publication_date": "2021-12-06",
    "publication_year": 2021,
    "authors": "Prasanth Chatarasi; Hyoukjun Kwon; Angshuman Parashar; Michael Pellauer; Tushar Krishna; Vivek Sarkar",
    "corresponding_authors": "",
    "abstract": "A spatial accelerator’s efficiency depends heavily on both its mapper and cost models to generate optimized mappings for various operators of DNN models. However, existing cost models lack a formal boundary over their input programs (operators) for accurate and tractable cost analysis of the mappings, and this results in adaptability challenges to the cost models for new operators. We consider the recently introduced Maestro Data-Centric (MDC) notation and its analytical cost model to address this challenge because any mapping expressed in the notation is precisely analyzable using the MDC’s cost model. In this article, we characterize the set of input operators and their mappings expressed in the MDC notation by introducing a set of conformability rules . The outcome of these rules is that any loop nest that is perfectly nested with affine tensor subscripts and without conditionals is conformable to the MDC notation. A majority of the primitive operators in deep learning are such loop nests. In addition, our rules enable us to automatically translate a mapping expressed in the loop nest form to MDC notation and use the MDC’s cost model to guide upstream mappers. Our conformability rules over the input operators result in a structured mapping space of the operators, which enables us to introduce a mapper based on our decoupled off-chip/on-chip approach to accelerate mapping space exploration. Our mapper decomposes the original higher-dimensional mapping space of operators into two lower-dimensional off-chip and on-chip subspaces and then optimizes the off-chip subspace followed by the on-chip subspace. We implemented our overall approach in a tool called Marvel , and a benefit of our approach is that it applies to any operator conformable with the MDC notation. We evaluated Marvel over major DNN operators and compared it with past optimizers.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W4200566604",
    "type": "article"
  },
  {
    "title": "CoMeT: An Integrated Interval Thermal Simulation Toolchain for 2D, 2.5D, and 3D Processor-Memory Systems",
    "doi": "https://doi.org/10.1145/3532185",
    "publication_date": "2022-06-14",
    "publication_year": 2022,
    "authors": "Lokesh Siddhu; Rajesh Kedia; Shailja Pandey; Martin Rapp; Anuj Pathania; Jörg Henkel; Preeti Ranjan Panda",
    "corresponding_authors": "",
    "abstract": "Processing cores and the accompanying main memory working in tandem enable modern processors. Dissipating heat produced from computation remains a significant problem for processors. Therefore, the thermal management of processors continues to be an active subject of research. Most thermal management research is performed using simulations, given the challenges in measuring temperatures in real processors. Fast yet accurate interval thermal simulation toolchains remain the research tool of choice to study thermal management in processors at the system level. However, the existing toolchains focus on the thermal management of cores in the processors, since they exhibit much higher power densities than memory. The memory bandwidth limitations associated with 2D processors lead to high-density 2.5D and 3D packaging technology: 2.5D packaging technology places cores and memory on the same package; 3D packaging technology takes it further by stacking layers of memory on the top of cores themselves. These new packagings significantly increase the power density of the processors, making them prone to overheating. Therefore, mitigating thermal issues in high-density processors (packaged with stacked memory) becomes even more pressing. However, given the lack of thermal modeling for memories in existing interval thermal simulation toolchains, they are unsuitable for studying thermal management for high-density processors. To address this issue, we present the first integrated Core and Memory interval Thermal ( CoMeT ) simulation toolchain. CoMeT comprehensively supports thermal simulation of high- and low-density processors corresponding to four different core-memory (integration) configurations—off-chip DDR memory, off-chip 3D memory, 2.5D, and 3D. CoMeT supports several novel features that facilitate overlying system research. CoMeT adds only an additional ~5% simulation-time overhead compared to an equivalent state-of-the-art core-only toolchain. The source code of CoMeT has been made open for public use under the MIT license.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W3203766485",
    "type": "article"
  },
  {
    "title": "Unified Buffer: Compiling Image Processing and Machine Learning Applications to Push-Memory Accelerators",
    "doi": "https://doi.org/10.1145/3572908",
    "publication_date": "2022-11-29",
    "publication_year": 2022,
    "authors": "Qiaoyi Liu; Jeff Setter; Dillon Huff; Maxwell Strange; Kathleen Feng; Mark Horowitz; Priyanka Raina; Fredrik Kjølstad",
    "corresponding_authors": "",
    "abstract": "Image processing and machine learning applications benefit tremendously from hardware acceleration. Existing compilers target either FPGAs, which sacrifice power and performance for programmability, or ASICs, which become obsolete as applications change. Programmable domain-specific accelerators, such as coarse-grained reconfigurable arrays (CGRAs), have emerged as a promising middle-ground, but they have traditionally been difficult compiler targets since they use a different memory abstraction. In contrast to CPUs and GPUs, the memory hierarchies of domain-specific accelerators use push memories : memories that send input data streams to computation kernels or to higher or lower levels in the memory hierarchy and store the resulting output data streams. To address the compilation challenge caused by push memories, we propose that the representation of these memories in the compiler be altered to directly represent them by combining storage with address generation and control logic in a single structure—a unified buffer. The unified buffer abstraction enables the compiler to separate generic push memory optimizations from the mapping to specific memory implementations in the backend. This separation allows our compiler to map high-level Halide applications to different CGRA memory designs, including some with a ready-valid interface. The separation also opens the opportunity for optimizing push memory elements on reconfigurable arrays. Our optimized memory implementation, the Physical Unified Buffer, uses a wide-fetch, single-port SRAM macro with built-in address generation logic to implement a buffer with two read and two write ports. It is 18% smaller and consumes 31% less energy than a physical buffer implementation using a dual-port memory that only supports two ports. Finally, our system evaluation shows that enabling a compiler to support CGRAs leads to performance and energy benefits. Over a wide range of image processing and machine learning applications, our CGRA achieves 4.7× better runtime and 3.5× better energy-efficiency compared to an FPGA.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W4310251122",
    "type": "article"
  },
  {
    "title": "Architectural Support for Sharing, Isolating and Virtualizing FPGA Resources",
    "doi": "https://doi.org/10.1145/3648475",
    "publication_date": "2024-02-16",
    "publication_year": 2024,
    "authors": "Panagiotis Miliadis; Dimitris Theodoropoulos; Dionisios Pnevmatikatos; Nectarios Koziris",
    "corresponding_authors": "",
    "abstract": "FPGAs are increasingly popular in cloud environments for their ability to offer on-demand acceleration and improved compute efficiency. Providers would like to increase utilization, by multiplexing customers on a single device, similar to how processing cores and memory are shared. Nonetheless, multi-tenancy still faces major architectural limitations including: (a) inefficient sharing of memory interfaces across hardware tasks (HT) exacerbated by technological limitations and peculiarities, (b) insufficient solutions for performance and data isolation and high quality of service, and (c) absent or simplistic allocation strategies to effectively distribute external FPGA memory across HT. This article presents a full-stack solution for enabling multi-tenancy on FPGAs. Specifically, our work proposes an intra-fpga virtualization layer to share FPGA interfaces and its resources across tenants. To achieve efficient inter-connectivity between virtual FPGAs (vFGPAs) and external interfaces, we employ a compact network-on-chip architecture to optimize resource utilization. Dedicated memory management units implement the concept of virtual memory in FPGAs, providing mechanisms to isolate the address space and enable memory protection. We also introduce a memory segmentation scheme to effectively allocate FPGA address space and enhance isolation through hardware-software support, while preserving the efficacy of memory transactions. We assess our solution on an Alveo U250 Data Center FPGA Card, employing 10 real-world benchmarks from the Rodinia and Rosetta suites. Our framework preserves the performance of HT from a non-virtualized environment, while enhancing the device aggregate throughput through resource sharing; up to 3.96x in isolated and up to 2.31x in highly congested settings, where an external interface is shared across four vFPGAs. Finally, our work ensures high-quality of service, with HT achieving up to 0.95x of their native performance, even when resource sharing introduces interference from other accelerators.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4391884125",
    "type": "article"
  },
  {
    "title": "Whole execution traces and their applications",
    "doi": "https://doi.org/10.1145/1089008.1089012",
    "publication_date": "2005-09-01",
    "publication_year": 2005,
    "authors": "Xiangyu Zhang; Rajiv Gupta",
    "corresponding_authors": "",
    "abstract": "Different types of program profiles (control flow, value, address, and dependence) have been collected and extensively studied by researchers to identify program characteristics that can then be exploited to develop more effective compilers and architectures. Because of the large amounts of profile data produced by realistic program runs, most work has focused on separately collecting and compressing different types of profiles. In this paper, we present a unified representation of profiles called Whole Execution Trace (WET), which includes the complete information contained in each of the above types of traces. Thus, WETs provide a basis for a next-generation software tool that will enable mining of program profiles to identify program characteristics that require understanding of relationships among various types of profiles. The key features of our WET representation are: WET is constructed by labeling a static program representation with profile information such that relevant and related profile information can be directly accessed by analysis algorithms as they traverse the representation; a highly effective two-tier strategy is used to significantly compress the WET; and compression techniques are designed such that they minimally affect the ability to rapidly traverse WET for extracting subsets of information corresponding to individual profile types as well as a combination of profile types. Our experimentation shows that on, an average, execution traces resulting from execution of 647 million statements can be stored in 331 megabytes of storage after compression. The compression factors range from 16 to 83. Moreover the rates at which different types of profiles can be individually or simultaneously extracted are high. We present two applications of WETs, dynamic program slicing and dynamic version matching, which make effective use of multiple kinds of profile information contained in WETs.",
    "cited_by_count": 48,
    "openalex_id": "https://openalex.org/W2038687965",
    "type": "article"
  },
  {
    "title": "Tradeoffs in buffering speculative memory state for thread-level speculation in multiprocessors",
    "doi": "https://doi.org/10.1145/1089008.1089010",
    "publication_date": "2005-09-01",
    "publication_year": 2005,
    "authors": "María Jesús Garzarán; Milos Prvulović; J.M. Llaberia; Víctor Viñals; Lawrence Rauchwerger; Josep Torrellas",
    "corresponding_authors": "",
    "abstract": "Thread-Level Speculation (TLS) provides architectural support to aggressively run hard-to-analyze code in parallel. As speculative tasks run concurrently, they generate unsafe or speculative memory state that needs to be separately buffered and managed in the presence of distributed caches and buffers. Such a state may contain multiple versions of the same variable. In this paper, we introduce a novel taxonomy of approaches to buffer and manage multiversion speculative memory state in multiprocessors. We also present a detailed complexity-benefit tradeoff analysis of the different approaches. Finally, we use numerical applications to evaluate the performance of the approaches under a single architectural framework. Our key insights are that support for buffering the state of multiple speculative tasks and versions per processor is more complexity-effective than support for lazily merging the state of tasks with main memory. Moreover, both supports can be gainfully combined and, in large machines, their effect is nearly fully additive. Finally, the more complex support for storing future state in the main memory can boost performance when buffers are under pressure, but hurts performance when squashes are frequent.",
    "cited_by_count": 48,
    "openalex_id": "https://openalex.org/W2114251233",
    "type": "article"
  },
  {
    "title": "Intraprogram dynamic voltage scaling",
    "doi": "https://doi.org/10.1145/1022969.1022973",
    "publication_date": "2004-09-01",
    "publication_year": 2004,
    "authors": "Fen Xie; Margaret Martonosi; Sharad Malik",
    "corresponding_authors": "",
    "abstract": "Dynamic voltage scaling (DVS) has become an important dynamic power-management technique to save energy. DVS tunes the power-performance tradeoff to the needs of the application. The goal is to minimize energy consumption while meeting performance needs. Since CPU power consumption is strongly dependent on the supply voltage, DVS exploits the ability to control the power consumption by varying a processor's supply voltage and clock frequency. However, because of the energy and time overhead associated with switching DVS modes, DVS control has been used mainly at the interprogram level.In this paper, we explore the opportunities and limits of intraprogram DVS scheduling. An analytical model is derived to predict the maximum energy savings that can be obtained using intraprogram DVS given a few known program and processor parameters. This model gives insights into scenarios where energy consumption benefits from intraprogram DVS and those where there is no benefit. The model helps us extrapolate the benefits of intraprogram DVS into the future as processor parameters change. We then examine how much of these predicted benefits can actually be achieved through compile-time optimal settings of DVS modes. We extend an existing mixed-integer linear program formulation for this scheduling problem by accurately accounting for DVS energy switching overhead, by providing finer-grained control on settings and by considering multiple data categories in the optimization. Overall, this research provides a comprehensive view of intraprogram compile-time DVS management, providing both practical techniques for its immediate deployment as well theoretical bounds for use into the future.",
    "cited_by_count": 46,
    "openalex_id": "https://openalex.org/W2093919199",
    "type": "article"
  },
  {
    "title": "Fast and efficient searches for effective optimization-phase sequences",
    "doi": "https://doi.org/10.1145/1071604.1071607",
    "publication_date": "2005-06-01",
    "publication_year": 2005,
    "authors": "Prasad A. Kulkarni; Stephen Hines; David Whalley; Jason D. Hiser; Jack W. Davidson; Douglas L. Jones",
    "corresponding_authors": "",
    "abstract": "It has long been known that a fixed ordering of optimization phases will not produce the best code for every application. One approach for addressing this phase-ordering problem is to use an evolutionary algorithm to search for a specific sequence of phases for each module or function. While such searches have been shown to produce more efficient code, the approach can be extremely slow because the application is compiled and possibly executed to evaluate each sequence's effectiveness. Consequently, evolutionary or iterative compilation schemes have been promoted for compilation systems targeting embedded applications where meeting strict constraints on execution time, code size, and power consumption is paramount and longer compilation times may be tolerated in the final stage of development, when an application is compiled one last time and embedded in a product. Unfortunately, even for small embedded applications, the search process can take many hours or even days making the approach less attractive to developers. In this paper, we describe two complementary general approaches for achieving faster searches for effective optimization sequences when using a genetic algorithm. The first approach reduces the search time by avoiding unnecessary executions of the application when possible. Results indicate search time reductions of 62%, on average, often reducing searches from hours to minutes. The second approach modifies the search so fewer generations are required to achieve the same results. Measurements show this approach decreases the average number of required generations by 59%. These improvements have the potential for making evolutionary compilation a viable choice for tuning embedded applications.",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W2072198393",
    "type": "article"
  },
  {
    "title": "Single-dimension software pipelining for multidimensional loops",
    "doi": "https://doi.org/10.1145/1216544.1216550",
    "publication_date": "2007-03-01",
    "publication_year": 2007,
    "authors": "Hongbo Rong; Zhizhong Tang; R. Govindarajan; Alban Douillet; Guang R. Gao",
    "corresponding_authors": "",
    "abstract": "Traditionally, software pipelining is applied either to the innermost loop of a given loop nest or from the innermost loop to outer loops. This paper proposes a three-step approach, called single-dimension software pipelining (SSP) , to software pipeline a loop nest at an arbitrary loop level that has a rectangular iteration space and contains no sibling inner loops in it. The first step identifies the most profitable loop level for software pipelining in terms of initiation rate, data reuse potential, or any other optimization criteria. The second step simplifies the multidimensional data-dependence graph (DDG) of the selected loop level into a one-dimensional DDG and constructs a one-dimensional (1D) schedule. Based on the one-dimensional schedule, the third step derives a simple mapping function that specifies the schedule time for the operation instances in the multidimensional loop. The classical modulo scheduling is subsumed by SSP as a special case. SSP is also closely related to hyperplane scheduling, and, in fact, extends it to be resource constrained. We prove that SSP schedules are correct and at least as efficient as those schedules generated by traditional modulo scheduling methods. We extend SSP to schedule imperfect loop nests, which are most common at the instruction level. Multiple initiation intervals are naturally allowed to improve execution efficiency. Feasibility and correctness of our approach are verified by a prototype implementation in the ORC compiler for the IA-64 architecture, tested with loop nests from Livermore and SPEC2000 floating-point benchmarks. Preliminary experimental results reveal that, compared to modulo scheduling, software pipelining at an appropriate loop level results in significant performance improvement. Software pipelining is beneficial even with prior loop transformations.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W2135234700",
    "type": "article"
  },
  {
    "title": "CAVA",
    "doi": "https://doi.org/10.1145/1138035.1138038",
    "publication_date": "2006-06-01",
    "publication_year": 2006,
    "authors": "Luís Ceze; Karin Strauß; James Tuck; Josep Torrellas; Jose Renau",
    "corresponding_authors": "",
    "abstract": "Modern superscalar processors often suffer long stalls because of load misses in on-chip L2 caches. To address this problem, we propose hiding L2 misses with Checkpoint-Assisted VAlue prediction (CAVA). On an L2 cache miss, a predicted value is returned to the processor. When the missing load finally reaches the head of the ROB, the processor checkpoints its state, retires the load, and speculatively uses the predicted value and continues execution. When the value in memory arrives at the L2 cache, it is compared to the predicted value. If the prediction was correct, speculation has succeeded and execution continues; otherwise, execution is rolled back and restarted from the checkpoint. CAVA uses fast checkpointing, speculative buffering, and a modest-sized value prediction structure that has about 50% accuracy. Compared to an aggressive superscalar processor, CAVA speeds up execution by up to 1.45 for SPECint applications and 1.58 for SPECfp applications, with a geometric mean of 1.14 for SPECint and 1.34 for SPECfp applications. We also evaluate an implementation of Runahead execution---a previously proposed scheme that does not perform value prediction and discards all work done between checkpoint and data reception from memory. Runahead execution speeds up execution by a geometric mean of 1.07 for SPECint and 1.18 for SPECfp applications, compared to the same baseline.",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W2152318498",
    "type": "article"
  },
  {
    "title": "Distilling the essence of proprietary workloads into miniature benchmarks",
    "doi": "https://doi.org/10.1145/1400112.1400115",
    "publication_date": "2008-08-01",
    "publication_year": 2008,
    "authors": "Ajay Joshi; Lieven Eeckhout; Robert H. Bell; Lizy K. John",
    "corresponding_authors": "",
    "abstract": "Benchmarks set standards for innovation in computer architecture research and industry product development. Consequently, it is of paramount importance that these workloads are representative of real-world applications. However, composing such representative workloads poses practical challenges to application analysis teams and benchmark developers (1) real-world workloads are intellectual property and vendors hesitate to share these proprietary applications; and (2) porting and reducing these applications to benchmarks that can be simulated in a tractable amount of time is a nontrivial task. In this paper, we address this problem by proposing a technique that automatically distills key inherent behavioral attributes of a proprietary workload and captures them into a miniature synthetic benchmark clone. The advantage of the benchmark clone is that it hides the functional meaning of the code but exhibits similar performance characteristics as the target application. Moreover, the dynamic instruction count of the synthetic benchmark clone is substantially shorter than the proprietary application, greatly reducing overall simulation time for SPEC CPU, the simulation time reduction is over five orders of magnitude compared to entire benchmark execution. Using a set of benchmarks representative of general-purpose, scientific, and embedded applications, we demonstrate that the power and performance characteristics of the synthetic benchmark clone correlate well with those of the original application across a wide range of microarchitecture configurations.",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W2164197246",
    "type": "article"
  },
  {
    "title": "Software-directed power-aware interconnection networks",
    "doi": "https://doi.org/10.1145/1216544.1216548",
    "publication_date": "2007-03-01",
    "publication_year": 2007,
    "authors": "Vassos Soteriou; Noel Eisley; Li-Shiuan Peh",
    "corresponding_authors": "",
    "abstract": "Interconnection networks have been deployed as the communication fabric in a wide spectrum of parallel computer systems, ranging from chip multiprocessors (CMPs) and embedded multicore systems-on-a-chip (SoCs) to clusters and server blades. Recent technology trends have permitted a rapid growth of chip resources, faster clock rates, and wider communication bandwidths, however, these trends have also led to an increase in power consumption that is becoming a key limiting factor in the design of such scalable interconnected systems. Power-aware networks, therefore, need to become inherent components of single and multi-chip parallel systems. In the hardware arena, recent interconnection network power-management research work has employed limited-scope techniques that mostly focus on reducing the power consumed by the network communication links. As these limited-scope techniques are not tailored to the applications running on the network, power savings and the corresponding impact on network latency vary significantly from one application to the next as we demonstrate in this paper; in many cases, network performance can severely suffer. In the software arena, extensive research on compile-time optimizations has produced parallelizing compilers that can efficiently map an application onto hardware for high performance. However, research into power-aware parallelizing compilers is in its infancy. In this paper, we take the first steps toward tailoring applications' communication needs at run-time for low power. We propose software techniques that extend the flow of a parallelizing compiler in order to direct run-time network power reduction. We target network links, a significant power consumer in these systems, allowing dynamic voltage scaling (DVS) instructions extracted during static compilation to orchestrate link voltage and frequency transitions for power savings during application run-time. Concurrently, an online hardware mechanism measures network congestion levels and adapts these off-line DVS settings to maximize network performance. Our simulations over three existing parallel systems, ranging from very fine-grained single-chip to coarse-grained multi-chip architectures, show that link power consumption can be reduced by up to 76.3%, with a minor increase in latency, ranging from 0.18 to 6.78% across a number of benchmark suites.",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W1992572001",
    "type": "article"
  },
  {
    "title": "Fairness enforcement in switch on event multithreading",
    "doi": "https://doi.org/10.1145/1275937.1275939",
    "publication_date": "2007-09-01",
    "publication_year": 2007,
    "authors": "Ron Gabor; Shlomo Weiss; Avi Mendelson",
    "corresponding_authors": "",
    "abstract": "The need to reduce power and complexity will increase the interest in Switch On Event multithreading (coarse-grained multithreading). Switch On Event multithreading is a low-power and low-complexity mechanism to improve processor throughput by switching threads on execution stalls. Fairness may, however, become a problem in a multithreaded processor. Unless fairness is properly handled, some threads may starve while others consume all of the processor cycles. Heuristics that were devised in order to improve fairness in simultaneous multithreading are not applicable to Switch On Event multithreading. This paper defines the fairness metric using the ratio of the individual threads' speedups and shows how it can be enforced in Switch On Event multithreading. Fairness is controlled by forcing additional thread switch points. These switch points are determined dynamically by runtime estimation of the single threaded performance of each of the individual threads. We analyze the impact of the fairness enforcement mechanism on aggregate IPC and weighted speedup. We present simulation results of the performance of Switch On Event multithreading. Switch On Event multithreading achieves an average aggregate IPC increase of 26% over single thread and 12% weighted speedup when no fairness is enforced. In this case, a sixth of our runs resulted in poor fairness in which one thread ran extremely slowly (10 to 100 times slower than its single-thread performance), while the other thread's performance was hardly affected. By using the proposed mechanism, we can guarantee fairness at different levels of strictness and, in most cases, even improve the weighted speedup.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W1985089255",
    "type": "article"
  },
  {
    "title": "Design exploration of hybrid caches with disparate memory technologies",
    "doi": "https://doi.org/10.1145/1880037.1880040",
    "publication_date": "2010-12-01",
    "publication_year": 2010,
    "authors": "Xiaoxia Wu; Jian Li; Lixin Zhang; Evan W. Speight; Ram Rajamony; Yuan Xie",
    "corresponding_authors": "",
    "abstract": "Traditional multilevel SRAM-based cache hierarchies, especially in the context of chip multiprocessors (CMPs), present many challenges in area requirements, core--to--cache balance, power consumption, and design complexity. New advancements in technology enable caches to be built from other technologies, such as Embedded DRAM (EDRAM), Magnetic RAM (MRAM), and Phase-change RAM (PRAM), in both 2D chips or 3D stacked chips. Caches fabricated in these technologies offer dramatically different power-performance characteristics when compared with SRAM-based caches, particularly in the areas of access latency, cell density, and overall power consumption. In this article, we propose to take advantage of the best characteristics that each technology has to offer through the use of Hybrid Cache Architecture (HCA) designs. We discuss and evaluate two types of hybrid cache architectures: intercache-Level HCA (LHCA), in which the levels in a cache hierarchy can be made of disparate memory technologies; and intracache-level or cache-Region-based HCA (RHCA), where a single level of cache can be partitioned into multiple regions, each of a different memory technology. We have studied a number of different HCA architectures and explored the potential of hardware support for intracache data movement and power consumption management within HCA caches. Utilizing a full-system simulator that has been validated against real hardware, we demonstrate that an LHCA design can provide a geometric mean 6% IPC improvement over a baseline 3-level SRAM cache design under the same area constraint across a collection of 30 workloads. A more aggressive RHCA-based design provides 10% IPC improvement over the baseline. A 2-layer 3D cache stack (3DHCA) of high density memory technology within the same chip footprint gives 16% IPC improvement over the baseline. We also achieve up to a 72% reduction in power consumption over a baseline SRAM-only design. Energy-delay and thermal evaluation for 3DHCA are also presented. In addition to the fast-slow region based RHCA, we further evaluate read-write region based RHCA designs.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W1968686941",
    "type": "article"
  },
  {
    "title": "Performance-aware thermal management via task scheduling",
    "doi": "https://doi.org/10.1145/1736065.1736070",
    "publication_date": "2010-04-01",
    "publication_year": 2010,
    "authors": "Xiuyi Zhou; Jun Yang; Marek Chrobák; Youtao Zhang",
    "corresponding_authors": "",
    "abstract": "High on-chip temperature impairs the processor's reliability and reduces its lifetime. Hardware-level dynamic thermal management (DTM) techniques can effectively constrain the chip temperature, but degrades the performance. We propose an OS-level technique that performs thermal-aware job scheduling to reduce DTMs. The algorithm is based on the observation that hot and cool jobs executed in a different order can make a difference in resulting temperature. Real-system implementation in Linux shows that our scheduler can remove 10.5% to 73.6% of the hardware DTMs in a medium thermal environment. The CPU throughput is improved by up to 7.6% (4.1%, on average) in a severe thermal environment.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2078073886",
    "type": "article"
  },
  {
    "title": "Feedback-driven binary code diversification",
    "doi": "https://doi.org/10.1145/2400682.2400683",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Bart Coppens; Bjorn De Sutter; Jonas Maebe",
    "corresponding_authors": "",
    "abstract": "As described in many blog posts and in the scientific literature, exploits for software vulnerabilities are often engineered on the basis of patches. For example, “Microsoft Patch Tuesday” is often followed by “Exploit Wednesday” during which yet unpatched systems become vulnerable to patch-based exploits. Part of the patch engineering includes the identification of the vulnerable binary code by means of reverse-engineering tools and diffing add-ons. In this article we present a feedback-driven compiler tool flow that iteratively transforms code until diffing tools become ineffective enough to close the “Exploit Wednesday” window of opportunity. We demonstrate the tool's effectiveness on a set of real-world patches and against the latest version of BinDiff.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W1999346693",
    "type": "article"
  },
  {
    "title": "Improving performance of nested loops on reconfigurable array processors",
    "doi": "https://doi.org/10.1145/2086696.2086711",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Yong‐Joo Kim; Jongeun Lee; X. Toan; Yunheung Paek",
    "corresponding_authors": "",
    "abstract": "Pipelining algorithms are typically concerned with improving only the steady-state performance, or the kernel time. The pipeline setup time happens only once and therefore can be negligible compared to the kernel time. However, for Coarse-Grained Reconfigurable Architectures (CGRAs) used as a coprocessor to a main processor, pipeline setup can take much longer due to the communication delay between the two processors, and can become significant if it is repeated in an outer loop of a loop nest. In this paper we evaluate the overhead of such non-kernel execution times when mapping nested loops for CGRAs, and propose a novel architecture-compiler cooperative scheme to reduce the overhead, while also minimizing the number of extra configurations required. Our experimental results using loops from multimedia and scientific domains demonstrate that our proposed techniques can greatly increase the performance of nested loops by up to 2.87 times compared to the conventional approach of accelerating only the innermost loops. Moreover, the mappings generated by our techniques require only a modest number of configurations that can fit in recent reconfigurable architectures.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W2018348175",
    "type": "article"
  },
  {
    "title": "Tile size selection revisited",
    "doi": "https://doi.org/10.1145/2541228.2555292",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Sanyam Mehta; Gautham Beeraka; Pen-Chung Yew",
    "corresponding_authors": "",
    "abstract": "Loop tiling is a widely used loop transformation to enhance data locality and allow data reuse. In the tiled code, however, tiles of different sizes can lead to significant variation in performance. Thus, selection of an optimal tile size is critical to performance of tiled codes. In the past, tile size selection has been attempted using both static analytical and dynamic empirical (auto-tuning) models. Past work using static models assumed a direct-mapped cache for the purpose of analysis and thus proved to be less robust. On the other hand, the auto-tuning models involve an exhaustive search in a large space of tiled codes. In this article, we propose a new analytical model for tile size selection that leverages the high set associativity in modern caches to minimize conflict misses. Our tile size selection model targets data reuse in multiple levels of cache. In addition, it considers the interaction of tiling with the SIMD unit in modern processors in estimating the optimal tile size. We find that these factors, not considered in previous models, are critical in developing a robust model for tile size selection. We implement our tile size selection model in a polyhedral compiler and test it on 12 benchmark kernels using two different problem sizes. Our model outperforms the previous analytical models that are based on reusing data in a single level of cache and achieves an average performance improvement of 9.7% and 20.4%, respectively, over the best square (cubic) tiles for the two problem sizes. In addition, the tile size chosen by our tile size selection algorithm is similar to the best performing size obtained through an extensive search, validating the analytical model underlying the algorithm.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W2025093669",
    "type": "article"
  },
  {
    "title": "Per-thread cycle accounting in multicore processors",
    "doi": "https://doi.org/10.1145/2400682.2400688",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Kristof Du Bois; Stijn Eyerman; Lieven Eeckhout",
    "corresponding_authors": "",
    "abstract": "While multicore processors improve overall chip throughput and hardware utilization, resource sharing among the cores leads to unpredictable performance for the individual threads running on a multicore processor. Unpredictable per-thread performance becomes a problem when considered in the context of multicore scheduling: system software assumes that all threads make equal progress, however, this is not what the hardware provides. This may lead to problems at the system level such as missed deadlines, reduced quality-of-service, non-satisfied service-level agreements, unbalanced parallel performance, priority inversion, unpredictable interactive performance, etc. This article proposes a hardware-efficient per-thread cycle accounting architecture for multicore processors. The counter architecture tracks per-thread progress in a multicore processor, detects how inter-thread interference affects per-thread performance, and predicts the execution time for each thread if run in isolation. The counter architecture captures the effects of additional conflict misses due to cache sharing as well as increased latency for other memory accesses due to resource and bandwidth contention in the memory subsystem. The proposed method accounts for 74.3% of the interference cycles, and estimates per-thread progress within 14.2% on average across a large set of multi-program workloads. Hardware cost is limited to 7.44KB for an 8-core processor, a reduction by almost 10× compared to prior work while being 63.8% more accurate. Making system software progress aware improves fairness by 22.5% on average over progress-agnostic scheduling.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2154198528",
    "type": "article"
  },
  {
    "title": "Refresh pausing in DRAM memory systems",
    "doi": "https://doi.org/10.1145/2579669",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "Prashant J. Nair; Chiachen Chou; Moinuddin K. Qureshi",
    "corresponding_authors": "",
    "abstract": "Dynamic Random Access Memory (DRAM) cells rely on periodic refresh operations to maintain data integrity. As the capacity of DRAM memories has increased, so has the amount of time consumed in doing refresh. Refresh operations contend with read operations, which increases read latency and reduces system performance. We show that eliminating latency penalty due to refresh can improve average performance by 7.2%. However, simply doing intelligent scheduling of refresh operations is ineffective at obtaining significant performance improvement. This article provides an alternative and scalable option to reduce the latency penalty due to refresh. It exploits the property that each refresh operation in a typical DRAM device internally refreshes multiple DRAM rows in JEDEC-based distributed refresh mode. Therefore, a refresh operation has well-defined points at which it can potentially be Paused to service a pending read request. Leveraging this property, we propose Refresh Pausing , a solution that is highly effective at alleviating the contention from refresh operations. It provides an average performance improvement of 5.1% for 8Gb devices and becomes even more effective for future high-density technologies. We also show that Refresh Pausing significantly outperforms the recently proposed Elastic Refresh scheme.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2046826852",
    "type": "article"
  },
  {
    "title": "MP-Tomasulo",
    "doi": "https://doi.org/10.1145/2459316.2459320",
    "publication_date": "2013-05-01",
    "publication_year": 2013,
    "authors": "Chao Wang; Xi Li; Junneng Zhang; Xuehai Zhou; Xiaoning Nie",
    "corresponding_authors": "",
    "abstract": "This article presents MP-Tomasulo, a dependency-aware automatic parallel task execution engine for sequential programs. Applying the instruction-level Tomasulo algorithm to MPSoC environments, MP-Tomasulo detects and eliminates Write-After-Write (WAW) and Write-After-Read (WAR) inter-task dependencies in the dataflow execution, therefore to operate out-of-order task execution on heterogeneous units. We implemented the prototype system within a single FPGA. Experimental results on EEMBC applications demonstrate that MP-Tomasulo can execute the tasks out-of-order to achieve as high as 93.6% to 97.6% of ideal peak speedup. A comparative study against a state-of-the-art dataflow execution scheme is illustrated with a classic JPEG application. The promising results show MP-Tomasulo enables programmers to uncover more task-level parallelism on heterogeneous systems, as well as to ease the burden of programmers.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2089572724",
    "type": "article"
  },
  {
    "title": "Automatic feature generation for machine learning--based optimising compilation",
    "doi": "https://doi.org/10.1145/2536688",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "Hugh Leather; Edwin V. Bonilla; Michael O’Boyle",
    "corresponding_authors": "",
    "abstract": "Recent work has shown that machine learning can automate and in some cases outperform handcrafted compiler optimisations. Central to such an approach is that machine learning techniques typically rely upon summaries or features of the program. The quality of these features is critical to the accuracy of the resulting machine learned algorithm; no machine learning method will work well with poorly chosen features. However, due to the size and complexity of programs, theoretically there are an infinite number of potential features to choose from. The compiler writer now has to expend effort in choosing the best features from this space. This article develops a novel mechanism to automatically find those features that most improve the quality of the machine learned heuristic. The feature space is described by a grammar and is then searched with genetic programming and predictive modelling. We apply this technique to loop unrolling in GCC 4.3.1 and evaluate our approach on a Pentium 6. On a benchmark suite of 57 programs, GCCs hard-coded heuristic achieves only 3% of the maximum performance available, whereas a state-of-the-art machine learning approach with hand-coded features obtains 59%. Our feature generation technique is able to achieve 76% of the maximum available speedup, outperforming existing approaches.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2101807486",
    "type": "article"
  },
  {
    "title": "Optimizing explicit data transfers for data parallel applications on the cell architecture",
    "doi": "https://doi.org/10.1145/2086696.2086716",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Selma Saidi; Pranav Tendulkar; Thierry Lepley; Oded Maler",
    "corresponding_authors": "",
    "abstract": "In this paper we investigate a general approach to automate some deployment decisions for a certain class of applications on multi-core computers. We consider data-parallelizable programs that use the well-known double buffering technique to bring the data from the off-chip slow memory to the local memory of the cores via a DMA (direct memory access) mechanism. Based on the computation time and size of elementary data items as well as DMA characteristics, we derive optimal and near optimal values for the number of blocks that should be clustered in a single DMA command. We then extend the results to the case where a computation for one data item needs some data in its neighborhood. In this setting we characterize the performance of several alternative mechanisms for data sharing. Our models are validated experimentally using a cycle-accurate simulator of the Cell Broadband Engine architecture.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2020322423",
    "type": "article"
  },
  {
    "title": "The CRNS framework and its application to programmable and reconfigurable cryptography",
    "doi": "https://doi.org/10.1145/2400682.2400692",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Samuel Antão; Leonel Sousa",
    "corresponding_authors": "",
    "abstract": "This article proposes the Computing with the ResidueNumber System (CRNS) framework, which aims at the design automation of accelerators for Modular Arithmetic (MA). The framework provides a comprehensive set of tools ranging from a programming language and respective compiler to back-ends targeting parallel computation platforms such as Graphical Processing Units (GPUs) and reconfigurable hardware. Given an input algorithm described with a high-level programming language, the CRNS can be used to obtain in a few seconds the corresponding optimized Parallel Thread Execution (PTX) program ready to be run on GPUs or the Hardware Description Language (HDL) specification of a fully functional accelerator suitable for reconfigurable hardware and embedded systems. The resulting framework's implementations benefit from the Residue Number System (RNS) arithmetic's parallelization properties in a fully automated way. Designers do not need to be familiar with the mathematical details concerning the employed arithmetic, namely the RNS representation. In order to thoroughly describe and evaluate the proposed framework, experimental results obtained for the supported back-ends (GPU and HDL) are presented targeting the implementation of the modular exponentiation used in the Rivest-Shamir-Adleman (RSA) algorithm and Elliptic Curve (EC) point multiplication. Results suggest competitive latency and throughput with minimum design effort and overcoming all the development issues that arise in the specification and verification of dedicated solutions.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2098282500",
    "type": "article"
  },
  {
    "title": "Boosting timestamp-based transactional memory by exploiting hardware cycle counters",
    "doi": "https://doi.org/10.1145/2541228.2555297",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Wenjia Ruan; Yujie Liu; Michael Spear",
    "corresponding_authors": "",
    "abstract": "Time-based transactional memories typically rely on a shared memory counter to ensure consistency. Unfortunately, such a counter can become a bottleneck. In this article, we identify properties of hardware cycle counters that allow their use in place of a shared memory counter. We then devise algorithms that exploit the x86 cycle counter to enable bottleneck-free transactional memory runtime systems. We also consider the impact of privatization safety and hardware ordering constraints on the correctness, performance, and generality of our algorithms.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2159587956",
    "type": "article"
  },
  {
    "title": "CompEx++",
    "doi": "https://doi.org/10.1145/3050440",
    "publication_date": "2017-03-31",
    "publication_year": 2017,
    "authors": "Poovaiah M. Palangappa; Kartik Mohanram",
    "corresponding_authors": "",
    "abstract": "Multilevel/triple-level cell nonvolatile memories (MLC/TLC NVMs) such as phase-change memory (PCM) and resistive RAM (RRAM) are the subject of active research and development as replacement candidates for DRAM, which is limited by its high refresh power and poor scaling potential. In addition to the benefits of nonvolatility (low refresh power) and improved scalability, MLC/TLC NVMs offer high data density and memory capacity over DRAM. However, the viability of MLC/TLC NVMs is limited primarily due to the high programming energy and latency as well as the low endurance of NVM cells; these are primarily attributed to the iterative program-and-verify procedure necessary for programming the NVM cells. This article proposes compression-expansion (CompEx) coding, a low overhead scheme that synergistically integrates pattern-based compression with expansion coding to realize simultaneous energy, latency, and lifetime improvements in MLC/TLC NVMs. CompEx coding is agnostic to the choice of compression technique; in this work, we evaluate CompEx coding using both frequent pattern compression (FPC) and base-delta-immediate (BΔI) compression. CompEx coding integrates FPC/BΔI with ( k , m ) q “expansion” coding; expansion codes are a class of q -ary linear block codes that encode data using only the low energy states of a q -ary NVM cell. CompEx coding simultaneously reduces energy and latency and improves lifetime for negligible-to-no memory overhead and negligible logic overhead (≈ 10k gates, which is &lt;0.1% per NVM module). Furthermore, we also propose CompEx++ coding, which extends CompEx coding by leveraging the variable compressibility of pattern-based compression techniques. CompEx++ coding integrates custom expansion codes to each of the compression patterns to exploit maximum energy/latency benefits of CompEx coding. Our full-system simulations using TLC RRAM show that CompEx/CompEx++ coding reduces total memory energy by 57%/61% and write latency by 23.5%/26%; these improvements translate to a 5.7%/10.6% improvement in IPC, a 11.8%/19.9% improvement in main memory bandwidth, and 1.8 × improvement in lifetime over classical binary coding using data-comparison write. CompEx/CompEx++ coding thus addresses the programming energy/latency and lifetime challenges of MLC/TLC NVMs that pose a serious technological roadblock to their adoption in high-performance computing systems.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2606951540",
    "type": "article"
  },
  {
    "title": "CACF",
    "doi": "https://doi.org/10.1145/3195799",
    "publication_date": "2018-05-23",
    "publication_year": 2018,
    "authors": "Yang Zhang; Dan Feng; Wei Tong; Yu Hua; Jingning Liu; Zhipeng Tan; Chengning Wang; Bing Wu; Zheng Li; Gaoxiang Xu",
    "corresponding_authors": "",
    "abstract": "Emerging Resistive Random Access Memory (ReRAM) is a promising candidate as the replacement for DRAM due to its low standby power, high density, high scalability, and nonvolatility. By employing the unique crossbar structure, ReRAM can be constructed with extremely high density. However, the crossbar ReRAM faces some serious challenges in terms of performance, reliability, and energy consumption. First, ReRAM’s crossbar structure causes an IR drop problem due to wire resistance and sneak currents, which results in nonuniform access latency in ReRAM banks and reduces its reliability. Second, without access transistors in the crossbar structure, write disturbance results in serious data reliability problem. Third, the access latency, reliability, and energy use of ReRAM arrays are significantly influenced by the data patterns involved in a write operation. To overcome the challenges of the crossbar ReRAM, we propose a novel circuit architecture co-optimization framework for improving the performance, reliability, and energy use of ReRAM-based main memory system, called CACF. The proposed CACF consists of three levels, including the circuit level, circuit architecture level, and architecture level. At the circuit level, to reduce the IR drops along bitlines, we propose a double-sided write driver design by applying write drivers along both sides of bitlines and selectively activating the write drivers. At the circuit architecture level, to address the write disturbance with low overheads, we propose a RESET disturbance detection scheme by adding disturbance reference cells and conditionally performing refresh operations. At the architecture level, a region partition with address remapping method is proposed to leverage the nonuniform access latency in ReRAM banks, and two flip schemes are proposed in different regions to optimize the data patterns involved in a write operation. The experimental results show that CACF improves system performance by 26.1%, decreases memory access latency by 22.4%, shortens running time by 20.1%, and reduces energy consumption by 21.6% on average over an aggressive baseline. Meanwhile, CACF significantly improves the reliability of ReRAM-based memory systems.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2803273246",
    "type": "article"
  },
  {
    "title": "Buri",
    "doi": "https://doi.org/10.1145/2808233",
    "publication_date": "2015-10-06",
    "publication_year": 2015,
    "authors": "Jishen Zhao; Sheng Li; Jichuan Chang; John Byrne; Laura Ramírez; Kevin Lim; Yuan Xie; Paolo Faraboschi",
    "corresponding_authors": "",
    "abstract": "Motivated by the challenges of scaling up memory capacity and fully exploiting the benefits of memory compression, we propose Buri, a hardware-based memory compression scheme, which simultaneously achieves cost efficiency, high performance, and ease of adoption. Buri combines (1) a self-contained, ready-to-adopt hardware compression module, which manages metadata compression and memory allocation/relocation operations; (2) a set of hardware optimization mechanisms, which reduce the area and performance overheads in accommodating the address indirection required by memory compression; and (3) lightweight BIOS/OS extensions used to handle exceptions. Our evaluation with large memory workload traces shows that Buri can increase capacity by 70%, in addition to the compression ratio already provided by database software.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2053425642",
    "type": "article"
  },
  {
    "title": "Optimizing software runtime systems for speculative parallelization",
    "doi": "https://doi.org/10.1145/2400682.2400698",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Paraskevas Yiapanis; Demian Rosas-Ham; Gavin Brown; Mikel Luján",
    "corresponding_authors": "",
    "abstract": "Thread-Level Speculation (TLS) overcomes limitations intrinsic with conservative compile-time auto-parallelizing tools by extracting parallel threads optimistically and only ensuring absence of data dependence violations at runtime. A significant barrier for adopting TLS (implemented in software) is the overheads associated with maintaining speculative state. Based on previous TLS limit studies, we observe that on future multicore systems we will likely have more cores idle than those which traditional TLS would be able to harness. This implies that a TLS system should focus on optimizing for small number of cores and find efficient ways to take advantage of the idle cores. Furthermore, research on optimistic systems has covered two important implementation design points: eager vs. lazy version management. With this knowledge, we propose new simple and effective techniques to reduce the execution time overheads for both of these design points. This article describes a novel compact version management data structure optimized for space overhead when using a small number of TLS threads. Furthermore, we describe two novel software runtime parallelization systems that utilize this compact data structure. The first software TLS system, MiniTLS, relies on eager memory data management (in-place updates) and, thus, when a misspeculation occurs a rollback process is required. MiniTLS takes advantage of the novel compact version management representation to parallelize the rollback process and is able to recover from misspeculation faster than existing software eager TLS systems. The second one, Lector (Lazy inspECTOR) is based on lazy version management. Since we have idle cores, the question is whether we can create “helper” tasks to determine whether speculation is actually needed without stopping or damaging the speculative execution. In Lector, for each conventional TLS thread running speculatively with lazy version management, there is associated with it a lightweight inspector . The inspector threads execute alongside to verify quickly whether data dependencies will occur. Inspector threads are generated by standard techniques for inspector/executor parallelization. We have applied both TLS systems to seven Java sequential benchmarks, including three benchmarks from SPECjvm2008. Two out of the seven benchmarks exhibit misspeculations. MiniTLS experiments report average speedups of 1.8x for 4 threads increasing close to 7x speedups with 32 threads. Facilitated by our novel compact representation, MiniTLS reduces the space overhead over state-of-the-art software TLS systems between 96% on 2 threads and 40% on 32 threads. The experiments for Lector, report average speedups of 1.7x for 2 threads (that is 1 TLS + 1 Inspector threads) increasing close to 8.2x speedups with 32 threads (16 + 16 threads). Compared to a well established software TLS baseline, Lector performs on average 1.7x faster for 32 threads and in no case ( x TLS + x Inspector threads) Lector delivers worse performance than the baseline TLS with the equivalent number of TLS threads (i.e. x TLS threads) nor doubling the equivalent number of TLS threads (i.e., x + x TLS threads).",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2093145075",
    "type": "article"
  },
  {
    "title": "A system architecture, processor, and communication protocol for secure implants",
    "doi": "https://doi.org/10.1145/2541228.2555313",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Christos Strydis; Robert M. Seepers; Pedro Peris-López; Dimitrios Siskos; Ioannis Sourdis",
    "corresponding_authors": "",
    "abstract": "Secure and energy-efficient communication between Implantable Medical Devices (IMDs) and authorized external users is attracting increasing attention these days. However, there currently exists no systematic approach to the problem, while solutions from neighboring fields, such as wireless sensor networks, are not directly transferable due to the peculiarities of the IMD domain. This work describes an original, efficient solution for secure IMD communication. A new implant system architecture is proposed, where security and main-implant functionality are made completely decoupled by running the tasks onto two separate cores. Wireless communication goes through a custom security ASIP, called SISC (Smart-Implant Security Core), which runs an energy-efficient security protocol. The security core is powered by RF-harvested energy until it performs external-reader authentication, providing an elegant defense mechanism against battery Denial-of-Service (DoS) and other, more common attacks. The system has been evaluated based on a realistic case study involving an artificial pancreas implant. When synthesized for a UMC 90nm CMOS ASIC technology, our system architecture achieves defense against unauthorized accesses having zero energy cost , running entity authentication through harvesting only 7.45μ J of RF energy from the requesting entity. In all other successfully authenticated accesses, our architecture achieves secure data exchange without affecting the performance of the main IMD functionality, adding less than 1‰ (1.3 mJ ) to the daily energy consumption of a typical implant. Compared to a singe-core, secure reference IMD, which would still be more vulnerable to some types of attacks, our secure system on chip (SoC) achieves high security levels at 56% energy savings and at an area overhead of less than 15%.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2102753983",
    "type": "article"
  },
  {
    "title": "MAMBO",
    "doi": "https://doi.org/10.1145/2896451",
    "publication_date": "2016-04-05",
    "publication_year": 2016,
    "authors": "Cosmin Gorgovan; Amanieu d'Antras; Mikel Luján",
    "corresponding_authors": "",
    "abstract": "As the ARM architecture expands beyond its traditional embedded domain, there is a growing interest in dynamic binary modification (DBM) tools for general-purpose multicore processors that are part of the ARM family. Existing DBM tools for ARM suffer from introducing large overheads in the execution of applications. The specific questions that this article addresses are (i) how to develop such DBM tools for the ARM architecture and (ii) whether new optimisations are plausible and needed. We describe the general design of MAMBO, a new DBM tool for ARM, which we release together with this publication, and introduce novel optimisations to handle indirect branches. In addition, we explore scenarios in which it may be possible to relax the transparency offered by DBM tools to allow extra optimisations to be applied. These scenarios arise from analysing the most typical usages: for example, application binaries without handcrafted assembly. The performance evaluation shows that MAMBO introduces small overheads for SPEC CPU2006 and PARSEC 3.0 when comparing with the execution times of the unmodified programs: a geometric mean overhead of 28% on a Cortex-A9 and of 34% on a Cortex-A15 for CPU2006, and between 27% and 32%, depending on the number of threads, for PARSEC on a Cortex-A15.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2323164027",
    "type": "article"
  },
  {
    "title": "Cooperative Caching for GPUs",
    "doi": "https://doi.org/10.1145/3001589",
    "publication_date": "2016-12-12",
    "publication_year": 2016,
    "authors": "Saumay Dublish; Vijay Nagarajan; Nigel Topham",
    "corresponding_authors": "",
    "abstract": "The rise of general-purpose computing on GPUs has influenced architectural innovation on them. The introduction of an on-chip cache hierarchy is one such innovation. High L1 miss rates on GPUs, however, indicate inefficient cache usage due to myriad factors, such as cache thrashing and extensive multithreading. Such high L1 miss rates in turn place high demands on the shared L2 bandwidth. Extensive congestion in the L2 access path therefore results in high memory access latencies. In memory-intensive applications, these latencies get exposed due to a lack of active compute threads to mask such high latencies. In this article, we aim to reduce the pressure on the shared L2 bandwidth, thereby reducing the memory access latencies that lie in the critical path. We identify significant replication of data among private L1 caches, presenting an opportunity to reuse data among L1s. We further show how this reuse can be exploited via an L1 Cooperative Caching Network (CCN), thereby reducing the bandwidth demand on L2. In the proposed architecture, we connect the L1 caches with a lightweight ring network to facilitate intercore communication of shared data. We show that this technique reduces traffic to the L2 cache by an average of 29%, freeing up the bandwidth for other accesses. We also show that the CCN reduces the average memory latency by 24%, thereby reducing core stall cycles by 26% on average. This translates into an overall performance improvement of 14.7% on average (and up to 49%) for applications that exhibit reuse across L1 caches. In doing so, the CCN incurs a nominal area and energy overhead of 1.3% and 2.5%, respectively. Notably, the performance improvement with our proposed CCN compares favorably to the performance improvement achieved by simply doubling the number of L2 banks by up to 34%.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2566040696",
    "type": "article"
  },
  {
    "title": "Bringing Parallel Patterns Out of the Corner",
    "doi": "https://doi.org/10.1145/3132710",
    "publication_date": "2017-10-24",
    "publication_year": 2017,
    "authors": "Daniele De Sensi; Tiziano De Matteis; Massimo Torquati; Gabriele Mencagli; Marco Danelutto",
    "corresponding_authors": "",
    "abstract": "High-level parallel programming is an active research topic aimed at promoting parallel programming methodologies that provide the programmer with high-level abstractions to develop complex parallel software with reduced time to solution. Pattern-based parallel programming is based on a set of composable and customizable parallel patterns used as basic building blocks in parallel applications. In recent years, a considerable effort has been made in empowering this programming model with features able to overcome shortcomings of early approaches concerning flexibility and performance. In this article, we demonstrate that the approach is flexible and efficient enough by applying it on 12 out of 13 PARSEC applications. Our analysis, conducted on three different multicore architectures, demonstrates that pattern-based parallel programming has reached a good level of maturity, providing comparable results in terms of performance with respect to both other parallel programming methodologies based on pragma-based annotations (i.e., Open mp and O mp S s ) and native implementations (i.e., P threads ). Regarding the programming effort, we also demonstrate a considerable reduction in lines of code and code churn compared to P threads and comparable results with respect to other existing implementations.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2766111638",
    "type": "article"
  },
  {
    "title": "BestSF",
    "doi": "https://doi.org/10.1145/3226228",
    "publication_date": "2018-09-04",
    "publication_year": 2018,
    "authors": "Mohamed Akrem Benatia; Weixing Ji; Yizhuo Wang; Feng Shi",
    "corresponding_authors": "",
    "abstract": "The Sparse Matrix-Vector Multiplication (SpMV) kernel dominates the computing cost in numerous scientific applications. Many implementations based on different sparse formats were proposed to improve this kernel on the recent GPU architectures. However, it has been widely observed that there is no “best-for-all” sparse format for the SpMV kernel on GPU. Indeed, serious performance degradation of an order of magnitude can be observed without a careful selection of the sparse format to use. To address this problem, we propose in this article BestSF (Best Sparse Format), a new learning-based sparse meta-format that automatically selects the most appropriate sparse format for a given input matrix. To do so, BestSF relies on a cost-sensitive classification system trained using Weighted Support Vector Machines (WSVMs) to predict the best sparse format for each input sparse matrix. Our experimental results on two different NVIDIA GPU architectures using a large number of real-world sparse matrices show that BestSF achieved a noticeable overall performance improvement over using a single sparse format. While BestSF is trained to select the best sparse format in terms of performance (GFLOPS), our further experimental investigations revealed that using BestSF also led, in most of the test cases, to the best energy efficiency (MFLOPS/W). To prove its practical effectiveness, we also evaluate the performance and energy efficiency improvement achieved when using BestSF as a building block in a GPU-based Preconditioned Conjugate Gradient (PCG) iterative solver.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2891818448",
    "type": "article"
  },
  {
    "title": "Layer-Centric Memory Reuse and Data Migration for Extreme-Scale Deep Learning on Many-Core Architectures",
    "doi": "https://doi.org/10.1145/3243904",
    "publication_date": "2018-09-17",
    "publication_year": 2018,
    "authors": "Hai Jin; Bo Liu; Wenbin Jiang; Yang Ma; Xuanhua Shi; Bingsheng He; Shaofeng Zhao",
    "corresponding_authors": "",
    "abstract": "Due to the popularity of Deep Neural Network (DNN) models, we have witnessed extreme-scale DNN models with the continued increase of the scale in terms of depth and width. However, the extremely high memory requirements for them make it difficult to run the training processes on single many-core architectures such as a Graphic Processing Unit (GPU), which compels researchers to use model parallelism over multiple GPUs to make it work. However, model parallelism always brings very heavy additional overhead. Therefore, running an extreme-scale model in a single GPU is urgently required. There still exist several challenges to reduce the memory footprint for extreme-scale deep learning. To address this tough problem, we first identify the memory usage characteristics for deep and wide convolutional networks, and demonstrate the opportunities for memory reuse at both the intra-layer and inter-layer levels. We then present Layrub, a runtime data placement strategy that orchestrates the execution of the training process. It achieves layer-centric reuse to reduce memory consumption for extreme-scale deep learning that could not previously be run on a single GPU. Experiments show that, compared to the original Caffe, Layrub can cut down the memory usage rate by an average of 58.2% and by up to 98.9%, at the moderate cost of 24.1% higher training execution time on average. Results also show that Layrub outperforms some popular deep learning systems such as GeePS, vDNN, MXNet, and Tensorflow. More importantly, Layrub can tackle extreme-scale deep learning tasks. For example, it makes an extra-deep ResNet with 1,517 layers that can be trained successfully in one GPU with 12GB memory, while other existing deep learning systems cannot.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2891993230",
    "type": "article"
  },
  {
    "title": "Supporting Superpages and Lightweight Page Migration in Hybrid Memory Systems",
    "doi": "https://doi.org/10.1145/3310133",
    "publication_date": "2019-04-09",
    "publication_year": 2019,
    "authors": "Xiaoyuan Wang; Haikun Liu; Xiaofei Liao; Ji Chen; Hai Jin; Yu Zhang; Long Zheng; Bingsheng He; Song Jiang",
    "corresponding_authors": "",
    "abstract": "Superpages have long been used to mitigate address translation overhead in large-memory systems. However, superpages often preclude lightweight page migration, which is crucial for performance and energy efficiency in hybrid memory systems composed of DRAM and non-volatile memory (NVM). In this article, we propose a novel memory management mechanism called Rainbow to bridge this fundamental conflict between superpages and lightweight page migration. Rainbow manages NVM at the superpage granularity, and uses DRAM to cache frequently accessed (hot) small pages within each superpage. Correspondingly, Rainbow utilizes split TLBs to support different page sizes. By introducing an efficient hot page identification mechanism and a novel NVM-to-DRAM address remapping mechanism, Rainbow supports lightweight page migration without splintering superpages. Experiment results show that Rainbow can significantly reduce applications’ TLB misses by 99.9%, and improve application performance (in terms of IPC) by up to 2.9× (45.3% on average) when compared to a state-of-the-art memory migration policy without a superpage support.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2963521199",
    "type": "article"
  },
  {
    "title": "Side-channel Timing Attack of RSA on a GPU",
    "doi": "https://doi.org/10.1145/3341729",
    "publication_date": "2019-08-13",
    "publication_year": 2019,
    "authors": "Chao Luo; Yunsi Fei; David Kaeli",
    "corresponding_authors": "",
    "abstract": "To increase computation throughput, general purpose Graphics Processing Units (GPUs) have been leveraged to accelerate computationally intensive workloads. GPUs have been used as cryptographic engines, improving encryption/decryption throughput and leveraging the GPU’s Single Instruction Multiple Thread (SIMT) model. RSA is a widely used public-key cipher and has been ported onto GPUs for signing and decrypting large files. Although performance has been significantly improved, the security of RSA on GPUs is vulnerable to side-channel timing attacks and is an exposure overlooked in previous studies. GPUs tend to be naturally resilient to side-channel attacks, given that they execute a large number of concurrent threads, performing many RSA operations on different data in parallel. Given the degree of parallel execution on a GPU, there will be a significant amount of noise introduced into the timing channel given the thousands of concurrent threads executing concurrently. In this work, we build a timing model to capture the parallel characteristics of an RSA public-key cipher implemented on a GPU. We consider optimizations that include using Montgomery multiplication and sliding-window exponentiation to implement cryptographic operations. Our timing model considers the challenges of parallel execution, complications that do not occur in single-threaded computing platforms. Based on our timing model, we launch successful timing attacks on RSA running on a GPU, extracting the private key of RSA. We also present an effective error detection and correction mechanism. Our results demonstrate that GPU acceleration of RSA is vulnerable to side-channel timing attacks. We propose several countermeasures to defend against this class of attacks.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2968183237",
    "type": "article"
  },
  {
    "title": "Energy Transparency for Deeply Embedded Programs",
    "doi": "https://doi.org/10.1145/3046679",
    "publication_date": "2017-03-21",
    "publication_year": 2017,
    "authors": "Kyriakos Georgiou; Steve Kerrison; Zbigniew Chamski; Kerstin Eder",
    "corresponding_authors": "",
    "abstract": "Energy transparency is a concept that makes a program’s energy consumption visible, from hardware up to software, through the different system layers. Such transparency can enable energy optimizations at each layer and between layers, as well as help both programmers and operating systems make energy-aware decisions. In this article, we focus on deeply embedded devices, typically used for Internet of Things (IoT) applications, and demonstrate how to enable energy transparency through existing static resource analysis (SRA) techniques and a new target-agnostic profiling technique, without hardware energy measurements. Our novel mapping technique enables software energy consumption estimations at a higher level than the Instruction Set Architecture (ISA), namely the LLVM intermediate representation (IR) level, and therefore introduces energy transparency directly to the LLVM optimizer. We apply our energy estimation techniques to a comprehensive set of benchmarks, including single- and multithreaded embedded programs from two commonly used concurrency patterns: task farms and pipelines. Using SRA, our LLVM IR results demonstrate a high accuracy with a deviation in the range of 1% from the ISA SRA. Our profiling technique captures the actual energy consumption at the LLVM IR level with an average error of 3%.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2520653324",
    "type": "article"
  },
  {
    "title": "LLOV",
    "doi": "https://doi.org/10.1145/3418597",
    "publication_date": "2020-12-22",
    "publication_year": 2020,
    "authors": "Utpal Bora; Santanu Das; Pankaj Kukreja; Saurabh Joshi; Ramakrishna Upadrasta; Sanjay Rajopadhye",
    "corresponding_authors": "",
    "abstract": "In the era of Exascale computing, writing efficient parallel programs is indispensable, and, at the same time, writing sound parallel programs is very difficult. Specifying parallelism with frameworks such as OpenMP is relatively easy, but data races in these programs are an important source of bugs. In this article, we propose LLOV, a fast, lightweight, language agnostic, and static data race checker for OpenMP programs based on the LLVM compiler framework. We compare LLOV with other state-of-the-art data race checkers on a variety of well-established benchmarks. We show that the precision, accuracy, and the F1 score of LLOV is comparable to other checkers while being orders of magnitude faster. To the best of our knowledge, LLOV is the only tool among the state-of-the-art data race checkers that can verify a C/C++ or FORTRAN program to be data race free.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W3116800730",
    "type": "article"
  },
  {
    "title": "A Simple Model for Portable and Fast Prediction of Execution Time and Power Consumption of GPU Kernels",
    "doi": "https://doi.org/10.1145/3431731",
    "publication_date": "2020-12-30",
    "publication_year": 2020,
    "authors": "Lorenz Braun; Sotirios Nikas; Chen Song; Vincent Heuveline; Holger Fröning",
    "corresponding_authors": "",
    "abstract": "Characterizing compute kernel execution behavior on GPUs for efficient task scheduling is a non-trivial task. We address this with a simple model enabling portable and fast predictions among different GPUs using only hardware-independent features. This model is built based on random forests using 189 individual compute kernels from benchmarks such as Parboil, Rodinia, Polybench-GPU, and SHOC. Evaluation of the model performance using cross-validation yields a median Mean Average Percentage Error (MAPE) of 8.86–52.0% for time and 1.84–2.94% for power prediction across five different GPUs, while latency for a single prediction varies between 15 and 108 ms.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W3115175310",
    "type": "article"
  },
  {
    "title": "Exploiting Parallelism Opportunities with Deep Learning Frameworks",
    "doi": "https://doi.org/10.1145/3431388",
    "publication_date": "2020-12-30",
    "publication_year": 2020,
    "authors": "Yu Emma Wang; Carole-Jean Wu; Xiaodong Wang; Kim Hazelwood; David Brooks",
    "corresponding_authors": "",
    "abstract": "State-of-the-art machine learning frameworks support a wide variety of design features to enable a flexible machine learning programming interface and to ease the programmability burden on machine learning developers. Identifying and using a performance-optimal setting in feature-rich frameworks, however, involves a non-trivial amount of performance profiling efforts and often relies on domain-specific knowledge. This article takes a deep dive into analyzing the performance impact of key design features in a machine learning framework and quantifies the role of parallelism. The observations and insights distill into a simple set of guidelines that one can use to achieve much higher training and inference speedup. Across a diverse set of real-world deep learning models, the evaluation results show that the proposed performance tuning guidelines outperform the Intel and TensorFlow recommended settings by 1.30× and 1.38×, respectively.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W3115348505",
    "type": "article"
  },
  {
    "title": "Architecting Optically Controlled Phase Change Memory",
    "doi": "https://doi.org/10.1145/3533252",
    "publication_date": "2022-10-28",
    "publication_year": 2022,
    "authors": "Aditya Narayan; Yvain Thonnart; Pascal Vivet; Ayse K. Coskun; Ajay Joshi",
    "corresponding_authors": "",
    "abstract": "Phase Change Memory (PCM) is an attractive candidate for main memory, as it offers non-volatility and zero leakage power while providing higher cell densities, longer data retention time, and higher capacity scaling compared to DRAM. In PCM, data is stored in the crystalline or amorphous state of the phase change material. The typical electrically controlled PCM (EPCM), however, suffers from longer write latency and higher write energy compared to DRAM and limited multi-level cell (MLC) capacities. These challenges limit the performance of data-intensive applications running on computing systems with EPCMs. Recently, researchers demonstrated optically controlled PCM (OPCM) cells with support for 5 bits / cell in contrast to 2 bits / cell in EPCM. These OPCM cells can be accessed directly with optical signals that are multiplexed in high-bandwidth-density silicon-photonic links. The higher MLC capacity in OPCM and the direct cell access using optical signals enable an increased read/write throughput and lower energy per access than EPCM. However, due to the direct cell access using optical signals, OPCM systems cannot be designed using conventional memory architecture. We need a complete redesign of the memory architecture that is tailored to the properties of OPCM technology. This article presents the design of a unified network and main memory system called COSMOS that combines OPCM and silicon-photonic links to achieve high memory throughput. COSMOS is composed of a hierarchical multi-banked OPCM array with novel read and write access protocols. COSMOS uses an Electrical-Optical-Electrical (E-O-E) control unit to map standard DRAM read/write commands (sent in electrical domain) from the memory controller on to optical signals that access the OPCM cells. Our evaluation of a 2.5D-integrated system containing a processor and COSMOS demonstrates 2.14 × average speedup across graph and HPC workloads compared to an EPCM system. COSMOS consumes 3.8× lower read energy-per-bit and 5.97× lower write energy-per-bit compared to EPCM. COSMOS is the first non-volatile memory that provides comparable performance and energy consumption as DDR5 in addition to increased bit density, higher area efficiency, and improved scalability.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W3184059350",
    "type": "article"
  },
  {
    "title": "An Accelerator for Sparse Convolutional Neural Networks Leveraging Systolic General Matrix-matrix Multiplication",
    "doi": "https://doi.org/10.1145/3532863",
    "publication_date": "2022-04-25",
    "publication_year": 2022,
    "authors": "Mohammadreza Soltaniyeh; Richard P. Martin; Santosh Nagarakatte",
    "corresponding_authors": "",
    "abstract": "This article proposes a novel hardware accelerator for the inference task with sparse convolutional neural networks (CNNs) by building a hardware unit to perform Image to Column ( Im2Col ) transformation of the input feature map coupled with a systolic-array-based general matrix-matrix multiplication (GEMM) unit. Our design carefully overlaps the Im2Col transformation with the GEMM computation to maximize parallelism. We propose a novel design for the Im2Col unit that uses a set of distributed local memories connected by a ring network, which improves energy efficiency and latency by streaming the input feature map only once. The systolic-array-based GEMM unit in the accelerator can be dynamically configured as multiple GEMM units with square-shaped systolic arrays or as a single GEMM unit with a tall systolic array. This dynamic reconfigurability enables effective pipelining of Im2Col and GEMM operations and attains high processing element utilization for a wide range of CNNs. Further, our accelerator is sparsity aware, improving performance and energy efficiency by effectively mapping the sparse feature maps and weights to the processing elements, skipping ineffectual operations and unnecessary data movements involving zeros. Our prototype, SPOTS, is on average 2.16 \\( \\times \\) , 1.74 \\( \\times \\) , and 1.63 \\( \\times \\) faster than Gemmini, Eyeriss, and Sparse-PE, which are prior hardware accelerators for dense and sparse CNNs, respectively. SPOTS is also 78 \\( \\times \\) and 12 \\( \\times \\) more energy-efficient when compared to CPU and GPU implementations, respectively.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4224612674",
    "type": "article"
  },
  {
    "title": "A Fast and Flexible FPGA-based Accelerator for Natural Language Processing Neural Networks",
    "doi": "https://doi.org/10.1145/3564606",
    "publication_date": "2022-10-03",
    "publication_year": 2022,
    "authors": "Suyeon Hur; Seongmin Na; Dongup Kwon; Joonsung Kim; Andrew Boutros; Eriko Nurvitadhi; Jangwoo Kim",
    "corresponding_authors": "",
    "abstract": "Deep neural networks (DNNs) have become key solutions in the natural language processing (NLP) domain. However, the existing accelerators customized for their narrow target models cannot support diverse NLP models. Therefore, naively running complex NLP models on the existing accelerators often leads to very marginal performance improvements. For these reasons, architects are now in dire need of a new accelerator that can run various NLP models while taking its full performance potential. In this article, we propose FlexRun, an FPGA-based modular accelerator to efficiently support diverse and complex NLP models. First, we identify key components commonly used by NLP models and implement them on top of a current state-of-the-art FPGA-based accelerator. Next, FlexRun conducts an in-depth design space exploration to find the best accelerator architecture for a target NLP model. Last, FlexRun automatically reconfigures the accelerator based on the exploration results. Our FlexRun design outperforms the current state-of-the-art FPGA-based accelerator by 1.21×–2.73× and 1.15×–1.50× for BERT and GPT2, respectively. Compared to Nvidia’s V100 GPU, FlexRun achieves 2.69× higher performance on average for various BERT and GPT2 models.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4300865759",
    "type": "article"
  },
  {
    "title": "Autotuning Convolutions Is Easier Than You Think",
    "doi": "https://doi.org/10.1145/3570641",
    "publication_date": "2022-11-08",
    "publication_year": 2022,
    "authors": "Nicolas Tollenaere; Guillaume Iooss; Stéphane Pouget; Hugo Brunie; Christophe Guillon; Albert Cohen; P. Sadayappan; Fabrice Rastello",
    "corresponding_authors": "",
    "abstract": "A wide range of scientific and machine learning applications depend on highly optimized implementations of tensor computations. Exploiting the full capacity of a given processor architecture remains a challenging task, due to the complexity of the microarchitectural features that come into play when seeking near-peak performance. Among the state-of-the-art techniques for loop transformations for performance optimization, AutoScheduler [Zheng et al. 2020a ] tends to outperform other systems. It often yields higher performance as compared to vendor libraries, but takes a large number of runs to converge, while also involving a complex training environment. In this article, we define a structured configuration space that enables much faster convergence to high-performance code versions, using only random sampling of candidates. We focus on two-dimensional convolutions on CPUs. Compared to state-of-the-art libraries, our structured search space enables higher performance for typical tensor shapes encountered in convolution stages in deep learning pipelines. Compared to auto-tuning code generators like AutoScheduler, it prunes the search space while increasing the density of efficient implementations. We analyze the impact on convergence speed and performance distribution, on two Intel x86 processors and one ARM AArch64 processor. We match or outperform the performance of the state-of-the-art oneDNN library and TVM’s AutoScheduler, while reducing the autotuning effort by at least an order of magnitude.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4308596141",
    "type": "article"
  },
  {
    "title": "Scale-out Systolic Arrays",
    "doi": "https://doi.org/10.1145/3572917",
    "publication_date": "2022-11-29",
    "publication_year": 2022,
    "authors": "Ahmet Caner Yüzügüler; Canberk Sönmez; Mario Drumond; Yunho Oh; Babak Falsafi; Pascal Frossard",
    "corresponding_authors": "",
    "abstract": "Multi-pod systolic arrays are emerging as the architecture of choice in DNN inference accelerators. Despite their potential, designing multi-pod systolic arrays to maximize effective throughput/Watt—i.e., throughput/Watt adjusted when accounting for array utilization—poses a unique set of challenges. In this work, we study three key pillars in multi-pod systolic array designs, namely array granularity, interconnect, and tiling. We identify optimal array granularity across workloads and show that state-of-the-art commercial accelerators use suboptimal array sizes for single-tenancy workloads. We, then evaluate the bandwidth/latency trade-offs in interconnects and show that Butterfly networks offer a scalable topology for accelerators with a large number of pods. Finally, we introduce a novel data tiling scheme with custom partition size to maximize utilization in optimally sized pods. We propose Scale-out Systolic Arrays , a multi-pod inference accelerator for both single- and multi-tenancy based on these three pillars. We show that SOSA exhibits scaling of up to 600 TeraOps/s in effective throughput for state-of-the-art DNN inference workloads, and outperforms state-of-the-art multi-pod accelerators by a factor of 1.5 ×. 1",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4310362310",
    "type": "article"
  },
  {
    "title": "QuCloud+: A Holistic Qubit Mapping Scheme for Single/Multi-programming on 2D/3D NISQ Quantum Computers",
    "doi": "https://doi.org/10.1145/3631525",
    "publication_date": "2023-11-07",
    "publication_year": 2023,
    "authors": "Lei Liu; Xinglei Dou",
    "corresponding_authors": "",
    "abstract": "Qubit mapping for NISQ superconducting quantum computers is essential to fidelity and resource utilization. The existing qubit mapping schemes meet challenges, e.g., crosstalk, SWAP overheads, diverse device topologies, etc., leading to qubit resource underutilization and low fidelity in computing results. This article introduces QuCloud+, a new qubit mapping scheme that tackles these challenges. QuCloud+ has several new designs. (1) QuCloud+ supports single/multi-programming quantum computing on quantum chips with 2D/3D topology. (2) QuCloud+ partitions physical qubits for concurrent quantum programs with the crosstalk-aware community detection technique and further allocates qubits according to qubit degree, improving fidelity, and resource utilization. (3) QuCloud+ includes an X-SWAP mechanism that avoids SWAPs with high crosstalk errors and enables inter-program SWAPs to reduce the SWAP overheads. (4) QuCloud+ schedules concurrent quantum programs to be mapped and executed based on estimated fidelity for the best practice. Experimental results show that, compared with the existing typical multi-programming study [ 12 ], QuCloud+ achieves up to 9.03% higher fidelity and saves on the required SWAPs during mapping, reducing the number of CNOT gates inserted by 40.92%. Compared with a recent study [ 30 ] that enables post-mapping gate optimizations to further reduce gates, QuCloud+ reduces the post-mapping circuit depth by 21.91% while using a similar number of gates.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4388455106",
    "type": "article"
  },
  {
    "title": "Interaction cost and shotgun profiling",
    "doi": "https://doi.org/10.1145/1022969.1022971",
    "publication_date": "2004-09-01",
    "publication_year": 2004,
    "authors": "Brian A. Fields; Rastislav Bodík; Mark D. Hill; Chris J. Newburn",
    "corresponding_authors": "",
    "abstract": "We observe that the challenges software optimizers and microarchitects face every day boil down to a single problem: bottleneck analysis. A bottleneck is any event or resource that contributes to execution time, such as a critical cache miss or window stall. Tasks such as tuning processors for energy efficiency and finding the right loads to prefetch all require measuring the performance costs of bottlenecks.In the past, simple event counts were enough to find the important bottlenecks. Today, the parallelism of modern processors makes such analysis much more difficult, rendering traditional performance counters less useful. If two microarchitectural events (such as a fetch stall and a cache miss) occur in the same cycle, which event should we blame for the cycle? What cost should we assign to each event? In this paper, we introduce a new model for understanding event costs to facilitate processor design and optimization.First, we observe that all instructions, hardware structures, and events in a machine can interact in only one of two ways (in parallel or serially). We quantify these interactions by defining interaction cost , which can be zero (independent, no interaction), positive (parallel), or negative (serial).Second, we illustrate the value of using interaction costs in processor design and optimization. In a processor with a long pipeline, we show how to mitigate the negative performance effect of long latency \"critical\" loops, such as the level-one cache access and issue-wakeup, by optimizing seemingly unrelated resources that interact with them.Finally, we propose shotgun profiling , a class of hardware profiling infrastructures that are parallelism-aware, in contrast to traditional event counters. Our recommended design requires only modest extensions to current hardware counters, while enabling the construction of full-featured dependence graphs of the microexecution. With these dependence graphs, many types of analyses can be performed, including identifying critical instructions, finding slack, as well as computing costs and interaction costs.",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W1964191343",
    "type": "article"
  },
  {
    "title": "Managing bounded code caches in dynamic binary optimization systems",
    "doi": "https://doi.org/10.1145/1162690.1162692",
    "publication_date": "2006-09-01",
    "publication_year": 2006,
    "authors": "Kim Hazelwood; Michael D. Smith",
    "corresponding_authors": "",
    "abstract": "Dynamic binary optimizers store altered copies of original program instructions in software-managed code caches in order to maximize reuse of transformed code. Code caches store code blocks that may vary in size, reference other code blocks, and carry a high replacement overhead. These unique constraints reduce the effectiveness of conventional cache management policies. Our work directly addresses these unique constraints and presents several contributions to the code-cache management problem. First, we show that evicting more than the minimum number of code blocks from the code cache results in less run-time overhead than the existing alternatives. Such granular evictions reduce overall execution time, as the fixed costs of invoking the eviction mechanism are amortized across multiple cache insertions. Second, a study of the ideal lifetimes of dynamically generated code blocks illustrates the benefit of a replacement algorithm based on a generational heuristic. We describe and evaluate a generational approach to code cache management that makes it easy to identify long-lived code blocks and simultaneously avoid any fragmentation because of the eviction of short-lived blocks. Finally, we present results from an implementation of our generational approach in the DynamoRIO framework and illustrate that, as dynamic optimization systems become more prevalent, effective code cache-management policies will be essential for reliable, scalable performance of modern applications.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W2004571421",
    "type": "article"
  },
  {
    "title": "Performance scalability of decoupled software pipelining",
    "doi": "https://doi.org/10.1145/1400112.1400113",
    "publication_date": "2008-08-01",
    "publication_year": 2008,
    "authors": "Ram Rangan; Neil Vachharajani; Guilherme Ottoni; David I. August",
    "corresponding_authors": "",
    "abstract": "Any successful solution to using multicore processors to scale general-purpose program performance will have to contend with rising intercore communication costs while exposing coarse-grained parallelism. Recently proposed pipelined multithreading (PMT) techniques have been demonstrated to have general-purpose applicability and are also able to effectively tolerate inter-core latencies through pipelined interthread communication. These desirable properties make PMT techniques strong candidates for program parallelization on current and future multicore processors and understanding their performance characteristics is critical to their deployment. To that end, this paper evaluates the performance scalability of a general-purpose PMT technique called decoupled software pipelining (DSWP) and presents a thorough analysis of the communication bottlenecks that must be overcome for optimal DSWP scalability.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2094020960",
    "type": "article"
  },
  {
    "title": "MemTracker",
    "doi": "https://doi.org/10.1145/1543753.1543754",
    "publication_date": "2009-06-01",
    "publication_year": 2009,
    "authors": "Guru Venkataramani; Ioannis Doudalis; Yan Solihin; Milos Prvulović",
    "corresponding_authors": "",
    "abstract": "Memory bugs are a broad class of bugs that is becoming increasingly common with increasing software complexity, and many of these bugs are also security vulnerabilities. Existing software and hardware approaches for finding and identifying memory bugs have a number of drawbacks including considerable performance overheads, target only a specific type of bug, implementation cost, and inefficient use of computational resources. This article describes MemTracker, a new hardware support mechanism that can be configured to perform different kinds of memory access monitoring tasks. MemTracker associates each word of data in memory with a few bits of state, and uses a programmable state transition table to react to different events that can affect this state. The number of state bits per word, the events to which MemTracker reacts, and the transition table are all fully programmable. MemTracker's rich set of states, events, and transitions can be used to implement different monitoring and debugging checkers with minimal performance overheads, even when frequent state updates are needed. To evaluate MemTracker, we map three different checkers onto it, as well as a checker that combines all three. For the most demanding (combined) checker with 8 bits state per memory word, we observe performance overheads of only around 3%, on average, and 14.5% worst-case across different benchmark suites. Such low overheads allow continuous (always-on) use of MemTracker-enabled checkers, even in production runs.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2041150587",
    "type": "article"
  },
  {
    "title": "Approximate graph clustering for program characterization",
    "doi": "https://doi.org/10.1145/2086696.2086700",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "John Demme; Simha Sethumadhavan",
    "corresponding_authors": "",
    "abstract": "An important aspect of system optimization research is the discovery of program traits or behaviors. In this paper, we present an automated method of program characterization which is able to examine and cluster program graphs, i.e., dynamic data graphs or control flow graphs. Our novel approximate graph clustering technology allows users to find groups of program fragments which contain similar code idioms or patterns in data reuse, control flow, and context. Patterns of this nature have several potential applications including development of new static or dynamic optimizations to be implemented in software or in hardware. For the SPEC CPU 2006 suite of benchmarks, our results show that approximate graph clustering is effective at grouping behaviorally similar functions. Graph based clustering also produces clusters that are more homogeneous than previously proposed non-graph based clustering methods. Further qualitative analysis of the clustered functions shows that our approach is also able to identify some frequent unexploited program behaviors. These results suggest that our approximate graph clustering methods could be very useful for program characterization.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2089806214",
    "type": "article"
  },
  {
    "title": "Preallocation instruction scheduling with register pressure minimization using a combinatorial optimization approach",
    "doi": "https://doi.org/10.1145/2512432",
    "publication_date": "2013-09-16",
    "publication_year": 2013,
    "authors": "Ghassan Shobaki; Maxim Shawabkeh; Najm Eldeen Abu Rmaileh",
    "corresponding_authors": "",
    "abstract": "Balancing Instruction-Level Parallelism (ILP) and register pressure during preallocation instruction scheduling is a fundamentally important problem in code generation and optimization. The problem is known to be NP-complete. Many heuristic techniques have been proposed to solve this problem. However, due to the inherently conflicting requirements of maximizing ILP and minimizing register pressure, heuristic techniques may produce poor schedules in many cases. If such cases occur in hot code, significant performance degradation may result. A few combinatorial optimization approaches have also been proposed, but none of them has been shown to solve large real-world instances within reasonable time. This article presents the first combinatorial algorithm that is efficient enough to optimally solve large instances of this problem (basic blocks with hundreds of instructions) within a few seconds per instance. The proposed algorithm uses branch-and-bound enumeration with a number of powerful pruning techniques to efficiently search the solution space. The search is based on a cost function that incorporates schedule length and register pressure. An implementation of the proposed scheduling algorithm has been integrated into the LLVM Compiler and evaluated using SPEC CPU 2006. On x86-64, with a time limit of 10ms per instruction, it optimally schedules 79% of the hot basic blocks in FP2006. Another 19% of the blocks are not optimally scheduled but are improved in cost relative to LLVM's heuristic. This improves the execution time of some benchmarks by up to 21%, with a geometric-mean improvement of 2.4% across the entire benchmark suite. With the use of precise latency information, the geometric-mean improvement is increased to 2.8%.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2017069727",
    "type": "article"
  },
  {
    "title": "Adaptive workload-aware task scheduling for single-ISA asymmetric multicore architectures",
    "doi": "https://doi.org/10.1145/2579674",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "Quan Chen; Minyi Guo",
    "corresponding_authors": "",
    "abstract": "Single-ISA Asymmetric Multicore (AMC) architectures have shown high performance as well as power efficiency. However, current parallel programming environments do not perform well on AMC because they are designed for symmetric multicore architectures in which all cores provide equal performance. Their random task scheduling policies can result in unbalanced workloads in AMC and severely degrade the performance of parallel applications. To balance the workloads of parallel applications in AMC, this article proposes an adaptive Workload-Aware Task Scheduler (WATS) that consists of a history-based task allocator and a preference-based task scheduler. The history-based task allocator is based on a near-optimal, static task allocation using the historical statistics collected during the execution of a parallel application. The preference-based task scheduler, which schedules tasks based on a preference list, can dynamically adjust the workloads in AMC if the task allocation is less optimal due to approximation in the history-based task allocator. Experimental results show that WATS can improve both the performance and energy efficiency of task-based applications, with the performance gain up to 66.1% compared with traditional task schedulers.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2055099017",
    "type": "article"
  },
  {
    "title": "ADAPT",
    "doi": "https://doi.org/10.1145/2400682.2400704",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Kishore Kumar Pusukuri; Rajiv Gupta; Laxmi N. Bhuyan",
    "corresponding_authors": "",
    "abstract": "Since multicore systems offer greater performance via parallelism, future computing is progressing towards use of multicore machines with large number of cores. However, the performance of emerging multithreaded programs often does not scale to fully utilize the available cores. Therefore, simultaneously running multiple multithreaded applications becomes inevitable to fully exploit the computing potential of such machines. However, maximizing the performance and throughput on multicore machines in the presence of multiple multithreaded programs is a challenge for the OS. We have observed that the state-of-the-art contention management algorithms fail to effectively coschedule multithreaded programs on multicore machines. To address the above challenge, we present ADAPT, a scheduling framework that continuously monitors the resource usage of multithreaded programs and adaptively coschedules them such that they interfere with each other's performance as little as possible. In addition, ADAPT selects appropriate memory allocation and scheduling policies according to the workload characteristics. We have implemented ADAPT on a 64-core Supermicro server running Solaris 11 and evaluated it using 26 multithreaded programs including the TATP database application, SPECjbb2005, and programs from Phoenix, PARSEC, and SPEC OMP suites. The experimental results show that ADAPT substantially improves total turnaround time and system utilization relative to the default Solaris 11 scheduler.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2056902710",
    "type": "article"
  },
  {
    "title": "A Retargetable Static Binary Translator for the ARM Architecture",
    "doi": "https://doi.org/10.1145/2629335",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "Bor-Yeh Shen; Wei‐Chung Hsu; Wuu Yang",
    "corresponding_authors": "",
    "abstract": "Machines designed with new but incompatible Instruction Set Architecture (ISA) may lack proper applications. Binary translation can address this incompatibility by migrating applications from one legacy ISA to a new one, although binary translation has problems such as code discovery for variable-length ISA and code location issues for handling indirect branches. Dynamic Binary Translation (DBT) has been widely adopted for migrating applications since it avoids those problems. Static Binary Translation (SBT) is a less general solution and has not been actively researched. However, SBT performs more aggressive optimizations, which could yield more compact code and better code quality. Applications translated by SBT can consume less memory, processor cycles, and power than DBT and can be started more quickly. These advantages are even more critical for embedded systems than for general systems. In this article, we designed and implemented a new SBT tool, called LLBT, which translates ARM instructions into LLVM IRs and then retargets the LLVM IRs to various ISAs, including ×86, ×86--64, ARM, and MIPS. LLBT leverages two important functionalities from LLVM: comprehensive optimizations and retargetability. More importantly, LLBT solves the code discovery problem for ARM/Thumb binaries without resorting to interpretation. LLBT also effectively reduced the size of the address mapping table, making SBT a viable solution for embedded systems. Our experiments based on the EEMBC benchmark suite show that the LLBT-generated code can run more than 6× and 2.3× faster on average than emulation with QEMU and HQEMU, respectively.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2064391085",
    "type": "article"
  },
  {
    "title": "Efficiently exploiting memory level parallelism on asymmetric coupled cores in the dark silicon era",
    "doi": "https://doi.org/10.1145/2086696.2086707",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "George Patsilaras; Niket K. Choudhary; James Tuck",
    "corresponding_authors": "",
    "abstract": "Extracting high memory-level parallelism (MLP) is essential for speeding up single-threaded applications which are memory bound. At the same time, the projected amount of dark silicon (the fraction of the chip powered off) on a chip is growing. Hence, Asymmetric Multicore Processors (AMP) offer a unique opportunity to integrate many types of cores, each powered at different times, in order to optimize for different regions of execution. In this work, we quantify the potential for exploiting core customization to speedup programs during regions of high MLP. Based on a careful design space exploration, we discover that an AMP that includes a narrow and fast specialized core has the potential to efficiently exploit MLP. Using the results of our analysis, we design an AMP with both an MLP and ILP specialized core, and we propose a hardware-level, application steering mechanism called Symbiotic Core Execution (SCE). SCE detects MLP phases by monitoring the L2 miss rate of the application, and it uses that information to steer the application to the best core. Interestingly, we show that L2 miss rates are important for deciding when an MLP region begins and when it ends. As a program runs, its execution migrates to a core customized for MLP during regions of high MLP; when the region ends, it is re-scheduled on the core that fits the application characteristics. Compared to a monolithic core optimized for both modes of operation, our AMP design provides a harmonic mean performance improvement of 5.3% and 6.6% for SPEC2000 and SPEC2006, respectively, with a maximum speedup of 14.5%. For the same study, it achieves a 18.3% and 21.1% energy delay 2 reduction for SPEC2000 and SPEC2006, respectively. Our findings yield an important message for designing AMPs with specialized cores: core customization enables efficient exploitation of MLP, and application steering mechanisms for MLP are simple to implement and effective.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2157920577",
    "type": "article"
  },
  {
    "title": "Delta-compressed caching for overcoming the write bandwidth limitation of hybrid main memory",
    "doi": "https://doi.org/10.1145/2400682.2400714",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Yu Du; Miao Zhou; Bruce R. Childers; Rami Melhem; Daniel Mossé",
    "corresponding_authors": "",
    "abstract": "Limited PCM write bandwidth is a critical obstacle to achieve good performance from hybrid DRAM/PCM memory systems. The write bandwidth is severely restricted in PCM devices, which harms application performance. Indeed, as we show, it is more important to reduce PCM write traffic than to reduce PCM read latency for application performance. To reduce the number of PCM writes, we propose a DRAM cache organization that employs compression. A new delta compression technique for modified data is used to achieve a large compression ratio. Our approach can selectively and predictively apply compression to improve its efficiency and performance. Our approach is designed to facilitate adoption in existing main memory compression frameworks. We describe an instance of how to incorporate delta compression in IBM's MXT memory compression architecture when used for DRAM cache in a hybrid main memory. For fourteen representative memory-intensive workloads, on average, our delta compression technique reduces the number of PCM writes by 54.3%, and improves IPC performance by 24.4%.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W1984580002",
    "type": "article"
  },
  {
    "title": "Runtime energy consumption estimation for server workloads based on chaotic time-series approximation",
    "doi": "https://doi.org/10.1145/2355585.2355588",
    "publication_date": "2012-09-01",
    "publication_year": 2012,
    "authors": "Adam Lewis; Nian-Feng Tzeng; Soumik Ghosh",
    "corresponding_authors": "",
    "abstract": "This article proposes a runtime model that relates server energy consumption to its overall thermal envelope, using hardware performance counters and experimental measurements. While previous studies have attempted system-wide modeling of server power consumption through subsystem models, our approach is different in that it links system energy input to subsystem energy consumption based on a small set of tightly correlated parameters. The proposed model takes into account processor power, bus activities, and system ambient temperature for real-time prediction on the power consumption of long running jobs. Using the HyperTransport and QuickPath Link structures as case studies and through electrical measurements on example server subsystems, we develop a chaotic time-series approximation for runtime power consumption, arriving at the Chaotic Attractor Predictor (CAP). With polynomial time complexity, CAP exhibits high prediction accuracy, having the prediction errors within 1.6% (or 3.3%) for servers based on the HyperTransport bus (or the QuickPath Links), as verified by a set of common processor benchmarks. Our CAP is a superior predictive mechanism over existing linear auto-regressive methods, which require expensive and complex corrective steps to address the nonlinear and chaotic aspects of the underlying physical system.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2043907785",
    "type": "article"
  },
  {
    "title": "Reuse Distance-Based Probabilistic Cache Replacement",
    "doi": "https://doi.org/10.1145/2818374",
    "publication_date": "2015-10-19",
    "publication_year": 2015,
    "authors": "Subhasis Das; Tor M. Aamodt; William J. Dally",
    "corresponding_authors": "",
    "abstract": "This article proposes Probabilistic Replacement Policy (PRP), a novel replacement policy that evicts the line with minimum estimated hit probability under optimal replacement instead of the line with maximum expected reuse distance. The latter is optimal under the independent reference model of programs, which does not hold for last-level caches (LLC). PRP requires 7% and 2% metadata overheads in the cache and DRAM respectively. Using a sampling scheme makes DRAM overhead negligible, with minimal performance impact. Including detailed overhead modeling and equal cache areas, PRP outperforms SHiP, a state-of-the-art LLC replacement algorithm, by 4% for memory-intensive SPEC-CPU2006 benchmarks.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2072873664",
    "type": "article"
  },
  {
    "title": "F <scp>ault</scp> S <scp>im</scp>",
    "doi": "https://doi.org/10.1145/2831234",
    "publication_date": "2015-12-08",
    "publication_year": 2015,
    "authors": "Prashant J. Nair; David Roberts; Moinuddin K. Qureshi",
    "corresponding_authors": "",
    "abstract": "As memory systems scale, maintaining their Reliability Availability and Serviceability (RAS) is becoming more complex. To make matters worse, recent studies of DRAM failures in data centers and supercomputer environments have highlighted that large-granularity failures are common in DRAM chips. Furthermore, the move toward 3D-stacked memories can make the system vulnerable to newer failure modes, such as those occurring from faults in Through-Silicon Vias (TSVs). To architect future systems and to use emerging technology, system designers will need to employ strong error correction and repair techniques. Unfortunately, evaluating the relative effectiveness of these reliability mechanisms is often difficult and is traditionally done with analytical models, which are both error prone and time-consuming to develop. To this end, this article proposes F ault S im , a fast configurable memory-reliability simulation tool for 2D and 3D-stacked memory systems. FaultSim employs Monte Carlo simulations, which are driven by real-world failure statistics. We discuss the novel algorithms and data structures used in FaultSim to accelerate the evaluation of different resilience schemes. We implement BCH-1 (SECDED) and ChipKill codes using FaultSim and validate against an analytical model. FaultSim implements BCH-1 and ChipKill codes with a deviation of only 0.032% and 8.41% from the analytical model. FaultSim can simulate 1 million Monte Carlo trials (each for a period of 7 years) of BCH-1 and ChipKill codes in only 34 seconds and 33 seconds, respectively.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2240238332",
    "type": "article"
  },
  {
    "title": "Automated Software Protection for the Masses Against Side-Channel Attacks",
    "doi": "https://doi.org/10.1145/3281662",
    "publication_date": "2018-11-16",
    "publication_year": 2018,
    "authors": "Nicolas Belleville; Damien Couroussé; Karine Heydemann; Henri‐Pierre Charles",
    "corresponding_authors": "",
    "abstract": "We present an approach and a tool to answer the need for effective, generic, and easily applicable protections against side-channel attacks. The protection mechanism is based on code polymorphism, so that the observable behaviour of the protected component is variable and unpredictable to the attacker. Our approach combines lightweight specialized runtime code generation with the optimization capabilities of static compilation. It is extensively configurable. Experimental results show that programs secured by our approach present strong security levels and meet the performance requirements of constrained systems.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2901397262",
    "type": "article"
  },
  {
    "title": "Volatile STT-RAM Scratchpad Design and Data Allocation for Low Energy",
    "doi": "https://doi.org/10.1145/2669556",
    "publication_date": "2014-12-08",
    "publication_year": 2014,
    "authors": "Gabriel Rodríguez; Juan Touriño; Mahmut Kandemir",
    "corresponding_authors": "",
    "abstract": "On-chip power consumption is one of the fundamental challenges of current technology scaling. Cache memories consume a sizable part of this power, particularly due to leakage energy. STT-RAM is one of several new memory technologies that have been proposed in order to improve power while preserving performance. It features high density and low leakage, but at the expense of write energy and performance. This article explores the use of STT-RAM--based scratchpad memories that trade nonvolatility in exchange for faster and less energetically expensive accesses, making them feasible for on-chip implementation in embedded systems. A novel multiretention scratchpad partitioning is proposed, featuring multiple storage spaces with different retention, energy, and performance characteristics. A customized compiler-based allocation algorithm suitable for use with such a scratchpad organization is described. Our experiments indicate that a multiretention STT-RAM scratchpad can provide energy savings of 53% with respect to an iso-area, hardware-managed SRAM cache.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W1986890602",
    "type": "article"
  },
  {
    "title": "Bones",
    "doi": "https://doi.org/10.1145/2665079",
    "publication_date": "2014-12-08",
    "publication_year": 2014,
    "authors": "Cedric Nugteren; Henk Corporaal",
    "corresponding_authors": "",
    "abstract": "The shift toward parallel processor architectures has made programming and code generation increasingly challenging. To address this programmability challenge, this article presents a technique to fully automatically generate efficient and readable code for parallel processors (with a focus on GPUs). This is made possible by combining algorithmic skeletons, traditional compilation, and “ algorithmic species ,” a classification of program code. Compilation starts by automatically annotating C code with class information (the algorithmic species). This code is then fed into the skeleton-based source-to-source compiler bones to generate CUDA code. To generate efficient code, bones also performs optimizations including host-accelerator transfer optimization and kernel fusion. This results in a unique approach, integrating a skeleton-based compiler for the first time into an automated flow. The benefits are demonstrated experimentally for PolyBench GPU kernels, showing geometric mean speed-ups of 1.4× and 2.4× compared to ppcg and Par4All , and for five Rodinia GPU benchmarks, showing a gap of only 1.2× compared to hand-optimized code.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2033485113",
    "type": "article"
  },
  {
    "title": "Automatic Vectorization of Interleaved Data Revisited",
    "doi": "https://doi.org/10.1145/2838735",
    "publication_date": "2015-12-08",
    "publication_year": 2015,
    "authors": "Andrew Anderson; Avinash Malik; David Gregg",
    "corresponding_authors": "",
    "abstract": "Automatically exploiting short vector instructions sets (SSE, AVX, NEON) is a critically important task for optimizing compilers. Vector instructions typically work best on data that is contiguous in memory, and operating on non-contiguous data requires additional work to gather and scatter the data. There are several varieties of non-contiguous access, including interleaved data access. An existing approach used by GCC generates extremely efficient code for loops with power-of-2 interleaving factors (strides). In this paper we propose a generalization of this approach that produces similar code for any compile-time constant interleaving factor. In addition, we propose several novel program transformations, which were made possible by our generalized representation of the problem. Experiments show that our approach achieves significant speedups for both power-of-2 and non--power-of-2 interleaving factors. Our vectorization approach results in mean speedups over scalar code of 1.77x on Intel SSE and 2.53x on Intel AVX2 in real-world benchmarking on a selection of BLAS Level 1 routines. On the same benchmark programs, GCC 5.0 achieves mean improvements of 1.43x on Intel SSE and 1.30x on Intel AVX2. In synthetic benchmarking on Intel SSE, our maximum improvement on data movement is over 4x for gathering operations and over 6x for scattering operations versus scalar code.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2292182532",
    "type": "article"
  },
  {
    "title": "The Art of Getting Deep Neural Networks in Shape",
    "doi": "https://doi.org/10.1145/3291053",
    "publication_date": "2018-12-31",
    "publication_year": 2018,
    "authors": "Rahim Mammadli; Felix Wolf; Ali Jannesari",
    "corresponding_authors": "",
    "abstract": "Training a deep neural network (DNN) involves selecting a set of hyperparameters that define the network topology and influence the accuracy of the resulting network. Often, the goal is to maximize prediction accuracy on a given dataset. However, non-functional requirements of the trained network -- such as inference speed, size, and energy consumption -- can be very important as well. In this article, we aim to automate the process of selecting an appropriate DNN topology that fulfills both functional and non-functional requirements of the application. Specifically, we focus on tuning two important hyperparameters, depth and width, which together define the shape of the resulting network and directly affect its accuracy, speed, size, and energy consumption. To reduce the time needed to search the design space, we train a fraction of DNNs and build a model to predict the performances of the remaining ones. We are able to produce tuned ResNets, which are up to 4.22 times faster than original depth-scaled ResNets on a batch of 128 images while matching their accuracy.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2908530716",
    "type": "article"
  },
  {
    "title": "UMH",
    "doi": "https://doi.org/10.1145/2996190",
    "publication_date": "2016-12-02",
    "publication_year": 2016,
    "authors": "Amir Kavyan Ziabari; Yifan Sun; Yenai Ma; Dana Schaa; José Luis Abellán; Rafael Ubal; John Kim; Ajay Joshi; David Kaeli",
    "corresponding_authors": "",
    "abstract": "In this article, we describe how to ease memory management between a Central Processing Unit (CPU) and one or multiple discrete Graphic Processing Units (GPUs) by architecting a novel hardware-based Unified Memory Hierarchy (UMH). Adopting UMH, a GPU accesses the CPU memory only if it does not find its required data in the directories associated with its high-bandwidth memory, or the NMOESI coherency protocol limits the access to that data. Using UMH with NMOESI improves performance of a CPU-multiGPU system by at least 1.92 × in comparison to alternative software-based approaches. It also allows the CPU to access GPUs modified data by at least 13 × faster.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2559253174",
    "type": "article"
  },
  {
    "title": "ITAP",
    "doi": "https://doi.org/10.1145/3291606",
    "publication_date": "2019-02-27",
    "publication_year": 2019,
    "authors": "Mohammad Sadrosadati; Seyed Borna Ehsani; Hajar Falahati; Rachata Ausavarungnirun; Arash Tavakkol; Mojtaba Abaee; Lois Orosa; Yaohua Wang; Hamid Sarbazi‐Azad; Onur Mutlu",
    "corresponding_authors": "",
    "abstract": "Graphics Processing Units (GPUs) are widely used as the accelerator of choice for applications with massively data-parallel tasks. However, recent studies show that GPUs suffer heavily from resource underutilization, which, combined with their large static power consumption, imposes a significant power overhead. One of the most power-hungry components of a GPU—the execution units—frequently experience idleness when (1) an underutilized warp is issued to the execution units, leading to partial lane idleness, and (2) there is no active warp to be issued for the execution due to warp stalls (e.g., waiting for memory access and synchronization). Although large in total, the idle time of execution units actually comes from short but frequent stalls, leaving little potential for common power saving techniques, such as power-gating. In this article, we propose ITAP , a novel idle-time-aware power management technique, which aims to effectively reduce the static energy consumption of GPU execution units. By taking advantage of different power management techniques (i.e., power-gating and different levels of voltage scaling), ITAP employs three static power reduction modes with different overheads and capabilities of static power reduction. ITAP estimates the idle period length of execution units using prediction and peek-ahead techniques in a synergistic way and then applies the most appropriate static power reduction mode based on the estimated idle period length. We design ITAP to be power-aggressive or performance-aggressive, not both at the same time. Our experimental results on several workloads show that the power-aggressive design of ITAP outperforms the state-of-the-art solution by an average of 27.6% in terms of static energy savings, with less than 2.1% performance overhead. However, the performance-aggressive design of ITAP improves the static energy savings by an average of 16.9%, while keeping the GPU performance almost unaffected (i.e., up to 0.4% performance overhead) compared to the state-of-the-art static energy savings mechanism.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2917720315",
    "type": "article"
  },
  {
    "title": "Polyhedral Specification and Code Generation of Sparse Tensor Contraction with Co-iteration",
    "doi": "https://doi.org/10.1145/3566054",
    "publication_date": "2022-10-25",
    "publication_year": 2022,
    "authors": "Tuowen Zhao; Tobi Popoola; Mary Hall; Catherine Olschanowsky; Michelle Mills Strout",
    "corresponding_authors": "",
    "abstract": "This article presents a code generator for sparse tensor contraction computations. It leverages a mathematical representation of loop nest computations in the sparse polyhedral framework (SPF), which extends the polyhedral model to support non-affine computations, such as those that arise in sparse tensors. SPF is extended to perform layout specification, optimization, and code generation of sparse tensor code: (1) We develop a polyhedral layout specification that decouples iteration spaces for layout and computation; and (2) we develop efficient co-iteration of sparse tensors by combining polyhedra scanning over the layout of one sparse tensor with the synthesis of code to find corresponding elements in other tensors through an SMT solver. We compare the generated code with that produced by a state-of-the-art tensor compiler, TACO. We achieve on average 1.63× faster parallel performance than TACO on sparse-sparse co-iteration and describe how to improve that to 2.72× average speedup by switching the find algorithms. We also demonstrate that decoupling iteration spaces of layout and computation enables additional layout and computation combinations to be supported.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4307380766",
    "type": "article"
  },
  {
    "title": "ULEEN: A Novel Architecture for Ultra-low-energy Edge Neural Networks",
    "doi": "https://doi.org/10.1145/3629522",
    "publication_date": "2023-10-25",
    "publication_year": 2023,
    "authors": "Zachary Susskind; Aman Arora; Igor D. S. Miranda; Alan T. L. Bacellar; Luis A. Q. Villon; Rafael F. Katopodis; Leandro Santiago; Diego Leonel Cadette Dutra; Priscila M. V. Lima; Felipe M. G. França; Maurício Breternitz; Lizy K. John",
    "corresponding_authors": "",
    "abstract": "‘‘Extreme edge” 1 devices, such as smart sensors, are a uniquely challenging environment for the deployment of machine learning. The tiny energy budgets of these devices lie beyond what is feasible for conventional deep neural networks, particularly in high-throughput scenarios, requiring us to rethink how we approach edge inference. In this work, we propose ULEEN, a model and FPGA-based accelerator architecture based on weightless neural networks (WNNs). WNNs eliminate energy-intensive arithmetic operations, instead using table lookups to perform computation, which makes them theoretically well-suited for edge inference. However, WNNs have historically suffered from poor accuracy and excessive memory usage. ULEEN incorporates algorithmic improvements and a novel training strategy inspired by binary neural networks (BNNs) to make significant strides in addressing these issues. We compare ULEEN against BNNs in software and hardware using the four MLPerf Tiny datasets and MNIST. Our FPGA implementations of ULEEN accomplish classification at 4.0–14.3 million inferences per second, improving area-normalized throughput by an average of 3.6× and steady-state energy efficiency by an average of 7.1× compared to the FPGA-based Xilinx FINN BNN inference platform. While ULEEN is not a universally applicable machine learning model, we demonstrate that it can be an excellent choice for certain applications in energy- and latency-critical edge environments.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4387933227",
    "type": "article"
  },
  {
    "title": "Tyche: An Efficient and General Prefetcher for Indirect Memory Accesses",
    "doi": "https://doi.org/10.1145/3641853",
    "publication_date": "2024-01-22",
    "publication_year": 2024,
    "authors": "Feng Xue; Chenji Han; Xinyu Li; Junliang Wu; Tingting Zhang; Tianyi Liu; Yifan Hao; Zidong Du; Qi Guo; Fuxin Zhang",
    "corresponding_authors": "",
    "abstract": "Indirect memory accesses (IMAs, i.e., A [ f ( B [ i ])]) are typical memory access patterns in applications such as graph analysis, machine learning, and database. IMAs are composed of producer-consumer pairs, where the consumers’ memory addresses are derived from the producers’ memory data. Due to the built-in value-dependent feature, IMAs exhibit poor locality, making prefetching ineffective. Hindered by the challenges of recording the potentially complex graphs of instruction dependencies among IMA producers and consumers, current state-of-the-art hardware prefetchers either (a) exhibit inadequate IMA identification abilities or (b) rely on the run-ahead mechanism to prefetch IMAs intermittently and insufficiently. To solve this problem, we propose Tyche, 1 an efficient and general hardware prefetcher to enhance IMA performance. Tyche adopts a bilateral propagation mechanism to precisely excavate the instruction dependencies in simple chains with moderate length (rather than complex graphs). Based on the exact instruction dependencies, Tyche can accurately identify various IMA patterns, including nonlinear ones, and generate accurate prefetching requests continuously. Evaluated on broad benchmarks, Tyche achieves an average performance speedup of 16.2% over the state-of-the-art spatial prefetcher Berti. More importantly, Tyche outperforms the state-of-the-art IMA prefetchers IMP, Gretch, and Vector Runahead, by 15.9%, 12.8%, and 10.7%, respectively, with a lower storage overhead of only 0.57 KB.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4391103886",
    "type": "article"
  },
  {
    "title": "Winols: A Large-Tiling Sparse Winograd CNN Accelerator on FPGAs",
    "doi": "https://doi.org/10.1145/3643682",
    "publication_date": "2024-01-31",
    "publication_year": 2024,
    "authors": "Kunpeng Xie; Ye Lu; Xinyu He; Dezhi Yi; Huijuan Dong; Yao Chen",
    "corresponding_authors": "",
    "abstract": "Convolutional Neural Networks (CNNs) can benefit from the computational reductions provided by the Winograd minimal filtering algorithm and weight pruning. However, harnessing the potential of both methods simultaneously introduces complexity in designing pruning algorithms and accelerators. Prior studies aimed to establish regular sparsity patterns in the Winograd domain, but they were primarily suited for small tiles, with domain transformation dictating the sparsity ratio. The irregularities in data access and domain transformation pose challenges in accelerator design, especially for larger Winograd tiles. This paper introduces “Winols,” an innovative algorithm-hardware co-design strategy that emphasizes the strengths of the large-tiling Winograd algorithm. Through a spatial-to-Winograd relevance degree evaluation, we extensively explore domain transformation and propose a cross-domain pruning technique that retains sparsity across both spatial and Winograd domains. To compress pruned weight matrices, we invent a relative column encoding scheme. We further design an FPGA-based accelerator for CNN models with large Winograd tiles and sparse matrix-vector operations. Evaluations indicate our pruning method achieves up to 80% weight tile sparsity in the Winograd domain without compromising accuracy. Our Winols accelerator outperforms dense accelerator by a factor of 31.7× in inference latency. When compared with prevailing sparse Winograd accelerators, Winols reduces latency by an average of 10.9×, and improves DSP and energy efficiencies by over 5.6× and 5.7×, respectively. When compared with the CPU and GPU platform, Winols accelerator with tile size 8× 8 achieves 24.6× and 2.84× energy efficiency improvements, respectively.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4391387876",
    "type": "article"
  },
  {
    "title": "Conserving network processor power consumption by exploiting traffic variability",
    "doi": "https://doi.org/10.1145/1216544.1216547",
    "publication_date": "2007-03-01",
    "publication_year": 2007,
    "authors": "Yan Luo; Jia Yu; Jun Yang; Laxmi N. Bhuyan",
    "corresponding_authors": "",
    "abstract": "Network processors (NPs) have emerged as successful platforms for providing both high performance and flexibility in building powerful routers. Typical NPs incorporate multiprocessing and multithreading to achieve maximum parallel processing capabilities. We observed that under low incoming traffic rates, processing elements (PEs) in an NP are idle for most of the time but still consume dynamic power. This paper develops a low-power technique to reduce the activities of PEs in accordance with the varying traffic volume. We propose to monitor the average number of idle threads in a time window, and gate off the clock signals to unnecessary PEs when a subset of PEs is enough to handle the network traffic. We solve the difficulties arising from clock gating the PEs, such as redirecting network packets, determining the thresholds of turning on/off PEs, and avoiding unnecessary packet loss. Our technique brings significant reduction in power consumption of NPs with no packet loss and little impact on overall throughput.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W2033721616",
    "type": "article"
  },
  {
    "title": "A compiler cost model for speculative parallelization",
    "doi": "https://doi.org/10.1145/1250727.1250732",
    "publication_date": "2007-06-01",
    "publication_year": 2007,
    "authors": "Jialin Dou; Marcelo Cintra",
    "corresponding_authors": "",
    "abstract": "Speculative parallelization is a technique that allows code sections that cannot be fully analyzed by the compiler to be aggressively executed in parallel. However, while speculative parallelization can potentially deliver significant speedups, several overheads associated with this technique can limit these speedups in practice. This paper proposes a novel compiler static cost model of speculative multithreaded execution that can be used to predict the resulting performance. This model attempts to predict the expected speedups, or slowdowns, of the candidate speculative sections based on the estimation of the combined runtime effects of various overheads, and taking into account the scheduling restrictions of most speculative execution environments. The model is based on estimating the likely execution duration of threads and considers all the possible permutations of these threads. This model also produces a quantitative estimate of the speedup, which is different from prior heuristics that only qualitatively estimate the benefits of speculative multithreaded execution. In previous work, a limited version of the framework was evaluated on a number of loops from a collection of SPEC benchmarks that suffer mainly from load imbalance and thread dispatch and commit overheads. In this work, an extended framework is also evaluated on loops that may suffer from data-dependence violations. Experimental results show that prediction accuracy is lower when loops with violations are included. Nevertheless, accuracy is still very high for a static model: the framework can identify, on average, 45% of the loops that cause slowdowns and, on average, 96% of the loops that lead to speedups; it predicts the speedups or slowdowns with an error of less than 20% for an average of 28% of the loops across the benchmarks and with an error of less than 50% for an average of 80% of the loops. Overall, the framework often outperforms, by as much as 25%, a naive approach that attempts to speculatively parallelize all the loops considered, and is able to curb the large slowdowns caused in many cases by this naive approach.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2069366701",
    "type": "article"
  },
  {
    "title": "Compiler and hardware support for reducing the synchronization of speculative threads",
    "doi": "https://doi.org/10.1145/1369396.1369399",
    "publication_date": "2008-05-01",
    "publication_year": 2008,
    "authors": "Antonia Zhai; J. Gregory Steffan; Christopher B. Colohan; Todd C. Mowry",
    "corresponding_authors": "",
    "abstract": "Thread-level speculation (TLS) allows us to automatically parallelize general-purpose programs by supporting parallel execution of threads that might not actually be independent. In this article, we focus on one important limitation of program performance under TLS, which stalls as a result of synchronizing and forwarding scalar values between speculative threads that would otherwise cause frequent data dependences and, hence, failed speculation. Using SPECint benchmarks that have been automatically transformed by our compiler to exploit TLS, we present, evaluate in detail, and compare both compiler and hardware techniques for improving the communication of scalar values. We find that through our dataflow algorithms for three increasingly aggressive instruction scheduling techniques, the compiler can drastically reduce the critical forwarding path introduced by the synchronization and forwarding of scalar values. We also show that hardware techniques for reducing synchronization can be complementary to compiler scheduling, but that the additional performance benefits are minimal and are generally not worth the cost.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2037303555",
    "type": "article"
  },
  {
    "title": "Eliminating voltage emergencies via software-guided code transformations",
    "doi": "https://doi.org/10.1145/1839667.1839674",
    "publication_date": "2010-09-01",
    "publication_year": 2010,
    "authors": "Vijay Janapa Reddi; Simone Campanoni; Meeta S. Gupta; Michael D. Smith; Gu-Yeon Wei; David Brooks; Kim Hazelwood",
    "corresponding_authors": "",
    "abstract": "In recent years, circuit reliability in modern high-performance processors has become increasingly important. Shrinking feature sizes and diminishing supply voltages have made circuits more sensitive to microprocessor supply voltage fluctuations. These fluctuations result from the natural variation of processor activity as workloads execute, but when left unattended, these voltage fluctuations can lead to timing violations or even transistor lifetime issues. In this article, we present a hardware--software collaborative approach to mitigate voltage fluctuations. A checkpoint-recovery mechanism rectifies errors when voltage violates maximum tolerance settings, while a runtime software layer reschedules the program's instruction stream to prevent recurring violations at the same program location. The runtime layer, combined with the proposed code-rescheduling algorithm, removes 60% of all violations with minimal overhead, thereby significantly improving overall performance. Our solution is a radical departure from the ongoing industry-standard approach to circumvent the issue altogether by optimizing for the worst-case voltage flux, which compromises power and performance efficiency severely, especially looking ahead to future technology generations. Existing conservative approaches will have severe implications on the ability to deliver efficient microprocessors. The proposed technique reassembles a traditional reliability problem as a runtime performance optimization problem, thus allowing us to design processors for typical case operation by building intelligent algorithms that can prevent recurring violations.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2093985935",
    "type": "article"
  },
  {
    "title": "Tiled-MapReduce",
    "doi": "https://doi.org/10.1145/2445572.2445575",
    "publication_date": "2013-04-01",
    "publication_year": 2013,
    "authors": "Rong Chen; Haibo Chen",
    "corresponding_authors": "",
    "abstract": "The prevalence of chip multiprocessors opens opportunities of running data-parallel applications originally in clusters on a single machine with many cores. MapReduce, a simple and elegant programming model to program large-scale clusters, has recently been shown a promising alternative to harness the multicore platform. The differences such as memory hierarchy and communication patterns between clusters and multicore platforms raise new challenges to design and implement an efficient MapReduce system on multicore. This article argues that it is more efficient for MapReduce to iteratively process small chunks of data in turn than processing a large chunk of data at a time on shared memory multicore platforms. Based on the argument, we extend the general MapReduce programming model with a “tiling strategy”, called Tiled - MapReduce (TMR). TMR partitions a large MapReduce job into a number of small subjobs and iteratively processes one subjob at a time with efficient use of resources; TMR finally merges the results of all subjobs for output. Based on Tiled-MapReduce, we design and implement several optimizing techniques targeting multicore, including the reuse of the input buffer among subjobs, a NUCA/NUMA-aware scheduler, and pipelining a subjob’s reduce phase with the successive subjob’s map phase, to optimize the memory, cache, and CPU resources accordingly. Further, we demonstrate that Tiled-MapReduce supports fine-grained fault tolerance and enables several usage scenarios such as online and incremental computing on multicore machines. Performance evaluation with our prototype system called Ostrich on a 48-core machine shows that Ostrich saves up to 87.6% memory, causes less cache misses, and makes more efficient use of CPU cores, resulting in a speedup ranging from 1.86x to 3.07x over Phoenix. Ostrich also efficiently supports fine-grained fault tolerance, online, and incremental computing with small performance penalty.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W1992615224",
    "type": "article"
  },
  {
    "title": "Parallelization libraries",
    "doi": "https://doi.org/10.1145/1952998.1953003",
    "publication_date": "2011-02-05",
    "publication_year": 2011,
    "authors": "Abhishek Bhattacharjee; Gilberto Contreras; Margaret Martonosi",
    "corresponding_authors": "",
    "abstract": "Creating efficient, scalable dynamic parallel runtime systems for chip multiprocessors (CMPs) requires understanding the overheads that manifest at high core counts and small task sizes. In this article, we assess these overheads on Intel's Threading Building Blocks (TBB) and OpenMP. First, we use real hardware and simulations to detail various scheduler and synchronization overheads. We find that these can amount to 47% of TBB benchmark runtime and 80% of OpenMP benchmark runtime. Second, we propose load balancing techniques such as occupancy-based and criticality-guided task stealing, to boost performance. Overall, our study provides valuable insights for creating robust, scalable runtime libraries.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2011875368",
    "type": "article"
  },
  {
    "title": "An efficient multicharacter transition string-matching engine based on the aho-corasick algorithm",
    "doi": "https://doi.org/10.1145/2541228.2541232",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Chien‐Chi Chen; Sheng‐De Wang",
    "corresponding_authors": "",
    "abstract": "A string-matching engine capable of inspecting multiple characters in parallel can multiply the throughput. However, the space required for implementing a matching engine that can process multiple characters in parallel generally grows exponentially with respect to the characters to be processed in parallel. Based on the Aho-Corasick algorithm (AC-algorithm), this work presents a novel multicharacter transition Nondeterministic Finite Automaton (NFA) approach, called multicharacter AC-NFA , to allow for the inspection of multiple characters in parallel. This approach first converts an AC-trie to an AC-NFA by allowing for the simultaneous activation of multiple states and then converts the AC-NFA to a k -character AC-NFA by an algorithm with concatenation operations and assistant transitions. Additionally, the alignment problem, which occurs while multiple characters are being inspected in parallel, is solved using assistant transitions. Moreover, a corresponding output is provided for each inspected character by introducing priority multiplexers to determine the final matching outputs during implementation of the multicharacter AC-NFA. Consequently, the number of derived k -character transitions grows linearly with respect to the number k . Furthermore, the derived multicharacter AC-NFA is implemented on FPGAs for evaluation. The resulting throughput grows approximately 14 times and the hardware cost grows about 18 times for 16-character AC-NFA implementation, as compared with that for 1-character AC-NFA implementation. The achievable throughput is 21.4Gbps for the 16-character AC-NFA implementation operating at a 167.36MHz clock.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2030995323",
    "type": "article"
  },
  {
    "title": "Exploiting reuse locality on inclusive shared last-level caches",
    "doi": "https://doi.org/10.1145/2400682.2400697",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Jorge Albericio; Pablo Ibáñez; Víctor Viñals; J.M. Llaberia",
    "corresponding_authors": "",
    "abstract": "Optimization of the replacement policy used for Shared Last-Level Cache (SLLC) management in a Chip-MultiProcessor (CMP) is critical for avoiding off-chip accesses. Temporal locality, while being exploited by first levels of private cache memories, is only slightly exhibited by the stream of references arriving at the SLLC. Thus, traditional replacement algorithms based on recency are bad choices for governing SLLC replacement. Recent proposals involve SLLC replacement policies that attempt to exploit reuse either by segmenting the replacement list or improving the rereference interval prediction. On the other hand, inclusive SLLCs are commonplace in the CMP market, but the interaction between replacement policy and the enforcement of inclusion has barely been discussed. After analyzing that interaction, this article introduces two simple replacement policies exploiting reuse locality and targeting inclusive SLLCs: Least Recently Reused (LRR) and Not Recently Reused (NRR). NRR has the same implementation cost as NRU, and LRR only adds one bit per line to the LRU cost. After considering reuse locality and its interaction with the invalidations induced by inclusion, the proposals are evaluated by simulating multiprogrammed workloads in an 8-core system with two private cache levels and an SLLC. LRR outperforms LRU by 4.5% (performing better in 97 out of 100 mixes) and NRR outperforms NRU by 4.2% (performing better in 99 out of 100 mixes). We also show that our mechanisms outperform rereference interval prediction, a recently proposed SLLC replacement policy and that similar conclusions can be drawn by varying the associativity or the SLLC size.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2061779882",
    "type": "article"
  },
  {
    "title": "Toward high-throughput algorithms on many-core architectures",
    "doi": "https://doi.org/10.1145/2086696.2086728",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Daniel Orozco; Elkin Garcia; Rishi Khan; Kelly Livingston; Guang R. Gao",
    "corresponding_authors": "",
    "abstract": "Advanced many-core CPU chips already have a few hundreds of processing cores (e.g., 160 cores in an IBM Cyclops-64 chip) and more and more processing cores become available as computer architecture progresses. The underlying runtime systems of such architectures need to efficiently serve hundreds of processors at the same time, requiring all basic data structures within the runtime to maintain unprecedented throughput. In this paper, we analyze the throughput requirements that must be met by algorithms in runtime systems to be able to handle hundreds of simultaneous operations in real time. We reach a surprising conclusion: Many traditional algorithm techniques are poorly suited for highly parallel computing environments because of their low throughput. We reach the conclusion that the intrinsic throughput of a parallel program depends on both its algorithm and the processor architecture where the program runs. We provide theory to quantify the intrinsic throughput of algorithms, and we provide a few examples, where we describe the intrinsic throughput of existing, common algorithms. Then, we go on to explain how to follow a throughput-oriented approach to develop algorithms that have very high intrinsic throughput in many core architectures. We compare our throughput-oriented algorithms with other well known algorithms that provide the same functionality and we show that a throughput-oriented design produces algorithms with equal or faster performance in highly concurrent environments. We provide both theoretical and experimental evidence showing that our algorithms are excellent choices over other state of the art algorithms. The major contributions of this paper are (1) motivating examples that show the importance of throughput in concurrent algorithms; (2) a mathematical framework that uses queueing theory to describe the intrinsic throughput of algorithms; (3) two highly concurrent algorithms with very high intrinsic throughput that are useful for task management in runtime systems; and (4) extensive experimental and theoretical results that show that for highly parallel systems, our proposed algorithms allow greater or at least equal scalability and performance than other well-known similar state-of-the-art algorithms.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2029419996",
    "type": "article"
  },
  {
    "title": "Adaptive timekeeping replacement",
    "doi": "https://doi.org/10.1145/1952998.1953001",
    "publication_date": "2011-02-05",
    "publication_year": 2011,
    "authors": "Carole-Jean Wu; Margaret Martonosi",
    "corresponding_authors": "",
    "abstract": "In chip multiprocessors (CMPs), several high-performance cores typically compete for capacity in a shared last-level cache. This causes degraded and unpredictable memory performance for multiprogrammed and parallel workloads. In response, recent schemes apportion cache bandwidth and capacity in ways that offer better aggregate performance for the workloads. These schemes, however, focus primarily on relatively coarse-grained capacity management without concern for operating system process priority levels. In this work, we explore capacity management approaches that are both temporally and spatially more fine-grained than prior work. We also consider operating system priority levels as part of capacity management. We propose a capacity management mechanism based on timekeeping techniques that track the time interval since the last access to cached data. This Adaptive Timekeeping Replacement (ATR) scheme maintains aggregate cache occupancies that reflect the priority and footprint of each application. The key novelties of our work are (1) ATR offers a complete cache capacity management framework taking into account application priorities and memory characteristics, and (2) ATR's fine-grained cache capacity control is demonstrated to be effective and important in improving the performance of parallel workloads in addition to sequential ones. We evaluate our ideas using a full-system simulator and multiprogrammed workloads of both sequential and parallel applications. This is the first detailed study of shared cache capacity management considering thread behaviors in parallel applications. ATR outperforms an unmanaged system by as much as 1.63X and by an average of 1.19X. ATR's fine-grained temporal control is particularly important for parallel applications, which are expected to be increasingly prevalent in years to come.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2074303271",
    "type": "article"
  },
  {
    "title": "Endurance-aware cache line management for non-volatile caches",
    "doi": "https://doi.org/10.1145/2579671",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "Jue Wang; Xiangyu Dong; Yuan Xie; Norman P. Jouppi",
    "corresponding_authors": "",
    "abstract": "Nonvolatile memories (NVMs) have the potential to replace low-level SRAM or eDRAM on-chip caches because NVMs save standby power and provide large cache capacity. However, limited write endurance is a common problem for NVM technologies, and today's cache management might result in unbalanced cache write traffic, causing heavily written cache blocks to fail much earlier than others. Although wear-leveling techniques for NVM-based main memories exist, we cannot simply apply them to NVM-based caches. This is because cache writes have intraset variations as well as interset variations, while writes to main memories only have interset variations. To solve this problem, we propose i 2 WAP, a new cache management policy that can reduce both inter- and intraset write variations. i 2 WAP has two features: Swap-Shift, an enhancement based on existing main memory wear leveling to reduce cache interset write variations, and Probabilistic Set Line Flush, a novel technique to reduce cache intraset write variations. Implementing i 2 WAP only needs two global counters and two global registers. In one of our studies, i 2 WAP can improve the NVM cache lifetime by 75% on average and up to 224%. We also validate that i 2 WAP is effective in systems with different cache configurations and workloads.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2076739924",
    "type": "article"
  },
  {
    "title": "Falcon",
    "doi": "https://doi.org/10.1145/2842618",
    "publication_date": "2015-12-22",
    "publication_year": 2015,
    "authors": "Unnikrishnan Cheramangalath; Rupesh Nasre; Y. N. Srikant",
    "corresponding_authors": "",
    "abstract": "Graph algorithms have been shown to possess enough parallelism to keep several computing resources busy—even hundreds of cores on a GPU. Unfortunately, tuning their implementation for efficient execution on a particular hardware configuration of heterogeneous systems consisting of multicore CPUs and GPUs is challenging, time consuming, and error prone. To address these issues, we propose a domain-specific language (DSL), Falcon, for implementing graph algorithms that (i) abstracts the hardware, (ii) provides constructs to write explicitly parallel programs at a higher level, and (iii) can work with general algorithms that may change the graph structure (morph algorithms). We illustrate the usage of our DSL to implement local computation algorithms (that do not change the graph structure) and morph algorithms such as Delaunay mesh refinement, survey propagation, and dynamic SSSP on GPU and multicore CPUs. Using a set of benchmark graphs, we illustrate that the generated code performs close to the state-of-the-art hand-tuned implementations.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2271498114",
    "type": "article"
  },
  {
    "title": "Improving MLC PCM Performance through Relaxed Write and Read for Intermediate Resistance Levels",
    "doi": "https://doi.org/10.1145/3177965",
    "publication_date": "2018-03-22",
    "publication_year": 2018,
    "authors": "Saeed Rashidi; Majid Jalili; Hamid Sarbazi‐Azad",
    "corresponding_authors": "",
    "abstract": "Phase Change Memory (PCM) is one of the most promising candidates to be used at the main memory level of the memory hierarchy due to poor scalability, considerable leakage power, and high cost/bit of DRAM. PCM is a new resistive memory that is capable of storing data based on resistance values. The wide resistance range of PCM allows for storing multiple bits per cell (MLC) rather than a single bit per cell (SLC). Unfortunately, higher density of MLC PCM comes at the expense of longer read/write latency, higher soft error rate, higher energy consumption, and earlier wearout compared to the SLC PCM. Some studies suggest removing the most error-prone level to mitigate soft error and write latency of MLC PCM, hence introducing a less dense memory called Tri-Level memory. Another scheme, called M-Metric, proposes a new read metric to address the soft error problem in MLC PCM. In order to deal with the limited lifetime of PCM, some extra storage per memory line is required to correct permanent hard errors (stuck-at faults). Since the extra storage is used only when permanent faults occur, it has a low utilization for a long time before hard errors start to occur. In this article, we utilize the extra storage to improve the read/write latency in a 2-bit MLC PCM using a relaxation scheme for reading and writing the cells for intermediate resistance levels. More specifically, we combine the most time-consuming levels (intermediate resistance levels) to reduce the number of resistance levels (making a Tri-Level PCM) and therefore improve write latency. We then store some error correction metadata in the extra storage section to successfully retrieve the exact data values in the read operation. We also modify the Tri-Level PCM cell to increase its read latency when the M-Metric scheme is used. Evaluation results show that the proposed scheme improves read latency by 57.2%, write latency by 56.1%, and overall system performance (IPC) by 26.9% over the baseline. It is noteworthy that combining the proposed scheme and FPC compression method improves read latency by 75.2%, write latency by 67%, and overall system performance (IPC) by 37.4%.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2792868738",
    "type": "article"
  },
  {
    "title": "ARI",
    "doi": "https://doi.org/10.1145/2543697",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Viacheslav V. Fedorov; Sheng Qiu; A. L. Narasimha Reddy; Paul V. Gratz",
    "corresponding_authors": "",
    "abstract": "Decreasing the traffic from the CPU LLC to main memory is a very important issue in modern systems. Recent work focuses on cache misses, overlooking the impact of writebacks on the total memory traffic, energy consumption, IPC, and so forth. Policies that foster a balanced approach, between reducing write traffic to memory and improving miss rates, can increase overall performance and improve energy efficiency and memory system lifetime for NVM memory technology, such as phase-change memory (PCM). We propose Adaptive Replacement and Insertion (ARI), an adaptive approach to last-level CPU cache management, optimizing the two parameters (miss rate and writeback rate) simultaneously. Our specific focus is to reduce writebacks as much as possible while maintaining or improving the miss rate relative to conventional LRU replacement policy. ARI reduces LLC writebacks by 33%, on average, while also decreasing misses by 4.7%, on average. In a typical system, this boosts IPC by 4.9%, on average, while decreasing energy consumption by 8.9%. These results are achieved with minimal hardware overheads.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W1986400496",
    "type": "article"
  },
  {
    "title": "Mechanistic Analytical Modeling of Superscalar In-Order Processor Performance",
    "doi": "https://doi.org/10.1145/2678277",
    "publication_date": "2015-01-09",
    "publication_year": 2015,
    "authors": "Maximilien Breughe; Stijn Eyerman; Lieven Eeckhout",
    "corresponding_authors": "",
    "abstract": "Superscalar in-order processors form an interesting alternative to out-of-order processors because of their energy efficiency and lower design complexity. However, despite the reduced design complexity, it is nontrivial to get performance estimates or insight in the application--microarchitecture interaction without running slow, detailed cycle-level simulations, because performance highly depends on the order of instructions within the application’s dynamic instruction stream, as in-order processors stall on interinstruction dependences and functional unit contention. To limit the number of detailed cycle-level simulations needed during design space exploration, we propose a mechanistic analytical performance model that is built from understanding the internal mechanisms of the processor. The mechanistic performance model for superscalar in-order processors is shown to be accurate with an average performance prediction error of 3.2% compared to detailed cycle-accurate simulation using gem5. We also validate the model against hardware, using the ARM Cortex-A8 processor and show that it is accurate within 10% on average. We further demonstrate the usefulness of the model through three case studies: (1) design space exploration, identifying the optimum number of functional units for achieving a given performance target; (2) program--machine interactions, providing insight into microarchitecture bottlenecks; and (3) compiler--architecture interactions, visualizing the impact of compiler optimizations on performance.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2087838646",
    "type": "article"
  },
  {
    "title": "Measuring Microarchitectural Details of Multi- and Many-Core Memory Systems through Microbenchmarking",
    "doi": "https://doi.org/10.1145/2687356",
    "publication_date": "2015-01-09",
    "publication_year": 2015,
    "authors": "Zhenman Fang; Sanyam Mehta; Pen-Chung Yew; Antonia Zhai; James Greensky; Gautham Beeraka; Binyu Zang",
    "corresponding_authors": "",
    "abstract": "As multicore and many-core architectures evolve, their memory systems are becoming increasingly more complex. To bridge the latency and bandwidth gap between the processor and memory, they often use a mix of multilevel private/shared caches that are either blocking or nonblocking and are connected by high-speed network-on-chip. Moreover, they also incorporate hardware and software prefetching and simultaneous multithreading (SMT) to hide memory latency. On such multi- and many-core systems, to incorporate various memory optimization schemes using compiler optimizations and performance tuning techniques, it is crucial to have microarchitectural details of the target memory system. Unfortunately, such details are often unavailable from vendors, especially for newly released processors. In this article, we propose a novel microbenchmarking methodology based on short elapsed-time events (SETEs) to obtain comprehensive memory microarchitectural details in multi- and many-core processors. This approach requires detailed analysis of potential interfering factors that could affect the intended behavior of such memory systems. We lay out effective guidelines to control and mitigate those interfering factors. Taking the impact of SMT into consideration, our proposed methodology not only can measure traditional cache/memory latency and off-chip bandwidth but also can uncover the details of software and hardware prefetching units not attempted in previous studies. Using the newly released Intel Xeon Phi many-core processor (with in-order cores) as an example, we show how we can use a set of microbenchmarks to determine various microarchitectural features of its memory system (many are undocumented from vendors). To demonstrate the portability and validate the correctness of such a methodology, we use the well-documented Intel Sandy Bridge multicore processor (with out-of-order cores) as another example, where most data are available and can be validated. Moreover, to illustrate the usefulness of the measured data, we do a multistage coordinated data prefetching case study on both Xeon Phi and Sandy Bridge and show that by using the measured data, we can achieve 1.3X and 1.08X performance speedup, respectively, compared to the state-of-the-art Intel ICC compiler. We believe that these measurements also provide useful insights into memory optimization, analysis, and modeling of such multicore and many-core architectures.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2112532446",
    "type": "article"
  },
  {
    "title": "Using Template Matching to Infer Parallel Design Patterns",
    "doi": "https://doi.org/10.1145/2688905",
    "publication_date": "2015-01-09",
    "publication_year": 2015,
    "authors": "Zia Ul Huda; Ali Jannesari; Felix Wolf",
    "corresponding_authors": "",
    "abstract": "The triumphant spread of multicore processors over the past decade increases the pressure on software developers to exploit the growing amount of parallelism available in the hardware. However, writing parallel programs is generally challenging. For sequential programs, the formulation of design patterns marked a turning point in software development, boosting programmer productivity and leading to more reusable and maintainable code. While the literature is now also reporting a rising number of parallel design patterns, programmers confronted with the task of parallelizing an existing sequential program still struggle with the question of which parallel pattern to apply where in their code. In this article, we show how template matching, a technique traditionally used in the discovery of sequential design patterns, can also be used to support parallelization decisions. After looking for matches in a previously extracted dynamic dependence graph, we classify code blocks of the input program according to the structure of the parallel patterns we find. Based on this information, the programmer can easily implement the detected pattern and create a parallel version of his or her program. We tested our approach with six programs, in which we successfully detected pipeline and do-all patterns.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2140131713",
    "type": "article"
  },
  {
    "title": "Snippets",
    "doi": "https://doi.org/10.1145/2764907",
    "publication_date": "2015-06-24",
    "publication_year": 2015,
    "authors": "Doug Simon; Christian Wimmer; Bernhard Urban; Gilles Duboscq; Lukas Stadler; Thomas Würthinger",
    "corresponding_authors": "",
    "abstract": "When building a compiler for a high-level language, certain intrinsic features of the language must be expressed in terms of the resulting low-level operations. Complex features are often expressed by explicitly weaving together bits of low-level IR, a process that is tedious, error prone, difficult to read, difficult to reason about, and machine dependent. In the Graal compiler for Java, we take a different approach: we use snippets of Java code to express semantics in a high-level, architecture-independent way. Two important restrictions make snippets feasible in practice: they are compiler specific, and they are explicitly prepared and specialized. Snippets make Graal simpler and more portable while still capable of generating machine code that can compete with other compilers of the Java HotSpot VM.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2197714735",
    "type": "article"
  },
  {
    "title": "Optimizing Indirect Branches in Dynamic Binary Translators",
    "doi": "https://doi.org/10.1145/2866573",
    "publication_date": "2016-04-05",
    "publication_year": 2016,
    "authors": "Amanieu d'Antras; Cosmin Gorgovan; Jim Garside; Mikel Luján",
    "corresponding_authors": "",
    "abstract": "Dynamic binary translation is a technology for transparently translating and modifying a program at the machine code level as it is running. A significant factor in the performance of a dynamic binary translator is its handling of indirect branches. Unlike direct branches, which have a known target at translation time, an indirect branch requires translating a source program counter address to a translated program counter address every time the branch is executed. This translation can impose a serious runtime penalty if it is not handled efficiently. MAMBO-X64, a dynamic binary translator that translates 32-bit ARM (AArch32) code to 64-bit ARM (AArch64) code, uses three novel techniques to improve the performance of indirect branch translation. Together, these techniques allow MAMBO-X64 to achieve a very low performance overhead of only 10% on average compared to native execution of 32-bit programs. Hardware-assisted function returns use a software return address stack to predict the targets of function returns, making use of several novel optimizations while also exploiting hardware return address prediction. This technique has a significant impact on most benchmarks, reducing binary translation overhead compared to native execution by 40% on average and by 90% on some benchmarks. Branch table inference , an algorithm for detecting and translating branch tables, can reduce the overhead of translated code by up to 40% on some SPEC CPU2006 benchmarks. The remaining indirect branches are handled using a fast atomic hash table , which is optimized to work with multiple threads. This last technique translates indirect branches using a single shared hash table while avoiding expensive synchronization in performance-critical lookup code. This allows the performance to be on par with thread-private hash tables while having superior memory scalability.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2325071780",
    "type": "article"
  },
  {
    "title": "Optimizing Convolutional Neural Networks on the Sunway TaihuLight Supercomputer",
    "doi": "https://doi.org/10.1145/3177885",
    "publication_date": "2018-03-22",
    "publication_year": 2018,
    "authors": "Wenlai Zhao; Haohuan Fu; Jiarui Fang; Weijie Zheng; Lin Gan; Guangwen Yang",
    "corresponding_authors": "",
    "abstract": "The Sunway TaihuLight supercomputer is powered by SW26010, a new 260-core processor designed with on-chip fusion of heterogeneous cores. In this article, we present our work on optimizing the training process of convolutional neural networks (CNNs) on the Sunway TaihuLight supercomputer. Specifically, a highly efficient library (swDNN) and a customized Caffe framework (swCaffe) are proposed. Architecture-oriented optimization methods targeting the many-core architecture of SW26010 are introduced and are able to achieve 48× speedup for the convolution routine in swDNN and 4× speedup for the complete training process of the VGG-16 network using swCaffe, compared to the unoptimized algorithm and framework. Compared to the cuDNN library and the Caffe framework based on the NVIDIA K40m GPU, the proposed swDNN library and swCaffe framework on SW26010 have nearly half the performance of K40m in single -precision and have 3.6× and 1.8× speedup over K40m in double precision, respectively.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2790601347",
    "type": "article"
  },
  {
    "title": "NUMA-Caffe",
    "doi": "https://doi.org/10.1145/3199605",
    "publication_date": "2018-06-08",
    "publication_year": 2018,
    "authors": "Probir Roy; Shuaiwen Leon Song; Sriram Krishnamoorthy; Abhinav Vishnu; Dipanjan Sengupta; Xu Liu",
    "corresponding_authors": "",
    "abstract": "Convolution Neural Networks (CNNs), a special subcategory of Deep Learning Neural Networks (DNNs), have become increasingly popular in industry and academia for their powerful capability in pattern classification, image processing, and speech recognition. Recently, they have been widely adopted in High Performance Computing (HPC) environments for solving complex problems related to modeling, runtime prediction, and big data analysis. Current state-of-the-art designs for DNNs on modern multi- and many-core CPU architectures, such as variants of Caffe, have reported promising performance in speedup and scalability, comparable with the GPU implementations. However, modern CPU architectures employ Non-Uniform Memory Access (NUMA) technique to integrate multiple sockets, which incurs unique challenges for designing highly efficient CNN frameworks. Without a careful design, DNN frameworks can easily suffer from long memory latency due to a large number of memory accesses to remote NUMA domains, resulting in poor scalability. To address this challenge, we propose NUMA-aware multi-solver-based CNN design, named NUMA-Caffe , for accelerating deep learning neural networks on multi- and many-core CPU architectures. NUMA-Caffe is independent of DNN topology, does not impact network convergence rates, and provides superior scalability to the existing Caffe variants. Through a thorough empirical study on four contemporary NUMA-based multi- and many-core architectures, our experimental results demonstrate that NUMA-Caffe significantly outperforms the state-of-the-art Caffe designs in terms of both throughput and scalability.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2808072032",
    "type": "article"
  },
  {
    "title": "Exploiting Bank Conflict-based Side-channel Timing Leakage of GPUs",
    "doi": "https://doi.org/10.1145/3361870",
    "publication_date": "2019-11-18",
    "publication_year": 2019,
    "authors": "Zhen Hang Jiang; Yunsi Fei; David Kaeli",
    "corresponding_authors": "",
    "abstract": "To prevent information leakage during program execution, modern software cryptographic implementations target constant-time function, where the number of instructions executed remains the same when program inputs change. However, the underlying microarchitecture behaves differently when processing different data inputs, impacting the execution time of the same instructions. These differences in execution time can covertly leak confidential information through a timing channel. Given the recent reports of covert channels present on commercial microprocessors, a number of microarchitectural features on CPUs have been re-examined from a timing leakage perspective. Unfortunately, a similar microarchitectural evaluation of the potential attack surfaces on GPUs has not been adequately performed. Several prior work has considered a timing channel based on the behavior of a GPU’s coalescing unit. In this article, we identify a second finer-grained microarchitectural timing channel, related to the banking structure of the GPU’s Shared Memory. By considering the timing channel caused by Shared Memory bank conflicts, we have developed a differential timing attack that can compromise table-based cryptographic algorithms. We implement our timing attack on an Nvidia Kepler K40 GPU and successfully recover the complete 128-bit encryption key of an Advanced Encryption Standard (AES) GPU implementation using 900,000 timing samples. We also evaluate the scalability of our attack method by attacking an implementation of the AES encryption algorithm that fully occupies the compute resources of the GPU. We extend our timing analysis onto other Nvidia architectures: Maxwell, Pascal, Volta, and Turing GPUs. We also discuss countermeasures and experiment with a novel multi-key implementation, evaluating its resistance to our side-channel timing attack and its associated performance overhead.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2983757913",
    "type": "article"
  },
  {
    "title": "Compiler/Runtime Framework for Dynamic Dataflow Parallelization of Tiled Programs",
    "doi": "https://doi.org/10.1145/2687652",
    "publication_date": "2015-01-09",
    "publication_year": 2015,
    "authors": "Martin Kong; Antoniu Pop; Louis-Noël Pouchet; R. Govindarajan; Albert Cohen; P. Sadayappan",
    "corresponding_authors": "",
    "abstract": "Task-parallel languages are increasingly popular. Many of them provide expressive mechanisms for intertask synchronization. For example, OpenMP 4.0 will integrate data-driven execution semantics derived from the StarSs research language. Compared to the more restrictive data-parallel and fork-join concurrency models, the advanced features being introduced into task-parallel models in turn enable improved scalability through load balancing, memory latency hiding, mitigation of the pressure on memory bandwidth, and, as a side effect, reduced power consumption. In this article, we develop a systematic approach to compile loop nests into concurrent, dynamically constructed graphs of dependent tasks. We propose a simple and effective heuristic that selects the most profitable parallelization idiom for every dependence type and communication pattern. This heuristic enables the extraction of interband parallelism (cross-barrier parallelism) in a number of numerical computations that range from linear algebra to structured grids and image processing. The proposed static analysis and code generation alleviates the burden of a full-blown dependence resolver to track the readiness of tasks at runtime. We evaluate our approach and algorithms in the PPCG compiler, targeting OpenStream, a representative dataflow task-parallel language with explicit intertask dependences and a lightweight runtime. Experimental results demonstrate the effectiveness of the approach.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2050626467",
    "type": "article"
  },
  {
    "title": "BPM/BPM+",
    "doi": "https://doi.org/10.1145/2579672",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "Lei Liu; Zehan Cui; Yong Li; Yungang Bao; Mingyu Chen; Chengyong Wu",
    "corresponding_authors": "",
    "abstract": "The main memory system is a shared resource in modern multicore machines that can result in serious interference leading to reduced throughput and unfairness. Many new memory scheduling mechanisms have been proposed to address the interference problem. However, these mechanisms usually employ relative complex scheduling logic and need modifications to Memory Controllers (MCs), which incur expensive hardware design and manufacturing overheads. This article presents a practical software approach to effectively eliminate the interference without any hardware modifications. The key idea is to modify the OS memory management system and adopt a page-coloring-based Bank-level Partitioning Mechanism (BPM) that allocates dedicated DRAM banks to each core (or thread). By using BPM, memory requests from distinct programs are segregated across multiple memory banks to promote locality/fairness and reduce interference. We further extend BPM to BPM+ by incorporating channel-level partitioning, on which we demonstrate additional gain over BPM in many cases. To achieve benefits in the presence of diverse application memory needs and avoid performance degradation due to resource underutilization, we propose a dynamic mechanism upon BPM/BPM+ that assigns appropriate bank/channel resources based on application memory/bandwidth demands monitored through PMU (performance-monitoring unit) and a low-overhead OS page table scanning process. We implement BPM/BPM+ in Linux 2.6.32.15 kernel and evaluate the technique on four-core and eight-core real machines by running a large amount of randomly generated multiprogrammed and multithreaded workloads. Experimental results show that BPM/BPM+ can improve the overall system throughput by 4.7%/5.9%, on average, (up to 8.6%/9.5%) and reduce the unfairness by an average of 4.2%/6.1% (up to 15.8%/13.9%).",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2099280458",
    "type": "article"
  },
  {
    "title": "Citadel",
    "doi": "https://doi.org/10.1145/2840807",
    "publication_date": "2016-01-06",
    "publication_year": 2016,
    "authors": "Prashant J. Nair; David Roberts; Moinuddin K. Qureshi",
    "corresponding_authors": "",
    "abstract": "Stacked memory modules are likely to be tightly integrated with the processor. It is vital that these memory modules operate reliably, as memory failure can require the replacement of the entire socket. To make matters worse, stacked memory designs are susceptible to newer failure modes (e.g., due to faulty through-silicon vias, or TSVs) that can cause large portions of memory, such as a bank, to become faulty. To avoid data loss from large-granularity failures, the memory system may use symbol-based codes that stripe the data for a cache line across several banks (or channels). Unfortunately, such data-striping reduces memory-level parallelism, causing significant slowdown and higher power consumption. This article proposes Citadel , a robust memory architecture that allows the memory system to retain each cache line within one bank. By retaining cache lines within banks, Citadel enables a high-performance and low-power memory system and also efficiently protects the stacked memory system from large-granularity failures. Citadel consists of three components; TSV-Swap , which can tolerate both faulty data-TSVs and faulty address-TSVs; Tri-Dimensional Parity (3DP), which can tolerate column failures, row failures, and bank failures; and Dynamic Dual-Granularity Sparing (DDS) , which can mitigate permanent faults by dynamically sparing faulty memory regions either at a row granularity or at a bank granularity. Our evaluations with real-world data for DRAM failures show that Citadel provides performance and power similar to maintaining the entire cache line in the same bank, and yet provides 700 × higher reliability than ChipKill-like ECC codes.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2233147800",
    "type": "article"
  },
  {
    "title": "A Framework for Application-Guided Task Management on Heterogeneous Embedded Systems",
    "doi": "https://doi.org/10.1145/2835177",
    "publication_date": "2015-12-08",
    "publication_year": 2015,
    "authors": "Francisco Gaspar; Luís Taniça; Pedro Tomás; Aleksandar Ilić; Leonel Sousa",
    "corresponding_authors": "",
    "abstract": "In this article, we propose a general framework for fine-grain application-aware task management in heterogeneous embedded platforms, which allows integration of different mechanisms for an efficient resource utilization, frequency scaling, and task migration. The proposed framework incorporates several components for accurate runtime monitoring by relying on the OS facilities and performance self-reporting for parallel and iterative applications. The framework efficiency is experimentally evaluated on a real hardware platform, where significant power and energy savings are attained for SPEC CPU2006 and PARSEC benchmarks, by guiding frequency scaling and intercluster migrations according to the runtime application behavior and predefined performance targets.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2249228740",
    "type": "article"
  },
  {
    "title": "The Polyhedral Model of Nonlinear Loops",
    "doi": "https://doi.org/10.1145/2838734",
    "publication_date": "2015-12-08",
    "publication_year": 2015,
    "authors": "Aravind Sukumaran-Rajam; Philippe Clauss",
    "corresponding_authors": "",
    "abstract": "Runtime code optimization and speculative execution are becoming increasingly prominent to leverage performance in the current multi- and many-core era. However, a wider and more efficient use of such techniques is mainly hampered by the prohibitive time overhead induced by centralized data race detection, dynamic code behavior modeling, and code generation. Most of the existing Thread Level Speculation (TLS) systems rely on naively slicing the target loops into chunks and trying to execute the chunks in parallel with the help of a centralized performance-penalizing verification module that takes care of data races. Due to the lack of a data dependence model, these speculative systems are not capable of doing advanced transformations, and, more importantly, the chances of rollback are high. The polyhedral model is a well-known mathematical model to analyze and optimize loop nests. The current state-of-art tools limit the application of the polyhedral model to static control codes. Thus, none of these tools can generally handle codes with while loops, indirect memory accesses, or pointers. Apollo (Automatic POLyhedral Loop Optimizer) is a framework that goes one step beyond and applies the polyhedral model dynamically by using TLS. Apollo can predict, at runtime, whether the codes are behaving linearly or not, and it applies polyhedral transformations on-the-fly. This article presents a novel system that enables Apollo to handle codes whose memory accesses and loop bounds are not necessarily linear. More generally, this approach expands the applicability of the polyhedral model at runtime to a wider class of codes. Plugging together both linear and nonlinear accesses to the dependence prediction model enables the application of polyhedral loop optimizing transformations even for nonlinear code kernels while also allowing a low-cost speculation verification.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2250964545",
    "type": "article"
  },
  {
    "title": "A Relational Theory of Locality",
    "doi": "https://doi.org/10.1145/3341109",
    "publication_date": "2019-08-20",
    "publication_year": 2019,
    "authors": "Liang Yuan; Chen Ding; Wesley Smith; Peter J. Denning; Yunquan Zhang",
    "corresponding_authors": "",
    "abstract": "In many areas of program and system analysis and optimization, locality is a common concept and has been defined and measured in many ways. This article aims to formally establish relations between these previously disparate types of locality. It categorizes locality definitions in three groups and shows whether and how they can be interconverted. For the footprint, a recent metric, it gives a new measurement algorithm that is asymptotically more time/space efficient than previous approaches. Using the conversion relations, the new algorithm derives with the same efficiency different locality metrics developed and used in program analysis, memory management, and cache design.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2969763349",
    "type": "article"
  },
  {
    "title": "Exploring Complex Brain-Simulation Workloads on Multi-GPU Deployments",
    "doi": "https://doi.org/10.1145/3371235",
    "publication_date": "2019-12-17",
    "publication_year": 2019,
    "authors": "Michiel van der Vlag; Georgios Smaragdos; Zaid Al-Ars; Christos Strydis",
    "corresponding_authors": "",
    "abstract": "In-silico brain simulations are the de-facto tools computational neuroscientists use to understand large-scale and complex brain-function dynamics. Current brain simulators do not scale efficiently enough to large-scale problem sizes (e.g., &gt;100,000 neurons) when simulating biophysically complex neuron models. The goal of this work is to explore the use of true multi-GPU acceleration through NVIDIA’s GPUDirect technology on computationally challenging brain models and to assess their scalability. The brain model used is a state-of-the-art, extended Hodgkin-Huxley, biophysically meaningful, three-compartmental model of the inferior-olivary nucleus. The Hodgkin-Huxley model is the most widely adopted conductance-based neuron representation, and thus the results from simulating this representative workload are relevant for many other brain experiments. Not only the actual network-simulation times but also the network-setup times were taken into account when designing and benchmarking the multi-GPU version, an aspect often ignored in similar previous work. Network sizes varying from 65K to 2M cells, with 10 and 1,000 synapses per neuron were executed on 8, 16, 24, and 32 GPUs. Without loss of generality, simulations were run for 100 ms of biological time. Findings indicate that communication overheads do not dominate overall execution while scaling the network size up is computationally tractable. This scalable design proves that large-network simulations of complex neural models are possible using a multi-GPU design with GPUDirect.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2994884340",
    "type": "article"
  },
  {
    "title": "FinPar",
    "doi": "https://doi.org/10.1145/2898354",
    "publication_date": "2016-06-27",
    "publication_year": 2016,
    "authors": "Christian Andreetta; Vivien Bégot; Jost Berthold; Martin Elsman; Fritz Henglein; Troels Henriksen; Maj-Britt Nordfang; Cosmin E. Oancea",
    "corresponding_authors": "",
    "abstract": "Commodity many-core hardware is now mainstream, but parallel programming models are still lagging behind in efficiently utilizing the application parallelism. There are (at least) two principal reasons for this. First, real-world programs often take the form of a deeply nested composition of parallel operators, but mapping the available parallelism to the hardware requires a set of transformations that are tedious to do by hand and beyond the capability of the common user. Second, the best optimization strategy, such as what to parallelize and what to efficiently sequentialize, is often sensitive to the input dataset and therefore requires multiple code versions that are optimized differently, which also raises maintainability problems. This article presents three array-based applications from the financial domain that are suitable for gpgpu execution. Common benchmark-design practice has been to provide the same code for the sequential and parallel versions that are optimized for only one class of datasets. In comparison, we document (1) all available parallelism via nested map-reduce functional combinators, in a simple Haskell implementation that closely resembles the original code structure, (2) the invariants and code transformations that govern the main trade-offs of a data-sensitive optimization space, and (3) report target cpu and multiversion gpgpu code together with an evaluation that demonstrates optimization trade-offs and other difficulties. We believe that this work provides useful insight into the language constructs and compiler infrastructure capable of expressing and optimizing such applications, and we report in-progress work in this direction.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2472217442",
    "type": "article"
  },
  {
    "title": "Hardware-Assisted Thread and Data Mapping in Hierarchical Multicore Architectures",
    "doi": "https://doi.org/10.1145/2975587",
    "publication_date": "2016-09-17",
    "publication_year": 2016,
    "authors": "Eduardo H. M. Cruz; Matthias Diener; Laércio Lima Pilla; Philippe O. A. Navaux",
    "corresponding_authors": "",
    "abstract": "The performance and energy efficiency of modern architectures depend on memory locality, which can be improved by thread and data mappings considering the memory access behavior of parallel applications. In this article, we propose intense pages mapping, a mechanism that analyzes the memory access behavior using information about the time the entry of each page resides in the translation lookaside buffer. It provides accurate information with a very low overhead. We present experimental results with simulation and real machines, with average performance improvements of 13.7% and energy savings of 4.4%, which come from reductions in cache misses and interconnection traffic.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2521259554",
    "type": "article"
  },
  {
    "title": "Dynamic Precision Autotuning with TAFFO",
    "doi": "https://doi.org/10.1145/3388785",
    "publication_date": "2020-05-29",
    "publication_year": 2020,
    "authors": "Stefano Cherubin; Daniele Cattaneo; Michele Chiari; Giovanni Agosta",
    "corresponding_authors": "",
    "abstract": "Many classes of applications, both in the embedded and high performance domains, can trade off the accuracy of the computed results for computation performance. One way to achieve such a trade-off is precision tuning—that is, to modify the data types used for the computation by reducing the bit width, or by changing the representation from floating point to fixed point. We present a methodology for high-accuracy dynamic precision tuning based on the identification of input classes (i.e., classes of input datasets that benefit from similar optimizations). When a new input region is detected, the application kernels are re-compiled on the fly with the appropriate selection of parameters. In this way, we obtain a continuous optimization approach that enables the exploitation of the reduced precision computation while progressively exploring the solution space, thus reducing the time required by compilation overheads. We provide tools to support the automation of the runtime part of the solution, leaving to the user only the task of identifying the input classes. Our approach provides a significant performance boost (up to 320%) on the typical approximate computing benchmarks, without meaningfully affecting the accuracy of the result, since the error remains always below 3%.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W3044486916",
    "type": "article"
  },
  {
    "title": "PolyDL",
    "doi": "https://doi.org/10.1145/3433103",
    "publication_date": "2021-01-07",
    "publication_year": 2021,
    "authors": "Sanket Tavarageri; Alexander Heinecke; Sasikanth Avancha; Bharat Kaul; Gagandeep Goyal; Ramakrishna Upadrasta",
    "corresponding_authors": "",
    "abstract": "Deep Neural Networks (DNNs) have revolutionized many aspects of our lives. The use of DNNs is becoming ubiquitous, including in software for image recognition, speech recognition, speech synthesis, language translation, to name a few. The training of DNN architectures, however, is computationally expensive. Once the model is created, its use in the intended application—the inference task, is computationally heavy too and the inference needs to be fast for real time use. For obtaining high performance today, the code of Deep Learning (DL) primitives optimized for specific architectures by expert programmers exposed via libraries is the norm. However, given the constant emergence of new DNN architectures, creating hand optimized code is expensive, slow and is not scalable. To address this performance-productivity challenge, in this article we present compiler algorithms to automatically generate high-performance implementations of DL primitives that closely match the performance of hand optimized libraries. We develop novel data reuse analysis algorithms using the polyhedral model to derive efficient execution schedules automatically. In addition, because most DL primitives use some variant of matrix multiplication at their core, we develop a flexible framework where it is possible to plug in library implementations of the same in lieu of a subset of the loops. We show that such a hybrid compiler plus a minimal library-use approach results in state-of-the-art performance. We develop compiler algorithms to also perform operator fusions that reduce data movement through the memory hierarchy of the computer system. Using Convolution Neural Network (CNN) models and matrix multiplication operations, we demonstrate that our approach automatically creates high performing DNN building blocks whose performance matches the performance of hand-crafted kernels of Intel’s oneDNN library on high end CPUs. At the same time, our techniques take only a fraction of time (1/20 or less) compared to AutoTVM, a deep learning auto-tuner to create optimized implementations.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W3119880013",
    "type": "article"
  },
  {
    "title": "Performance Evaluation of Intel Optane Memory for Managed Workloads",
    "doi": "https://doi.org/10.1145/3451342",
    "publication_date": "2021-04-22",
    "publication_year": 2021,
    "authors": "Shoaib Akram",
    "corresponding_authors": "Shoaib Akram",
    "abstract": "Intel Optane memory offers non-volatility, byte addressability, and high capacity. It suits managed workloads that prefer large main memory heaps. We investigate Optane as the main memory for managed (Java) workloads, focusing on performance scalability. As the workload (core count) increases, we note Optane’s performance relative to DRAM. A few workloads incur a slight slowdown on Optane memory, which helps conserve limited DRAM capacity. Unfortunately, other workloads scale poorly beyond a few core counts. This article investigates scaling bottlenecks for Java workloads on Optane memory, analyzing the application, runtime, and microarchitectural interactions. Poorly scaling workloads allocate objects rapidly and access objects in Optane memory frequently. These characteristics slow down the mutator and substantially slow down garbage collection (GC). At the microarchitecture level, load, store, and instruction miss penalties rise. To regain performance, we partition heaps across DRAM and Optane memory, a hybrid that scales considerably better than Optane alone. We exploit state-of-the-art GC approaches to partition heaps. Unfortunately, existing GC approaches needlessly waste DRAM capacity because they ignore runtime behavior. This article also introduces performance impact-guided memory allocation (PIMA) for hybrid memories. PIMA maximizes Optane utilization, allocating in DRAM only if it improves performance. It estimates the performance impact of allocating heaps in either memory type by sampling. We target PIMA at graph analytics workloads, offering a novel performance estimation method and detailed evaluation. PIMA identifies workload phases that benefit from DRAM with high (94.33%) accuracy, incurring only a 2% sampling overhead. PIMA operates stand-alone or combines with prior approaches to offer new performance versus DRAM capacity trade-offs. This work opens up Optane memory to a real-life role as the main memory for Java workloads.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W3157154645",
    "type": "article"
  },
  {
    "title": "Gem5-X",
    "doi": "https://doi.org/10.1145/3461662",
    "publication_date": "2021-07-17",
    "publication_year": 2021,
    "authors": "Yasir Mahmood Qureshi; William Simon; Marina Zapater; Katzalin Olcoz; David Atienza",
    "corresponding_authors": "",
    "abstract": "The increasing adoption of smart systems in our daily life has led to the development of new applications with varying performance and energy constraints, and suitable computing architectures need to be developed for these new applications. In this article, we present gem5-X, a system-level simulation framework, based on gem-5, for architectural exploration of heterogeneous many-core systems. To demonstrate the capabilities of gem5-X, real-time video analytics is used as a case-study. It is composed of two kernels, namely, video encoding and image classification using convolutional neural networks (CNNs). First, we explore through gem5-X the benefits of latest 3D high bandwidth memory (HBM2) in different architectural configurations. Then, using a two-step exploration methodology, we develop a new optimized clustered-heterogeneous architecture with HBM2 in gem5-X for video analytics application. In this proposed clustered-heterogeneous architecture, ARMv8 in-order cluster with in-cache computing engine executes the video encoding kernel, giving 20% performance and 54% energy benefits compared to baseline ARM in-order and Out-of-Order systems, respectively. Furthermore, thanks to gem5-X, we conclude that ARM Out-of-Order clusters with HBM2 are the best choice to run visual recognition using CNNs, as they outperform DDR4-based system by up to 30% both in terms of performance and energy savings.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W3161571687",
    "type": "article"
  },
  {
    "title": "KernelFaRer",
    "doi": "https://doi.org/10.1145/3459010",
    "publication_date": "2021-06-28",
    "publication_year": 2021,
    "authors": "João P. L. de Carvalho; Braedy Kuzma; Ivan Korostelev; José Nelson Amaral; Christopher Barton; José E. Moreira; Guido Araújo",
    "corresponding_authors": "",
    "abstract": "Well-crafted libraries deliver much higher performance than code generated by sophisticated application programmers using advanced optimizing compilers. When a code pattern for which a well-tuned library implementation exists is found in the source code of an application, the highest performing solution is to replace the pattern with a call to the library. Idiom-recognition solutions in the past either required pattern matching machinery that was outside of the compilation framework or provided a very brittle solution that would fail even for minor variants in the pattern source code. This article introduces Kernel Find &amp; Replacer ( KernelFaRer ), an idiom recognizer implemented entirely in the existing LLVM compiler framework. The versatility of KernelFaRer is demonstrated by matching and replacing two linear algebra idioms, general matrix-matrix multiplication (GEMM), and symmetric rank-2k update (SYR2K). Both GEMM and SYR2K are used extensively in scientific computation, and GEMM is also a central building block for deep learning and computer graphics algorithms. The idiom recognition in KernelFaRer is much more robust than alternative solutions, has a much lower compilation overhead, and is fully integrated in the broadly used LLVM compilation tools. KernelFaRer replaces existing GEMM and SYR2K idioms with computations performed by BLAS, Eigen, MKL (Intel’s x86), ESSL (IBM’s PowerPC), and BLIS (AMD). Gains in performance that reach 2000× over hand-crafted source code compiled at the highest optimization level demonstrate that replacing application code with library call is a performant solution.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W3175004880",
    "type": "article"
  },
  {
    "title": "Register-Pressure-Aware Instruction Scheduling Using Ant Colony Optimization",
    "doi": "https://doi.org/10.1145/3505558",
    "publication_date": "2022-01-31",
    "publication_year": 2022,
    "authors": "Ghassan Shobaki; V. Scott Gordon; Paul McHugh; Théodore Dubois; Austin Kerbow",
    "corresponding_authors": "",
    "abstract": "This paper describes a new approach to register-pressure-aware instruction scheduling, using Ant Colony Optimization (ACO) . ACO is a nature-inspired optimization technique that researchers have successfully applied to NP-hard sequencing problems like the Traveling Salesman Problem (TSP) and its derivatives. In this work, we describe an ACO algorithm for solving the long-standing compiler optimization problem of balancing Instruction-Level Parallelism (ILP) and Register Pressure (RP) in pre-allocation instruction scheduling. Three different cost functions are studied for estimating RP during instruction scheduling. The proposed ACO algorithm is implemented in the LLVM open-source compiler, and its performance is evaluated experimentally on three different machines with three different instruction-set architectures: Intel x86, ARM, and AMD GPU. The proposed ACO algorithm is compared to an exact Branch-and-Bound (B&amp;B) algorithm proposed in previous work. On x86 and ARM, both algorithms are evaluated relative to LLVM's generic scheduler, while on the AMD GPU, the algorithms are evaluated relative to AMD's production scheduler. The experimental results show that using SPECrate 2017 Floating Point, the proposed algorithm gives geometric-mean improvements of 1.13% and 1.25% in execution speed on x86 and ARM, respectively, relative to the LLVM scheduler. Using PlaidML on an AMD GPU, it gives a geometric-mean improvement of 7.14% in execution speed relative to the AMD scheduler. The proposed ACO algorithm gives approximately the same execution-time results as the B&amp;B algorithm, with each algorithm outperforming the other on a substantial number of hard scheduling regions. ACO gives better results than B&amp;B on many large instances that B&amp;B times out on. Both ACO and B&amp;B outperform the LLVM algorithm on the CPU and the AMD algorithm on the GPU.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4210408109",
    "type": "article"
  },
  {
    "title": "High-performance Deterministic Concurrency Using <scp>Lingua Franca</scp>",
    "doi": "https://doi.org/10.1145/3617687",
    "publication_date": "2023-08-29",
    "publication_year": 2023,
    "authors": "Christian Menard; Marten Lohstroh; Soroush Bateni; Matthew Chorlian; Arthur Deng; Peter Donovan; Clément Fournier; Shaokai Lin; Felix Suchert; Tassilo Tanneberger; Hokeun Kim; Jerónimo Castrillón; Edward A. Lee",
    "corresponding_authors": "",
    "abstract": "Actor frameworks and similar reactive programming techniques are widely used for building concurrent systems. They promise to be efficient and scale well to a large number of cores or nodes in a distributed system. However, they also expose programmers to nondeterminism, which often makes implementations hard to understand, debug, and test. The recently proposed reactor model is a promising alternative that enables deterministic concurrency. In this article, we present an efficient, parallel implementation of reactors and demonstrate that the determinacy of reactors does not imply a loss in performance. To show this, we evaluate Lingua Franca (LF), a reactor-oriented coordination language. LF equips mainstream programming languages with a deterministic concurrency model that automatically takes advantage of opportunities to exploit parallelism. Our implementation of the Savina benchmark suite demonstrates that, in terms of execution time, the runtime performance of LF programs even exceeds popular and highly optimized actor frameworks. We compare against Akka and CAF, which LF outperforms by 1.86× and 1.42×, respectively.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4386254894",
    "type": "article"
  },
  {
    "title": "Efficient and flexible architectural support for dynamic monitoring",
    "doi": "https://doi.org/10.1145/1061267.1061269",
    "publication_date": "2005-03-01",
    "publication_year": 2005,
    "authors": "Yuanyuan Zhou; Pin Zhou; Feng Qin; Wei Liu; Josep Torrellas",
    "corresponding_authors": "",
    "abstract": "Recent impressive performance improvements in computer architecture have not led to significant gains in the case of debugging. Software debugging often relies on inserting run-time software checks. In many cases, however, it is hard to find the root cause of a bug. Moreover, program execution typically slows down significantly, often by 10--100 times.To address this problem, this paper introduces the intelligent watcher (iWatcher) , a novel architectural scheme to monitor dynamic execution automatically, flexibly, and with minimal overhead. iWatcher associates program-specified monitoring functions with memory locations. When any such location is accessed, the monitoring function is automatically triggered with low overhead. To further reduce overhead and support rollback, iWatcher can optionally leverage thread-level speculation (TLS). The iWatcher architecture can be used to detect various bugs, including buffer overflow, accessing freed locations, memory leaks, stack-smashing and value-invariant violations. To evaluate iWatcher, we use seven applications with various real and injected bugs. Our results show that iWatcher detects many more software bugs than Valgrind, a well-known open-source bug detector. Moreover, iWatcher only induces a 0.1--179% execution overhead, which is orders of magnitude less than Valgrind. Our sensitivity study shows that even with 20% of the dynamic loads monitored in a program, iWatcher adds only 72--182% overhead. Finally, TLS is effective at reducing overheads for programs with substantial monitoring.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2153484975",
    "type": "article"
  },
  {
    "title": "A SIMD optimization framework for retargetable compilers",
    "doi": "https://doi.org/10.1145/1509864.1509866",
    "publication_date": "2009-03-30",
    "publication_year": 2009,
    "authors": "Manuel Hohenauer; Felix Engel; Rainer Leupers; Gerd Ascheid; H. Meyr",
    "corresponding_authors": "",
    "abstract": "Retargetable C compilers are currently widely used to quickly obtain compiler support for new embedded processors and to perform early processor architecture exploration. A partially inherent problem of the retargetable compilation approach, though, is the limited code quality as compared to hand-written compilers or assembly code due to the lack of dedicated optimizations techniques. This problem can be circumvented by designing flexible, retargetable code optimization techniques that apply to a certain range of target architectures. This article focuses on target machines with SIMD instruction support, a common feature in embedded processors for multimedia applications. However, SIMD optimization is known to be a difficult task since SIMD architectures are largely nonuniform, support only a limited set of data types and impose several memory alignment constraints. Additionally, such techniques require complicated loop transformations, which are tailored to the SIMD architecture in order to exhibit the necessary amount of parallelism in the code. Thus, integrating the SIMD optimization and the required loop transformations together in a single retargeting formalism is an ambitious challenge. In this article, we present an efficient and quickly retargetable SIMD code optimization framework that is integrated into an industrial retargetable C compiler. Experimental results for different processors demonstrate that the proposed technique applies to real-life target machines and that it produces code quality improvements close to the theoretical limit.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2064508949",
    "type": "article"
  },
  {
    "title": "Online diagnosis of hard faults in microprocessors",
    "doi": "https://doi.org/10.1145/1250727.1250728",
    "publication_date": "2007-06-01",
    "publication_year": 2007,
    "authors": "Fred A. Bower; Daniel J. Sorin; Sule Ozev",
    "corresponding_authors": "",
    "abstract": "We develop a microprocessor design that tolerates hard faults, including fabrication defects and in-field faults, by leveraging existing microprocessor redundancy. To do this, we must: detect and correct errors, diagnose hard faults at the field deconfigurable unit (FDU) granularity, and deconfigure FDUs with hard faults. In our reliable microprocessor design, we use DIVA dynamic verification to detect and correct errors. Our new scheme for diagnosing hard faults tracks instructions' core structure occupancy from decode until commit. If a DIVA checker detects an error in an instruction, it increments a small saturating error counter for every FDU used by that instruction, including that DIVA checker. A hard fault in an FDU quickly leads to an above-threshold error counter for that FDU and thus diagnoses the fault. For deconfiguration, we use previously developed schemes for functional units and buffers and present a scheme for deconfiguring DIVA checkers. Experimental results show that our reliable microprocessor quickly and accurately diagnoses each hard fault that is injected and continues to function, albeit with somewhat degraded performance.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2139727248",
    "type": "article"
  },
  {
    "title": "Tolerating process variations in large, set-associative caches",
    "doi": "https://doi.org/10.1145/1543753.1543757",
    "publication_date": "2009-06-01",
    "publication_year": 2009,
    "authors": "Cheng‐Kok Koh; Weng‐Fai Wong; Yiran Chen; Hai Li",
    "corresponding_authors": "",
    "abstract": "One important trend in today's microprocessor architectures is the increase in size of the processor caches. These caches also tend to be set associative. As technology scales, process variations are expected to increase the fault rates of the SRAM cells that compose such caches. As an important component of the processor, the parametric yield of SRAM cells is crucial to the overall performance and yield of the microchip. In this article, we propose a microarchitectural solution, called the buddy cache that permits large, set-associative caches to tolerate faults in SRAM cells due to process variations. In essence, instead of disabling a faulty cache block in a set (as is the current practice), it is paired with another faulty cache block in the same set—the buddy. Although both cache blocks are faulty, if the faults of the two blocks do not overlap, then instead of losing two blocks, buddying will yield a functional block from the nonfaulty portions of the two blocks. We found that with buddying, caches can better mitigate the negative impacts of process variations on performance and yield, gracefully downgrading performance as opposed to catastrophic failure. We will describe the details of the buddy cache and give insights as to why it is both more performance and yield resilient to faults.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W1992912817",
    "type": "article"
  },
  {
    "title": "Energy-efficient register caching with compiler assistance",
    "doi": "https://doi.org/10.1145/1596510.1596511",
    "publication_date": "2009-10-01",
    "publication_year": 2009,
    "authors": "Timothy M. Jones; Michael O’Boyle; Jaume Abella; Antonio González; Oğuz Ergin",
    "corresponding_authors": "",
    "abstract": "The register file is a critical component in a modern superscalar processor. It must be large enough to accommodate the results of all in-flight instructions. It must also have enough ports to allow simultaneous issue and writeback of many values each cycle. However, this makes it one of the most energy-consuming structures within the processor with a high access latency. As technology scales, there comes a point where register accesses are the bottleneck to performance and so must be pipelined over several cycles. This increases the pipeline depth, lowering performance. To overcome these challenges, we propose a novel use of compiler analysis to aid register caching. Adding a register cache allows us to preserve single-cycle register accesses, maintaining performance and reducing energy consumption. We do this by passing information to the processor using free bits in a real ISA, allowing us to cache only the most important registers. Evaluating the register cache over a variety of sizes and associativities and varying the read ports into the cache, our best scheme achieves an energy-delay-squared (EDD) product of 0.81, with a performance increase of 11%. Another configuration saves 13% of register system energy. Using four register cache read ports brings both performance gains and energy savings, consistently outperforming two state-of-the-art hardware approaches.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2148060071",
    "type": "article"
  },
  {
    "title": "Understanding the behavior and implications of context switch misses",
    "doi": "https://doi.org/10.1145/1880043.1880048",
    "publication_date": "2010-12-01",
    "publication_year": 2010,
    "authors": "Fang Liu; Yan Solihin",
    "corresponding_authors": "",
    "abstract": "One of the essential features in modern computer systems is context switching, which allows multiple threads of execution to time-share a limited number of processors. While very useful, context switching can introduce high performance overheads, with one of the primary reasons being the cache perturbation effect. Between the time a thread is switched out and when it resumes execution, parts of its working set in the cache may be perturbed by other interfering threads, leading to (context switch) cache misses to recover from the perturbation. The goal of this article is to understand how cache parameters and application behavior influence the number of context switch misses the application suffers from. We characterize a previously unreported type of context switch misses that occur as the artifact of the interaction of cache replacement policy and an application's temporal reuse behavior. We characterize the behavior of these “reordered misses” for various applications, cache sizes, and various amount of cache perturbation. As a second contribution, we develop an analytical model that reveals the mathematical relationship between cache design parameters, an application's temporal reuse pattern, and the number of context switch misses the application suffers from. We validate the model against simulation studies and find that it is sufficiently accurate in predicting the trends of context switch misses with regard to various cache perturbation amount. The mathematical relationship provided by the model allows us to derive insights into precisely why some applications are more vulnerable to context switch misses than others. Through a case study on prefetching, we find that prefetching tends to aggravate the number of context switch misses and a less aggresive prefetching technique can reduce the number of context switch misses the application suffers from. We also investigate how cache sizes affect context switch misses. Our study shows that under relatively heavy workloads in the system, the worst-case number of context switch misses an application suffers from tends to increase proportionally with cache sizes, to the extent that may completely negate the reduction in other types of cache misses.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2078498955",
    "type": "article"
  },
  {
    "title": "ABS",
    "doi": "https://doi.org/10.1145/2086696.2086698",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Jorge Albericio; Rubén Gran Tejero; Pablo Ibáñez; Víctor Viñals; J.M. Llaberia",
    "corresponding_authors": "",
    "abstract": "Hardware data prefetch is a very well known technique for hiding memory latencies. However, in a multicore system fitted with a shared Last-Level Cache (LLC), prefetch induced by a core consumes common resources such as shared cache space and main memory bandwidth. This may degrade the performance of other cores and even the overall system performance unless the prefetch aggressiveness of each core is controlled from a system standpoint. On the other hand, LLCs in commercial chip multiprocessors are more and more frequently organized in independent banks. In this contribution, we target for the first time prefetch in a banked LLC organization and propose ABS, a low-cost controller with a hill-climbing approach that runs stand-alone at each LLC bank without requiring inter-bank communication. Using multiprogrammed SPEC2K6 workloads, our analysis shows that the mechanism improves both user-oriented metrics (Harmonic Mean of Speedups by 27% and Fairness by 11%) and system-oriented metrics (Weighted Speedup increases 22% and Memory Bandwidth Consumption decreases 14%) over an eight-core baseline system that uses aggressive sequential prefetch with a fixed degree. Similar conclusions can be drawn by varying the number of cores or the LLC size, when running parallel applications, or when other prefetch engines are controlled.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W1994438830",
    "type": "article"
  },
  {
    "title": "Polyhedral parallelization of binary code",
    "doi": "https://doi.org/10.1145/2086696.2086718",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Benoît Pradelle; Alain Ketterlin; Philippe Clauss",
    "corresponding_authors": "",
    "abstract": "Many automatic software parallelization systems have been proposed in the past decades, but most of them are dedicated to source-to-source transformations. This paper shows that parallelizing executable programs is feasible, even if they require complex transformations, and in effect decouples parallelization from compilation, for example, for closed-source or legacy software, where binary code is the only available representation. We propose an automatic parallelizer, which is able to perform advanced parallelization on binary code. It first parses the binary code and extracts high-level information. From this information, a C program is generated. This program captures only a subset of the program semantics, namely, loops and memory accesses. This C program is then parallelized using existing, state-of-the-art parallelizers, including advanced polyhedral parallelizers. The original program semantics is then re-injected, and the transformed parallel loop nests are recompiled by a standard C compiler. We show on the PolyBench benchmark suite that our system successfully detects and parallelizes almost all the loop nests from the binary code, using a recent polyhedral loop parallelizer as a backend. The paper ends by elaborating a strategy to parallelize more complex programs, such as those containing non-linear accesses to memory, and provides a few example case-studies.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2099574169",
    "type": "article"
  },
  {
    "title": "Defragmentation of Tasks in Many-Core Architecture",
    "doi": "https://doi.org/10.1145/3050437",
    "publication_date": "2017-03-13",
    "publication_year": 2017,
    "authors": "Anuj Pathania; Vanchinathan Venkataramani; Muhammad Shafique; Tulika Mitra; Jörg Henkel",
    "corresponding_authors": "",
    "abstract": "Many-cores can execute multiple multithreaded tasks in parallel. A task performs most efficiently when it is executed over a spatially connected and compact subset of cores so that performance loss due to communication overhead imposed by the task’s threads spread across the allocated cores is minimal. Over a span of time, unallocated cores can get scattered all over the many-core, creating fragments in the task mapping. These fragments can prevent efficient contiguous mapping of incoming new tasks leading to loss of performance. This problem can be alleviated by using a task defragmenter, which consolidates smaller fragments into larger fragments wherein the incoming tasks can be efficiently executed. Optimal defragmentation of a many-core is an NP-hard problem in the general case. Therefore, we simplify the original problem to a problem that can be solved optimally in polynomial time. In this work, we introduce a concept of exponentially separable mapping (ESM), which defines a set of task mapping constraints on a many-core. We prove that an ESM enforcing many-core can be defragmented optimally in polynomial time.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2603251955",
    "type": "article"
  },
  {
    "title": "<i>QuMan</i>",
    "doi": "https://doi.org/10.1145/3210560",
    "publication_date": "2018-08-28",
    "publication_year": 2018,
    "authors": "Yannis Sfakianakis; Christos Kozanitis; Christos Kozyrakis; Angelos Bilas",
    "corresponding_authors": "",
    "abstract": "Modern data centers consolidate workloads to increase server utilization and reduce total cost of ownership, and cope with scaling limitations. However, server resource sharing introduces performance interference across applications and, consequently, increases performance volatility, which negatively affects user experience. Thus, a challenging problem is to increase server utilization while maintaining application QoS. In this article, we present QuMan , a server resource manager that uses application isolation and profiling to increase server utilization while controlling degradation of application QoS. Previous solutions, either estimate interference across applications and then restrict colocation to “compatible” applications, or assume that application requirements are known. Instead, QuMan estimates the required resources of applications. It uses an isolation mechanism to create properly-sized resource slices for applications, and arbitrarily colocates applications. QuMan ’s mechanisms can be used with a variety of admission control policies, and we explore the potential of two such policies: (1) A policy that allows users to specify a minimum performance threshold and (2) an automated policy, which operates without user input and is based on a new combined QoS-utilization metric. We implement QuMan on top of Linux servers, and we evaluate its effectiveness using containers and real applications. Our single-node results show that QuMan balances highly effectively the tradeoff between server utilization and application performance, as it achieves 80% server utilization while the performance of each application does not drop below 80% the respective standalone performance. We also deploy QuMan on a cluster of 100 AWS instances that are managed by a modified version of the Sparrow scheduler [37] and, we observe a 48% increase in application performance on a highly utilized cluster, compared to the performance of the same cluster under the same load when it is managed by native Sparrow or Apache Mesos.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2889265917",
    "type": "article"
  },
  {
    "title": "Predicting New Workload or CPU Performance by Analyzing Public Datasets",
    "doi": "https://doi.org/10.1145/3284127",
    "publication_date": "2018-12-31",
    "publication_year": 2018,
    "authors": "Yu Wang; Victor Lee; Gu-Yeon Wei; David Brooks",
    "corresponding_authors": "",
    "abstract": "The marketplace for general-purpose microprocessors offers hundreds of functionally similar models, differing by traits like frequency, core count, cache size, memory bandwidth, and power consumption. Their performance depends not only on microarchitecture, but also on the nature of the workloads being executed. Given a set of intended workloads, the consumer needs both performance and price information to make rational buying decisions. Many benchmark suites have been developed to measure processor performance, and their results for large collections of CPUs are often publicly available. However, repositories of benchmark results are not always helpful when consumers need performance data for new processors or new workloads . Moreover, the aggregate scores for benchmark suites designed to cover a broad spectrum of workload types can be misleading. To address these problems, we have developed a deep neural network (DNN) model, and we have used it to learn the relationship between the specifications of Intel CPUs and their performance on the SPEC CPU2006 and Geekbench 3 benchmark suites. We show that we can generate useful predictions for new processors and new workloads . We also cross-predict the two benchmark suites and compare their performance scores. The results quantify the self-similarity of these suites for the first time in the literature. This work should discourage consumers from basing purchasing decisions exclusively on Geekbench 3, and it should encourage academics to evaluate research using more diverse workloads than the SPEC CPU suites alone.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2910377526",
    "type": "article"
  },
  {
    "title": "GPU Performance and Power Tuning Using Regression Trees",
    "doi": "https://doi.org/10.1145/2736287",
    "publication_date": "2015-05-11",
    "publication_year": 2015,
    "authors": "Wenhao Jia; Elba Garza; Kelly A. Shaw; Margaret Martonosi",
    "corresponding_authors": "",
    "abstract": "GPU performance and power tuning is difficult, requiring extensive user expertise and time-consuming trial and error. To accelerate design tuning, statistical design space exploration methods have been proposed. This article presents Starchart, a novel design space partitioning tool that uses regression trees to approach GPU tuning problems. Improving on prior work, Starchart offers more automation in identifying key design trade-offs and models design subspaces with distinctly different behaviors. Starchart achieves good model accuracy using very few random samples: less than 0.3% of a given design space; iterative sampling can more quickly target subspaces of interest.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W1442104222",
    "type": "article"
  },
  {
    "title": "Practical Iterative Optimization for the Data Center",
    "doi": "https://doi.org/10.1145/2739048",
    "publication_date": "2015-05-11",
    "publication_year": 2015,
    "authors": "Shuangde Fang; Wenwen Xu; Yang Chen; Lieven Eeckhout; Olivier Temam; Yunji Chen; Chengyong Wu; Xiaobing Feng",
    "corresponding_authors": "",
    "abstract": "Iterative optimization is a simple but powerful approach that searches the best possible combination of compiler optimizations for a given workload. However, iterative optimization is plagued by several practical issues that prevent it from being widely used in practice: a large number of runs are required to find the best combination, the optimum combination is dataset dependent, and the exploration process incurs significant overhead that needs to be compensated for by performance benefits. Therefore, although iterative optimization has been shown to have a significant performance potential, it seldom is used in production compilers. In this article, we propose iterative optimization for the data center (IODC): we show that the data center offers a context in which all of the preceding hurdles can be overcome. The basic idea is to spawn different combinations across workers and recollect performance statistics at the master, which then evolves to the optimum combination of compiler optimizations. IODC carefully manages costs and benefits, and it is transparent to the end user. To bring IODC to practice, we evaluate it in the presence of co-runners to better reflect real-life data center operation with multiple applications co-running per server. We enhance IODC with the capability to find compatible co-runners along with a mechanism to dynamically adjust the level of aggressiveness to improve its robustness in the presence of co-running applications. We evaluate IODC using both MapReduce and compute-intensive throughput server applications. To reflect the large number of users interacting with the system, we gather a very large collection of datasets (up to hundreds of millions of unique datasets per program), for a total storage of 16.4TB and 850 days of CPU time. We report an average performance improvement of 1.48 × and up to 2.08 × for five MapReduce applications, and 1.12 × and up to 1.39 × for nine server applications. Furthermore, our experiments demonstrate that IODC is effective in the presence of co-runners, improving performance by greater than 13% compared to the worst possible co-runner schedule.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W1451759955",
    "type": "article"
  },
  {
    "title": "An Optimizing Code Generator for a Class of Lattice-Boltzmann Computations",
    "doi": "https://doi.org/10.1145/2739047",
    "publication_date": "2015-05-27",
    "publication_year": 2015,
    "authors": "Irshad Pananilath; Aravind Acharya; Vinay Vasista; Uday Bondhugula",
    "corresponding_authors": "",
    "abstract": "The Lattice-Boltzmann method (LBM), a promising new particle-based simulation technique for complex and multiscale fluid flows, has seen tremendous adoption in recent years in computational fluid dynamics. Even with a state-of-the-art LBM solver such as Palabos, a user has to still manually write the program using library-supplied primitives. We propose an automated code generator for a class of LBM computations with the objective to achieve high performance on modern architectures. Few studies have looked at time tiling for LBM codes. We exploit a key similarity between stencils and LBM to enable polyhedral optimizations and in turn time tiling for LBM. We also characterize the performance of LBM with the Roofline performance model. Experimental results for standard LBM simulations like Lid Driven Cavity, Flow Past Cylinder, and Poiseuille Flow show that our scheme consistently outperforms Palabos—on average by up to 3× while running on 16 cores of an Intel Xeon (Sandybridge). We also obtain an improvement of 2.47× on the SPEC LBM benchmark.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W1751458545",
    "type": "article"
  },
  {
    "title": "Accelerating Divergent Applications on SIMD Architectures Using Neural Networks",
    "doi": "https://doi.org/10.1145/2717311",
    "publication_date": "2015-03-09",
    "publication_year": 2015,
    "authors": "Beayna Grigorian; Glenn Reinman",
    "corresponding_authors": "",
    "abstract": "The purpose of this research is to find a neural-network-based solution to the well-known problem of branch divergence in Single Instruction Multiple Data (SIMD) architectures. Our approach differs from existing techniques that handle branch (or control-flow) divergence, which use costly hardware modifications, low-utilization masking techniques, or static prediction methods. As we examine divergent applications, we characterize the degree of data-dependent control flow seen in each and isolate the code regions (or “kernels”) that cause the most performance degradation due to branch divergence. We then train neural networks (NNs) offline to approximate these kernels and inject the NN computations directly into the applications as substitutes for the kernels they approximate. This essentially translates control flow into nondivergent computation, trading off precision for performance. As our methodology manipulates application source code directly, it is inherently platform agnostic and can be adopted as a general means for accelerating divergent applications on data-parallel architectures. In this article, we present the Neuralizer, an automated software flow for kernel identification, NN training, and NN integration, as well as supplementary user-controlled optimization techniques. Evaluating our approach on a variety of divergent applications run on a Graphics Processing Unit (GPU), we on average achieve performance gains of 13.6 × and energy savings of 14.8 × with 96% accuracy.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W1984506768",
    "type": "article"
  },
  {
    "title": "Disjoint out-of-order execution processor",
    "doi": "https://doi.org/10.1145/2355585.2355592",
    "publication_date": "2012-09-01",
    "publication_year": 2012,
    "authors": "Mageda Sharafeddine; Komal Jothi; Haitham Akkary",
    "corresponding_authors": "",
    "abstract": "High-performance superscalar architectures used to exploit instruction level parallelism in single-thread applications have become too complex and power hungry for the multicore processors era. We propose a new architecture that uses multiple small latency-tolerant out-of-order cores to improve single-thread performance. Improving single-thread performance with multiple small out-of-order cores allows designers to place more of these cores on the same die. Consequently, emerging highly parallel applications can take full advantage of the multicore parallel hardware without sacrificing performance of inherently serial and hard to parallelize applications. Our architecture combines speculative multithreading (SpMT) with checkpoint recovery and continual flow pipeline architectures. It splits single-thread program execution into disjoint control and data threads that execute concurrently on multiple cooperating small and latency-tolerant out-of-order cores. Hence we call this style of execution Disjoint Out-of-Order Execution (DOE). DOE uses latency tolerance to overcome performance issues of SpMT caused by interthread data dependences. To evaluate this architecture, we have developed a microarchitecture performance model of DOE based on PTLSim, a simulation infrastructure of the x86 instruction set architecture. We evaluate the potential performance of DOE processor architecture using a simple heuristic to fork control independent threads in hardware at the target addresses of future procedure return instructions. Using applications from SpecInt 2000, we study DOE under ideal as well as realistic architectural constraints. We discuss the performance impact of key DOE architecture and application variables such as number of cores, interthread data dependences, intercore data communication delay, buffers capacity, and branch mispredictions. Without any DOE specific compiler optimizations, our results show that DOE outperforms conventional SpMT architectures by 15%, on average. We also show that DOE with four small cores can perform on average equally well to a large superscalar core, consuming about the same power. Most importantly, DOE improves throughput performance by a significant amount over a large superscalar core, up to 2.5 times, when running multitasking applications.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2033785728",
    "type": "article"
  },
  {
    "title": "Effective Transactional Memory Execution Management for Improved Concurrency",
    "doi": "https://doi.org/10.1145/2633048",
    "publication_date": "2014-08-13",
    "publication_year": 2014,
    "authors": "Miguel A. Gonzalez-Mesa; Eladio Gutiérrez; Emilio L. Zapata; Óscar Plata",
    "corresponding_authors": "",
    "abstract": "This article describes a transactional memory execution model intended to exploit maximum parallelism from sequential and multithreaded programs. A program code section is partitioned into chunks that will be mapped onto threads and executed transactionally. These transactions run concurrently and out of order, trying to exploit maximum parallelism but managed by a specific fully distributed commit control to meet data dependencies. To accomplish correct parallel execution, a partial precedence order relation is derived from the program code section and/or defined by the programmer. When a conflict between chunks is eagerly detected, the precedence order relation is used to determine the best policy to solve the conflict that preserves the precedence order while maximizing concurrency. The model defines a new transactional state called executed but not committed . This state allows exploiting concurrency on two levels: intrathread and interthread. Intrathread concurrency is improved by having pending uncommitted transactions while executing a new one in the same thread. The new state improves interthread concurrency because it permits out-of-order transaction commits regarding the precedence order. Our model has been implemented in a lightweight software transactional memory system, TinySTM, and has been evaluated on a set of benchmarks obtaining an important performance improvement over the baseline TM system.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2047625235",
    "type": "article"
  },
  {
    "title": "DUCATI",
    "doi": "https://doi.org/10.1145/3309710",
    "publication_date": "2019-03-08",
    "publication_year": 2019,
    "authors": "Aamer Jaleel; Eiman Ebrahimi; Sam Duncan",
    "corresponding_authors": "",
    "abstract": "Conventional on-chip TLB hierarchies are unable to fully cover the growing application working-set sizes. To make things worse, Last-Level TLB (LLT) misses require multiple accesses to the page table even with the use of page walk caches. Consequently, LLT misses incur long address translation latency and hurt performance. This article proposes two low-overhead hardware mechanisms for reducing the frequency and penalty of on-die LLT misses. The first, Unified CAche and TLB (UCAT) , enables the conventional on-die Last-Level Cache to store cache lines and TLB entries in a single unified structure and increases on-die TLB capacity significantly. The second, DRAM-TLB , memoizes virtual to physical address translations in DRAM and reduces LLT miss penalty when UCAT is unable to fully cover total application working-set. DRAM-TLB serves as the next larger level in the TLB hierarchy that significantly increases TLB coverage relative to on-chip TLBs. The combination of these two mechanisms, DUCATI , is an address translation architecture that improves GPU performance by 81%; (up to 4.5×) while requiring minimal changes to the existing system design. We show that DUCATI is within 20%, 5%, and 2% the performance of a perfect LLT system when using 4KB, 64KB, and 2MB pages, respectively.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2920870926",
    "type": "article"
  },
  {
    "title": "CODA",
    "doi": "https://doi.org/10.1145/3232521",
    "publication_date": "2018-09-04",
    "publication_year": 2018,
    "authors": "Hyojong Kim; Ramyad Hadidi; Lifeng Nai; Hyesoon Kim; Nuwan Jayasena; Yasuko Eckert; Onur Kayıran; Gabriel H. Loh",
    "corresponding_authors": "",
    "abstract": "Recent studies have demonstrated that near-data processing (NDP) is an effective technique for improving performance and energy efficiency of data-intensive workloads. However, leveraging NDP in realistic systems with multiple memory modules introduces a new challenge. In today's systems, where no computation occurs in memory modules, the physical address space is interleaved at a fine granularity among all memory modules to help improve the utilization of processor-memory interfaces by distributing the memory traffic. However, this is at odds with efficient use of NDP, which requires careful placement of data in memory modules such that near-data computations and their exclusively used data can be localized in individual memory modules, while distributing shared data among memory modules to reduce hotspots. In order to address this new challenge, we propose a set of techniques that (1) enable collections of OS pages to either be fine-grain interleaved among memory modules (as is done today) or to be placed contiguously on individual memory modules (as is desirable for NDP private data), and (2) decide whether to localize or distribute each memory object based on its anticipated access pattern and steer computations to the memory where the data they access is located. Our evaluations across a wide range of workloads show that the proposed mechanism improves performance by 31% and reduces 38% remote data accesses over a baseline system that cannot exploit computate-data affinity characteristics.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W3098136731",
    "type": "article"
  },
  {
    "title": "Dynamic Memory Balancing for Virtualization",
    "doi": "https://doi.org/10.1145/2851501",
    "publication_date": "2016-03-28",
    "publication_year": 2016,
    "authors": "Zhigang Wang; Xiaolin Wang; Fang Hou; Yingwei Luo; Zhenlin Wang",
    "corresponding_authors": "",
    "abstract": "Allocating memory dynamically for virtual machines (VMs) according to their demands provides significant benefits as well as great challenges. Efficient memory resource management requires knowledge of the memory demands of applications or systems at runtime. A widely proposed approach is to construct a miss ratio curve (MRC) for a VM, which not only summarizes the current working set size (WSS) of the VM but also models the relationship between its performance and the target memory allocation size. Unfortunately, the cost of monitoring and maintaining the MRC structures is nontrivial. This article first introduces a low-cost WSS tracking system with effective optimizations on data structures, as well as an efficient mechanism to decrease the frequency of monitoring. We also propose a Memory Balancer (MEB), which dynamically reallocates guest memory based on the predicted WSS. Our experimental results show that our prediction schemes yield a high accuracy of 95.2% and low overhead of 2%. Furthermore, the overall system throughput can be significantly improved with MEB, which brings a speedup up to 7.4 for two to four VMs and 4.54 for an overcommitted system with 16 VMs.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2316300312",
    "type": "article"
  },
  {
    "title": "Some Mathematical Facts About Optimal Cache Replacement",
    "doi": "https://doi.org/10.1145/3017992",
    "publication_date": "2016-12-16",
    "publication_year": 2016,
    "authors": "Pierre Michaud",
    "corresponding_authors": "Pierre Michaud",
    "abstract": "This article exposes and proves some mathematical facts about optimal cache replacement that were previously unknown or not proved rigorously. An explicit formula is obtained, giving OPT hits and misses as a function of past references. Several mathematical facts are derived from this formula, including a proof that OPT miss curves are always convex, and a new algorithm called OPT tokens , for reasoning about optimal replacement.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2564076424",
    "type": "article"
  },
  {
    "title": "A Case for a More Effective, Power-Efficient Turbo Boosting",
    "doi": "https://doi.org/10.1145/3170433",
    "publication_date": "2018-03-22",
    "publication_year": 2018,
    "authors": "Sushant Kondguli; Michael Huang",
    "corresponding_authors": "",
    "abstract": "Single-thread performance and throughput often pose different design constraints and require compromises. Mainstream CPUs today incorporate a non-trivial number of cores, even for mobile devices. For power and thermal considerations, by default, a single core does not operate at the maximum performance level. When operating conditions allow, however, commercial products often rely on turbo boosting, which temporarily increases the clock frequency to increase single-thread performance. However, increasing clock speed may result in a poor performance return for invested energy. In this article, we make a case for a more effective boosting strategy, which invests energy in activities with the best estimated return. In addition to running faster clocks, we can also use a look-ahead thread to overlap the penalties of cache misses and branch mispredicts. Overall, for similar power consumptions, the proposed adaptive turbo boosting strategy can achieve about twice the performance benefits while halving the energy overhead.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2791692693",
    "type": "article"
  },
  {
    "title": "On-GPU Thread-Data Remapping for Branch Divergence Reduction",
    "doi": "https://doi.org/10.1145/3242089",
    "publication_date": "2018-09-30",
    "publication_year": 2018,
    "authors": "Huanxin Lin; Cho‐Li Wang; Hongyuan Liu",
    "corresponding_authors": "",
    "abstract": "General Purpose GPU computing (GPGPU) plays an increasingly vital role in high performance computing and other areas like deep learning. However, arising from the SIMD execution model, the branch divergence issue lowers efficiency of conditional branching on GPUs, and hinders the development of GPGPU. To achieve runtime on-the-spot branch divergence reduction, we propose the first on-GPU thread-data remapping scheme. Before kernel launching, our solution inserts codes into GPU kernels immediately before each target branch so as to acquire actual runtime divergence information. GPU software threads can be remapped to datasets multiple times during single kernel execution. We propose two thread-data remapping algorithms that are tailored to the GPU architecture. Effective on two generations of GPUs from both NVIDIA and AMD, our solution achieves speedups up to 2.718 with third-party benchmarks. We also implement three GPGPU frontier benchmarks from areas including computer vision, algorithmic trading and data analytics. They are hindered by more complex divergence coupled with different memory access patterns, and our solution works better than the traditional thread-data remapping scheme in all cases. As a compiler-assisted runtime solution, it can better reduce divergence for divergent applications that gain little acceleration on GPUs for the time being.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2895519740",
    "type": "article"
  },
  {
    "title": "Declarative Loop Tactics for Domain-specific Optimization",
    "doi": "https://doi.org/10.1145/3372266",
    "publication_date": "2019-12-26",
    "publication_year": 2019,
    "authors": "Lorenzo Chelini; Oleksandr Zinenko; Tobias Grosser; Henk Corporaal",
    "corresponding_authors": "",
    "abstract": "Increasingly complex hardware makes the design of effective compilers difficult. To reduce this problem, we introduce Declarative Loop Tactics , which is a novel framework of composable program transformations based on an internal tree-like program representation of a polyhedral compiler. The framework is based on a declarative C++ API built around easy-to-program matchers and builders, which provide the foundation to develop loop optimization strategies. Using our matchers and builders, we express computational patterns and core building blocks, such as loop tiling, fusion, and data-layout transformations, and compose them into algorithm-specific optimizations. Declarative Loop Tactics (Loop Tactics for short) can be applied to many domains. For two of them, stencils and linear algebra, we show how developers can express sophisticated domain-specific optimizations as a set of composable transformations or calls to optimized libraries. By allowing developers to add highly customized optimizations for a given computational pattern, we expect our approach to reduce the need for DSLs and to extend the range of optimizations that can be performed by a current general-purpose compiler.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2997238241",
    "type": "article"
  },
  {
    "title": "A Novel, Highly Integrated Simulator for Parallel and Distributed Systems",
    "doi": "https://doi.org/10.1145/3378934",
    "publication_date": "2020-03-04",
    "publication_year": 2020,
    "authors": "Nikolaos Tampouratzis; Ioannis Papaefstathiou; Antonios Nikitakis; Andreas Brokalakis; Stamatis Andrianakis; Apostolos Dollas; Marco Marcon; Emanuele Plebani",
    "corresponding_authors": "",
    "abstract": "In an era of complex networked parallel heterogeneous systems, simulating independently only parts, components, or attributes of a system-under-design is a cumbersome, inaccurate, and inefficient approach. Moreover, by considering each part of a system in an isolated manner, and due to the numerous and highly complicated interactions between the different components, the system optimization capabilities are severely limited. The presented fully-distributed simulation framework (called as COSSIM) is the first known open-source, high-performance simulator that can handle holistically system-of-systems including processors, peripherals and networks; such an approach is very appealing to both Cyber Physical Systems (CPS) and Highly Parallel Heterogeneous Systems designers and application developers. Our highly integrated approach is further augmented with accurate power estimation and security sub-tools that can tap on all system components and perform security and robustness analysis of the overall system under design—something that was unfeasible up to now. Additionally, a sophisticated Eclipse-based Graphical User Interface (GUI) has been developed to provide easy simulation setup, execution, and visualization of results. COSSIM has been evaluated when executing the widely used Netperf benchmark suite as well as a number of real-world applications. Final results demonstrate that the presented approach has up to 99% accuracy (when compared with the performance of the real system), while the overall simulation time can be accelerated almost linearly with the number of CPUs utilized by the simulator.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W3009565526",
    "type": "article"
  },
  {
    "title": "Schedule Synthesis for Halide Pipelines on GPUs",
    "doi": "https://doi.org/10.1145/3406117",
    "publication_date": "2020-08-03",
    "publication_year": 2020,
    "authors": "Savvas Sioutas; Sander Stuijk; Twan Basten; Henk Corporaal; Lou Somers",
    "corresponding_authors": "",
    "abstract": "The Halide DSL and compiler have enabled high-performance code generation for image processing pipelines targeting heterogeneous architectures through the separation of algorithmic description and optimization schedule. However, automatic schedule generation is currently only possible for multi-core CPU architectures. As a result, expert knowledge is still required when optimizing for platforms with GPU capabilities. In this work, we extend the current Halide Autoscheduler with novel optimization passes to efficiently generate schedules for CUDA-based GPU architectures. We evaluate our proposed method across a variety of applications and show that it can achieve performance competitive with that of manually tuned Halide schedules, or in many cases even better performance. Experimental results show that our schedules are on average 10% faster than manual schedules and over 2× faster than previous autoscheduling attempts.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W3047196046",
    "type": "article"
  },
  {
    "title": "Contech",
    "doi": "https://doi.org/10.1145/2776893",
    "publication_date": "2015-07-08",
    "publication_year": 2015,
    "authors": "Brian P. Railing; Eric R. Hein; Thomas M. Conte",
    "corresponding_authors": "",
    "abstract": "Parallel programs can be characterized by task graphs encoding instructions, memory accesses, and the parallel work’s dependencies, while representing any threading library and architecture. This article presents Contech, a high performance framework for generating dynamic task graphs from arbitrary parallel programs, and a novel representation enabling programmers and compiler optimizations to understand and exploit program aspects. The Contech framework supports a variety of languages (including C, C++, and Fortran), parallelization libraries, and ISAs (including × 86 and ARM). Running natively for collection speed and minimizing program perturbation, the instrumentation shows 4 × improvement over a Pin-based implementation on PARSEC and NAS benchmarks.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W1037867572",
    "type": "article"
  },
  {
    "title": "RATT-ECC",
    "doi": "https://doi.org/10.1145/2957758",
    "publication_date": "2016-09-17",
    "publication_year": 2016,
    "authors": "Hsing-Min Chen; Carole-Jean Wu; Trevor Mudge; Chaitali Chakrabarti",
    "corresponding_authors": "",
    "abstract": "This article proposes a rate-adaptive, two-tiered error-correction scheme (RATT-ECC) that provides strong reliability (10 10 x reduction in raw FIT rate) for an HBM-like 3D DRAM system. The tier-1 code is a strong symbol-based code that can correct errors due to small granularity faults and detect errors caused by large granularity faults; the tier-2 code is an XOR-based code that corrects errors detected by the tier-1 code. The rate-adaptive feature of RATT-ECC enables permanent bank failures to be handled through sparing. It can also be used to significantly reduce the refresh power consumption without decreasing reliability and timing performance.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2522781526",
    "type": "article"
  },
  {
    "title": "A Software Cache Partitioning System for Hash-Based Caches",
    "doi": "https://doi.org/10.1145/3018113",
    "publication_date": "2016-12-16",
    "publication_year": 2016,
    "authors": "Alberto Scolari; Davide B. Bartolini; Marco D. Santambrogio",
    "corresponding_authors": "",
    "abstract": "Contention on the shared Last-Level Cache (LLC) can have a fundamental negative impact on the performance of applications executed on modern multicores. An interesting software approach to address LLC contention issues is based on page coloring , which is a software technique that attempts to achieve performance isolation by partitioning a shared cache through careful memory management. The key assumption of traditional page coloring is that the cache is physically addressed. However, recent multicore architectures (e.g., Intel Sandy Bridge and later) switched from a physical addressing scheme to a more complex scheme that involves a hash function. Traditional page coloring is ineffective on these recent architectures. In this article, we extend page coloring to work on these recent architectures by proposing a mechanism able to handle their hash-based LLC addressing scheme. Just as for traditional page coloring, the goal of this new mechanism is to deliver performance isolation by avoiding contention on the LLC, thus enabling predictable performance. We implement this mechanism in the Linux kernel, and evaluate it using several benchmarks from the SPEC CPU2006 and PARSEC 3.0 suites. Our results show that our solution is able to deliver performance isolation to concurrently running applications by enforcing partitioning of a Sandy Bridge LLC, which traditional page coloring techniques are not able to handle.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2566356404",
    "type": "article"
  },
  {
    "title": "Securing Branch Predictors with Two-Level Encryption",
    "doi": "https://doi.org/10.1145/3404189",
    "publication_date": "2020-08-03",
    "publication_year": 2020,
    "authors": "Jaekyu Lee; Yasuo Ishii; Dam Sunwoo",
    "corresponding_authors": "",
    "abstract": "Modern processors rely on various speculative mechanisms to meet performance demand. Branch predictors are one of the most important micro-architecture components to deliver performance. However, they have been under heavy scrutiny because of recent side-channel attacks. Branch predictors are indexed using the PC and recent branch histories. An adversary can manipulate these parameters to access and control the same branch predictor entry that a victim uses. Recent Spectre attacks exploit this to set up speculative-execution-based security attacks. In this article, we aim to mitigate branch predictor side-channels using two-level encryption. At the first level, we randomize the set-index by encrypting the PC using a per-context secret key. At the second level, we encrypt the data in each branch predictor entry. While periodic key changes make the branch predictor more secure, performance degradation can be significant. To alleviate performance degradation, we propose a practical set update mechanism that also considers parallelism in multi-banked branch predictors. We show that our mechanism exhibits only 1.0% and 0.2% performance degradation while changing keys every 10K and 50K cycles, respectively, which is much lower than other state-of-the-art approaches.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W3047462776",
    "type": "article"
  },
  {
    "title": "Bayesian Optimization for Efficient Accelerator Synthesis",
    "doi": "https://doi.org/10.1145/3427377",
    "publication_date": "2020-12-30",
    "publication_year": 2020,
    "authors": "Atefeh Mehrabi; Aninda Manocha; Benjamin C. Lee; Daniel J. Sorin",
    "corresponding_authors": "",
    "abstract": "Accelerator design is expensive due to the effort required to understand an algorithm and optimize the design. Architects have embraced two technologies to reduce costs. High-level synthesis automatically generates hardware from code. Reconfigurable fabrics instantiate accelerators while avoiding fabrication costs for custom circuits. We further reduce design effort with statistical learning. We build an automated framework, called Prospector, that uses Bayesian techniques to optimize synthesis directives, reducing execution latency and resource usage in field-programmable gate arrays. We show in a certain amount of time that designs discovered by Prospector are closer to Pareto-efficient designs compared to prior approaches. Prospector permits new studies for heterogeneous accelerators.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W3113937871",
    "type": "article"
  },
  {
    "title": "Low I/O Intensity-aware Partial GC Scheduling to Reduce Long-tail Latency in SSDs",
    "doi": "https://doi.org/10.1145/3460433",
    "publication_date": "2021-08-18",
    "publication_year": 2021,
    "authors": "Zhibing Sha; Jun Li; Lihao Song; Jiewen Tang; Min Huang; Zhigang Cai; Lianju Qian; Jianwei Liao; Zhiming Liu",
    "corresponding_authors": "",
    "abstract": "This article proposes a low I/O intensity-aware scheduling scheme on garbage collection (GC) in SSDs for minimizing the I/O long-tail latency to ensure I/O responsiveness. The basic idea is to assemble partial GC operations by referring to several determinable factors (e.g., I/O characteristics) and dispatch them to be processed together in idle time slots of I/O processing. To this end, it first makes use of Fourier transform to explore the time slots having relative sparse I/O requests for conducting time-consuming GC operations, as the number of affected I/O requests can be limited. After that, it constructs a mathematical model to further figure out the types and quantities of partial GC operations, which are supposed to be dealt with in the explored idle time slots, by taking the factors of I/O intensity, read/write ratio, and the SSD use state into consideration. Through a series of simulation experiments based on several realistic disk traces, we illustrate that the proposed GC scheduling mechanism can noticeably reduce the long-tail latency by between 5.5% and 232.3% at the 99.99th percentile, in contrast to state-of-the-art methods.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W3193687774",
    "type": "article"
  },
  {
    "title": "An FPGA Overlay for CNN Inference with Fine-grained Flexible Parallelism",
    "doi": "https://doi.org/10.1145/3519598",
    "publication_date": "2022-05-04",
    "publication_year": 2022,
    "authors": "Ziaul Choudhury; Shashwat Shrivastava; Lavanya Ramapantulu; Suresh Purini",
    "corresponding_authors": "",
    "abstract": "Increasingly, pre-trained convolutional neural networks (CNNs) are being deployed for inference in various computer vision applications, both on the server-side in the data centers and at the edge. CNN inference is a very compute-intensive task. It is a challenge to meet performance metrics such as latency and throughput while optimizing power. Special-purpose ASICs and FPGAs are suitable candidates to meet these power and performance budgets simultaneously. Rapidly evolving CNN architectures involve novel convolution operations such as point convolutions, depth separable convolutions, and so on. This leads to substantial variation in the computational structure across CNNs and layers within a CNN. Because of this, FPGA reconfigurability provides an attractive tradeoff compared to ASICs. FPGA-based hardware designers address the structural variability issue by generating a network-specific accelerator for a single network or a class of networks. However, homogeneous accelerators are network agnostic and often sacrifice throughput and FPGA LUTs for flexibility. In this article, we propose an FPGA overlay for efficient processing of CNNs that can be scaled based on the available compute and memory resources of the FPGA. The overlay is configured on the fly through control words sent by the host on a per-layer basis. Unlike current overlays, our architecture exploits all forms of parallelism inside a convolution operation. A constraint system is employed at the host end to find out the per-layer configuration of the overlay that uses all forms of parallelism in the processing of the layer, resulting in the highest throughput for that layer. We studied the effectiveness of our overlay by using it to process AlexNet, VGG16, YOLO, MobileNet, and ResNet-50 CNNs targeting a Virtex7 and a bigger Ultrascale+VU9P FPGAs. The chosen CNNs have a mix of different types of convolution layers and filter sizes, presenting a good variation in model size and structure. Our accelerator reported a maximum throughput of 1,200 GOps/second on the Virtex7, an improvement of 1.2 \\( \\times \\) to 5 \\( \\times \\) over the recent designs. Also, the reported performance density, measured in giga operations per second per KLUT, is 1.3 \\( \\times \\) to 4 \\( \\times \\) improvement over existing works. Similar speed-up and performance density is also observed for the Ultrascale+VU9P FPGA.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4225426816",
    "type": "article"
  },
  {
    "title": "Multi-objective Hardware-aware Neural Architecture Search with Pareto Rank-preserving Surrogate Models",
    "doi": "https://doi.org/10.1145/3579853",
    "publication_date": "2023-01-11",
    "publication_year": 2023,
    "authors": "Hadjer Benmeziane; Hamza Ouarnoughi; Kaoutar El Maghraoui; Smaïl Niar",
    "corresponding_authors": "",
    "abstract": "Deep learning (DL) models such as convolutional neural networks (ConvNets) are being deployed to solve various computer vision and natural language processing tasks at the edge. It is a challenge to find the right DL architecture that simultaneously meets the accuracy, power, and performance budgets of such resource-constrained devices. Hardware-aware Neural Architecture Search (HW-NAS) has recently gained steam by automating the design of efficient DL models for a variety of target hardware platforms. However, such algorithms require excessive computational resources. Thousands of GPU days are required to evaluate and explore an architecture search space such as FBNet [ 45 ]. State-of-the-art approaches propose using surrogate models to predict architecture accuracy and hardware performance to speed up HW-NAS. Existing approaches use independent surrogate models to estimate each objective, resulting in non-optimal Pareto fronts. In this article, HW-PR-NAS, 1 a novel Pareto rank-preserving surrogate model for edge computing platforms, is presented. Our model integrates a new loss function that ranks the architectures according to their Pareto rank, regardless of the actual values of the various objectives. We employ a simple yet effective surrogate model architecture that can be generalized to any standard DL model. We then present an optimized evolutionary algorithm that uses and validates our surrogate model. Our approach has been evaluated on seven edge hardware platforms from various classes, including ASIC, FPGA, GPU, and multi-core CPU. The evaluation results show that HW-PR-NAS achieves up to 2.5× speedup compared to state-of-the-art methods while achieving 98% near the actual Pareto front.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4315647171",
    "type": "article"
  },
  {
    "title": "Advancing Direct Convolution Using Convolution Slicing Optimization and ISA Extensions",
    "doi": "https://doi.org/10.1145/3625004",
    "publication_date": "2023-09-20",
    "publication_year": 2023,
    "authors": "Victor Ferrari; Rafael Sousa; Marcio Pereira; João P. L. de Carvalho; José Nelson Amaral; José E. Moreira; Guido Araújo",
    "corresponding_authors": "",
    "abstract": "Convolution is one of the most computationally intensive operations that must be performed for machine learning model inference. A traditional approach to computing convolutions is known as the Im2Col + BLAS method. This article proposes SConv: a direct-convolution algorithm based on an MLIR/LLVM code-generation toolchain that can be integrated into machine-learning compilers. This algorithm introduces: (a) Convolution Slicing Analysis (CSA)—a convolution-specific 3D cache-blocking analysis pass that focuses on tile reuse over the cache hierarchy; (b) Convolution Slicing Optimization—a code-generation pass that uses CSA to generate a tiled direct-convolution macro-kernel; and (c) Vector-based Packing—an architecture-specific optimized input-tensor packing solution based on vector-register shift instructions for convolutions with unitary stride. Experiments conducted on 393 convolutions from full ONNX-MLIR machine learning models indicate that the elimination of the Im2Col transformation and the use of fast packing routines result in a total packing time reduction, on full model inference, of 2.3×–4.0× on Intel x86 and 3.3×–5.9× on IBM POWER10. The speed-up over an Im2Col + BLAS method based on current BLAS implementations for end-to-end machine-learning model inference is in the range of 11%–27% for Intel x86 and 11%–34% for IBM POWER10 architectures. The total convolution speedup for model inference is 13%–28% on Intel x86 and 23%–39% on IBM POWER10. SConv also outperforms BLAS GEMM, when computing pointwise convolutions in more than 82% of the 219 tested instances.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4386896826",
    "type": "article"
  },
  {
    "title": "A lifetime optimal algorithm for speculative PRE",
    "doi": "https://doi.org/10.1145/1138035.1138036",
    "publication_date": "2006-06-01",
    "publication_year": 2006,
    "authors": "Jingling Xue; Qiong Cai",
    "corresponding_authors": "",
    "abstract": "A lifetime optimal algorithm, called MC-PRE, is presented for the first time that performs speculative PRE based on edge profiles. In addition to being computationally optimal in the sense that the total number of dynamic computations for an expression in the transformed code is minimized, MC-PRE is also lifetime optimal since the lifetimes of introduced temporaries are also minimized. The key in achieving lifetime optimality lies not only in finding a unique minimum cut on a transformed graph of a given CFG, but also in performing a data-flow analysis directly on the CFG to avoid making unnecessary code insertions and deletions. The lifetime optimal results are rigorously proved. We evaluate our algorithm in GCC against three previously published PRE algorithms, namely, MC-PRE copt (Qiong and Xue's computationally optimal version of MC-PRE), LCM (Knoop, Rüthing, and Steffen's lifetime optimal algorithm for performing nonspeculative classic PRE), and CMP-PRE (Bodik, Gupta, and Soffa's PRE algorithm based on code-motion preventing (CMP) regions, which is speculative but not computationally optimal). We report and analyze our experimental results, obtained from both actual program execution and instrumentation, for all 22 C, C++ and FORTRAN 77 benchmarks from SPECcpu2000 on an Itanium 2 computer system. Our results show that MC-PRE (or MC-PRE copt ) is capable of eliminating more partial redundancies than both LCM and CMP-PRE (especially in functions with complex control flow), and, in addition, MC-PRE inserts temporaries with shorter lifetimes than MC-PRE copt . Each of both benefits has contributed to the performance improvements in benchmark programs at the costs of only small compile-time and code-size increases in some benchmarks.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2086831405",
    "type": "article"
  },
  {
    "title": "Exploiting selective placement for low-cost memory protection",
    "doi": "https://doi.org/10.1145/1455650.1455653",
    "publication_date": "2008-11-01",
    "publication_year": 2008,
    "authors": "Mojtaba Mehrara; Todd Austin",
    "corresponding_authors": "",
    "abstract": "Many embedded processing applications, such as those found in the automotive or medical field, require hardware designs that are at the same time low cost and reliable. Traditionally, reliable memory systems have been implemented using coded storage techniques, such as ECC. While these designs can effectively detect and correct memory faults such as transient errors and single-bit defects, their use bears a significant cost overhead. In this article, we propose a novel partial memory protection scheme that provides high-coverage fault protection for program code and data, but with much lower cost than traditional approaches. Our approach profiles program code and data usage to assess which program elements are most critical to maintaining program correctness. Critical code and variables are then placed into a limited protected storage resources. To ensure high coverage of program elements, our placement technique considers all program components simultaneously, including code, global variables, stack frames, and heap variables. The fault coverage of our approach is gauged using Monte Carlo fault-injection experiments, which confirm that our technique provides high levels of fault protection (99% coverage) with limited memory protection resources (36% protected area).",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W1985910385",
    "type": "article"
  },
  {
    "title": "Making secure processors OS- and performance-friendly",
    "doi": "https://doi.org/10.1145/1498690.1498691",
    "publication_date": "2009-03-01",
    "publication_year": 2009,
    "authors": "Siddhartha Chhabra; Brian Rogers; Yan Solihin; Milos Prvulović",
    "corresponding_authors": "",
    "abstract": "In today's digital world, computer security issues have become increasingly important. In particular, researchers have proposed designs for secure processors that utilize hardware-based memory encryption and integrity verification to protect the privacy and integrity of computation even from sophisticated physical attacks. However, currently proposed schemes remain hampered by problems that make them impractical for use in today's computer systems: lack of virtual memory and Inter-Process Communication support as well as excessive storage and performance overheads. In this article, we propose (1) address independent seed encryption (AISE), a counter-mode-based memory encryption scheme using a novel seed composition, and (2) bonsai Merkle trees (BMT), a novel Merkle tree-based memory integrity verification technique, to eliminate these system and performance issues associated with prior counter-mode memory encryption and Merkle tree integrity verification schemes. We present both a qualitative discussion and a quantitative analysis to illustrate the advantages of our techniques over previously proposed approaches in terms of complexity, feasibility, performance, and storage. Our results show that AISE+BMT reduces the overhead of prior memory encryption and integrity verification schemes from 12% to 2% on average for single-threaded benchmarks on uniprocessor systems, and from 15% to 4% for coscheduled benchmarks on multicore systems while eliminating critical system-level problems.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2008941869",
    "type": "article"
  },
  {
    "title": "Impact of high-level transformations within the ROCCC framework",
    "doi": "https://doi.org/10.1145/1880043.1880044",
    "publication_date": "2010-12-01",
    "publication_year": 2010,
    "authors": "Betul Buyukkurt; John Cortes; Jason Villarreal; Walid Najjar",
    "corresponding_authors": "",
    "abstract": "Reconfigurable computers, where one or more FPGAs are attached to a conventional microprocessor, are promising platforms for code acceleration. Despite their advantages, programmability concerns and the lack of efficient design tools/compilers for FPGAs are preventing the technology's widespread adoption. The traditional compiler technology is microprocessor-based-systems-specific and needs to be customized and augmented to address the needs in reconfigurable computing. The challenges are several due to the resources and performance constraints for FPGAs being drastically different than those of microprocessors, and also that compiling for FPGAs requires laying the computation in space by a circuit rather than in time by a sequence of instructions. ROCCC is an optimizing C-to-VHDL compiler specifically targeting the reconfigurable computer platforms. ROCCC includes several high-level optimizations that parallelize and optimize the source code for minimized area and critical path length and maximized throughput. This article presents the effect of ROCCC's high-level transformations on the performance of the generated VHDL output. ROCCC utilizes: (1) several array access optimizations to eliminate redundant memory accesses, (2) procedure-level optimizations to achieve circuit area reductions of up to 88% compared to circuit areas generated from unoptimized codes, (3) loop-level optimizations to increase the throughput, and (4) transformations unique to certain classes of applications. The preceding listed features help ROCCC generate circuits with very large degrees of parallelism capable of very high computation rates.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W1973845444",
    "type": "article"
  },
  {
    "title": "Seamlessly portable applications",
    "doi": "https://doi.org/10.1145/2086696.2086721",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Mario Kicherer; Fabian Nowak; Rainer Buchty; Wolfgang Karl",
    "corresponding_authors": "",
    "abstract": "Nowadays, many possible configurations of heterogeneous systems exist, posing several new challenges to application development: different types of processing units usually require individual programming models with dedicated runtime systems and accompanying libraries. If these are absent on an end-user system, e.g. because the respective hardware is not present, an application linked against these will break. This handicaps portability of applications being developed on one system and executed on other, differently configured heterogeneous systems. Moreover, the individual profit of different processing units is normally not known in advance. In this work, we propose a technique to effectively decouple applications from their accelerator-specific parts, respectively code. These parts are only linked on demand and thereby an application can be made portable across systems with different accelerators. As there are usually multiple hardware-specific implementations for a certain task, e.g., a CPU and a GPU version, a method is required to determine which are usable at all and which one is most suitable for execution on the current system. With our approach, application and hardware programmers can express the requirements and the abilities of the application and the hardware-specific implementations in a simplified manner. During runtime, the requirements and abilities are compared with regard to the present hardware in order to determine the usable implementations of a task. If multiple implementations are usable, an online-learning history-based selector is employed to determine the most efficient one. We show that our approach chooses the fastest usable implementation dynamically on several systems while introducing only a negligible overhead itself. Applied to an MPI application, our mechanism enables exploitation of local accelerators on different heterogeneous hosts without preliminary knowledge or modification of the application.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W1967518986",
    "type": "article"
  },
  {
    "title": "Dynamic access distance driven cache replacement",
    "doi": "https://doi.org/10.1145/2019608.2019613",
    "publication_date": "2011-10-01",
    "publication_year": 2011,
    "authors": "Min Feng; Chen Tian; Changhui Lin; Rajiv Gupta",
    "corresponding_authors": "",
    "abstract": "In this article, we propose a new cache replacement policy that makes the replacement decision based on the reuse information of the cache lines and the requested data. We present the architectural support and evaluate the performance of our approach using SPEC benchmarks. We also develop two reuse information predictors: a profile-based static predictor and a runtime predictor. The applicability of each predictor is discussed in this paper. We further extend our reuse information predictors so that the cache can adaptively choose between the reuse information based replacement policy and an approximation of LRU policy. According to the experimental results, our adaptive reuse information based replacement policy performs either better than or close to the LRU policy. Our experiments show that L2 cache misses are reduced by 12.32% and 19.95% using the profiling-based static and runtime adaptive predictors respectively.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2007811959",
    "type": "article"
  },
  {
    "title": "Evaluating indirect branch handling mechanisms in software dynamic translation systems",
    "doi": "https://doi.org/10.1145/1970386.1970390",
    "publication_date": "2011-06-21",
    "publication_year": 2011,
    "authors": "Jason D. Hiser; Daniel Williams; Wei Hu; Jack W. Davidson; Jason Mars; Bruce R. Childers",
    "corresponding_authors": "",
    "abstract": "Software Dynamic Translation (SDT) is used for instrumentation, optimization, security, and many other uses. A major source of SDT overhead is the execution of code to translate an indirect branch's target address into the translated destination block's address. This article discusses sources of Indirect Branch (IB) overhead in SDT systems and evaluates techniques for overhead reduction. Measurements using SPEC CPU2000 show that the appropriate choice and configuration of IB translation mechanisms can significantly reduce the overhead. Further, cross-architecture evaluation of these mechanisms reveals that the most efficient implementation and configuration can be highly dependent on the architecture implementation.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2029880463",
    "type": "article"
  },
  {
    "title": "On-the-fly structure splitting for heap objects",
    "doi": "https://doi.org/10.1145/2086696.2086705",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Zhenjiang Wang; Chenggang Wu; Pen-Chung Yew; Jianjun Li; Di Xu",
    "corresponding_authors": "",
    "abstract": "With the advent of multicore systems, the gap between processor speed and memory latency has grown worse because of their complex interconnect. Sophisticated techniques are needed more than ever to improve an application's spatial and temporal locality. This paper describes an optimization that aims to improve heap data layout by structure-splitting. It also provides runtime address checking by piggybacking on the existing page protection mechanism to guarantee the correctness of such optimization that has eluded many previous attempts due to safety concerns. The technique can be applied to both sequential and parallel programs at either compile time or runtime. However, we focus primarily on sequential programs (i.e., single-threaded programs) at runtime in this paper. Experimental results show that some benchmarks in SPEC 2000 and 2006 can achieve a speedup of up to 142.8%.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W1971578705",
    "type": "article"
  },
  {
    "title": "Dynamic microarchitectural adaptation using machine learning",
    "doi": "https://doi.org/10.1145/2541228.2541238",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Christophe Dubach; Timothy M. Jones; Edwin V. Bonilla",
    "corresponding_authors": "",
    "abstract": "Adaptive microarchitectures are a promising solution for designing high-performance, power-efficient microprocessors. They offer the ability to tailor computational resources to the specific requirements of different programs or program phases. They have the potential to adapt the hardware cost-effectively at runtime to any application’s needs. However, one of the key challenges is how to dynamically determine the best architecture configuration at any given time, for any new workload. This article proposes a novel control mechanism based on a predictive model for microarchitectural adaptivity control. This model is able to efficiently control adaptivity by monitoring the behaviour of an application’s different phases at runtime. We show that by using this model on SPEC 2000, we double the energy/performance efficiency of the processor when compared to the best static configuration tuned for the whole benchmark suite. This represents 74% of the improvement available if we know the best microarchitecture for each program phase ahead of time. In addition, we present an extended analysis of the best configurations found and show that the overheads associated with the implementation of our scheme have a negligible impact on performance and power.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W1997763350",
    "type": "article"
  },
  {
    "title": "MAPS",
    "doi": "https://doi.org/10.1145/2680544",
    "publication_date": "2014-12-08",
    "publication_year": 2014,
    "authors": "Eri Rubin; Ely Levy; Amnon Barak; Tal Ben‐Nun",
    "corresponding_authors": "",
    "abstract": "GPUs play an increasingly important role in high-performance computing. While developing naive code is straightforward, optimizing massively parallel applications requires deep understanding of the underlying architecture. The developer must struggle with complex index calculations and manual memory transfers. This article classifies memory access patterns used in most parallel algorithms, based on Berkeley’s Parallel “Dwarfs.” It then proposes the MAPS framework, a device-level memory abstraction that facilitates memory access on GPUs, alleviating complex indexing using on-device containers and iterators. This article presents an implementation of MAPS and shows that its performance is comparable to carefully optimized implementations of real-world applications.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2030961306",
    "type": "article"
  },
  {
    "title": "Extending Halide to Improve Software Development for Imaging DSPs",
    "doi": "https://doi.org/10.1145/3106343",
    "publication_date": "2017-08-30",
    "publication_year": 2017,
    "authors": "Sander Vocke; Henk Corporaal; Roel Jordans; Rosilde Corvino; Rick Nas",
    "corresponding_authors": "",
    "abstract": "Specialized Digital Signal Processors (DSPs), which can be found in a wide range of modern devices, play an important role in power-efficient, high-performance image processing. Applications including camera sensor post-processing and computer vision benefit from being (partially) mapped onto such DSPs. However, due to their specialized instruction sets and dependence on low-level code optimization, developing applications for DSPs is more time-consuming and error-prone than for general-purpose processors. Halide is a domain-specific language (DSL) that enables low-effort development of portable, high-performance imaging pipelines—a combination of qualities that is hard, if not impossible, to find among DSP programming models. We propose a set of extensions and modifications to Halide to generate code for DSP C compilers, focusing specifically on diverse SIMD target instruction sets and heterogeneous scratchpad memory hierarchies. We implement said techniques for a commercial DSP found in an Intel Image Processing Unit (IPU), demonstrating that this solution can be used to achieve performance within 20% of highly tuned, manually written C code, while leading to a reduction in code complexity. By comparing performance of Halide algorithms using our solution to results on CPU and GPU targets, we confirm the value of using DSP targets with Halide.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2582073145",
    "type": "article"
  },
  {
    "title": "ALEA",
    "doi": "https://doi.org/10.1145/3050436",
    "publication_date": "2017-03-13",
    "publication_year": 2017,
    "authors": "Lev Mukhanov; Pavlos Petoumenos; Zheng Wang; Nikos Parasyris; Dimitrios S. Nikolopoulos; Bronis R. de Supinski; Hugh Leather",
    "corresponding_authors": "",
    "abstract": "Energy efficiency is becoming increasingly important, yet few developers understand how source code changes affect the energy and power consumption of their programs. To enable them to achieve energy savings, we must associate energy consumption with software structures, especially at the fine-grained level of functions and loops. Most research in the field relies on direct power/energy measurements taken from on-board sensors or performance counters. However, this coarse granularity does not directly provide the needed fine-grained measurements. This article presents ALEA, a novel fine-grained energy profiling tool based on probabilistic analysis for fine-grained energy accounting. ALEA overcomes the limitations of coarse-grained power-sensing instruments to associate energy information effectively with source code at a fine-grained level. We demonstrate and validate that ALEA can perform accurate energy profiling at various granularity levels on two different architectures: Intel Sandy Bridge and ARM big.LITTLE. ALEA achieves a worst-case error of only 2% for coarse-grained code structures and 6% for fine-grained ones, with less than 1% runtime overhead. Our use cases demonstrate that ALEA supports energy optimizations, with energy savings of up to 2.87 times for a latency-critical option pricing workload under a given power budget.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2596871245",
    "type": "article"
  },
  {
    "title": "LD",
    "doi": "https://doi.org/10.1145/3046678",
    "publication_date": "2017-03-21",
    "publication_year": 2017,
    "authors": "Pengcheng Li; Xiaoyu Hu; Dong Chen; Jacob Brock; Hao Luo; Eddy Z. Zhang; Chen Ding",
    "corresponding_authors": "",
    "abstract": "Data race detection has become an important problem in GPU programming. Previous designs of CPU race-checking tools are mainly task parallel and incur high overhead on GPUs due to access instrumentation, especially when monitoring many thousands of threads routinely used by GPU programs. This article presents a novel data-parallel solution designed and optimized for the GPU architecture. It includes compiler support and a set of runtime techniques. It uses value-based checking, which detects the races reported in previous work, finds new races, and supports race-free deterministic GPU execution. More important, race checking is massively data parallel and does not introduce divergent branching or atomic synchronization. Its slowdown is less than 5 × for over half of the tests and 10 × on average, which is orders of magnitude more efficient than the cuda-memcheck tool by Nvidia and the methods that use fine-grained access instrumentation.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2597739964",
    "type": "article"
  },
  {
    "title": "Cache Exclusivity and Sharing",
    "doi": "https://doi.org/10.1145/3134437",
    "publication_date": "2017-11-14",
    "publication_year": 2017,
    "authors": "Chencheng Ye; Chen Ding; Hao Luo; Jacob Brock; Dong Chen; Hai Jin",
    "corresponding_authors": "",
    "abstract": "A problem on multicore systems is cache sharing, where the cache occupancy of a program depends on the cache usage of peer programs. Exclusive cache hierarchy as used on AMD processors is an effective solution to allow processor cores to have a large private cache while still benefitting from shared cache. The shared cache stores the “victims” (i.e., data evicted from private caches). The performance depends on how victims of co-run programs interact in shared cache. This article presents a new metric called the victim footprint (VFP). It is measured once per program in its solo execution and can then be combined to compute the performance of any exclusive cache hierarchy, replacing parallel testing with theoretical analysis. The work evaluates the VFP by using it to analyze cache sharing by parallel mixes of sequential programs, comparing the accuracy of the theory to hardware counter results, and measuring the benefit of exclusivity-aware analysis and optimization.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2769265599",
    "type": "article"
  },
  {
    "title": "Energy-Efficient Runtime Management of Heterogeneous Multicores using Online Projection",
    "doi": "https://doi.org/10.1145/3293446",
    "publication_date": "2018-12-31",
    "publication_year": 2018,
    "authors": "Stavros Tzilis; Pedro Trancoso; Ioannis Sourdis",
    "corresponding_authors": "",
    "abstract": "Heterogeneous multicores offer flexibility in the form of different core types and Dynamic Voltage and Frequency Scaling (DVFS), defining a vast configuration space. The optimal configuration choice is not always straightforward, even for single applications, and becomes a very difficult problem for dynamically changing scenarios of concurrent applications with unpredictable spawn and termination times and individual performance requirements. This article proposes an integrated approach for runtime decision making for energy efficiency on such systems. The approach consists of a model that predicts performance and power for any possible decision and low-complexity heuristics that use this model to evaluate a subset of possible decisions to choose the best. The model predicts performance by projecting standalone application profiling data to the current status of the system and power by using a set of platform-specific parameters that are determined only once for a given system and are independent of the application mix. Our approach is evaluated with a plethora of dynamic, multi-application scenarios. When considering best effort performance to be adequate, our runtime achieves on average 3% higher energy efficiency compared to the powersave governor and 2× better compared to the other linux governors. Moreover, when also considering individual applications’ performance requirements, our runtime is able to satisfy them, giving away 18% of the system’s energy efficiency compared to the powersave, which, however, misses the performance targets by 23%; at the same time, our runtime maintains an efficiency advantage of about 55% compared to the other governors, which also satisfy the performance constraints.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2910338735",
    "type": "article"
  },
  {
    "title": "SAQIP",
    "doi": "https://doi.org/10.1145/3311879",
    "publication_date": "2019-04-18",
    "publication_year": 2019,
    "authors": "Sahar Sargaran; Naser Mohammadzadeh",
    "corresponding_authors": "",
    "abstract": "Proposing an architecture that efficiently compensates for the inefficiencies of physical hardware with extra resources is one of the key issues in quantum computer design. Although the demonstration of quantum systems has been limited to some dozen qubits, scaling the current small-sized lab quantum systems to large-scale quantum systems that are capable of solving meaningful practical problems can be the main goal of much research. Focusing on this issue, in this article a scalable architecture for quantum information processors, called SAQIP, is proposed. Moreover, a flow is presented to map and schedule a quantum circuit on this architecture. Experimental results show that the proposed architecture and design flow decrease the average latency and the average area of quantum circuits by about 81% and 11%, respectively, for the attempted benchmarks.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2939193123",
    "type": "article"
  },
  {
    "title": "A Metric-Guided Method for Discovering Impactful Features and Architectural Insights for Skylake-Based Processors",
    "doi": "https://doi.org/10.1145/3369383",
    "publication_date": "2019-12-17",
    "publication_year": 2019,
    "authors": "Ahmad Yasin; Jawad Haj-Yahya; Yosi Ben-Asher; Avi Mendelson",
    "corresponding_authors": "",
    "abstract": "The slowdown in technology scaling puts architectural features at the forefront of the innovation in modern processors. This article presents a Metric-Guided Method (MGM) that extends Top-Down analysis with carefully selected, dynamically adapted metrics in a structured approach. Using MGM, we conduct two evaluations at the microarchitecture and the Instruction Set Architecture (ISA) levels. Our results show that simple optimizations, such as improved representation of CISC instructions, broadly improve performance, while changes in the Floating-Point execution units had mixed impact. Overall, we report 10 architectural insights—at the microarchitecture, ISA, and compiler fronts—while quantifying their impact on the SPEC CPU benchmarks.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2996353500",
    "type": "article"
  },
  {
    "title": "Fast pattern-specific routing for fat tree networks",
    "doi": "https://doi.org/10.1145/2541228.2555293",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Bogdan Prisacari; Germán Rodríguez; Cyriel Minkenberg; Torsten Hoefler",
    "corresponding_authors": "",
    "abstract": "In the context of eXtended Generalized Fat Tree (XGFT) topologies, widely used in HPC and datacenter network designs, we propose a generic method, based on Integer Linear Programming (ILP), to efficiently determine optimal routes for arbitrary workloads. We propose a novel approach that combines ILP with dynamic programming, effectively reducing the time to solution. Specifically, we divide the network into smaller subdomains optimized using a custom ILP formulation that ensures global optimality of local solutions. Local solutions are then combined into an optimal global solution using dynamic programming. Finally, we demonstrate through a series of extensive benchmarks that our approach scales in practice to networks interconnecting several thousands of nodes, using a single-threaded, freely available linear programming solver on commodity hardware, with the potential for higher scalability by means of commercial, parallel solvers.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W1969486455",
    "type": "article"
  },
  {
    "title": "HPar",
    "doi": "https://doi.org/10.1145/2541228.2555301",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Zhijia Zhao; Michael Bebenita; Dave Herman; Jianhua Sun; Xipeng Shen",
    "corresponding_authors": "",
    "abstract": "Parallelizing HTML parsing is challenging due to the complexities of HTML documents and the inherent dependencies in its parsing algorithm. As a result, despite numerous studies in parallel parsing, HTML parsing remains sequential today. It forms one of the final barriers for fully parallelizing browser operations to minimize the browser’s response time—an important variable for user experiences, especially on portable devices. This article provides a comprehensive analysis on the special complexities of parallel HTML parsing and presents a systematic exploration in overcoming those difficulties through specially designed speculative parallelizations. This work develops, to the best of our knowledge, the first pipelining and data-level parallel HTML parsers. The data-level parallel parser, named HPar , achieves up to 2.4× speedup on quadcore devices. This work demonstrates the feasibility of efficient, parallel HTML parsing for the first time and offers a set of novel insights for parallel HTML parsing",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2002952260",
    "type": "article"
  },
  {
    "title": "Efficient Power Gating of SIMD Accelerators Through Dynamic Selective Devectorization in an HW/SW Codesigned Environment",
    "doi": "https://doi.org/10.1145/2629681",
    "publication_date": "2014-07-31",
    "publication_year": 2014,
    "authors": "Rakesh Kumar; A. Martínez; Antonio González",
    "corresponding_authors": "",
    "abstract": "Leakage energy is a growing concern in current and future microprocessors. Functional units of microprocessors are responsible for a major fraction of this energy. Therefore, reducing functional unit leakage has received much attention in recent years. Power gating is one of the most widely used techniques to minimize leakage energy. Power gating turns off the functional units during the idle periods to reduce the leakage. Therefore, the amount of leakage energy savings is directly proportional to the idle time duration. This article focuses on increasing the idle interval for the higher SIMD lanes. The applications are profiled dynamically, in a hardware/software codesigned environment, to find the higher SIMD lanes' usage pattern. If the higher lanes need to be turned on for small time periods, the corresponding portion of the code is devectorized to keep the higher lanes off. The devectorized code is executed on the lowest SIMD lane. Our experimental results show that the average energy savings of the proposed mechanism are 15%, 12%, and 71% greater than power gating for SPECFP2006, Physicsbench, and Eigen benchmark suites, respectively. Moreover, the slowdown caused by devectorization is negligible.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2002991280",
    "type": "article"
  },
  {
    "title": "Could Compression Be of General Use? Evaluating Memory Compression across Domains",
    "doi": "https://doi.org/10.1145/3138805",
    "publication_date": "2017-12-05",
    "publication_year": 2017,
    "authors": "Somayeh Sardashti; David A. Wood",
    "corresponding_authors": "",
    "abstract": "Recent proposals present compression as a cost-effective technique to increase cache and memory capacity and bandwidth. While these proposals show potentials of compression, there are several open questions to adopt these proposals in real systems including the following: (1) Do these techniques work for real-world workloads running for long time? (2) Which application domains would potentially benefit the most from compression? (3) At which level of memory hierarchy should we apply compression: caches, main memory, or both? In this article, our goal is to shed light on some main questions on applicability of compression. We evaluate compression in the memory hierarchy for selected examples from different application classes. We analyze real applications with real data and complete runs of several benchmarks. While simulators provide a pretty accurate framework to study potential performance/energy impacts of ideas, they mostly limit us to a small range of workloads with short runtimes. To enable studying real workloads, we introduce a fast and simple methodology to get samples of memory and cache contents of a real machine (a desktop or a server). Compared to a cycle-accurate simulator, our methodology allows us to study real workloads as well as benchmarks. Our toolset is not a replacement for simulators but mostly complements them. While we can use a simulator to measure performance/energy impact of a particular compression proposal, here with our methodology we can study the potentials with long running workloads in early stages of the design. Using our toolset, we evaluate a collection of workloads from different domains, such as a web server of CS department of UW—Madison for 24h, Google Chrome (watching a 1h-long movie on YouTube), and Linux games (playing for about an hour). We also use several benchmarks from different domains, including SPEC, mobile, and big data. We run these benchmarks to completion. Using these workloads and our toolset, we analyze different compression properties for both real applications and benchmarks. We focus on eight main hypotheses on compression, derived from previous work on compression. These properties (Table 2) act as foundation of several proposals on compression, so performance of those proposals depends very much on these basic properties. Overall, our results suggest that compression could be of general use both in main memory and caches. On average, the compression ratio is ≥2 for 64% and 54% of workloads, respectively, for memory and cache data. Our evaluation indicates significant potential for both cache and memory compression, with higher compressibility in memory due to abundance of zero blocks. Among application domains we studied, servers show on average the highest compressibility, while our mobile benchmarks show the lowest compressibility. For comparing benchmarks with real workloads, we show that (1) it is critical to run benchmarks to completion or considerably long runtimes to avoid biased conclusions, and (2) SPEC benchmarks are good representative of real Desktop applications in terms of compressibility of their datasets. However, this does not hold for all compression properties. For example, SPEC benchmarks have much better compression locality (i.e., neighboring blocks have similar compressibility) than real workloads. Thus, it is critical for designers to consider wider range of workloads, including real applications, to evaluate their compression techniques.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2774409802",
    "type": "article"
  },
  {
    "title": "DarkCache",
    "doi": "https://doi.org/10.1145/3186895",
    "publication_date": "2018-05-01",
    "publication_year": 2018,
    "authors": "Davide Zoni; Luca Colombo; William Fornaciari",
    "corresponding_authors": "",
    "abstract": "The Last Level Cache (LLC) is a key element to improve application performance in multi-cores. To handle the worst case, the main design trend employs tiled architectures with a large LLC organized in banks, which goes underutilized in several realistic scenarios. Our proposal, named DarkCache , aims at properly powering off such unused banks to optimize the Energy-Delay Product (EDP) through an adaptive cache reconfiguration, thus aggressively reducing the leakage energy. The implemented solution is general and it can recognize and skip the activation of the DarkCache policy for the few strong memory intensive applications that actually require the use of the entire LLC. The validation has been carried out on 16- and 64-core architectures also accounting for two state-of-the-art methodologies. Compared to the baseline solution, DarkCache exhibits a performance overhead within 2% and an average EDP improvement of 32.58% and 36.41% considering 16 and 64 cores, respectively. Moreover, DarkCache shows an average EDP gain between 16.15% (16 cores) and 21.05% (64 cores) compared to the best state-of-the-art we evaluated, and it confirms a good scalability since the gain improves with the size of the architecture.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2802133576",
    "type": "article"
  },
  {
    "title": "Decoupled Fused Cache",
    "doi": "https://doi.org/10.1145/3293447",
    "publication_date": "2018-12-31",
    "publication_year": 2018,
    "authors": "Evangelos Vasilakis; Vassilis Papaefstathiou; Pedro Trancoso; Ioannis Sourdis",
    "corresponding_authors": "",
    "abstract": "DRAM caches have shown excellent potential in capturing the spatial and temporal data locality of applications capitalizing on advances of 3D-stacking technology; however, they are still far from their ideal performance. Besides the unavoidable DRAM access to fetch the requested data, tag access is in the critical path, adding significant latency and energy costs. Existing approaches are not able to remove these overheads and in some cases limit DRAM cache design options. For instance, caching DRAM cache tags adds constant latency to every access; accessing the DRAM cache using the TLB calls for OS support and DRAM cachelines as large as a page; reusing the last-level cache (LLC) tags to access the DRAM cache limits LLC performance as it requires indexing the LLC using higher-order address bits. In this article, we introduce Decoupled Fused Cache , a DRAM cache design that alleviates the cost of tag accesses by fusing DRAM cache tags with the tags of the on-chip LLC without affecting LLC performance. In essence, the Decoupled Fused Cache relies in most cases on the LLC tag access to retrieve the required information for accessing the DRAM cache while avoiding additional overheads. Compared to current DRAM cache designs of the same cacheline size, Decoupled Fused Cache improves system performance by 6% on average and by 16% to 18% for large cacheline sizes. Finally, Decoupled Fused Cache reduces DRAM cache traffic by 18% and DRAM cache energy consumption by 7%.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2911012662",
    "type": "article"
  },
  {
    "title": "GEVO",
    "doi": "https://doi.org/10.1145/3418055",
    "publication_date": "2020-11-25",
    "publication_year": 2020,
    "authors": "Jhe-Yu Liou; Xiaodong Wang; Stephanie Forrest; Carole-Jean Wu",
    "corresponding_authors": "",
    "abstract": "GPUs are a key enabler of the revolution in machine learning and high-performance computing, functioning as de facto co-processors to accelerate large-scale computation. As the programming stack and tool support have matured, GPUs have also become accessible to programmers, who may lack detailed knowledge of the underlying architecture and fail to fully leverage the GPU’s computation power. GEVO (Gpu optimization using EVOlutionary computation) is a tool for automatically discovering optimization opportunities and tuning the performance of GPU kernels in the LLVM representation. GEVO uses population-based search to find edits to GPU code compiled to LLVM-IR and improves performance on desired criteria while retaining required functionality. We demonstrate that GEVO improves the execution time of general-purpose GPU programs and machine learning (ML) models on NVIDIA Tesla P100. For the Rodinia benchmarks, GEVO improves GPU kernel runtime performance by an average of 49.48% and by as much as 412% over the fully compiler-optimized baseline. If kernel output accuracy is relaxed to tolerate up to 1% error, GEVO can find kernel variants that outperform the baseline by an average of 51.08%. For the ML workloads, GEVO achieves kernel performance improvement for SVM on the MNIST handwriting recognition (3.24×) and the a9a income prediction (2.93×) datasets with no loss of model accuracy. GEVO achieves 1.79× kernel performance improvement on image classification using ResNet18/CIFAR-10, with less than 1% model accuracy reduction.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W3109066900",
    "type": "article"
  },
  {
    "title": "The Effects of Granularity and Adaptivity on Private/Shared Classification for Coherence",
    "doi": "https://doi.org/10.1145/2790301",
    "publication_date": "2015-08-31",
    "publication_year": 2015,
    "authors": "Mahdad Davari; Alberto Ros; Erik Hägersten; Stefanos Kaxiras",
    "corresponding_authors": "",
    "abstract": "Classification of data into private and shared has proven to be a catalyst for techniques to reduce coherence cost, since private data can be taken out of coherence and resources can be concentrated on providing coherence for shared data. In this article, we examine how granularity—page-level versus cache-line level—and adaptivity—going from shared to private—affect the outcome of classification and its final impact on coherence. We create a classification technique, called Generational Classification , and a coherence protocol called Generational Coherence, which treats data as private or shared based on cache-line generations. We compare two coherence protocols based on self-invalidation/self-downgrade with respect to data classification. Our findings are enlightening: (i) Some programs benefit from finer granularity, some benefit further from adaptivity, but some do not benefit from either. (ii) Reducing the amount of shared data has no perceptible impact on coherence misses caused by self-invalidation of shared data, hence no impact on performance. (iii) In contrast, classifying more data as private has implications for protocols that employ write-through as a means of self-downgrade, resulting in network traffic reduction—up to 30%—by reducing write-through traffic.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2033726722",
    "type": "article"
  },
  {
    "title": "Low-Power High-Efficiency Video Decoding using General-Purpose Processors",
    "doi": "https://doi.org/10.1145/2685551",
    "publication_date": "2015-01-09",
    "publication_year": 2015,
    "authors": "Chi Bun Ching; Mauricio Álvarez-Mesa; Ben Juurlink",
    "corresponding_authors": "",
    "abstract": "In this article, we investigate how code optimization techniques and low-power states of general-purpose processors improve the power efficiency of HEVC decoding. The power and performance efficiency of the use of SIMD instructions, multicore architectures, and low-power active and idle states are analyzed in detail for offline video decoding. In addition, the power efficiency of techniques such as “race to idle” and “exploiting slack” with DVFS are evaluated for real-time video decoding. Results show that “exploiting slack” is more power efficient than “race to idle” for all evaluated platforms representing smartphone, tablet, laptop, and desktop computing systems.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2056321415",
    "type": "article"
  },
  {
    "title": "Efficient Correction of Anomalies in Snapshot Isolation Transactions",
    "doi": "https://doi.org/10.1145/2693260",
    "publication_date": "2015-01-09",
    "publication_year": 2015,
    "authors": "Heiner Litz; Ricardo J. Dias; David R. Cheriton",
    "corresponding_authors": "",
    "abstract": "Transactional memory systems providing snapshot isolation enable concurrent access to shared data without incurring aborts on read-write conflicts. Reducing aborts is extremely relevant as it leads to higher concurrency, greater performance, and better predictability. Unfortunately, snapshot isolation does not provide serializability as it allows certain anomalies that can lead to subtle consistency violations. While some mechanisms have been proposed to verify the correctness of a program utilizing snapshot isolation transactions, it remains difficult to repair incorrect applications. To reduce the programmer’s burden in this case, we present a technique based on dynamic code and graph dependency analysis that automatically corrects existing snapshot isolation anomalies in transactional memory programs. Our evaluation shows that corrected applications retain the performance benefits characteristic of snapshot isolation over conventional transactional memory systems.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2082484926",
    "type": "article"
  },
  {
    "title": "HMTT",
    "doi": "https://doi.org/10.1145/2579668",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "Yongbing Huang; Licheng Chen; Zehan Cui; Ruan Yuan; Yungang Bao; Mingyu Chen; Ninghui Sun",
    "corresponding_authors": "",
    "abstract": "DRAM access traces (i.e., off-chip memory references) can be extremely valuable for the design of memory subsystems and performance tuning of software. Hardware snooping on the off-chip memory interface is an effective and nonintrusive approach to monitoring and collecting real-life DRAM accesses. However, compared with software-based approaches, hardware snooping approaches typically lack semantic information, such as process/function/object identifiers, virtual addresses, and lock contexts, that is essential to the complete understanding of the systems and software under investigation. In this article, we propose a hybrid hardware/software mechanism that is able to collect off-chip memory reference traces with semantic information. We have designed and implemented a prototype system called HMTT (Hybrid Memory Trace Tool), which uses a custom-made DIMM connector to collect off-chip memory references and a high-level event-encoding scheme to correlate semantic information with memory references. In addition to providing complete, undistorted DRAM access traces, the proposed system is also able to perform various types of low-overhead profiling, such as object-relative accesses and multithread lock accesses.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2147540684",
    "type": "article"
  },
  {
    "title": "Autotuning Runtime Specialization for Sparse Matrix-Vector Multiplication",
    "doi": "https://doi.org/10.1145/2851500",
    "publication_date": "2016-03-28",
    "publication_year": 2016,
    "authors": "Buse Yılmaz; Barış Aktemur; María Jesús Garzarán; Sam Kamin; Furkan Kıraç",
    "corresponding_authors": "",
    "abstract": "Runtime specialization is used for optimizing programs based on partial information available only at runtime. In this paper we apply autotuning on runtime specialization of Sparse Matrix-Vector Multiplication to predict a best specialization method among several. In 91% to 96% of the predictions, either the best or the second-best method is chosen. Predictions achieve average speedups that are very close to the speedups achievable when only the best methods are used. By using an efficient code generator and a carefully designed set of matrix features, we show the runtime costs can be amortized to bring performance benefits for many real-world cases.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2321358174",
    "type": "article"
  },
  {
    "title": "Hardware-Accelerated Cross-Architecture Full-System Virtualization",
    "doi": "https://doi.org/10.1145/2996798",
    "publication_date": "2016-10-25",
    "publication_year": 2016,
    "authors": "Tom Spink; Harry Wagstaff; Björn Franke",
    "corresponding_authors": "",
    "abstract": "Hardware virtualization solutions provide users with benefits ranging from application isolation through server consolidation to improved disaster recovery and faster server provisioning. While hardware assistance for virtualization is supported by all major processor architectures, including Intel, ARM, PowerPC, and MIPS, these extensions are targeted at virtualization of the same architecture, for example, an x86 guest on an x86 host system. Existing techniques for cross-architecture virtualization, for example, an ARM guest on an x86 host, still incur a substantial overhead for CPU, memory, and I/O virtualization due to the necessity for software emulation of these mismatched system components. In this article, we present a new hardware-accelerated hypervisor called C aptive , employing a range of novel techniques that exploit existing hardware virtualization extensions for improving the performance of full-system cross-platform virtualization. We illustrate how (1) guest memory management unit (MMU) events and operations can be mapped onto host memory virtualization extensions, eliminating the need for costly software MMU emulation, (2) a block-based dynamic binary translation engine inside the virtual machine can improve CPU virtualization performance, (3) memory-mapped guest I/O can be efficiently translated to fast I/O specific calls to emulated devices, and (4) the cost for asynchronous guest interrupts can be reduced. For an ARM-based Linux guest system running on an x86 host with Intel VT support, we demonstrate application performance levels, based on SPEC CPU2006 benchmarks, of up to 5.88× over state-of-the-art Q emu and 2.5× on average, achieving a guest dynamic instruction throughput of up to 1280 MIPS (million instructions per second) and 915.52 MIPS, on average.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2533787090",
    "type": "article"
  },
  {
    "title": "Dynamic Colocation Policies with Reinforcement Learning",
    "doi": "https://doi.org/10.1145/3375714",
    "publication_date": "2020-03-04",
    "publication_year": 2020,
    "authors": "Yuhao Li; Dan Sun; Benjamin C. Lee",
    "corresponding_authors": "",
    "abstract": "We draw on reinforcement learning frameworks to design and implement an adaptive controller for managing resource contention. During runtime, the controller observes the dynamic system conditions and optimizes control policies that satisfy latency targets yet improve server utilization. We evaluate a physical prototype that guarantees 95th percentile latencies for a search engine and improves server utilization by up to 70%, compared to exclusively reserving servers for interactive services, for varied batch workloads in machine learning.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W3009121635",
    "type": "article"
  },
  {
    "title": "Optimizing the SSD Burst Buffer by Traffic Detection",
    "doi": "https://doi.org/10.1145/3377705",
    "publication_date": "2020-03-04",
    "publication_year": 2020,
    "authors": "Xuanhua Shi; Wei Liu; Ligang He; Hai Jin; Ming Li; Yong Chen",
    "corresponding_authors": "",
    "abstract": "Currently, HPC storage systems still use hard disk drive (HDD) as their dominant storage device. Solid state drive (SSD) is widely deployed as the buffer to HDDs. Burst buffer has also been proposed to manage the SSD buffering of bursty write requests. Although burst buffer can improve I/O performance in many cases, we find that it has some limitations such as requiring large SSD capacity and harmonious overlapping between computation phase and data flushing phase. In this article, we propose a scheme, called SSDUP+. 1 SSDUP+ aims to improve the burst buffer by addressing the above limitations. First, to reduce the demand for the SSD capacity, we develop a novel method to detect and quantify the data randomness in the write traffic. Further, an adaptive algorithm is proposed to classify the random writes dynamically. By doing so, much less SSD capacity is required to achieve the similar performance as other burst buffer schemes. Next, to overcome the difficulty of perfectly overlapping the computation phase and the flushing phase, we propose a pipeline mechanism for the SSD buffer, in which data buffering and flushing are performed in pipeline. In addition, to improve the I/O throughput, we adopt a traffic-aware flushing strategy to reduce the I/O interference in HDD. Finally, to further improve the performance of buffering random writes in SSD, SSDUP+ transforms the random writes to sequential writes in SSD by storing the data with a log structure. Further, SSDUP+ uses the AVL tree structure to store the sequence information of the data. We have implemented a prototype of SSDUP+ based on OrangeFS and conducted extensive experiments. The experimental results show that our proposed SSDUP+ can save an average of 50% SSD space while delivering almost the same performance as other common burst buffer schemes. In addition, SSDUP+ can save about 20% SSD space compared with the previous version of this work, SSDUP, while achieving 20–30% higher I/O throughput than SSDUP.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W3010437116",
    "type": "article"
  },
  {
    "title": "Inter-kernel Reuse-aware Thread Block Scheduling",
    "doi": "https://doi.org/10.1145/3406538",
    "publication_date": "2020-08-17",
    "publication_year": 2020,
    "authors": "Muhammad Huzaifa; Johnathan Alsop; Abdulrahman Mahmoud; Giordano Salvador; Matthew D. Sinclair; Sarita V. Adve",
    "corresponding_authors": "",
    "abstract": "As GPUs have become more programmable, their performance and energy benefits have made them increasingly popular. However, while GPU compute units continue to improve in performance, on-chip memories lag behind and data accesses are becoming increasingly expensive in performance and energy. Emerging GPU coherence protocols can mitigate this bottleneck by exploiting data reuse in GPU caches across kernel boundaries. Unfortunately, current GPU thread block schedulers are typically not designed to expose such reuse. This article proposes new hardware thread block schedulers that optimize inter-kernel reuse while using work stealing to preserve load balance. Our schedulers are simple, decentralized, and have extremely low overhead. Compared to a baseline round-robin scheduler, the best performing scheduler reduces average execution time and energy by 19% and 11%, respectively, in regular applications, and 10% and 8%, respectively, in irregular applications.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W3057934400",
    "type": "article"
  },
  {
    "title": "AsynGraph",
    "doi": "https://doi.org/10.1145/3416495",
    "publication_date": "2020-09-30",
    "publication_year": 2020,
    "authors": "Yu Zhang; Xiaofei Liao; Lin Gu; Hai Jin; Kan Hu; Haikun Liu; Bingsheng He",
    "corresponding_authors": "",
    "abstract": "Recently, iterative graph algorithms are proposed to be handled by GPU-accelerated systems. However, in iterative graph processing, the parallelism of GPU is still underutilized by existing GPU-based solutions. In fact, because of the power-law property of the natural graphs, the paths between a small set of important vertices (e.g., high-degree vertices) play a more important role in iterative graph processing’s convergence speed. Based on this fact, for faster iterative graph processing on GPUs, this article develops a novel system, called AsynGraph , to maximize its data parallelism. It first proposes an efficient structure-aware asynchronous processing way . It enables the state propagations of most vertices to be effectively conducted on the GPUs in a concurrent way to get a higher GPU utilization ratio through efficiently handling the paths between the important vertices. Specifically, a graph sketch (consisting of the paths between the important vertices) is extracted from the original graph to serve as a fast bridge for most state propagations. Through efficiently processing this sketch more times within each round of graph processing, higher parallelism of GPU can be utilized to accelerate most state propagations. In addition, a forward-backward intra-path processing way is also adopted to asynchronously handle the vertices on each path, aiming to further boost propagations along paths and also ensure smaller data access cost. In comparison with existing GPU-based systems, i.e., Gunrock, Groute, Tigr, and DiGraph, AsynGraph can speed up iterative graph processing by 3.06–11.52, 2.47–5.40, 2.23–9.65, and 1.41–4.05 times, respectively.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W3091559170",
    "type": "article"
  },
  {
    "title": "LargeGraph",
    "doi": "https://doi.org/10.1145/3477603",
    "publication_date": "2021-09-29",
    "publication_year": 2021,
    "authors": "Yu Zhang; Da Peng; Xiaofei Liao; Hai Jin; Haikun Liu; Lin Gu; Bingsheng He",
    "corresponding_authors": "",
    "abstract": "Many out-of-GPU-memory systems are recently designed to support iterative processing of large-scale graphs. However, these systems still suffer from long time to converge because of inefficient propagation of active vertices’ new states along graph paths. To efficiently support out-of-GPU-memory graph processing, this work designs a system LargeGraph . Different from existing out-of-GPU-memory systems, LargeGraph proposes a dependency-aware data-driven execution approach , which can significantly accelerate active vertices’ state propagations along graph paths with low data access cost and also high parallelism. Specifically, according to the dependencies between the vertices, it only loads and processes the graph data associated with dependency chains originated from active vertices for smaller access cost. Because most active vertices frequently use a small evolving set of paths for their new states’ propagation because of power-law property, this small set of paths are dynamically identified and maintained and efficiently handled on the GPU to accelerate most propagations for faster convergence, whereas the remaining graph data are handled over the CPU. For out-of-GPU-memory graph processing, LargeGraph outperforms four cutting-edge systems: Totem (5.19–11.62×), Graphie (3.02–9.41×), Garaph (2.75–8.36×), and Subway (2.45–4.15×).",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W3202489111",
    "type": "article"
  },
  {
    "title": "GRAM",
    "doi": "https://doi.org/10.1145/3441830",
    "publication_date": "2021-02-09",
    "publication_year": 2021,
    "authors": "Nhut-Minh Ho; Himeshi De Silva; Weng‐Fai Wong",
    "corresponding_authors": "",
    "abstract": "This article presents GRAM (&lt;underline&gt;G&lt;/underline&gt;PU-based &lt;underline&gt;R&lt;/underline&gt;untime &lt;underline&gt;A&lt;/underline&gt;daption for &lt;underline&gt;M&lt;/underline&gt;ixed-precision) a framework for the effective use of mixed precision arithmetic for CUDA programs. Our method provides a fine-grain tradeoff between output error and performance. It can create many variants that satisfy different accuracy requirements by assigning different groups of threads to different precision levels adaptively at runtime . To widen the range of applications that can benefit from its approximation, GRAM comes with an optional half-precision approximate math library. Using GRAM, we can trade off precision for any performance improvement of up to 540%, depending on the application and accuracy requirement.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W3126212707",
    "type": "article"
  },
  {
    "title": "Low-precision Logarithmic Number Systems",
    "doi": "https://doi.org/10.1145/3461699",
    "publication_date": "2021-07-17",
    "publication_year": 2021,
    "authors": "Syed Asad Alam; J. W. Garland; David Gregg",
    "corresponding_authors": "",
    "abstract": "Logarithmic number systems (LNS) are used to represent real numbers in many applications using a constant base raised to a fixed-point exponent making its distribution exponential. This greatly simplifies hardware multiply, divide and square root. LNS with base-2 is most common, but in this paper we show that for low-precision LNS the choice of base has a significant impact. We make four main contributions. First, LNS is not closed under addition and subtraction, so the result is approximate. We show that choosing a suitable base can manipulate the distribution to reduce the average error. Second, we show that low-precision LNS addition and subtraction can be implemented efficiently in logic rather than commonly used ROM lookup tables, the complexity of which can be reduced by an appropriate choice of base. A similar effect is shown where the result of arithmetic has greater precision than the input. Third, where input data from external sources is not expected to be in LNS, we can reduce the conversion error by selecting a LNS base to match the expected distribution of the input. Thus, there is no one base which gives the global optimum, and base selection is a trade-off between different factors. Fourth, we show that circuits realized in LNS require lower area and power consumption for short word lengths.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W3185317953",
    "type": "article"
  },
  {
    "title": "ERASE: Energy Efficient Task Mapping and Resource Management for Work Stealing Runtimes",
    "doi": "https://doi.org/10.1145/3510422",
    "publication_date": "2022-03-07",
    "publication_year": 2022,
    "authors": "Jing Chen; Madhavan Manivannan; Mustafa Abduljabbar; Miquel Pericàs",
    "corresponding_authors": "",
    "abstract": "Parallel applications often rely on work stealing schedulers in combination with fine-grained tasking to achieve high performance and scalability. However, reducing the total energy consumption in the context of work stealing runtimes is still challenging, particularly when using asymmetric architectures with different types of CPU cores. A common approach for energy savings involves dynamic voltage and frequency scaling (DVFS) wherein throttling is carried out based on factors like task parallelism, stealing relations, and task criticality. This article makes the following observations: (i) leveraging DVFS on a per-task basis is impractical when using fine-grained tasking and in environments with cluster/chip-level DVFS; (ii) task moldability, wherein a single task can execute on multiple threads/cores via work-sharing, can help to reduce energy consumption; and (iii) mismatch between tasks and assigned resources (i.e., core type and number of cores) can detrimentally impact energy consumption. In this article, we propose EneRgy Aware SchedulEr (ERASE), an intra-application task scheduler on top of work stealing runtimes that aims to reduce the total energy consumption of parallel applications. It achieves energy savings by guiding scheduling decisions based on per-task energy consumption predictions of different resource configurations. In addition, ERASE is capable of adapting to both given static frequency settings and externally controlled DVFS. Overall, ERASE achieves up to 31% energy savings and improves performance by 44% on average, compared to the state-of-the-art DVFS-based schedulers.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4220866225",
    "type": "article"
  },
  {
    "title": "Performance and Power Prediction for Concurrent Execution on GPUs",
    "doi": "https://doi.org/10.1145/3522712",
    "publication_date": "2022-03-16",
    "publication_year": 2022,
    "authors": "Diksha Moolchandani; Anshul Kumar; Smruti R. Sarangi",
    "corresponding_authors": "",
    "abstract": "The unprecedented growth of edge computing and 5G has led to an increased offloading of mobile applications to cloud servers or edge cloudlets. 1 The most prominent workloads comprise computer vision applications. Conventional wisdom suggests that computer vision workloads perform significantly well on SIMD/SIMT architectures such as GPUs owing to the dominance of linear algebra kernels in their composition. In this work, we debunk this popular belief by performing a lot of experiments with the concurrent execution of these workloads, which is the most popular pattern in which these workloads are executed on cloud servers. We show that the performance of these applications on GPUs does not scale well with an increase in the number of concurrent applications primarily because of contention at the shared resources and lack of efficient virtualization techniques for GPUs. Hence, there is a need to accurately predict the performance and power of such ensemble workloads on a GPU. Sadly, most of the prior work in the area of performance/power prediction is for only a single application. To the best of our knowledge, we propose the first machine learning-based predictor to predict the performance and power of an ensemble of applications on a GPU. In this article, we show that by using the execution statistics of stand-alone workloads and the fairness of execution when these workloads are executed with three representative microbenchmarks, we can get a reasonably accurate prediction. This is the first such work in the direction of performance and power prediction for concurrent applications that does not rely on the features extracted from concurrent executions or GPU profiling data. Our predictors achieve an accuracy of 91% and 96% in estimating the performance and power of executing two applications concurrently, respectively. We also demonstrate a method to extend our models to four or five concurrently running applications on modern GPUs.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4220877690",
    "type": "article"
  },
  {
    "title": "Fast One-Sided RDMA-Based State Machine Replication for Disaggregated Memory",
    "doi": "https://doi.org/10.1145/3587096",
    "publication_date": "2023-03-10",
    "publication_year": 2023,
    "authors": "Jingwen Du; Fang Wang; Dan Feng; Changchen Gan; Yuchao Cao; Xiaomin Zou; Fan Li",
    "corresponding_authors": "",
    "abstract": "Disaggregated memory architecture has risen in popularity for large datacenters with the advantage of improved resource utilization, failure isolation, and elasticity. Replicated state machines (RSMs) have been extensively used for reliability and consistency. In traditional RSM protocols, each replica stores replicated data and has the computing power to participate in some part of the protocols. However, traditional RSM protocols fail to work in the disaggregated memory architecture due to asymmetric resources on CPU nodes and memory nodes. This article proposes ECHO, a fast one-sided RDMA-based RSM protocol with lightweight log replication and remote applying, efficient linearizability guarantee, and fast coordinator failure recovery. ECHO enables all operations in the protocol to be efficiently executed using only one-sided RDMA, without the participation of any computing resource in the memory pool. To provide lightweight log replication and remote applying, ECHO couples the replicated log and the state machine to avoid dual-copy and performs remote applying by updating pointers. To enable efficient remote log state management, ECHO leverages a hitchhiked log state updating scheme to eliminate extra network round trips. To provide efficient linearizability guarantee, ECHO performs immediate remote applying after log replication and leverages the local locks at the coordinator to ensure linear consistency. Moreover, ECHO adopts a commit-aware log cache to make data visible immediately after being committed. To achieve fast failure recovery, ECHO leverages a commit point identification scheme to reduce the overhead of log consistency recovery. Experimental results demonstrate that ECHO outperforms the state-of-the-art RSM protocol (namely Sift) in multiple scenarios. For example, ECHO achieves 27%–52% higher throughput on typical write-intensive workloads. Moreover, ECHO reduces the consistency recovery time by three orders of magnitude for coordinator failure.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4323841535",
    "type": "article"
  },
  {
    "title": "An Efficient Hybrid Deep Learning Accelerator for Compact and Heterogeneous CNNs",
    "doi": "https://doi.org/10.1145/3639823",
    "publication_date": "2024-01-08",
    "publication_year": 2024,
    "authors": "Fareed Qararyah; Muhammad Waqar Azhar; Pedro Trancoso",
    "corresponding_authors": "",
    "abstract": "Resource-efficient Convolutional Neural Networks (CNNs) are gaining more attention. These CNNs have relatively low computational and memory requirements. A common denominator among such CNNs is having more heterogeneity than traditional CNNs. This heterogeneity is present at two levels: intra-layer type and inter-layer type. Generic accelerators do not capture these levels of heterogeneity, which harms their efficiency. Consequently, researchers have proposed model-specific accelerators with dedicated engines. When designing an accelerator with dedicated engines, one option is to dedicate one engine per CNN layer. We refer to accelerators designed with this approach as single-engine single-layer (SESL). This approach enables optimizing each engine for its specific layer. However, such accelerators are resource-demanding and unscalable. Another option is to design a minimal number of dedicated engines such that each engine handles all layers of one type. We refer to these accelerators as single-engine multiple-layer (SEML). SEML accelerators capture the inter-layer-type but not the intra-layer-type heterogeneity. We propose the Fixed Budget Hybrid CNN Accelerator (FiBHA), a hybrid accelerator composed of an SESL part and an SEML part, each processing a subset of CNN layers. FiBHA captures more heterogeneity than SEML while being more resource-aware and scalable than SESL. Moreover, we propose a novel module, Fused Inverted Residual Bottleneck (FIRB), a fine-grained and memory-light SESL architecture building block. The proposed architecture is implemented and evaluated using high-level synthesis (HLS) on different Field Programmable Gate Arrays representing various resource budgets. Our evaluation shows that FiBHA improves the throughput by up to 4 x and 2.5 x compared to state-of-the-art SESL and SEML accelerators, respectively. Moreover, FiBHA reduces memory and energy consumption compared to an SEML accelerator. The evaluation also shows that FIRB reduces the required memory by up to 54%, and energy requirements by up to 35% compared to traditional pipelining.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4390658705",
    "type": "article"
  },
  {
    "title": "Dedicated Hardware Accelerators for Processing of Sparse Matrices and Vectors: A Survey",
    "doi": "https://doi.org/10.1145/3640542",
    "publication_date": "2024-01-17",
    "publication_year": 2024,
    "authors": "Valentin Isaac--Chassande; Adrian Evans; Yves Durand; Frédéric Rousseau",
    "corresponding_authors": "",
    "abstract": "Performance in scientific and engineering applications such as computational physics, algebraic graph problems or Convolutional Neural Networks (CNN), is dominated by the manipulation of large sparse matrices—matrices with a large number of zero elements. Specialized software using data formats for sparse matrices has been optimized for the main kernels of interest: SpMV and SpMSpM matrix multiplications, but due to the indirect memory accesses, the performance is still limited by the memory hierarchy of conventional computers. Recent work shows that specific hardware accelerators can reduce memory traffic and improve the execution time of sparse matrix multiplication, compared to the best software implementations. The performance of these sparse hardware accelerators depends on the choice of the sparse format, COO , CSR , etc, the algorithm, inner-product , outer-product , Gustavson , and many hardware design choices. In this article, we propose a systematic survey which identifies the design choices of state-of-the-art accelerators for sparse matrix multiplication kernels. We introduce the necessary concepts and then present, compare, and classify the main sparse accelerators in the literature, using consistent notations. Finally, we propose a taxonomy for these accelerators to help future designers make the best choices depending on their objectives.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4390946831",
    "type": "article"
  },
  {
    "title": "FASA-DRAM: Reducing DRAM Latency with Destructive Activation and Delayed Restoration",
    "doi": "https://doi.org/10.1145/3649455",
    "publication_date": "2024-05-21",
    "publication_year": 2024,
    "authors": "Haitao Du; Yuhan Qin; Song Chen; Yi Kang",
    "corresponding_authors": "",
    "abstract": "DRAM memory is a performance bottleneck for many applications, due to its high access latency. Previous work has mainly focused on data locality, introducing small but fast regions to cache frequently accessed data, thereby reducing the average latency. However, these locality-based designs have three challenges in modern multi-core systems: (1) inter-application interference leads to random memory access traffic, (2) fairness issues prevent the memory controller from over-prioritizing data locality, and (3) write-intensive applications have much lower locality and evict substantial dirty entries. With frequent data movement between the fast in-DRAM cache and slow regular arrays, the overhead induced by moving data may even offset the performance and energy benefits of in-DRAM caching. In this article, we decouple the data movement process into two distinct phases. The first phase is Load-Reduced Destructive Activation (LRDA), which destructively promotes data into the in-DRAM cache. The second phase is Delayed Cycle-Stealing Restoration (DCSR), which restores the original data when the DRAM bank is idle. LRDA decouples the most time-consuming restoration phase from activation, and DCSR hides the restoration latency through prevalent bank-level parallelism. We propose FASA-DRAM, incorporating destructive activation and delayed restoration techniques to enable both in-DRAM caching and proactive latency-hiding mechanisms. Our evaluation shows that FASA-DRAM improves the average performance by 19.9% and reduces average DRAM energy consumption by 18.1% over DDR4 DRAM for four-core workloads, with less than 3.4% extra area overhead. Furthermore, FASA-DRAM outperforms state-of-the-art designs in both performance and energy efficiency.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4398169008",
    "type": "article"
  },
  {
    "title": "A Survey of General-purpose Polyhedral Compilers",
    "doi": "https://doi.org/10.1145/3674735",
    "publication_date": "2024-06-22",
    "publication_year": 2024,
    "authors": "Arun Thangamani; Vincent Loechner; Stéphane Genaud",
    "corresponding_authors": "",
    "abstract": "Since the 1990s, many implementations of polyhedral compilers have been written and distributed, either as source-to-source translating compilers or integrated into wider-purpose compilers. This article provides a survey on those various available implementations as of today, 2024. First, we list and describe most commonly available polyhedral schedulers and compiler implementations. Then, we compare the general-purpose polyhedral compilers using two main criteria—robustness and performance—on the PolyBench/C set of benchmarks.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4399918171",
    "type": "article"
  },
  {
    "title": "Agile C-states: A Core C-state Architecture for Latency Critical Applications Optimizing both Transition and Cold-Start Latency",
    "doi": "https://doi.org/10.1145/3674734",
    "publication_date": "2024-07-02",
    "publication_year": 2024,
    "authors": "Georgia Antoniou; Davide B. Bartolini; Haris Volos; Marios Kleanthous; Z.M. Wang; Kleovoulos Kalaitzidis; Tom Rollet; Z.G. Li; Onur Mutlu; Yiannakis Sazeides; Jawad Haj-Yahya",
    "corresponding_authors": "",
    "abstract": "Latency-critical applications running in modern datacenters exhibit irregular request arrival patterns and are implemented using multiple services with strict latency requirements (30–250μs). These characteristics render existing energy-saving idle CPU sleep states ineffective due to the performance overhead caused by the state’s transition latency. Besides the state transition latency, another important contributor to the performance overhead of sleep states is the cold-start latency, or in other words, the time required to warm up the microarchitectural state (e.g., cache contents, branch predictor metadata) that is flushed or discarded when transitioning to a lower-power state. Both the transition latency and cold-start latency can be particularly detrimental to the performance of latency critical applications with short execution times. While prior work focuses on mitigating the effects of transition and cold-start latency by optimizing request scheduling, in this work we propose a redesign of the core C-state architecture for latency-critical applications. In particular, we introduce C6Awarm, a new Agile core C-state that drastically reduces the performance overhead caused by idle sleep state transition latency and cold-start latency while maintaining significant energy savings. C6Awarm achieves its goals by (1) implementing medium-grained power gating, (2) preserving the microarchitectural state of the core, and (3) keeping the clock generator and PLL active and locked. Our analysis for a set of microservices based on an Intel Skylake server shows that C6Awarm manages to reduce the energy consumption by up to 70% with limited performance degradation (at most 2%).",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4400245999",
    "type": "article"
  },
  {
    "title": "MST: Topology-Aware Message Aggregation for Exascale Graph Processing of Traversal-Centric Algorithms",
    "doi": "https://doi.org/10.1145/3676846",
    "publication_date": "2024-07-03",
    "publication_year": 2024,
    "authors": "Xinbiao Gan; Tiejun Li; Feng Xiong; Bo Yang; Xinhai Chen; Chunye Gong; Shijie Li; Kai Lu; Qiao Li; Yiming Zhang",
    "corresponding_authors": "",
    "abstract": "This article presents MST, a communication-efficient message library for fast graph traversal on exascale clusters. The key idea is to follow the multi-level network topology to perform topology-aware message aggregation, where small messages are gathered and scattered at each level of domain. To facilitate message aggregation, we equip MST with flexible buffer management including active buffer switching and dynamic buffer expansion. We implement MST on the newest-generation Tianhe supercomputer and evaluated its performance using various traversal-centric algorithms on both synthetic trillion-scale graphs and real-world big graphs. The results show that MST-based graph traversal is orders of magnitude faster than that based on Active Messages Library (AML). For the Graph500-BFS benchmark, MST-based Tianhe (with 77.2 K nodes) outperforms the Fugaku supercomputer (with 148.5 K nodes) by 18.53%, while Fugaku is ranked No. 1 in the latest Graph500-BFS ranking (June 2023). MST also greatly improves graph processing performance on other commercial large-scale computing systems at the National Supercomputing Center in Changsha (NSCC) and WuzhenLight.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4400292057",
    "type": "article"
  },
  {
    "title": "Understanding Silent Data Corruption in Processors for Mitigating its Effects",
    "doi": "https://doi.org/10.1145/3690825",
    "publication_date": "2024-09-02",
    "publication_year": 2024,
    "authors": "S. M. Wang; Guangyan Zhang; Junyu Wei; Yang Wang; Jiesheng Wu; Qingchao Luo",
    "corresponding_authors": "",
    "abstract": "Silent Data Corruption (SDC) in processors can lead to various application-level issues, such as incorrect calculations and even data loss. Since traditional techniques are not effective in detecting these errors, it is very hard to address problems caused by SDCs in processors. For the same reason, knowledge about these SDCs in the wild is limited. In this paper, we conduct an extensive study on CPU SDCs in a large production CPU population, encompassing over one million processors. In addition to collecting overall statistics, we perform a detailed study to understand 1) whether certain processor features are particularly vulnerable and their potential impacts on applications; 2) the reproducibility of CPU SDCs and the triggering conditions (e.g., temperature) of those less reproducible SDCs; and 3) the challenges to mitigate and handle CPU SDCs. We further investigate the implications which our observations obtained from the above researches have, on the SDC fault models, SDC mitigation strategies and the future research fields. In addition, we design an efficient SDC mitigation approach called Farron, which uses prioritized testing to detect highly reproducible SDCs and temperature control to mitigate less reproducible SDCs. Our experimental results indicate that Farron can achieve better coverage of CPU SDCs with lower overall overhead, compared to the baseline used in Alibaba Cloud. This demonstrates that our observations are able to assist in SDC mitigation.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4402134771",
    "type": "article"
  },
  {
    "title": "An analysis of a resource efficient checkpoint architecture",
    "doi": "https://doi.org/10.1145/1044823.1044826",
    "publication_date": "2004-12-01",
    "publication_year": 2004,
    "authors": "Haitham Akkary; Ravi Rajwar; Srikanth T. Srinivasan",
    "corresponding_authors": "",
    "abstract": "Large instruction window processors achieve high performance by exposing large amounts of instruction level parallelism. However, accessing large hardware structures typically required to buffer and process such instruction window sizes significantly degrade the cycle time. This paper proposes a novel checkpoint processing and recovery (CPR) microarchitecture, and shows how to implement a large instruction window processor without requiring large structures thus permitting a high clock frequency.We focus on four critical aspects of a microarchitecture: (1) scheduling instructions, (2) recovering from branch mispredicts, (3) buffering a large number of stores and forwarding data from stores to any dependent load, and (4) reclaiming physical registers. While scheduling window size is important, we show the performance of large instruction windows to be more sensitive to the other three design issues. Our CPR proposal incorporates novel microarchitectural schemes for addressing these design issues---a selective checkpoint mechanism for recovering from mispredicts, a hierarchical store queue organization for fast store-load forwarding, and an effective algorithm for aggressive physical register reclamation. Our proposals allow a processor to realize performance gains due to instruction windows of thousands of instructions without requiring large cycle-critical hardware structures.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2077398506",
    "type": "article"
  },
  {
    "title": "Improving WCET by applying a WC code-positioning optimization",
    "doi": "https://doi.org/10.1145/1113841.1113842",
    "publication_date": "2005-12-01",
    "publication_year": 2005,
    "authors": "Wankang Zhao; David Whalley; Christopher Healy; Frank Mueller",
    "corresponding_authors": "",
    "abstract": "Applications in embedded systems often need to meet specified timing constraints. It is advantageous to not only calculate the worst-case execution time (WCET) of an application, but to also perform transformation, which reduce the WCET, since an application with a lower WCET will be less likely to violate its timing constraints. Some processors incur a pipeline delay whenever an instruction transfers control to a target that is not the next sequential instruction. Code-positioning optimizations attempt to reduce these delays by positioning the basic blocks to minimize the number of unconditional jumps and taken conditional branches that occur. Traditional code-positioning algorithms use profile data to find the frequently executed edges between basic blocks, then minimize the transfers of control along these edges to reduce the average case execution time (ACET). This paper introduces a WCET code-positioning optimization, driven by the worst-case (WC) path information from a timing analyzer, to reduce the WCET instead of ACET. This WCET optimization changes the layout of the code in memory to reduce the branch penalties along the WC paths. Unlike the frequency of edges in traditional profile-driven code positioning, the WC path may change after code-positioning decisions are made. Thus, WCET code positioning is inherently more challenging than ACET code positioning. The experimental results show that this optimization typically finds the optimal layout of the basic blocks with the minimal WCET. The results show over a 7% reduction in WCET is achieved after code positioning is performed.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W1977439654",
    "type": "article"
  },
  {
    "title": "Accelerated warmup for sampled microarchitecture simulation",
    "doi": "https://doi.org/10.1145/1061267.1061272",
    "publication_date": "2005-03-01",
    "publication_year": 2005,
    "authors": "John W. Haskins; Kevin Skadron",
    "corresponding_authors": "",
    "abstract": "To reduce the cost of cycle-accurate software simulation of microarchitectures, many researchers use statistical sampling: by simulating only a small, representative subset of the end-to-end dynamic instruction stream in cycle-accurate detail, simulation results complete in much less time than simulating the cycle-by-cycle progress of an entire benchmark. In order for sampled simulation results to accurately reflect the nature the full dynamic instruction stream, however, state in the simulated cache and branch predictor must match or closely approximate state as it would have appeared had cycle-accurate simulation been used for the entire simulation. Researchers typically address this issue by prefixing a period of warmup---in which cache and branch predictor state are modeled in addition to programmer-visible architected state---to each cluster of contiguous instructions in the sample.One conservative, but slow approach is to always simulate cache and branch predictor state, whether among the cycle-accurate clusters, or among the instructions preceding each cluster. To save time, warmup heuristics have been proposed, but there is no one-size-fits-all heuristic for any benchmark. More rigorous, analytical warmup approaches are necessary in order to balance the requirements of high accuracy and rapidity from sampled simulations. This paper explores this issue and in particular demonstrates the merits of memory reference reuse latency (MRRL).Relative to the IPC measured by modeling all precluster cache and branch predictor activity, MRRL generated an average error in IPC of less than 1% and simultaneously reduced simulation running times by an average of approximately 50% (or 95% of the maximum potential speedup).",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2017193067",
    "type": "article"
  },
  {
    "title": "Inter-cluster communication in VLIW architectures",
    "doi": "https://doi.org/10.1145/1250727.1250731",
    "publication_date": "2007-06-01",
    "publication_year": 2007,
    "authors": "Andrei Terechko; Henk Corporaal",
    "corresponding_authors": "",
    "abstract": "The traditional VLIW (very long instruction word) architecture with a single register file does not scale up well to address growing performance demands on embedded media processors. However, splitting a VLIW processor in smaller clusters, which are comprised of function units fully connected to local register files, can significantly improve VLSI implementation characteristics of the processor, such as speed, energy consumption, and area. In our paper we reveal that achieving the best characteristics of a clustered VLIW requires a thorough selection of an Inter-cluster Communication (ICC) model , which is the way clustering is exposed in the Instruction Set Architecture. For our study we, first, define a taxonomy of ICC models including copy operations , dedicated issue slots , extended operands , extended results , and multicast . Evaluation of the execution time of the models requires both the dynamic cycle count and clock period. We developed an advanced instruction scheduler for all the five ICC models in order to quantify the dynamic cycle counts of our multimedia C benchmarks. To assess the clock period of the ICC models we designed and laid out VLIW datapaths using the RTL hardware descriptions derived from a deeply pipelined commercial TriMedia processor. In contrast to prior art, our research shows that fully distributed register file architectures (with eight clusters in our study) often underperform compared to moderately clustered machines with two or four clusters because of explosion of the cycle count overhead in the former. Among the evaluated ICC models, performance of the copy operation model, popular both in academia and industry, is severely limited by the copy operations hampering scheduling of regular operations in high ILP (instruction-level parallelism) code. The dedicated issue slots model combats this limitation by dedicating extra VLIW issue slots purely for ICC, reaching the highest 1.74 execution time speedup relative to the unicluster. Furthermore, our VLSI experiments show that the lowest area and energy consumption of 42 and 57% relative to the unicluster, respectively, are achieved by the extended operands model, which, nevertheless, provides higher performance than the copy operation model.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2116548071",
    "type": "article"
  },
  {
    "title": "ALP",
    "doi": "https://doi.org/10.1145/1216544.1216546",
    "publication_date": "2007-03-01",
    "publication_year": 2007,
    "authors": "Ruchira Sasanka; Man-Lap Li; Sarita V. Adve; Yen-Kuang Chen; Eric Debes",
    "corresponding_authors": "",
    "abstract": "The real-time execution of contemporary complex media applications requires energy-efficient processing capabilities beyond those of current superscalar processors. We observe that the complexity of contemporary media applications requires support for multiple forms of parallelism, including ILP, TLP, and various forms of DLP, such as subword SIMD, short vectors, and streams. Based on our observations, we propose an architecture, called ALP, that efficiently integrates all of these forms of parallelism with evolutionary changes to the programming model and hardware. The novel part of ALP is a DLP technique called SIMD vectors and streams (SVectors/SStreams) , which is integrated within a conventional superscalar-based CMP/SMT architecture with subword SIMD. This technique lies between subword SIMD and vectors, providing significant benefits over the former at a lower cost than the latter. Our evaluations show that each form of parallelism supported by ALP is important. Specifically, SVectors/SStreams are effective, compared to a system with the other enhancements in ALP. They give speedups of 1.1 to 3.4X and energy-delay product improvements of 1.1 to 5.1X for applications with DLP.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2142205523",
    "type": "article"
  },
  {
    "title": "PiPA",
    "doi": "https://doi.org/10.1145/1880037.1880038",
    "publication_date": "2010-12-01",
    "publication_year": 2010,
    "authors": "Qin Zhao; Ioana Cutcutache; Weng‐Fai Wong",
    "corresponding_authors": "",
    "abstract": "Profiling and online analysis are important tasks in program understanding and feedback-directed optimization. However, fine-grained profiling and online analysis tend to seriously slow down the application. To cope with the slowdown, one may have to terminate the process early or resort to sampling. The former tends to distort the result because of warm-up effects. The latter runs the risk of missing important effects because sampling was turned off during the time that these effects appeared. A promising approach is to make use of the parallel processing capabilities of the now ubiquitous multicore processors to speed up the profiling and analysis process. In this article, we present Pipelined Profiling and Analysis (PiPA), which is a novel technique for parallelizing dynamic program profiling and analysis by taking advantage of multicore systems. In essence, the application under examination is profiled using a dynamic instrumentation tool. Optimized instrumentation code outputs the profile information in a succinct format, that we call the REP format, to buffers. This lightweight trace compression minimizes the processing overhead impinged on the application whenever a buffer is full. Another thread recovers the required information from the REP buffer. The recovered full profile is then divided up and passed to multiple threads for further analysis. To achieve the best performance, the entire system has to be well-balanced. We have implemented prototypes of PiPA using two dynamic instrumentation systems, namely DynamoRIO and Pin, thereby demonstrating its portability. Our experiments show that PiPA is able to speed up the overall profiling and analysis tasks significantly. Compared to the more than 100× slowdown of Cachegrind and the 32× slowdown of Pin dcache, we achieved a mere 10.2× slowdown on an 8-core system. In this paper, we will also describe the insights we gained in obtaining the balance needed for PiPA to perform optimally.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W1964104501",
    "type": "article"
  },
  {
    "title": "Applied inference",
    "doi": "https://doi.org/10.1145/1839667.1839670",
    "publication_date": "2010-09-01",
    "publication_year": 2010,
    "authors": "Benjamin C. Lee; David Brooks",
    "corresponding_authors": "",
    "abstract": "We propose and apply a new simulation paradigm for microarchitectural design evaluation and optimization. This paradigm enables more comprehensive design studies by combining spatial sampling and statistical inference. Specifically, this paradigm (i) defines a large, comprehensive design space, (ii) samples points from the space for simulation, and (iii) constructs regression models based on sparse simulations. This approach greatly improves the computational efficiency of microarchitectural simulation and enables new capabilities in design space exploration. We illustrate new capabilities in three case studies for a large design space of approximately 260,000 points: (i) Pareto frontier, (ii) pipeline depth, and (iii) multiprocessor heterogeneity analyses. In particular, regression models are exhaustively evaluated to identify Pareto optimal designs that maximize performance for given power budgets. These models enable pipeline depth studies in which all parameters vary simultaneously with depth, thereby more effectively revealing interactions with nondepth parameters. Heterogeneity analysis combines regression-based optimization with clustering heuristics to identify efficient design compromises between similar optimal architectures. These compromises are potential core designs in a heterogeneous multicore architecture. Increasing heterogeneity can improve bips 3 / w efficiency by as much as 2.4×, a theoretical upper bound on heterogeneity benefits that neglects contention between shared resources as well as design complexity. Collectively these studies demonstrate regression models' ability to expose trends and identify optima in diverse design regions, motivating the application of such models in statistical inference for more effective use of modern simulator infrastructure.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2017375765",
    "type": "article"
  },
  {
    "title": "DisIRer",
    "doi": "https://doi.org/10.1145/1880043.1880045",
    "publication_date": "2010-12-01",
    "publication_year": 2010,
    "authors": "Yuan‐Shin Hwang; Tzong-Yen Lin; Rong‐Guey Chang",
    "corresponding_authors": "",
    "abstract": "This article proposes an alternative yet effective way of constructing a multiplatform binary translator, by converting a retargetable compiler into a binary translator. The rationale is that a retargetable compiler usually parses source programs into an Intermediate Representation (IR), and then translates IR into object code of different targets after performing analysis and optimizations. Specifically, the mechanism of code generation for multiple platforms from IR is already in place, and the missing link of building a multiplatform binary translator is a tool to transform binary programs back into IR. In order to fill in this missing link, this article presents a tool, called the disIRer . Just as a translator from machine language to assembly language is called a disassembler, a tool that translates executable binary programs to IR is called here a disIRer. The unique feature of this approach is that the retargetability of the binary translator is inherited directly from the retargetable compiler. A prototype multiplatform binary translator has been implemented upon GCC (the GNU Compiler Collection). DisIRer first converts binary programs back into GCC IR (Intermediate Representation), and afterward the GCC backend translates the IR to target binary programs of specified platforms. Experimental results show that x86 binary programs can be translated by this technique into ARM and Alpha binaries with reasonable code density and quality.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2093122758",
    "type": "article"
  },
  {
    "title": "Comparative evaluation of memory models for chip multiprocessors",
    "doi": "https://doi.org/10.1145/1455650.1455651",
    "publication_date": "2008-11-01",
    "publication_year": 2008,
    "authors": "Jacob Leverich; Hideho Arakida; Alex Solomatnikov; Amin Firoozshahian; Mark Horowitz; Christos Kozyrakis",
    "corresponding_authors": "",
    "abstract": "There are two competing models for the on-chip memory in Chip Multiprocessor (CMP) systems: hardware-managed coherent caches and software-managed streaming memory . This paper performs a direct comparison of the two models under the same set of assumptions about technology, area, and computational capabilities. The goal is to quantify how and when they differ in terms of performance, energy consumption, bandwidth requirements, and latency tolerance for general-purpose CMPs. We demonstrate that for data-parallel applications on systems with up to 16 cores, the cache-based and streaming models perform and scale equally well. For certain applications with little data reuse, streaming scales better due to better bandwidth use and macroscopic software prefetching. However, the introduction of techniques such as hardware prefetching and nonallocating stores to the cache-based model eliminates the streaming advantage. Overall, our results indicate that there is not sufficient advantage in building streaming memory systems where all on-chip memory structures are explicitly managed. On the other hand, we show that streaming at the programming model level is particularly beneficial, even with the cache-based model, as it enhances locality and creates opportunities for bandwidth optimizations. Moreover, we observe that stream programming is actually easier with the cache-based model because the hardware guarantees correct, best-effort execution even when the programmer cannot fully regularize an application's code.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2122876246",
    "type": "article"
  },
  {
    "title": "Optimal trace scheduling using enumeration",
    "doi": "https://doi.org/10.1145/1498690.1498694",
    "publication_date": "2009-03-01",
    "publication_year": 2009,
    "authors": "Ghassan Shobaki; Kent Wilken; Mark Heffernan",
    "corresponding_authors": "",
    "abstract": "This article presents the first optimal algorithm for trace scheduling. The trace is a global scheduling region used by compilers to exploit instruction-level parallelism across basic block boundaries. Several heuristic techniques have been proposed for trace scheduling, but the precision of these techniques has not been studied relative to optimality. This article describes a technique for finding provably optimal trace schedules, where optimality is defined in terms of a weighted sum of schedule lengths across all code paths in a trace. The optimal algorithm uses branch-and-bound enumeration to efficiently explore the entire solution space. Experimental evaluation of the algorithm shows that, with a time limit of 1 s per problem, 91% of the hard trace scheduling problems in the SPEC CPU 2006 Integer Benchmarks are solved optimally. For 58% of these hard problems, the optimal schedule is improved compared to that produced by a heuristic scheduler with a geometric mean improvement of 3.2% in weighted schedule length and 18% in compensation code size.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2094304406",
    "type": "article"
  },
  {
    "title": "Dynamic QoS management for chip multiprocessors",
    "doi": "https://doi.org/10.1145/2355585.2355590",
    "publication_date": "2012-09-01",
    "publication_year": 2012,
    "authors": "Bin Li; Li-Shiuan Peh; Li Zhao; Ravi Iyer",
    "corresponding_authors": "",
    "abstract": "With the continuing scaling of semiconductor technologies, chip multiprocessor (CMP) has become the de facto design for modern high performance computer architectures. It is expected that more and more applications with diverse requirements will run simultaneously on the CMP platform. However, this will exert contention on shared resources such as the last level cache, network-on-chip bandwidth and off-chip memory bandwidth, thus affecting the performance and quality-of-service (QoS) significantly. In this environment, efficient resource sharing and a guarantee of a certain level of performance is highly desirable. Researchers have proposed different frameworks for providing QoS. Most of these frameworks focus on individual resource for QoS management. Coordinated management of multiple QoS-aware shared resources at runtime remains an open problem. Recently, there has been work that proposed a class-of-serviced based framework to jointly managing cache, NoC and memory resources simultaneously. However, the work allocates shared resources statically at the beginning of application runtime, and do not dynamically track, manage and share shared resources across applications. In this article, we address this limitation by proposing dynamic resource management policies that monitor the resource usage of applications at runtime, then steals resources from the high-priority applications for lower-priority ones. The goal is to maintain the targeted level of performance for high-priority applications while improving the performance of lower-priority applications. We use a PI (Proportional-Integral gain) feedback controller based technique to maintain stability in our framework. Our evaluation results show that our policy can improve performance for lower-priority applications significantly while maintaining the performance for high-priority application, thus demonstrating the effectiveness of our dynamic QoS resource management policy.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W1994825940",
    "type": "article"
  },
  {
    "title": "TL-plane-based multi-core energy-efficient real-time scheduling algorithm for sporadic tasks",
    "doi": "https://doi.org/10.1145/2086696.2086726",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Dongsong Zhang; Deke Guo; Fangyuan Chen; Fei Wu; Tong Wu; Ting Cao; Shiyao Jin",
    "corresponding_authors": "",
    "abstract": "As the energy consumption of multi-core systems becomes increasingly prominent, it's a challenge to design an energy-efficient real-time scheduling algorithm in multi-core systems for reducing the system energy consumption while guaranteeing the feasibility of real-time tasks. In this paper, we focus on multi-core processors, with the global Dynamic Voltage Frequency Scaling (DVFS) and Dynamic Power Management (DPM) technologies. In this setting, we propose an energy-efficient real-time scheduling algorithm, the Time Local remaining execution plane based Dynamic Voltage Frequency Scaling (TL-DVFS). TL-DVFS utilizes the concept of Time Local remaining execution (TL) plane to dynamically scale the voltage and frequency of a processor at the initial time of each TL plane as well as at the release time of a sporadic task in each TL plane. Consequently, TL-DVFS can obtain a reasonable tradeoff between the real-time constraint and the energy-saving while realizing the optimal feasibility of sporadic tasks. Mathematical analysis and extensive simulations demonstrate that TL-DVFS always saves more energy than existing algorithms, especially in the case of high workloads, and guarantees the optimal feasibility of sporadic tasks at the same time.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2027297729",
    "type": "article"
  },
  {
    "title": "The gradient-based cache partitioning algorithm",
    "doi": "https://doi.org/10.1145/2086696.2086723",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "William Hasenplaugh; Pritpal S. Ahuja; Aamer Jaleel; Simon C. Steely; Joel Emer",
    "corresponding_authors": "",
    "abstract": "This paper addresses the problem of partitioning a cache between multiple concurrent threads and in the presence of hardware prefetching. Cache replacement designed to preserve temporal locality (e.g., LRU) will allocate cache resources proportional to the miss-rate of each competing thread irrespective of whether the cache space will be utilized [Qureshi and Patt 2006]. This is clearly suboptimal as applications vary dramatically in their use of recently accessed data. We address this problem by partitioning a shared cache such that a global goodness metric is optimized. This paper introduces the Gradient-based Cache Partitioning Algorithm (GPA), whose variants optimize either hitrate, total instructions per cycle (IPC) or a weighted IPC metric designed to enforce Quality of Service (QoS) [Iyer 2004]. In the context of QoS, GPA enables us to obtain the maximum throughput of low-priority threads, while ensuring high performance on high-priority threads. The GPA mechanism is robust, low-cost, integrates easily with existing cache designs and improves the throughput of an in-order 8-core system sharing an 8MB L3 cache by ∼14%.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2028301031",
    "type": "article"
  },
  {
    "title": "ReNIC",
    "doi": "https://doi.org/10.1145/2086696.2086719",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Yaozu Dong; Yu Chen; Zhenhao Pan; Jinquan Dai; Yunhong Jiang",
    "corresponding_authors": "",
    "abstract": "Virtualization is gaining popularity in cloud computing and has become the key enabling technology in cloud infrastructure. By replicating the virtual server state to multiple independent platforms, virtualization improves the reliability and availability of cloud systems. Unfortunately, existing Virtual Machine (VM) replication solutions were designed only for software virtualized I/O, which suffers from large performance and scalability overheads. Although hardware-assisted I/O virtualization (such as SR-IOV) can achieve close to native performance and very good scalability, they cannot be properly replicated across different physical machines due to architectural limitations (such as lack of efficient device state read/write, buffering outbound packets, etc.). In this paper, we address those architectural limitations, by proposing ReNIC, an architectural extension to SR-IOV I/O virtualization for efficient I/O replications. We have extended Xen hypervisor and the Remus rapid checkpoint solution to support this new architectural extension. We developed a system simulator on multi-core systems to extensively evaluate ReNIC. The experimental results demonstrate that ReNIC achieves up to 54% CPU usage reduction, compared to software based I/O virtualization at runtime, and up to 16.2% performance advantage over software based I/O virtualization in rapid checkpoint. During migration, ReNIC reduces service shutdown time by about 50%, compared to device emulation and paravirtualized I/O, and over 71% compared to teaming driver.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2048335062",
    "type": "article"
  },
  {
    "title": "Making wide-issue VLIW processors viable on FPGAs",
    "doi": "https://doi.org/10.1145/2086696.2086712",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Madhura Purnaprajna; Paolo Ienne",
    "corresponding_authors": "",
    "abstract": "Soft and highly-customized processors are emerging as a common way to efficiently control large amount of computing resources available on FPGAs. Yet, some processor architectures of choice for DSP and media applications, such as wide-issue VLIW processors, remain impractical: the multi-ported register file makes a very inefficient use of the resources in the FPGA fabric. This paper proposes modifications to existing FPGAs to make soft-VLIW processor viable. We introduce an embedded multi-ported RAM that can be customized to match the issue-width of VLIW processors. To ascertain the benefits of this approach, we map an extensible VLIW processor onto a standard FPGA from Xilinx. For the register file implemented in the modified FPGA, the area occupied is at least 102× smaller and the dynamic power is reduced by 41% as compared to the implementation using configurable logic blocks in existing standard FPGAs. A subset of this embedded multi-ported RAM can also be used for mapping the register file in soft-RISC processors. For the soft-RISC processor, the register file in the modified FPGA is at least 22× smaller than its equivalent that uses configurable logic blocks and 1.5× the size in comparison to the implementation using block RAMs. Reduction of routing area and the maximum net length is about 39% and 51% respectively for RISC processors. As a result, this approach works towards enhancing soft-processor density in FPGAs by orders of magnitude.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2004232634",
    "type": "article"
  },
  {
    "title": "ReSense",
    "doi": "https://doi.org/10.1145/2541228.2555298",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Tanima Dey; Wei Wang; Jack W. Davidson; Mary Lou Soffa",
    "corresponding_authors": "",
    "abstract": "To utilize the full potential of modern chip multiprocessors and obtain scalable performance improvements, it is critical to mitigate resource contention created by multithreaded workloads. In this article, we describe ReSense, the first runtime system that uses application characteristics to dynamically map multithreaded applications from dynamic workloads—workloads where multithreaded applications arrive, execute, and terminate continuously in unpredictable ways. ReSense mitigates contention for the shared resources in the memory hierarchy by applying a novel thread-mapping algorithm that dynamically adjusts the mapping of threads from dynamic workloads using a precalculated sensitivity score. The sensitivity score quantifies an application's sensitivity to sharing a particular memory resource and is calculated by an efficient characterization process that involves running the multithreaded application by itself on the target platform. To measure ReSense's effectiveness, sensitivity scores were determined for 21 benchmarks from PARSEC-2.1 and NPB-OMP-3.3 for the shared resources in the memory hierarchy on four different platforms. Using three different-sized dynamic workloads composed of randomly selected two, four, and eight corunning benchmarks with randomly selected start times, ReSense was able to improve the average response time of the three workloads by up to 27.03%, 20.89%, and 29.34% and throughput by up to 19.97%, 46.56%, and 29.86%, respectively, over the native OS on real hardware. By estimating and comparing ReSense's effectiveness with the optimal thread mapping for two different workloads, we found that the maximum average difference with the experimentally determined optimal performance was 1.49% for average response time and 2.08% for throughput.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2007059865",
    "type": "article"
  },
  {
    "title": "Hybrid analytical modeling of pending cache hits, data prefetching, and MSHRs",
    "doi": "https://doi.org/10.1145/2019608.2019609",
    "publication_date": "2011-10-01",
    "publication_year": 2011,
    "authors": "Xi E. Chen; Tor M. Aamodt",
    "corresponding_authors": "",
    "abstract": "This article proposes techniques to predict the performance impact of pending cache hits, hardware prefetching, and miss status holding register resources on superscalar microprocessors using hybrid analytical models. The proposed models focus on timeliness of pending hits and prefetches and account for a limited number of MSHRs. They improve modeling accuracy of pending hits by 3.9× and when modeling data prefetching, a limited number of MSHRs, or both, these techniques result in average errors of 9.5% to 17.8%. The impact of non-uniform DRAM memory latency is shown to be approximated well by using a moving average of memory access latency.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2014468639",
    "type": "article"
  },
  {
    "title": "C1C",
    "doi": "https://doi.org/10.1145/2541228.2555308",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Yong Li; Yaojun Zhang; Hai Li; Yiran Chen; Alex K. Jones",
    "corresponding_authors": "",
    "abstract": "Spin-Transfer Torque RAM (STT-RAM), a promising alternative to SRAM for reducing leakage power consumption, has been widely studied to mitigate the impact of its asymmetrically long write latency. Recently, STT-RAM has been proposed for L1 caches by relaxing the data retention time to improve write performance and dynamic energy. However, as the technology scales down from 65nm to 22nm, the performance of the read operation scales poorly due to reduced sense margins and sense amplifier delays. In this article, we leverage a dual-mode STT memory cell to design a configurable L1 cache architecture termed C1C to mitigate read performance barriers with technology scaling. Guided by application access characteristics discovered through novel compiler analyses, the proposed cache adaptively switches between a high performance and a low-power access mode. Our evaluation demonstrates that the proposed cache with compiler guidance outperforms a state-of-the-art STT-RAM cache design by 9% with high dynamic energy efficiency, leading to significant performance/watt improvements over several competing approaches.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2015426867",
    "type": "article"
  },
  {
    "title": "Hardware support for accurate per-task energy metering in multicore systems",
    "doi": "https://doi.org/10.1145/2541228.2555291",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Qixiao Liu; Miquel Moretó; Víctor Jiménez; Jaume Abella; Francisco J. Cazorla; Mateo Valero",
    "corresponding_authors": "",
    "abstract": "Accurately determining the energy consumed by each task in a system will become of prominent importance in future multicore-based systems because it offers several benefits, including (i) better application energy/performance optimizations, (ii) improved energy-aware task scheduling, and (iii) energy-aware billing in data centers. Unfortunately, existing methods for energy metering in multicores fail to provide accurate energy estimates for each task when several tasks run simultaneously. This article makes a case for accurate Per-Task Energy Metering (PTEM) based on tracking the resource utilization and occupancy of each task. Different hardware implementations with different trade-offs between energy prediction accuracy and hardware-implementation complexity are proposed. Our evaluation shows that the energy consumed in a multicore by each task can be accurately measured. For a 32-core, 2-way, simultaneous multithreaded core setup, PTEM reduces the average accuracy error from more than 12% when our hardware support is not used to less than 4% when it is used. The maximum observed error for any task in the workload we used reduces from 58% down to 9% when our hardware support is used.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2038007362",
    "type": "article"
  },
  {
    "title": "Improved loop tiling based on the removal of spurious false dependences",
    "doi": "https://doi.org/10.1145/2400682.2400711",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Riyadh Baghdadi; Albert Cohen; Sven Verdoolaege; Konrad Trifunović",
    "corresponding_authors": "",
    "abstract": "To preserve the validity of loop nest transformations and parallelization, data dependences need to be analyzed. Memory dependences come in two varieties: true dependences or false dependences. While true dependences must be satisfied in order to preserve the correct order of computations, false dependences are induced by the reuse of a single memory location to store multiple values. False dependences reduce the degrees of freedom for loop transformations. In particular, loop tiling is severely limited in the presence of these dependences. While array expansion removes all false dependences, the overhead on memory and the detrimental impact on register-level reuse can be catastrophic. We propose and evaluate a compilation technique to safely ignore a large number of false dependences in order to enable loop nest tiling in the polyhedral model. It is based on the precise characterization of interferences between live range intervals, and it does not incur any scalar or array expansion. Our algorithms have been implemented in the Pluto polyhedral compiler, and evaluated on the PolyBench suite.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2046382913",
    "type": "article"
  },
  {
    "title": "Automatic data allocation and buffer management for multi-GPU machines",
    "doi": "https://doi.org/10.1145/2544100",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Thejas Ramashekar; Uday Bondhugula",
    "corresponding_authors": "",
    "abstract": "Multi-GPU machines are being increasingly used in high-performance computing. Each GPU in such a machine has its own memory and does not share the address space either with the host CPU or other GPUs. Hence, applications utilizing multiple GPUs have to manually allocate and manage data on each GPU. Existing works that propose to automate data allocations for GPUs have limitations and inefficiencies in terms of allocation sizes, exploiting reuse, transfer costs, and scalability. We propose a scalable and fully automatic data allocation and buffer management scheme for affine loop nests on multi-GPU machines. We call it the Bounding-Box-based Memory Manager (BBMM). BBMM can perform at runtime , during standard set operations like union, intersection, and difference, finding subset and superset relations on hyperrectangular regions of array data (bounding boxes). It uses these operations along with some compiler assistance to identify, allocate, and manage data required by applications in terms of disjoint bounding boxes. This allows it to (1) allocate exactly or nearly as much data as is required by computations running on each GPU, (2) efficiently track buffer allocations and hence maximize data reuse across tiles and minimize data transfer overhead, and (3) and as a result, maximize utilization of the combined memory on multi-GPU machines. BBMM can work with any choice of parallelizing transformations, computation placement, and scheduling schemes, whether static or dynamic. Experiments run on a four-GPU machine with various scientific programs showed that BBMM reduces data allocations on each GPU by up to 75% compared to current allocation schemes, yields performance of at least 88% of manually written code, and allows excellent weak scaling.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2051388139",
    "type": "article"
  },
  {
    "title": "Efficient Cache Performance Modeling in GPUs Using Reuse Distance Analysis",
    "doi": "https://doi.org/10.1145/3291051",
    "publication_date": "2018-12-19",
    "publication_year": 2018,
    "authors": "Mohsen Kiani; Amir Rajabzadeh",
    "corresponding_authors": "",
    "abstract": "Reuse distance analysis (RDA) is a popular method for calculating locality profiles and modeling cache performance. The present article proposes a framework to apply the RDA algorithm to obtain reuse distance profiles in graphics processing unit (GPU) kernels. To study the implications of hardware-related parameters in RDA, two RDA algorithms were employed, including a high-level cache-independent RDA algorithm, called HLRDA, and a detailed RDA algorithm, called DRDA. DRDA models the effects of reservation fails in cache blocks and miss status holding registers to provide accurate cache-related performance metrics. In this case, the reuse profiles are cache-specific. In a selection of GPU kernels, DRDA obtained the L1 miss-rate breakdowns with an average error of 3.86% and outperformed the state-of-the-art RDA in terms of accuracy. In terms of performance, DRDA is 246,000× slower than the real GPU executions and 11× faster than GPGPU-Sim. HLRDA ignores the cache-related parameters and its obtained reuse profiles are general, which can be used to calculate miss rates in all cache sizes. Moreover, the average error incurred by HLRDA was 16.9%.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2903738101",
    "type": "article"
  },
  {
    "title": "AVPP",
    "doi": "https://doi.org/10.1145/3239567",
    "publication_date": "2018-12-07",
    "publication_year": 2018,
    "authors": "Lois Orosa; Rodolfo Azevedo; Onur Mutlu",
    "corresponding_authors": "",
    "abstract": "Value prediction improves instruction level parallelism in superscalar processors by breaking true data dependencies. Although this technique can significantly improve overall performance, most of the state-of-the-art value prediction approaches require high hardware cost, which is the main obstacle for its wide adoption in current processors. To tackle this issue, we revisit load value prediction as an efficient alternative to the classical approaches that predict all instructions. By speculating only on loads, the pressure over shared resources (e.g., the Physical Register File) and the predictor size can be substantially reduced (e.g., more than 90% reduction compared to recent works). We observe that existing value predictors cannot achieve very high performance when speculating only on load instructions. To solve this problem, we propose a new, accurate and low-cost mechanism for predicting the values of load instructions: the Address-first Value-next Predictor with Value Prefetching (AVPP). The key idea of our predictor is to predict the load address first (which, we find, is much more predictable than the value) and to use a small non-speculative Value Table (VT)—indexed by the predicted address—to predict the value next. To increase the coverage of AVPP, we aim to increase the hit rate of the VT by predicting also the load address of a future instance of the same load instruction and prefetching its value in the VT. We show that AVPP is relatively easy to implement, requiring only 2.5% of the area of a 32KB L1 data cache. We compare our mechanism with five state-of-the-art value prediction techniques, evaluated within the context of load value prediction, in a relatively narrow out-of-order processor. On average, our AVPP predictor achieves 11.2% speedup and 3.7% of energy savings over the baseline processor, outperforming all the state-of-the-art predictors in 16 of the 23 benchmarks we evaluate. We evaluate AVPP implemented together with different prefetching techniques, showing additive performance gains (20% average speedup). In addition, we propose a new taxonomy to classify different value predictor policies regarding predictor update, predictor availability, and in-flight pending updates. We evaluate these policies in detail.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2904519778",
    "type": "article"
  },
  {
    "title": "Accelerating Synchronization Using Moving Compute to Data Model at 1,000-core Multicore Scale",
    "doi": "https://doi.org/10.1145/3300208",
    "publication_date": "2019-02-13",
    "publication_year": 2019,
    "authors": "Halit Dogan; Masab Ahmad; Brian Kahne; Omer Khan",
    "corresponding_authors": "",
    "abstract": "Thread synchronization using shared memory hardware cache coherence paradigm is prevalent in multicore processors. However, as the number of cores increase on a chip, cache line ping-pong prevents performance scaling for algorithms that deploy fine-grain synchronization. This article proposes an in-hardware moving computation to data model (MC) that pins shared data at dedicated cores. The critical code sections are serialized and executed at these cores in a spatial setting to enable data locality optimizations. In-hardware messages enable non-blocking and blocking communication between cores, without involving the cache coherence protocol. The in-hardware MC model is implemented on Tilera Tile-Gx72 multicore platform to evaluate 8- to 64-core count scale. A simulated RISC-V multicore environment is built to further evaluate the performance scaling advantages of the MC model at 1,024-cores scale. The evaluation using graph and machine-learning benchmarks illustrates that atomic instructions based synchronization scales up to 512 cores, and the MC model at the same core count outperforms by 27% in completion time and 39% in dynamic energy consumption.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2913778848",
    "type": "article"
  },
  {
    "title": "Flextended Tiles",
    "doi": "https://doi.org/10.1145/3369382",
    "publication_date": "2019-12-17",
    "publication_year": 2019,
    "authors": "Jie Zhao; Albert Cohen",
    "corresponding_authors": "",
    "abstract": "Loop tiling to exploit data locality and parallelism plays an essential role in a variety of general-purpose and domain-specific compilers. Affine transformations in polyhedral frameworks implement classical forms of rectangular and parallelogram tiling, but these lead to pipelined start with rather inefficient wavefront parallelism. Multiple extensions to polyhedral compilers evaluated sophisticated shapes such as trapezoid or diamond tiles, enabling concurrent start along the axes of the iteration space; yet these resort to custom schedulers and code generators insufficiently integrated within the general framework. One of these modified shapes referred to as overlapped tiling also lacks a unifying framework to reason about its composition with affine transformations; this prevents its application in general-purpose loop-nest optimizers and the fair comparison with other techniques. We revisit overlapped tiling, recasting it as an affine transformation on schedule trees composable with any affine scheduling algorithm. We demonstrate how to derive tighter tile shapes with less redundant computations. Our method models the traditional “scalene trapezoid” shapes and novel “right-rectangle” variants. It goes beyond the state of the art by avoiding the restriction to a domain-specific language or introducing post-pass rescheduling and custom code generation. We conduct experiments on the PolyMage benchmarks and iterated stencils, validating the effectiveness and applicability of our technique on both general-purpose multicores and GPU accelerators.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2995257150",
    "type": "article"
  },
  {
    "title": "ShiftsReduce: Minimizing Shifts in Racetrack Memory 4.0",
    "doi": null,
    "publication_date": "2019-12-26",
    "publication_year": 2019,
    "authors": "Asif Ali Khan; Fazal Hameed; Robin Bläsing; S. Parkin; Jerónimo Castrillón",
    "corresponding_authors": "",
    "abstract": "Racetrack memories (RMs) have significantly evolved since their conception in 2008, making them a serious contender in the field of emerging memory technologies. Despite key technological advancements, the access latency and energy consumption of an RM-based system are still highly influenced by the number of shift operations. These operations are required to move bits to the right positions in the racetracks. This article presents data-placement techniques for RMs that maximize the likelihood that consecutive references access nearby memory locations at runtime, thereby minimizing the number of shifts. We present an integer linear programming (ILP) formulation for optimal data placement in RMs, and we revisit existing offset assignment heuristics, originally proposed for random-access memories. We introduce a novel heuristic tailored to a realistic RM and combine it with a genetic search to further improve the solution. We show a reduction in the number of shifts of up to 52.5%, outperforming the state of the art by up to 16.1%.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W3100951578",
    "type": "article"
  },
  {
    "title": "The Impact of the SIMD Width on Control-Flow and Memory Divergence",
    "doi": "https://doi.org/10.1145/2687355",
    "publication_date": "2015-01-09",
    "publication_year": 2015,
    "authors": "Thomas Schaub; Simon Moll; Ralf Karrenberg; Sebastian Hack",
    "corresponding_authors": "",
    "abstract": "Power consumption is a prevalent issue in current and future computing systems. SIMD processors amortize the power consumption of managing the instruction stream by executing the same instruction in parallel on multiple data. Therefore, in the past years, the SIMD width has steadily increased, and it is not unlikely that it will continue to do so. In this article, we experimentally study the influence of the SIMD width to the execution of data-parallel programs. We investigate how an increasing SIMD width (up to 1024) influences control-flow divergence and memory-access divergence, and how well techniques to mitigate them will work on larger SIMD widths. We perform our study on 76 OpenCL applications and show that a group of programs scales well up to SIMD width 1024, whereas another group of programs increasingly suffers from control-flow divergence. For those programs, thread regrouping techniques may become increasingly important for larger SIMD widths. We show what average speedups can be expected when increasing the SIMD width. For example, when switching from scalar execution to SIMD width 64, one can expect a speedup of 60.11, which increases to 62.46 when using thread regrouping. We also analyze the frequency of regular (uniform, consecutive) memory access patterns and observe a monotonic decrease of regular memory accesses from 82.6 at SIMD width 4 to 43.1% at SIMD width 1024.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W1965061255",
    "type": "article"
  },
  {
    "title": "Automated Fine-Grained CPU Provisioning for Virtual Machines",
    "doi": "https://doi.org/10.1145/2637480",
    "publication_date": "2014-07-31",
    "publication_year": 2014,
    "authors": "Davide B. Bartolini; Filippo Sironi; Donatella Sciuto; Marco D. Santambrogio",
    "corresponding_authors": "",
    "abstract": "Ideally, the pay-as-you-go model of Infrastructure as a Service (IaaS) clouds should enable users to rent just enough resources (e.g., CPU or memory bandwidth) to fulfill their service level objectives (SLOs). Achieving this goal is hard on current IaaS offers, which require users to explicitly specify the amount of resources to reserve; this requirement is nontrivial for users, because estimating the amount of resources needed to attain application-level SLOs is often complex, especially when resources are virtualized and the service provider colocates virtual machines (VMs) on host nodes. For this reason, users who deploy VMs subject to SLOs are usually prone to overprovisioning resources, thus resulting in inflated business costs. This article tackles this issue with AutoPro : a runtime system that enhances IaaS clouds with automated and fine-grained resource provisioning based on performance SLOs. Our main contribution with AutoPro is filling the gap between application-level performance SLOs and allocation of a contended resource, without requiring explicit reservations from users. In this article, we focus on CPU bandwidth allocation to throughput-driven, compute-intensive multithreaded applications colocated on a multicore processor; we show that a theoretically sound, yet simple, control strategy can enable automated fine-grained allocation of this contended resource, without the need for offline profiling. Additionally, AutoPro helps service providers optimize infrastructure utilization by provisioning idle resources to best-effort workloads, so as to maximize node-level utilization. Our extensive experimental evaluation confirms that AutoPro is able to automatically determine and enforce allocations to meet performance SLOs while maximizing node-level utilization by supporting batch workloads on a best-effort basis.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W1990731757",
    "type": "article"
  },
  {
    "title": "Dynamic MIPS Rate Stabilization for Complex Processors",
    "doi": "https://doi.org/10.1145/2714575",
    "publication_date": "2015-04-02",
    "publication_year": 2015,
    "authors": "Jinho Suh; Chieh-Ting Huang; Michel Dubois",
    "corresponding_authors": "",
    "abstract": "Modern microprocessor cores reach their high performance levels with the help of high clock rates, parallel and speculative execution of a large number of instructions, and vast cache hierarchies. Modern cores also have adaptive features to regulate power and temperature and avoid thermal emergencies. All of these features contribute to highly unpredictable execution times. In this article, we demonstrate that the execution time of in-order (IO), out-of-order (OoO), and OoO simultaneous multithreaded processors can be stable and predictable by stabilizing their mega instructions executed per second (MIPS) rate via a proportional, integral, and differential (PID) gain feedback controller and dynamic voltage and frequency scaling (DVFS). Processor cores in idle cycles are continuously consuming power, which is highly undesirable in systems, especially in real-time systems. In addition to meeting deadlines in real-time systems, our MIPS rate stabilization framework can be applied on top of it to reduce power and energy by avoiding idle cycles. If processors are equipped with MIPS rate stabilization, the execution time can be predicted. Because the MIPS rate remains steady, a stabilized processor meets deadlines on time in real-time systems or in systems with quality-of-service execution latency requirements at the lowest possible frequency. To demonstrate and evaluate this capability, we have selected a subset of the MiBench benchmarks with the widest execution rate variations. We stabilize their MIPS rate on a 1GHz Pentium III--like OoO single-thread microarchitecture, a 1.32GHz StrongARM-like IO microarchitecture, and the 1GHz OoO processor augmented with two-way and four-way simultaneous multithreading. Both IO and OoO cores can take advantage of the stabilization framework, but the energy per instruction of the stabilized OoO core is less because it runs at a lower frequency to meet the same deadlines. The MIPS rate stabilization of complex processors using a PID feedback control loop is a general technique applicable to environments in which lower power or energy coupled with steady, predictable performance are desirable, although we target more specifically real-time systems in this article.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2006928869",
    "type": "article"
  },
  {
    "title": "DPCS",
    "doi": "https://doi.org/10.1145/2792982",
    "publication_date": "2015-08-31",
    "publication_year": 2015,
    "authors": "Mark Gottscho; Abbas BanaiyanMofrad; Nikil Dutt; Alex Nicolau; Puneet Gupta",
    "corresponding_authors": "",
    "abstract": "Fault-Tolerant Voltage-Scalable (FTVS) SRAM cache architectures are a promising approach to improve energy efficiency of memories in the presence of nanoscale process variation. Complex FTVS schemes are commonly proposed to achieve very low minimum supply voltages, but these can suffer from high overheads and thus do not always offer the best power/capacity trade-offs. We observe on our 45nm test chips that the “fault inclusion property” can enable lightweight fault maps that support multiple runtime supply voltages. Based on this observation, we propose a simple and low-overhead FTVS cache architecture for power/capacity scaling. Our mechanism combines multilevel voltage scaling with optional architectural support for power gating of blocks as they become faulty at low voltages. A static (SPCS) policy sets the runtime cache VDD once such that a only a few cache blocks may be faulty in order to minimize the impact on performance. We describe a Static Power/Capacity Scaling (SPCS) policy and two alternate Dynamic Power/Capacity Scaling (DPCS) policies that opportunistically reduce the cache voltage even further for more energy savings. This architecture achieves lower static power for all effective cache capacities than a recent more complex FTVS scheme. This is due to significantly lower overheads, despite the inability of our approach to match the min-VDD of the competing work at a fixed target yield. Over a set of SPEC CPU2006 benchmarks on two system configurations, the average total cache (system) energy saved by SPCS is 62% (22%), while the two DPCS policies achieve roughly similar energy reduction, around 79% (26%). On average, the DPCS approaches incur 2.24% performance and 6% area penalties.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2016391673",
    "type": "article"
  },
  {
    "title": "Beyond reuse distance analysis",
    "doi": "https://doi.org/10.1145/2541228.2555309",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Naznin Fauzia; Venmugil Elango; M. Ravishankar; J. Ramanujam; Fabrice Rastello; Atanas Rountev; Louis-Noël Pouchet; P. Sadayappan",
    "corresponding_authors": "",
    "abstract": "Emerging computer architectures will feature drastically decreased flops/byte (ratio of peak processing rate to memory bandwidth) as highlighted by recent studies on Exascale architectural trends. Further, flops are getting cheaper, while the energy cost of data movement is increasingly dominant. The understanding and characterization of data locality properties of computations is critical in order to guide efforts to enhance data locality. Reuse distance analysis of memory address traces is a valuable tool to perform data locality characterization of programs. A single reuse distance analysis can be used to estimate the number of cache misses in a fully associative LRU cache of any size, thereby providing estimates on the minimum bandwidth requirements at different levels of the memory hierarchy to avoid being bandwidth bound. However, such an analysis only holds for the particular execution order that produced the trace. It cannot estimate potential improvement in data locality through dependence-preserving transformations that change the execution schedule of the operations in the computation. In this article, we develop a novel dynamic analysis approach to characterize the inherent locality properties of a computation and thereby assess the potential for data locality enhancement via dependence-preserving transformations. The execution trace of a code is analyzed to extract a Computational-Directed Acyclic Graph (CDAG) of the data dependences. The CDAG is then partitioned into convex subsets, and the convex partitioning is used to reorder the operations in the execution trace to enhance data locality. The approach enables us to go beyond reuse distance analysis of a single specific order of execution of the operations of a computation in characterization of its data locality properties. It can serve a valuable role in identifying promising code regions for manual transformation, as well as assessing the effectiveness of compiler transformations for data locality enhancement. We demonstrate the effectiveness of the approach using a number of benchmarks, including case studies where the potential shown by the analysis is exploited to achieve lower data movement costs and better performance.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2086539288",
    "type": "article"
  },
  {
    "title": "Making the Most of SMT in HPC",
    "doi": "https://doi.org/10.1145/2687651",
    "publication_date": "2015-01-09",
    "publication_year": 2015,
    "authors": "Leo Porter; Michael A. Laurenzano; Ananta Tiwari; Adam Jundt; William A. Ward; Roy Campbell; Laura Carrington",
    "corresponding_authors": "",
    "abstract": "This work presents an end-to-end methodology for quantifying the performance and power benefits of simultaneous multithreading (SMT) for HPC centers and applies this methodology to a production system and workload. Ultimately, SMT’s value system-wide depends on whether users effectively employ SMT at the application level. However, predicting SMT’s benefit for HPC applications is challenging; by doubling the number of threads, the application’s characteristics may change. This work proposes statistical modeling techniques to predict the speedup SMT confers to HPC applications. This approach, accurate to within 8%, uses only lightweight, transparent performance monitors collected during a single run of the application.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2087055960",
    "type": "article"
  },
  {
    "title": "Compiler support for lightweight context switching",
    "doi": "https://doi.org/10.1145/2400682.2400695",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Stephen K. Dolan; Servesh Muralidharan; David Gregg",
    "corresponding_authors": "",
    "abstract": "We propose a new language-neutral primitive for the LLVM compiler, which provides efficient context switching and message passing between lightweight threads of control. The primitive, called S wapstack , can be used by any language implementation based on LLVM to build higher-level language structures such as continuations, coroutines, and lightweight threads. As part of adding the primitives to LLVM, we have also added compiler support for passing parameters across context switches. Our modified LLVM compiler produces highly efficient code through a combination of exposing the context switching code to existing compiler optimizations, and adding novel compiler optimizations to further reduce the cost of context switches. To demonstrate the generality and efficiency of our primitives, we add one-shot continuations to C++, and provide a simple fiber library that allows millions of fibers to run on multiple cores, with a work-stealing scheduler and fast inter-fiber sychronization. We argue that compiler-supported lightweight context switching can be significantly faster than using a library to switch between contexts, and provide experimental evidence to support the position.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2132114837",
    "type": "article"
  },
  {
    "title": "A Bimodal Scheduler for Coarse-Grained Reconfigurable Arrays",
    "doi": "https://doi.org/10.1145/2893475",
    "publication_date": "2016-06-06",
    "publication_year": 2016,
    "authors": "Panagiotis Theocharis; Bjorn De Sutter",
    "corresponding_authors": "",
    "abstract": "Compilers for Course-Grained Reconfigurable Array (CGRA) architectures suffer from long compilation times and code quality levels far below the theoretical upper bounds. This article presents a new scheduler, called the Bimodal Modulo Scheduler (BMS), to map inner loops onto (heterogeneous) CGRAs of the Architecture for Dynamically Reconfigurable Embedded Systems (ADRES) family. BMS significantly outperforms existing schedulers for similar architectures in terms of generated code quality and compilation time. This is achieved by combining new schemes for backtracking with extended and adapted forms of priority functions and cost functions, as described in the article. BMS is evaluated by mapping multimedia and software-defined radio benchmarks onto tuned ADRES instances.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2411096328",
    "type": "article"
  },
  {
    "title": "An Online and Real-Time Fault Detection and Localization Mechanism for Network-on-Chip Architectures",
    "doi": "https://doi.org/10.1145/2930670",
    "publication_date": "2016-06-14",
    "publication_year": 2016,
    "authors": "Kypros Chrysanthou; Panayiotis Englezakis; Andreas Prodromou; Andreas Panteli; Chrysostomos Nicopoulos; Yiannakis Sazeides; Giorgos Dimitrakopoulos",
    "corresponding_authors": "",
    "abstract": "Networks-on-Chip (NoC) are becoming increasingly susceptible to emerging reliability threats. The need to detect and localize the occurrence of faults at runtime is steadily becoming imperative. In this work, we propose NoCAlert , a comprehensive online and real-time fault detection and localization mechanism that demonstrates 0% false negatives within the interconnect for the fault models and stimulus set used in this study. Based on the concept of invariance checking, NoCAlert employs a group of lightweight microchecker modules that collectively implement real-time hardware assertions. The checkers operate concurrently with normal NoC operation, thus eliminating the need for periodic, or triggered-based, self-testing. Based on the pattern/signature of asserted checkers, NoCAlert can pinpoint the location of the fault at various granularity levels. Most important, 97% of the transient and 90% of the permanent faults are detected instantaneously, within a single clock cycle upon fault manifestation. The fault localization accuracy ranges from 90% to 100%, depending on the desired localization granularity. Extensive cycle-accurate simulations in a 64-node CMP and analysis at the RTL netlist-level demonstrate the efficacy of the proposed technique.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2425628794",
    "type": "article"
  },
  {
    "title": "Implementing Dense Optical Flow Computation on a Heterogeneous FPGA SoC in C",
    "doi": "https://doi.org/10.1145/2948976",
    "publication_date": "2016-08-17",
    "publication_year": 2016,
    "authors": "Wenjie Chen; Zhibin Wang; Qin Wu; Jiuzhen Liang; Zhilei Chai",
    "corresponding_authors": "",
    "abstract": "High-quality optical flow computation algorithms are computationally intensive. The low computational speed of such algorithms causes difficulties for real-world applications. In this article, we propose an optimized implementation of the classical Combine-Brightness-Gradient (CBG) model on the Xilinx ZYNQ FPGA-SoC, by taking advantage of the inherent algorithmic parallelism and ZYNQ architecture. The execution time decreases to 0.82 second with a lower power consumption (1.881W). It is better than software implementation on PC (Intel i7-3520M, 2.9GHz), which costs 2.635 seconds and 35W. We use C rather than HDLs to describe the algorithm for rapid prototyping.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2508765146",
    "type": "article"
  },
  {
    "title": "A Cross-Platform SpMV Framework on Many-Core Architectures",
    "doi": "https://doi.org/10.1145/2994148",
    "publication_date": "2016-10-25",
    "publication_year": 2016,
    "authors": "Yunquan Zhang; Shigang Li; Shengen Yan; Huiyang Zhou",
    "corresponding_authors": "",
    "abstract": "Sparse Matrix-Vector multiplication (SpMV) is a key operation in engineering and scientific computing. Although the previous work has shown impressive progress in optimizing SpMV on many-core architectures, load imbalance and high memory bandwidth remain the critical performance bottlenecks. We present our novel solutions to these problems, for both GPUs and Intel MIC many-core architectures. First, we devise a new SpMV format, called Blocked Compressed Common Coordinate (BCCOO). BCCOO extends the blocked Common Coordinate (COO) by using bit flags to store the row indices to alleviate the bandwidth problem. We further improve this format by partitioning the matrix into vertical slices for better data locality. Then, to address the load imbalance problem, we propose a highly efficient matrix-based segmented sum/scan algorithm for SpMV, which eliminates global synchronization. At last, we introduce an autotuning framework to choose optimization parameters. Experimental results show that our proposed framework has a significant advantage over the existing SpMV libraries. In single precision, our proposed scheme outperforms clSpMV COCKTAIL format by 255% on average on AMD FirePro W8000, and outperforms CUSPARSE V7.0 by 73.7% on average and outperforms CSR5 by 53.6% on average on GeForce Titan X; in double precision, our proposed scheme outperforms CUSPARSE V7.0 by 34.0% on average and outperforms CSR5 by 16.2% on average on Tesla K20, and has equivalent performance compared with CSR5 on Intel MIC.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2535926538",
    "type": "article"
  },
  {
    "title": "Evaluation of Histogram of Oriented Gradients Soft Errors Criticality for Automotive Applications",
    "doi": "https://doi.org/10.1145/2998573",
    "publication_date": "2016-11-15",
    "publication_year": 2016,
    "authors": "Fernando Fernandes dos Santos; Lucas Weigel; Cláudio R. Jung; Philippe O. A. Navaux; Luigi Carro; Paolo Rech",
    "corresponding_authors": "",
    "abstract": "Pedestrian detection reliability is a key problem for autonomous or aided driving, and methods that use Histogram of Oriented Gradients (HOG) are very popular. Embedded Graphics Processing Units (GPUs) are exploited to run HOG in a very efficient manner. Unfortunately, GPUs architecture has been shown to be particularly vulnerable to radiation-induced failures. This article presents an experimental evaluation and analytical study of HOG reliability. We aim at quantifying and qualifying the radiation-induced errors on pedestrian detection applications executed in embedded GPUs. We analyze experimental results obtained executing HOG on embedded GPUs from two different vendors, exposed for about 100 hours to a controlled neutron beam at Los Alamos National Laboratory. We consider the number and position of detected objects as well as precision and recall to discriminate critical erroneous computations. The reported analysis shows that, while being intrinsically resilient (65% to 85% of output errors only slightly impact detection), HOG experienced some particularly critical errors that could result in undetected pedestrians or unnecessary vehicle stops. Additionally, we perform a fault-injection campaign to identify HOG critical procedures. We observe that Resize and Normalize are the most sensitive and critical phases, as about 20% of injections generate an output error that significantly impacts HOG detection. With our insights, we are able to find those limited portions of HOG that, if hardened, are more likely to increase reliability without introducing unnecessary overhead.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2555834960",
    "type": "article"
  },
  {
    "title": "Pareto Governors for Energy-Optimal Computing",
    "doi": "https://doi.org/10.1145/3046682",
    "publication_date": "2017-03-13",
    "publication_year": 2017,
    "authors": "Rathijit Sen; David A. Wood",
    "corresponding_authors": "",
    "abstract": "The original definition of energy-proportional computing does not characterize the energy efficiency of recent reconfigurable computers, resulting in nonintuitive “super-proportional” behavior. This article introduces a new definition of ideal energy-proportional computing, new metrics to quantify computational energy waste, and new SLA-aware OS governors that seek Pareto optimality to achieve power-efficient performance.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2597815813",
    "type": "article"
  },
  {
    "title": "ECS",
    "doi": "https://doi.org/10.1145/3151083",
    "publication_date": "2017-12-13",
    "publication_year": 2017,
    "authors": "Shivam Swami; Poovaiah M. Palangappa; Kartik Mohanram",
    "corresponding_authors": "",
    "abstract": "Emerging nonvolatile memories (NVMs) suffer from low write endurance, resulting in early cell failures (hard errors), which reduce memory lifetime. It was recognized early on that conventional error-correcting codes (ECCs), which are designed for soft errors, are a poor choice for addressing hard errors in NVMs. This led to the evolution of hard error correction schemes like dynamically replicated memory (DRM), error-correcting pointers (ECPs), SAFER, FREE-p, PAYG, and Zombie memory to improve NVM lifetime. Whereas these approaches made significant inroads in addressing hard errors and low memory lifetime in NVMs, overcoming the challenges of underutilization of error-correcting resources and/or implementation overhead (e.g., codec latency, hardware support) remain areas of active research and development. This article proposes error-correcting strings (ECSs) as a high-utilization, low-latency solution for hard error correction in single-/multi-/triple-level cell (SLC/MLC/TLC) NVMs. At its core, ECS adopts a base-offset approach to store pointers to the failed memory cells; in this work, base is the address of the first failed cell in a memory block and offsets are the distances between successive failed cells in that memory block. Unlike ECP, which uses fixed-length pointers, ECS uses variable-length offsets to point to the failed cells, thereby realizing more pointers to tolerate more hard errors per memory block. Further, this article proposes eXtended-ECS (XECS), a page-level error correction architecture, which employs dynamic on-demand ECS allocation and opportunistic pattern-based data compression to improve NVM lifetime by 2× over ECP-6 for comparable overhead and negligible impact to system performance. Finally, this article demonstrates that ECS is a drop-in replacement for ECP to extend the lifetime of state-of-the-art ECP-based techniques like PAYG and Zombie memory; ECS is also compatible with MLC/TLC NVMs, where it complements drift-induced soft error reduction techniques like ECC and incomplete data mapping to simultaneously extend NVM lifetime.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2774677654",
    "type": "article"
  },
  {
    "title": "Caliper",
    "doi": "https://doi.org/10.1145/3323090",
    "publication_date": "2019-06-17",
    "publication_year": 2019,
    "authors": "Ram Srivatsa Kannan; Michael A. Laurenzano; Jeongseob Ahn; Jason Mars; Lingjia Tang",
    "corresponding_authors": "",
    "abstract": "We introduce Caliper , a technique for accurately estimating performance interference occurring in shared servers. Caliper overcomes the limitations of prior approaches by leveraging a micro-experiment-based technique. In contrast to state-of-the-art approaches that focus on periodically pausing co-running applications to estimate slowdown, Caliper utilizes a strategic phase-triggered technique to capture interference due to co-location. This enables Caliper to orchestrate an accurate and low-overhead interference estimation technique that can be readily deployed in existing production systems. We evaluate Caliper for a broad spectrum of workload scenarios, demonstrating its ability to seamlessly support up to 16 applications running simultaneously and outperform the state-of-the-art approaches.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2950722551",
    "type": "article"
  },
  {
    "title": "DCMI",
    "doi": "https://doi.org/10.1145/3352813",
    "publication_date": "2019-10-11",
    "publication_year": 2019,
    "authors": "Mostafa Koraei; Omid Fatemi; Magnus Jahre",
    "corresponding_authors": "",
    "abstract": "Iterative Stencil Loops (ISLs) are the key kernel within a range of compute-intensive applications. To accelerate ISLs with Field Programmable Gate Arrays, it is critical to exploit parallelism (1) among elements within the same iteration and (2) across loop iterations. We propose a novel ISL acceleration scheme called Direct Computation of Multiple Iterations (DCMI) that improves upon prior work by pre-computing the effective stencil coefficients after a number of iterations at design time—resulting in accelerators that use minimal on-chip memory and avoid redundant computation. This enables DCMI to improve throughput by up to 7.7× compared to the state-of-the-art cone-based architecture.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2980104160",
    "type": "article"
  },
  {
    "title": "EchoBay",
    "doi": "https://doi.org/10.1145/3404993",
    "publication_date": "2020-08-17",
    "publication_year": 2020,
    "authors": "Luca Cerina; Marco D. Santambrogio; Giuseppe Franco; Claudio Gallicchio; Alessio Micheli",
    "corresponding_authors": "",
    "abstract": "The increase in computational power of embedded devices and the latency demands of novel applications brought a paradigm shift on how and where the computation is performed. Although AI inference is slowly moving from the cloud to end-devices with limited resources, time-centric recurrent networks like Long-Short Term Memory remain too complex to be transferred on embedded devices without extreme simplifications and limiting the performance of many notable applications. To solve this issue, the Reservoir Computing paradigm proposes sparse, untrained non-linear networks, the Reservoir, that can embed temporal relations without some of the hindrances of Recurrent Neural Networks training, and with a lower memory occupation. Echo State Networks (ESN) and Liquid State Machines are the most notable examples. In this scenario, we propose EchoBay , a comprehensive C++ library for ESN design and training. EchoBay is architecture-agnostic to guarantee maximum performance on different devices (whether embedded or not), and it offers the possibility to optimize and tailor an ESN on a particular case study, reducing at the minimum the effort required on the user side. This can be done thanks to the Bayesian Optimization (BO) process, which efficiently and automatically searches hyper-parameters that maximize a fitness function. Additionally, we designed different optimization techniques that take in consideration resource constraints of the device to minimize memory footprint and inference time. Our results in different scenarios show an average speed-up in training time of 119x compared to Grid and Random search of hyper-parameters, a decrease of 94% of trained models size and 95% in inference time, maintaining comparable performance for the given task. The EchoBay library is Open Source and publicly available at https://github.com/necst/Echobay.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W3058788517",
    "type": "article"
  },
  {
    "title": "OD-SGD",
    "doi": "https://doi.org/10.1145/3417607",
    "publication_date": "2020-09-30",
    "publication_year": 2020,
    "authors": "Yemao Xu; Dezun Dong; Yawei Zhao; Weixia Xu; Xiangke Liao",
    "corresponding_authors": "",
    "abstract": "The training of modern deep learning neural network calls for large amounts of computation, which is often provided by GPUs or other specific accelerators. To scale out to achieve faster training speed, two update algorithms are mainly applied in the distributed training process, i.e., the Synchronous SGD algorithm (SSGD) and Asynchronous SGD algorithm (ASGD). SSGD obtains good convergence point while the training speed is slowed down by the synchronous barrier. ASGD has faster training speed but the convergence point is lower when compared to SSGD. To sufficiently utilize the advantages of SSGD and ASGD, we propose a novel technology named One-step Delay SGD (OD-SGD) to combine their strengths in the training process. Therefore, we can achieve similar convergence point and training speed as SSGD and ASGD separately. To the best of our knowledge, we make the first attempt to combine the features of SSGD and ASGD to improve distributed training performance. Each iteration of OD-SGD contains a global update in the parameter server node and local updates in the worker nodes, the local update is introduced to update and compensate the delayed local weights. We evaluate our proposed algorithm on MNIST, CIFAR-10, and ImageNet datasets. Experimental results show that OD-SGD can obtain similar or even slightly better accuracy than SSGD, while its training speed is much faster, which even exceeds the training speed of ASGD.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W3090287616",
    "type": "article"
  },
  {
    "title": "Preventing STT-RAM Last-Level Caches from Port Obstruction",
    "doi": "https://doi.org/10.1145/2633046",
    "publication_date": "2014-07-31",
    "publication_year": 2014,
    "authors": "Jue Wang; Xiangyu Dong; Yuan Xie",
    "corresponding_authors": "",
    "abstract": "Many new nonvolatile memory (NVM) technologies have been heavily studied to replace the power-hungry SRAM/DRAM-based memory hierarchy in today's computers. Among various emerging NVM technologies, Spin-Transfer Torque RAM (STT-RAM) has many benefits, such as fast read latency, low leakage power, and high density, making it a promising candidate for last-level caches (LLCs). 1 However, STT-RAM write operation is expensive. In particular, a long STT-RAM cache write operation might obstruct other cache accesses and result in severe performance degradation. Consequently, how to mitigate STT-RAM write overhead is critical to the success of STT-RAM adoption. In this article, we propose an obstruction-aware cache management policy called OAP. OAP monitors cache traffic, detects LLC-obstructive processes, and differentiates the cache accesses from different processes. Our experiment on a four-core architecture with an 8MB STT-RAM L3 cache shows a 14% performance improvement and 64% energy reduction.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2052391539",
    "type": "article"
  },
  {
    "title": "Architectural Support for Data-Driven Execution",
    "doi": "https://doi.org/10.1145/2686874",
    "publication_date": "2015-01-09",
    "publication_year": 2015,
    "authors": "George Matheou; Paraskevas Evripidou",
    "corresponding_authors": "",
    "abstract": "The exponential growth of sequential processors has come to an end, and thus, parallel processing is probably the only way to achieve performance growth. We propose the development of parallel architectures based on data-driven scheduling. Data-driven scheduling enforces only a partial ordering as dictated by the true data dependencies, which is the minimum synchronization possible. This is very beneficial for parallel processing because it enables it to exploit the maximum possible parallelism. We provide architectural support for data-driven execution for the Data-Driven Multithreading (DDM) model. In the past, DDM has been evaluated mostly in the form of virtual machines. The main contribution of this work is the development of a highly efficient hardware support for data-driven execution and its integration into a multicore system with eight cores on a Virtex-6 FPGA. The DDM semantics make barriers and cache coherence unnecessary, which reduces the synchronization latencies significantly and makes the cache simpler. The performance evaluation has shown that the support for data-driven execution is very efficient with negligible overheads. Our prototype can support very small problem sizes (matrix 16×16) and ultra-lightweight threads (block of 4x4) that achieve speedups close to linear. Such results cannot be achieved by software-based systems.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2057506192",
    "type": "article"
  },
  {
    "title": "Efficient Data Encoding for Convolutional Neural Network application",
    "doi": "https://doi.org/10.1145/2685394",
    "publication_date": "2015-01-09",
    "publication_year": 2015,
    "authors": "Hong-Phuc Trinh; Marc Duranton; Michel Paindavoine",
    "corresponding_authors": "",
    "abstract": "This article presents an approximate data encoding scheme called Significant Position Encoding (SPE) . The encoding allows efficient implementation of the recall phase (forward propagation pass) of Convolutional Neural Networks (CNN)—a typical Feed-Forward Neural Network. This implementation uses only 7 bits data representation and achieves almost the same classification performance compared with the initial network: on MNIST handwriting recognition task, using this data encoding scheme losses only 0.03% in terms of recognition rate (99.27% vs. 99.3%). In terms of storage, we achieve a 12.5% gain compared with an 8 bits fixed-point implementation of the same CNN. Moreover, this data encoding allows efficient implementation of processing unit thanks to the simplicity of scalar product operation—the principal operation in a Feed-Forward Neural Network.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2131117478",
    "type": "article"
  },
  {
    "title": "Performance Portability Across Heterogeneous SoCs Using a Generalized Library-Based Approach",
    "doi": "https://doi.org/10.1145/2608253",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "Shuangde Fang; Zidong Du; Yuntan Fang; Yuanjie Huang; Yang Chen; Lieven Eeckhout; Olivier Temam; Huawei Li; Yunji Chen; Chengyong Wu",
    "corresponding_authors": "",
    "abstract": "Because of tight power and energy constraints, industry is progressively shifting toward heterogeneous system-on-chip (SoC) architectures composed of a mix of general-purpose cores along with a number of accelerators. However, such SoC architectures can be very challenging to efficiently program for the vast majority of programmers, due to numerous programming approaches and languages. Libraries, on the other hand, provide a simple way to let programmers take advantage of complex architectures, which does not require programmers to acquire new accelerator-specific or domain-specific languages. Increasingly, library-based, also called algorithm-centric, programming approaches propose to generalize the usage of libraries and to compose programs around these libraries, instead of using libraries as mere complements. In this article, we present a software framework for achieving performance portability by leveraging a generalized library-based approach. Inspired by the notion of a component, as employed in software engineering and HW/SW codesign, we advocate nonexpert programmers to write simple wrapper code around existing libraries to provide simple but necessary semantic information to the runtime. To achieve performance portability, the runtime employs machine learning (simulated annealing) to select the most appropriate accelerator and its parameters for a given algorithm. This selection factors in the possibly complex composition of algorithms used in the application, the communication among the various accelerators, and the tradeoff between different objectives (i.e., accuracy, performance, and energy). Using a set of benchmarks run on a real heterogeneous SoC composed of a multicore processor and a GPU, we show that the runtime overhead is fairly small at 5.1% for the GPU and 6.4% for the multi-core. We then apply our accelerator selection approach to a simulated SoC platform containing multiple inexact accelerators. We show that accelerator selection together with hardware parameter tuning achieves an average 46.2% energy reduction and a speedup of 2.1× while meeting the desired application error target.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2171601645",
    "type": "article"
  },
  {
    "title": "Managing Mismatches in Voltage Stacking with CoreUnfolding",
    "doi": "https://doi.org/10.1145/2835178",
    "publication_date": "2015-11-16",
    "publication_year": 2015,
    "authors": "Ehsan K. Ardestani; Rafael Trapani Possignolo; José Luis Briz; Jose Renau",
    "corresponding_authors": "",
    "abstract": "Five percent to 25% of power could be wasted before it is delivered to the computational resources on a die, due to inefficiencies of voltage regulators and resistive loss. The power delivery could benefit if, at the same power, the delivered voltage increases and the current decreases. This article presents CoreUnfolding , a technique that leverages voltage Stacking to improve power delivery efficiency. Our experiments show that about 10% system-wide power can be saved, the voltage regulator area can be reduced by 30%, di / dt improves 49%, and the power pin count is reduced by 40% (≈ 20% reduction in packaging costs), with negligible performance degradation.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2180675311",
    "type": "article"
  },
  {
    "title": "A Compile-Time Optimization Method for WCET Reduction in Real-Time Embedded Systems through Block Formation",
    "doi": "https://doi.org/10.1145/2845083",
    "publication_date": "2016-01-04",
    "publication_year": 2016,
    "authors": "Morteza Mohajjel Kafshdooz; Mohammadkazem Taram; Sepehr Assadi; Alireza Ejlali",
    "corresponding_authors": "",
    "abstract": "Compile-time optimizations play an important role in the efficient design of real-time embedded systems. Usually, compile-time optimizations are designed to reduce average-case execution time (ACET). While ACET is a main concern in high-performance computing systems, in real-time embedded systems, concerns are different and worst-case execution time (WCET) is much more important than ACET. Therefore, WCET reduction is more desirable than ACET reduction in many real-time embedded systems. In this article, we propose a compile-time optimization method aimed at reducing WCET in real-time embedded systems. In the proposed method, based on the predicated execution capability of embedded processors, program code blocks that are in the worst-case paths of the program are merged to increase instruction-level parallelism and opportunity for WCET reduction. The use of predicated execution enables merging code blocks from different worst-case paths that can be very effective in WCET reduction. The experimental results show that the proposed method can reduce WCET by up to 45% as compared to previous compile-time block formation methods. It is noteworthy that compared to previous works, while the proposed method usually achieves more WCET reduction, it has considerably less negative impact on ACET and code size.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2252020632",
    "type": "article"
  },
  {
    "title": "HeapCheck: Low-cost Hardware Support for Memory Safety",
    "doi": "https://doi.org/10.1145/3495152",
    "publication_date": "2022-01-23",
    "publication_year": 2022,
    "authors": "Gururaj Saileshwar; Rick Boivie; Tong Chen; Benjamin M. Segal; Alper Buyuktosunoglu",
    "corresponding_authors": "",
    "abstract": "Programs written in C/C++ are vulnerable to memory-safety errors like buffer-overflows and use-after-free. While several mechanisms to detect such errors have been previously proposed, they suffer from a variety of drawbacks, including poor performance, imprecise or probabilistic detection of errors, or requiring invasive changes to the ISA, binary-layout, or source-code that results in compatibility issues. As a result, memory-safety errors continue to be hard to detect and a principal cause of security problems. In this work, we present a minimally invasive and low-cost hardware-based memory-safety checking framework for detecting out-of-bounds accesses and use-after-free errors. The key idea of our mechanism is to re-purpose some of the “unused bits” in a pointer in 64-bit architectures to store an index into a bounds information table that can be used to catch out-bounds errors and use-after-free errors without any change to the binary layout. Using this memory-safety checking framework, we enable HeapCheck, a design for detecting Out-of-bounds and Use-after-free accesses for heap-objects, that are responsible for the majority of memory-safety errors in the wild. Our evaluations using C/C++ SPEC CPU 2017 workloads on Gem5 show that our solution incurs 1.5% slowdown on average, using an 8 KB on-chip SRAM cache for caching bounds-information. Our mechanism allows detection of out-of-bounds errors in user-code as well as in unmodified shared-library functions. Our mechanism has detected out-of-bounds accesses in 87 lines of code in the SPEC CPU 2017 benchmarks, primarily in Glibc v2.27 functions, that, to our knowledge, have not been previously detected even with popular tools like Address Sanitizer.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4206959453",
    "type": "article"
  },
  {
    "title": "IBing: An Efficient Interleaved Bidirectional Ring All-Reduce Algorithm for Gradient Synchronization",
    "doi": "https://doi.org/10.1145/3711818",
    "publication_date": "2025-01-08",
    "publication_year": 2025,
    "authors": "Ruixing Zong; Jiapeng Zhang; Zhuo Tang; Kenli Li",
    "corresponding_authors": "",
    "abstract": "Ring all-reduce is currently the most commonly used collective communication technique in the fields of data parallel and distributed computing. It consists of three phases: communication establishment, data transmission, and data processing at each step. However, this method may suffer from increased communication latency as the number of computation nodes increases, excessive communication steps and data processing procedures can lead to insufficient bandwidth utilization. To address this issue, this paper proposes an Interleaved Bidirectional Ring (IBing) all-reduce method, which uses specially crafted communication operations to improve communication efficiency by reducing the effects of both communication establishment and data processing time. IBing reduces the number of communication steps by half compared to the Ring all-reduce. The results of extensive experiments indicate that the proposed IBing design can reduce total communication consumption by an average of 8.49% and up to 49.73%.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406166720",
    "type": "article"
  },
  {
    "title": "Enhancing High-Throughput GPU Random Walks Through Multi-Task Concurrency Orchestration",
    "doi": "https://doi.org/10.1145/3711820",
    "publication_date": "2025-01-10",
    "publication_year": 2025,
    "authors": "Cheng Xu; Chao Li; Xiaofeng Hou; Junyi Mei; Jing Wang; Pengyu Wang; Shixuan Sun; Minyi Guo; Baoping Hao",
    "corresponding_authors": "",
    "abstract": "Random walk is a powerful tool for large-scale graph learning, but its high computational demand presents a challenge. While GPUs can accelerate random walk tasks, current frameworks fail to fully utilize GPU parallelism due to memory-to-compute bandwidth imbalance. In this paper, CoWalker, an efficient GPU framework, is proposed to facilitate concurrent execution of random walks for high overall throughput. CoWalker features three novel designs. First, it incorporates a multi-level execution model that effectively orchestrates diverse walk tasks and reduces GPU stalls based on multiple graph characteristics. Second, it collaboratively manages graph data and streaming multiprocessors to minimize memory access interference and maximize core utilization under concurrent tasks. Finally, a multi-dimensional scheduler selects compatible random walk task combinations based on memory footprints to achieve maximum throughput. CoWalker significantly improves throughput over state-of-the-art baselines by mitigating concurrency overheads and effectively harnessing GPU parallelism. Our extensive evaluations on real-world workloads demonstrate that CoWalker achieves 2.75 × higher overall system throughput compared with commercial tools and 1.56 × over the SOTA academic system.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406245347",
    "type": "article"
  },
  {
    "title": "gHyPart: GPU-friendly End-to-End Hypergraph Partitioner",
    "doi": "https://doi.org/10.1145/3711925",
    "publication_date": "2025-01-10",
    "publication_year": 2025,
    "authors": "Zhenlin Wu; Haosong Zhao; Hongyuan Liu; Wujie Wen; Jiajia Li",
    "corresponding_authors": "",
    "abstract": "Hypergraph partitioning finds practical applications in various fields, such as high-performance computing and circuit partitioning in VLSI physical design, where high-performance solutions often demand substantial parallelism beyond what existing CPU-based solutions can offer. While GPUs are promising in this regard, their potential in hypergraph partitioning remains unexplored. In this work, we first develop an end-to-end deterministic hypergraph partitioner on GPUs, ported from state-of-the-art multi-threaded CPU work, and identify three major performance challenges by characterizing its performance. We propose the first end-to-end solution, gHyPart , to unleash the potentials of hypergraph partitioning on GPUs. To overcome the challenges of GPU thread underutilization due to imbalanced workload, long critical path, and high work complexity due to excessive operations, we redesign GPU algorithms with diverse parallelization strategies thus expanding optimization space; to address the challenge of no one-size-fits-all implementation for various input hypergraphs, we propose a decision tree-based strategy to choose a suitable parallelization strategy for each kernel. Evaluation on 500 hypergraphs shows up to 125.7 × (17.5 × on average), 640.0 × (24.2 × on average), and 171.6 × (1.4 × on average) speedups over two CPU partitioners and our GPU baseline gHyPart-B , respectively.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406251082",
    "type": "article"
  },
  {
    "title": "Taming Flexible Job Packing in Deep Learning Training Clusters",
    "doi": "https://doi.org/10.1145/3711927",
    "publication_date": "2025-01-13",
    "publication_year": 2025,
    "authors": "Pengyu Yang; Weihao Cui; Chunyu Xue; Han Zhao; Chen Chen; Quan Chen; Jing Yang; Minyi Guo",
    "corresponding_authors": "",
    "abstract": "Job packing is an effective technique to harvest the idle resources allocated to the deep learning (DL) training jobs but not fully utilized, especially when clusters may experience low utilization, and users may overestimate their resource needs. However, existing job packing techniques tend to be conservative due to the mismatch in scope and granularity between job packing and cluster scheduling. In particular, tapping the potential of job packing in the training cluster requires a local and fine-grained coordination mechanism. To this end, we propose a novel job-packing middleware named Gimbal , which operates between the cluster scheduler and the hardware resources. As middleware, Gimbal must not only facilitate coordination among the packed jobs but also support various scheduling objectives of different schedulers. Gimbal achieves dual functionality by introducing a set of worker calibration primitives designed to calibrate workers’ execution status in a fine-grained manner. The primitives obscure the complexity of the underlying job and resource management mechanisms, thus offering the generality and extensibility for crafting coordination policies tailored to various scheduling objectives. We implement Gimbal on a real-world GPU cluster and evaluate it with a set of representative DL training jobs. The results show that Gimbal improves different scheduling objectives up to 1.32 × compared with the state-of-the-art job packing techniques.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406302347",
    "type": "article"
  },
  {
    "title": "Maximizing Data and Hardware Reuse for HLS with Early-Stage Symbolic Partitioning",
    "doi": "https://doi.org/10.1145/3711926",
    "publication_date": "2025-01-16",
    "publication_year": 2025,
    "authors": "Tzung-Han Juang; Christophe Dubach",
    "corresponding_authors": "",
    "abstract": "While traditional HLS (High-Level Synthesis) converts “high-level” C-like programs into hardware automatically, producing high-performance designs still requires hardware expertise. Optimizations such as data partitioning can have a large impact on performance since they directly affect data reuse patterns and the ability to reuse hardware. However, optimizing partitioning is a difficult process since minor changes in the parameter choices can lead to totally unpredictable performance. Functional array-based languages have been proposed instead of C-based approaches, as they offer stronger performance guarantees. This paper proposes to follow a similar approach and exposes a divide-and-conquer primitive at the algorithmic level to let users partition any arbitrary computation. The compiler is then free to explore different partition shapes to maximize both data and hardware reuse automatically. The main challenge remains that the impact of partitioning is only known much later in the compilation flow. This is due to the hard-to-predict effects of the many optimizations applied during compilation. To solve this problem, the partitioning is expressed using a set of symbolic tunable parameters, introduced early in the compilation pipeline. A symbolic performance model is then used in the last compilation stage to predict performance based on the possible values of the tunable parameters. Using this approach, a design space exploration is conducted on an Intel Arria 10 FPGAs (Field Programmable Gate Arrays), and competitive performance is achieved on the classical VGG and TinyYolo neural networks.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406428192",
    "type": "article"
  },
  {
    "title": "Dynamic Power Management Through Multi-agent Deep Reinforcement Learning for Heterogeneous Systems",
    "doi": "https://doi.org/10.1145/3716872",
    "publication_date": "2025-02-10",
    "publication_year": 2025,
    "authors": "Yiming Wang; Weizhe Zhang; Meng Hao; Wenzheng Kong; Yuan Wen",
    "corresponding_authors": "",
    "abstract": "Power management and optimization play a significant role in modern computer systems, from battery-powered devices to servers running in data centres. Existing approaches for power capping fail to meet the requirements presented by dynamic workloads, and the situation becomes even more severe, given the divergent energy efficiency of workloads on heterogeneous hardware platforms. Adaptively optimizing energy consumption for dynamic workloads presents a great challenge to heterogeneous systems. To tackle this challenge, we present a machine learning based method to improve system-level power efficiency. We employ multi-agent deep reinforcement learning (MADRL) to automatically explore the relationship between long-term performance and the power budget for workloads of different types on classic CPU-GPU heterogeneous platforms. Our framework equips each device with an agent, enabling decentralized control over its power budget while maintaining centralized coordination to maximize the running time of applications within a power cap. We evaluate our approach against state-of-the-art methods on CPU-GPU platforms. Experimental results show that our method improves performance by an average of 8.5%. Additionally, our method is significantly more stable compared to the state-of-the-art heuristic approach.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407301365",
    "type": "article"
  },
  {
    "title": "OptiFX: Automatic Optimization for Convolutional Neural Networks with Aggressive Operator Fusion on GPUs",
    "doi": "https://doi.org/10.1145/3716876",
    "publication_date": "2025-02-10",
    "publication_year": 2025,
    "authors": "Xueying Wang; Shigang Li; 豪 日浅; Fan Luo; Ziru Hao; Tong Wu; Ruiyuan Xu; Huimin Cui; Xiaobing Feng; Guangli Li; Jingling Xue",
    "corresponding_authors": "",
    "abstract": "Convolutional Neural Networks (CNNs) are fundamental to advancing computer vision technologies. As CNNs become more complex and larger, optimizing model inference remains a critical challenge in both industry and academia. On modern GPU platforms, CNN operators are typically memory-bound, leading to significant performance degradation due to memory wall effects. While recent advancements have utilized operator fusion—merging multiple operators into one—to enhance inference performance, the fusion of multiple region-based operators like convolution is seldom addressed. This paper introduces AFusion , a novel operator fusion technique aimed at improving inference performance, and OptiFX, an automatic optimization framework based on this approach. OptiFX employs a cost-based backtracking search to identify optimal sub-graphs for fusion and utilizes template-based code generation to create efficient kernels for these fused sub-graphs. We evaluate OptiFX across seven prominent CNN architectures—GoogLeNet, ResNet, DenseNet, MobileNet, SqueezeNet, NasNet, and UNet—on Nvidia A6000 Ada, RTX 4090, and Jetson AGX Orin platforms. Our results demonstrate that OptiFX significantly outperforms existing methods, achieving average speedups of 2.91 ×, 3.30 ×, and 2.09 × in accelerating inference performance on these platforms, respectively.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407319523",
    "type": "article"
  },
  {
    "title": "Comprehensive Evaluation and Opportunity Discovery for Deterministic Concurrency Control",
    "doi": "https://doi.org/10.1145/3715126",
    "publication_date": "2025-02-11",
    "publication_year": 2025,
    "authors": "Xinyuan Wang; Xingchen Li; Yün Peng; Hejiao Huang",
    "corresponding_authors": "",
    "abstract": "Deterministic concurrency control (DCC) guarantees that the same input transactions produce the same serializable result. It offers benefits in both distributed databases and blockchain systems. Dozens of DCC algorithms have emerged in the past decade. However, there is a lack of comprehensive evaluations for them. To study the performance of existing DCC algorithms and discover further opportunities, we make the following contributions. First, we abstract five essential features from the existing DCC algorithms: generality, speculative mechanism, version strategy, batch strategy, and concurrency mode. Each distinct combination of these features corresponds to a specific algorithm. Second, we implement 13 DCC algorithms and conduct evaluations focused on their features by using 10 workloads, to conclude each feature’s strengths and weaknesses. Thirdly, based on our feature analysis, we discover opportunities for improvement in two existing DCC algorithms, resulting in performance boosts of up to 2.3x and 3.4x.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407372911",
    "type": "article"
  },
  {
    "title": "CesASMe and Staticdeps: static detection of memory-carried dependencies for code analyzers",
    "doi": "https://doi.org/10.1145/3715125",
    "publication_date": "2025-02-11",
    "publication_year": 2025,
    "authors": "Théophile Bastian; Hugo Pompougnac; Alban Dutilleul; Fabrice Rastello",
    "corresponding_authors": "",
    "abstract": "A variety of code analyzers, such as IACA , uiCA , llvm-mca or Ithemal , strive to statically predict the throughput of a computation kernel. Each analyzer is based on its own simplified CPU model reasoning at the scale of a basic block. Facing this diversity, evaluating their strengths and weaknesses is important to guide both their usage and their enhancement. We present CesASMe , a fully-tooled solution to evaluate code analyzers on C-level benchmarks composed of a benchmark derivation procedure that feeds an evaluation harness. We conclude that memory-carried data dependencies are a major source of imprecision for these tools. We tackle this issue with staticdeps , a static analyzer extracting memory-carried data dependencies, including across loop iterations, from an assembly basic block. We integrate its output to uiCA , a state-of-the-art code analyzer, to evaluate staticdeps ’ impact on a code analyzer’s precision through CesASMe .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407375809",
    "type": "article"
  },
  {
    "title": "Sniper: Exploiting Spatial and Temporal Sampling for Large-Scale Performance Analysis",
    "doi": "https://doi.org/10.1145/3720544",
    "publication_date": "2025-02-26",
    "publication_year": 2025,
    "authors": "Zhibo Xuan; Xin You; Tianyu Feng; Hailong Yang; Zhongzhi Luan; Yi Liu; Depei Qian",
    "corresponding_authors": "",
    "abstract": "MPI tracing tools is essential to collect the communication events and performance metrics of large-scale programs for further performance analysis and optimization. However, towards the exascale era, the performance and storage overhead for tracing becomes extremely prohibitive that significantly disturbs the original execution of MPI programs, leading to distorted tracing data and thus mislead analysis results. Although process sampling can effectively reduce the tracing overhead, it can easily miss important execution information that is necessary for subsequent performance analysis. In this paper, we propose SimTrace , a scalable MPI tracing tool with novel spatial and temporal sampling strategies that exploits the similarity among MPI processes to achieve both low tracing overhead as well as obtain sufficient tracing information. The experimental results demonstrate that SimTrace can significantly reduce the MPI tracing overhead compared to the state-of-the-art tracing tools, meanwhile enabling effective analysis to guide performance optimization of large-scale programs.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407974108",
    "type": "article"
  },
  {
    "title": "FusionFS: A Contention-Resilient File System for Persistent CPU Caches",
    "doi": "https://doi.org/10.1145/3719656",
    "publication_date": "2025-02-26",
    "publication_year": 2025,
    "authors": "Congyong Chen; Shengan Zheng; Yuhang Zhang; Linpeng Huang",
    "corresponding_authors": "",
    "abstract": "Byte-addressable storage (BAS), such as persistent memory and CXL-SSDs, does not meet system designers’ expectations for data flushing and access granularity. Persistent CPU caches, enabled by recent techniques like Intel’s eADR and CXL’s Global Persistent Flush, can mitigate these issues without sacrificing consistency. However, the shared nature of CPU caches can lead to cache contention, which can result in cached data being frequently evicted to the BAS and reloaded into caches, negating the benefits of caching. If the BAS write granularity is larger than the cacheline eviction granularity, this can also lead to severe write amplification. In this article, we identify, characterize, and propose solutions to the problem of contention in persistent CPU caches, which is largely overlooked by existing systems. These systems either simply assume that cached data is hot enough to survive cache evictions or use unsupported cache allocation techniques without testing their effectiveness. We also present FusionFS, a contention-resilient kernel file system that uses persistent CPU caches to redesign data update approaches. FusionFS employs an adaptive data update approach that chooses the most effective mechanism based on file access patterns during system calls and memory mapping accesses, minimizing BAS media writes and improving throughput. FusionFS also employs contention-aware cache allocation to minimize various types of cache contention. Experimental results show that FusionFS outperforms existing file systems and effectively mitigates various types of cache contention.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407974256",
    "type": "article"
  },
  {
    "title": "LitTLS: Lightweight Thread-Level Speculation on Little Cores",
    "doi": "https://doi.org/10.1145/3719655",
    "publication_date": "2025-02-26",
    "publication_year": 2025,
    "authors": "Xin Cheng; Jinpeng Ye; Huijing Deng; Tingting Zhang; Tianyi Liu; Jian Wang",
    "corresponding_authors": "",
    "abstract": "Thread-Level Speculation (TLS) utilizes speculative parallelization to accelerate hard-to-parallelize serial codes on multi-cores. As the heterogeneous multi-core architecture is becoming ubiquitous, it presents an opportunity for TLS to reorganize little cores for the acceleration of these serial codes instead of a big core with similar or more area and power. However, previous TLS designs significantly suffer from extended hardware overhead and costly speculative forwarding. We present LitTLS, a lightweight TLS design with versioning caches to eliminate significant extended hardware overhead by storing versions in caches without speculative write buffers and memory undo-logs. Additionally, LitTLS introduces the Speculative Address Table, a novel component to accelerate speculative forwarding with a central structure to trace memory dependencies. Evaluations on four little cores show that LitTLS achieves an average performance speedup of 2.87 × compared to a little core, outperforming a big core by 94% with similar area and less power. The extended area size is only 0.07 mm 2 , and the maximum increase in dynamic power consumption is limited to 0.3%, compared to four little cores.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407981628",
    "type": "article"
  },
  {
    "title": "Shift-CIM: In-SRAM Alignment To Support General-Purpose Bit-level Sparsity Exploration in SRAM Multiplication",
    "doi": "https://doi.org/10.1145/3719654",
    "publication_date": "2025-03-01",
    "publication_year": 2025,
    "authors": "Guomin Zhao; Q.L. Li; Rongzhen Lin; Yaohua Wang",
    "corresponding_authors": "",
    "abstract": "Multiplication plays a critical role in SRAM-based Computing-in-Memory (CIM) architectures. However, current SRAM-based CIMs face three major limitations. First, they do not fully exploit bit-level sparsity, resulting in unnecessary overhead in both latency and energy consumption. Second, the generation of numerous zero-dot products is superfluous. Third, the irregular organization of SRAM complicates the implementation. To address these issues, we propose Shift-CIM, a general-purpose approach that fully leverages bit-level sparsity within SRAM-based multiplications. Shift-CIM aligns the multipliers within the SRAM array, accumulating only the required dot products based on the non-zero bits of the multipliers. Shift-CIM achieves a regular SRAM organization by assembling two irregular SRAM arrays in a transposed manner. Our evaluations show that Shift-CIM is highly efficient, operating at a supply voltage of 0.9V and a frequency of 833MHz, while incurring only a 4.8% area overhead. Despite these modest requirements, Shift-CIM significantly accelerates multiplication operations, achieving up to 3.08 × the performance improvement and a 60% reduction in energy consumption compared to state-of-the-art designs.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408070695",
    "type": "article"
  },
  {
    "title": "ODGS: Dependency-Aware Scheduling for High-Level Synthesis with Graph Neural Network and Reinforcement Learning",
    "doi": "https://doi.org/10.1145/3721289",
    "publication_date": "2025-03-04",
    "publication_year": 2025,
    "authors": "Shen Ming-hua; Aoxiang Qin; Nong Xiao",
    "corresponding_authors": "",
    "abstract": "Scheduling determines the execution order and time of operations in program. The order is related to operation dependencies, including data and resource dependencies. Data dependency is intrinsic in program, showing operation data flow. Resource dependency is determined by scheduling methods, resolving operation resource contention. Existing scheduling methods focus on data dependency, rather than building and exploiting operation dependency graph (ODG) with extra resource dependency. As ODG contains all dependencies determining operation execution order, it provides global program information, facilitating efficient scheduling. In this work, we propose ODGS, a dependency-aware scheduling method for high-level synthesis with graph neural network (GNN) and reinforcement learning (RL). We adopt GNN to perceive accurate relations between operations. We use the relations to guide an RL agent in building a complete ODG. We perform feedback-guided iterative scheduling with ODG to converge to a high-quality solution. Experiments show that our method reduces 16.4% latency and 26.5% resource usage on average, compared with the latest RL-based method. Moreover, we reduce an average 2.9% latency over the GNN-based method under the same resource usage. The same resource usage is obtained by improving the GNN-based method with manual resource constraint tuning. Without tuning, its basic version consumes an average 237.6% more resources than our method.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408155172",
    "type": "article"
  },
  {
    "title": "TSN Cache: Exploiting Data Localities in Graph Computing Applications",
    "doi": "https://doi.org/10.1145/3721286",
    "publication_date": "2025-03-04",
    "publication_year": 2025,
    "authors": "Cuiyuan Jia; Jingyu Liu; Shi Chen; Kai Lü; Li Shen",
    "corresponding_authors": "",
    "abstract": "This paper finds that the reusability of vertices in the same graph in graph processing differs, and the high-reuse and low-reuse vertices are stored together. These phenomena lead to the inability of existing GPU architectures to capture the reusability of graph processing. The most advanced cache optimization strategies cannot implement different management strategies for data with different reusability, which is an essential reason for graph processing’s poor performance. Therefore, we propose a TSN cache scheme for the GPU platform. This scheme employs distinct management strategies for data with varying reusability in the cache, effectively leveraging the locality of these different data types. In addition, the TSN cache scheme can also reduce the probability of cache thrashing caused by low-reuse data. This paper evaluates multiple graph algorithms and datasets and shows that the TSN cache scheme achieves an average speedup of 1.38 compared with the baseline scheme.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408155285",
    "type": "article"
  },
  {
    "title": "Overlapping Aware Data Placement Optimizations for LSM Tree-Based Store on ZNS SSDs",
    "doi": "https://doi.org/10.1145/3721287",
    "publication_date": "2025-03-04",
    "publication_year": 2025,
    "authors": "Jingcheng Shen; Letu Yang; Linbo Long; Zhenhua Tan; Congming Gao; Kan Zhong; Masao Okita; Fumihiko Ino",
    "corresponding_authors": "",
    "abstract": "Solid State Drives (SSDs) based on the NVMe Zoned Namespaces (ZNS) interface can notably reduce the costs of address mapping, garbage collection, and over-provisioning by dividing the storage space into multiple zones for sequential writes and random reads. The Log-Structured Merge (LSM) tree, which is extensively used in key-value storage systems, converts random writes to sequential writes, hence a suitable scenario to utilize ZNS SSDs. However, LSM tree associated data significantly varies in lifetime due to the levels and merging mechanisms of the LSM tree. Therefore, without an accurate method to estimate data lifetime, data with disparate lifetimes may be placed in the same zone, thus causing low space utilization and high write amplification within the SSD. To address these issues, the paper proposes two data overlapping aware optimizations to realize intelligent data placement: a zone allocation scheme and a garbage collection scheme. The key technique of these optimizations is an accurate data-lifetime estimation by considering both the associated tree level of the data and the data overlapping ratio between the data and those in the neighboring level. Using the estimation technique, the zone allocation optimization can place data with similar lifetimes in the same zone. Besides, the garbage collection optimization can reclaim zones in an adaptive manner based on overlapping ratios to reduce the amount of data migration. Experimental results demonstrate that the optimization schemes effectively reduce garbage collection-incurred data copy by average factors of 2.11 × and 1.50 × in comparison to a conventional work and a state-of-the-art work, respectively. Consequently, the proposed work successfully alleviates the write amplification effect by 18% and 6%, compared to the conventional work and the state-of-the-art work, respectively.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408155468",
    "type": "article"
  },
  {
    "title": "A Lock-free RDMA-friendly Index in CPU-parsimonious Environments",
    "doi": "https://doi.org/10.1145/3722112",
    "publication_date": "2025-03-07",
    "publication_year": 2025,
    "authors": "Yuting Li; Yun Xu; Pengcheng Wang; Yonghui Xu; Weiguang Wang",
    "corresponding_authors": "",
    "abstract": "In CPU-parsimonious environments, such as disaggregated memory systems, the limited CPU power on the memory side constrains the ability to perform more operations. Thus, reducing CPU usage and enhancing concurrency performance are critical for indexing key-value storage in these scenarios. Current hash indexes support one-sided RDMA access and lock-free concurrency control with good performance but lack range query support. In contrast, existing tree indexes support range query but only implement expensive locks for concurrency control. Therefore, designing a tree index that supports lock-free concurrent access and one-sided RDMA is a significant challenge. To address these issues, this paper proposes a lock-free tree index based on the van Emde Boas (vEB) tree, called vBoost. vBoost inherits the vEB tree’s characteristics of index nodes without splitting or merging, and simplifies concurrency control by managing changes at a single node. It redesigns tree nodes with an 8-byte compact data structure, allowing each key-value pair to support RDMA_CAS atomic operations for lock-free concurrency. Additionally, vBoost leverages the RDMA Doorbell technique to reduce RTTs, enhancing range query and write performance. Evaluation results show that, vBoost achieves up to 3.85 × higher throughput and better scalability under YCSB workloads compared to state-of-the-art tree indexes.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408169690",
    "type": "article"
  },
  {
    "title": "SEED: Speculative Security Metadata Updates for Low-Latency Secure Memory",
    "doi": "https://doi.org/10.1145/3722111",
    "publication_date": "2025-03-07",
    "publication_year": 2025,
    "authors": "Xueliang Wei; Dan Feng; Wei Tong; Bing Wu; Xu Jiang",
    "corresponding_authors": "",
    "abstract": "Securing systems’ main memory is important for building trusted data centers. To ensure memory security, encryption and integrity verification techniques update the security metadata (e.g., encryption counters and integrity trees) during memory data writes. Existing studies are optimistic about the effect of data writes on system performance since they regard all data writes as background operations. However, we show that security metadata updates significantly increase data write latency. High-latency data writes frequently fill up write buffers in the system, forcing the system to perform the writes in the critical path. As a result, performance-critical data reads need to wait for the execution of these writes, which increases data read latency and degrades system performance. In this paper, we propose SEED that improves the performance of secure memory systems by speculatively updating security metadata in the background before data writes arrive. To enable speculative updates, SEED predicts which dirty cache lines will be written to memory through natural evictions. We find that cache evictions depend on multiple factors. To decouple the dependencies for accurate predictions, we devise a two-step eviction prediction method based on our observation that the next eviction victim rarely changes in a set. The first step predicts which cache sets will evict cache lines, while the second step predicts which cache lines will be evicted by finding the next eviction victims in the sets. For predicted evictions, we develop a speculative updater to perform speculative updates. We analyze the invariants that must be followed by the updater to ensure the correctness of speculative updates. The updater rolls back the speculatively updated security metadata of inaccurate predictions. To reduce the rollback overhead, we devise a rollback batching and an update pausing optimization for the updater. Experimental results show that SEED reduces data write latency by 39.8%, data read latency by 44.9%, and improves performance by 40.0% on average compared with the state-of-the-art secure memory design.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408169989",
    "type": "article"
  },
  {
    "title": "PANDA: Adaptive Prefetching and Decentralized Scheduling for Dataflow Architectures",
    "doi": "https://doi.org/10.1145/3721288",
    "publication_date": "2025-03-06",
    "publication_year": 2025,
    "authors": "Shantian Qin; Zhihua Fan; Wenming Li; Zhen Wang; Xuejun An; Xiaochun Ye; Dongrui Fan",
    "corresponding_authors": "",
    "abstract": "Dataflow architectures are considered promising architecture, offering a commendable balance of performance, efficiency, and flexibility. Abundant prior works have been proposed to improve the performance of dataflow architectures. Nevertheless, these solutions can be further improved due to the lack of efficient data prefetching and flexible task scheduling. In this paper, we propose a novel dataflow architecture with adaptive p refetching an d d ecentr a lized scheduling (PANDA). Firstly, we present an application-adaptive data prefetching method and on-chip memory microarchitecture designed to overlap memory access latency. Secondly, we introduce a decentralized dataflow scheduling approach and processing element (PE) microarchitecture aimed at improving hardware utilization. Experimental results show that in a wide range of real-world applications, PANDA attains up to 2.53 × performance improvement and 1.79 × energy efficiency improvement over the state-of-the-art dataflow architectures.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408198440",
    "type": "article"
  },
  {
    "title": "SRSparse: Generating Codes for High-Performance Sparse Matrix-Vector Semiring Computations",
    "doi": "https://doi.org/10.1145/3722114",
    "publication_date": "2025-03-07",
    "publication_year": 2025,
    "authors": "Zhen Du; Ying Liu; Ninghui Sun; Huimin Cui; Xiaobing Feng; Jiajia Li",
    "corresponding_authors": "",
    "abstract": "Sparse matrix-vector semiring computation is a key operation in sparse matrix computations, with performance strongly dependent on both program design and the features of the sparse matrices. Given the diversity of sparse matrices, designing a tailored program for each matrix is challenging. To address this, we propose SRSparse 1 an program generator that creates tailored programs by automatically combining program designing methods to fit specific input matrices. It provides two components: the problem definition configuration , which declares the computation, and the scheduling language , which can be leveraged by an auto-tuner to specify the program designs. The two are lowered to the intermediate representations of SRSparse, the Format IR and Kernel IR , which respectively generates format conversion routine and kernel code. We evaluate SRSparse on four representative sparse kernels and three format conversion routines. For sparse kernels, SRSparse achieves median speedups over handwritten programs: COO (3.50 ×), CSR-Adaptive (5.36 ×), CSR5 (2.06 ×), ELL (1.63 ×), Gunrock (1.57 ×), and GraphBLAST (1.96 ×); over an auto-tuner: AlphaSparse (1.16 ×); and over a compiler: TACO (1.71 ×). For format conversion routines, SRSparse achieves median speedups over handwritten implementations: Intel MKL (7.60 ×), SPARSKIT (2.61 ×), CUSP (2.77 ×), and Ginkgo (1.74 ×); and over a compiler: TACO (4.04 ×).",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408232191",
    "type": "article"
  },
  {
    "title": "Koala: Efficient Pipeline Training through Automated Schedule Searching on Domain-Specific Language",
    "doi": "https://doi.org/10.1145/3722113",
    "publication_date": "2025-03-07",
    "publication_year": 2025,
    "authors": "Yu Tang; Lujia Yin; Qiao Li; Hongyu Zhu; Hengjie Li; Xingcheng Zhang; Linbo Qiao; Dongsheng Li; Jiaxin Li",
    "corresponding_authors": "",
    "abstract": "Pipeline parallelism is a crucial technique for large-scale model training, enabling parameter splitting and performance enhancement. However, creating effective pipeline schedules often requires significant manual effort and coding skills, leading to practical inconveniences and complex debugging. Major frameworks such as DeepSpeed and ColossalAI simplify the process by adopting predefined pipeline schedule strategies, like GPipe and 1F1B. The use of predefined schedules offers limited flexibility and suboptimal training efficiency, as the limited number of manually-set candidates cannot provide the optimal strategy for arbitrary model training. To deal with the issue, this paper aims to automatically search for the optimal strategy with high efficiency. Since current frameworks only support a limited set of fixed strategies, lacking the technical capability to create a comprehensive strategy search space, we first design a novel domain-specific language (DSL) for pipeline schedule development. The DSL exhibits great understandability, agility, and reusability, supporting the development of all known pipeline schedule strategies and their variants. Second, we are the first to model the complete pipeline schedule strategy space via the DSL, enabling an automated end-to-end globally optimal pipeline schedule searching, while past work may stuck in a local optimum. Finally, we propose to optimize pipeline performance by modeling and solving the pipeline schedule as a Binary-Tree-Traversing (BTT) optimization problem. Based on the formalization, we further adopt a Dynamic Try-Test Genetic Algorithm to search for the best pipeline schedule strategy, which overwhelms a variety of pre-defined ones. Experimental results show that Koala achieves an enhanced performance by up to 1.53 × over state-of-the-art approaches. Besides, the pipeline schedule strategy searched by Koala outperforms pre-defined pipeline schedule strategies by 1.10 × ∼1.55 ×. Moreover, Koala has superior scalability and effectiveness in combining with data parallelism and tensor parallelism.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408232561",
    "type": "article"
  },
  {
    "title": "Gator: Accelerating Graph Attention Networks by Jointly Optimizing Attention and Graph Processing",
    "doi": "https://doi.org/10.1145/3722219",
    "publication_date": "2025-03-08",
    "publication_year": 2025,
    "authors": "Xiaobo Lu; Jianbin Fang; Peng Lin; Chun Huang; Zixiao Yu; Tiejun Li",
    "corresponding_authors": "",
    "abstract": "Graph attention networks (GATs) have advanced performance in various application domains by introducing the attention mechanism into the graph neural networks (GNNs). The inefficiency of running GATs on CPUs or GPUs necessitates specialized hardware designs. Unfortunately, previous specialized architecture designs have focused on either the GNN architecture or the attention mechanism, resulting in limited performance and leaving ample room for improvement. This paper presents Gator , a joint optimization approach with software-hardware co-designs for GAT inference. On the software level, Gator leverages degree-weighted graph partitioning and parameter-adaptive feature selection techniques to preprocess the input graph data, mining subgraph-level parallelism and mitigating the computation bottleneck of the dedicated dataflow. On the hardware level, Gator designs a unified processing engine to support various kernels by extracting a common computation pattern, and a dimension-aware microarchitecture for efficient partial sum reduction. Extensive experiments show that our approach can achieve 11.5 × more efficiency compared to NVIDIA RTX 4090, and provide a speedup of 3 × to 9.4 ×, along with a 2.6 × to 4.7 × reduction in memory traffic when compared to six state-of-the-art methods, with minimal accuracy loss.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408256689",
    "type": "article"
  },
  {
    "title": "SnsBooster: Enhancing Sampling-Based \\mu Arch Evaluation Efficiency through Online Performance Sensitivity Analysis",
    "doi": "https://doi.org/10.1145/3727637",
    "publication_date": "2025-04-01",
    "publication_year": 2025,
    "authors": "Chenji Han; Z. H. Zhang; Feng Xue; Xinyu Li; Yanqing Wu; Tingting Zhang; Tianyi Liu; Qi Guo; Fuxin Zhang",
    "corresponding_authors": "",
    "abstract": "Sampling-based methods, such as SimPoint, are widely used for efficient pre-silicon \\mu Arch evaluations, where the costs are the number of simulation points multiplied by the number of evaluated \\mu Arch designs. However, these costs keep growing with an increasing number of simulation points and expanding \\mu Arch design space. Although techniques have been developed to accelerate the \\mu Arch design space exploration, less attention has been given to further reducing the simulation budget of each \\mu Arch evaluation. Common strategies like reducing simulation coverage or sampling fewer simulation points typically compromise estimation accuracy. Therefore, further reducing the simulation budget without compromising estimation accuracy remains a critical research problem. In this work, we propose SnsBooster to enhance sampling-based \\mu Arch evaluation efficiency, based on two insights: (a) large portions of simulation points’ performance changes are typically insensitive to the evaluated \\mu Arch changes, and (b) simulation points’ performance sensitivities under specific \\mu Arch change correlate with their inherent characteristics. By online building a \\mu Arch-specific performance sensitivity classifier via progressive simulation and continuous validation, SnsBooster can identify and selectively evaluate only performance-sensitive points, thus reducing the simulation budget without compromising estimation accuracy. When applied across various \\mu Arch changes, SnsBooster achieves an average simulation budget reduction of 39.04% with an accuracy loss of only 0.14%, compared to simulating all the sampled points. Under the same accuracy loss, SnsBooster’s simulation budgets are only 64.73% and 65.60% of those required by methods of reducing simulation coverage or sampling fewer points. Besides, under identical simulation budgets, the average accuracy losses of these methods are 1.41% and 1.23%, which is substantially higher than that of SnsBooster.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409042472",
    "type": "article"
  },
  {
    "title": "Supporting Dynamic Program Sizes in Deep Learning-Based Cost Models for Code Optimization",
    "doi": "https://doi.org/10.1145/3727638",
    "publication_date": "2025-04-01",
    "publication_year": 2025,
    "authors": "Yacine Hakimi; Riyadh Baghdadi; Yacine Challal",
    "corresponding_authors": "",
    "abstract": "Automatic code optimization enables developers to write high-level code relying on compilers to optimize it and generate efficient code for target hardware. State-of-the-art methods for automatic code optimization leverage deep learning to build cost models that predict the impact of code optimizations on execution time. However, these models are typically limited in terms of the size and complexity of the programs they support. This research presents a novel approach to developing deep learning-based cost models that address these limitations. Our approach introduces a new program representation that efficiently represents programs with complex structures and large sizes such as varying loop depths, buffer numbers, and dimensions. Furthermore, we propose a novel deep learning architecture, that can handle this dynamic program representation. This allows the model to work on larger and more complex programs than those it was trained on. We implemented this model in Tiramisu, a state-of-the-art compiler. Our evaluation shows that our proposed model can generalize to programs larger than those seen during training, while the original Tiramisu cost model cannot. We also show that such generality does not lead to a significant increase in our proposed model’s MAPE (Mean Absolute Percentage Error) or a decrease in the quality of code optimizations found when the model is used for automatic code optimization. In contrast, our proposed model on average achieves a 41.89% improvement in speed compared to the original cost model when both models are trained on the same dataset, showing better generalization over unseen programs. This is a significant advantage over previous approaches, which typically do not support program sizes beyond those seen during the training.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409061765",
    "type": "article"
  },
  {
    "title": "Unleashing Parallelism with Elastic-Barriers",
    "doi": "https://doi.org/10.1145/3727639",
    "publication_date": "2025-04-02",
    "publication_year": 2025,
    "authors": "Abhiprayah Tiwari; V. Krishna Nandivada",
    "corresponding_authors": "",
    "abstract": "With the rise of multi-core processors, parallel programming has become essential, and managing synchronization overheads has become crucial for efficiency. Barriers, commonly used to synchronize threads, divide the program into different phases. Existing scheduling schemes address intra-phase load imbalance to some extent but do not fully resolve the issue of thread idling, especially in the context of programs with irregular parallel for-loops. This paper proposes an innovative solution called the elastic-barrier , where threads that arrive early at a barrier can execute iterations of the parallel-loop from the next phase, thereby reducing the idle time, and reduce load imbalance. The approach guarantees safety by ensuring that before executing any work W from the subsequent phase, all the works in the current phase that W depends on have been executed. The paper presents a compilation scheme that integrates compile-time and runtime techniques to optimize execution. We implemented our proposed scheme in the IMOP framework. Experimental results on graph-based benchmarks show that our approach improves the performance significantly.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409085586",
    "type": "article"
  },
  {
    "title": "ModNEF : An Open Source Modular Neuromorphic Emulator for FPGA for Low-Power In-Edge Artificial Intelligence",
    "doi": "https://doi.org/10.1145/3730581",
    "publication_date": "2025-04-16",
    "publication_year": 2025,
    "authors": "Aurélie Saulquin; Mazdak Fatahi; Pierre Boulet; Samy Meftali",
    "corresponding_authors": "",
    "abstract": "Neuromorphic computing is a novel computational paradigm that draws inspiration from the structure and function of the human brain. Spiking Neural Networks (SNNs) are a promising approach for implementing energy-efficient Artificial Neural Networks (ANNs) in embedded systems. In this paper, we present ModNEF, an open-source, neuromorphic digital hardware architecture designed for Field Programmable Gate Arrays (FPGAs). ModNEF is based on a modular architecture, where independent modules communicate via point-to-point connections to emulate SNNs. Our architecture offers two neuron models based on the Leaky Integrate and Fire (LIF) model, with a different emulation strategy. The modular nature of ModNEF allows researchers to extend the architecture by developing new modules to emulate different types of neurons or implement online learning rules. ModNEF is a clock-driven emulator, meaning that the neuron state is updated at regular intervals, even in the absence of input data. We evaluated the performance of the emulator using the MNIST and NMNIST datasets, with offline, full-precision training.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409483313",
    "type": "article"
  },
  {
    "title": "LarQucut: A New Cutting and Mapping Approach for Large-sized Quantum Circuits in Distributed Quantum Computing (DQC) Environments",
    "doi": "https://doi.org/10.1145/3730585",
    "publication_date": "2025-04-18",
    "publication_year": 2025,
    "authors": "Lei Liu",
    "corresponding_authors": "Lei Liu",
    "abstract": "Distributed quantum computing (DQC) is a promising way to achieve large-scale quantum computing. However, mapping large-sized quantum circuits in DQC is a challenging job; for example, it is difficult to find an ideal cutting and mapping solution when many qubits, complicated qubit operations, and diverse QPUs are involved. In this study, we propose LarQucut, a new quantum circuit cutting and mapping approach for large-sized circuits in DQC. LarQucut has several new designs. (1) LarQucut can have cutting solutions that use fewer cuts, and it does not cut a circuit into independent sub-circuits, therefore reducing the overall cutting and computing overheads. (2) LarQucut finds isomorphic sub-circuits and reuses their execution results. So, LarQucut can reduce the number of sub-circuits that need to be executed to reconstruct the large circuit's output, reducing the time spent on sampling the sub-circuits. (3) We design an adaptive quantum circuit mapping approach, which identifies qubit interaction patterns and accordingly enables the best-fit mapping policy in DQC. The experimental results show that, for large circuits with hundreds to thousands of qubits in DQC, LarQucut can provide a better cutting and mapping solution with lower overall overheads and achieves results closer to the ground truth.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409585285",
    "type": "article"
  },
  {
    "title": "DCMA: Accelerating Parallel DMA Transfers with a Multi-Port Direct Cached Memory Access in a Massive-Parallel Vector Processor",
    "doi": "https://doi.org/10.1145/3730582",
    "publication_date": "2025-04-18",
    "publication_year": 2025,
    "authors": "Gia Bao Thieu; Sven Gesper; Guillermo Payá–Vayá",
    "corresponding_authors": "",
    "abstract": "State-of-the-art applications, such as convolutional neural networks, demand specialized hardware accelerators that address performance and efficiency constraints. An efficient memory hierarchy is mandatory for such hardware systems. While the memory architectures of general-purpose processors (e.g., CPU or GPUs) are based on cache systems, dedicated accelerators have mostly adopted the DMA (Direct Memory Access) concept due to the application field of image processing. DMA features like 2D data transfers or data padding can optimize the memory accesses of image processing. However, DMA lacks the capability to exploit temporal and spatial data reuse, a feature common in cache systems, particularly when multiple DMAs operate in parallel. This paper proposes a novel Direct Cached Memory Access (DCMA) architecture, combining both DMA and cache methodologies and their respective advantages. Optimized for image-based AI algorithms, the DCMA architecture facilitates enhanced memory access by integrating multiple, parallel DMA ports with caching capabilities. This design allows for efficient data reuse and parallel memory access. Optimal parameters for the DCMA are determined through a comprehensive design space exploration. The DCMA is evaluated on a state-of-the-art Xilinx UltraScale+ FPGA board coupled with a massive-parallel vertical vector co-processor, called V 2 PRO. The results show the mitigation of the vector processor’s memory bottleneck. By using the proposed DCMA, speedups of up to x17 for the ResNet-50 CNN can be achieved.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409585609",
    "type": "article"
  },
  {
    "title": "GNNPilot: A Holistic Framework for High-Performance Graph Neural Network Computations on GPUs",
    "doi": "https://doi.org/10.1145/3730586",
    "publication_date": "2025-04-22",
    "publication_year": 2025,
    "authors": "Zhengding Hu; Jingwei Sun; Guangzhong Sun",
    "corresponding_authors": "",
    "abstract": "Graph Neural Networks (GNNs) have emerged as powerful tools for graph-based machine learning tasks, but their performance is often constrained by inefficient sparse operators and limited hardware utilization during multi-operator workflows. This paper presents GNNPilot, a holistic optimization framework that addresses these challenges through three key innovations. First, we introduce two packing strategies for gather operators, including neighbor packing for load balancing in sparser graphs, and bin packing with a new sparse format for enhanced data locality in denser graphs. Second, we propose dynamic parallelization methods and a novel row panel-based kernel fusion technique to optimize complex multi-operator GNN models. Third, we develop a lightweight sampling-based auto-tuning mechanism that adapts the framework’s optimization strategies to varying input characteristics. Built upon tensor expression-based intermediate representations, GNNPilot maintains the flexibility to optimize both popular and customized GNN models. Extensive experiments across diverse GNN models and graph datasets demonstrate that GNNPilot achieves substantial speedups over state-of-the-art implementations in both the performance of single operators and the efficiency of end-to-end inference. These results establish GNNPilot as an efficient and adaptive solution for accelerating GNN computations on modern GPU architectures.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409653046",
    "type": "article"
  },
  {
    "title": "HEngine: A High Performance Optimization Framework on a GPU for Homomorphic Encryption",
    "doi": "https://doi.org/10.1145/3732942",
    "publication_date": "2025-04-28",
    "publication_year": 2025,
    "authors": "Jinghao Zhao; Hongwei Yang; Meng Hao; Weizhe Zhang; Hui He; Desheng Wang",
    "corresponding_authors": "",
    "abstract": "Homomorphic encryption (HE) represents an encryption technology that allows for direct computation on encrypted data without requiring decryption. However, the substantial computational complexity and significant latency associated with HE has impeded its broader adoption in practical applications. To address these challenges, we propose a GPU-based acceleration framework, namely HEngine, tailored for homomorphic encryption tasks. Specifically, we first propose a warp shuffle-based optimization method for two key phases, i.e., inverse Chinese Remainder Theorem (ICRT) and number theoretic transformation (NTT), to mitigate synchronization overhead in homomorphic encryption. Secondly, we propose to fuse the NTT kernel with the inner product kernel to address the imbalance between memory access and computation. Thirdly, considering the potential difference in the amount of tasks of users in the real world, we design two different encoding methods for small batch and large batch inference tasks to improve computational efficiency. Finally, experiments demonstrate that our proposed framework achieves a 218 × speedup on homomorphic multiplication tasks compared with the CPU-based SEAL library. In addition, for convolutional neural network inference tasks on shallow network structures, our proposed framework achieves amortized inference performance at the millisecond level and sub-millisecond level on small batch and large batch data, respectively. For convolutional neural network inference tasks on deeper network structures (i.e., ResNet-20), our proposed framework achieves second-level inference.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409869611",
    "type": "article"
  },
  {
    "title": "9Ring: A 3D-Stacked Memory-Based Accelerator for Flexible and Efficient Deep CNN Applications",
    "doi": "https://doi.org/10.1145/3732940",
    "publication_date": "2025-04-28",
    "publication_year": 2025,
    "authors": "Wen Cheng; Qimin Cheng; Yi Liu; Lingfang Zeng; André Brinkmann; Yang Wang",
    "corresponding_authors": "",
    "abstract": "The massive computational and memory requirements of deep convolutional neural networks (DCNNs) have led to the development of neural network (NN) accelerators. However, as DCNN models grow in size, the demands on NN accelerators in terms of performance, memory bandwidth, and power efficiency continue to increase. We therefore present 9Ring , a flexible and efficient DCNN accelerator that takes full advantage of 3D-stacked memory, focusing on its hardware architecture, software scheduling, and optimization strategy. In particular, we first show that the mismatch between DCNN accelerators and DCNN models can lead to increased energy consumption and performance bottlenecks. We then present three flexible dataflow scheduling strategies to mitigate this mismatch. Afterwards, we introduce an energy efficiency analysis tool that can automatically search for the optimal scheduling scheme with respect to different DCNN models for energy efficiency. Finally, we conduct an empirical study showing that 9Ring can reduce energy consumption by 31.4% and 43.9% on average, and improve performance by 12% and 10% on average, compared to Tetris and the NN accelerators on conventional low-power DRAM memory systems, respectively.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409869741",
    "type": "article"
  },
  {
    "title": "GOLDYLOC: Global Optimizations &amp; Lightweight Dynamic Logic for Concurrency",
    "doi": "https://doi.org/10.1145/3730584",
    "publication_date": "2025-05-08",
    "publication_year": 2025,
    "authors": "Suchita Pati; Shaizeen Aga; Nuwan Jayasena; Matthew D. Sinclair",
    "corresponding_authors": "",
    "abstract": "Modern accelerators like GPUs increasingly execute independent operations concurrently to improve the device’s compute utilization. However, effectively harnessing it on GPUs for important primitives such as general matrix multiplications (GEMMs) remains challenging. Although modern GPUs have significant hardware and software GEMM support, their kernel implementations and optimizations typically assume each kernel executes in isolation and can utilize all GPU resources. This approach is highly efficient when kernels execute in isolation, but causes significant resource contention and slowdowns when kernels execute concurrently. Moreover, current approaches often only statically expose and control parallelism within an application, without considering runtime information such as varying input size and concurrent applications—often exacerbating contention. These issues limit performance benefits from concurrently executing independent operations. Accordingly, we propose GOLDYLOC , which considers the global resources across all concurrent operations to identify performant GEMM kernels, which we call globally optimized (GO)-Kernels. GOLDYLOC also introduces a lightweight dynamic logic which considers the dynamic execution environment for available parallelism and input sizes to execute performant combinations of concurrent GEMMs on the GPU. Overall, GOLDYLOC improves the performance of concurrent GEMMs on a real GPU by up to 2× (18% geomean per workload) versus the default concurrency approach and provides up to 2.5× (43% geomean per workload) speedup over sequential execution.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410211360",
    "type": "article"
  },
  {
    "title": "AOBO: A Fast-Switching Online Binary Optimizer on AArch64",
    "doi": "https://doi.org/10.1145/3736170",
    "publication_date": "2025-05-16",
    "publication_year": 2025,
    "authors": "Wenlong Mu; Yue Tang; Bo Huang; Jianmei Guo",
    "corresponding_authors": "",
    "abstract": "As the complexity of real-world server applications continues to grow, performance optimizations for large-scale applications are becoming increasingly challenging. The success of online optimization offered by OCOLOS and Dynimize proves that binary rewriting based on edge profiling data can significantly accelerate these applications. However, no similar online binary optimizer is currently available on the AArch64 platform. In response to the growing adoption of the AArch64 platform, this paper introduces AOBO, a fast-switching online binary optimizer specifically designed for AArch64. In addition to providing practical and efficient engineering support for AArch64-specific features, AOBO overcomes the challenge of lacking hardware counters for edge profiling on most commercially available AArch64 servers. In particular, AOBO embraces a novel edge weight estimation scheme to deliver more accurate edge estimation, which in turn allows AOBO’s binary rewriter to generate more efficient code. Furthermore, time spent on AOBO’s online code replacement stage is optimized to work at a subsecond level, thus enabling a fast switch from running the original binary to run the optimized one. We evaluate AOBO with CINT2017, GCC, MySQL and MongoDB, measuring the accuracy and coverage of the estimated edge weights, the performance improvements of the optimized binaries, and the online optimization cost. To make a fair comparison, we are using the performance data of the binaries generated by the default compilation scripts in the software packages as a baseline. Experimental data shows that AOBO can offer a more accurate edge weight estimation and generate binaries with superior performance. Furthermore, AOBO achieves online optimization with a very small overhead and significantly improves the performance of large-scale applications. Compared with the baselines, AOBO’s online optimization can achieve 24.7% and 31.11% performance improvement respectively for MySQL and MongoDB. Notably, application pause time is reduced from 1599.8 milliseconds to 462.1 milliseconds for MySQL, and from 1765.9 milliseconds to 507.1 milliseconds for MongoDB.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410443651",
    "type": "article"
  },
  {
    "title": "Cheetah: Accelerating Dynamic Graph Mining with Grouping Updates",
    "doi": "https://doi.org/10.1145/3736173",
    "publication_date": "2025-05-16",
    "publication_year": 2025,
    "authors": "Yi Zhang; Xiaomeng Yi; Yu Huang; Jingrui Yuan; Chuangyi Gui; Dan Chen; Long Zheng; Jianhui Yue; Xiaofei Liao; Hai Jin; Jingling Xue",
    "corresponding_authors": "",
    "abstract": "Graph pattern mining is essential for deciphering complex networks. In the real world, graphs are dynamic and evolve over time, necessitating updates in mining patterns to reflect these changes. Traditional methods use fine-grained incremental computation to avoid full re-mining after each update, which improves speed but often overlooks potential gains from examining inter-update interactions holistically, thus missing out on overall efficiency improvements. In this paper, we introduce Cheetah, a dynamic graph mining system that processes updates in a coarse-grained manner by leveraging exploration domains . These domains exploit the community structure of real-world graphs to uncover data reuse opportunities typically missed by existing approaches. Exploration domains, which encapsulate extensive portions of the graph relevant to updates, allow multiple updates to explore the same regions efficiently. Cheetah dynamically constructs these domains using a management module that identifies and maintains areas of redundancy as the graph changes. By grouping updates within these domains and employing a neighbor-centric expansion strategy, Cheetah minimizes redundant data accesses. Our evaluation of Cheetah across five real-world datasets shows it outperforms current leading systems by an average factor of 2.63 ×.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410443838",
    "type": "article"
  },
  {
    "title": "Performance, Energy and NVM Lifetime-Aware Data Structure Refinement and Placement for Heterogeneous Memory Systems",
    "doi": "https://doi.org/10.1145/3736174",
    "publication_date": "2025-05-16",
    "publication_year": 2025,
    "authors": "Manolis Katsaragakis; Christos Baloukas; Lazaros Papadopoulos; Francky Catthoor; Dimitrios Soudris",
    "corresponding_authors": "",
    "abstract": "The need for increased memory capacity, which also needs to be affordable and sustainable, leads to the adoption of heterogeneous memory hierarchies, combining DRAM and NVM technologies. This work proposes a memory management methodology that relies on multi-objective optimization in terms of performance, energy consumption and impact on NVM’s lifetime, for applications deployed on heterogeneous (i.e. DRAM/NVM) memory systems. We propose a scalable and lightweight data structure exploration flow for supporting data type refinement based on access pattern analysis, enhanced with a weighted-based data placement decision support for multi-objective exploration and optimization. The evaluation of the methodology was performed both on emulated and real DRAM/NVM hardware for different applications and data placement algorithms. The experimental results show up to 58.7% lower execution time and 48.3% less energy consumption compared to the results obtained by the initial versions of the applications. Moreover, we observed 72.6% less NVM write operations, which can significantly extend the lifetime of the NVM memory. Finally, thorough evaluation shows that the methodology is flexible and scalable, as it can integrate different data placement algorithms and NVM technologies and requires reasonable exploration time.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410443876",
    "type": "article"
  },
  {
    "title": "BigLittleMCA: A Spatially-Optimal Tiled Hardware Accelerator for MCMC Image Processing",
    "doi": "https://doi.org/10.1145/3736171",
    "publication_date": "2025-05-20",
    "publication_year": 2025,
    "authors": "Chris Kjellqvist; Lisa Wu Wills; Alvin R. Lebeck",
    "corresponding_authors": "",
    "abstract": "Markov-Chain Monte-Carlo (MCMC) algorithms offer a general framework for performing interpretable inference but have high overheads due to the computational complexity of the sampling process and the large number of samples required to produce an accurate result. Computer Vision is a common class of workloads that can be performed using MCMC methods. As computer vision workloads trend toward high-resolution real-time inference, it becomes challenging to perform inference in contexts such as edge computing, which operates under strict power and area budgets. Previous work explores hardware techniques for efficient sampling; however, MCMC algorithms still require many samples. We reduce the overheads of Gibbs Sampling, an MCMC algorithm, using an approach we call mixed-resolution sampling. This approach uses low-resolution inference to provide a starting point for full-resolution sampling. We evaluate this approach on three important computer vision tasks: stereo matching, optical flow, and blind source separation. Mixed-resolution sampling reduces root mean square error (RMSE) by an average of 19.6% for stereo-matching tasks, 13% for optical flow tasks, and 6.3% for blind source separation relative to traditional Gibbs Sampling. To enable real-time, explainable MCMC inference under edge power constraints, we exploit the structure of mixed-resolution sampling to architect and implement a hardware-software co-designed accelerator architecture, BigLittleMCA ( Big - Little MC MC A ccelerator). BigLittleMCA is a tiled MCMC accelerator architecture that uses a small sampler for low-resolution sampling and a large sampler for full-resolution sampling. Our results show that the architecture sustains real-time 720p inference at 30 FPS (frames per second) using 48.5% less power than prior work.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410518810",
    "type": "article"
  },
  {
    "title": "Benchmarking WebAssembly for Embedded Systems",
    "doi": "https://doi.org/10.1145/3736169",
    "publication_date": "2025-05-20",
    "publication_year": 2025,
    "authors": "Konrad Moron; Stefan Wallentowitz",
    "corresponding_authors": "",
    "abstract": "WebAssembly is a modern, low-level virtual machine with designed for improved application performance in web browsers. Recently, WebAssembly gained interest for its use outside the web, for example as a replacement for serverless container runtimes. A number of non-web WebAssembly implementations are actively supported, some of which target microcontrollers, IoT devices, and embedded systems. Such hardware platforms have strict resource constraints which may render the usage of WebAssembly impossible or too costly, for example due to its performance overhead and memory requirements. However, it is currently unclear what performance to expect of WebAssembly on low-resource microcontrollers compared to machine code and alternative application virtual machines. To answer this question, we evaluated the processing overhead and memory characteristics of WebAssembly application virtual machines on microcontrollers, and compare it to native execution, and the established application virtual machines: MicroPython and Lua. Furthermore, we analyzed the feature-set and architecture of the WebAssembly implementations in more detail, and measured the performance impact different runtime features have. We found that WebAssembly, despite its high extensibility and versatility in supported source languages, application paradigms, and target hardware, delivers very competitive performance. We conclude that WebAssembly can find wider industry usage for embedded systems and could replace other more costly or less flexible virtualization techniques, such as Java.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410518907",
    "type": "article"
  },
  {
    "title": "gECC: A GPU-based high-throughput framework for Elliptic Curve Cryptography",
    "doi": "https://doi.org/10.1145/3736176",
    "publication_date": "2025-05-20",
    "publication_year": 2025,
    "authors": "Qi-lin Xiong; Weiliang Ma; Xuanhua Shi; Yongluan Zhou; Hai Jin; Kai Huang; H F Wang; Zhengru Wang",
    "corresponding_authors": "",
    "abstract": "Elliptic Curve Cryptography (ECC) is an encryption method that provides security comparable to traditional techniques like Rivest–Shamir–Adleman (RSA) but with lower computational complexity and smaller key sizes, making it a competitive option for applications such as blockchain, secure multi-party computation, and database security. However, the throughput of ECC is still hindered by the significant performance overhead associated with elliptic curve (EC) operations, which can affect their efficiency in real-world scenarios. This paper presents gECC, a versatile framework for ECC optimized for GPU architectures, specifically engineered to achieve high-throughput performance in EC operations. To maximize throughput, gECC incorporates batch-based execution of EC operations and microarchitecture-level optimization of modular arithmetic. It employs Montgomery’s trick [40] to enable batch EC computation and incorporates novel computation parallelization and memory management techniques to maximize the computation parallelism and minimize the access overhead of GPU global memory. Furthermore, we analyze the primary bottleneck in modular multiplication by investigating how the user codes of modular multiplication are compiled into hardware instructions and what these instructions’ issuance rates are. We identify that the efficiency of modular multiplication is highly dependent on the number of Integer Multiply-Add (IMAD) instructions. To eliminate this bottleneck, we propose novel techniques to minimize the number of IMAD instructions by leveraging predicate registers to pass the carry information and using addition and subtraction instructions (IADD3) to replace IMAD instructions. Our experimental results show that, for ECDSA and ECDH, the two commonly used ECC algorithms, gECC can achieve performance improvements of 5.56 × and 4.94 ×, respectively, compared to the state-of-the-art GPU-based system. In a real-world blockchain application, we can achieve performance improvements of 1.56 ×, compared to the state-of-the-art CPU-based system. gECC is completely and freely available at https://github.com/CGCL-codes/gECC.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410519055",
    "type": "article"
  },
  {
    "title": "Attack and Defense: Enhancing Robustness of Binary Hyper-Dimensional Computing",
    "doi": "https://doi.org/10.1145/3736172",
    "publication_date": "2025-05-20",
    "publication_year": 2025,
    "authors": "Haoming Li; Fangxin Liu; Zongwu Wang; Ning Yang; Shiyuan Huang; Xiaoyao Liang; Haibing Guan; Li Jiang",
    "corresponding_authors": "",
    "abstract": "Hyper-Dimensional Computing (HDC) has emerged as a lightweight computational model, renowned for its robust and efficient learning capabilities, particularly suitable for resource-constrained hardware. As HDC often finds its application in edge devices, the associated security challenges pose a critical concern that cannot be ignored. In this work, we aim to quantitatively delve into the robustness of binary hyper-dimensional computing, which is widely recognized for its robustness. Employing the bit-flip attack as our initial focal point, we meticulously devise both an attack mechanism and a corresponding defense mechanism. Our objective is to comprehensively explore the robustness of the binary hyper-dimensional computation model, aiming to gain a deeper understanding of its security vulnerabilities and potential defenses. Specifically, we introduce a novel attack framework for HDC, named HyperAttack, which is capable of compromising a robust binary HDC model by maliciously flipping a minimal number of bits within its memory system (specifically, the DRAM) that houses the associative memory. The bit-flip operation is executed through the well-known Row Hammer attack, and HyperAttack optimizes the accuracy degradation by pinpointing the most vulnerable bits in the hyper-dimensional vectors (represented as binary vectors within the associative memory) of the HDC model. The proposed HyperAttack framework is grounded in the principles of fuzziness, seamlessly integrating dimensional ranking and feature similarity analysis within hypervectors to precisely identify the bits to be flipped. Furthermore, we have developed a defense mechanism named HyperDefense, designed to bolster the robustness of binary hyper-dimensional computational models against bit-flip attacks. This defense scheme is tailored specifically for HDC models, providing a robust safeguard against potential threats. HyperDefense operates directly on the associative memory of HDC models, strengthening their defenses. By meticulously modifying selected bits, HyperDefense maintains a high level of accuracy close to the original model, even in the face of increased bit flip rates. This defense mechanism leverages redundant dimensions as backups for critical information. Through a thorough analysis of dimension importance, HyperDefense achieves superior robustness by gracefully sacrificing non-critical dimensions, thus ensuring the model’s robustness against potential attacks.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410519148",
    "type": "article"
  },
  {
    "title": "In-SRAM Parallel Data Shuffle",
    "doi": "https://doi.org/10.1145/3743136",
    "publication_date": "2025-06-05",
    "publication_year": 2025,
    "authors": "Cuiyuan Jia; Zhang Dunbo; Qingjie Lang; Ruoxi Wang; Li Shen",
    "corresponding_authors": "",
    "abstract": "While Single Instruction Multiple Data (SIMD) units are widely employed in processors for neural networks, signal processing, and high-performance computing, they suffer from expensive shuffle operations dedicated to data alignment. In fact, shuffle operations only change the layout of data and ideally should be done entirely within memory. To this end, we propose Shuffle SRAM in this paper, which can shuffle multiple data elements simultaneously across SRAM banks. The key idea is exploiting inter-bank word line wise data movement to shuffle data in parallel, where all data elements on the same word line of SRAM can be shuffled simultaneously, achieving a high level of parallelism. Through suitable data layout preparation and proper control, Shuffle SRAM efficiently supports a wide range of commonly used shuffle operations. Our evaluation results show that the Shuffle SRAM can reap performance benefits of 14.3 × for data reorganization only applications and 1.97 × for data reorganization + computation applications over conventional shuffle architecture on general-purpose processors. With Shuffle SRAM, the state-of-the-art vector processor can obtain 2.58 × energy efficiency. Compared with traditional SRAM, Shuffle SRAM only increases 3.5% additional area overhead.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411066161",
    "type": "article"
  },
  {
    "title": "Leveraging iterative applications to improve the scalability of task-based programming models on distributed systems",
    "doi": "https://doi.org/10.1145/3743134",
    "publication_date": "2025-06-05",
    "publication_year": 2025,
    "authors": "Omar Shaaban; Juliette Fournis d'Albiat; Isabel Piedrahita; Vicenç Beltrán; Xavier Martorell; Paul Carpenter; Eduard Ayguadé; Jesús Labarta",
    "corresponding_authors": "",
    "abstract": "Distributed tasking models such as OmpSs-2@Cluster, StarPU-MPI, and PaRSEC express HPC applications as task graphs with explicit dependencies. The single task graph unifies the representation of parallelism across CPU cores, accelerators, and distributed-memory nodes, offering higher programmer productivity compared to traditional MPI + X. Most task-based models construct the task graph sequentially, which provides a clear and familiar programming model, simplifying code development, maintenance and porting. However, this design introduces a bottleneck in task creation and dependency management, limiting performance and scalability. As a result, unless the tasks are very coarse-grained, current distributed sequential tasking models cannot match the performance of MPI + X. Many scientific applications, however, are iterative in nature, constructing the same directed acyclic task graph at each timestep. We exploit this structure to eliminate the sequential bottleneck and control message overhead in a sequentially-constructed distributed tasking model, while preserving its simplicity and productivity. Our approach builts on the recently proposed taskiter directive for OpenMP and OmpSs-2, allowing a single iteration to be expressed as a cyclic graph. The runtime partitions the cyclic graph across nodes, precomputes the MPI transfers, and then executes the loop body at low overhead. By integrating the MPI communications directly into the application’s task graph, our approach naturally overlaps computation and communication, in some cases exposing dramatically more parallelism than fork–join MPI + OpenMP. We define the programming model and describe the full runtime implementation, and integrate our proposal into OmpSs-2@Cluster. We evaluate it using five benchmarks on up to 128 nodes of the MareNostrum 5 supercomputer. For applications with fork–join parallelism, our approach has performance similar to fork–join MPI + OpenMP, making it a viable productive alternative, unlike the existing OmpSs-2@Cluster model, which is up to 7.7 times slower than MPI + OpenMP. For a 2D Gauss–Seidel stencil computation, our approach enables 3D wavefront computation, giving performance up to 22 times faster than fork–join MPI + OpenMP and on-a-par with state-of-the-art TAMPI + OmpSs-2. All software, comprising the compiler, runtime and benchmarks, is released open source.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411066362",
    "type": "article"
  },
  {
    "title": "MetaEC: An Efficient and Resilient Erasure-Coded KV Store on Disaggregated Memory",
    "doi": "https://doi.org/10.1145/3744905",
    "publication_date": "2025-06-16",
    "publication_year": 2025,
    "authors": "Qiliang Li; Min Lyu; Tian Liu; Liangliang Xu; Wei Wang; Yinlong Xu",
    "corresponding_authors": "",
    "abstract": "In-memory KV stores have recently been migrated from traditional monolithic servers to disaggregated memory (DM) for higher resource utilization and elasticity. These works use replication-based schemes for fault tolerance, which can be replaced with erasure coding (EC) for space efficiency. However, existing EC schemes designed in KV stores on traditional monolithic architectures encounter performance constraints when directly implemented in DM due to the challenges in EC metadata management and consistent parity updating. This paper proposes MetaEC, an erasure-coded KV store on DM with high efficiency and resilience. First, for organizing KV pairs to stripes, MetaEC logically forms data chunks and leverages lazy coding to remove the accumulating and coding latency from the critical path. Second, for efficient EC metadata management, MetaEC designs EC metadata structures based on accessing features, and employs a hybrid redundancy schema with deterministic distribution to provide fault tolerance with high storage efficiency. Third, for consistent parity updating, we design a parity updating protocol based on parity logging and co-design EC metadata structures to handle concurrent conflicts by allowing only concurrent reads or writes. Experimental results show that compared with the state-of-the-art replication-based KV stores on DM, MetaEC achieves up to 53.33% latency reduction, up to 31.01% throughput improvement, and 58.17% memory consumption savings.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411350494",
    "type": "article"
  },
  {
    "title": "Accelerating Parallel Structures in DNNs via Parallel Fusion and Operator Co-Optimization",
    "doi": "https://doi.org/10.1145/3744906",
    "publication_date": "2025-06-16",
    "publication_year": 2025,
    "authors": "Zhanyuan Di; Leping Wang; Zhaojia Ma; En Shao; Jie Zhao; Ziyi Ren; S.M. Feng; Dingwen Tao; Guangming Tan; Ninghui Sun",
    "corresponding_authors": "",
    "abstract": "Parallel structures have become a key pattern in deep neural networks (DNNs), offering improved efficiency and scalability. However, existing machine learning compilers (MLCs) face challenges in optimizing these structures due to limited parallel fusion scope and insufficient analysis of intra-operator characteristics. This paper introduces Magneto, a framework designed to accelerate DNN inference by co-optimizing parallel operators. Magneto broadens the fusion scope and incorporates a specialized co-tuning algorithm to optimize operators jointly. Our approach addresses the unique challenges inherent in optimizing parallel structures, enabling significant performance improvements across various hardware platforms. Experimental results show that Magneto outperforms state-of-the-art NVIDIA TensorRT and AMD MIGraphX, achieving geometric mean speedups of 2.27 × and 2.88 ×, respectively.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411350627",
    "type": "article"
  },
  {
    "title": "HopScotch: A Holistic Approach to Data Layout-Aware Mapping on NPUs for High-Performance DNN Inference",
    "doi": "https://doi.org/10.1145/3711821",
    "publication_date": "2025-06-25",
    "publication_year": 2025,
    "authors": "Suhong Lee; Boyeal Kim; Yongseok Choi; Hyuk‐Jae Lee",
    "corresponding_authors": "",
    "abstract": "Modern deep neural networks (DNNs) are widely utilized across a broad range of domains, scaling rapidly and often comprising hundreds of diverse layers with varying types and configurations. To accelerate DNN execution, specialized hardware solutions, known as neural processing units (NPUs), have been developed. However, this heterogeneity of layers in a DNN model may cause performance degradation on NPUs. For example, while a layer’s execution or dataflow is generally associated with a specific data access order, the data layout in on-chip memory may not be well aligned with it, introducing bubble cycles for layout reordering. Given the hundreds of diverse layers in DNNs, this layout reordering overhead presents a new challenge for achieving efficient end-to-end DNN inference on NPUs. To address this problem, this paper introduces HopScotch, a holistic approach to data layout-aware mapping of DNNs on NPUs. First, HopScotch adopts a routing interconnect between the on-chip memory and the systolic array utilizing three-input multiplexers, paired with an on-chip programmable vector processor to manage arbitrary data layout reordering at runtime. Additionally, it introduces a tailored data layout to accommodate a variety of convolutional configurations within the proposed microarchitecture. Second, HopScotch presents a novel layout mapping solver that employs a top-k selection strategy based on a beam search algorithm, facilitating the efficient exploration of the vast layout mapping space at compile time. Third, the proposed layout mapping solver is integrated into the HopScotch mapping framework (HMF) to explore the layout mapping space and evaluate the resulting performance. Experiments with popular DNN models show that HopScotch reduces layout reordering costs by up to 98.2% and 90.3%, resulting in speedups of 2.62 × and 1.64 × in end-to-end latency, compared to XLA and GCD 2 , respectively.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411652463",
    "type": "article"
  },
  {
    "title": "Performance Implications of Pipelining the Data Transfer in CPU-GPU Heterogeneous Systems",
    "doi": "https://doi.org/10.1145/3746231",
    "publication_date": "2025-06-26",
    "publication_year": 2025,
    "authors": "Ruihao Li; Bagus Hanindhito; Sanjana Yadav; Qinzhe Wu; Krishna M. Kavi; Gayatri Mehta; Neeraja J. Yadwadkar; Lizy K. John",
    "corresponding_authors": "",
    "abstract": "Driven by the increasing demands of machine learning, heterogeneous systems combining CPUs and GPUs have emerged as the dominant architecture for parallel computing in recent years. To optimize memory management and data transfer between CPUs and GPUs, Nvidia GPUs have introduced unified virtual memory ( UVM ) and pinned memory ( PM ) over the last decade. UVM can avoid explicit memory copies and potentially overlap GPU kernel computations with CPU-GPU data transfer. PM ensures that data with high locality remains in the main memory, preventing it from being paged out. In addition to these two techniques, asynchronous memory copy ( Async Memcpy ) was introduced recently in Nvidia GPUs to improve the CPU-GPU pipeline further. By utilizing Async Memcpy , the data transfer from GPU global memory to shared memory can be overlapped with GPU computations, adding an additional stage to the CPU-GPU data transfer pipeline. A thorough performance analysis of how Async Memcpy affects the current UVM and PM CPU-GPU data transfer scheme is desired. In this paper, we provide performance implications of the combined effect of UVM , PM , and Async Memcpy , exploring which applications benefit from which combination of these features. We implement all these features on a suite of 25 workloads, including microbenchmarks and realworld applications. We observe an average performance gain of \\(24\\% \\) when utilizing UVM and a \\(34\\% \\) gain when employing PM on realworld applications, compared to not applying any data transfer optimization techniques. The performance benefits of Async Memcpy vary across different workloads. For workloads featuring extensive shared memory usage and high compute density (e.g., kmeans and lud ), Async Memcpy delivers around a 20% performance improvement over using UVM or PM alone. In other workloads like knn , we note a \\(20\\% \\) performance degradation when using Async Memcpy . Furthermore, we conduct an in-depth investigation of the GPU kernel using performance counters to uncover the root causes of performance differences among various data transfer models. We also perform sensitivity analyses to examine how the number of blocks and threads, as well as the L1-cache/shared memory partitioning, impact performance. We explore future research directions aimed at enhancing the data transfer pipeline by overlapping memory allocation with data transfer and computation across GPU kernels.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411670522",
    "type": "article"
  },
  {
    "title": "ZNSFQ: An Efficient and High-Performance Fair Queue Scheduling Scheme for ZNS SSDs",
    "doi": "https://doi.org/10.1145/3746230",
    "publication_date": "2025-06-26",
    "publication_year": 2025,
    "authors": "Yachun Liu; Dan Feng; Jianxi Chen; Jing Hu; Zhouxuan Peng; Jinlei Hu",
    "corresponding_authors": "",
    "abstract": "The Zoned Namespace (ZNS) interface transfers most storage maintenance responsibilities from the underlying Solid-State Drives (SSDs) to the host. This shift creates new opportunities to ensure fairness and high performance in multi-tenant cloud computing environments at both hardware and software levels. However, when applications with different workloads share a single ZNS SSD hardware, traditional fair queueing schedulers fail to achieve fairness due to their limited awareness of workload characteristics. Moreover, allowing multiple outstanding requests to access the device simultaneously improves resource utilization but often leads to significant I/O interference among these requests. This interference results in over-throttling, which subsequently degrades the performance of existing fair queueing schedulers. To address the above problems, this paper proposes an efficient and high-performance fair queueing scheduling scheme for ZNS SSD (ZNSFQ) on the host side. Firstly, ZNSFQ introduces a workload-aware fair scheduler that enhances fairness by accurately estimating the I/O cost for each application based on its workload characteristics. Secondly, to optimize performance while ensuring fairness, ZNSFQ designs a request dispatch parallelism adjuster. This adjuster manages the channel-level request dispatch parallelism for each application to minimize I/O interference. Finally, ZNSFQ employs a global adaptive coordinator to alleviate device-level I/O blocking, reducing tail latency and CPU consumption while satisfying fairness and performance. A comprehensive evaluation demonstrates that ZNSFQ significantly enhances fairness and performance compared to the latest fair queuing schedulers. In sequential access scenarios, ZNSFQ enhances fairness by over 38.13% and increases I/O bandwidth by more than 49.24%. Furthermore, in random access scenarios, it reduces CPU utilization by 70.22% while maintaining both fairness and high performance.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411670559",
    "type": "article"
  },
  {
    "title": "DCSolver: Accelerating Sparse Iterative Solvers via Divide-and-Conquer on GPUs",
    "doi": "https://doi.org/10.1145/3746233",
    "publication_date": "2025-06-26",
    "publication_year": 2025,
    "authors": "Haozhong Qiu; Chuanfu Xu; Jianbin Fang; Jian Zhang; Liang Deng; Z. G. Dai; Yue Ding; Yue Wang; Zhiping Han; Yonggang Che; Jie Liu",
    "corresponding_authors": "",
    "abstract": "Sparse iterative solvers are commonly used in various fields. However, certain essential kernels of these solvers, such as sparse triangular solves (SpTRSV), present significant challenges for efficient parallelization due to data dependencies . Previous methods, like level-scheduling or multi-coloring, typically involve creating a Task Dependency Graph (TDG) to represent data dependencies and identify independent sets from the TDG for parallel execution. However, these approaches often result in limited parallelism with substantial synchronization overheads or negatively impact the solver convergence rate. This paper introduces DCSolver , a Divide-and-Conquer (DC) framework designed to efficiently parallelize sparse solvers with data dependencies on GPUs. To achieve this, we break down the solver TDG into independent subgraphs, allowing us to exploit both coarse-grained and fine-grained parallelism. To efficiently allocate GPU threads for subgraphs with varying degrees of parallelism, we have developed an adaptive in-warp scheduling strategy. Additionally, we propose a hybrid parallelization scheme in DCSolver, which involves employing different parallel approaches for different DC recursions to achieve a more optimal balance between parallelism and convergence for solvers. To evaluate the effectiveness of DCSolver, we apply it to two preconditioned Krylov subspace solvers and an unstructured mesh Computational Fluid Dynamics (CFD) solver. Our results show that when compared to the state-of-the-art methods, DCSolver accelerates the time-to-solution of solvers by an average speedup of up to 26.19X.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411670560",
    "type": "article"
  },
  {
    "title": "TianheGraph: Topology-aware Graph Processing",
    "doi": "https://doi.org/10.1145/3750450",
    "publication_date": "2025-07-22",
    "publication_year": 2025,
    "authors": "Xinbiao Gan",
    "corresponding_authors": "Xinbiao Gan",
    "abstract": "Many real-world graph data can have billions to trillions of edges. Processing graphs at such scales requires the efficient use of parallel computing systems. However, current graph processing engines and methods struggle to scale beyond a few dozen computing nodes because they (i) cannot efficiently store and process graph data on this scale due to the huge memory footprint incurred and (ii) do not account for the variations in communication costs across different levels of the interconnection hierarchy. We introduce TianheGraph, a software approach to reduce the memory footprint of graphs and optimize graph processing on large-scale parallel systems with complex hardware interconnection components. TianheGraph integrates a new space-time-efficient graph compression technique to reduce the memory footprint of large-scale graphs. It provides a novel graph partitioning method to improve load balancing and minimize communication overhead across various levels of the interconnection hierarchy. We evaluate TianheGraph by applying it to fundamental graph operations on synthetic and real-world graphs, using up to 79,024 computing nodes and over 1.2 million processor cores. Our extensive experiments show that TianheGraph outperforms state-of-the-art parallel graph processing engines in throughput and scalability. Moreover, TianheGraph outperformed the top-ranked systems on the Graph 500 list at the time of submission.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412554204",
    "type": "article"
  },
  {
    "title": "Supports of Data Cache Division for Computational Solid-state Drives",
    "doi": "https://doi.org/10.1145/3747845",
    "publication_date": "2025-07-22",
    "publication_year": 2025,
    "authors": "Zhibing Sha; Shuaiwen Yu; Chengyong Tang; Zhigang Cai; P. S. Tang; Min Huang; Jun Li; Jianwei Liao",
    "corresponding_authors": "",
    "abstract": "The computational SSD ( CompSSD ), with high computing capabilities, can function not only as a storage device but also as a computing node. The data cache of the CompSSD device stores both the output data from host-side tasks and the input data for tasks executed on the CompSSD . However, current cache management strategies are optimized for traditional SSDs and are incompatible with the unique requirements of CompSSD . To address the issue of cache management for CompSSD , this paper proposes a novel cache division scheme, to dynamically divide the cache into two parts, for separately buffering output data from host-side tasks and input data used by CompSSD -side tasks. To this end, we construct a mathematical model that periodically estimate an optimal cache division ratio, by considering the factors of the ratios of read/write data amount, the cache hits, and the overhead of data transfer between the storage device and the host. Besides, we propose a scheme of proactive data flushing to write the output data to the underlying flash arrays, without impacts on I/O responsiveness. The trace-driven experiments show that our scheme can improve the overall I/O latency by 35.4% on average, in contrast to existing cache management schemes for CompSSD devices.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412554209",
    "type": "article"
  },
  {
    "title": "Ephemera: Accelerating I/O-Intensive Serverless Workloads with a Harvested In-memory File System",
    "doi": "https://doi.org/10.1145/3747846",
    "publication_date": "2025-07-22",
    "publication_year": 2025,
    "authors": "Lingxiao Jin; Zinuo Cai; H Wang; Zongpu Zhang; Ruhui Ma; Haibing Guan; Yuan Liu; Rajkumar Buyya",
    "corresponding_authors": "",
    "abstract": "Serverless computing has gained popularity for its ability to shift the burden of server management from developers to cloud providers, which allows providers to exercise greater control over resource management, optimizing configurations to enhance efficiency and performance. The diversity of serverless computing tasks, from short-lived, event-driven tasks to more complex workloads, highlights the growing importance of efficient file I/O performance for I/O-intensive workloads, yet effectively handling ephemeral storage for I/O-intensive tasks remains a challenge. Traditional file system approaches often introduce substantial latency and fail to fully leverage available memory resources within the execution environment, limiting performance and efficiency. Our work stems from the observation of the under-utilization of memory resources in serverless computing platforms and the potential efficiency improvement of I/O operations using an in-memory file system. Based on this observation, we propose Ephemera , a system designed to enhance ephemeral storage efficiency and memory utilization. Ephemera satisfies three design goals: transparent memory I/O integration , heterogeneous tasks resource synergy , and harmonized cluster workload orchestration . Ephemera integrates three components: the Runtime Daemon, responsible for managing a container’s in-memory file system; the Tenant Manager, facilitating memory configuration sharing across containers; and the Cluster Controller, optimizing workload balancing. Our experiments demonstrate that Ephemera significantly improves performance for I/O-intensive tasks compared to traditional file systems. Specifically, Ephemera decreases I/O processing time by 50% on average and reduces latency by up to 95.73% in certain scenarios with negligible overhead.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412554211",
    "type": "article"
  },
  {
    "title": "Mobile-3DCNN: An Acceleration Framework for Ultra-Real-Time Execution of Large 3D CNNs on Mobile Devices",
    "doi": "https://doi.org/10.1145/3747842",
    "publication_date": "2025-07-22",
    "publication_year": 2025,
    "authors": "Wei Niu; Mengshu Sun; Zhengang Li; Jou-An Chen; Jiexiong Guan; Xipeng Shen; Jun Liu; Mei Zhang; Yanzhi Wang; Xue Lin; Bin Ren",
    "corresponding_authors": "",
    "abstract": "It is challenging to deploy 3D Convolutional Neural Networks (3D CNNs) on mobile devices, specifically if both real-time execution and high inference accuracy are in demand, because the increasingly large model size and complex model structure of 3D CNNs usually require tremendous computation and memory resources. Weight pruning is proposed to mitigate this challenge. However, existing pruning is either not compatible with modern parallel architectures, resulting in long inference latency or subject to significant accuracy degradation. This paper proposes an end-to-end 3D CNN acceleration framework based on pruning/compilation co-design called Mobile-3DCNN that consists of two parts: a novel, fine-grained structured pruning enhanced by a prune/Winograd adaptive selection (that is mobile-hardware-friendly and can achieve high pruning accuracy), and a set of compiler optimization and code generation techniques enabled by our pruning (to fully transform the pruning benefit to real performance gains). The evaluation demonstrates that Mobile-3DCNN outperforms state-of-the-art end-to-end DNN acceleration frameworks that support 3D CNN execution on mobile devices, Alibaba Mobile Neural Networks and Pytorch-Mobile with speedup up to 34 × with minor accuracy degradation, proving it is possible to execute high-accuracy large 3D CNNs on mobile devices in real-time (or even ultra-real-time).",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412554219",
    "type": "article"
  },
  {
    "title": "Cppless: Single-Source and High-Performance Serverless Programming in C++",
    "doi": "https://doi.org/10.1145/3747841",
    "publication_date": "2025-07-22",
    "publication_year": 2025,
    "authors": "Marcin Copik; Lukas Möller; Alexandru Calotoiu; Torsten Hoefler",
    "corresponding_authors": "",
    "abstract": "The rise of serverless computing introduced a new class of scalable, elastic and widely available parallel workers in the cloud. Many systems and applications benefit from offloading computations and parallel tasks to dynamically allocated resources. However, the developers of C++ applications find it difficult to integrate functions due to complex deployment, lack of compatibility between client and cloud environments, and loosely typed input and output data. To enable single-source and efficient serverless acceleration in C++, we introduce Cppless , an end-to-end framework for implementing remote functions which handles the creation, deployment, and invocation of serverless functions. Cppless is built on top of LLVM and requires only two compiler extensions to automatically extract C++ function objects and deploy them to the cloud. We demonstrate that offloading parallel computations, such as from a C++ application to serverless workers, can provide up to 59x speedup with minimal cost increase while requiring only minor code modifications.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412554220",
    "type": "article"
  },
  {
    "title": "HAKV: A Hotness-Aware Zone Management Approach to Optimizing Performance of LSM-tree-based Key-Value Stores",
    "doi": "https://doi.org/10.1145/3747848",
    "publication_date": "2025-07-22",
    "publication_year": 2025,
    "authors": "Hui Sun; Qihang Yue; Guanzhong Chen; Yi Zou; Yinliang Yue; Xiao Qin",
    "corresponding_authors": "",
    "abstract": "Log-Structured Merge tree-based key-value (KV) stores, like LevelDB and RocksDB, are extensively applied in large-scale data storage systems. This design excels in write-intensive environments by converting random writes into sequential append operations. Despite its advantages, KV stores struggle with real-world workloads where most updates in KV pairs are infrequent. The compaction process and hierarchical data organization result in high write and read amplification. To mitigate these issues, we propose HAKV – a hotness-aware zone management approach to optimizing performance of KV stores. HAKV first separates hot KV pairs from cold KV pairs, storing hot KV pairs in dedicated zones within persistent memory (PM), enabling centralized and lightweight compaction. Second, we propose a storage zone structure in PM to achieve space optimization for cold KV pairs. Third, to bolster cache hit ratio in PM, we provide a hierarchical data framework for hot KV pairs – and a recycling strategy for invalid hot KV pairs in a zone to enhance the space utilization of PM for hot KV pairs. Finally, we design a dynamic window-based adaptive adjustment mechanism for zone pool in PM to optimize the space utilization. Thus, HAKV significantly reduces write amplification while boosting overall read and write performance. The experimental results demonstrate that HAKV achieves write amplification reduction by up to 92.3%, 79.2%, 90.2%, 41.1%, 80.6%, and 62.4% compared with LevelDB, RocksDB, NoveLSM, LightKV, Wisckey, and UniKV, respectively, with average reduction rates of 89.6%, 74.4%, 84.9% 32.3%, 63.7%, and 42.5%. Furthermore, HAKV boosts random write performance by up to 54.2 ×, 51.5 ×, 44.2 ×, 4.3 ×, 3.1 ×, and 4.3 ×, respectively – and the average improvement reaches 25.8 ×, 20.9 ×, 23.9 ×, 2.7 ×, 2.5 ×, and 3.4 ×.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412554222",
    "type": "article"
  },
  {
    "title": "SampDedup: Sampling Prediction for Efficient Inline Data Deduplication on Non-volatile Memory",
    "doi": "https://doi.org/10.1145/3750447",
    "publication_date": "2025-07-23",
    "publication_year": 2025,
    "authors": "Ziyue Xu; Yunfei Li; Ranzhe Deng; Liping Yi; Yusen Li; Gang Wang; Xiaoguang Liu",
    "corresponding_authors": "",
    "abstract": "Data deduplication is an effective technique for reducing redundant data storage space in various storage systems. Generally, deduplication consists of four steps: chunking, fingerprinting, fingerprint lookup, and data management. Recently, Non-volatile Memory (NVM) as an emerging storage device has received widespread attention. Directly applying the deduplication technique on NVM for storage cost savings faces many challenges: a) deduplication on NVM devices suffers from computation bottleneck instead of the I/O bottleneck faced by deduplication on traditional storage devices (such as HDD and SSD); b) new fingerprint indexes and metadata are required to be re-designed to adapt to NVM characteristics; c) inline deduplication on NVM is more sensitive to the latency. To solve these challenges, we propose a novel Samp ling prediction-based inline data Dedup lication method ( SampDedup ) on NVM devices. It aims to ensure high deduplication ratios while reducing computation costs and latency by optimizing data chunking , fingerprinting , and fingerprint lookup . a) For data chunking , a sampling prediction-based chunking method ( SampChunk ) is proposed to leverage chunk similarity to distinguish duplicate chunks and skip them for chunking. This method can be easily integrated into most sliding-window based and non-window based CDC chunking algorithms. b) For fingerprinting , the commonly used SHA-1 algorithm is further optimized to reduce the extra computational overhead introduced by SampChunk, and an asynchronous fingerprinting method is proposed to reduce the fingerprinting latency of unique chunks. c) For fingerprint lookup , we design a header fingerprint index and metadata table for each data chunk constructed by SampChunk on NVM, and we use a fast-read buffer to replace the traditional slow LRU cache to improve search efficiency. Experiments on 4 real-world datasets demonstrate that SampDedup consistently presents high inline data deduplication ratios on NVM with different workloads and data partitioning algorithms while saving more than \\(90\\% \\) chunking time compared with state-of-the-art deduplication baselines.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412593892",
    "type": "article"
  },
  {
    "title": "GenCNN: A Partition-Aware Multi-Objective Mapping Framework for CNN Accelerators Based on Genetic Algorithm",
    "doi": "https://doi.org/10.1145/3747844",
    "publication_date": "2025-07-23",
    "publication_year": 2025,
    "authors": "Y. Mu; Zhihua Fan; Wenming Li; Zhiyuan Zhang; Xuejun An; Dongrui Fan; Xiaochun Ye",
    "corresponding_authors": "",
    "abstract": "Convolutional Neural Networks (CNNs) require partitioning to efficiently run on CNN accelerators, which offer multiple parallel processing dimensions, such as Processing Element (PE) array topologies and Single Instruction Multiple Data (SIMD) execution. The choice of parallelization strategy directly impacts accelerator performance. However, the vast search space for CNN partitioning and parallelization makes manual optimization costly and complex, especially when addressing both aspects simultaneously. This highlights the need for an automated framework to efficiently map CNNs onto accelerators. Our key insight is that existing approaches suffer from inadequate accelerator performance modeling and a lack of multi-objective optimization strategies that jointly consider task partitioning and convolution parallelization. To address this, we propose GenCNN, a multi-objective genetic algorithm-based mapping framework for CNN accelerators. GenCNN first constructs a fine-grained performance model that captures both off-chip data access and on-chip data processing. It then applies the Non-dominated Sorting Genetic Algorithm II improved by Multi-Objective Bayesian Optimization to derive a Pareto-optimal partitioning and parallelization strategy that balances off-chip latency and PE utilization. Finally, GenCNN optimizes scheduling and routing to minimize data transfers. Experimental results show that GenCNN achieves up to 17.66 × speedup in compilation and 6.47 × in execution compared to state-of-the-art mapping frameworks.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412593931",
    "type": "article"
  },
  {
    "title": "RACER: Avoiding End-to-End Slowdowns in Accelerated Chip Multi-Processors",
    "doi": "https://doi.org/10.1145/3750448",
    "publication_date": "2025-07-24",
    "publication_year": 2025,
    "authors": "Neel Patel; Ren Wang; Mohammad Alian",
    "corresponding_authors": "",
    "abstract": "Recent chip multiprocessors incorporate several on-chip accelerators, marking the beginning of the Accelerated Chip Multi-Processor (XMP) era in datacenters. Despite the close proximity of accelerators and general-purpose cores, offloading functions to accelerators may not always be beneficial. Offloading to hardware accelerators can introduce several end-to-end overheads that can negate the speedup of the accelerable function. In this paper, we design RACER, a hardware architecture and runtime system that evades the danger of end-to-end slowdowns when using hardware acceleration. RACER leverages a low-overhead interface between general-purpose cores and on-chip accelerators, fine-grained context switching, accelerator-initiated preemption, and seamless data motion between general-purpose cores and accelerators to improve the performance of workloads that use on-chip accelerators. We evaluate RACER on five representative request processing workloads featuring diverse memory access patterns, accelerable functions, and compute intensities. RACER improves the performance of hardware acceleration on a real XMP by an average of 1.31 × on a range of diverse workloads and guarantees that accelerator offloads never cause slowdowns.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412636366",
    "type": "article"
  },
  {
    "title": "A Sparsity-Aware Autonomous Path Planning Accelerator with HW/SW Co-Design and Multi-Level Dataflow Optimization",
    "doi": "https://doi.org/10.1145/3750449",
    "publication_date": "2025-07-24",
    "publication_year": 2025,
    "authors": "Yifan Zhang; Xiaoyu Niu; Hongzheng Tian; Yanjun Zhang; Yu Bo; Shaoshan Liu; Sitao Huang",
    "corresponding_authors": "",
    "abstract": "Path planning is a critical task for autonomous driving, aiming to generate smooth, collision-free, and feasible paths based on input perception and localization information. The planning task is both highly time-sensitive and computationally intensive, posing significant challenges to resource-constrained autonomous driving hardware. In this paper, we propose an end-to-end framework for accelerating path planning on FPGA platforms. This framework focuses on accelerating quadratic programming (QP) solving, which is the core of optimization-based path planning and has the most computationally-intensive workloads. Our method leverages a hardware-friendly alternating direction method of multipliers (ADMM) to solve QP problems while employing a highly parallelizable preconditioned conjugate gradient (PCG) method for solving the associated linear systems. We analyze the sparse patterns of matrix operations in QP and design customized storage schemes along with efficient sparse matrix multiplication and sparse matrix-vector multiplication units. Our customized design significantly reduces resource consumption for data storage and computation while dramatically speeding up matrix operations. Additionally, we propose a multi-level dataflow optimization strategy. Within individual operators, we achieve acceleration through parallelization and pipelining. For different operators in an algorithm, we analyze inter-operator data dependencies to enable fine-grained pipelining. At the system level, we map different steps of the planning process to the CPU and FPGA and pipeline these steps to enhance end-to-end throughput. We implement and validate our design on the AMD ZCU102 platform. Our implementation achieves state-of-the-art performance in both latency and energy efficiency compared to existing works, including an average 1.48 × speedup over the best FPGA-based design, a 2.89 × speedup compared to the state-of-the-art QP solver on an Intel i7-11800H CPU, a 5.62 × speedup over an ARM Cortex-A57 embedded CPU, and a 1.56 × speedup over state-of-the-art GPU-based work. Furthermore, our design delivers a 2.05 × improvement in throughput compared to the state-of-the-art FPGA-based design.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412636583",
    "type": "article"
  },
  {
    "title": "SpMARD: A Sparse-Sparse Matrix Multiplication Accelerator with Reconfigurable Dataflow for DNN Workloads",
    "doi": "https://doi.org/10.1145/3747847",
    "publication_date": "2025-08-04",
    "publication_year": 2025,
    "authors": "Bo Wang; Sheng Ma; Yunping Zhao; Shengbai Luo; Lizhou Wu; Jianmin Zhang; Dongsheng Li; Tiejun Li; Zhuojun Chen",
    "corresponding_authors": "",
    "abstract": "Deep learning becomes increasingly popular, and its main workload is Sparse-Sparse Matrix Multiplication (SpMSpM). Most SpMSpM accelerators usually only support a single dataflow. Different dataflows have different performance in different computing environments. Therefore, the single-dataflow accelerator cannot maintain the highest performance in all environments. Compared with single-dataflow accelerators, multi-dataflow accelerators provide flexible options for different workloads and improve the overall performance. Flexagon, Sparm, and SPADA are state-of-the-art multi-dataflow accelerators. However, the computation process of Flexagon and Sparm is not fully pipelined, and SPADA cannot support inner product dataflow. Additionally, Flexagon, Sparm, and SPADA cannot switch dataflows quickly and accurately. Inspired by these observations, we present SpMARD, a SpMSpM accelerator with reconfigurable dataflow. The computation process of SpMARD is fully pipelined, and SpMARD can support six dataflow variants simultaneously. Through the design of a Two-stage Pipeline Adder Network (TPAN) and a Position-based Psum Array (PPA), SpMARD can execute element-level merging, which can hide the merging overhead. Through the quantitative analysis of dataflows, we implement a Dataflow Switcher (DSwitcher), which can switch dataflows more efficiently. For the SpMSpM workload, the performance (GOPS) of the SpMARD we proposed is 1.27 times that of Flexagon, 1.18 times that of Sparm, and 1.22 times that of SPADA.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412891128",
    "type": "article"
  },
  {
    "title": "Matrix: Multi-Cipher Structures Dataflow for Parallel and Pipelined TFHE Accelerator",
    "doi": "https://doi.org/10.1145/3750446",
    "publication_date": "2025-07-29",
    "publication_year": 2025,
    "authors": "Ling Liang; Zhen Gu; Fahong Zhang; Zhaohui Chen; Zhirui Li; Xin Fan; Dimin Niu; Meng Li; Zhiyong Li; Zongwei Wang; Hongzhong Zheng; Yimao Cai; Yuan Xie",
    "corresponding_authors": "",
    "abstract": "Fully homomorphic encryption over torus (TFHE) enables the execution of arbitrary functions on encrypted data through programmable bootstrapping (PBS). However, performing all operations on ciphertext during PBS results in high computational and memory requirements, limiting the deployment of PBS in real-world scenarios. Previous TFHE accelerator designs have attempted to improve performance by employing specific dataflow and functional units, but these techniques may require large off-chip bandwidth or on-chip storage when scaling up computation capacity. Additionally, the design of specialized functional units may limit the utilization of computation units when facing dynamic secure parameter settings. To address these challenges and further improve PBS throughput in TFHE, we propose Matrix , an ASIC-based architecture that balances off-chip bandwidth and on-chip storage according to the execution flow of PBS. In Matrix , we utilize a unified special-prime-based processing element (PE) that achieves high utilization with minimal resource overhead. Furthermore, we propose a hybrid PBS dataflow that can efficiently reduce computation complexity and memory requirements. Compared to state-of-the-art TFHE accelerators, Matrix achieves 1.43 × -5.66 × throughput improvement for PBS. For ZAMA Deep-NN benchmark, we achieve 525.60 × and 68.06 × speedup compared to CPU and GPU, respectively.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412900079",
    "type": "article"
  },
  {
    "title": "Capability-Based Efficient Data Transmission Mechanism for Serverless Computing",
    "doi": "https://doi.org/10.1145/3730583",
    "publication_date": "2025-07-17",
    "publication_year": 2025,
    "authors": "Chongmin Wu; Xingguo Jia; Boshi Yu; Xiaoran Wang; Yun Wang; Kaicheng Guo; Zhengwei Qi; Haibing Guan",
    "corresponding_authors": "",
    "abstract": "Serverless computing has gained widespread popularity due to its rapid deployment, elastic scaling, and pay-as-you-go advantages. Large-scale pipeline applications designed for big data processing, video analytics, and machine learning are gradually evolving toward the serverless architecture. However, the inefficiency of large data transmission among functions has limited the further adoption of the serverless paradigm in these applications. With the emergence of capability Instruction Set Architecture (ISA), the multi-tenant security isolation model and data transmission methods in the cloud have ushered in new opportunities. This paper proposes Hummer, an efficient communication and data transmission scheme based on the capability ISA. The capability ISA allows multiple functions to run in separate isolation domains in a single address space. Function instances can efficiently share memory by directly passing hardware capabilities. This paper extends the CHERI capability ISA, introducing the novel shared capability and shared capability store tracking (SCST) mechanism to manage the shared memory access rights, enabling a zero-copy communication mechanism. We compare Hummer with existing communication mechanisms and show that it improves data transmission rate by 6 – 40 ×. Redis implemented with Hummer APIs doubles its throughput.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413079923",
    "type": "article"
  },
  {
    "title": "Address/Data Instruction Steering in Clustered General Purpose Processors",
    "doi": "https://doi.org/10.1145/3744908",
    "publication_date": "2025-08-11",
    "publication_year": 2025,
    "authors": "Chandana S. Deshpande; Arthur Pérais; Frédéric Pétrot",
    "corresponding_authors": "",
    "abstract": "Although they differentiate between integer and floating-point datum, modern Instruction Set Architectures and their implementations do not differentiate integer datum used to address memory from integer datum used in purely arithmetic and logical computations. This is a perfectly reasonable choice as addresses are, in fact, integral quantities. However, in many cases, there is already a fundamental difference between addresses and integer data: Their width. As computer systems moved from 16 to 32, then to 64-bit pointers, with a potential future where 128-bit might be used for specific systems, the data width required to compute a given output with a given algorithm has remained the same, e.g., an ASCII character is still represented on a byte. This work aims to leverage this dichotomy to revisit hardware clustering, a well-known microarchitectural technique used to mitigate the cost of scaling processor backend structures by dividing the backend into several mostly independent execution clusters. We show that by treating instructions as manipulating addresses or data and steering them to a “data” or an “address” cluster accordingly, reasonable cluster load balancing can be achieved without the need for complex steering policies that can lead to performance on par with the baseline with limited hardware overhead. Moreover, we highlight two possible optimizations stemming from this distribution. First, the registers of the “address” cluster can easily be compressed thanks to address spatial and temporal locality. Second, if a processor requires a large address space but only processes narrow data (e.g., 32-bit data with 64-bit pointers or 64-bit data with 128-bit pointers), the “data” cluster datapath can be kept narrower than the “address” cluster datapath.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413221908",
    "type": "article"
  },
  {
    "title": "Ecmas+: Efficient Circuit Mapping and Scheduling for Surface Code Encoded Circuit on Quantum Cloud Platform",
    "doi": "https://doi.org/10.1145/3760783",
    "publication_date": "2025-08-20",
    "publication_year": 2025,
    "authors": "Mingzheng Zhu; Hao Fu; H. Y. Song; Jun Wu; Chi Zhang; Wei Xie; Xiang‐Yang Li",
    "corresponding_authors": "",
    "abstract": "As the leading candidate for quantum error correction, the surface code faces substantial overhead, such as redundant physical qubits and prolonged execution time. Reducing the space-time cost of circuit execution can significantly improve the throughput of modern quantum cloud platforms. While utilizing more physical qubits can reduce execution time, different quantum circuits vary in their ability to leverage chip resources. Therefore, optimizing the compilation of surface code circuits on quantum chips becomes critical. In this work, we address the mapping and scheduling problem in compiling surface code to reduce the cost. First, we introduce a novel metric Circuit Parallelism Degree to characterize circuit properties in detail and select the most suitable chip from a list of available options. Next, we will quantitatively assess the resources to determine if they are sufficient for the circuit. We then propose a resource-adaptive mapping and scheduling method called Ecmas+ , which customizes the initialization of chip resources for each circuit. Ecmas+ significantly reduces execution time in the double defect and lattice surgery models. Extensive numerical tests on practical datasets demonstrate that Ecmas+ outperforms state-of-the-art methods, reducing execution time by an average of 46% for the double defect model and 29.7% for the lattice surgery model.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413338350",
    "type": "article"
  },
  {
    "title": "Silk: Runtime-Guided Memory Management for Reducing Application Running Janks on Mobile Devices",
    "doi": "https://doi.org/10.1145/3760785",
    "publication_date": "2025-08-16",
    "publication_year": 2025,
    "authors": "Ying Yuan; Zhipeng Tan; Dan Feng; Songquan Wei; Jie Gan; Yang Xiao; Wenjie Qi; Jing Zhang",
    "corresponding_authors": "",
    "abstract": "As an economical method to expand mobile devices’ memory, swap is expected to enhance application performance. However, this paper found two limitations of the current kernel memory management on mobile devices running applications developed in high-level languages. Firstly, application threads access data based on small objects in the Android Runtime (ART) heap. Experimental results reveal that a single page contains multiple objects with varying hotness. The existing page-based kernel memory management fails to accurately recognize the objects’ hotness within a page, and mistakenly prioritizes the reclamation of app-hot objects, which increases the swap in of app-hot objects and causes application running janks. Secondly, ART employs garbage collection (GC) to reclaim invalid objects, yet the kernel’s LRU-based memory management lacks insight into objects’ hotness within the GC working set. This results in the delayed reclamation of GC-cold objects and frequent swapping of GC-hot objects, leading to memory thrashing and substantial degradation in application running performance. To mitigate these issues, we propose Silk, a runtime-guided memory management schema. Silk includes two components: 1) for the application thread, it identifies object hotness in the application working set and groups objects with similar hotness in the same page in ART, caching app-hot objects in memory to reduce swap in; 2)for the GC thread, it identifies object hotness in the GC working set and guides the kernel to prioritize reclaiming GC-cold objects, thus preventing memory thrashing. We implement Silk on Android mobile devices and evaluate it with different categories of popular applications. Experimental results demonstrate that Silk reduces swap in size by 45.4% and reduces application running janks by 55.3%, compared to state-of-the-art work. Additionally, Silk achieves comparable application switch acceleration to the state-of-the-art.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413377350",
    "type": "article"
  },
  {
    "title": "Augur: Semantics-Aware Temporal Prefetching for Linked Data Structure",
    "doi": "https://doi.org/10.1145/3762997",
    "publication_date": "2025-08-22",
    "publication_year": 2025,
    "authors": "Feng Xue; Junliang Wu; Chenji Han; Xinyu Li; Tingting Zhang; Tianyi Liu; Fuxin Zhang",
    "corresponding_authors": "",
    "abstract": "Linked data structures (LDS), such as lists and trees, are widely used in modern applications. Traversing LDS typically involves a significant amount of pointer chasing. Due to the serial nature of memory access in pointer chasing, the incurred long memory latency of traversing LDS has become a critical performance bottleneck. Furthermore, the poor spatial locality in LDS makes it difficult for spatial prefetchers to predict access addresses. Although temporal prefetchers can handle irregular memory access patterns, hindered by the challenges of collecting semantic information, current state-of-the-art temporal prefetchers suffer from significant metadata redundancy and frequent metadata conflicts. Consequently, there remain substantial opportunities to enhance the LDS prefetching. To solve this problem, we propose Augur, a semantics-aware temporal prefetcher to enhance LDS performance. Augur utilizes a novel pruning method to obtain semantic information and effectively extracts node address correlations from the perspective of nodes in LDS, thereby diminishing the metadata redundancy and conflicts. Additionally, Augur employs efficient metadata management strategies that guarantee a minimal storage overhead. Evaluated on LDS workloads, Augur achieves an average performance speedup of 17.8% and 11.7% over the baseline stride prefetcher and state-of-the-art spatial prefetcher Berti, respectively. Furthermore, Augur outperforms the state-of-the-art temporal prefetcher MISB, Triage, and Triangel, by 17.4%, 12.8%, and 6.3%, respectively, with a significantly lower storage overhead of only 1.26 KB.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413429948",
    "type": "article"
  },
  {
    "title": "Swift: High Parallelism Program Generation of Tensor Operators for Accelerating Deep Learning Inference",
    "doi": "https://doi.org/10.1145/3762660",
    "publication_date": "2025-08-25",
    "publication_year": 2025,
    "authors": "Xiang-Ling Yu; Jun Bi; Yuanbo Wen; Jianxing Xu; Di Huang; Jia‐Ming Guo; Wei Li; Zidong Du; Jing Li; Tianshi Chen; Qi Guo",
    "corresponding_authors": "",
    "abstract": "Optimizing deep learning inference, particularly reducing the execution latency of tensor computations at small batch sizes, is crucial for the successful and widespread adoption of deep neural network (DNN) models. However, current deep learning compilers and hand-tuned libraries often fail to achieve high hardware efficiency when executing small-batch workloads. The primary reason is the inherently sequential nature of reductions (e.g., along the hidden dimension in the flattened GEMM for LLM decoding), which is difficult to parallelize and therefore fails to fully utilize available hardware resources. In this paper, we propose Swift, a novel search-based approach for efficiently generating high-performance programs for GPUs by maximizing hardware utilization. The key insight is that reduction parallelization can be incorporated into a unified representation alongside the existing tile structure, significantly expanding the search space for high-performance programs. Concretely, by enumerating all possible parallel mappings of loops, we first generate a large search space that contains high-performance programs. Then, to efficiently explore the extended search space, we employ subspace shifting exploration to identify promising regions, effectively prune large portions of the less‑promising search space. We conduct experiments on three distinct GPU architectures using a diverse set of benchmarks representative of typical application scenarios. Experimental results demonstrate that Swift achieves an average speedup of 1.19 × over the state-of-the-art compiler-based approaches. Moreover, compared to vendor-provided hand-tuned libraries, Swift achieves an average speedup of 2.40 ×.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413520990",
    "type": "article"
  },
  {
    "title": "Optimizing OpenCL Barrier Synchronization and Memory Efficiency on Multi-Core DSPs",
    "doi": "https://doi.org/10.1145/3762661",
    "publication_date": "2025-08-25",
    "publication_year": 2025,
    "authors": "Wei Gao; Jianbin Fang; Peng Zhang; Chun Huang; Ting Wang; Jie Ren",
    "corresponding_authors": "",
    "abstract": "Heterogeneous platforms combining CPUs and DSPs offer the potential for energy-efficient computing but are difficult to program. OpenCL enables code portability across processors but does not guarantee performance portability on DSPs due to the unique challenges in barrier synchronization and local memory management. This paper introduces OctoCL , a compiler-based framework that addresses these challenges through two key optimizations: (1) a recomputation-based variable expansion technique to reduce barrier synchronization overhead, and (2) automatic merging of memory access patterns into DSP-DMA operations for efficient data transfers. We evaluate OctoCL using 18 OpenCL benchmarks from the Parboil and Rodinia suites. Experimental results show that OctoCL achieves an average speedup of 1.46 × (up to 3.36 ×) over the baseline OpenCL compiler on the FT-M7032 CPU-DSP platform. These optimizations enhance performance portability while preserving backward compatibility, requiring no manual code modifications.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413602069",
    "type": "article"
  },
  {
    "title": "NeutronAscend: Optimizing GNN Training with Ascend AI Processors",
    "doi": "https://doi.org/10.1145/3762662",
    "publication_date": "2025-08-26",
    "publication_year": 2025,
    "authors": "Xin Ai; Bing Zhang; Qiange Wang; Yanfeng Zhang; Hao Yuan; Shufeng Gong; Ge Yu",
    "corresponding_authors": "",
    "abstract": "Graph Neural Networks (GNNs) have achieved remarkable success in graph data analysis. However, its application requires extensive computing resources. Recently, a new generation of Neural Processing Units (NPUs), such as the Ascend AI processor, has emerged as an essential component of computing power in data centers and technology companies due to its outstanding performance and low threshold for acquisition. Deploying GNN models on these accelerators for performance optimization is a natural choice; however, it remains a challenging task. The Ascend AI processor is typically architected on multiple AI Cores that are physically decoupled and designed for dense matrix computation. When processing graph data with inherent sparsity and power-law distribution, the Ascend AI processors suffer from the inter-core workload imbalance and inefficient intra-core resource utilization. In this paper, we present NeutronAscend, an efficient GNN training framework tailored for the Ascend AI processor. NeutronAscend employs two critical designs for both inter-core and intra-core performance optimization. At the inter-core level, we employ GNN tensor-data hybrid parallelism that ensures load balance by partitioning vertex features evenly across AI Cores and partitioning graph data across small dimensions to maximize computing resource utilization. At the intra-core level, computation-aware task scheduling aligns the GNN process with computational units, leveraging locality-aware graph compression to reduce irregular memory access and redundant computation, while utilizing inter-unit pipelining to further enhance overall performance. Experimental results on the Ascend 910B processor demonstrate that NeutronAscend achieves an average 4.71 × speedup compared to the current publicly available baselines on Ascend NPUs. While our work is tailored to Ascend, its design principles are broadly applicable to other NPUs that adopt physically decoupled, matrix-centric compute architectures.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413645169",
    "type": "article"
  },
  {
    "title": "ESMPC: An Efficient Neural Network Training Framework for Secure Two- and Three-Party Computation",
    "doi": "https://doi.org/10.1145/3762663",
    "publication_date": "2025-08-26",
    "publication_year": 2025,
    "authors": "Hongwei Yang; J. Q. Li; Meng Hao; Weizhe Zhang; Hui He; Jinghao Zhao; Ling Yang; Zhixiang Qin",
    "corresponding_authors": "",
    "abstract": "In the era of big data, privacy-preserving deep neural network (DNN) training has emerged as a critical research area. Secure Multi-Party Computation (SMPC) has become a key technique for enabling collaborative model training while safeguarding data confidentiality. However, the high communication overhead inherent in SMPC protocols poses a substantial obstacle to their practical deployment, particularly in large-scale deep learning applications. To address this challenge, we propose ESMPC, a computation–communication co-optimization framework designed to enhance communication efficiency in SMPC-based DNN training. Within ESMPC, we propose SecurePipe, a novel pipeline-parallelism strategy tailored for SMPC-based DNN training. SecurePipe effectively improves both model and data parallelism, enabling parallel computation and communication under encryption, thereby enhancing overall training throughput and significantly improving GPU utilization. To further reduce communication costs, we design three communication optimization algorithms specifically targeting both linear and non-linear operations. Additionally, we propose two protocol-level optimizations that substantially lower communication overhead during secure training. Comprehensive experimental evaluations validate the effectiveness of ESMPC. In two-party computation settings, ESMPC reduces the training time of AlexNet by over 30%, and achieves more than 45% reduction for FCNN, LeNet, and ViT. Similarly, in three-party computation settings, the training time for all four models is reduced by over 45%, demonstrating the scalability and efficiency of the proposed framework in real-world SMPC training environments.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413696696",
    "type": "article"
  },
  {
    "title": "HAVIT: An Efficient Hardware-Accelerator for Vision Transformer with Informative Patch Selection Techniques",
    "doi": "https://doi.org/10.1145/3764865",
    "publication_date": "2025-08-29",
    "publication_year": 2025,
    "authors": "Achal Kumar Goyal; Gaurav Sangwan; Alok Kumar Patel; Palash Das",
    "corresponding_authors": "",
    "abstract": "Vision Transformers (ViTs) are widely utilized in the domain of computer vision. However, they encounter challenges related to high computational expenses, particularly for real-time inference on devices with limited resources. To tackle this challenge, we design HAVIT, an efficient ViT accelerator that integrates lightweight informative patch selection techniques with the core ViT acceleration module. The informative patch selection techniques introduce minimal computation, comprising only about 0.03% on average compared to the savings achieved, as they rely on edge detection methods like Sobel and Canny, in contrast to the computationally intensive AI models. After the edge detection, we use three proposed algorithms—Density-based Patch selector (DSP), Row Column Intersection patch selector (RCI), and Bounded Envelop Patch selector (BEP)—each offering unique paths for selecting informative patches with distinct accuracy-performance trade-offs. Upon receiving the informative patches, the ViT acceleration portion of the proposed HAVIT employs parallelism by identifying independent data to enhance the performance further while being energy efficient. Experimental results show that our policies can significantly reduce computational workload by around 27% − 57% compared to the baseline (no_patch_selection). The proposed HAVIT also outperforms state-of-the-art accelerators in terms of overall system performance.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413835388",
    "type": "article"
  },
  {
    "title": "Efficient GPU-Centered Singular Value Decomposition Using the Divide-and-Conquer Method",
    "doi": "https://doi.org/10.1145/3764932",
    "publication_date": "2025-09-01",
    "publication_year": 2025,
    "authors": "Shifang Liu; Huiyuan Li; H. Y. Sheng; Hong-yu Gui; Xiaoyu Zhang",
    "corresponding_authors": "",
    "abstract": "Singular Value Decomposition (SVD) is a fundamental matrix factorization technique in linear algebra, widely applied in numerous matrix-related problems. However, traditional SVD approaches are hindered by slow panel factorization and frequent CPU-GPU data transfers in heterogeneous systems, despite advancements in GPU computational capabilities. In this paper, we introduce a GPU-centered SVD algorithm, incorporating a novel GPU-based bidiagonal divide-and-conquer (BDC) method. We reformulate the algorithm and data layout of different steps for SVD computation, performing all panel-level computations and trailing matrix updates entirely on GPU to eliminate CPU-GPU data transfers. Furthermore, we integrate related computations to optimize BLAS utilization, thereby increasing arithmetic intensity and fully leveraging the computational capabilities of GPUs. Additionally, we introduce a newly developed GPU-based BDC algorithm that restructures the workflow to eliminate matrix-level CPU-GPU data transfers and enable asynchronous execution between the CPU and GPU. Experimental results on AMD MI210 and NVIDIA V100 GPUs demonstrate that our proposed method achieves speedups of up to 1293.64x/7.47x and 14.10x/12.38x compared to rocSOLVER/cuSOLVER and MAGMA, respectively.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413876982",
    "type": "article"
  },
  {
    "title": "The Design of an Efficient Lossy Compressor for Time Series Databases",
    "doi": "https://doi.org/10.1145/3767158",
    "publication_date": "2025-09-16",
    "publication_year": 2025,
    "authors": "Xiangyu Zou; S. S. Wang; Shi Yang; X Chen; Sian Jin; Dingwen Tao; Wen Xia",
    "corresponding_authors": "",
    "abstract": "Time-series databases (denoted as TSDB), which are designed for handling rapidly growing time-series data, usually apply compression techniques to reduce storage overhead. However, existing compressors are limited in key metrics for TSDB compression, such as compression ratio or decompression speed, primarily due to a mismatch between their design and the features specific to TSDB. To this end, we propose a lossy compressor Machete. It achieves a much higher compression ratio and fast decompression speed, while promising a user-specific and point-wise error bound to preserve the analytical value of the data. First, Machete proposes a pattern-based predictor and an efficient hybrid encoder to monitor data trends, which successfully achieve higher compression rates through better understanding of the data. Second, Machete proposes a SIMD-based decompression acceleration technique, which exploits the repeatedly intermediate calculations in decompression and shares them in decompression iterations through parallelism. Our evaluation on four real-world datasets shows that Machete outperforms state-of-the-art compressors by \\(69\\% \\) – \\(114\\% \\) on compression ratio and achieves the fastest decompression speed on two datasets. When applied to a well-known time series database InfluxDB, Machete saves disk usage \\(40\\% \\) – \\(72\\% \\) and improves the query performance of the InfluxDB database by saving I/O.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414244119",
    "type": "article"
  },
  {
    "title": "Accelerating Verifiable Queries over Blockchain Database System Using Processing-in-memory",
    "doi": "https://doi.org/10.1145/3768318",
    "publication_date": "2025-09-16",
    "publication_year": 2025,
    "authors": "Yifan Hua; Shengan Zheng; Weihan Kong; Dongliang Xue; 克巳 小西; Yuheng Wen; Linpeng Huang; Hong Mei",
    "corresponding_authors": "",
    "abstract": "Blockchain database systems, such as Ethereum and vChain, suffer from limited memory bandwidth and high memory access latency when retrieving user-requested data. Emerging processing-in-memory (PIM) technologies are promising to accelerate users’ queries, by enabling low-latency memory access and aggregated memory bandwidth scaling with the number of PIM modules. In this paper, we present Panther, the first PIM-based blockchain database system supporting efficient verifiable queries. Blocks are distributed to PIM modules for high parallelism with low inter-PIM communication cost, managed by a regression-based model. For load balance across PIM modules, data are adaptively promoted and demoted between the host and PIM sides. In multiple datasets, Panther achieves up to 23.6× speedup for verifiable queries and reduces metadata storage by orders of magnitude compared to state-of-the-art designs on real PIM hardware.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414244148",
    "type": "article"
  },
  {
    "title": "RBC: A Randomness-Resistant Block-Grained Compaction Strategy for ZNS SSDs",
    "doi": "https://doi.org/10.1145/3764588",
    "publication_date": "2025-09-16",
    "publication_year": 2025,
    "authors": "Tianqi Zhan; Dan Feng; Shu Li; Zhengyong Wang; Wei Tong",
    "corresponding_authors": "",
    "abstract": "Research on LSM-tree optimization for ZNS-based SSDs has garnered significant attention. Existing studies primarily focus on reducing garbage collection overhead by ensuring consistent data block lifetimes within a zone. While many works address cascading write amplification between LSM-trees and ZNS SSDs, few evaluate the effectiveness of existing WA reduction strategies on ZNS. Block compaction, a commonly adopted optimization approach, reduces write amplification by shrinking the compaction granularity to the data block level and remapping reusable data blocks. However, Block compaction causes fragmentation in ZNS SSDs, increasing data migration and write amplification during zone reclamation. Moreover, fragmentation reduces the prefetch cache hit rate of ZNS file systems, thereby increasing data block read latency and negatively impacting user read operations and range lookup performance. Additionally, block compaction with a low proportion of reusable data blocks generates more random read operations, leading to higher compaction latency and blocking user read/write operations. To address these issues, we propose the Randomness-Resistant Block-grained Compaction (RBC) strategy. RBC uses a remapping table to track the location of fragmented data blocks, allowing for more efficient prefetch cache utilization. Furthermore, RBC reduces the data migration overhead caused by fragmentation by delaying the reclamation of highly fragmented zones and migrating remapped data blocks during idle periods. RBC introduces a dynamic compaction strategy that applies block-level compaction for high dirty block ratios and switches to file-level compaction for low dirty block ratios. Additionally, it implements a low-CPU-overhead dirty block detection strategy. Experimental results show that RBC improves throughput by 69% and 37% compared to traditional RocksDB and other block-grained compaction methods, respectively. RBC also improves range query performance by 1.75x compared to other block-grained compaction techniques.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414244187",
    "type": "article"
  },
  {
    "title": "PDGNN: Efficient Micro-batch GNN Training via Degree-Pruned Partitioning and Redundancy Elimination",
    "doi": "https://doi.org/10.1145/3767325",
    "publication_date": "2025-09-13",
    "publication_year": 2025,
    "authors": "Lizhi Zhang; Menghan Jia; Ping Gong; Zhiquan Lai; Dongsheng Li; Yongquan Fu; Ao Shen; Kai Lü",
    "corresponding_authors": "",
    "abstract": "Graph Neural Networks (GNNs) have become essential for graph-based applications. Existing GNN training frameworks primarily adopt sampling-based mini-batch approach, which samples vertex neighborhoods to construct subgraphs for training iteratively, but faces a memory-accuracy trade-off. To address this, naive micro-batch training with gradients accumulation can be applied, yet introduces severe redundancy from repeated neighbor sampling. Recent micro-batch training methods overcome this by adopting batch-level graph partitioning strategies that partition full-batch subgraph sampled from all target vertices to generate micro-batches. Nevertheless, these approaches struggle to balance partitioning overhead with redundancy. This paper proposes PDGNN, a system designed for efficient micro-batch GNN training on single machine, reducing both training time and memory consumption. First, we propose a degree-pruning-based graph partitioning method that efficiently processes on simplified and sparsified multi-level bipartite graphs by removing high-degree vertices, significantly reducing partitioning overhead. Second, we propose a dependency-aware redundancy matching and elimination technique that maximizes inter-micro-batch redundancy matching, and eliminates redundant computation by embedding caching and reuse while maintaining model accuracy. Experiments on real-world datasets demonstrate that PDGNN substantially improves the efficiency of large-scale GNN training, achieving up to 3.5 × speedup in epoch time, 46% lower peak GPU memory consumption, and 64% reduced peak CPU memory consumption compared to state-of-the-art batch-level training methods.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414261025",
    "type": "article"
  },
  {
    "title": "FlashGEMM: Optimizing Sequences of Matrix Multiplication by Exploiting Data Reuse on CPUs",
    "doi": "https://doi.org/10.1145/3760784",
    "publication_date": "2025-09-13",
    "publication_year": 2025,
    "authors": "Junwen Zhang; Weiling Yang; Jianbin Fang; Dezun Dong; Xianzhang Chen",
    "corresponding_authors": "",
    "abstract": "General Matrix Multiplication (GEMM) is a fundamental operation in high-performance computing (HPC) and deep learning (DL) applications. While mainstream linear algebra libraries on CPUs, such as MKL and OpenBLAS, achieve high performance for individual, large-scale, and regular-shaped GEMM operations, which are commonly used in emerging HPC and DL applications. We present FlashGEMM , a novel and efficient approach for optimizing sequences of GEMM on x86 CPUs. FlashGEMM introduces a new data packing strategy that reduces the memory access overhead associated with packing operations. It also offers new micro-kernels designed to fully utilize the Vector Neural Network Instructions (VNNI) units of x86 CPUs, thereby increasing the compute-to-memory ratio (CMR). Additionally, FlashGEMM includes new loop fusion strategies to reuse intermediate data across consecutive GEMM operations. Experimental results demonstrate that FlashGEMM can outperform state-of-the-art across most GEMM workloads on multi-core CPUs.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414261157",
    "type": "article"
  },
  {
    "title": "Ares: Fair and Efficient Scheduling of Deep Learning Jobs with Elastic Fair Queuing",
    "doi": "https://doi.org/10.1145/3766896",
    "publication_date": "2025-09-18",
    "publication_year": 2025,
    "authors": "Yifei Liu; Chen Chen; Qiang Wang; Yu Feng; Weihao Cui; Quan Chen; Minyi Guo",
    "corresponding_authors": "",
    "abstract": "Schedulers play a vital role for GPU cluster serving model training jobs, and an ideal scheduler shall behave well in both fairness and efficiency. However, existing clusters mostly focus on only one aspect and fall short in the other. To solve that problem, given that the resource demand of a model training job can often be approximated a priori, our insight is to preferentially service jobs that complete earlier under instantaneous fair sharing, which can emulate shortest job first while avoiding starvation. Following that insight, in this paper we propose Ares, an efficient and also fair scheduler for deep learning jobs. Ares leverages the conception of virtual finish time in network fair queuing methods, which supports efficient estimation of job completion order at job arrival time. For the jobs with earlier virtual finish times, we allow it to use more resources than it originally demands to attain fast completion—so that those resources can also be released sooner and no job is actually hurt. We keep the global batch size unchanged to ensure accuracy validity, and also ensure that the degradation of resource utilization caused by scaling-out is bounded. We call such scheduling method elastic fair queuing, which can provide theoretical fairness guarantee. We evaluate Ares performance with both testbed experiments and large-scale simulations. The results show that Ares can reduce the average job completion time by over 20% and also reduce the number of unfairly-served jobs by over 40%.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414343946",
    "type": "article"
  },
  {
    "title": "Efficient Flexible Edge Inference for Mixed-Precision Quantized DNN using Customized RISC-V Core",
    "doi": "https://doi.org/10.1145/3768630",
    "publication_date": "2025-09-19",
    "publication_year": 2025,
    "authors": "Tsung-Che Lu; Jiangyang Ding; Huachen Zhang; Bowen Jiang; Wei Xu; Zhilei Chai",
    "corresponding_authors": "",
    "abstract": "Mixed-Precision Quantization (MPQ) has become a key technique for deploying deep neural networks on resource-constrained IoT edge devices, enabling efficient TinyML applications while maintaining accuracy. However, current MPQ solutions face the following major limitations: insufficient flexibility in supporting diverse neural architectures and lack of energy-efficient hardware acceleration tailored for edge deployment. To address these challenges, we propose an integrated software-hardware co-design framework that optimizes Mixed-Precision Quantized Neural Networks for edge AI inference. Our framework introduces following key innovations: an adaptive reorderd pack scheme to improve data utilization, dynamic data flow schemes for low-bit-width computationally intensive operators, and customized SIMD (Single-Instruction, Multiple-Data) instruction extensions for RISC-V processors optimized for flexible mixed-precision computations. Experimental results demonstrate that our framework achieves 1.23×-1.58× speedup for 2-8 bit operations compared to existing RISC-V implementations while maintaining excellent energy efficiency of 1204 GOPS/W. Our design also keeps the power range to 2.74mW, making it particularly suitable for cost-sensitive edge deployments. These advances provide a practical solution for next-generation IoT systems requiring adaptive precision and ultra-low-power execution on edge devices.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414354696",
    "type": "article"
  },
  {
    "title": "SimHost: A Lightweight End-to-End Simulation Framework for HPC Network Systems",
    "doi": "https://doi.org/10.1145/3767339",
    "publication_date": "2025-09-23",
    "publication_year": 2025,
    "authors": "Hong Zou; Huaxi Gu; Wenting Wei; Tiantian Li; 絵未 恩田",
    "corresponding_authors": "",
    "abstract": "As high-performance computing (HPC) systems continue to grow in scale and complexity, simulation has emerged as a crucial tool for evaluating network performance, especially when real hardware is unavailable. However, achieving highly accurate results quickly and with low overhead in large-scale network evaluations remains a significant challenge. To tackle this issue, we propose SimHost, a lightweight end-to-end offline simulation framework featuring flexible communication and host models tailored for large-scale network simulations. SimHost enables the replay of application communication events without requiring an actual or virtualized execution environment or hardware. It accurately restores the message passing and system processing overheads of real application execution, delivering evaluation results consistent with physical deployments. With its cross-platform and discrete event design, SimHost integrates seamlessly with existing simulators, needing only minimal modifications for synchronous operation. Evaluations and use cases confirm that SimHost’s workloads accurately replicate real-world scenarios. Experimental results indicate that SimHost achieves less than 5.12% error in application completion time compared to the testbed, while maintaining only 7% simulation runtime overhead when scaled to 2048 nodes relative to synthetic workloads.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414448647",
    "type": "article"
  },
  {
    "title": "Second-level Caches: Not for Instructions",
    "doi": "https://doi.org/10.1145/3769080",
    "publication_date": "2025-09-26",
    "publication_year": 2025,
    "authors": "Muhammad Hassan; Chang Hyun Park; David Black-Schaffer",
    "corresponding_authors": "",
    "abstract": "Growing instruction footprints are straining processor front-ends, increasing fetch latency, and causing pipeline stalls. The universal approach to addressing this has been keeping instructions in each level of the cache hierarchy, but a plethora of more advanced techniques, ranging from instruction prefetching to prioritizing instruction cache-lines, have also been proposed to reduce instruction fetch latency. In this work we identify a significant subset of benchmarks that exhibit an insensitivity to instruction fetch latency to the extent that we can take the opposite approach and explicitly bypass the L2 for instructions. While conventional wisdom suggests that this will hurt performance, our detailed analysis of 2120 traces finds that two factors mitigate this effect. First, the presence of ubiquitous effective instruction prefetching in the form of fetch-directed instruction prefetching (FDIP) means that as long as instructions are present in the LLC, the additional latency of missing in the L2 is often insignificant. And, second, by keeping instructions out of the L2, we find that the increase in data locality in the L2 leads to significant performance gains. Based on our analysis, we propose bypassing the L2 for instructions for a subset of applications and demonstrate how a simple decision tree can be trained offline and used online to cheaply and effectively choose when to bypass. Our technique increases performance by 3.4% geomean for sensitive benchmarks (those that see an improvement of at least 1% with an infinite I-cache) compared to an aggressive FDIP baseline, reaching up to 55% of the perfect I-cache performance with minimal to no overhead.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414555951",
    "type": "article"
  },
  {
    "title": "HotLD: a Workload-Aware Method for Global Code-Layout Optimization of Shared Libraries",
    "doi": "https://doi.org/10.1145/3769310",
    "publication_date": "2025-09-30",
    "publication_year": 2025,
    "authors": "Xueqin Ning; Jun Ma; Zhouyang Jia; Yusong Tan; Jie Yu; Pan Dong; J. Wang; Lianghao Shen",
    "corresponding_authors": "",
    "abstract": "Dynamic linking is an important technique in the process of software development. While dynamic linking can save memory and enhance maintainability, it also incurs performance overhead and hinders the application of profile-guided code layout optimization techniques to third-party libraries. Existing works (such as BOLT) can solve the problem by generating optimized versions of shared libraries for a given workload. Online PGO methods (such as OCOLOS) can further support on-the-fly code replacement to adapt to workload changes. These methods, however, are limited in several aspects: (1) hard to perform global optimization, (2) high memory consumption and performance overhead, and 3) limited usage scenarios. To address these issues, we propose HotLD , a workload-aware method for global code-layout optimization of shared libraries. HotLD can improve the performance of shared libraries while introducing limited performance and memory overhead. The core idea behind HotLD is to create dedicated code copies for each typical application workload during the offline phase, and perform global optimization according to workload characteristics. At runtime, HotLD monitors the running workload of the target program, then dynamically selects and links an appropriate code copy. We conducted experiments using real-world applications to evaluate the effectiveness and efficiency of HotLD . The results demonstrate that HotLD can improve their performance (up to 22.37%) with limited performance overhead (at the millisecond level). Compared with existing works, HotLD uses 1%-46% memory to support 2 × -9 × workloads.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414664465",
    "type": "article"
  },
  {
    "title": "Intra-request Lag-aware Cache Management to Enhance I/O Responsiveness of SSDs",
    "doi": "https://doi.org/10.1145/3770752",
    "publication_date": "2025-10-03",
    "publication_year": 2025,
    "authors": "Shi Zhong; Jiaojiao Wu; Xuewei Guo; Fan Yang; Aobo Yang; Qiyu Liu; Zhigang Cai; Jianwei Liao",
    "corresponding_authors": "",
    "abstract": "Solid State Disks (SSDs) provides high degrees of channel-level parallelism, and sub-requests of a large request are forwarded to different flash channels to maximize parallelism and utilization. The scheduler, however, may experience differential wait times since their individual queues of channels are not coordinated and keep balanced workloads at all times. Since a large request is regarded as a complete only when its last sub-request is completed, some of its sub-requests that are completed earlier inevitably wait for this last one that is termed as intra-request lag , thus affecting I/O responsiveness. This paper proposes intra-request lag-aware cache management to maximize channel-level read parallelism, thus improving user experience, as well as balance I/O workloads over all SSD channels. Once a part of sub-request(s) belonging to the same large read requests are blocked on the congested channel(s), indicating intra-request lag(s) appear, we preferentially load the lag-corresponding data, instead of all relevant data of read request, into the data cache. Consequently, it can respond quickly and balance the workload over channels if the read is hit in the near future. Besides, in order to specifically buffer the data of small read requests or the data of intra-request lags of large read request to improve read responsiveness, we present a cache partition model for splitting the data cache into the read cache and the write cache. Trace-driven experiments show that our proposal can reduce the long-tail latency of I/O requests by 26.2 % at the 99.9-th percentile, and the overall I/O time by 35.0 % on average, in contrast to existing approaches.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414780077",
    "type": "article"
  },
  {
    "title": "Towards Efficient Extendible Perfect Hashing for Hybrid PM-DRAM Memory",
    "doi": "https://doi.org/10.1145/3770859",
    "publication_date": "2025-10-04",
    "publication_year": 2025,
    "authors": "Hao Hu; Qi Chen; Yiming Yin; Xiangyu Zou; Ting Yao; Hongpeng Wang; Shiyi Li; Wen Xia",
    "corresponding_authors": "",
    "abstract": "Hashing is a widely used and efficient indexing mechanism for key-value storage. The emergence of persistent memory (PM) has further enhanced hash indexes by providing non-volatility and DRAM-like performance. However, current research on PM-based hash indexes primarily focuses on hardware-specific persistence optimizations or write performance optimization, while neglecting the critical aspect of read performance. Our study reveals that hash collisions significantly impair the read performance of hash indexes on PM. Thus, perfect hashing, which eliminates collisions, has the potential to improve read performance. Nevertheless, due to PM’s high access latency, the data movement overhead during hash table expansion and the random accesses introduced by perfect hashing itself present performance bottlenecks. In this paper, we propose EEPH+, an efficient perfect hashing scheme for PM that effectively eliminates hash collisions to enhance read performance. EEPH+ employs three techniques to alleviate the aforementioned bottlenecks and improve performance: an extendible hashing technique to reduce the data movement overhead during hash table expansion, a hybrid PM–DRAM layout where the index structure resides in DRAM combined with a complement move algorithm to minimize the random accesses inherent to perfect hashing, and a batching &amp; prefetching scheme to facilitate the parallel execution of hash computations and data accesses across multiple searches, thereby further boosting read performance. We compare EEPH+ with the state-of-the-art hash indexes on PM by conducting comprehensive experiments on several real-world read-intensive and read-skew workloads. The experimental results confirm the superiority of our EEPH+, which outperforms state-of-the-art hash indexes by up to 3.6 × in read throughput and 3.14 × in 99th percentile latency.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414810819",
    "type": "article"
  },
  {
    "title": "Object-Aware Memory Compression for Smartphones",
    "doi": "https://doi.org/10.1145/3771285",
    "publication_date": "2025-10-08",
    "publication_year": 2025,
    "authors": "Xinrui Li; Zhenyu Yang; Mingyu Wu; Haibo Chen; Binyu Zang",
    "corresponding_authors": "",
    "abstract": "Memory compression is vitally important in controlling the memory consumption of smartphones. Unfortunately, mainstream compression schemes like zRAM do not work well with garbage collection (GC) in managed language runtimes. When GC is triggered, it traces objects in the heap and thus causes frequent swapping and repetitive memory compression/decompression, which introduces significant performance overhead and user experience degradation. To this end, we propose an object-aware compressed memory scheme named oaRAM , which allows GC algorithms to directly interpret compressed memory and thus eliminates the need for decompression during GC. Furthermore, we build swap-less memory reclamation by further bridging the semantic gap between the memory compression module in OS and the garbage collector in the language runtime. On the OS side, we provide an object-aware memory compression module, which leverages object semantics from language runtimes to compress and manage data in the kernel. On the language runtime side, we propose shadow heap, which allows GC threads to directly access compressed contents in the kernel. We have implemented oaRAM on Android, and the evaluation results show that it significantly reduces the number of swapping operations under memory-hungry scenarios, which results in improvements in application performance, the system’s application caching capability and device lifespan.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414934565",
    "type": "article"
  },
  {
    "title": "An End-to-End Framework for Compiling Dense and Sparse Matrix-Vector Multiplications for FPGA-HBM Acceleration",
    "doi": "https://doi.org/10.1145/3771723",
    "publication_date": "2025-10-13",
    "publication_year": 2025,
    "authors": "Veronia Iskandar; Sergio A. Pertuz; Carlos Eduardo da Silva Santos; Mohamed A. Abd El Ghany; Diana Göhringer",
    "corresponding_authors": "",
    "abstract": "The bandwidth improvement provided by high-bandwidth memory (HBM), and the capability of FPGAs to customize the processing and memory hierarchy, results in a considerable performance increase for memory-intensive workloads such as graph processing, sorting, machine learning, and database analytics. Modern systems integrating 3D-stacked DRAM memory can be leveraged to realize the Near-Memory Computing (NMC) paradigm by offloading some computations to accelerators placed near the HBM. Matrix-vector multiplication (MVM) kernels, which are memory-bound, can significantly benefit from being executed on FPGA-HBM platforms. MVM kernels can be broadly categorized into two types: dense (General Matrix-Vector Multiplication, GEMV) and sparse (Sparse Matrix-Vector Multiplication, SpMV). Recent literature has predominantly focused on optimizing SpMV for FPGA-HBM, leaving a unified solution relatively unexplored. In this work, we introduce an end-to-end framework for compiling MVM kernels for FPGA-HBM Acceleration. It consists of a software and a hardware components. The software component introduces the MATIO compiler, a novel toolflow for detecting MVM and matrix multiplication (MM) kernels in C or C++ code, and replacing MVM kernels with a call to our FPGA accelerator. MATIO is capable of detecting 90% of MVM and MM kernels in real-world benchmarks collected from Github. Additionally, it is faster than state-of-the-art detection methods by at least 45x. On the hardware side, we introduce VecMADS, a novel FPGA architecture designed to efficiently handle both GEMV and SpMV operations. Our architecture leverages the high memory bandwidth of HBM to overcome memory bottlenecks, providing a comprehensive solution for accelerating matrix-vector multiplication on FPGAs. Evaluation results show that VecMADS delivers 1.5x higher throughput and 4.8x higher energy efficiency compared to cuSPARSE library on GPU. Considering dense benchmarks, VecMADS achieves 1.26x higher throughput than the hipBLAS library running on GPU.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415114182",
    "type": "article"
  },
  {
    "title": "Datapath and control for quantum wires",
    "doi": "https://doi.org/10.1145/980152.980155",
    "publication_date": "2004-03-01",
    "publication_year": 2004,
    "authors": "Nemanja Isailovic; Mark Whitney; Yatish Patel; John Kubiatowicz; Dean Copsey; Frederic T. Chong; Isaac L. Chuang; Mark Oskin",
    "corresponding_authors": "",
    "abstract": "As quantum computing moves closer to reality the need for basic architectural studies becomes more pressing. Quantum wires, which transport quantum data, will be a fundamental component in all anticipated silicon quantum architectures. Since they cannot consist of a stream of electrons, as in the classical case, quantum wires must fundamentally be designed differently. In this paper, we present two quantum wire designs: a swap wire, based on swapping of adjacent qubits, and a teleportation wire, based on the quantum teleportation primitive. We characterize the latency and bandwidth of these two alternatives in a device-independent way. Furthermore, unlike classical wires, quantum wires need control signals in order to operate. We explore the complexity of the control mechanisms and the fundamental tension between the scale of quantum effects and the scale of the classical logic needed to control them. This \"pitch-matching\" problem imposes constraints on minimum wire lengths and wire intersections, leading us to use a SIMD approach for the control mechanisms. We ultimately show that qubit decoherence imposes a basic limit on the maximum communication distance of the swapping wire, while relatively large overhead imposes a basic limit on the minimum communication distance of the teleportation wire.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W1998561227",
    "type": "article"
  },
  {
    "title": "Tolerating memory latency through push prefetching for pointer-intensive applications",
    "doi": "https://doi.org/10.1145/1044823.1044827",
    "publication_date": "2004-12-01",
    "publication_year": 2004,
    "authors": "Chia-Lin Yang; Alvin R. Lebeck; Hung‐Wei Tseng; Chien-Hao Lee",
    "corresponding_authors": "",
    "abstract": "Prefetching is often used to overlap memory latency with computation for array-based applications. However, prefetching for pointer-intensive applications remains a challenge because of the irregular memory access pattern and pointer-chasing problem. In this paper, we proposed a cooperative hardware/software prefetching framework, the push architecture, which is designed specifically for linked data structures. The push architecture exploits program structure for future address generation instead of relying on past address history. It identifies the load instructions that traverse a LDS and uses a prefetch engine to execute them ahead of the CPU execution. This allows the prefetch engine to successfully generate future addresses. To overcome the serial nature of LDS address generation, the push architecture employs a novel data movement model. It attaches the prefetch engine to each level of the memory hierarchy and pushes , rather than pulls , data to the CPU. This push model decouples the pointer dereference from the transfer of the current node up to the processor. Thus a series of pointer dereferences becomes a pipelined process rather than a serial process. Simulation results show that the push architecture can reduce up to 100% of memory stall time on a suite of pointer-intensive applications, reducing overall execution time by an average 15%.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2118818247",
    "type": "article"
  },
  {
    "title": "Efficient remote profiling for resource-constrained devices",
    "doi": "https://doi.org/10.1145/1132462.1132465",
    "publication_date": "2006-03-01",
    "publication_year": 2006,
    "authors": "Priya Nagpurkar; Hussam Mousa; Chandra Krintz; Timothy Sherwood",
    "corresponding_authors": "",
    "abstract": "The widespread use of ubiquitous, mobile, and continuously connected computing agents has inspired software developers to change the way they test, debug, and optimize software. Users now play an active role in the software evolution cycle by dynamically providing valuable feedback about the execution of a program to developers. Software developers can use this information to isolate bugs in, maintain, and improve the performance of a wide-range of diverse and complex embedded device applications. The collection of such feedback poses a major challenge to systems researchers since it must be performed without degrading a user's experience with, or consuming the severely restricted resources of the mobile device. At the same time, the resource constraints of embedded devices prohibit the use of extant software profiling solutions. To achieve efficient remote profiling of embedded devices, we couple two efficient hardware/software program monitoring techniques: Hybrid Profiling Support(HPS) and Phase-Aware Sampling. HPS efficiently inserts profiling instructions into an executing program using a novel extension to Dynamic-Instruction Stream Editing(DISE). Phase-aware sampling exploits the recurring behavior of programs to identify key opportunities during execution in order to collect profile information (i.e. sample). Our prior work on phase-aware sampling required code duplication to toggle sampling. By guiding low-overhead, hardware-supported sampling according to program phase behavior via HPS, our system is able to collect highly accurate profiles transparently. We evaluate our system assuming a general purpose configuration as well as a popular handheld device configuration. We measure the accuracy and overhead of our techniques and quantify the overhead in terms of computation, communication, and power consumption. We compare our system to random and periodic sampling for a number of widely used performance profile types. Our results indicate that our system significantly reduces the overhead of sampling while maintaining high accuracy.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2022668546",
    "type": "article"
  },
  {
    "title": "Future execution",
    "doi": "https://doi.org/10.1145/1187976.1187979",
    "publication_date": "2006-12-01",
    "publication_year": 2006,
    "authors": "Ilya Ganusov; Martin Burtscher",
    "corresponding_authors": "",
    "abstract": "This paper describes future execution (FE), a simple hardware-only technique to accelerate individual program threads running on multicore microprocessors. Our approach uses available idle cores to prefetch important data for the threads executing on the active cores. FE is based on the observation that many cache misses are caused by loads that execute repeatedly and whose address-generating program slices do not change (much) between consecutive executions. To exploit this property, FE dynamically creates a prefetching thread for each active core by simply sending a copy of all committed, register-writing instructions to an otherwise idle core. The key innovation is that on the way to the second core, a value predictor replaces each predictable instruction in the prefetching thread with a load immediate instruction, where the immediate is the predicted result that the instruction is likely to produce during its n th next dynamic execution. Executing this modified instruction stream (i.e., the prefetching thread) on another core allows to compute the future results of the instructions that are not directly predictable, issue prefetches into the shared memory hierarchy, and thus reduce the primary threads' memory access time. We demonstrate the viability and effectiveness of future execution by performing cycle-accurate simulations of a two-way CMP running the single-threaded SPECcpu2000 benchmark suite. Our mechanism improves program performance by 12%, on average, over a baseline that already includes an optimized hardware stream prefetcher. We further show that FE is complementary to runahead execution and that the combination of these two techniques raises the average speedup to 20% above the performance of the baseline processor with the aggressive stream prefetcher.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2133528302",
    "type": "article"
  },
  {
    "title": "Unified control flow and data dependence traces",
    "doi": "https://doi.org/10.1145/1275937.1275943",
    "publication_date": "2007-09-01",
    "publication_year": 2007,
    "authors": "Sriraman Tallam; Rajiv Gupta",
    "corresponding_authors": "",
    "abstract": "We describe the design, generation, and compression of the extended whole program path (eWPP), representation that not only captures the control flow history of a program execution but also its data dependence history. This representation is motivated by the observation that, typically, a significant fraction of data dependence history can be recovered from the control flow trace. To capture the remainder of the data dependence history, we introduce disambiguation checks in the program whose control flow signatures capture the results of the checks. The resulting extended control flow trace enables the recovery of otherwise irrecoverable data dependences. The code for the checks is designed to minimize the increase in program execution time and the extended control flow trace size when compared to directly collecting control flow and address traces. Our experiments show that compressed eWPPs are only one-quarter of the size of combined compressed control flow and address traces. However, their collection incurs a 5× increase in runtime overhead relative to the overhead required for directly collecting the control flow and address traces, respectively.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2053155247",
    "type": "article"
  },
  {
    "title": "Federation",
    "doi": "https://doi.org/10.1145/1880043.1880046",
    "publication_date": "2010-12-01",
    "publication_year": 2010,
    "authors": "Michael Boyer; David Tarjan; Kevin Skadron",
    "corresponding_authors": "",
    "abstract": "Manycore architectures designed for parallel workloads are likely to use simple, highly multithreaded, in-order cores. This maximizes throughput, but only with enough threads to keep hardware utilized. For applications or phases with more limited parallelism, we describe creating an out-of-order processor on-the-fly, by federating two neighboring in-order cores. We reuse the large register file in the multithreaded cores to implement some out-of-order structures and reengineer other large, associative structures into simpler lookup tables. The resulting federated core provides twice the single-thread performance of the underlying in-order core, allowing the architecture to efficiently support a wider range of parallelism.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2008026264",
    "type": "article"
  },
  {
    "title": "DeFT",
    "doi": "https://doi.org/10.1145/1970386.1970389",
    "publication_date": "2011-06-21",
    "publication_year": 2011,
    "authors": "Guru Venkataramani; Christopher J. Hughes; Sanjeev Kumar; Milos Prvulović",
    "corresponding_authors": "",
    "abstract": "While multicore processors promise large performance benefits for parallel applications, writing these applications is notoriously difficult. Tuning a parallel application to achieve good performance, also known as performance debugging, is often more challenging than debugging the application for correctness. Parallel programs have many performance-related issues that are not seen in sequential programs. An increase in cache misses is one of the biggest challenges that programmers face. To minimize these misses, programmers must not only identify the source of the extra misses, but also perform the tricky task of determining if the misses are caused by interthread communication (i.e., coherence misses) and if so, whether they are caused by true or false sharing (since the solutions for these two are quite different). In this article, we propose a new programmer-centric definition of false sharing misses and describe our novel algorithm to perform coherence miss classification. We contrast our approach with existing data-centric definitions of false sharing. A straightforward implementation of our algorithm is too expensive to be incorporated in real hardware. Therefore, we explore the design space for low-cost hardware support that can classify coherence misses on-the-fly into true and false sharing misses, allowing existing performance counters and profiling tools to expose and attribute them. We find that our approximate schemes achieve good accuracy at only a fraction of the cost of the ideal scheme. Additionally, we demonstrate the usefulness of our work in a case study involving a real application.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2023582707",
    "type": "article"
  },
  {
    "title": "Automatic code overlay generation and partially redundant code fetch elimination",
    "doi": "https://doi.org/10.1145/2207222.2207226",
    "publication_date": "2012-06-01",
    "publication_year": 2012,
    "authors": "Choonki Jang; Jaejin Lee; Bernhard Egger; Soojung Ryu",
    "corresponding_authors": "",
    "abstract": "There is an increasing interest in explicitly managed memory hierarchies, where a hierarchy of distinct memories is exposed to the programmer and managed explicitly in software. These hierarchies can be found in typical embedded systems and an emerging class of multicore architectures. To run an application that requires more code memory than the available higher-level memory, typically an overlay structure is needed. The overlay structure is generated manually by the programmer or automatically by a specialized linker. Manual code overlaying requires the programmer to deeply understand the program structure for maximum memory savings as well as minimum performance degradation. Although the linker can automatically generate the code overlay structure, its memory savings are limited and it even brings significant performance degradation because traditional techniques do not consider the program context. In this article, we propose an automatic code overlay generation technique that overcomes the limitations of traditional automatic code overlaying techniques. We are dealing with a system context that imposes two distinct constraints: (1) no hardware support for address translation and (2) a spatially and temporally coarse grained faulting mechanism at the function level. Our approach addresses those two constraints as efficiently as possible. Our technique statically computes the Worst-Case Number of Conflict misses (WCNC) between two different code segments using path expressions. Then, it constructs a static temporal relationship graph with the WCNCs and emits an overlay structure for a given higher-level memory size. We also propose an inter-procedural partial redundancy elimination technique that minimizes redundant code copying caused by the generated overlay structure. Experimental results show that our approach is promising.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2040257309",
    "type": "article"
  },
  {
    "title": "Managing SMT resource usage through speculative instruction window weighting",
    "doi": "https://doi.org/10.1145/2019608.2019611",
    "publication_date": "2011-10-01",
    "publication_year": 2011,
    "authors": "Hans Vandierendonck; André Seznec",
    "corresponding_authors": "",
    "abstract": "Simultaneous multithreading processors dynamically share processor resources between multiple threads. In general, shared SMT resources may be managed explicitly, for instance, by dynamically setting queue occupation bounds for each thread as in the DCRA and Hill-Climbing policies. Alternatively, resources may be managed implicitly; that is, resource usage is controlled by placing the desired instruction mix in the resources. In this case, the main resource management tool is the instruction fetch policy which must predict the behavior of each thread (branch mispredictions, long-latency loads, etc.) as it fetches instructions. In this article, we present the use of Speculative Instruction Window Weighting (SIWW) to bridge the gap between implicit and explicit SMT fetch policies. SIWW estimates for each thread the amount of outstanding work in the processor pipeline. Fetch proceeds for the thread with the least amount of work left. SIWW policies are implicit as fetch proceeds for the thread with the least amount of work left. They are also explicit as maximum resource allocation can also be set. SIWW can use and combine virtually any of the indicators that were previously proposed for guiding the instruction fetch policy (number of in-flight instructions, number of low confidence branches, number of predicted cache misses, etc.). Therefore, SIWW is an approach to designing SMT fetch policies, rather than a particular fetch policy. Targeting fairness or throughput is often contradictory and a SMT scheduling policy often optimizes only one performance metric at the sacrifice of the other metric. Our simulations show that the SIWW fetch policy can achieve at the same time state-of-the-art throughput, state-of-the-art fairness and state-of-the-art harmonic performance mean.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2076828486",
    "type": "article"
  },
  {
    "title": "DDRNoC",
    "doi": "https://doi.org/10.1145/3200201",
    "publication_date": "2018-06-08",
    "publication_year": 2018,
    "authors": "Ahsen Ejaz; Vassilios Papaefstathiou; Ioannis Sourdis",
    "corresponding_authors": "",
    "abstract": "This article introduces DDRNoC, an on-chip interconnection network capable of routing packets at Dual Data Rate. The cycle time of current 2D-mesh Network-on-Chip routers is limited by their control as opposed to the datapath (switch and link traversal), which exhibits significant slack. DDRNoC capitalizes on this observation, allowing two flits per cycle to share the same datapath. Thereby, DDRNoC achieves higher throughput than a Single Data Rate (SDR) network. Alternatively, using lower voltage circuits, the above slack can be exploited to reduce power consumption while matching the SDR network throughput. In addition, DDRNoC exhibits reduced clock distribution power, improving energy efficiency, as it needs a slower clock than a SDR network that routes packets at the same rate. Post place and route results in 28nm technology show that, compared to an iso-voltage (1.1V) SDR network, DDRNoC improves throughput proportionally to the SDR datapath slack. Moreover, a low-voltage (0.95V) DDRNoC implementation converts that slack to power reduction offering the 1.1V SDR throughput at a substantially lower energy cost.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2750938214",
    "type": "article"
  },
  {
    "title": "Energy-Performance Considerations for Data Offloading to FPGA-Based Accelerators Over PCIe",
    "doi": "https://doi.org/10.1145/3180263",
    "publication_date": "2018-03-22",
    "publication_year": 2018,
    "authors": "Dimitrios Mbakoyiannis; Othon Tomoutzoglou; George Kornaros",
    "corresponding_authors": "",
    "abstract": "Modern data centers increasingly employ FPGA-based heterogeneous acceleration platforms as a result of their great potential for continued performance and energy efficiency. Today, FPGAs provide more hardware parallelism than is possible with GPUs or CPUs, whereas C-like programming environments facilitate shorter development time, even close to software cycles. In this work, we address limitations and overheads in access and transfer of data to accelerators over common CPU-accelerator interconnects such as PCIe. We present three different FPGA accelerator dispatching methods for streaming applications (e.g., multimedia, vision computing). The first uses zero-copy data transfers and on-chip scratchpad memory (SPM) for energy efficiency, and the second uses also zero-copy but shared copy engines among different accelerator instances and local external memory. The third uses the processor’s memory management unit to acquire the physical address of user pages and uses scatter-gather data transfers with SPM. Even though all techniques exhibit advantages in terms of scalability and relieve the processor from control overheads through using integrated schedulers, the first method presents the best energy-efficient acceleration in streaming applications.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2789687162",
    "type": "article"
  },
  {
    "title": "SynchroTrace",
    "doi": "https://doi.org/10.1145/3158642",
    "publication_date": "2018-03-22",
    "publication_year": 2018,
    "authors": "Karthik Sangaiah; Michael Lui; Radhika Jagtap; Stephan Diestelhorst; Siddharth Nilakantan; Ankit More; Barış Taşkın; Mark Hempstead",
    "corresponding_authors": "",
    "abstract": "Trace-driven simulation of chip multiprocessor (CMP) systems offers many advantages over execution-driven simulation, such as reducing simulation time and complexity, allowing portability, and scalability. However, trace-based simulation approaches have difficulty capturing and accurately replaying multithreaded traces due to the inherent nondeterminism in the execution of multithreaded programs. In this work, we present SynchroTrace, a scalable, flexible, and accurate trace-based multithreaded simulation methodology. By recording synchronization events relevant to modern threading libraries (e.g., Pthreads and OpenMP) and dependencies in the traces, independent of the host architecture, the methodology is able to accurately model the nondeterminism of multithreaded programs for different hardware platforms and threading paradigms. Through capturing high-level instruction categories, the SynchroTrace average CPI trace Replay timing model offers fast and accurate simulation of many-core in-order CMPs. We perform two case studies to validate the SynchroTrace simulation flow against the gem5 full-system simulator: (1) a constraint-based design space exploration with traditional CMP benchmarks and (2) a thread-scalability study with HPC-representative applications. The results from these case studies show that (1) our trace-based approach with trace filtering has a peak speedup of up to 18.7× over simulation in gem5 full-system with an average of 9.6× speedup, (2) SynchroTrace maintains the thread-scaling accuracy of gem5 and can efficiently scale up to 64 threads, and (3) SynchroTrace can trace in one platform and model any platform in early stages of design.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2793449904",
    "type": "article"
  },
  {
    "title": "AUKE",
    "doi": "https://doi.org/10.1145/3291055",
    "publication_date": "2018-12-31",
    "publication_year": 2018,
    "authors": "Thomas Debrunner; Sajad Saeedi; Paul H. J. Kelly",
    "corresponding_authors": "",
    "abstract": "Focal-plane Sensor-Processor Arrays (FPSPs) are new imaging devices with parallel Single Instruction Multiple Data (SIMD) computational capabilities built into every pixel. Compared to traditional imaging devices, FPSPs allow for massive pixel-parallel execution of image processing algorithms. This enables the application of certain algorithms at extreme frame rates (&gt;10,000 frames per second). By performing some early-stage processing in-situ, systems incorporating FPSPs can consume less power compared to conventional approaches using standard digital cameras. In this article, we explore code generation for an FPSP whose 256 × 256 processors operate on analogue signal data, leading to further opportunities for power reduction—and additional code synthesis challenges. While rudimentary image processing algorithms have been demonstrated on FPSPs before, progress with higher-level computer vision algorithms has been sparse due to the unique architecture and limits of the devices. This article presents a code generator for convolution filters for the SCAMP-5 FPSP, with applications in many high-level tasks such as convolutional neural networks, pose estimation, and so on. The SCAMP-5 FPSP has no effective multiply operator. Convolutions have to be implemented through sequences of more primitive operations such as additions, subtractions, and multiplications/divisions by two. We present a code generation algorithm to optimise convolutions by identifying common factors in the different weights and by determining an optimised pattern of pixel-to-pixel data movements to exploit them. We present evaluation in terms of both speed and energy consumption for a suite of well-known convolution filters. Furthermore, an application of the method is shown by the implementation of a Viola-Jones face detection algorithm.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2910203559",
    "type": "article"
  },
  {
    "title": "JIT technology with C/C++",
    "doi": "https://doi.org/10.1145/2541228.2555315",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Dorit Nuzman; Revital Eres; Sergei Dyshel; Marcel Zalmanovici; José G. Castaños",
    "corresponding_authors": "",
    "abstract": "The growing gap between the advanced capabilities of static compilers as reflected in benchmarking results and the actual performance that users experience in real-life scenarios makes client-side dynamic optimization technologies imperative to the domain of static languages. Dynamic optimization of software distributed in the form of a platform-agnostic Intermediate-Representation (IR) has been very successful in the domain of managed languages, greatly improving upon interpreted code, especially when online profiling is used. However, can such feedback-directed IR-based dynamic code generation be viable in the domain of statically compiled, rather than interpreted, languages? We show that fat binaries, which combine the IR together with the statically compiled executable, can provide a practical solution for software vendors, allowing their software to be dynamically optimized without the limitation of binary-level approaches, which lack the high-level IR of the program, and without the warm-up costs associated with the IR-only software distribution approach. We describe and evaluate the fat-binary-based runtime compilation approach using SPECint2006, demonstrating that the overheads it incurs are low enough to be successfully surmounted by dynamic optimization. Building on Java JIT technologies, our results already improve upon common real-world usage scenarios, including very small workloads.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W1968815260",
    "type": "article"
  },
  {
    "title": "Information flow tracking meets just-in-time compilation",
    "doi": "https://doi.org/10.1145/2541228.2555295",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Christoph Kerschbaumer; Eric Hennigan; Per Larsen; Stefan Brunthaler; Michael Franz",
    "corresponding_authors": "",
    "abstract": "Web applications are vulnerable to cross-site scripting attacks that enable data thefts. Information flow tracking in web browsers can prevent communication of sensitive data to unintended recipients and thereby stop such data thefts. Unfortunately, existing solutions have focused on incorporating information flow into browsers’ JavaScript interpreters, rather than just-in-time compilers, rendering the resulting performance noncompetitive. Few users will switch to a safer browser if it comes at the cost of significantly degrading web application performance. We present the first information flow tracking JavaScript engine that is based on a true just-in-time compiler, and that thereby outperforms all previous interpreter-based information flow tracking JavaScript engines by more than a factor of two. Our JIT-based engine (i) has the same coverage as previous interpreter- based solutions, (ii) requires reasonable implementation effort, and (iii) introduces new optimizations to achieve acceptable performance. When evaluated against three industry-standard JavaScript benchmark suites, there is still an average slowdown of 73% over engines that do not support information flow, but this is now well within the range that many users will find an acceptable price for obtaining substantially increased security.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2007831853",
    "type": "article"
  },
  {
    "title": "Vectorization technology to improve interpreter performance",
    "doi": "https://doi.org/10.1145/2400682.2400685",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Erven Rohou; Kevin Williams; David Yuste",
    "corresponding_authors": "",
    "abstract": "In the present computing landscape, interpreters are in use in a wide range of systems. Recent trends in consumer electronics have created a new category of portable, lightweight software applications. Typically, these applications have fast development cycles and short life spans. They run on a wide range of systems and are deployed in a target independent bytecode format over Internet and cellular networks. Their authors are untrusted third-party vendors, and they are executed in secure managed runtimes or virtual machines. Furthermore, due to security policies or development time constraints, these virtual machines often lack just-in-time compilers and rely on interpreted execution. At the other end of the spectrum, interpreters are also a reality in the field of high-performance computations because of the flexibility they provide. The main performance penalty in interpreters arises from instruction dispatch. Each bytecode requires a minimum number of machine instructions to be executed. In this work, we introduce a novel approach for interpreter optimization that reduces instruction dispatch thanks to vectorization technology. We extend the split compilation paradigm to interpreters, thus guaranteeing that our approach exhibits almost no overhead at runtime. We take advantage of the vast research in vectorization and its presence in modern compilers. Complex analyses are performed ahead of time, and their results are conveyed to the executable bytecode. At runtime, the interpreter retrieves this additional information to build the SIMD IR (intermediate representation) instructions that carry the vector semantics. The bytecode language remains unmodified, making this representation compatible with legacy interpreters and previously proposed JIT compilers. We show that this approach drastically reduces the number of instructions to interpret and decreases execution time of vectorizable applications. Moreover, we map SIMD IR instructions to hardware SIMD instructions when available, with a substantial additional improvement. Finally, we finely analyze the impact of our extension on the behavior of the caches and branch predictors.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2009217012",
    "type": "article"
  },
  {
    "title": "Designing on-chip networks for throughput accelerators",
    "doi": "https://doi.org/10.1145/2512429",
    "publication_date": "2013-09-16",
    "publication_year": 2013,
    "authors": "Ali Bakhoda; John Kim; Tor M. Aamodt",
    "corresponding_authors": "",
    "abstract": "As the number of cores and threads in throughput accelerators such as Graphics Processing Units (GPU) increases, so does the importance of on-chip interconnection network design. This article explores throughput-effective Network-on-Chips (NoC) for future compute accelerators that employ Bulk-Synchronous Parallel (BSP) programming models such as CUDA and OpenCL. A hardware optimization is “throughput effective” if it improves parallel application-level performance per unit chip area. We evaluate performance of future looking workloads using detailed closed-loop simulations modeling compute nodes, NoC, and the DRAM memory system. We start from a mesh design with bisection bandwidth balanced to off-chip demand. Accelerator workloads tend to demand high off-chip memory bandwidth which results in a many-to-few traffic pattern when coupled with expected technology constraints of slow growth in pins-per-chip. Leveraging these observations we reduce NoC area by proposing a “checkerboard” NoC which alternates between conventional full routers and half routers with limited connectivity. Next, we show that increasing network terminal bandwidth at the nodes connected to DRAM controllers alleviates a significant fraction of the remaining imbalance resulting from the many-to-few traffic pattern. Furthermore, we propose a “double checkerboard inverted” NoC organization which takes advantage of channel slicing to reduce area while maintaining the performance improvements of the aforementioned techniques. This organization also has a simpler routing mechanism and improves average application throughput per unit area by 24.3%.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2035709574",
    "type": "article"
  },
  {
    "title": "Dynamic code duplication with vulnerability awareness for soft error detection on VLIW architectures",
    "doi": "https://doi.org/10.1145/2400682.2400707",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Jongwon Lee; Yohan Ko; Kyoungwoo Lee; Jonghee M. Youn; Yunheung Paek",
    "corresponding_authors": "",
    "abstract": "Soft errors are becoming a critical concern in embedded system designs. Code duplication techniques have been proposed to increase the reliability in multi-issue embedded systems such as VLIW by exploiting empty slots for duplicated instructions. However, they increase code size, another important concern, and ignore vulnerability differences in instructions, causing unnecessary or inefficient protection when selecting instructions to be duplicated under constraints. In this article, we propose a compiler-assisted dynamic code duplication method to minimize the code size overhead, and present vulnerability-aware duplication algorithms to maximize the effectiveness of instruction duplication with least overheads for VLIW architecture. Our experimental results with SoarGen and Synopsys simulation environments demonstrate that our proposals can reduce the code size by up to 40% and detect more soft errors by up to 82% via fault injection experiments over benchmarks from DSPstone and Livermore Loops as compared to the previously proposed instruction duplication technique.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2051856194",
    "type": "article"
  },
  {
    "title": "PS-TLB",
    "doi": "https://doi.org/10.1145/2400682.2400687",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Yong Li; Rami Melhem; Alex K. Jones",
    "corresponding_authors": "",
    "abstract": "Traversing the page table during virtual to physical address translation causes pipeline stalls when misses occur in the translation-lookaside buffer (TLB). State-of-the-art translation proposals typically optimize a single aspect of translation performance (e.g., translation sharing, context switch performance, etc.) with potential trade-offs of additional hardware complexity, increased translation latency, or reduced scalability. In this article, we propose the partial sharing TLB (PS-TLB), a fast and scalable solution that reduces off-chip translation misses without sacrificing the timing-critical requirement of on-chip translation. We introduce the partial sharing buffer (PSB) which leverages application page sharing characteristics using minimal additional hardware resources. Compared to the leading TLB proposal that leverages sharing, PS-TLB provides a more than 45% improvement in translation latency with a 9% application speedup while using fewer storage resources. In addition, the page classification and PS-TLB architecture provide further optimizations including an over 30% reduction of interprocessor interrupts for coherence, and reduced context switch misses with fewer resources compared with existing methods.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2058829041",
    "type": "article"
  },
  {
    "title": "Time- and space-efficient flow-sensitive points-to analysis",
    "doi": "https://doi.org/10.1145/2541228.2555296",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Rupesh Nasre",
    "corresponding_authors": "Rupesh Nasre",
    "abstract": "Compilation of real-world programs often requires hours. The term nightly build known to industrial researchers is an artifact of long compilation times. Our goal is to reduce the absolute analysis times for large C codes (of the order of millions of lines). Pointer analysis is one of the key analyses performed during compilation. Its scalability is paramount to achieve the efficiency of the overall compilation process and its precision directly affects that of the client analyses. In this work, we design a time- and space-efficient flow-sensitive pointer analysis and parallelize it on graphics processing units. Our analysis proposes to use an extended bloom filter, called multibloom, to store points-to information in an approximate manner and develops an analysis in terms of the operations over the multibloom. Since bloom filter is a probabilistic data structure, we develop ways to gain back the analysis precision. We achieve effective parallelization by achieving memory coalescing, reducing thread divergence, and improving load balance across GPU warps. Compared to a state-of-the-art sequential solution, our parallel version achieves a 7.8 × speedup with less than 5% precision loss on a suite of six large programs. Using two client transformations, we show that this loss in precision only minimally affects a client’s precision.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2086039932",
    "type": "article"
  },
  {
    "title": "Leveraging Strength-Based Dynamic Information Flow Analysis to Enhance Data Value Prediction",
    "doi": "https://doi.org/10.1145/2133382.2133383",
    "publication_date": "2012-03-01",
    "publication_year": 2012,
    "authors": "Walid J. Ghandour; Haitham Akkary; Wes Masri",
    "corresponding_authors": "",
    "abstract": "Value prediction is a technique to increase parallelism by attempting to overcome serialization constraints caused by true data dependences. By predicting the outcome of an instruction before it executes, value prediction allows data dependent instructions to issue and execute speculatively, hence increasing parallelism when the prediction is correct. In case of a misprediction, the execution is redone with the corrected value. If the benefit from increased parallelism outweighs the misprediction recovery penalty, overall performance could be improved. Enhancing performance with value prediction therefore requires highly accurate prediction methods. Most existing general value prediction techniques are local, that is, future outputs of an instruction are predicted based on outputs from previous executions of the same instruction. In this article, we investigate leveraging strength-based dynamic information flow analysis to enhance data value prediction. We use dynamic information flow analysis (DIFA) to determine when a specific value predictor can perform well and even outperform other predictors. We apply information theory to mathematically prove the validity and benefits of correlating value predictors. We also introduce the concept of the linear value predictors, a new technique that predicts a new value from another one using a linear relation. We finally present a variant of stride predictor that we call update stride . We then conduct an empirical analysis using Pin , a dynamic binary instrumentation tool, and DynFlow , a dynamic information flow analysis tool, that we apply to programs from the SPECjvm2008 and Siemens benchmarks. Our empirical measurements support our mathematical theory and allow us to make important observations on the relation between predictability of data values and information flow. Our analysis and empirical results show that the values of a set of selected variables can be predicted with a very high accuracy, up to 100%. Such prediction is based on the previous history and/or the values of one or more other source variables that have strong information flow into the predicted variable. Using our selection criteria, we show that a DIFA-directed predictor outperforms hardware value prediction for all subject programs, and sometimes by a significant margin. This was observed even when using an ideal tagged hardware value prediction table that does not suffer from aliasing or capacity misses.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2163189379",
    "type": "article"
  },
  {
    "title": "MATOG",
    "doi": "https://doi.org/10.1145/3106341",
    "publication_date": "2017-08-30",
    "publication_year": 2017,
    "authors": "Nicolas Weber; Michael Goesele",
    "corresponding_authors": "",
    "abstract": "Optimal code performance is (besides correctness and accuracy) the most important objective in compute intensive applications. In many of these applications, Graphic Processing Units (GPUs) are used because of their high amount of compute power. However, caused by their massively parallel architecture, the code has to be specifically adjusted to the underlying hardware to achieve optimal performance and therefore has to be reoptimized for each new generation. In reality, this is usually not the case as productive code is normally at least several years old and nobody has the time to continuously adjust existing code to new hardware. In recent years more and more approaches have emerged that automatically tune the performance of applications toward the underlying hardware. In this article, we present the MATOG auto-tuner and its concepts. It abstracts the array memory access in CUDA applications and automatically optimizes the code according to the used GPUs. MATOG only requires few profiling runs to analyze even complex applications, while achieving significant speedups over non-optimized code, independent of the used GPU generation and without the need to manually tune the code.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2729851701",
    "type": "article"
  },
  {
    "title": "Decoupling Data Supply from Computation for Latency-Tolerant Communication in Heterogeneous Architectures",
    "doi": "https://doi.org/10.1145/3075620",
    "publication_date": "2017-06-28",
    "publication_year": 2017,
    "authors": "Tae Jun Ham; Juan L. Aragón; Margaret Martonosi",
    "corresponding_authors": "",
    "abstract": "In today’s computers, heterogeneous processing is used to meet performance targets at manageable power. In adopting increased compute specialization, however, the relative amount of time spent on communication increases. System and software optimizations for communication often come at the costs of increased complexity and reduced portability. The Decoupled Supply-Compute (DeSC) approach offers a way to attack communication latency bottlenecks automatically, while maintaining good portability and low complexity. Our work expands prior Decoupled Access Execute techniques with hardware/software specialization. For a range of workloads, DeSC offers roughly 2 × speedup, and additional specialized compression optimizations reduce traffic between decoupled units by 40%.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2731544116",
    "type": "article"
  },
  {
    "title": "Iterative Schedule Optimization for Parallelization in the Polyhedron Model",
    "doi": "https://doi.org/10.1145/3109482",
    "publication_date": "2017-08-22",
    "publication_year": 2017,
    "authors": "Stefan Ganser; Armin Größlinger; Norbert Siegmund; Sven Apel; Christian Lengauer",
    "corresponding_authors": "",
    "abstract": "The polyhedron model is a powerful model to identify and apply systematically loop transformations that improve data locality (e.g., via tiling) and enable parallelization. In the polyhedron model, a loop transformation is, essentially, represented as an affine function. Well-established algorithms for the discovery of promising transformations are based on performance models. These algorithms have the drawback of not being easily adaptable to the characteristics of a specific program or target hardware. An iterative search for promising loop transformations is more easily adaptable and can help to learn better models. We present an iterative optimization method in the polyhedron model that targets tiling and parallelization. The method enables either a sampling of the search space of legal loop transformations at random or a more directed search via a genetic algorithm. For the latter, we propose a set of novel, tailored reproduction operators. We evaluate our approach against existing iterative and model-driven optimization strategies. We compare the convergence rate of our genetic algorithm to that of random exploration. Our approach of iterative optimization outperforms existing optimization techniques in that it finds loop transformations that yield significantly higher performance. If well configured, then random exploration turns out to be very effective and reduces the need for a genetic algorithm.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2747830075",
    "type": "article"
  },
  {
    "title": "Cooperative Multi-Agent Reinforcement Learning-Based Co-optimization of Cores, Caches, and On-chip Network",
    "doi": "https://doi.org/10.1145/3132170",
    "publication_date": "2017-11-14",
    "publication_year": 2017,
    "authors": "Rahul Jain; Preeti Ranjan Panda; Sreenivas Subramoney",
    "corresponding_authors": "",
    "abstract": "Modern multi-core systems provide huge computational capabilities, which can be used to run multiple processes concurrently. To achieve the best possible performance within limited power budgets, the various system resources need to be allocated effectively. Any mismatch between runtime resource requirement and allocation leads to a sub-optimal energy-delay product (EDP). Different optimization techniques exist for addressing the problem of mismatch between the dynamic requirement and runtime allocation of the system resources. Choosing between multiple optimizations at runtime is complex due to the non-additive effects, making the scenario suitable for the application of machine learning techniques. We present a novel method, Machine Learned Machines (MLM), by using online reinforcement learning (RL) to perform dynamic partitioning of the last level cache (LLC), along with dynamic voltage and frequency scaling (DVFS) of the core and uncore (interconnection network and LLC). We have proposed and evaluated three different MLM co-optimization techniques based on independent and cooperative multi-agent learners. We show that the co-optimization results in a much lower system EDP than any of the techniques applied individually. We explore various RL models targeted toward optimization of different system metrics and study their effects on a system EDP, system throughput (STP), and Fairness. The various proposed techniques have been extensively evaluated with a mix of 20 workloads on a 4-core system using Spec2006 benchmarks. We have further evaluated our cooperative MLM techniques on a 16-core system. The results show an average of 20.5% and 19.1% system EDP improvement on a 4-core and 16-core system, respectively, with limited degradation of STP and Fairness.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2768195099",
    "type": "article"
  },
  {
    "title": "Elastic Places",
    "doi": "https://doi.org/10.1145/3185458",
    "publication_date": "2018-05-01",
    "publication_year": 2018,
    "authors": "Miquel Pericàs",
    "corresponding_authors": "Miquel Pericàs",
    "abstract": "The diversity and complexity of modern computing platforms makes the development of high-performance software challenging. Designing scalable software requires tuning for a large set of resources, including cores (parallelism), memory bandwidths, and various levels of private and shared caches, as well as developing strategies for optimizing locality. But highly optimized implementations are often inefficient when executed on a different platform. This is the performance portability problem. One approach to scalability and portability is to tune the amount of work per task based on runtime overheads and concurrency. This results in a better balance between parallelism and scheduling overheads, but it can neither tune data reuse nor avoid inter-task interference. We propose a complementary approach that consists in tuning the amount of resources allocated to tasks and combine it with software-defined task topologies to provide portable locality. These ideas are combined into a low-overhead resource management scheme called Elastic Places . Elastic Places is implemented in the XiTAO software framework but the core ideas are equally applicable to other languages and runtimes. Experimental results on an AMD-based NUMA machine and an Intel Knights Landing system show that elastic places provides both high scalability and performance portability, with speed-ups of up to 2.3× on both platforms compared to state-of-the-art runtimes.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2801090658",
    "type": "article"
  },
  {
    "title": "Predictable Thread Coarsening",
    "doi": "https://doi.org/10.1145/3194242",
    "publication_date": "2018-06-12",
    "publication_year": 2018,
    "authors": "Nicolai Stawinoga; Tony Field",
    "corresponding_authors": "",
    "abstract": "Thread coarsening on GPUs combines the work of several threads into one. We show how thread coarsening can be implemented as a fully automated compile-time optimisation that estimates the optimal coarsening factor based on a low-cost, approximate static analysis of cache line re-use and an occupancy prediction model. We evaluate two coarsening strategies on three different NVidia GPU architectures. For NVidia reduction kernels we achieve a maximum speedup of 5.08x, and for the Rodinia benchmarks we achieve a mean speedup of 1.30x over 8 of 19 kernels that were determined safe to coarsen.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2808709390",
    "type": "article"
  },
  {
    "title": "An Efficient GPU Cache Architecture for Applications with Irregular Memory Access Patterns",
    "doi": "https://doi.org/10.1145/3322127",
    "publication_date": "2019-06-17",
    "publication_year": 2019,
    "authors": "Bingchao Li; Jizeng Wei; Jizhou Sun; Murali Annavaram; Nam Sung Kim",
    "corresponding_authors": "",
    "abstract": "GPUs provide high-bandwidth/low-latency on-chip shared memory and L1 cache to efficiently service a large number of concurrent memory requests. Specifically, concurrent memory requests accessing contiguous memory space are coalesced into warp-wide accesses. To support such large accesses to L1 cache with low latency, the size of L1 cache line is no smaller than that of warp-wide accesses. However, such L1 cache architecture cannot always be efficiently utilized when applications generate many memory requests with irregular access patterns especially due to branch and memory divergences that make requests uncoalesced and small. Furthermore, unlike L1 cache, the shared memory of GPUs is not often used in many applications, which essentially depends on programmers. In this article, we propose Elastic-Cache, which can efficiently support both fine- and coarse-grained L1 cache line management for applications with both regular and irregular memory access patterns to improve the L1 cache efficiency. Specifically, it can store 32- or 64-byte words in non-contiguous memory space to a single 128-byte cache line. Furthermore, it neither requires an extra memory structure nor reduces the capacity of L1 cache for tag storage, since it stores auxiliary tags for fine-grained L1 cache line managements in the shared memory space that is not fully used in many applications. To improve the bandwidth utilization of L1 cache with Elastic-Cache for fine-grained accesses, we further propose Elastic-Plus to issue 32-byte memory requests in parallel, which can reduce the processing latency of memory instructions and improve the throughput of GPUs. Our experiment result shows that Elastic-Cache improves the geometric-mean performance of applications with irregular memory access patterns by 104% without degrading the performance of applications with regular memory access patterns. Elastic-Plus outperforms Elastic-Cache and improves the performance of applications with irregular memory access patterns by 131%.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2950969115",
    "type": "article"
  },
  {
    "title": "Correct-by-Construction Parallelization of Hard Real-Time Avionics Applications on Off-the-Shelf Predictable Hardware",
    "doi": "https://doi.org/10.1145/3328799",
    "publication_date": "2019-07-08",
    "publication_year": 2019,
    "authors": "Keryan Didier; Dumitru Potop‐Butucaru; Guillaume Iooss; Albert Cohen; Jean Souyris; Philippe Baufreton; Amaury Graillat",
    "corresponding_authors": "",
    "abstract": "We present the first end-to-end modeling and compilation flow to parallelize hard real-time control applications while fully guaranteeing the respect of real-time requirements on off-the-shelf hardware. It scales to thousands of dataflow nodes and has been validated on two production avionics applications. Unlike classical optimizing compilation, it takes as input non-functional requirements (real time, resource limits). To enforce these requirements, the compiler follows a static resource allocation strategy, from coarse-grain tasks communicating over an interconnection network all the way to individual variables and memory accesses. It controls timing interferences resulting from mapping decisions in a precise, safe, and scalable way.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2960647341",
    "type": "article"
  },
  {
    "title": "Simplifying Transactional Memory Support in C++",
    "doi": "https://doi.org/10.1145/3328796",
    "publication_date": "2019-07-25",
    "publication_year": 2019,
    "authors": "Pantea Zardoshti; Tingzhe Zhou; Pavithra Balaji; Michael L. Scott; Michael Spear",
    "corresponding_authors": "",
    "abstract": "C++ has supported a provisional version of Transactional Memory (TM) since 2015, via a technical specification. However, TM has not seen widespread adoption, and compiler vendors have been slow to implement the technical specification. We conjecture that the proposed TM support is too difficult for programmers to use, too complex for compiler designers to implement and verify, and not industry-proven enough to justify final standardization in its current form. To address these problems, we present a different design for supporting TM in C++. By forbidding explicit self-abort, and by introducing an executor-based mechanism for running transactions, our approach makes it easier for developers to get code up and running with TM. Our proposal should also be appealing to compiler developers, as it allows a spectrum of levels of support for TM, with varying performance, and varying reliance on hardware TM support in order to provide scalability. &lt;?tight?&gt;While our design does not enable some of the optimizations admitted by the current technical specification, we show that it enables the implementation of robust support for TM in a small, orthogonal compiler extension. Our implementation is able to handle a wide range of transactional programs, delivering low instrumentation overhead and scalability and performance on par with the current state of the art. Based on this experience, we believe our approach to be a viable means of reinvigorating the standardization of TM in C++.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2963622657",
    "type": "article"
  },
  {
    "title": "Enabling Highly Efficient Batched Matrix Multiplications on SW26010 Many-core Processor",
    "doi": "https://doi.org/10.1145/3378176",
    "publication_date": "2020-03-04",
    "publication_year": 2020,
    "authors": "Lijuan Jiang; Chao Yang; Wenjing Ma",
    "corresponding_authors": "",
    "abstract": "We present a systematic methodology for optimizing batched matrix multiplications on SW26010 many-core processor of the Sunway TaihuLight supercomputer. Five surrogate algorithms and a machine learning–based algorithm selector are proposed to fully exploit the computing capability of SW26010 and cope with the sophisticated algorithm characteristics of batched matrix multiplications. Experiment results show that the algorithm selector is able to adaptively choose the appropriate algorithm for various matrix shapes and batch sizes with low overhead and high accuracy. In particular, the optimized batched matrix multiplications can substantially outperform the non-batched version and reach around 84.8% of the performance upper bound.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W3009472344",
    "type": "article"
  },
  {
    "title": "GPU Fast Convolution via the Overlap-and-Save Method in Shared Memory",
    "doi": "https://doi.org/10.1145/3394116",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "Karel Adámek; Sofia Dimoudi; Michael B. Giles; Wesley Armour",
    "corresponding_authors": "",
    "abstract": "We present an implementation of the overlap-and-save method, a method for the convolution of very long signals with short response functions, which is tailored to GPUs. We have implemented several FFT algorithms (using the CUDA programming language), which exploit GPU shared memory, allowing for GPU accelerated convolution. We compare our implementation with an implementation of the overlap-and-save algorithm utilizing the NVIDIA FFT library (cuFFT). We demonstrate that by using a shared-memory-based FFT, we can achieved significant speed-ups for certain problem sizes and lower the memory requirements of the overlap-and-save method on GPUs.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W3040913567",
    "type": "article"
  },
  {
    "title": "Orchestrating stream graphs using model checking",
    "doi": "https://doi.org/10.1145/2512435",
    "publication_date": "2013-09-16",
    "publication_year": 2013,
    "authors": "Avinash Malik; David Gregg",
    "corresponding_authors": "",
    "abstract": "In this article we use model checking to statically distribute and schedule Synchronous DataFlow (SDF) graphs on heterogeneous execution architectures . We show that model checking is capable of providing an optimal solution and it arrives at these solutions faster (in terms of algorithm runtime) than equivalent ILP formulations. Furthermore, we also show how different types of optimizations such as task parallelism, data parallelism, and state sharing can be included within our framework. Finally, comparison of our approach with the current state-of-the-art heuristic techniques show the pitfalls of these techniques and gives a glimpse of how these heuristic techniques can be improved.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W1964986712",
    "type": "article"
  },
  {
    "title": "Improving Hybrid FTL by Fully Exploiting Internal SSD Parallelism with Virtual Blocks",
    "doi": "https://doi.org/10.1145/2677160",
    "publication_date": "2014-12-08",
    "publication_year": 2014,
    "authors": "Dan He; Fang Wang; Hong Jiang; Dan Feng; Jing Ning Liu; Wei Tong; Zheng Zhang",
    "corresponding_authors": "",
    "abstract": "Compared with either block or page-mapping Flash Translation Layer (FTL), hybrid-mapping FTL for flash Solid State Disks (SSDs), such as Fully Associative Section Translation (FAST), has relatively high space efficiency because of its smaller mapping table than the latter and higher flexibility than the former. As a result, hybrid-mapping FTL has become the most commonly used scheme in SSDs. But the hybrid-mapping FTL incurs a large number of costly full-merge operations. Thus, a critical challenge to hybrid-mapping FTL is how to reduce the cost of full-merge operations and improve partial merge operations and switch operations. In this article, we propose a novel FTL scheme, called Virtual Block-based Parallel FAST (VBP-FAST), that divides flash area into Virtual Blocks (VBlocks) and Physical Blocks (PBlocks) where VBlocks are used to fully exploit channel-level, die-level, and plane-level parallelism of flash. Leveraging these three levels of parallelism, the cost of full merge in VBP-FAST is significantly reduced from that of FAST. In the meantime, VBP-FAST uses PBlocks to retain the advantages of partial merge and switch operations. Our extensive trace-driven simulation results show that VBP-FAST speeds up FAST by a factor of 5.3--8.4 for random workloads and of 1.7 for sequential workloads with channel-level, die-level, and plane-level parallelism of 8, 2, and 2 (i.e., eight channels, two dies, and two planes).",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W1988725660",
    "type": "article"
  },
  {
    "title": "Hadoop Extensions for Distributed Computing on Reconfigurable Active SSD Clusters",
    "doi": "https://doi.org/10.1145/2608199",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "Abdulrahman Kaitoua; Hazem Hajj; Mazen A. R. Saghir; Hassan Artail; Haitham Akkary; Mariette Awad; Mageda Sharafeddine; Khaleel Mershad",
    "corresponding_authors": "",
    "abstract": "In this article, we propose new extensions to Hadoop to enable clusters of reconfigurable active solid-state drives (RASSDs) to process streaming data from SSDs using FPGAs. We also develop an analytical model to estimate the performance of RASSD clusters running under Hadoop. Using the Hadoop RASSD platform and network simulators, we validate our design and demonstrate its impact on performance for different workloads taken from Stanford's Phoenix MapReduce project. Our results show that for a hardware acceleration factor of 20×, compute-intensive workloads processing 153MB of data can run up to 11× faster than a standard Hadoop cluster.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W1995383969",
    "type": "article"
  },
  {
    "title": "Hardware-Assisted Cooperative Integration of Wear-Leveling and Salvaging for Phase Change Memory",
    "doi": "https://doi.org/10.1145/2459316.2459318",
    "publication_date": "2013-05-01",
    "publication_year": 2013,
    "authors": "Lei Jiang; Yu Du; Bo Zhao; Youtao Zhang; Bruce R. Childers; Jun Yang",
    "corresponding_authors": "",
    "abstract": "Phase Change Memory (PCM) has recently emerged as a promising memory technology. However, PCM’s limited write endurance restricts its immediate use as a replacement for DRAM. To extend the lifetime of PCM chips, wear-leveling and salvaging techniques have been proposed. Wear-leveling balances write operations across different PCM regions while salvaging extends the duty cycle and provides graceful degradation for a nonnegligible number of failures. Current wear-leveling and salvaging schemes have not been designed and integrated to work cooperatively to achieve the best PCM device lifetime. In particular, a noncontiguous PCM space generated from salvaging complicates wear-leveling and incurs large overhead. In this article, we propose LLS, a Line-Level mapping and Salvaging design. By allocating a dynamic portion of total space in a PCM device as backup space, and mapping failed lines to backup PCM, LLS constructs a contiguous PCM space and masks lower-level failures from the OS and applications. LLS integrates wear-leveling and salvaging and copes well with modern OSes. Our experimental results show that LLS achieves 31% longer lifetime than the state-of-the-art. It has negligible hardware cost and performance overhead.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W1995868334",
    "type": "article"
  },
  {
    "title": "Hardware Fault Recovery for I/O Intensive Applications",
    "doi": "https://doi.org/10.1145/2656342",
    "publication_date": "2014-10-27",
    "publication_year": 2014,
    "authors": "Pradeep Ramachandran; Siva Kumar Sastry Hari; Man-Lap Li; Sarita V. Adve",
    "corresponding_authors": "",
    "abstract": "With continued process scaling, the rate of hardware failures in commodity systems is increasing. Because these commodity systems are highly sensitive to cost, traditional solutions that employ heavy redundancy to handle such failures are no longer acceptable owing to their high associated costs. Detecting such faults by identifying anomalous software execution and recovering through checkpoint-and-replay is emerging as a viable low-cost alternative for future commodity systems. An important but commonly ignored aspect of such solutions is ensuring that external outputs to the system are fault-free. The outputs must be delayed until the detectors guarantee this, influencing fault-free performance. The overheads for resiliency must thus be evaluated while taking these delays into consideration; prior work has largely ignored this relationship. This article concerns recovery for I/O intensive applications from in-core faults. We present a strategy to buffer external outputs using dedicated hardware and show that checkpoint intervals previously considered as acceptable incur exorbitant overheads when hardware buffering is considered. We then present two techniques to reduce the checkpoint interval and demonstrate a practical solution that provides high resiliency while incurring low overheads.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W1997195086",
    "type": "article"
  },
  {
    "title": "Techniques to improve performance in requester-wins hardware transactional memory",
    "doi": "https://doi.org/10.1145/2541228.2555299",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Adrià Armejach; Rubén Titos-Gil; Anurag Negi; Osman Ünsal; Adrián Cristal",
    "corresponding_authors": "",
    "abstract": "The simplicity of requester-wins Hardware Transactional Memory (HTM) makes it easy to incorporate in existing chip multiprocessors. Hence, such systems are expected to be widely available in the near future. Unfortunately, these implementations are prone to suffer severe performance degradation due to transient and persistent livelock conditions. This article shows that existing techniques are unable to mitigate this degradation effectively. It then proposes and evaluates four novel techniques—two software-based that employ information provided by the hardware and two that require simple core-local hardware additions—which have the potential to boost the performance of requester-wins HTM designs substantially.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2038974501",
    "type": "article"
  },
  {
    "title": "SPCM",
    "doi": "https://doi.org/10.1145/2829951",
    "publication_date": "2015-11-16",
    "publication_year": 2015,
    "authors": "Morteza Hoseinzadeh; Mohammad Arjomand; Hamid Sarbazi‐Azad",
    "corresponding_authors": "",
    "abstract": "Phase Change Memory (PCM) devices are one of the known promising technologies to take the place of DRAM devices with the aim of overcoming the obstacles of reducing feature size and stopping ever growing amounts of leakage power. In exchange for providing high capacity, high density, and nonvolatility, PCM Multilevel Cells (MLCs) impose high write energy and long latency. Many techniques have been proposed to resolve these side effects. However, read performance issues are usually left behind the great importance of write latency, energy, and lifetime. In this article, we focus on read performance and improve the critical path latency of the main memory system. To this end, we exploit striping scheme by which multiple lines are grouped and lie on a single MLC line array. In order to achieve more performance gain, an adaptive ordering mechanism is used to sort lines in a group based on their read frequency. This scheme imposes large energy and lifetime overheads due to its intensive demand for higher write bandwidth. Thus, we equipped our design with a grouping/pairing write queue to synchronize write-back requests such that all updates to an MLC array occur at once. The design is also augmented by a directional write scheme that takes benefits of the uniformity of accesses to the PCM device—caused by the large DRAM cache—to determine the writing mode (striped or nonstriped). This adaptation to write operations relaxes the energy and lifetime overheads. We improve the read latency of a 2-bit MLC PCM memory by more than 24% (and Instructions Per Cycle (IPC) by about 9%) and energy-delay product by about 20% for a small lifetime degradation of 8%, on average.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2266341648",
    "type": "article"
  },
  {
    "title": "Boosting the Priority of Garbage",
    "doi": "https://doi.org/10.1145/2875424",
    "publication_date": "2016-03-07",
    "publication_year": 2016,
    "authors": "Shoaib Akram; Jennifer B. Sartor; Kenzo Van Craeynest; Wim Heirman; Lieven Eeckhout",
    "corresponding_authors": "",
    "abstract": "While hardware is evolving toward heterogeneous multicore architectures, modern software applications are increasingly written in managed languages. Heterogeneity was born of a need to improve energy efficiency; however, we want the performance of our applications not to suffer from limited resources. How best to schedule managed language applications on a mix of big, out-of-order cores and small, in-order cores is an open question, complicated by the host of service threads that perform key tasks such as memory management. These service threads compete with the application for core and memory resources, and garbage collection (GC) must sometimes suspend the application if there is not enough memory available for allocation. In this article, we explore concurrent garbage collection’s behavior, particularly when it becomes critical, and how to schedule it on a heterogeneous system to optimize application performance. While some applications see no difference in performance when GC threads are run on big versus small cores, others—those with GC criticality —see up to an 18% performance improvement. We develop a new, adaptive scheduling algorithm that responds to GC criticality signals from the managed runtime, giving more big-core cycles to the concurrent collector when it is under pressure and in danger of suspending the application. Our experimental results show that our GC-criticality-aware scheduler is robust across a range of heterogeneous architectures with different core counts and frequency scaling and across heap sizes. Our algorithm is performance and energy neutral for GC-uncritical Java applications and significantly speeds up GC-critical applications by 16%, on average, while being 20% more energy efficient for a heterogeneous multicore with three big cores and one small core.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2301758480",
    "type": "article"
  },
  {
    "title": "Application-Specific Arithmetic in High-Level Synthesis Tools",
    "doi": "https://doi.org/10.1145/3377403",
    "publication_date": "2020-03-04",
    "publication_year": 2020,
    "authors": "Yohann Uguen; Florent de Dinechin; Victor Lezaud; Steven Derrien",
    "corresponding_authors": "",
    "abstract": "This work studies hardware-specific optimization opportunities currently unexploited by high-level synthesis compilers. Some of these optimizations are specializations of floating-point operations that respect the usual semantics of the input program without changing the numerical result. Some other optimizations, locally triggered by the programmer thanks to a pragma, assume a different semantics, where floating-point code is interpreted as the specification of computation with real numbers. The compiler is then in charge to ensure an application-level accuracy constraint expressed in the pragma and has the freedom to use non-standard arithmetic hardware when more efficient. These two classes of optimizations are prototyped in the GeCoS source-to-source compiler and evaluated on the Polybench and EEMBC benchmark suites. Latency is reduced by up to 93%, and resource usage is reduced by up to 58%.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2961292910",
    "type": "article"
  },
  {
    "title": "Effective Loop Fusion in Polyhedral Compilation Using Fusion Conflict Graphs",
    "doi": "https://doi.org/10.1145/3416510",
    "publication_date": "2020-09-30",
    "publication_year": 2020,
    "authors": "Aravind Acharya; Uday Bondhugula; Albert Cohen",
    "corresponding_authors": "",
    "abstract": "Polyhedral auto-transformation frameworks are known to find efficient loop transformations that maximize locality and parallelism and minimize synchronization. While complex loop transformations are routinely modeled in these frameworks, they tend to rely on ad hoc heuristics for loop fusion. Although there exist multiple loop fusion models with cost functions to maximize locality and parallelism, these models involve separate optimization steps rather than seamlessly integrating with other loop transformations like loop permutation, scaling, and shifting. Incorporating parallelism-preserving loop fusion heuristics into existing affine transformation frameworks like Pluto, LLVM-Polly, PPCG, and PoCC requires solving a large number of Integer Linear Programming formulations, which increase auto-transformation times significantly. In this work, we incorporate polynomial time loop fusion heuristics into the Pluto-lp-dfp framework. We present a data structure called the fusion conflict graph (FCG), which enables us to efficiently model loop fusion in the presence of other affine loop transformations. We propose a clustering heuristic to group the vertices of the FCG, which further enables us to provide three different polynomial time greedy fusion heuristics, namely, maximal fusion , typed fusion , and hybrid fusion , while maintaining the compile time improvements of Pluto-lp-dfp over Pluto. Our experiments reveal that the hybrid fusion model, in conjunction with Pluto’s cost function, finds efficient transformations that outperform PoCC and Pluto by mean factors of 1.8× and 1.07×, respectively, with a maximum performance improvement of 14× over PoCC and 2.6× over Pluto.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W3091055609",
    "type": "article"
  },
  {
    "title": "Understanding Cache Compression",
    "doi": "https://doi.org/10.1145/3457207",
    "publication_date": "2021-06-08",
    "publication_year": 2021,
    "authors": "Daniel Rodrigues Carvalho; André Seznec",
    "corresponding_authors": "",
    "abstract": "Hardware cache compression derives from software-compression research; yet, its implementation is not a straightforward translation, since it must abide by multiple restrictions to comply with area, power, and latency constraints. This study sheds light on the challenges of adopting compression in cache design—from the shrinking of the data until its physical placement. The goal of this article is not to summarize proposals but to put in evidence the solutions they employ to handle those challenges. An in-depth description of the main characteristics of multiple methods is provided, as well as criteria that can be used as a basis for the assessment of such schemes. It is expected that this article will ease the understanding of decisions to be taken for the design of compressed systems and provide directions for future work.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W3170866278",
    "type": "article"
  },
  {
    "title": "Online Application Guidance for Heterogeneous Memory Systems",
    "doi": "https://doi.org/10.1145/3533855",
    "publication_date": "2022-05-04",
    "publication_year": 2022,
    "authors": "M. Ben Olson; Brandon Kammerdiener; Michael R. Jantz; Kshitij Doshi; Terry Jones",
    "corresponding_authors": "",
    "abstract": "As scaling of conventional memory devices has stalled, many high-end computing systems have begun to incorporate alternative memory technologies to meet performance goals. Since these technologies present distinct advantages and tradeoffs compared to conventional DDR* SDRAM, such as higher bandwidth with lower capacity or vice versa, they are typically packaged alongside conventional SDRAM in a heterogeneous memory architecture. To utilize the different types of memory efficiently, new data management strategies are needed to match application usage to the best available memory technology. However, current proposals for managing heterogeneous memories are limited, because they either (1) do not consider high-level application behavior when assigning data to different types of memory or (2) require separate program execution (with a representative input) to collect information about how the application uses memory resources. This work presents a new data management toolset to address the limitations of existing approaches for managing complex memories. It extends the application runtime layer with automated monitoring and management routines that assign application data to the best tier of memory based on previous usage, without any need for source code modification or a separate profiling run. It evaluates this approach on a state-of-the-art server platform with both conventional DDR4 SDRAM and non-volatile Intel Optane DC memory, using both memory-intensive high-performance computing (HPC) applications as well as standard benchmarks. Overall, the results show that this approach improves program performance significantly compared to a standard unguided approach across a variety of workloads and system configurations. The HPC applications exhibit the largest benefits, with speedups ranging from 1.4× to 7× in the best cases. Additionally, we show that this approach achieves similar performance as a comparable offline profiling-based approach after a short startup period, without requiring separate program execution or offline analysis steps.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3203778346",
    "type": "article"
  },
  {
    "title": "E-BATCH: Energy-Efficient and High-Throughput RNN Batching",
    "doi": "https://doi.org/10.1145/3499757",
    "publication_date": "2022-01-23",
    "publication_year": 2022,
    "authors": "Franyell Silfa; José-María Arnau; Antonio González",
    "corresponding_authors": "",
    "abstract": "Recurrent Neural Network (RNN) inference exhibits low hardware utilization due to the strict data dependencies across time-steps. Batching multiple requests can increase throughput. However, RNN batching requires a large amount of padding since the batched input sequences may vastly differ in length. Schemes that dynamically update the batch every few time-steps avoid padding. However, they require executing different RNN layers in a short time span, decreasing energy efficiency. Hence, we propose E-BATCH, a low-latency and energy-efficient batching scheme tailored to RNN accelerators. It consists of a runtime system and effective hardware support. The runtime concatenates multiple sequences to create large batches, resulting in substantial energy savings. Furthermore, the accelerator notifies it when the evaluation of an input sequence is done. Hence, a new input sequence can be immediately added to a batch, thus largely reducing the amount of padding. E-BATCH dynamically controls the number of time-steps evaluated per batch to achieve the best trade-off between latency and energy efficiency for the given hardware platform. We evaluate E-BATCH on top of E-PUR and TPU. E-BATCH improves throughput by 1.8× and energy efficiency by 3.6× in E-PUR, whereas in TPU, it improves throughput by 2.1× and energy efficiency by 1.6×, over the state-of-the-art.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4207032740",
    "type": "article"
  },
  {
    "title": "Optimizing Small-Sample Disk Fault Detection Based on LSTM-GAN Model",
    "doi": "https://doi.org/10.1145/3500917",
    "publication_date": "2022-01-23",
    "publication_year": 2022,
    "authors": "Yufei Wang; Xiaoshe Dong; Longxiang Wang; Weiduo Chen; Xingjun Zhang",
    "corresponding_authors": "",
    "abstract": "In recent years, researches on disk fault detection based on SMART data combined with different machine learning algorithms have been proven to be effective. However, these methods require a large amount of data. In the early stages of the establishment of a data center or the deployment of new storage devices, the amount of reliability data for disks is relatively limited, and the amount of failed disk data is even less, resulting in the unsatisfactory detection performances of machine learning algorithms. To solve the above problems, we propose a novel small sample disk fault detection (SSDFD) 1 optimizing method based on Generative Adversarial Networks (GANs). Combined with the characteristics of hard disk reliability data, the generator of the original GAN is improved based on Long Short-Term Memory (LSTM), making it suitable for the generation of failed disk data. To alleviate the problem of data imbalance and expand the failed disk dataset with reduced amounts of original data, the proposed model is trained through adversarial training, which focuses on the generation of failed disk data. Experimental results on real HDD datasets show that SSDFD can generate enough virtual failed disk data to enable the machine learning algorithm to detect disk faults with increased accuracy under the condition of a few original failed disk data. Furthermore, the model trained with 300 original failed disk data has a significant effect on improving the accuracy of HDD fault detection. The optimal amount of generated virtual data are, 20–30 times that of the original data.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4207079035",
    "type": "article"
  },
  {
    "title": "MemHC: An Optimized GPU Memory Management Framework for Accelerating Many-body Correlation",
    "doi": "https://doi.org/10.1145/3506705",
    "publication_date": "2022-03-24",
    "publication_year": 2022,
    "authors": "Qihan Wang; Zhen Peng; Bin Ren; Jie Chen; Robert G. Edwards",
    "corresponding_authors": "",
    "abstract": "The many-body correlation function is a fundamental computation kernel in modern physics computing applications, e.g., Hadron Contractions in Lattice quantum chromodynamics (QCD). This kernel is both computation and memory intensive, involving a series of tensor contractions, and thus usually runs on accelerators like GPUs. Existing optimizations on many-body correlation mainly focus on individual tensor contractions (e.g., cuBLAS libraries and others). In contrast, this work discovers a new optimization dimension for many-body correlation by exploring the optimization opportunities among tensor contractions. More specifically, it targets general GPU architectures (both NVIDIA and AMD) and optimizes many-body correlation’s memory management by exploiting a set of memory allocation and communication redundancy elimination opportunities: first, GPU memory allocation redundancy : the intermediate output frequently occurs as input in the subsequent calculations; second, CPU-GPU communication redundancy : although all tensors are allocated on both CPU and GPU, many of them are used (and reused) on the GPU side only, and thus, many CPU/GPU communications (like that in existing Unified Memory designs) are unnecessary; third, GPU oversubscription: limited GPU memory size causes oversubscription issues, and existing memory management usually results in near-reuse data eviction, thus incurring extra CPU/GPU memory communications. Targeting these memory optimization opportunities, this article proposes MemHC, an optimized systematic GPU memory management framework that aims to accelerate the calculation of many-body correlation functions utilizing a series of new memory reduction designs. These designs involve optimizations for GPU memory allocation, CPU/GPU memory movement, and GPU memory oversubscription, respectively. More specifically, first, MemHC employs duplication-aware management and lazy release of GPU memories to corresponding host managing for better data reusability. Second, it implements data reorganization and on-demand synchronization to eliminate redundant (or unnecessary) data transfer. Third, MemHC exploits an optimized Least Recently Used (LRU) eviction policy called Pre-Protected LRU to reduce evictions and leverage memory hits. Additionally, MemHC is portable for various platforms including NVIDIA GPUs and AMD GPUs. The evaluation demonstrates that MemHC outperforms unified memory management by \\( 2.18\\times \\) to \\( 10.73\\times \\) . The proposed Pre-Protected LRU policy outperforms the original LRU policy by up to \\( 1.36\\times \\) improvement. 1",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4221100183",
    "type": "article"
  },
  {
    "title": "Reducing Minor Page Fault Overheads through Enhanced Page Walker",
    "doi": "https://doi.org/10.1145/3547142",
    "publication_date": "2022-07-11",
    "publication_year": 2022,
    "authors": "Chandrahas Tirumalasetty; Chih Chieh Chou; Narasimha Reddy; Paul V. Gratz; Ayman Abouelwafa",
    "corresponding_authors": "",
    "abstract": "Application virtual memory footprints are growing rapidly in all systems from servers down to smartphones. To address this growing demand, system integrators are incorporating ever larger amounts of main memory, warranting rethinking of memory management. In current systems, applications produce page fault exceptions whenever they access virtual memory regions that are not backed by a physical page. As application memory footprints grow, they induce more and more minor page faults. Handling of each minor page fault can take a few thousands of CPU cycles and blocks the application till the OS kernel finds a free physical frame. These page faults can be detrimental to the performance when their frequency of occurrence is high and spread across application runtime. Specifically, lazy allocation-induced minor page faults are increasingly impacting application performance. Our evaluation of several workloads indicates an overhead due to minor page faults as high as 29% of execution time. In this article, we propose to mitigate this problem through a hardware, software co-design approach. Specifically, we first propose to parallelize portions of the kernel page allocation to run ahead of fault time in a separate thread. Then we propose the Minor Fault Offload Engine (MFOE), a per-core hardware accelerator for minor fault handling. MFOE is equipped with a pre-allocated page frame table that it uses to service a page fault. On a page fault, MFOE quickly picks a pre-allocated page frame from this table, makes an entry for it in the TLB, and updates the page table entry to satisfy the page fault. The pre-allocation frame tables are periodically refreshed by a background kernel thread, which also updates the data structures in the kernel to account for the handled page faults. We evaluate this system in the gem5 architectural simulator with a modified Linux kernel running on top of simulated hardware containing the MFOE accelerator. Our results show that MFOE improves the average critical path fault handling latency by 33× and tail critical path latency by 51×. Among the evaluated applications, we observed an improvement of runtime by an average of 6.6%.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4285027960",
    "type": "article"
  },
  {
    "title": "Practical Software-Based Shadow Stacks on x86-64",
    "doi": "https://doi.org/10.1145/3556977",
    "publication_date": "2022-08-16",
    "publication_year": 2022,
    "authors": "Changwei Zou; Yaoqing Gao; Jingling Xue",
    "corresponding_authors": "",
    "abstract": "Control-Flow Integrity (CFI) techniques focus often on protecting forward edges and assume that backward edges are protected by shadow stacks. However, software-based shadow stacks that can provide performance, security, and compatibility are still hard to obtain, leaving an important security gap on x86-64. In this article, we introduce a simple, efficient, and effective parallel shadow stack design (based on LLVM), FlashStack , for protecting return addresses in single- and multi-threaded programs running under 64-bit Linux on x86-64, with three distinctive features. First, we introduce a novel dual-prologue approach to enable a protected function to thwart the TOCTTOU attacks, which are constructed by Microsoft’s red team and lead to the deprecation of Microsoft’s RFG. Second, we design a new mapping mechanism, Segment+Rsp-S , to allow the parallel shadow stack to be accessed efficiently while satisfying the constraints of arch_prctl() and ASLR in 64-bit Linux. Finally, we introduce a lightweight inspection mechanism, SideChannel-K , to harden FlashStack further by detecting entropy-reduction attacks efficiently and protecting the parallel shadow stack effectively with a 10-ms shuffling policy. Our evaluation on SPEC CPU2006 , Nginx, and Firefox shows that FlashStack can provide high performance, meaningful security, and reasonable compatibility for server- and client-side programs on x86-64.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4291972620",
    "type": "article"
  },
  {
    "title": "GraphTune: An Efficient Dependency-Aware Substrate to Alleviate Irregularity in Concurrent Graph Processing",
    "doi": "https://doi.org/10.1145/3600091",
    "publication_date": "2023-05-26",
    "publication_year": 2023,
    "authors": "Jin Zhao; Yu Zhang; Ligang He; Qikun Li; Xiang Zhang; Xinyu Jiang; Hui Yu; Xiaofei Liao; Hai Jin; Lin Gu; Haikun Liu; Bingsheng He; Ji Zhang; Xianzheng Song; Lin Wang; Jun Zhou",
    "corresponding_authors": "",
    "abstract": "With the increasing need for graph analysis, massive Concurrent iterative Graph Processing (CGP) jobs are usually performed on the common large-scale real-world graph. Although several solutions have been proposed, these CGP jobs are not coordinated with the consideration of the inherent dependencies in graph data driven by graph topology. As a result, they suffer from redundant and fragmented accesses of the same underlying graph dispersed over distributed platform, because the same graph is typically irregularly traversed by these jobs along different paths at the same time. In this work, we develop GraphTune , which can be integrated into existing distributed graph processing systems, such as D-Galois, Gemini, PowerGraph, and Chaos, to efficiently perform CGP jobs and enhance system throughput. The key component of GraphTune is a dependency-aware synchronous execution engine in conjunction with several optimization strategies based on the constructed cross-iteration dependency graph of chunks. Specifically, GraphTune transparently regularizes the processing behavior of the CGP jobs in a novel synchronous way and assigns the chunks of graph data to be handled by them based on the topological order of the dependency graph so as to maximize the performance. In this way, it can transform the irregular accesses of the chunks into more regular ones so that as many CGP jobs as possible can fully share the data accesses to the common graph. Meanwhile, it also efficiently synchronizes the communications launched by different CGP jobs based on the dependency graph to minimize the communication cost. We integrate it into four cutting-edge distributed graph processing systems and a popular out-of-core graph processing system to demonstrate the efficiency of GraphTune. Experimental results show that GraphTune improves the throughput of CGP jobs by 3.1∼6.2, 3.8∼8.5, 3.5∼10.8, 4.3∼12.4, and 3.8∼6.9 times over D-Galois, Gemini, PowerGraph, Chaos, and GraphChi, respectively.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4378470200",
    "type": "article"
  },
  {
    "title": "Accelerating Convolutional Neural Network by Exploiting Sparsity on GPUs",
    "doi": "https://doi.org/10.1145/3600092",
    "publication_date": "2023-05-27",
    "publication_year": 2023,
    "authors": "Weizhi Xu; Yintai Sun; Shengyu Fan; Hui Yu; Xin Fu",
    "corresponding_authors": "",
    "abstract": "The convolutional neural network (CNN) is an important deep learning method, which is widely used in many fields. However, it is very time consuming to implement the CNN where convolution usually takes most of the time. There are many zero values in feature maps and filters, which leads to redundant calculations and memory accesses if dense methods are used to compute convolution. Many works recently have made use of sparsity to skip the calculations for zero values to reduce the inference time of the CNN. On the graphics processing unit platform, current works cannot fully exploit the sparsity of the feature map and achieve satisfactory performance. Therefore, we design a new parallel strategy to transform the feature map into a new storage format to avoid the redundant computation of zero values on graphics processing units. Also considering the sparsity in the feature map, we propose a fused storage format to combine the convolution operation with the following pooling operation, to further improve the performance. We carry out experiments with mainstream CNN models and achieve better performance compared with cuDNN and cuSPARSE. For VGG-19, ResNet-50, DenseNet-121, and RegNetX-16GF, 1.97×, 2.23×, 2.74×, and 1.58× speedups respectively are obtained over cuDNN. The speedups over cuSPARSE respectively are 2.10×, 1.83×, 2.35×, and 1.35× when only using the first method.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4378573650",
    "type": "article"
  },
  {
    "title": "QoS-pro: A QoS-enhanced Transaction Processing Framework for Shared SSDs",
    "doi": "https://doi.org/10.1145/3632955",
    "publication_date": "2023-11-14",
    "publication_year": 2023,
    "authors": "Hao Fan; Y. Ye; Shadi Ibrahim; Zhuo Huang; Xingru Li; Weibin Xue; Song Wu; Yu Chen; Xuanhua Shi; Hai Jin",
    "corresponding_authors": "",
    "abstract": "Solid State Drives (SSDs) are widely used in data-intensive scenarios due to their high performance and decreasing cost. However, in shared environments, concurrent workloads can interfere with each other, leading to a violation of Quality of Service (QoS). While QoS mechanisms like fairness guarantees and latency constraints have been integrated into SSDs, existing transaction processing frameworks offer limited QoS guarantees and can significantly degrade overall performance in a shared environment. The reason is that the internal components of an SSD, originally designed to exploit parallelism, struggle to coordinate effectively when QoS mechanisms are applied to them. This article proposes a novel QoS -enhanced transaction pro cessing framework, called QoS-pro, which enhances QoS guarantees for concurrent workloads while maintaining high parallelism for SSDs. QoS-pro achieves this by redesigning transaction processing procedures to fully exploit the parallelism of shared SSDs and enhancing QoS-oriented transaction translation and scheduling with parallelism features in mind. In terms of fairness guarantees, QoS-pro outperforms state-of-the-art methods by achieving 96% fairness improvement and 64% maximum latency reduction. QoS-pro also shows almost no loss in throughput when compared with parallelism-oriented methods. Additionally, QoS-pro triggers the fewest Garbage Collection (GC) operations and minimally affects concurrently running workloads during GC operations.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4388657333",
    "type": "article"
  },
  {
    "title": "SAC: An Ultra-Efficient Spin-based Architecture for Compressed DNNs",
    "doi": "https://doi.org/10.1145/3632957",
    "publication_date": "2023-11-14",
    "publication_year": 2023,
    "authors": "Yunping Zhao; Sheng Ma; Heng Liu; Libo Huang; Yi Dai",
    "corresponding_authors": "",
    "abstract": "Deep Neural Networks (DNNs) have achieved great progress in academia and industry. But they have become computational and memory intensive with the increase of network depth. Previous designs seek breakthroughs in software and hardware levels to mitigate these challenges. At the software level, neural network compression techniques have effectively reduced network scale and energy consumption. However, the conventional compression algorithm is complex and energy intensive. At the hardware level, the improvements in the semiconductor process have effectively reduced power and energy consumption. However, it is difficult for the traditional Von-Neumann architecture to further reduce the power consumption, due to the memory wall and the end of Moore’s law. To overcome these challenges, the spintronic device based DNN machines have emerged for their non-volatility, ultra low power, and high energy efficiency. However, there is no spin-based design that has achieved innovation at both the software and hardware level. Specifically, there is no systematic study of spin-based DNN architecture to deploy compressed networks. In our study, we present an ultra-efficient Spin-based Architecture for Compressed DNNs (SAC), to substantially reduce power consumption and energy consumption. Specifically, we propose a One-Step Compression algorithm (OSC) to reduce the computational complexity with minimum accuracy loss. We also propose a spin-based architecture to realize better performance for the compressed network. Furthermore, we introduce a novel computation flow that enables the reuse of activations and weights. Experimental results show that our study can reduce the computational complexity of compression algorithm from 𝒪( Tk 3 to 𝒪( k 2 log k ), and achieve 14× ∼ 40× compression ratio. Furthermore, our design can attain a 2× enhancement in power efficiency and a 5× improvement in computational efficiency compared to the Eyeriss. Our models are available at an anonymous link https://bit.ly/39cdtTa .",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4388671982",
    "type": "article"
  },
  {
    "title": "Hardware-hardened Sandbox Enclaves for Trusted Serverless Computing",
    "doi": "https://doi.org/10.1145/3632954",
    "publication_date": "2023-11-14",
    "publication_year": 2023,
    "authors": "Joongun Park; Seunghyo Kang; Sanghyeon Lee; Taehoon Kim; Jongse Park; Youngjin Kwon; Jaehyuk Huh",
    "corresponding_authors": "",
    "abstract": "In cloud-based serverless computing, an application consists of multiple functions provided by mutually distrusting parties. For secure serverless computing, the hardware-based trusted execution environment (TEE) can provide strong isolation among functions. However, not only protecting each function from the host OS and other functions, but also protecting the host system from the functions, is critical for the security of the cloud servers. Such an emerging trusted serverless computing poses new challenges: Each TEE must be isolated from the host system bi-directionally, and the system calls from it must be validated. In addition, the resource utilization of each TEE must be accountable in a mutually trusted way. However, the current TEE model cannot efficiently represent such trusted serverless applications. To overcome the lack of such hardware support, this article proposes an extended TEE model called Cloister , designed for trusted serverless computing. Cloister proposes four new key techniques. First, it extends the hardware-based memory isolation in SGX to confine a deployed function only within its TEE (enclave). Second, it proposes a trusted monitor enclave that filters and validates system calls from enclaves. Third, it provides a trusted resource accounting mechanism for enclaves that is agreeable to both service developers and cloud providers. Finally, Cloister accelerates enclave loading by redesigning its memory verification for fast function deployment. Using an emulated Intel SGX platform with the proposed extensions, this article shows that trusted serverless applications can be effectively supported with small changes in the SGX hardware.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4388672093",
    "type": "article"
  },
  {
    "title": "WIPE: a Write-Optimized Learned Index for Persistent Memory",
    "doi": "https://doi.org/10.1145/3634915",
    "publication_date": "2023-11-28",
    "publication_year": 2023,
    "authors": "Zhonghua Wang; Chen Ding; Fengguang Song; Kai Lü; Jiguang Wan; Zhihu Tan; Changsheng Xie; Guokuan Li",
    "corresponding_authors": "",
    "abstract": "Learned Index, which utilizes effective machine learning models to accelerate locating sorted data positions, has gained increasing attention in many big data scenarios. Using efficient learned models, the learned indexes build large nodes and flat structures, thereby greatly improving the performance. However, most of the state-of-the-art learned indexes are designed for DRAM, and there is hence an urgent need to enable high-performance learned indexes for emerging Non-Volatile Memory (NVM). In this article, we first evaluate and analyze the performance of the existing learned indexes on NVM. We discover that these learned indexes encounter severe write amplification and write performance degradation due to the requirements of maintaining large sorted/semi-sorted data nodes. To tackle the problems, we propose a novel three-tiered architecture of write-optimized persistent learned index, which is named WIPE , by adopting unsorted fine-granularity data nodes to achieve high write performance on NVM. Thereinto, we devise a new root node construction algorithm to accelerate searching numerous small data nodes. The algorithm ensures stable flat structure and high read performance in large-size datasets by introducing an intermediate layer (i.e., index nodes) and achieving accurate prediction of index node positions from the root node. Our extensive experiments on Intel DCPMM show that WIPE can improve write throughput and read throughput by up to 3.9× and 7×, respectively, compared to the state-of-the-art learned indexes. Also, WIPE can recover from a system crash in ∼ 18 ms. WIPE is free as an open-source software package. 1",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4389098559",
    "type": "article"
  },
  {
    "title": "ApHMM: Accelerating Profile Hidden Markov Models for Fast and Energy-efficient Genome Analysis",
    "doi": "https://doi.org/10.1145/3632950",
    "publication_date": "2023-12-28",
    "publication_year": 2023,
    "authors": "Can Fırtına; Kamlesh Pillai; Gurpreet S. Kalsi; Bharathwaj Suresh; Damla Senol Cali; Jeremie S. Kim; Taha Shahroodi; Meryem Banu Cavlak; Joël Lindegger; Mohammed Alser; Juan Gómez-Luna; Sreenivas Subramoney; Onur Mutlu",
    "corresponding_authors": "",
    "abstract": "Profile hidden Markov models (pHMMs) are widely employed in various bioinformatics applications to identify similarities between biological sequences, such as DNA or protein sequences. In pHMMs, sequences are represented as graph structures, where states and edges capture modifications (i.e., insertions, deletions, and substitutions) by assigning probabilities to them. These probabilities are subsequently used to compute the similarity score between a sequence and a pHMM graph. The Baum-Welch algorithm, a prevalent and highly accurate method, utilizes these probabilities to optimize and compute similarity scores. Accurate computation of these probabilities is essential for the correct identification of sequence similarities. However, the Baum-Welch algorithm is computationally intensive, and existing solutions offer either software-only or hardware-only approaches with fixed pHMM designs. When we analyze state-of-the-art works, we identify an urgent need for a flexible, high-performance, and energy-efficient hardware-software co-design to address the major inefficiencies in the Baum-Welch algorithm for pHMMs. We introduce ApHMM , the first flexible acceleration framework designed to significantly reduce both computational and energy overheads associated with the Baum-Welch algorithm for pHMMs. ApHMM employs hardware-software co-design to tackle the major inefficiencies in the Baum-Welch algorithm by (1) designing flexible hardware to accommodate various pHMM designs, (2) exploiting predictable data dependency patterns through on-chip memory with memoization techniques, (3) rapidly filtering out unnecessary computations using a hardware-based filter, and (4) minimizing redundant computations. ApHMM achieves substantial speedups of 15.55×–260.03×, 1.83×–5.34×, and 27.97× when compared to CPU, GPU, and FPGA implementations of the Baum-Welch algorithm, respectively. ApHMM outperforms state-of-the-art CPU implementations in three key bioinformatics applications: (1) error correction, (2) protein family search, and (3) multiple sequence alignment, by 1.29×–59.94×, 1.03×–1.75×, and 1.03×–1.95×, respectively, while improving their energy efficiency by 64.24×–115.46×, 1.75×, and 1.96×.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4390317732",
    "type": "article"
  },
  {
    "title": "KINDRED: Heterogeneous Split-Lock Architecture for Safe Autonomous Machines",
    "doi": "https://doi.org/10.1145/3711924",
    "publication_date": "2025-01-13",
    "publication_year": 2025,
    "authors": "Yiming Gan; Jingwen Leng; Yu Bo; Yuhao Zhu",
    "corresponding_authors": "",
    "abstract": "With the increasing practicality of autonomous vehicles and drones, the importance of reliability requirements has escalated substantially. In many instances, traditional system designs tend to overlook reliability issues, emphasizing primarily on performance constraints. On the other hand, certain designers may opt for a lock-step (redundant) system design, duplicating every component, which in turn can result in significant performance, energy, and cost overheads. In software for autonomous machines, such as self-driving vehicles, performance degradation can increase reaction time, posing safety risks and reducing mission success rates. This paper introduces a novel multi-domain lock-step system design, Kindred , which places a strong emphasis on maximizing reliability while minimizing performance overhead. The proposed approach capitalizes on the inherent diversity in fault tolerance among various tasks within autonomous machine software, intelligently scheduling only the vulnerable nodes in the lock-domain. The primary challenge addressed in this study involves the intelligent task scheduling across different domains, complemented by efficient error detection and correction in the lock-domain. In a real system demonstration, we illustrate the effectiveness of Kindred , showcasing its ability to attain the same level of reliability as a full lock-step system while incurring only a mere 2.8% overhead, as opposed to a fully split system, indicating the advantages and potential of our multi-domain lock-step system design in achieving high reliability without compromising performance.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406302364",
    "type": "article"
  },
  {
    "title": "LIA: Latency-Improved Adaptive routing for Dragonfly networks",
    "doi": "https://doi.org/10.1145/3711914",
    "publication_date": "2025-01-13",
    "publication_year": 2025,
    "authors": "Mariano Benito; Enrique Vallejo; Ramón Beivide",
    "corresponding_authors": "",
    "abstract": "Low-diameter network topologies require non-minimal routing, such as Valiant routing, to avoid network congestion under challenging traffic patterns like the so-called adversarial. However, this mechanism tends to increase the average path length, base latency, and network load. The use of shorter non-minimal paths has the potential to enhance performance, but it may also introduce congestion depending on the traffic patterns. This paper introduces LIA (Latency-Improved Adaptive), a routing mechanism for Dragonfly networks which dynamically exploits minimal and non-minimal paths. LIA harnesses the traffic counters already present in contemporary switches to determine when it is safe to shorten non-minimal paths and to adjust routing decisions based on their information about the network conditions. Evaluations reveal that LIA achieves nearly optimal latency, outperforming state-of-the-art adaptive routing mechanisms by reducing latency by up to 30% while maintaining stable throughput and fairness.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406302383",
    "type": "article"
  },
  {
    "title": "Exploiting Dynamic Regular Patterns in Irregular Programs for Efficient Vectorization",
    "doi": "https://doi.org/10.1145/3716874",
    "publication_date": "2025-02-10",
    "publication_year": 2025,
    "authors": "Kelun Lei; Shaokang Du; Xin You; Hailong Yang; Zhongzhi Luan; Yi Liu; Depei Qian",
    "corresponding_authors": "",
    "abstract": "Modern optimizing compilers are able to exploit memory access or computation patterns to generate vectorized codes. However, such patterns in irregular programs are unknown until runtime due to the input dependence. Thus, either compiler’s static optimization or profile-guided optimization cannot represent the patterns for any common input, which leads to suboptimal vectorization. To address the above drawback, we propose DynVec , a framework to automatically exploit regular patterns buried deeply inside irregular programs and apply corresponding optimizations for better vectorization. Due to the integration of workload distribution and the ability to represent instruction features and identify regular patterns with effective feature extraction and data re-arranging methods, DynVec can generate highly efficient vectorized codes for both serial and parallel irregular programs by replacing gather / scatter / reduction operations with optimized operation groups. We evaluate DynVec on optimizing irregular programs such as SpMV and graph programs with representative sparse matrix datasets. The experiment results show that DynVec achieves significant speedup compared to the state-of-the-art implementations across a range of X86 and ARM platforms.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407318935",
    "type": "article"
  },
  {
    "title": "ScaWL: Scaling <i>k</i> -WL (Weisfeiler-Lehman) Algorithms in Memory and Performance on Shared and Distributed-Memory Systems",
    "doi": "https://doi.org/10.1145/3715124",
    "publication_date": "2025-02-10",
    "publication_year": 2025,
    "authors": "Coby Soss; Aravind Sukumaran-Rajam; Janet Layne; Edoardo Serra; Mahantesh Halappanavar; Assefaw H. Gebremedhin",
    "corresponding_authors": "",
    "abstract": "The k -dimensional Weisfeiler-Leman ( k -WL) algorithm—developed as an efficient heuristic for testing if two graphs are isomorphic—is a fundamental kernel for node embedding in the emerging field of graph neural networks. Unfortunately, the k -WL algorithm has exponential storage requirements, limiting the size of graphs that can be handled. This work presents a novel k -WL scheme with a storage requirement orders of magnitude lower while maintaining the same accuracy as the original k -WL algorithm. Due to the reduced storage requirement, our scheme allows for processing much bigger graphs than previously possible on a single compute node. For even bigger graphs, we provide the first distributed-memory implementation. Our k -WL scheme also has significantly reduced communication volume and offers high scalability. Our experimental results demonstrate that our approach is significantly faster and has superior scalability compared to five other implementations employing state-of-the-art techniques.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407319771",
    "type": "article"
  },
  {
    "title": "Ceiba: An Efficient and Scalable DNN Scheduler for Spatial Accelerators",
    "doi": "https://doi.org/10.1145/3715123",
    "publication_date": "2025-02-10",
    "publication_year": 2025,
    "authors": "Fuyu Wang; Minghua Shen; Yutong Lu; Nong Xiao",
    "corresponding_authors": "",
    "abstract": "Spatial accelerators are domain-specific architectures to elevate performance and energy efficiency for deep neural networks (DNNs). They also bring a large number of schedule parameters to determine computation and data movement patterns of DNNs. Previous works formulate the schedule problem as design space exploration or integer linear programming. However, these advanced techniques face the challenge of efficiency or scalability. In this paper, we propose Ceiba, which is a deep reinforcement learning-based DNN scheduler for spatial accelerators. Ceiba observes the running DNN computation as well as the spatial architecture to make schedule decisions. Then, Ceiba receives a reward to learn and produce the best-fit policy. To provide efficient and scalable scheduling, Ceiba constructs a DNN-architecture-specific action space. It is defined by upper and lower bounds to exclude invalid and sub-optimal schedule candidates. Extensive experiments demonstrate that Ceiba generally provides better performance for spatial accelerators under a fixed number of searching steps or a fixed amount of time. Specifically, Ceiba achieves an average 2.2 × speedup for the Simba accelerator, compared with the state-of-the-art scheduler. When scaling the batch size and the hardware architecture up by 64 ×, the performance gains of Ceiba are 1.8 × and 1.2 × on average, respectively. Moreover, Ceiba exhibits better scalability for the Eyeriss accelerator.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407319928",
    "type": "article"
  },
  {
    "title": "ARACHNE: Optimizing Distributed Parallel Applications with Reduced Inter-Process Communication",
    "doi": "https://doi.org/10.1145/3716871",
    "publication_date": "2025-02-11",
    "publication_year": 2025,
    "authors": "Yifu He; Han Zhao; Weihao Cui; Shulai Zhang; Quan Chen; Minyi Guo",
    "corresponding_authors": "",
    "abstract": "In high-performance computing (HPC), parallelization is essential for improving computational efficiency as data and computation scales exceed single-node capacity. Existing methods, such as the polyhedral model used in Pluto -Distmem, focus on loop and array optimizations within shared memory but struggle with high communication overheads and inflexibility in distributed environments. These methods often fail to effectively partition computation and manage data across nodes, leading to suboptimal performance. This paper presents Arachne , an innovative system designed to address these shortcomings by generating distributed parallel code with minimized communication overhead. The system introduces a dynamic programming algorithm to optimally distribute computational tasks across multiple processes, ensuring minimal communication costs. It also incorporates user-friendly compiler directives, allowing programmers to influence code generation easily and accommodate a broader range of parallelization scenarios without needing in-depth knowledge of parallel architectures. Arachne significantly reduces the learning curve and need for extensive code modifications, making parallel programming more accessible and efficient. Evaluation of various HPC benchmarks demonstrates that Arachne outperforms existing methods by reducing communication overhead, lowering memory requirements, and supporting more complex parallel logic, thus enhancing the overall scalability and efficiency of HPC applications.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407371698",
    "type": "article"
  },
  {
    "title": "TransCL: An Automatic CUDA-to-OpenCL Programs Transformation Framework",
    "doi": "https://doi.org/10.1145/3718987",
    "publication_date": "2025-02-20",
    "publication_year": 2025,
    "authors": "Changqing Shi; Yufei Sun; Rui Chen; Jiahao Wang; Qiang Guo; Chunye Gong; Yicheng Sui; Ya Li Jin; Yuzhi Zhang",
    "corresponding_authors": "",
    "abstract": "With the rising demand for computational power and the increasing variety of computational scenarios, considerable interest has emerged in transforming existing CUDA programs into more general-purpose OpenCL programs, enabling them to run across diverse hardware platforms. However, manual methods, typically designed for specific applications, lack flexibility. Current automated conversion techniques also face considerable challenges, particularly in handling diverse programming interfaces, memory management, etc., and are insufficient for converting large-scale, complex CUDA projects. In this paper, we propose a novel source-to-source program transformation framework, TransCL, which automates the conversion of CUDA programs in four key aspects: source code, execution model, programming model and memory model. To achieve this, we abstract a set of conversion rules aligned with the latest CUDA standards, develop a transcoder, implement an OpenCL-compatible programming interface library, and establish a memory mapping mechanism between CUDA and OpenCL. Experiments demonstrate that TransCL provides a high level of automation in converting CUDA-based applications and is effective in handling large, complex projects such as TensorFlow. Moreover, the converted AI framework successfully conducted model training for the first time. The experiment also validates that the converted program can execute correctly across multiple platforms and demonstrate good performance.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407784865",
    "type": "article"
  },
  {
    "title": "Towards Optimizing Learned Index for High Performance, Memory Efficiency and NUMA Awareness",
    "doi": "https://doi.org/10.1145/3736168",
    "publication_date": "2025-06-12",
    "publication_year": 2025,
    "authors": "Lixiao Cui; K. C. Yang; Yusen Li; Gang Wang; Xiaoguang Liu",
    "corresponding_authors": "",
    "abstract": "Learned indexes provide significant performance advantages over classical ordered indexes. However, current learned indexes face challenges regarding tradeoffs between performance and space, as well as scalability issues in platforms with multiple NUMA nodes. These limitations hinder the practical application of learned indexes in production environments. This paper presents DiffLex, a learned index with high-performance, memory-efficiency and NUMA-awareness. The core design of DiffLex is to perform differentiated management based on the popularity of data. For optimal performance, DiffLex stores newly inserted data in sparse delta arrays and frequently accessed data in sparse hot cache arrays. However, for cold data that occupy a majority of the storage space, DiffLex stores them in dense arrays and conducts compression to reduce memory costs. DiffLex ensures NUMA-awareness by partitioning sparse deltas and replicating the hot cache arrays across multiple NUMA nodes. Additionally, we propose a persistent version of DiffLex tailored for emerging persistent memory devices. Our evaluation results demonstrate that DiffLex achieving 3.88x and 1.82x performance improvements compared to state-of-the-art learned indexes, while maintaining a compact index size.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411242349",
    "type": "article"
  },
  {
    "title": "Scheduling Language Chronology: Past, Present, and Future",
    "doi": "https://doi.org/10.1145/3743135",
    "publication_date": "2025-06-12",
    "publication_year": 2025,
    "authors": "Mary Hall; Cosmin E. Oancea; Anne C. Elster; Ari Rasch; Sameeran Joshi; Amir Mohammad Tavakkoli; Richard Schulze",
    "corresponding_authors": "",
    "abstract": "Scheduling languages express to a compiler – or equivalently, a code generator – a sequence of optimizations to apply. Performance tools that support a scheduling language interface allow exploration of optimizations, i.e., exploratory compilers . While scheduling languages have become a common feature of tools for experts, the proliferation of these languages without unifying common features may be confusing to users. Moreover, we recognize a need to organize the compiler developer community around common exploratory compiler infrastructure, and future advances to address, for example, data layout and data movement. To support a broader set of users may require raising the level of abstraction. This paper provides a chronology of scheduling languages, discussing their origins in iterative compilation and autotuning, noting the common features that are used in existing frameworks, and calling for changes to increase their utility and portability.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411292544",
    "type": "article"
  },
  {
    "title": "3D GNLM: Efficient 3D Non-Local Means Kernel with Nested Reuse Strategies for Embedded GPUs",
    "doi": "https://doi.org/10.1145/3744909",
    "publication_date": "2025-06-16",
    "publication_year": 2025,
    "authors": "Xiang Li; Qiong Chang; Yun Li; Jun Miyazaki",
    "corresponding_authors": "",
    "abstract": "The 3D Non-Local Means (NLM) algorithm has become a crucial preprocessing technique for 3D image data sets due to its effectiveness in denoising while preserving fine details. This method has been proven to be highly efficient in high-demand tasks within industrial applications such as medical imaging and remote sensing. The 3D NLM algorithm computes the filtered value for each voxel by calculating the weighted average of all voxels within a 3D search window, where the weights are determined by the similarity between pairs of 3D template windows. Therefore, the computational burden becomes significant, especially in embedded GPUs with limited computational power and memory resources. To address this issue, we propose an efficient GPU parallel kernel to minimize redundant computations and memory accesses. The kernel integrates three nested reuse strategies to handle redundant computations in three dimensions: for columns, we leverage the fast data exchange mechanism to reuse column computation results via on-chip registers; for rows, we use a sliding window strategy, utilizing GPU global memory as an intermediary to store and reuse similarity values between filtered rows; and for channels, we introduce a zigzag scanning strategy that enables simultaneous computation across multiple channels and employs on-chip registers to facilitate channel computation reuse. Experimental results demonstrate that our kernel achieves an average speedup of 7.7x on the embedded Jetson AGX Xavier platform across a range of 3D image data sets compared to existing methods, showcasing exceptional performance.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411350482",
    "type": "article"
  },
  {
    "title": "CGCGraph: Efficient CPU-GPU Co-execution for Concurrent Dynamic Graph Processing",
    "doi": "https://doi.org/10.1145/3744904",
    "publication_date": "2025-06-16",
    "publication_year": 2025,
    "authors": "Yiming Sun; Jie Zhang; Huawei Cao; Yuan Zhang; Xuejun An; Junying Huang; Xiaochun Ye",
    "corresponding_authors": "",
    "abstract": "With the continuous growth of user scale and application data, the demand for large-scale concurrent graph processing is increasing. Typically, large-scale concurrent graph processing jobs need to process corresponding snapshots of dynamically changing graph data to obtain information at different time points. To enhance the throughput of such applications, current solutions concurrently process multiple graph snapshots on the GPU. However, when dealing with rapidly changing graph data, transferring multiple snapshots of concurrent jobs to the GPU results in high data transfer overhead between CPU and GPU. Additionally, the execution mode of existing work suffers from underutilization of GPU computational resources. In this work, we introduce CGCGraph, which can be integrated into existing GPU graph processing systems like Subway, to enable efficient concurrent graph snapshot processing jobs and enhance overall system resource utilization. The key idea is to offload unshared graph data of multiple concurrent snapshots to the CPU, reducing CPU-GPU transfer overhead. By implementing CPU-GPU co-execution, there is potential for enhanced utilization of GPU computing resources. Specifically, CGCGraph leverages kernel fusion to process shared graph data concurrently on the GPU, while executing all snapshots in parallel on the CPU, with each snapshot assigned a dedicated thread. This approach enables efficient concurrent processing within a novel CPU-GPU co-execution model, incorporating three optimization strategies targeting storage, computation, and synchronization. We integrate CGCGraph with Subway, an existing system designed for out-of-GPU-memory static graph processing. Experimental results show that the integration of CGCGraph with current GPU-based systems obtains performance improvements ranging from 1.7 to 4.5 times.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411350565",
    "type": "article"
  },
  {
    "title": "A Two-Stage Degradation-Based Topology Reconfiguration Algorithm for Fault-Tolerant Multiprocessor Arrays",
    "doi": "https://doi.org/10.1145/3744907",
    "publication_date": "2025-06-16",
    "publication_year": 2025,
    "authors": "Hao Ding; Ping Song; Y. Li; Junyan Qian",
    "corresponding_authors": "",
    "abstract": "As the integration density of multiprocessor arrays increases, the likelihood of permanent faults in processing elements (PEs) rises, requiring effective topology reconfiguration for system reliability. However, existing router-based multiprocessor arrays reconfiguration methods predominantly rely on redundancy techniques and lack effective degradation strategies for applications of varying sizes. To address this, we propose a two-stage degradation-based topology reconfiguration algorithm to construct a maximized and high-performance logical array. First, we introduce a novel fault compensation mechanism by defining a set of faulty PE candidates to identify locally optimal fault-free PEs for compensation, minimizing the compensation path. Building upon this, we develop a greedy bidirectional column reconfiguration algorithm that constructs an initial fault-free logical array with short interconnects and prove its maximality. Lastly, we propose a satisfiability-based reconfiguration algorithm, transforming the topology reconfiguration problem into a satisfiability problem via a SAT model, reducing interconnect redundancy, and further optimizing array performance. Experimental results demonstrate that the proposed algorithm consistently outperforms state-of-the-art methods in reducing communication latency and alleviating link congestion, especially under high fault density conditions. Furthermore, as array size and fault density increase, the effectiveness of the proposed method becomes more pronounced, showcasing excellent scalability and robustness.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411350604",
    "type": "article"
  },
  {
    "title": "EDAS: Enabling Fast Data Loading for GPU Serverless Computing",
    "doi": "https://doi.org/10.1145/3743137",
    "publication_date": "2025-06-17",
    "publication_year": 2025,
    "authors": "Han Zhao; Weihao Cui; Quan Chen; Zijun Li; Zhenhua Han; Nan Wang; Yu Feng; Jieru Zhao; Chen Chen; Jingwen Leng; Minyi Guo",
    "corresponding_authors": "",
    "abstract": "Integrating GPUs into serverless computing platforms is crucial for improving efficiency. Many GPU functions, such as DNN inferences and scientific services, benefit from GPU usage, which requires only tens to hundreds of milliseconds for pure computation. Under these circumstances, fast data loading is imperative for function performance. However, existing GPU serverless systems face significant data stall issues, leading to extremely low GPU efficiency. Faced with the above problems, we observe opportunities to optimize data loading, such as data preloading and deduplicated data loading. However, these optimizations are impossible in existing GPU serverless systems due to the lack of insights into data information, such as data sizes and read-write attributes of function inputs. To address this, we propose a novel GPU serverless system, EDAS. EDAS first enhances user request specifications, allowing users to annotate data retrieved by GPU functions from the database with additional attributes. Based on this, EDAS takes over data loading from GPU functions and proposes two innovative data loading management schemes: a parallelized data loading scheme and a multi-stage resource exit scheme. Our experimental results show that EDAS reduces function duration by 16.2 × and improves system throughput by 1.91 × compared to the state-of-the-art serverless platform.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411378203",
    "type": "article"
  },
  {
    "title": "Partitioned Scheduling and Analysis for a Typed DAG Task on Heterogeneous Multi-Cores",
    "doi": "https://doi.org/10.1145/3746232",
    "publication_date": "2025-07-24",
    "publication_year": 2025,
    "authors": "Yulong Wu; Yehan Ma; Mingdong Xie; Weizhe Zhang",
    "corresponding_authors": "",
    "abstract": "Heterogeneous multi-core architectures are gaining popularity in recent years as they combine the benefits of different processors, resulting in improved execution capacity and energy efficiency. However, analyzing response times and allocating resources for the typed directed acyclic graph (DAG) task, which has complex execution logic, on heterogeneous multi-core systems poses significant challenges. Major approaches may yield overly pessimistic worst-case response time (WCRT) estimates in certain scenarios while failing to adequately address critical structural characteristics inherent to typed DAG tasks. To address these limitations, this paper explores the WCRT analysis and core allocations for the typed DAG task under partitioned scheduling. In this work, we first delve into the characteristics of the topology structure of the typed DAG task and propose a novel WCRT upper bound to enhance the accuracy of WCRT analysis. Then, a subtask allocation strategy is presented, which enables an effectively utilization of the resources of multi-cores. Finally, the performance of the proposed analysis algorithm and allocation strategy are tested by implementing a verification system on a real heterogeneous multi-core platform. Experimental results demonstrate that our proposed WCRT analysis algorithm exhibits substantial improvements of \\(38.7\\% \\) and \\(37.43\\% \\) in the theoretical analysis performance and actual analysis accuracy, respectively. Similarly, our proposed core allocation strategy improves the theoretical and the actual execution efficiency of the system by \\(10.6\\% \\) and \\(7.41\\% \\) , respectively. These results substantiate the practical value of our enhanced WCRT derivation methodology and allocation scheme in improving system resource utilization efficiency.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412636545",
    "type": "article"
  },
  {
    "title": "TRACED: A Temporal Graph Neural Networks-based Model for Data Prefetching",
    "doi": "https://doi.org/10.1145/3747843",
    "publication_date": "2025-08-06",
    "publication_year": 2025,
    "authors": "He Jiang; L.M. Fu; Dong Liu; Zhilei Ren; Yuting Chen; Lei Qiao",
    "corresponding_authors": "",
    "abstract": "In modern microarchitectures, machine-learning-based prefetchers use past memory requests to learn access patterns and predict memory addresses, thereby prefetching data into the cache to mitigate the processor-memory speed gap. However, they face two key challenges in capturing irregular access patterns generated by complex data structures and algorithms. One is data dispersion: the disorderliness of memory addresses makes it difficult for prefetchers to extract meaningful data features. The other is temporal and spatial complexity: existing prefetchers fail to effectively learn temporal and spatial characteristics, and thus are unable to explore more complex access patterns. To resolve these challenges, we propose TRACED, a novel temporal graph neural network-based prefetcher aimed at learning access patterns of memory addresses. TRACED consists of two key components: a dynamic clustering component and a temporal graph neural network component. In the dynamic clustering component, we introduce a similarity function to quantify the similarity of memory addresses. Based on the quantified similarity, we dynamically group unordered memory addresses into different clusters. This ensures that the memory addresses in each cluster are ordered and change smoothly, thus resolving the first challenge. The temporal graph neural network component constructs a spatiotemporal graph to represent relationships among memory addresses. This helps capture temporal and spatial characteristics both across and within clusters, thus resolving the second challenge. This paper demonstrates the effectiveness of the proposed prefetcher through experiments. Specifically, in terms of accuracy, TRACED outperforms BO, SPP, DOMINO, Delta-LSTM, and VOYAGER by 2.29%-40.83% on average. Furthermore, TRACED attains remarkable coverage of 55.67% and IPC of 43.75%, outperforming all competing approaches in both metrics.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413023281",
    "type": "article"
  },
  {
    "title": "A Low-latency On-chip Cache Hierarchy for Load-to-use Stall Reduction in GPUs",
    "doi": "https://doi.org/10.1145/3760782",
    "publication_date": "2025-08-18",
    "publication_year": 2025,
    "authors": "Negin Mahani; Hajar Falahati; Sina Darabi; Ahmad Javadi-Nezhad; Yunho Oh; Mohammad Sadrosadati; Hamid Sarbazi‐Azad; Babak Falsafi",
    "corresponding_authors": "",
    "abstract": "Memory hierarchy in Graphics Processing Units (GPUs) is conventionally designed to provide high bandwidth rather than low latency. In particular, because of the high tolerance to load-to-use latency (i.e., the time that warps wait for data fetched by memory loads), GPU L1D caches are optimized for density, capacity, and low power with latencies that are often orders of magnitude longer than conventional CPU caches. However, there are many important classes of data-parallel applications (e.g., graph, tree, priority queue processing, and sparse deep learning applications) that benefit from lower load-to-use latency than that offered by modern GPUs due to their inherent divergence and low effective Thread-Level Parallelism (TLP). This paper introduces an innovative on-chip cache hierarchy that incorporates a decoupled L1D cache with reduced latency (LoTUS) and its management scheme. LoTUS is a minimally sized fully associative cache placed in each GPU subcore that captures the primary working set of data-parallel applications. It exploits conventional high-performance low-density SRAM cells and dramatically reduces load-to-use latency. We also propose an intelligent extension of LoTUS, called LoTUSage, which employs a lightweight learning-based model to predict the utility of caching requests in LoTUS. Evaluation results show that LoTUS and LoTUSage improve the average performance by 23.9% and 35.4% and reduce the average energy consumption by 27.8% and 38.5%, respectively, for the applications suffering from high load-to-use stalls with negligible area and power overheads.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413286977",
    "type": "article"
  },
  {
    "title": "Q-Infer: Towards Efficient GPU-CPU Collaborative LLM Inference via Sparsity-Aware Dynamic Scheduling",
    "doi": "https://doi.org/10.1145/3764589",
    "publication_date": "2025-08-28",
    "publication_year": 2025,
    "authors": "Kai Lü; Qiang Wei; Yan Lin; P. X. Liu; Haoran Wang; Jiguang Wan; Ting Yao; Huatao Wu; Daohui Wang",
    "corresponding_authors": "",
    "abstract": "Large Language Models (LLMs) have sparked a new wave of exciting AI applications, yet their large model size imposes significant computational and storage costs during inference. Offloading parameters to the CPU and conducting GPU-CPU collaborative inference is a highly cost-effective strategy to alleviate GPU memory constraints. However, current solutions struggle to balance latency and throughput, and suffer from accuracy loss and performance fluctuations under various workloads and configurations. In this paper, we propose Q-Infer, an efficient GPU-CPU collaborative inference system that significantly improves the performance and quality of LLM inference through several optimizations: 1) Q-Infer designs a dynamic caching strategy for important parameters by exploiting model sparsity and locality. 2) Q-Infer proposes a multi-window-based approach for selecting important tokens, which reduces the KV cache while maintaining high accuracy. 3) Q-Infer develops a novel GPU-CPU collaborative inference and dynamic scheduling strategy to enhance performance across different environments. We evaluate Q-Infer using various models and workloads across different hardware configurations. The results demonstrate Q-Infer’s superior inference performance while retaining model accuracy compared to state-of-the-art GPU-CPU systems.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413792188",
    "type": "article"
  },
  {
    "title": "JUNO++: Optimizing ANNS and Enabling Efficient Sparse Attention in LLM via Ray Tracing Core",
    "doi": "https://doi.org/10.1145/3768585",
    "publication_date": "2025-09-18",
    "publication_year": 2025,
    "authors": "Zihan Liu; Wentao Ni; Jingwen Leng; Yu Feng; Cong Guo; Quan Chen; Chao Li; Minyi Guo; Yufei Ma; Feng Zhang; Yun Liang",
    "corresponding_authors": "",
    "abstract": "Approximate Nearest Neighbor Search (ANNS) is a fundamental technique in modern intelligent applications, including recommendation systems and vector databases. With the advent of large language models (LLMs), ANNS plays a critical role in enabling attention pruning mechanism that exploit the sparsity of attention, such as top-K attention and retrieval attention. As a result, the efficiency of ANNS has become increasingly crucial. In this paper, we identify a key inefficiency in state-of-the art ANNS methods based on product quantization: the redundant computation and accumulation of pairwise distance with codebook. To address this, we propose JUNO++ , the system consists of i) an end-to-end ANNS search pipeline based on ray-tracing core leveraging sparsity-aware algorithm and ii) an integration of the ray-tracing based ANNS search pipeline to the attention computation. For ANNS search pipeline, evaluation on four datasets indicate 2.2x to 8.5x search throughput improvement. For ANNS-powered sparse attention, JUNO++ achieves a 46% reduction in latency of q × k ⊤ calculation comparing to the baseline with almost identical accuracy, which is not only a key component of retrieval-based sparse attention, but also the dominant component in long-context scenario, implying a considerable end-to-end improvement.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414332403",
    "type": "article"
  },
  {
    "title": "HOPPERFISH: Holistic Profiling with Portable Extensible and Robust Framework Intended for Systems with Heterogeneity",
    "doi": "https://doi.org/10.1145/3769087",
    "publication_date": "2025-09-20",
    "publication_year": 2025,
    "authors": "Mustafa Ghanim; Serhan Gener; H. Umut Suluhan; Parker Dattilo; Ali Akoglu",
    "corresponding_authors": "",
    "abstract": "We introduce HOPPERFISH , a holistic profiling framework that unifies analysis across the application, runtime, microarchitecture, and hardware layers to streamline robust feature correlation in heterogeneous computing systems. HOPPERFISH provides comprehensive insights into dynamic workloads, hardware configurations, and scheduling policies by capturing features across the system stack for any heterogeneous System on Chip (SoC), whether it is an off-the-shelf platform or an architecture emulated on an FPGA. The framework enables data-driven analysis for real-world applications and unsupervised learning for heterogeneous systems where features vary dynamically and cannot be labeled in advance. As a use case, we utilize HOPPERFISH in the anomaly detection context for heterogeneous systems and build an autoencoder model under varying workload, hardware, and scheduling scenarios without the need to retrain separate models for each scenario. HOPPERFISH extracts correlations among features across the entire system stack, which leads to requiring a smaller number of parameters to build a representative model for anomaly detection. This compressed form leads to the implementation of a robust and accurate yet lightweight model that can detect abnormal behaviors in real-time based on unsupervised behavior analysis. As a proof of concept, we also demonstrate hardware deployment of the real-time anomaly detection model with a latency of 18 μ s on MPSoC ZCU102 FPGA consuming 6.3 μ J, achieving 16.67 × lower latency and 117 × less energy consumption compared to its software implementation. HOPPERFISH is open‐source and accompanied by an extensible workload benchmark suite, facilitating broader research tasks for the development of future heterogeneous computing systems, including security analysis, fault diagnosis, and data‐driven design optimization decisions.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414371542",
    "type": "article"
  },
  {
    "title": "Toward Comprehensive Design Space Exploration on Heterogeneous Multi-core Processors",
    "doi": "https://doi.org/10.1145/3770080",
    "publication_date": "2025-09-30",
    "publication_year": 2025,
    "authors": "Duo Wang; Mingyu Yan; Dengke Han; Xiaochun Ye; Dongrui Fan",
    "corresponding_authors": "",
    "abstract": "The theoretical maximum efficiency of heterogeneous multi-core processors (HMPs) is fundamentally determined by their hardware designs. Nonetheless, the achieved efficiency of HMPs is significantly influenced by the strategy employed in resource management (RM). To alleviate the time-consuming nature of comprehensive design space exploration for HMPs, existing work typically follows a route that independently explores HMP designs and RM strategies. However, this route tends to restrict the overall exploration space and efficiency. In this work, we introduce HetDSE, a comprehensive design space exploration framework for HMP design that coordinately explores both HMP designs and RM strategies. We first identify requirements that enable comprehensive exploration, including high-quality design space, fast evaluation, and high-accuracy exploration. To meet these requirements, we propose representative-based design space generation, unified prediction-based evaluation, and iterative refinement exploration methods to achieve comprehensive exploration within a reasonable timeframe. Experimental results demonstrate that HetDSE reduces energy delay square product by 88% compared to the state-of-the-art framework.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414664803",
    "type": "article"
  },
  {
    "title": "GTSM: A multi-edge-centric temporal subgraph matching framework on GPUs",
    "doi": "https://doi.org/10.1145/3771286",
    "publication_date": "2025-10-09",
    "publication_year": 2025,
    "authors": "Jiezhong He; Menghan Jia; Yixin Chen; Zhouyang Liu; Dongsheng Li",
    "corresponding_authors": "",
    "abstract": "Temporal subgraph matching aims to identify subgraphs in temporal networks that satisfy both structural and temporal constraints, with applications ranging from social network analysis to fraud detection. As this NP-hard problem involves massive computation on large graphs, GPU acceleration becomes critical. However, existing edge-centric approaches suffer from computational redundancy, inefficient memory management, and limited scalability on large graphs, hindering efficient GPU acceleration. To address these challenges, we propose GTSM 1 , a GPU-optimized temporal subgraph matching system featuring three innovations: (1) A multi-edge-centric paradigm that reduces redundant search space through multi-edge compressions along with an efficient decompression algorithm; (2) A memory-bound optimization that maximizes GPU resource utilization. (3) A heterogeneous BFS-DFS execution model where CPU performs Breadth-First Search (BFS) to ensure load balancing across GPUs. Experiments demonstrate that GTSM achieves a 5.5 × -93.2 × speedup over the state-of-the-art GPU systems, while solving 10%-40% more queries. With our heterogeneous execution model, our system achieves near-linear scaling in multi-GPU configurations.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414982164",
    "type": "article"
  },
  {
    "title": "Fragmentation Harmonization for the Arm Ecosystem: A Unified Method to Measure Memory Bandwidth via Network-on-Chip",
    "doi": "https://doi.org/10.1145/3772287",
    "publication_date": "2025-10-21",
    "publication_year": 2025,
    "authors": "Tongyu Liu; Hung-Hsiang Cheng; E Erqi; Ning Li; Haoyu Liao; Bo Huang; Jianmei Guo",
    "corresponding_authors": "",
    "abstract": "Arm-based platforms have significant hardware fragmentation issues due to the nature of Intellectual Property (IP) licensing. It is common for hardware vendors to customize memory controllers. Conventional memory bandwidth measurements based on memory controllers’ performance monitoring units (PMUs) face numerous obstacles, including the lack of kernel driver support and official documentation. To this end, we lift the perspective to the more general and upper level, i.e., the Network-on-Chip (NoC) level, instead of relying on various customized memory controllers. We propose a unified method for measuring memory bandwidth based on NoC traffic monitoring. Through a purely PMU-data-driven detection, our method can automatically reveal the hidden physical locations of memory controllers. By monitoring the upstream and downstream data traffic of the ports where memory controllers are connected on the NoC, our method can achieve accurate read and write memory bandwidth measurements without accessing memory controller PMUs, thus achieving fragmentation harmonization. We summarize all memory bandwidth methods currently applicable to Arm-based platforms and conduct a systematical evaluation. The experimental results show that our proposed method has an average error of 1.05% compared to the oracle answer, which can serve as a generic method to measure memory bandwidth for Arm-based SoCs with vendor-customized memory controllers.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415382272",
    "type": "article"
  },
  {
    "title": "Unified and Efficient Factor Graph Accelerator Design for Robotic Optimization",
    "doi": "https://doi.org/10.1145/3771846",
    "publication_date": "2025-10-22",
    "publication_year": 2025,
    "authors": "Qiang Liu; Yihao Hua; Yuhui Hao; Yu Bo; Shaoshan Liu; Yiming Gan",
    "corresponding_authors": "",
    "abstract": "Despite extensive efforts, existing approaches to design accelerators for optimization-based robotic applications have limitations related to insufficient real-time performance and high energy consumption. Some methods focus on designing general-purpose matrix computation units, but fail to consider specific characteristics of robotic algorithms. Other methods aim to design dedicated accelerators that achieve excellent performance but suffer from limited flexibility. To balance between general-purpose and specialized designs, this paper proposes a hardware accelerator that, through a unified pose representation and factor graph abstraction, can solve nonlinear optimization algorithms for localization, planning, and control on the same piece of circuits. Through carefully designed pipeline, circuit structure optimization, fixed-point arithmetic, and sparse data compression, the accelerator design achieves high performance and high energy efficiency. The experimental results on FPGA demonstrate that compared to state-of-the-art acceleration solutions, our design achieves up to 107.9 × speedup, 7.2 × energy reduction, while achieving similar accuracy.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415423868",
    "type": "article"
  },
  {
    "title": "Reducing instruction cache energy consumption using a compiler-based strategy",
    "doi": "https://doi.org/10.1145/980152.980154",
    "publication_date": "2004-03-01",
    "publication_year": 2004,
    "authors": "W. Zhang; Jie Hu; V. Degalahal; Mahmut Kandemir; N. Vijaykrishnan; M. Irwin",
    "corresponding_authors": "",
    "abstract": "Excessive power consumption is widely considered as a major impediment to designing future microprocessors. With the continued scaling down of threshold voltages, the power consumed due to leaky memory cells in on-chip caches will constitute a significant portion of the processor's power budget. This work focuses on reducing the leakage energy consumed in the instruction cache using a compiler-directed approach.We present and analyze two compiler-based strategies termed as conservative and optimistic. The conservative approach does not put a cache line into a low leakage mode until it is certain that the current instruction in it is dead. On the other hand, the optimistic approach places a cache line in low leakage mode if it detects that the next access to the instruction will occur only after a long gap. We evaluate different optimization alternatives by combining the compiler strategies with state-preserving and state-destroying leakage control mechanisms. We also evaluate the sensitivity of these optimizations to different high-level compiler transformations, energy parameters, and soft errors.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W1979106679",
    "type": "article"
  },
  {
    "title": "Adapting branch-target buffer to improve the target predictability of java code",
    "doi": "https://doi.org/10.1145/1071604.1071605",
    "publication_date": "2005-06-01",
    "publication_year": 2005,
    "authors": "Tao Li; Ravi Bhargava; Lizy K. John",
    "corresponding_authors": "",
    "abstract": "Java programs are increasing in popularity and prevalence on numerous platforms, including high-performance general-purpose processors. The success of Java technology largely depends on the efficiency in executing the portable Java bytecodes. However, the dynamic characteristics of the Java runtime system present unique performance challenges for several aspects of microarchitecture design. In this work, we focus on the effects of indirect branches on branch-target address prediction performance. Runtime bytecode translation, just-in-time (JIT) compilation, frequent calls to the native interface libraries, and dependence on virtual methods increase the frequency of polymorphic indirect branches. Therefore, accurate target address prediction for indirect branches is very important for Java code.This paper characterizes the indirect branch behavior in Java processing and proposes an adaptive branch-target buffer (BTB) design to enhance the predictability of the targets. Our characterization shows that a traditional BTB will frequently mispredict a few polymorphic indirect branches, significantly deteriorating predictor accuracy in Java processing. Therefore, we propose a rehashable branch-target buffer (R-BTB), which dynamically identifies polymorphic indirect branches and adapts branch-target storage to accommodate multiple targets for a branch.The R-BTB improves the target predictability of indirect branches without sacrificing overall target prediction accuracy. Simulations show that the R-BTB eliminates 61% of the indirect branch mispredictions suffered with a traditional BTB for Java programs running in interpreter mode (46% in JIT mode), which leads to a 57% decrease in overall target address misprediction rate (29% in JIT mode). With an equivalent number of entries, the R-BTB also outperforms the previously proposed target cache scheme for a majority of Java programs by adapting to a greater variety of indirect branch behaviors.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W1998885205",
    "type": "article"
  },
  {
    "title": "Precise automatable analytical modeling of the cache behavior of codes with indirections",
    "doi": "https://doi.org/10.1145/1275937.1275940",
    "publication_date": "2007-09-01",
    "publication_year": 2007,
    "authors": "Diego Andrade; Basilio B. Fraguela; Ramón Doallo",
    "corresponding_authors": "",
    "abstract": "The performance of memory hierarchies, in which caches play an essential role, is critical in nowadays general-purpose and embedded computing systems because of the growing memory bottleneck problem. Unfortunately, cache behavior is very unstable and difficult to predict. This is particularly true in the presence of irregular access patterns, which exhibit little locality. Such patterns are very common, for example, in applications in which pointers or compressed sparse matrices give place to indirections. Nevertheless, cache behavior in the presence of irregular access patterns has not been widely studied. In this paper we present an extension of a systematic analytical modeling technique based on PMEs (probabilistic miss equations), previously developed by the authors, that allows the automated analysis of the cache behavior for codes with irregular access patterns resulting from indirections. The model generates very accurate predictions despite the irregularities and has very low computing requirements, being the first model that gathers these desirable characteristics that can automatically analyze this kind of codes. These properties enable this model to help drive compiler optimizations, as we show with an example.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2007712750",
    "type": "article"
  },
  {
    "title": "Versatility of extended subwords and the matrix register file",
    "doi": "https://doi.org/10.1145/1369396.1369401",
    "publication_date": "2008-05-01",
    "publication_year": 2008,
    "authors": "Asadollah Shahbahrami; Ben Juurlink; Stamatis Vassiliadis",
    "corresponding_authors": "",
    "abstract": "Extended subwords and the matrix register file (MRF) are two micro architectural techniques that address some of the limitations of existing SIMD architectures. Extended subwords are wider than the data stored in memory. Specifically, for every byte of data stored in memory, there are four extra bits in the media register file. This avoids the need for data-type conversion instructions. The MRF is a register file organization that provides both conventional row-wise, as well as column-wise, access to the register file. In other words, it allows to view the register file as a matrix in which corresponding subwords in different registers corresponds to a column of the matrix. It was introduced to accelerate matrix transposition which is a very common operation in multimedia applications. In this paper, we show that the MRF is very versatile, since it can also be used for other permutations than matrix transposition. Specifically, it is shown how it can be used to provide efficient access to strided data, as is needed in, e.g., color space conversion. Furthermore, it is shown that special-purpose instructions (SPIs), such as the sum-of-absolute differences (SAD) instruction, have limited usefulness when extended subwords and a few general SIMD instructions that we propose are supported, for the following reasons. First, when extended subwords are supported, the SAD instruction provides only a relatively small performance improvement. Second, the SAD instruction processes 8-bit subwords only, which is not sufficient for quarter-pixel resolution nor for cost functions used in image and video retrieval. Results obtained by extending the SimpleScalar toolset show that the proposed techniques provide a speedup of up to 3.00 over the MMX architecture. The results also show that using, at most, 13 extra media registers yields an additional performance improvement ranging from 1.38 to 1.57.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W1969092323",
    "type": "article"
  },
  {
    "title": "Exploiting virtual registers to reduce pressure on real registers",
    "doi": "https://doi.org/10.1145/1328195.1328198",
    "publication_date": "2008-01-01",
    "publication_year": 2008,
    "authors": "Jun Yan; Wei Zhang",
    "corresponding_authors": "",
    "abstract": "It is well known that a large fraction of variables are short-lived. This paper proposes a novel approach to exploiting this fact to reduce the register pressure for pipelined processors with data-forwarding network. The idea is that the compiler can allocate virtual registers (i.e., place holders to identify dependences among instructions) to short-lived variables, which do not need to be stored to physical storage locations. As a result, real registers (i.e., physically existed registers) can be reserved for long-lived variables for mitigating the register pressure and decreasing the register spills, leading to performance improvement. In this paper, we develop the architectural and compiler support for exploiting virtual registers for statically scheduled processors. Our experimental results show that virtual registers are very effective at reducing the register spills, which, in many cases, can achieve the performance close to the processor with twice number of real registers. Our results also indicate that, for some applications, using 24 virtual, in addition to 8 real registers, can attain even higher performance than that of 16 real without any virtual registers.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2016271465",
    "type": "article"
  },
  {
    "title": "Analysis of dependence tracking algorithms for task dataflow execution",
    "doi": "https://doi.org/10.1145/2541228.2555316",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Hans Vandierendonck; George Tzenakis; Dimitrios S. Nikolopoulos",
    "corresponding_authors": "",
    "abstract": "Processor architectures has taken a turn toward many-core processors, which integrate multiple processing cores on a single chip to increase overall performance, and there are no signs that this trend will stop in the near future. Many-core processors are harder to program than multicore and single-core processors due to the need for writing parallel or concurrent programs with high degrees of parallelism. Moreover, many-cores have to operate in a mode of strong scaling because of memory bandwidth constraints. In strong scaling, increasingly finer-grain parallelism must be extracted in order to keep all processing cores busy. Task dataflow programming models have a high potential to simplify parallel programming because they alleviate the programmer from identifying precisely all intertask dependences when writing programs. Instead, the task dataflow runtime system detects and enforces intertask dependences during execution based on the description of memory accessed by each task. The runtime constructs a task dataflow graph that captures all tasks and their dependences. Tasks are scheduled to execute in parallel, taking into account dependences specified in the task graph. Several papers report important overheads for task dataflow systems, which severely limits the scalability and usability of such systems. In this article, we study efficient schemes to manage task graphs and analyze their scalability. We assume a programming model that supports input, output, and in/out annotations on task arguments, as well as commutative in/out and reductions. We analyze the structure of task graphs and identify versions and generations as key concepts for efficient management of task graphs. Then, we present three schemes to manage task graphs building on graph representations, hypergraphs , and lists . We also consider a fourth edgeless scheme that synchronizes tasks using integers. Analysis using microbenchmarks shows that the graph representation is not always scalable and that the edgeless scheme introduces least overhead in nearly all situations.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W1974485377",
    "type": "article"
  },
  {
    "title": "Evaluating placement policies for managing capacity sharing in CMP architectures with private caches",
    "doi": "https://doi.org/10.1145/2019608.2019614",
    "publication_date": "2011-10-01",
    "publication_year": 2011,
    "authors": "Ahmad Samih; Yan Solihin; Anil Krishna",
    "corresponding_authors": "",
    "abstract": "Chip Multiprocessors (CMP) with distributed L2 caches suffer from a cache fragmentation problem; some caches may be overutilized while others may be underutilized. To avoid such fragmentation, researchers have proposed capacity sharing mechanisms where applications that need additional cache space can place their victim blocks in remote caches. However, we found that only allowing victim blocks to be placed on remote caches tends to cause a high number of remote cache hits relative to local cache hits. In this article, we show that many of the remote cache hits can be converted into local cache hits if we allow newly fetched blocks to be selectively placed directly in a remote cache, rather than in the local cache. To demonstrate this, we use future trace information to estimate the near-upperbound performance that can be gained from combined placement and replacement decisions in capacity sharing. Motivated by encouraging experimental results, we design a simple, predictor-based, scheme called Adaptive Placement Policy (APP) that learns from past cache behavior to make a better decision on whether to place a newly fetched block in the local or remote cache. We found that across 50 multiprogrammed workload mixes running on a 4-core CMP, APP's capacity sharing mechanism increases aggregate performance by 29% on average. At the same time, APP outperforms the state-of-the-art capacity sharing mechanism that uses only replacement-based decisions by up to 18.2%, with a maximum degradation of only 0.5%, and an average improvement of 3%.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2015890419",
    "type": "article"
  },
  {
    "title": "Exploring single and multilevel JIT compilation policy for modern machines <sup>1</sup>",
    "doi": "https://doi.org/10.1145/2541228.2541229",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Michael R. Jantz; Prasad A. Kulkarni",
    "corresponding_authors": "",
    "abstract": "Dynamic or Just-in-Time (JIT) compilation is essential to achieve high-performance emulation for programs written in managed languages, such as Java and C#. It has been observed that a conservative JIT compilation policy is most effective to obtain good runtime performance without impeding application progress on single-core machines. At the same time, it is often suggested that a more aggressive dynamic compilation strategy may perform best on modern machines that provide abundant computing resources, especially with virtual machines (VMs) that are also capable of spawning multiple concurrent compiler threads. However, comprehensive research on the best JIT compilation policy for such modern processors and VMs is currently lacking. The goal of this work is to explore the properties of single-tier and multitier JIT compilation policies that can enable existing and future VMs to realize the best program performance on modern machines. In this work, we design novel experiments and implement new VM configurations to effectively control the compiler aggressiveness and optimization levels ( if and when methods are compiled) in the industry-standard Oracle HotSpot Java VM to achieve this goal. We find that the best JIT compilation policy is determined by the nature of the application and the speed and effectiveness of the dynamic compilers. We extend earlier results showing the suitability of conservative JIT compilation on single-core machines for VMs with multiple concurrent compiler threads. We show that employing the free compilation resources (compiler threads and hardware cores) to aggressively compile more program methods quickly reaches a point of diminishing returns. At the same time, we also find that using the free resources to reduce compiler queue backup (compile selected hot methods early ) significantly benefits program performance, especially for slower (highly optimizing) JIT compilers. For such compilers, we observe that accurately prioritizing JIT method compiles is crucial to realize the most performance benefit with the smallest hardware budget. Finally, we show that a tiered compilation policy, although complex to implement, greatly alleviates the impact of more and early JIT compilation of programs on modern machines.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2067940794",
    "type": "article"
  },
  {
    "title": "Exploring the effects of on-chip thermal variation on high-performance multicore architectures",
    "doi": "https://doi.org/10.1145/1952998.1953000",
    "publication_date": "2011-02-05",
    "publication_year": 2011,
    "authors": "Chen-Yong Cher; Eren Kursun",
    "corresponding_authors": "",
    "abstract": "Inherent temperature variation among cores in a multicore architecture can be caused by a number of factors including process variation, cooling and packaging imperfections, and even placement of the chip in the module. Current dynamic thermal management techniques assume identical heating profiles for homogeneous multicore architectures. Our experimental results indicate that inherent thermal variation is very common in existing multicores. While most multicore chips accommodate multiple thermal sensors, the dynamic power/thermal management schemes are oblivious of the inherent heating tendencies. Hence, in the case of variation, the chip faces repetitive hotspots running on such cores. In this article, we propose a technique that leverages the on-chip sensor infrastructure as well as the capabilities of power/thermal management to effectively reduce the heating and minimize local hotspots. This technique can be used in existing multicore chips as long as the thermal sensor data can be made transparent to the power/thermal management at the software layer. According to our experimental analysis on test-chips, 5°C peak temperature reduction can be achieved with no performance degradation, hence the inherent energy efficiency of the chip can be improved without any performance or cost penalty.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2164332729",
    "type": "article"
  },
  {
    "title": "Cluster Programming using the OpenMP Accelerator Model",
    "doi": "https://doi.org/10.1145/3226112",
    "publication_date": "2018-08-28",
    "publication_year": 2018,
    "authors": "Hervé Yviquel; Lauro Cruz; Guido Araújo",
    "corresponding_authors": "",
    "abstract": "Computation offloading is a programming model in which program fragments (e.g., hot loops) are annotated so that their execution is performed in dedicated hardware or accelerator devices. Although offloading has been extensively used to move computation to GPUs, through directive-based annotation standards like OpenMP, offloading computation to very large computer clusters can become a complex and cumbersome task. It typically requires mixing programming models (e.g., OpenMP and MPI) and languages (e.g., C/C++ and Scala), dealing with various access control mechanisms from different cloud providers (e.g., AWS and Azure), and integrating all this into a single application. This article introduces computer cluster nodes as simple OpenMP offloading devices that can be used either from a local computer or from the cluster head-node. It proposes a methodology that transforms OpenMP directives to Spark runtime calls with fully integrated communication management, in a way that a cluster appears to the programmer as yet another accelerator device. Experiments using LLVM 3.8, OpenMP 4.5 on well known cloud infrastructures (Microsoft Azure and Amazon EC2) show the viability of the proposed approach, enable a thorough analysis of its performance, and make a comparison with an MPI implementation. The results show that although data transfers can impose overheads, cloud offloading from a local machine can still achieve promising speedups for larger granularity: up to 115× in 256 cores for the 2MM benchmark using 1GB sparse matrices. In addition, the parallel implementation of a complex and relevant scientific application reveals a 80× speedup on a 320 core machine when executed directly from the headnode of the cluster.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2889187474",
    "type": "article"
  },
  {
    "title": "Performance Tuning and Analysis for Stencil-Based Applications on POWER8 Processor",
    "doi": "https://doi.org/10.1145/3264422",
    "publication_date": "2018-10-10",
    "publication_year": 2018,
    "authors": "Jingheng Xu; Haohuan Fu; Wen Shi; Lin Gan; Yuxuan Li; Wayne Luk; Guangwen Yang",
    "corresponding_authors": "",
    "abstract": "This article demonstrates an approach for combining general tuning techniques with the POWER8 hardware architecture through optimizing three representative stencil benchmarks. Two typical real-world applications, with kernels similar to those of the winning programs of the Gordon Bell Prize 2016 and 2017, are employed to illustrate algorithm modifications and a combination of hardware-oriented tuning strategies with the application algorithms. This work fills the gap between hardware capability and software performance of the POWER8 processor, and provides useful guidance for optimizing stencil-based scientific applications on POWER systems.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2899342811",
    "type": "article"
  },
  {
    "title": "SCORE",
    "doi": "https://doi.org/10.1145/3291052",
    "publication_date": "2018-12-19",
    "publication_year": 2018,
    "authors": "You Zhou; Fei Wu; Zhonghai Lu; Xubin He; Ping Huang; Changsheng Xie",
    "corresponding_authors": "",
    "abstract": "Technology scaling and program/erase cycling result in an increasing bit error rate in NAND flash storage. Some solid state drives (SSDs) adopt overlong error correction codes (ECCs) , whose redundancy size exceeds the spare area limit of flash pages, to protect user data for improved reliability and lifetime. However, the read performance is significantly degraded, because a logical data page and its ECC redundancy are stored in two flash pages. In this article, we find that caching ECCs has a large potential to reduce flash reads by achieving higher hit rates, compared to caching data. Then, we propose a novel &lt;underline&gt;s&lt;/underline&gt;cheme to efficiently &lt;underline&gt;c&lt;/underline&gt;ache &lt;underline&gt;o&lt;/underline&gt;ve&lt;underline&gt;r&lt;/underline&gt;long &lt;underline&gt;E&lt;/underline&gt;CCs, called SCORE , to improve the SSD performance. Exceeding ECC redundancy (called ECC residues ) of logically consecutive data pages are grouped into ECC pages . SCORE partitions RAM to cache both data pages and ECC pages in a workload-adaptive manner. Finally, we verify SCORE using extensive trace-driven simulations. The results show that SCORE obtains high ECC hit rates without sacrificing data hit rates, thus improving the read performance by an average of 22% under various workloads, compared to the state-of-the-art schemes.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2904679400",
    "type": "article"
  },
  {
    "title": "Schedule Synthesis for Halide Pipelines through Reuse Analysis",
    "doi": "https://doi.org/10.1145/3310248",
    "publication_date": "2019-04-18",
    "publication_year": 2019,
    "authors": "Savvas Sioutas; Sander Stuijk; Luc Waeijen; Twan Basten; Henk Corporaal; Lou Somers",
    "corresponding_authors": "",
    "abstract": "Efficient code generation for image processing applications continues to pose a challenge in a domain where high performance is often necessary to meet real-time constraints. The inherently complex structure found in most image-processing pipelines, the plethora of transformations that can be applied to optimize the performance of an implementation, as well as the interaction of these optimizations with locality, redundant computation and parallelism, can be indentified as the key reasons behind this issue. Recent domain-specific languages (DSL) such as the Halide DSL and compiler attempt to encourage high-level design-space exploration to facilitate the optimization process. We propose a novel optimization strategy that aims to maximize producer-consumer locality by exploiting reuse in image-processing pipelines. We implement our analysis as a tool that can be used alongside the Halide DSL to automatically generate schedules for pipelines implemented in Halide and test it on a variety of benchmarks. Experimental results on three different multi-core architectures show an average performance improvement of 40% over the Halide Auto-Scheduler and 75% over a state-of-the art approach that targets the PolyMage DSL.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2938476095",
    "type": "article"
  },
  {
    "title": "Efficient Checkpointing with Recompute Scheme for Non-volatile Main Memory",
    "doi": "https://doi.org/10.1145/3323091",
    "publication_date": "2019-05-29",
    "publication_year": 2019,
    "authors": "Mohammad Alshboul; Hussein Elnawawy; Reem Elkhouly; Keiji Kimura; James Tuck; Yan Solihin",
    "corresponding_authors": "",
    "abstract": "Future main memory will likely include Non-Volatile Memory. Non-Volatile Main Memory (NVMM) provides an opportunity to rethink checkpointing strategies for providing failure safety to applications. While there are many checkpointing and logging schemes in the literature, their use must be revisited as they incur high execution time overheads as well as a large number of additional writes to NVMM, which may significantly impact write endurance. In this article, we propose a novel recompute-based failure safety approach and demonstrate its applicability to loop-based code. Rather than keeping a fully consistent logging state, we only log enough state to enable recomputation. Upon a failure, our approach recovers to a consistent state by determining which parts of the computation were not completed and recomputing them. Effectively, our approach removes the need to keep checkpoints or logs, thus reducing execution time overheads and improving NVMM write endurance at the expense of more complex recovery. We compare our new approach against logging and checkpointing on five scientific workloads, including tiled matrix multiplication, on a computer system model that was built on gem5 and supports Intel PMEM instruction extensions. For tiled matrix multiplication, our recompute approach incurs an execution time overhead of only 5%, in contrast to 8% overhead with logging and 207% overhead with checkpointing. Furthermore, recompute only adds 7% additional NVMM writes, compared to 111% with logging and 330% with checkpointing. We also conduct experiments on real hardware, allowing us to run our workloads to completion while varying the number of threads used for computation. These experiments substantiate our simulation-based observations and provide a sensitivity study and performance comparison between the Recompute Scheme and Naive Checkpointing.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2947555139",
    "type": "article"
  },
  {
    "title": "Compiler-Directed Power Management for Superscalars",
    "doi": "https://doi.org/10.1145/2685393",
    "publication_date": "2015-01-09",
    "publication_year": 2015,
    "authors": "Jawad Haj-Yihia; Yosi Ben Asher; Efraim Rotem; Ahmad Yasin; Ran Ginosar",
    "corresponding_authors": "",
    "abstract": "Modern superscalar CPUs contain large complex structures and diverse execution units, consuming wide dynamic power range. Building a power delivery network for the worst-case power consumption is not energy efficient and often is impossible to fit in small systems. Instantaneous power excursions can cause voltage droops. Power management algorithms are too slow to respond to instantaneous events. In this article, we propose a novel compiler-directed framework to address this problem. The framework is validated on a 4th Generation Intel® Core™ processor and with simulator on output trace. Up to 16% performance speedup is measured over baseline for the SPEC CPU2006 benchmarks.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W1980255621",
    "type": "article"
  },
  {
    "title": "DEFCAM",
    "doi": "https://doi.org/10.1145/2019608.2019616",
    "publication_date": "2011-10-01",
    "publication_year": 2011,
    "authors": "Hyunjin Lee; Sangyeun Cho; Bruce R. Childers",
    "corresponding_authors": "",
    "abstract": "Advances in deep submicron technology call for a careful review of existing cache designs and design practices in terms of yield, area, and performance. This article presents a Design and Evaluation Framework for defect-tolerant Cache Memories (DEFCAM), which enables processor architects to consider yield, area, and performance together in a unified framework. Since there is a complex, changing trade-off among these metrics depending on the technology, the cache organization, and the yield enhancement scheme employed, such a design flow is invaluable to processor architects when they assess a design and explore the design space quickly at an early stage. We develop a complete framework supporting the proposed DEFCAM design flow, from injecting defects into a wafer to evaluating program performance of individual processors on the wafer. Using DEFCAM, interesting interactions between architectural, organizational, and layout/defect related parameters can be easily evaluated. Moreover, we propose practical set remapping schemes to contain hard faults in cache memory. In a set remapping scheme, accesses that would go to an unusable faulty set are directed to a sound set. Case studies are presented to demonstrate the effectiveness of the proposed design flow and developed tools. Experimental results show that a set remapping is the most efficient fault covering method among prevailing strategies.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2003344292",
    "type": "article"
  },
  {
    "title": "Scalable high-radix router microarchitecture using a network switch organization",
    "doi": "https://doi.org/10.1145/2512433",
    "publication_date": "2013-09-16",
    "publication_year": 2013,
    "authors": "Jung Ho Ahn; Young Hoon Son; John Kim",
    "corresponding_authors": "",
    "abstract": "As the system size of supercomputers and datacenters increases, cost-efficient networks become critical in achieving good scalability on those systems. High -radix routers reduce network cost by lowering the network diameter while providing a high bisection bandwidth and path diversity. The building blocks of these large-scale networks are the routers or the switches and they need to scale accordingly to the increasing port count and increasing pin bandwidth. However, as the port count increases, the high-radix router microarchitecture itself needs to scale efficiently. Hierarchical crossbar switch organization has been proposed where a single large crossbar used for a router switch is partitioned into many small crossbars and overcomes the limitations of conventional router microarchitecture. Although the organization provides high performance, it has limited scalability due to excessive power and area overheads by the wires and intermediate buffers. In this article, we propose scalable router microarchitectures that leverage a network within the switch design of the high-radix routers themselves. These alternative designs lower the wiring complexity and buffer requirements. For example, when a folded-Clos switch is used instead of the hierarchical crossbar switch for a radix-64 router, it provides up to 73%, 58%, and 87% reduction in area, energy-delay product, and energy-delay-area product, respectively. We also explore more efficient switch designs by exploiting the traffic-pattern characteristics of the global network and its impact on the local network design within the switch for both folded-Clos and flattened butterfly networks. In particular, we propose a bilateral butterfly switch organization that has fewer crossbars and global wires compared to the topology-agnostic folded-Clos switch while achieving better low-load latency and equivalent saturation throughput.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2008232676",
    "type": "article"
  },
  {
    "title": "Layout-oblivious compiler optimization for matrix computations",
    "doi": "https://doi.org/10.1145/2400682.2400694",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Huimin Cui; Qing Yi; Jingling Xue; Xiaobing Feng",
    "corresponding_authors": "",
    "abstract": "Most scientific computations serve to apply mathematical operations to a set of preconceived data structures, e.g., matrices, vectors, and grids. In this article, we use a number of widely used matrix computations from the LINPACK library to demonstrate that complex internal organizations of data structures can severely degrade the effectiveness of compiler optimizations. We then present a data-layout-oblivious optimization methodology, where by isolating an abstract representation of the computations from complex implementation details of their data, we enable these computations to be much more accurately analyzed and optimized through varying state-of-the-art compiler technologies. We evaluated our approach on an Intel 8-core platform using two source-to-source compiler infrastructures, Pluto and EPOD. Our results show that while the efficiency of a computational kernel differs when using different data layouts, the alternative implementations typically benefit from a common set of optimizations on the operations. Therefore separately optimizing the operations and the data layout of a computation could dramatically enhance the effectiveness of compiler optimizations compared with the conventional approaches of using a unified representation.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2018564121",
    "type": "article"
  },
  {
    "title": "Algorithmic species",
    "doi": "https://doi.org/10.1145/2400682.2400699",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Cedric Nugteren; Pieter Custers; Henk Corporaal",
    "corresponding_authors": "",
    "abstract": "Code generation and programming have become ever more challenging over the last decade due to the shift towards parallel processing. Emerging processor architectures such as multi-cores and GPUs exploit increasingly parallelism, requiring programmers and compilers to deal with aspects such as threading, concurrency, synchronization, and complex memory partitioning. We advocate that programmers and compilers can greatly benefit from a structured classification of program code. Such a classification can help programmers to find opportunities for parallelization, reason about their code, and interact with other programmers. Similarly, parallelising compilers and source-to-source compilers can take threading and optimization decisions based on the same classification. In this work, we introduce algorithmic species , a classification of affine loop nests based on the polyhedral model and targeted for both automatic and manual use. Individual classes capture information such as the structure of parallelism and the data reuse. To make the classification applicable for manual use, a basic vocabulary forms the base for the creation of a set of intuitive classes. To demonstrate the use of algorithmic species, we identify 115 classes in a benchmark set. Additionally, we demonstrate the suitability of algorithmic species for automated uses by showing a tool to automatically extract species from program code, a species-based source-to-source compiler, and a species-based performance prediction model.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2019295209",
    "type": "article"
  },
  {
    "title": "Perfect Reconstructability of Control Flow from Demand Dependence Graphs",
    "doi": "https://doi.org/10.1145/2693261",
    "publication_date": "2015-01-09",
    "publication_year": 2015,
    "authors": "Helge Bahmann; Nico Reißmann; Magnus Jahre; Jan Christian Meyer",
    "corresponding_authors": "",
    "abstract": "Demand-based dependence graphs (DDGs), such as the (Regionalized) Value State Dependence Graph ((R)VSDG), are intermediate representations (IRs) well suited for a wide range of program transformations. They explicitly model the flow of data and state, and only implicitly represent a restricted form of control flow. These features make DDGs especially suitable for automatic parallelization and vectorization, but cannot be leveraged by practical compilers without efficient construction and destruction algorithms. Construction algorithms remodel the arbitrarily complex control flow of a procedure to make it amenable to DDG representation, whereas destruction algorithms reestablish control flow for generating efficient object code. Existing literature presents solutions to both problems, but these impose structural constraints on the generatable control flow, and omit qualitative evaluation. The key contribution of this article is to show that there is no intrinsic structural limitation in the control flow directly extractable from RVSDGs. This fundamental result originates from an interpretation of loop repetition and decision predicates as computed continuations, leading to the introduction of the predicate continuation normal form. We provide an algorithm for constructing RVSDGs in predicate continuation form, and propose a novel destruction algorithm for RVSDGs in this form. Our destruction algorithm can generate arbitrarily complex control flow; we show this by proving that the original CFG an RVSDG was derived from can, apart from overspecific detail, be reconstructed perfectly. Additionally, we prove termination and correctness of these algorithms. Furthermore, we empirically evaluate the performance, the representational overhead at compile time, and the reduction in branch instructions compared to existing solutions. In contrast to previous work, our algorithms impose no additional overhead on the control flow of the produced object code. To our knowledge, this is the first scheme that allows the original control flow of a procedure to be recovered from a DDG representation.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2026352419",
    "type": "article"
  },
  {
    "title": "Lock-contention-aware scheduler",
    "doi": "https://doi.org/10.1145/2400682.2400703",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Yan Cui; Yingxin Wang; Yu Chen; Yuanchun Shi",
    "corresponding_authors": "",
    "abstract": "In response to the increasing ubiquity of multicore processors, there has been widespread development of multithreaded applications that strive to realize their full potential. Unfortunately, lock contention within operating systems can limit the scalability of multicore systems so severely that an increase in the number of cores can actually lead to reduced performance (i.e., scalability collapse). Existing efforts of solving scalability collapse mainly focus on making critical sections of kernel code fine-grained or designing new synchronization primitives. However, these methods have disadvantages in scalability or energy efficiency. In this article, we observe that the percentage of lock-waiting time over the total execution time for a lock intensive task has a significant correlation with the occurrence of scalability collapse. Based on this observation, a lock-contention-aware scheduler is proposed. Specifically, each task in the scheduler monitors its percentage of lock waiting time continuously. If the percentage exceeds a predefined threshold, this task is considered as lock intensive and migrated to a Special Set of Cores (i.e., SSC). In this way, the number of concurrently running lock-intensive tasks is limited to the number of cores in the SSC, and therefore, the degree of lock contention is controlled. A central challenge of using this scheme is how many cores should be allocated in the SSC to handle lock-intensive tasks. In our scheduler, the optimal number of cores is determined online by the model-driven search. The proposed scheduler is implemented in the recent Linux kernel and evaluated using micro- and macrobenchmarks on AMD and Intel 32-core systems. Experimental results suggest that our proposal is able to remove scalability collapse completely and sustains the maximal throughput of the spin-lock-based system for most applications. Furthermore, the percentage of lock-waiting time can be reduced by up to 84%. When compared with scalability collapse reduction methods such as requester-based locking scheme and sleeping-based synchronization primitives, our scheme exhibits significant advantages in scalability, power consumption, and energy efficiency.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2047333955",
    "type": "article"
  },
  {
    "title": "VSim",
    "doi": "https://doi.org/10.1145/2086696.2086731",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Frederick Ryckbosch; Stijn Polfliet; Lieven Eeckhout",
    "corresponding_authors": "",
    "abstract": "Simulating contemporary computer systems is a challenging endeavor, especially when it comes to simulating high-end setups involving multiple servers. The simulation environment needs to run complete software stacks, including operating systems, middleware, and application software, and it needs to simulate network and disk activity next to CPU performance. In addition, it needs the ability to scale out to a large number of server nodes while attaining good accuracy and reasonable simulation speeds. This paper presents VSim, a novel simulation methodology for multi-server systems. VSim leverages virtualization technology for simulating a target system on a host system. VSim controls CPU, network and disk performance on the host, and it gives the illusion to the software stack to run on a target system through time dilation. VSim can simulate multiple targets per host, and it employs a distributed simulation scheme across multiple hosts for simulations at scale. Our experimental results demonstrate VSim's accuracy: typical errors are below 6% for CPU, disk, and network performance. Real-life workloads involving the Lucene search engine and the Olio Web 2.0 benchmark illustrate VSim's utility and accuracy (average error of 3.2%). Our current setup can simulate up to five target servers per host, and we provide a Hadoop workload case study in which we simulate 25 servers. These simulation results are obtained at a simulation slowdown of one order of magnitude compared to native hardware execution.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2058817615",
    "type": "article"
  },
  {
    "title": "Fast asymmetric thread synchronization",
    "doi": "https://doi.org/10.1145/2400682.2400686",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Jimmy Cleary; Owen Callanan; Mark Purcell; David Gregg",
    "corresponding_authors": "",
    "abstract": "For most multi-threaded applications, data structures must be shared between threads. Ensuring thread safety on these data structures incurs overhead in the form of locking and other synchronization mechanisms. Where data is shared among multiple threads these costs are unavoidable. However, a common access pattern is that data is accessed primarily by one dominant thread, and only very rarely by the other, non-dominant threads. Previous research has proposed biased locks, which are optimized for a single dominant thread, at the cost of greater overheads for non-dominant threads. In this article we propose a new family of biased synchronization mechanisms that, using a modified interface, push accesses to shared data from the non-dominant threads to the dominant one, via a novel set of message passing mechanisms. We present mechanisms for protecting critical sections, for queueing work, for caching shared data in registers where it is safe to do so, and for asynchronous critical section accesses. We present results for the conventional Intel® Sandy Bridge processor and for the emerging network-optimized many-core IBM® PowerEN™ processor. We find that our algorithms compete well with existing biased locking algorithms, and, in particular, perform better than existing algorithms as accesses from non-dominant threads increase.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2134058640",
    "type": "article"
  },
  {
    "title": "A New Memory-Disk Integrated System with HW Optimizer",
    "doi": "https://doi.org/10.1145/2738053",
    "publication_date": "2015-05-11",
    "publication_year": 2015,
    "authors": "Do-Heon Lee; Su-Kyung Yoon; Jung-Geun Kim; Charles Weems; Shin‐Dug Kim",
    "corresponding_authors": "",
    "abstract": "Current high-performance computer systems utilize a memory hierarchy of on-chip cache, main memory, and secondary storage due to differences in device characteristics. Limiting the amount of main memory causes page swap operations and duplicates data between the main memory and the storage device. The characteristics of next-generation memory, such as nonvolatility, byte addressability, and scaling to greater capacity, can be used to solve these problems. Simple replacement of secondary storage with new forms of nonvolatile memory in a traditional memory hierarchy still causes typical problems, such as memory bottleneck, page swaps, and write overhead. Thus, we suggest a single architecture that merges the main memory and secondary storage into a system called a Memory-Disk Integrated System (MDIS). The MDIS architecture is composed of a virtually decoupled NVRAM and a nonvolatile memory performance optimizer combining hardware and software to support this system. The virtually decoupled NVRAM module can support conventional main memory and disk storage operations logically without data duplication and can reduce write operations to the NVRAM. To increase the lifetime and optimize the performance of this NVRAM, another hardware module called a Nonvolatile Performance Optimizer (NVPO) is used that is composed of four small buffers. The NVPO exploits spatial and temporal characteristics of static/dynamic data based on program execution characteristics. Enhanced virtual memory management and address translation modules in the operating system can support these hardware components to achieve a seamless memory-storage environment. Our experimental results show that the proposed architecture can improve execution time by about 89% over a conventional DRAM main memory/HDD storage system, and 77% over a state-of-the-art PRAM main memory/HDD disk system with DRAM buffer. Also, the lifetime of the virtually decoupled NVRAM is estimated to be 40% longer than that of a traditional hierarchy based on the same device technology.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2240976812",
    "type": "article"
  },
  {
    "title": "Integer Linear Programming-Based Scheduling for Transport Triggered Architectures",
    "doi": "https://doi.org/10.1145/2845082",
    "publication_date": "2015-12-22",
    "publication_year": 2015,
    "authors": "Tomi Äijö; Pekka Jääskeläinen; Tapio Elomaa; Heikki Kultala; Jarmo Takala",
    "corresponding_authors": "",
    "abstract": "Static multi-issue machines, such as traditional Very Long Instructional Word (VLIW) architectures, move complexity from the hardware to the compiler. This is motivated by the ability to support high degrees of instruction-level parallelism without requiring complicated scheduling logic in the processor hardware. The simpler-control hardware results in reduced area and power consumption, but leads to a challenge of engineering a compiler with good code-generation quality. Transport triggered architectures (TTA), and other so-called exposed datapath architectures, take the compiler-oriented philosophy even further by pushing more details of the datapath under software control. The main benefit of this is the reduced register file pressure, with a drawback of adding even more complexity to the compiler side. In this article, we propose an Integer Linear Programming (ILP) -based instruction scheduling model for TTAs. The model describes the architecture characteristics, the particular processor resource constraints, and the operation dependencies of the scheduled program. The model is validated and measured by compiling application kernels to various TTAs with a different number of datapath components and connectivity. In the best case, the cycle count is reduced to 52% when compared to a heuristic scheduler. In addition to producing shorter schedules, the number of register accesses in the compiled programs is generally notably less than those with the heuristic scheduler; in the best case, the ILP scheduler reduced the number of register file reads to 33% of the heuristic results and register file writes to 18%. On the other hand, as expected, the ILP-based scheduler uses distinctly more time to produce a schedule than the heuristic scheduler, but the compilation time is within tolerable limits for production-code generation.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2294034716",
    "type": "article"
  },
  {
    "title": "Extendable pattern-oriented optimization directives",
    "doi": "https://doi.org/10.1145/2355585.2355587",
    "publication_date": "2012-09-01",
    "publication_year": 2012,
    "authors": "Huimin Cui; Jingling Xue; Lei Wang; Yang Yang; Xiaobing Feng; Dongrui Fan",
    "corresponding_authors": "",
    "abstract": "Algorithm-specific, that is, semantic-specific optimizations have been observed to bring significant performance gains, especially for a diverse set of multi/many-core architectures. However, current programming models and compiler technologies for the state-of-the-art architectures do not exploit well these performance opportunities. In this article, we propose a pattern-making methodology that enables algorithm-specific optimizations to be encapsulated into “optimization patterns”. Such optimization patterns are expressed in terms of preprocessor directives so that simple annotations can result in significant performance improvements. To validate this new methodology, a framework, named EPOD, is developed to map these directives into the underlying optimization schemes for a particular architecture. It is difficult to create an exact performance model to determine an optimal or near-optimal optimization scheme (including which optimizations to apply and in which order) for a specific application, due to the complexity of applications and architectures. However, it is trackable to build individual optimization components and let compiler developers synthesize an optimization scheme from these components. Therefore, our EPOD framework provides an Optimization Programming Interface (OPI) for compiler developers to define new optimization schemes. Thus, new patterns can be integrated into EPOD in a flexible manner. We have identified and implemented a number of optimization patterns for three representative computer platforms. Our experimental results show that a pattern-guided compiler can outperform the state-of-the-art compilers and even achieve performance as competitive as hand-tuned code. Therefore, such a pattern-making methodology represents an encouraging direction for domain experts' experience and knowledge to be integrated into general-purpose compilers.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2623645495",
    "type": "article"
  },
  {
    "title": "A Transactional Correctness Tool for Abstract Data Types",
    "doi": "https://doi.org/10.1145/3148964",
    "publication_date": "2017-11-14",
    "publication_year": 2017,
    "authors": "Christina Peterson; Damian Dechev",
    "corresponding_authors": "",
    "abstract": "Transactional memory simplifies multiprocessor programming by providing the guarantee that a sequential block of code in the form of a transaction will exhibit atomicity and isolation. Transactional data structures offer the same guarantee to concurrent data structures by enabling the atomic execution of a composition of operations. The concurrency control of transactional memory systems preserves atomicity and isolation by detecting read/write conflicts among multiple concurrent transactions. State-of-the-art transactional data structures improve on this concurrency control protocol by providing explicit transaction-level synchronization for only non-commutative operations. Since read/write conflicts are handled by thread-level concurrency control, the correctness of transactional data structures cannot be evaluated according to the read/write histories. This presents a challenge for existing correctness verification techniques for transactional memory, because correctness is determined according to the transitions taken by the transactions in the presence of read/write conflicts. In this article, we present Transactional Correctness tool for Abstract Data Types (TxC-ADT), the first tool that can check the correctness of transactional data structures. TxC-ADT elevates the standard definitions of transactional correctness to be in terms of an abstract data type, an essential aspect for checking correctness of transactions that synchronize only for high-level semantic conflicts. To accommodate a diverse assortment of transactional correctness conditions, we present a technique for defining correctness as a happens-before relation. Defining a correctness condition in this manner enables an automated approach in which correctness is evaluated by generating and analyzing a transactional happens-before graph during model checking. A transactional happens-before graph is maintained on a per-thread basis, making our approach applicable to transactional correctness conditions that do not enforce a total order on a transactional execution. We demonstrate the practical applications of TxC-ADT by checking Lock Free Transactional Transformation and Transactional Data Structure Libraries for serializability, strict serializability, opacity, and causal consistency.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2768327291",
    "type": "article"
  },
  {
    "title": "SCALO",
    "doi": "https://doi.org/10.1145/3158643",
    "publication_date": "2017-12-18",
    "publication_year": 2017,
    "authors": "Giorgis Georgakoudis; Hans Vandierendonck; Peter Thoman; Bronis R. de Supinski; Thomas Fahringer; Dimitrios S. Nikolopoulos",
    "corresponding_authors": "",
    "abstract": "Shared memory machines continue to increase in scale by adding more parallelism through additional cores and complex memory hierarchies. Often, executing multiple applications concurrently, dividing among them hardware threads, provides greater efficiency rather than executing a single application with large thread counts. However, contention for shared resources can limit the improvement of concurrent application execution: orchestrating the number of threads used by each application and is essential. In this article, we contribute SCALO, a solution to orchestrate concurrent application execution to increase throughput. SCALO monitors co-executing applications at runtime to evaluate their scalability. Its optimizing thread allocator analyzes these scalability estimates to adapt the parallelism of each program. Unlike previous approaches, SCALO differs by including dynamic contention effects on scalability and by controlling the parallelism during the execution of parallel regions. Thus, it improves throughput when other state-of-the-art approaches fail and outperforms them by up to 40% when they succeed.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2769464294",
    "type": "article"
  },
  {
    "title": "HAWS",
    "doi": "https://doi.org/10.1145/3291050",
    "publication_date": "2019-04-18",
    "publication_year": 2019,
    "authors": "Xun Gong; Xiang Gong; Leiming Yu; David Kaeli",
    "corresponding_authors": "",
    "abstract": "Graphics Processing Units (GPUs) have become an attractive platform for accelerating challenging applications on a range of platforms, from High Performance Computing (HPC) to full-featured smartphones. They can overcome computational barriers in a wide range of data-parallel kernels. GPUs hide pipeline stalls and memory latency by utilizing efficient thread preemption. But given the demands on the memory hierarchy due to the growth in the number of computing cores on-chip, it has become increasingly difficult to hide all of these stalls. In this article, we propose a novel Hint-Assisted Wavefront Scheduler (HAWS) to bypass long-latency stalls. HAWS starts by enhancing a compiler infrastructure to identify potential opportunities that can bypass memory stalls. HAWS includes a wavefront scheduler that can continue to execute instructions in the shadow of a memory stall, executing instructions speculatively, guided by compiler-generated hints. HAWS increases utilization of GPU resources by aggressively fetching/executing speculatively. Based on our simulation results on the AMD Southern Islands GPU architecture, at an estimated cost of 0.4% total chip area, HAWS can improve application performance by 14.6% on average for memory intensive applications.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2936491961",
    "type": "article"
  },
  {
    "title": "Symmetry in Software Synthesis",
    "doi": "https://doi.org/10.1145/3095747",
    "publication_date": "2017-06-30",
    "publication_year": 2017,
    "authors": "Andrés Goens; Sergio Siccha; Jerónimo Castrillón",
    "corresponding_authors": "",
    "abstract": "With the surge of multi- and manycores, much research has focused on algorithms for mapping and scheduling on these complex platforms. Large classes of these algorithms face scalability problems. This is why diverse methods are commonly used for reducing the search space. While most such approaches leverage the inherent symmetry of architectures and applications, they do it in a problem-specific and intuitive way. However, intuitive approaches become impractical with growing hardware complexity, like Network-on-Chip interconnect or heterogeneous cores. In this paper, we present a formal framework that can determine the inherent symmetry of architectures and applications algorithmically and leverage these for problems in software synthesis. Our approach is based on the mathematical theory of groups and a generalization called inverse semigroups. We evaluate our approach in two state-of-the-art mapping frameworks. Even for the platforms with a handful of cores of today and moderate-size benchmarks, our approach consistently yields reductions of the overall execution time of algorithms, accelerating them by a factor up to 10 in our experiments, or improving the quality of the results.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2951829237",
    "type": "article"
  },
  {
    "title": "Informed Prefetching for Indirect Memory Accesses",
    "doi": "https://doi.org/10.1145/3374216",
    "publication_date": "2020-03-04",
    "publication_year": 2020,
    "authors": "Mustafa Çavus; Resit Sendag; Joshua J. Yi",
    "corresponding_authors": "",
    "abstract": "Indirect memory accesses have irregular access patterns that limit the performance of conventional software and hardware-based prefetchers. To address this problem, we propose the Array Tracking Prefetcher (ATP), which tracks array-based indirect memory accesses using a novel combination of software and hardware. ATP is first configured by special metadata instructions, which are inserted by programmer or compiler to pass data structure traversal knowledge. It then calculates and issues prefetches based on this information. ATP also employs a novel mechanism for dynamically adjusting prefetching distance to reduce early or late prefetches. ATP yields average speedup of 2.17 as compared to a single-core without prefetching. By contrast, the speedup for conventional software and hardware-based prefetching is 1.84 and 1.32, respectively. For four cores, the average speedup for ATP is 1.85, while the corresponding speedups for software and hardware-based prefetching are 1.60 and 1.25, respectively.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3009555218",
    "type": "article"
  },
  {
    "title": "Efficient Nearest-Neighbor Data Sharing in GPUs",
    "doi": "https://doi.org/10.1145/3429981",
    "publication_date": "2020-12-30",
    "publication_year": 2020,
    "authors": "Negin Mahani; Mohammad Sadrosadati; Hajar Falahati; Marzieh Barkhordar; Mario Drumond; Hamid Sarbazi‐Azad; Babak Falsafi",
    "corresponding_authors": "",
    "abstract": "Stencil codes (a.k.a. nearest-neighbor computations) are widely used in image processing, machine learning, and scientific applications. Stencil codes incur nearest-neighbor data exchange because the value of each point in the structured grid is calculated as a function of its value and the values of a subset of its nearest-neighbor points. When running on Graphics Processing Unit (GPUs), stencil codes exhibit a high degree of data sharing between nearest-neighbor threads. Sharing is typically implemented through shared memories, shuffle instructions, and on-chip caches and often incurs performance overheads due to the redundancy in memory accesses. In this article, we propose Neighbor Data (NeDa), a direct nearest-neighbor data sharing mechanism that uses two registers embedded in each streaming processor (SP) that can be accessed by nearest-neighbor SP cores. The registers are compiler-allocated and serve as a data exchange mechanism to eliminate nearest-neighbor shared accesses. NeDa is embedded carefully with local wires between SP cores so as to minimize the impact on density. We place and route NeDa in an open-source GPU and show a small area overhead of 1.3%. The cycle-accurate simulation indicates an average performance improvement of 21.8% and power reduction of up to 18.3% for stencil codes in General-Purpose Graphics Processing Unit (GPGPU) standard benchmark suites. We show that NeDa’s performance is within 13.2% of an ideal GPU with no overhead for nearest-neighbor data exchange.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3117273852",
    "type": "article"
  },
  {
    "title": "On the Anatomy of Predictive Models for Accelerating GPU Convolution Kernels and Beyond",
    "doi": "https://doi.org/10.1145/3434402",
    "publication_date": "2021-01-07",
    "publication_year": 2021,
    "authors": "Paolo Sylos Labini; Marco Cianfriglia; Damiano Perri; Osvaldo Gervasi; Grigori Fursin; Anton Lokhmotov; Cedric Nugteren; Bruno Carpentieri; Fabiana Zollo; Flavio Vella",
    "corresponding_authors": "",
    "abstract": "Efficient high-performance libraries often expose multiple tunable parameters to provide highly optimized routines. These can range from simple loop unroll factors or vector sizes all the way to algorithmic changes, given that some implementations can be more suitable for certain devices by exploiting hardware characteristics such as local memories and vector units. Traditionally, such parameters and algorithmic choices are tuned and then hard-coded for a specific architecture and for certain characteristics of the inputs. However, emerging applications are often data-driven, thus traditional approaches are not effective across the wide range of inputs and architectures used in practice. In this paper, we present a new adaptive framework for data-driven applications which uses a predictive model to select the optimal algorithmic parameters by training with synthetic and real datasets. We demonstrate the effectiveness of a BLAS library and specifically on its matrix multiplication routine. We present experimental results for two GPU architectures and show significant performance gains of up to 3x (on a high-end NVIDIA Pascal GPU) and 2.5x (on an embedded ARM Mali GPU) when compared to a traditionally optimized library.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3120016685",
    "type": "article"
  },
  {
    "title": "Gretch",
    "doi": "https://doi.org/10.1145/3439803",
    "publication_date": "2021-02-09",
    "publication_year": 2021,
    "authors": "Anirudh Mohan Kaushik; Gennady Pekhimenko; Hiren Patel",
    "corresponding_authors": "",
    "abstract": "Data-dependent memory accesses (DDAs) pose an important challenge for high-performance graph analytics (GA). This is because such memory accesses do not exhibit enough temporal and spatial locality resulting in low cache performance. Prior efforts that focused on improving the performance of DDAs for GA are not applicable across various GA frameworks. This is because (1) they only focus on one particular graph representation, and (2) they require workload changes to communicate specific information to the hardware for their effective operation. In this work, we propose a hardware-only solution to improving the performance of DDAs for GA across multiple GA frameworks. We present a hardware prefetcher for GA called Gretch, that addresses the above limitations. An important observation we make is that identifying certain DDAs without hardware-software communication is sensitive to the instruction scheduling. A key contribution of this work is a hardware mechanism that activates Gretch to identify DDAs when using either in-order or out-of-order instruction scheduling. Our evaluation shows that Gretch provides an average speedup of 38% over no prefetching, 25% over conventional stride prefetcher, and outperforms prior DDAs prefetchers by 22% with only 1% increase in power consumption when executed on different GA workloads and frameworks.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3126960560",
    "type": "article"
  },
  {
    "title": "A Reusable Characterization of the Memory System Behavior of SPEC2017 and SPEC2006",
    "doi": "https://doi.org/10.1145/3446200",
    "publication_date": "2021-03-08",
    "publication_year": 2021,
    "authors": "Muhammad Hassan; Chang Hyun Park; David Black-Schaffer",
    "corresponding_authors": "",
    "abstract": "The SPEC CPU Benchmarks are used extensively for evaluating and comparing improvements to computer systems. This ubiquity makes characterization critical for researchers to understand the bottlenecks the benchmarks do and do not expose and where new designs should and should not be expected to show impact. However, in characterization there is a tradeoff between accuracy and reusability: The more precisely we characterize a benchmark’s performance on a given system, the less usable it is across different micro-architectures and varying memory configurations. For SPEC, most existing characterizations include system-specific effects (e.g., via performance counters) and/or only look at aggregate behavior (e.g., averages over the full application execution). While such approaches simplify characterization, they make it difficult to separate the applications’ intrinsic behavior from the system-specific effects and/or lose the diverse phase-based behaviors. In this work we focus on characterizing the applications’ intrinsic memory behaviour by isolating them from micro-architectural configuration specifics. We do this by providing a simplified generic system model that evaluates the applications’ memory behavior across multiple cache sizes, with and without prefetching, and over time. The resulting characterization can be reused across a range of systems to understand application behavior and allow us to see how frequently different behaviors occur. We use this approach to compare the SPEC 2006 and 2017 suites, providing insight into their memory system behaviour beyond previous system-specific and/or aggregate results. We demonstrate the ability to use this characterization in different contexts by showing a portion of the SPEC 2017 benchmark suite that could benefit from giga-scale caches, despite aggregate results indicating otherwise.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3134718172",
    "type": "article"
  },
  {
    "title": "Transactional Read-Modify-Write Without Aborts",
    "doi": "https://doi.org/10.1145/2688904",
    "publication_date": "2015-01-09",
    "publication_year": 2015,
    "authors": "Wenjia Ruan; Yujie Liu; Michael Spear",
    "corresponding_authors": "",
    "abstract": "Language-level transactions are said to provide “atomicity,” implying that the order of operations within a transaction should be invisible to concurrent transactions and thus that independent operations within a transaction should be safe to execute in any order. In this article, we present a mechanism for dynamically reordering memory operations within a transaction so that read-modify-write operations on highly contended locations can be delayed until the very end of the transaction. When integrated with traditional transactional conflict detection mechanisms, our approach reduces aborts on hot memory locations, such as statistics counters, thereby improving throughput and reducing wasted work. We present three algorithms for delaying highly contended read-modify-write operations within transactions, and we evaluate their impact on throughput for eager and lazy transactional systems across multiple workloads. We also discuss complications that arise from the interaction between our mechanism and the need for strong language-level semantics, and we propose algorithmic extensions that prevent errors from occurring when accesses are aggressively reordered in a transactional memory implementation with weak semantics.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W1972170503",
    "type": "article"
  },
  {
    "title": "A Portable Optimization Engine for Accelerating Irregular Data-Traversal Applications on SIMD Architectures",
    "doi": "https://doi.org/10.1145/2632215",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "Bin Ren; Todd Mytkowicz; Gagan Agrawal",
    "corresponding_authors": "",
    "abstract": "Fine-grained data parallelism is increasingly common in the form of longer vectors integrated with mainstream processors (SSE, AVX) and various GPU architectures. This article develops support for exploiting such data parallelism for a class of nonnumeric, nongraphic applications, which perform computations while traversing many independent, irregular data structures. We address this problem by developing several novel techniques. First, for code generation, we develop an intermediate language for specifying such traversals, followed by a runtime scheduler that maps traversals to various SIMD units. Second, we observe that good data locality is crucial to sustained performance from SIMD architectures, whereas many applications that operate on irregular data structures (e.g., trees and graphs) have poor data locality. To address this challenge, we develop a set of data layout optimizations that improve spatial locality for applications that traverse many irregular data structures. Unlike prior data layout optimizations, our approach incorporates a notion of both interthread and intrathread spatial reuse into data layout. Finally, we enable performance portability (i.e., the ability to automatically optimize applications for different architectures) by accurately modeling the impact of inter- and intrathread locality on program performance. As a consequence, our model can predict which data layout optimization to use on a wide variety of SIMD architectures. To demonstrate the efficacy of our approach and optimizations, we first show how they enable up to a 12X speedup on one SIMD architecture for a set of real-world applications. To demonstrate that our approach enables performance portability, we show how our model predicts the optimal layout for applications across a diverse set of three real-world SIMD architectures, which offers as much as 45% speedup over a suboptimal solution.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W1979790590",
    "type": "article"
  },
  {
    "title": "Generalized Task Parallelism",
    "doi": "https://doi.org/10.1145/2723164",
    "publication_date": "2015-04-02",
    "publication_year": 2015,
    "authors": "Kevin Streit; Johannes Doerfert; Clemens Hammacher; Andreas Zeller; Sebastian Hack",
    "corresponding_authors": "",
    "abstract": "Existing approaches to automatic parallelization produce good results in specific domains. Yet, it is unclear how to integrate their individual strengths to match the demands and opportunities of complex software. This lack of integration has both practical reasons, as integrating those largely differing approaches into one compiler would impose an engineering hell, as well as theoretical reasons, as no joint cost model exists that would drive the choice between parallelization methods. By reducing the problem of generating parallel code from a program dependence graph to integer linear programming, &lt;i&gt;generalized task parallelization&lt;/i&gt; integrates central aspects of existing parallelization approaches into a single unified framework. Implemented on top of LLVM, the framework seamlessly integrates enabling technologies such as speculation, privatization, and the realization of reductions. Evaluating our implementation on various C programs from different domains, we demonstrate the effectiveness and generality of generalized task parallelization. On a quad-core machine with hyperthreading we achieve speedups of up to 4.6 ×.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2019119315",
    "type": "article"
  },
  {
    "title": "A New Compilation Flow for Software-Defined Radio Applications on Heterogeneous MPSoCs",
    "doi": "https://doi.org/10.1145/2910583",
    "publication_date": "2016-06-06",
    "publication_year": 2016,
    "authors": "Mickaël Dardaillon; Kévin Marquet; Tanguy Risset; Jérôme Martin; Henri‐Pierre Charles",
    "corresponding_authors": "",
    "abstract": "The advent of portable software-defined radio ( sdr ) technology is tightly linked to the resolution of a difficult problem: efficient compilation of signal processing applications on embedded computing devices. Modern wireless communication protocols use packet processing rather than infinite stream processing and also introduce dependencies between data value and computation behavior leading to dynamic dataflow behavior. Recently, parametric dataflow has been proposed to support dynamicity while maintaining the high level of analyzability needed for efficient real-life implementations of signal processing computations. This article presents a new compilation flow that is able to compile parametric dataflow graphs. Built on the llvm compiler infrastructure, the compiler offers an actor-based C++ programming model to describe parametric graphs, a compilation front end for graph analysis, and a back end that currently matches the Magali platform: a prototype heterogeneous MPSoC dedicated to LTE-Advanced. We also introduce an innovative scheduling technique, called microscheduling , allowing one to adapt the mapping of parametric dataflow programs to the specificities of the different possible MPSoCs targeted. A specific focus on fifo sizing on the target architecture is presented. The experimental results show compilation of 3 gpp lte - a dvanced demodulation on Magali with tight memory size constraints. The compiled programs achieve performance similar to handwritten code.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2413645043",
    "type": "article"
  },
  {
    "title": "Exploiting Hierarchical Locality in Deep Parallel Architectures",
    "doi": "https://doi.org/10.1145/2897783",
    "publication_date": "2016-06-14",
    "publication_year": 2016,
    "authors": "Ahmad Anbar; Olivier Serres; Engin Kayraklioglu; Abdel‐Hameed A. Badawy; Tarek El‐Ghazawi",
    "corresponding_authors": "",
    "abstract": "Parallel computers are becoming deeply hierarchical. Locality-aware programming models allow programmers to control locality at one level through establishing affinity between data and executing activities. This, however, does not enable locality exploitation at other levels. Therefore, we must conceive an efficient abstraction of hierarchical locality and develop techniques to exploit it. Techniques applied directly by programmers, beyond the first level, burden the programmer and hinder productivity. In this article, we propose the Parallel Hierarchical Locality Abstraction Model for Execution (PHLAME). PHLAME is an execution model to abstract and exploit machine hierarchical properties through locality-aware programming and a runtime that takes into account machine characteristics, as well as a data sharing and communication profile of the underlying application. This article presents and experiments with concepts and techniques that can drive such runtime system in support of PHLAME. Our experiments show that our techniques scale up and achieve performance gains of up to 88%.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2429918913",
    "type": "article"
  },
  {
    "title": "Extending the WCET Problem to Optimize for Runtime-Reconfigurable Processors",
    "doi": "https://doi.org/10.1145/3014059",
    "publication_date": "2016-12-12",
    "publication_year": 2016,
    "authors": "Marvin Damschen; Lars Bauer; Jörg Henkel",
    "corresponding_authors": "",
    "abstract": "The correctness of a real-time system does not depend on the correctness of its calculations alone but also on the non-functional requirement of adhering to deadlines. Guaranteeing these deadlines by static timing analysis, however, is practically infeasible for current microarchitectures with out-of-order scheduling pipelines, several hardware threads, and multiple (shared) cache layers. Novel timing-analyzable features are required to sustain the strongly increasing demand for processing power in real-time systems. Recent advances in timing analysis have shown that runtime-reconfigurable instruction set processors are one way to escape the scarcity of analyzable processing power while preserving the flexibility of the system. When moving calculations from software to hardware by means of reconfigurable custom instructions (CIs)—additional to a considerable speedup—the overestimation of a task’s worst-case execution time (WCET) can be reduced. CIs typically implement functionality that corresponds to several hundred instructions on the central processing unit (CPU) pipeline. While analyzing instructions for worst-case latency may introduce pessimism, the latency of CIs—executed on the reconfigurable fabric—is precisely known. In this work, we introduce the problem of selecting reconfigurable CIs to optimize the WCET of an application. We model this problem as an extension to state-of-the-art integer linear programming (ILP)-based program path analysis. This way, we enable optimization based on accurate WCET estimates with integration of information about global program flow, for example, infeasible paths. We present an optimal solution with effective techniques to prune the search space and a greedy heuristic that performs a maximum number of steps linear in the number of partitions of reconfigurable area available. Finally, we show the effectiveness of optimizing the WCET on a reconfigurable processor by evaluating a complex multimedia application with multiple reconfigurable CIs for several hardware parameters.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2565732133",
    "type": "article"
  },
  {
    "title": "NEM-GNN - DAC/ADC-less, Scalable, Reconfigurable, Graph and Sparsity-Aware Near-Memory Accelerator for Graph Neural Networks",
    "doi": "https://doi.org/10.1145/3652607",
    "publication_date": "2024-03-14",
    "publication_year": 2024,
    "authors": "Siddhartha Raman Sundara Raman; Lizy K. John; Jaydeep P. Kulkarni",
    "corresponding_authors": "",
    "abstract": "Graph neural networks (GNNs) are of great interest in real-life applications such as citation networks and drug discovery owing to GNN’s ability to apply machine learning techniques on graphs. GNNs utilize a two-step approach to classify the nodes in a graph into pre-defined categories. The first step uses a combination kernel to perform data-intensive convolution operations with regular memory access patterns. The second step uses an aggregation kernel that operates on sparse data having irregular access patterns. These mixed data patterns render CPU/GPU-based compute energy-inefficient. Von Neumann based accelerators like AWB-GCN [ 7 ] suffer from increased data movement, as the data-intensive combination requires large data movement to/from memory to perform computations. ReFLIP [ 8 ] performs resistive random access memory based in-memory (PIM) compute to overcome data movement costs. However, ReFLIP suffers from increased area requirement due to dedicated accelerator arrangement, and reduced performance due to limited parallelism and energy due to fundamental issues in ReRAM-based compute. This article presents a scalable (non-exponential storage requirement), DAC/ADC-less PIM-based combination, with (i) early compute termination and (ii) pre-compute by reconfiguring SOC components. Graph and sparsity-aware near-memory aggregation using the proposed compute-as-soon-as-ready (CAR) broadcast approach improves performance and energy further. NEM-GNN achieves ∼80–230x, ∼80–300x, ∼850–1,134x, and ∼7–8x improvement over ReFLIP, in terms of performance, throughput, energy efficiency, and compute density.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4392806407",
    "type": "article"
  },
  {
    "title": "<i>TEA+</i> : A Novel Temporal Graph Random Walk Engine With Hybrid Storage Architecture",
    "doi": "https://doi.org/10.1145/3652604",
    "publication_date": "2024-03-14",
    "publication_year": 2024,
    "authors": "Chengying Huan; Yongchao Liu; Heng Zhang; Shuaiwen Leon Song; Santosh Pandey; Shiyang Chen; Xiangfei Fang; Yue Jin; Baptiste Lepers; Yanjun Wu; Hang Liu",
    "corresponding_authors": "",
    "abstract": "Many real-world networks are characterized by being temporal and dynamic, wherein the temporal information signifies the changes in connections, such as the addition or removal of links between nodes. Employing random walks on these temporal networks is a crucial technique for understanding the structural evolution of such graphs over time. However, existing state-of-the-art sampling methods are designed for traditional static graphs, and as such, they struggle to efficiently handle the dynamic aspects of temporal networks. This deficiency can be attributed to several challenges, including increased sampling complexity, extensive index space, limited programmability, and a lack of scalability. In this article, we introduce TEA+ , a robust, fast, and scalable engine for conducting random walks on temporal graphs. Central to TEA+ is an innovative hybrid sampling method that amalgamates two Monte Carlo sampling techniques. This fusion significantly diminishes space complexity while maintaining a fast sampling speed. Additionally, TEA+ integrates a range of optimizations that significantly enhance sampling efficiency. This is further supported by an effective graph updating strategy, skilled in managing dynamic graph modifications and adeptly handling the insertion and deletion of both edges and vertices. For ease of implementation, we propose a temporal-centric programming model, designed to simplify the development of various random walk algorithms on temporal graphs. To ensure optimal performance across storage constraints, TEA+ features a degree-aware hybrid storage architecture, capable of adeptly scaling in different memory environments. Experimental results showcase the prowess of TEA+ , as it attains up to three orders of magnitude speedups compared to current random walk engines on extensive temporal graphs.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4392806453",
    "type": "article"
  },
  {
    "title": "Cross-Core Data Sharing for Energy-Efficient GPUs",
    "doi": "https://doi.org/10.1145/3653019",
    "publication_date": "2024-03-18",
    "publication_year": 2024,
    "authors": "Hajar Falahati; Mohammad Sadrosadati; Qiumin Xu; Juan Gómez-Luna; Banafsheh Saber Latibari; Hyeran Jeon; Shaahin Hesaabi; Hamid Sarbazi‐Azad; Onur Mutlu; Murali Annavaram; Masoud Pedram",
    "corresponding_authors": "",
    "abstract": "Graphics Processing Units (GPUs) are the accelerator of choice in a variety of application domains, because they can accelerate massively parallel workloads and can be easily programmed using general-purpose programming frameworks such as CUDA and OpenCL. Each Streaming Multiprocessor (SM) contains an L1 data cache (L1D) to exploit the locality in data accesses. L1D misses are costly for GPUs for two reasons. First, L1D misses consume a lot of energy as they need to access the L2 cache (L2) via an on-chip network and the off-chip DRAM in case of L2 misses. Second, L1D misses impose performance overhead if the GPU does not have enough active warps to hide the long memory access latency. We observe that threads running on different SMs share 55% of the data they read from the memory. Unfortunately, as the L1Ds are in the non-coherent memory domain, each SM independently fetches data from the L2 or the off-chip memory into its L1D, even though the data may be currently available in the L1D of another SM. Our goal is to service L1D read misses via other SMs, as much as possible, to cut down costly accesses to the L2 or the off-chip DRAM. To this end, we propose a new data-sharing mechanism, called Cross-Core Data Sharing (CCDS) . CCDS employs a predictor to estimate whether the required cache block exists in another SM. If the block is predicted to exist in another SM’s L1D, then CCDS fetches the data from the L1D that contain the block. Our experiments on a suite of 26 workloads show that CCDS improves average energy and performance by 1.30× and 1.20×, respectively, compared to the baseline GPU. Compared to the state-of-the-art data-sharing mechanism, CCDS improves average energy and performance by 1.37× and 1.11×, respectively.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4392910976",
    "type": "article"
  },
  {
    "title": "Knowledge-Augmented Mutation-Based Bug Localization for Hardware Design Code",
    "doi": "https://doi.org/10.1145/3660526",
    "publication_date": "2024-04-22",
    "publication_year": 2024,
    "authors": "Jiang Wu; Zhuo Zhang; Deheng Yang; Jianjun Xu; Jiayu He; Xiaoguang Mao",
    "corresponding_authors": "",
    "abstract": "Verification of hardware design code is crucial for the quality assurance of hardware products. Being an indispensable part of verification, localizing bugs in the hardware design code is significant for hardware development but is often regarded as a notoriously difficult and time-consuming task. Thus, automated bug localization techniques that could assist manual debugging have attracted much attention in the hardware community. However, existing approaches are hampered by the challenge of achieving both demanding bug localization accuracy and facile automation in a single method. Simulation-based methods are fully automated but have limited localization accuracy, slice-based techniques can only give an approximate range of the presence of bugs, and spectrum-based techniques can also only yield a reference value for the likelihood that a statement is buggy. Furthermore, formula-based bug localization techniques suffer from the complexity of combinatorial explosion for automated application in industrial large-scale hardware designs. In this work, we propose Kummel, a K nowledge-a u g m ented m utation-bas e d bug loca l ization for hardware design code to address these limitations. Kummel achieves the unity of precise bug localization and full automation by utilizing the knowledge augmentation through mutation analysis. To evaluate the effectiveness of Kummel, we conduct large-scale experiments on 76 versions of 17 hardware projects by seven state-of-the-art bug localization techniques. The experimental results clearly show that Kummel is statistically more effective than baselines, e.g., our approach can improve the seven original methods by 64.48% on average under the RImp metric. It brings fresh insights of hardware bug localization to the community.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4395001508",
    "type": "article"
  },
  {
    "title": "COER: A Network Interface Offloading Architecture for RDMA and Congestion Control Protocol Codesign",
    "doi": "https://doi.org/10.1145/3660525",
    "publication_date": "2024-04-22",
    "publication_year": 2024,
    "authors": "Ke Wu; Dezun Dong; Weixia Xu",
    "corresponding_authors": "",
    "abstract": "RDMA (Remote Direct Memory Access) networks require efficient congestion control to maintain their high throughput and low latency characteristics. However, congestion control protocols deployed at the software layer suffer from slow response times due to the communication overhead between host hardware and software. This limitation has hindered their ability to meet the demands of high-speed networks and applications. Harnessing the capabilities of rapidly advancing Network Interface Cards (NICs) can drive progress in congestion control. Some simple congestion control protocols have been offloaded to RDMA NICs to enable faster detection and processing of congestion. However, offloading congestion control to the RDMA NIC faces a significant challenge in integrating the RDMA transport protocol with advanced congestion control protocols that involve complex mechanisms. We have observed that reservation-based proactive congestion control protocols share strong similarities with RDMA transport protocols, allowing them to integrate seamlessly and combine the functionalities of the transport layer and network layer. In this article, we present COER, an RDMA NIC architecture that leverages the functional components of RDMA to perform reservations and completes the scheduling of congestion control during the scheduling process of the RDMA protocol. COER facilitates the streamlined development of offload strategies for congestion control techniques —specifically, proactive congestion control —on RDMA NICs. We use COER to design offloading schemes for 11 congestion control protocols, which we implement and evaluate using a network emulator with a cycle-accurate RDMA NIC model that can load Message Passing Interface (MPI) programs. The evaluation results demonstrate that the architecture of COER does not compromise the original characteristics of the congestion control protocols. Compared with a layered protocol stack approach, COER enables the performance of RDMA networks to reach new heights.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4395010658",
    "type": "article"
  },
  {
    "title": "Characterizing and Optimizing LDPC Performance on 3D NAND Flash Memories",
    "doi": "https://doi.org/10.1145/3663478",
    "publication_date": "2024-05-03",
    "publication_year": 2024,
    "authors": "Qiao Li; Yu Chen; Guanyu Wu; Yajuan Du; Min Ye; Xinbiao Gan; Jie Zhang; Zhirong Shen; Jiwu Shu; Chun Jason Xue",
    "corresponding_authors": "",
    "abstract": "With the development of NAND flash memories’ bit density and stacking technologies, while storage capacity keeps increasing, the issue of reliability becomes increasingly prominent. Low-density parity check (LDPC) code, as a robust error-correcting code, is extensively employed in flash memory. However, when the RBER is prohibitively high, LDPC decoding would introduce long latency. To study how LDPC performs on the latest 3D NAND flash memory, we conduct a comprehensive analysis of LDPC decoding performance using both the theoretically derived threshold voltage distribution model obtained through modeling (Modeling-based method) and the actual voltage distribution collected from on-chip data through testing (Ideal case). Based on LDPC decoding results under various interference conditions, we summarize four findings that can help us gain a better understanding of the characteristics of LDPC decoding in 3D NAND flash memory. Following our characterization, we identify the differences in LDPC decoding performance between the Modeling-based method and the Ideal case. Due to the accuracy of initial probability information, the threshold voltage distribution derived through modeling deviates by certain degrees from the actual threshold voltage distribution. This leads to a performance gap between using the threshold voltage distribution derived from the Modeling-based method and the actual distribution. By observing the abnormal behaviors in the decoding with the Modeling-based method, we introduce an Offsetted Read Voltage (ΔRV) method for optimizing LDPC decoding performance by offsetting the reading voltage in each layer of a flash block. The evaluation results show that our ΔRV method enhances the decoding performance of LDPC on the Modeling-based method by reducing the total number of sensing levels needed for LDPC decoding by 0.67% to 18.92% for different interference conditions on average, under the P/E cycles from 3,000 to 7,000.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4396624882",
    "type": "article"
  },
  {
    "title": "Scythe: A Low-latency RDMA-enabled Distributed Transaction System for Disaggregated Memory",
    "doi": "https://doi.org/10.1145/3666004",
    "publication_date": "2024-05-27",
    "publication_year": 2024,
    "authors": "Kai Lü; Siqi Zhao; Haikang Shan; Qiang Wei; Guokuan Li; Jiguang Wan; Ting Yao; Huatao Wu; Daohui Wang",
    "corresponding_authors": "",
    "abstract": "Disaggregated memory separates compute and memory resources into independent pools connected by RDMA (Remote Direct Memory Access) networks, which can improve memory utilization, reduce cost, and enable elastic scaling of compute and memory resources. However, existing RDMA-based distributed transactions on disaggregated memory suffer from severe long-tail latency under high-contention workloads. In this article, we propose Scythe, a novel low-latency RDMA-enabled distributed transaction system for disaggregated memory. Scythe optimizes the latency of high-contention transactions in three approaches: (1) Scythe proposes a hot-aware concurrency control policy that uses optimistic concurrency control (OCC) to improve transaction processing efficiency in low-conflict scenarios. Under high conflicts, Scythe designs a timestamp-ordered OCC (TOCC) strategy based on fair locking to reduce the number of retries and cross-node communication overhead. (2) Scythe presents an RDMA-friendly timestamp service for improved timestamp management. And, (3) Scythe designs an RDMA-optimized RPC framework to improve RDMA bandwidth utilization. The evaluation results show that, compared with state-of-the-art distributed transaction systems, Scythe achieves more than 2.5× lower latency with 1.8× higher throughput under high-contention workloads.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4399054679",
    "type": "article"
  },
  {
    "title": "GraphService: Topology-aware Constructor for Large-scale Graph Applications",
    "doi": "https://doi.org/10.1145/3689341",
    "publication_date": "2024-08-17",
    "publication_year": 2024,
    "authors": "Xinbiao Gan",
    "corresponding_authors": "Xinbiao Gan",
    "abstract": "Graph-based services are becoming integrated into everyday life through graph applications and graph learning systems. While traditional graph processing approaches boast excellent throughput with millisecond-level processing time, the construction phase before executing kernel graph operators (e.g., BFS, SSSP) can take up to tens of hours, severely impacting the quality of graph service. Is it feasible to develop a fast graph constructor that can complete the construction process within minutes, or even seconds? This paper aims to answer this question. We present GraphService , a flexible and efficient graph constructor for fast graph applications. To facilitate graph applications with better service, we equip GraphService with a hierarchy-aware graph partitioner based on communication topology, as well as a graph topology-aware compression by exploiting a huge number of identical-degree vertices within graph topology. Our evaluation, performed on a range of graph operations and datasets, shows that GraphService significantly reduces communication cost by three orders of magnitude improvement to construct a graph. Furthermore, we tailor GraphService for downstream graph tasks and deploy it on a production supercomputer using 79,024 computing nodes, achieving a remarkable graph processing throughput that outperforms the top-ranked supercomputer on the latest Graph500 list, with construction time reduced by orders of magnitude.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4401657013",
    "type": "article"
  },
  {
    "title": "Potamoi: Accelerating Neural Rendering via a Unified Streaming Architecture",
    "doi": "https://doi.org/10.1145/3689340",
    "publication_date": "2024-08-21",
    "publication_year": 2024,
    "authors": "Yu Feng; Weikai Lin; Zihan Liu; Jingwen Leng; Minyi Guo; Han Zhao; Xiaofeng Hou; Jieru Zhao; Yuhao Zhu",
    "corresponding_authors": "",
    "abstract": "Neural Radiance Field (NeRF) has emerged as a promising alternative for photorealistic rendering. Despite recent algorithmic advancements, achieving real-time performance on today’s resource-constrained devices remains challenging. In this article, we identify the primary bottlenecks in current NeRF algorithms and introduce a unified algorithm-architecture co-design, Potamoi , designed to accommodate various NeRF algorithms. Specifically, we introduce a runtime system featuring a plug-and-play algorithm, SpaRW , which significantly reduces the per-frame computational workload and alleviates compute inefficiencies. Furthermore, our unified streaming pipeline coupled with customized hardware support effectively tames both SRAM and DRAM inefficiencies by minimizing repetitive DRAM access and completely eliminating SRAM bank conflicts. When evaluated against a baseline utilizing a dedicated DNN accelerator, our framework demonstrates a speedup and energy reduction of 53.1× and 67.7×, respectively, all while maintaining high visual quality with less than a 1.0 dB reduction in peak signal-to-noise ratio.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4401725674",
    "type": "article"
  },
  {
    "title": "An Optimized GPU Implementation for GIST Descriptor",
    "doi": "https://doi.org/10.1145/3689339",
    "publication_date": "2024-08-23",
    "publication_year": 2024,
    "authors": "Xiang Li; Qiong Chang; Aolong Zha; Shijie Chang; Yun Li; Jun Miyazaki",
    "corresponding_authors": "",
    "abstract": "The GIST descriptor is a classic feature descriptor primarily used for scene categorization and recognition tasks. It drives a bank of Gabor filters, which respond to edges and textures at various scales and orientations to capture the spatial structures in an image. Compared to other scene recognition algorithms that rely on detailed object detection, GIST has lower computational complexity, allowing it to be widely applied. However, its internal multi-scale and multi-orientation Gabor filters also mean that systems based on it cannot be executed fast enough. This article proposes an optimized GPU kernel for the GIST descriptor. It fully takes advantage of the symmetry of Gabor filters and proposes different optimization strategies for both oblique and orthogonal orientations. Extensive experiments demonstrate that the proposed kernel is adaptable to images of various scales and different GPUs. Compared to the cuFFT library, our kernel achieves 12.09× and 3.86× acceleration on an RTX 3080 GPU and a Jetson AGX Xavier GPU, respectively.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4401818004",
    "type": "article"
  },
  {
    "title": "ShieldCXL: A Practical Obliviousness Support with Sealed CXL Memory",
    "doi": "https://doi.org/10.1145/3703354",
    "publication_date": "2024-11-04",
    "publication_year": 2024,
    "authors": "Key‐Sun Choi; I Jong Kim; S. R. Lee; Jaehyuk Huh",
    "corresponding_authors": "",
    "abstract": "The CXL (Compute Express Link) technology is an emerging memory interface with high-level commands. Recent studies applied the CXL memory expanding technique to mitigate the capacity limitation of the conventional DDRx memory. Unlike the prior studies to use the CXL memory as the capacity expander, this study proposes to use the CXL-based memory as a secure main memory device, while removing the conventional memory. In the conventional DDRx memory, to provide confidentiality, integrity, replay protection, and obliviousness, costly mechanisms such as counter-based integrity trees and location shuffling by ORAM (Oblivious RAM) are used. Such mechanisms incur significant performance degradation in the current DDR-based memory systems, and their costs increase as the capacity of the memory increases. To mitigate the performance degradation, the prior work proposed an obfuscated channel for a secure memory module enclosing its controller in the package. Based on the approach, we propose a secure CXL-only memory architecture called ShieldCXL . It uses the channel encryption and integrity protection mechanism of the CXL interface to provide a practical ORAM while supporting confidentiality, integrity, and replay protection from physical attacks and rowhammers. To protect the PCIe-connected memory expanding board, this study proposes to use the standard physical sealing technique to detect physical intrusion. To mitigate the increased latency with the sealed CXL memory module, the study further optimizes performance by adopting an in-package DRAM cache. In addition, this study investigates destination obfuscation when a CXL switch is used to route among multiple hosts and memory devices. The evaluation shows that ShieldCXL provides 9.16x performance improvements over the prior ORAM technique.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4404047513",
    "type": "article"
  },
  {
    "title": "The optimum pipeline depth considering both power and performance",
    "doi": "https://doi.org/10.1145/1044823.1044824",
    "publication_date": "2004-12-01",
    "publication_year": 2004,
    "authors": "A. Hartstein; Thomas R. Puzak",
    "corresponding_authors": "",
    "abstract": "The impact of pipeline length on both the power and performance of a microprocessor is explored both by theory and by simulation. A theory is presented for a range of power/performance metrics, BIPS m /W . The theory shows that the more important power is to the metric, the shorter the optimum pipeline length that results. For typical parameters neither BIPS/W nor BIPS 2 /W yield an optimum, i.e., a non-pipelined design is optimal. For BIPS 3 /W the optimum, averaged over all 55 workloads studied, occurs at a 22.5 FO4 design point, a 7 stage pipeline, but this value is highly dependent on the assumed growth in latch count with pipeline depth. As dynamic power grows, the optimal design point shifts to shorter pipelines. Clock gating pushes the optimum to deeper pipelines. Surprisingly, as leakage power grows, the optimum is also found to shift to deeper pipelines. The optimum pipeline depth varies for different classes of workloads: SPEC95 and SPEC2000 integer applications, traditional (legacy) database and on-line transaction processing applications, modern (e. g. web) applications, and floating point applications.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2036741467",
    "type": "article"
  },
  {
    "title": "A compiler framework for speculative optimizations",
    "doi": "https://doi.org/10.1145/1022969.1022970",
    "publication_date": "2004-09-01",
    "publication_year": 2004,
    "authors": "Jin Lin; Tong Chen; Wei‐Chung Hsu; Pen-Chung Yew; Roy Dz-Ching Ju; Tin‐Fook Ngai; Sun Chan",
    "corresponding_authors": "",
    "abstract": "Speculative execution, such as control speculation or data speculation, is an effective way to improve program performance. Using edge/path profile information or simple heuristic rules, existing compiler frameworks can adequately incorporate and exploit control speculation. However, very little has been done so far to allow existing compiler frameworks to incorporate and exploit data speculation effectively in various program transformations beyond instruction scheduling. This paper proposes a speculative static single assignment form to incorporate information from alias profiling and/or heuristic rules for data speculation, thus allowing existing frameworks to be extended to support both control and data speculation. Such a general framework is very useful for EPIC architectures that provide run-time checking (such as advanced load address table ) on data speculation to guarantee the correctness of program execution. We use SSAPRE as one example to illustrate how to incorporate data speculation in partial redundancy elimination, register promotion, and strength reduction. Our extended framework allows both control and data speculations to be performed on top of SSAPRE and, thus, enables more aggressive speculative optimizations. The proposed framework has been implemented on Intel's Open Research Compiler. We present experimental data on some SPEC2000 benchmark programs to demonstrate the usefulness of this framework.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2065596062",
    "type": "article"
  },
  {
    "title": "Memory-level parallelism aware fetch policies for simultaneous multithreading processors",
    "doi": "https://doi.org/10.1145/1509864.1509867",
    "publication_date": "2009-03-30",
    "publication_year": 2009,
    "authors": "Stijn Eyerman; Lieven Eeckhout",
    "corresponding_authors": "",
    "abstract": "A thread executing on a simultaneous multithreading (SMT) processor that experiences a long-latency load will eventually stall while holding execution resources. Existing long-latency load aware SMT fetch policies limit the amount of resources allocated by a stalled thread by identifying long-latency loads and preventing the thread from fetching more instructions—and in some implementations, instructions beyond the long-latency load are flushed to release allocated resources. This article proposes an SMT fetch policy that takes into account the available memory-level parallelism (MLP) in a thread. The key idea proposed in this article is that in case of an isolated long-latency load (i.e., there is no MLP), the thread should be prevented from allocating additional resources. However, in case multiple independent long-latency loads overlap (i.e., there is MLP), the thread should allocate as many resources as needed in order to fully expose the available MLP. MLP-aware fetch policies achieve better performance for MLP-intensive threads on SMT processors, leading to higher overall system throughput and shorter average turnaround time than previously proposed fetch policies.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2116360965",
    "type": "article"
  },
  {
    "title": "Addressing thermal nonuniformity in SMT workloads",
    "doi": "https://doi.org/10.1145/1369396.1369400",
    "publication_date": "2008-05-01",
    "publication_year": 2008,
    "authors": "Jonathan A. Winter; David H. Albonesi",
    "corresponding_authors": "",
    "abstract": "We explore DTM techniques within the context of uniform and nonuniform SMT workloads. While DVS is suitable for addressing workloads with uniformly high temperatures, for nonuniform workloads, performance loss occurs because of the slowdown of the cooler thread. To address this, we propose and evaluate DTM mechanisms that exploit the steering-based thread management mechanisms inherent in a clustered SMT architecture. We show that in contrast to DVS, which operates globally, our techniques are more effective at controlling temperature for nonuniform workloads. Furthermore, we devise a DTM technique that combines steering and DVS to achieve consistently good performance across all workloads.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2158857966",
    "type": "article"
  },
  {
    "title": "A hardware/software framework for instruction and data scratchpad memory allocation",
    "doi": "https://doi.org/10.1145/1736065.1736067",
    "publication_date": "2010-04-01",
    "publication_year": 2010,
    "authors": "Zhong-Ho Chen; Alvin W. Su",
    "corresponding_authors": "",
    "abstract": "Previous researches show that a scratchpad memory device consumed less energy than a cache device with the same capacity. In this article, we locate the scratchpad memory (SPM) in the top level of the memory hierarchy to reduce the energy consumption. To take the advantage of a SPM, we address two issues of utilizing a SPM. First, the program's locality should be improved. The second issue is SPM management. To tackle these two issues, we present a hardware/software framework for dynamically allocating both instructions and data in SPM. The software flow could be divided into three phases: locality improving, locality extraction, and runtime SPM management. Without modifying the original compiler and the source code, we improve the locality of a program. An optimization algorithm is proposed to extract the SPM allocations. At runtime, an SPM management program is employed. In hardware, an address translation logic (ATL) is proposed to reduce the overhead of SPM management. The results show that the proposed framework can reduce energy delay product (EDP) by 63%, on average, when compared with the traditional cache architecture. The reduction in EDP is contributed by properly allocating both instructions and data in SPM. By allocating only instructions in SPM, the EDPs are reduced by 45%, on average. By allocating only data in SPM, the EDPs are reduced by 14%, on average.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2033339079",
    "type": "article"
  },
  {
    "title": "Automatic feedback-directed object fusing",
    "doi": "https://doi.org/10.1145/1839667.1839669",
    "publication_date": "2010-09-01",
    "publication_year": 2010,
    "authors": "Christian Wimmer; Hanspeter Mössenbösck",
    "corresponding_authors": "",
    "abstract": "Object fusing is an optimization that embeds certain referenced objects into their referencing object. The order of objects on the heap is changed in such a way that objects that are accessed together are placed next to each other in memory. Their offset is then fixed, that is, the objects are colocated, allowing field loads to be replaced by address arithmetic. Array fusing specifically optimizes arrays, which are frequently used for the implementation of dynamic data structures. Therefore, the length of arrays often varies, and fields referencing such arrays have to be changed. An efficient code pattern detects these changes and allows the optimized access of such fields. We integrated these optimizations into Sun Microsystems' Java HotSpot™ VM. The analysis is performed automatically at runtime, requires no actions on the part of the programmer, and supports dynamic class loading. To safely eliminate a field load, the colocation of the object that holds the field and the object that is referenced by the field must be guaranteed. Two preconditions must be satisfied: The objects must be allocated at the same time, and the field must not be overwritten later. These preconditions are checked by the just-in-time compiler to avoid an interprocedural data flow analysis. The garbage collector ensures that groups of colocated objects are not split by copying groups as a whole. The evaluation shows that the dynamic approach successfully identifies and optimizes frequently accessed fields for several benchmarks with a low compilation and analysis overhead. It leads to a speedup of up to 76% for simple benchmarks and up to 6% for complex workloads.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2047248271",
    "type": "article"
  },
  {
    "title": "Exploring the limits of early register release",
    "doi": "https://doi.org/10.1145/1582710.1582714",
    "publication_date": "2009-09-01",
    "publication_year": 2009,
    "authors": "Timothy M. Jones; Michael O’Boyle; Jaume Abella; Antonio González; Oğuz Ergin",
    "corresponding_authors": "",
    "abstract": "Register pressure in modern superscalar processors can be reduced by releasing registers early and by copying their contents to cheap back-up storage. This article quantifies the potential benefits of register occupancy reduction and shows that existing hardware-based schemes typically achieve only a small fraction of this potential. This is because they are unable to accurately determine the last use of a register and must wait until the redefining instruction enters the pipeline. On the other hand, compilers have a global view of the program and, using simple dataflow analysis, can determine the last use. This article evaluates the extent to which compiler analysis can aid early releasing, explores the design space, and introduces commit and issue-based early releasing schemes, quantifying their benefits. Using simple compiler analysis and microarchitecture changes, we achieve 70% of the potential register file occupancy reduction. By adding more hardware support, we can increase this to 94%. Our schemes are compared to state-of-the-art approaches for varying register file sizes and are shown to outperform these existing techniques.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2168870607",
    "type": "article"
  },
  {
    "title": "Metric Selection for GPU Kernel Classification",
    "doi": "https://doi.org/10.1145/3295690",
    "publication_date": "2018-12-31",
    "publication_year": 2018,
    "authors": "S. Kazem Shekofteh; Hamid Noori; Mahmoud Naghibzadeh; Hadi Sadoghi Yazdi; Holger Fröning",
    "corresponding_authors": "",
    "abstract": "Graphics Processing Units (GPUs) are vastly used for running massively parallel programs. GPU kernels exhibit different behavior at runtime and can usually be classified in a simple form as either “compute-bound” or “memory-bound.” Recent GPUs are capable of concurrently running multiple kernels, which raises the question of how to most appropriately schedule kernels to achieve higher performance. In particular, co-scheduling of compute-bound and memory-bound kernels seems promising. However, its benefits as well as drawbacks must be determined along with which kernels should be selected for a concurrent execution. Classifying kernels can be performed online by instrumentation based on performance counters. This work conducts a thorough analysis of the metrics collected from various benchmarks from Rodinia and CUDA SDK. The goal is to find the minimum number of effective metrics that enables online classification of kernels with a low overhead. This study employs a wrapper-based feature selection method based on the Fisher feature selection criterion. The results of experiments show that to classify kernels with a high accuracy, only three and five metrics are sufficient on a Kepler and a Pascal GPU, respectively. The proposed method is then utilized for a runtime scheduler. The results show an average speedup of 1.18× and 1.1× compared with a serial and a random scheduler, respectively.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2910063209",
    "type": "article"
  },
  {
    "title": "Deterministic finite automata characterization and optimization for scalable pattern matching",
    "doi": "https://doi.org/10.1145/1952998.1953002",
    "publication_date": "2011-02-05",
    "publication_year": 2011,
    "authors": "Lucas Vespa; Ning Weng",
    "corresponding_authors": "",
    "abstract": "Memory-based Deterministic Finite Automata (DFA) are ideal for pattern matching in network intrusion detection systems due to their deterministic performance and ease of update of new patterns, however severe DFA memory requirements make it impractical to implement thousands of patterns. This article aims to understand the basic relationship between DFA characteristics and memory requirements, and to design a practical memory-based pattern matching engine. We present a methodology that consists of theoretical DFA characterization, encoding optimization, and implementation architecture. Results show the validity of the characterization metrics, effectiveness of the encoding techniques, and efficiency of the memory-based pattern engines.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W1974429469",
    "type": "article"
  },
  {
    "title": "E <sup>3</sup> CC: A memory error protection scheme with novel address mapping for subranked and low-power memories",
    "doi": "https://doi.org/10.1145/2541228.2541239",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Long Chen; Yanan Cao; Zhao Zhang",
    "corresponding_authors": "",
    "abstract": "This study presents and evaluates E 3 CC (Enhanced Embedded ECC), a full design and implementation of a generic embedded ECC scheme that enables power-efficient error protection for subranked memory systems. It incorporates a novel address mapping scheme called Biased Chinese Remainder Mapping (BCRM) to resolve the address mapping issue for memories of page interleaving, plus a simple and effective cache design to reduce extra ECC traffic. Our evaluation using SPEC CPU2006 benchmarks confirms the performance and power efficiency of the E 3 CC scheme for subranked memories as well as conventional memories.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W1977575464",
    "type": "article"
  },
  {
    "title": "Combining recency of information with selective random and a victim cache in last-level caches",
    "doi": "https://doi.org/10.1145/2355585.2355589",
    "publication_date": "2012-09-01",
    "publication_year": 2012,
    "authors": "Alejandro Valero; Julio Sahuquillo; Salvador Petit; Pedro López; J. Duato",
    "corresponding_authors": "",
    "abstract": "Memory latency has become an important performance bottleneck in current microprocessors. This problem aggravates as the number of cores sharing the same memory controller increases. To palliate this problem, a common solution is to implement cache hierarchies with large or huge Last-Level Cache (LLC) organizations. LLC memories are implemented with a high number of ways (e.g., 16) to reduce conflict misses. Typically, caches have implemented the LRU algorithm to exploit temporal locality, but its performance goes away from the optimal as the number of ways increases. In addition, the implementation of a strict LRU algorithm is costly in terms of area and power. This article focuses on a family of low-cost replacement strategies, whose implementation scales with the number of ways while maintaining the performance. The proposed strategies track the accessing order for just a few blocks, which cannot be replaced. The victim is randomly selected among those blocks exhibiting poor locality. Although, in general, the random policy helps improving the performance, in some applications the scheme fails with respect to the LRU policy leading to performance degradation. This drawback can be overcome by the addition of a small victim cache of the large LLC. Experimental results show that, using the best version of the family without victim cache, MPKI reduction falls in between 10% and 11% compared to a set of the most representative state-of-the-art algorithms, whereas the reduction grows up to 22% with respect to LRU. The proposal with victim cache achieves speedup improvements, on average, by 4% compared to LRU. In addition, it reduces dynamic energy, on average, up to 8%. Finally, compared to the studied algorithms, hardware complexity is largely reduced by the baseline algorithm of the family.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2007884410",
    "type": "article"
  },
  {
    "title": "Selecting representative benchmark inputs for exploring microprocessor design spaces",
    "doi": "https://doi.org/10.1145/2541228.2555294",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Maximilien Breughe; Lieven Eeckhout",
    "corresponding_authors": "",
    "abstract": "The design process of a microprocessor requires representative workloads to steer the search process toward an optimum design point for the target application domain. However, considering a broad set of workloads to cover the large space of potential workloads is infeasible given how time-consuming design space exploration typically is. Hence, it is crucial to select a small yet representative set of workloads, which leads to a shorter design cycle while yielding a (near) optimal design. Prior work has mostly looked into selecting representative benchmarks; however, limited attention was given to the selection of benchmark inputs and how this affects workload representativeness during design space exploration. Using a set of 1,000 inputs for a number of embedded benchmarks and a design space with around 1,700 design points, we find that selecting a single or three random input(s) per benchmark potentially (in a worst-case scenario) leads to a suboptimal design that is 56% and 33% off, on average, relative to the optimal design in our design space in terms of Energy-Delay Product (EDP). We then propose and evaluate a number of methods for selecting representative inputs and show that we can find the optimum design point with as few as three inputs.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2014522258",
    "type": "article"
  },
  {
    "title": "Modeling the impact of permanent faults in caches",
    "doi": "https://doi.org/10.1145/2541228.2541236",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Daniel Sánchez; Yiannakis Sazeides; Juan M. Cebrián; José M. Garcı́a; Juan L. Aragón",
    "corresponding_authors": "",
    "abstract": "The traditional performance cost benefits we have enjoyed for decades from technology scaling are challenged by several critical constraints including reliability. Increases in static and dynamic variations are leading to higher probability of parametric and wear-out failures and are elevating reliability into a prime design constraint. In particular, SRAM cells used to build caches that dominate the processor area are usually minimum sized and more prone to failure. It is therefore of paramount importance to develop effective methodologies that facilitate the exploration of reliability techniques for caches. To this end, we present an analytical model that can determine for a given cache configuration, address trace, and random probability of permanent cell failure the exact expected miss rate and its standard deviation when blocks with faulty bits are disabled. What distinguishes our model is that it is fully analytical, it avoids the use of fault maps, and yet, it is both exact and simpler than previous approaches. The analytical model is used to produce the miss-rate trends ( expected miss-rate ) for future technology nodes for both uncorrelated and clustered faults. Some of the key findings based on the proposed model are (i) block disabling has a negligible impact on the expected miss-rate unless probability of failure is equal or greater than 2.6e-4, (ii) the fault map methodology can accurately calculate the expected miss-rate as long as 1,000 to 10,000 fault maps are used, and (iii) the expected miss-rate for execution of parallel applications increases with the number of threads and is more pronounced for a given probability of failure as compared to sequential execution.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2056592359",
    "type": "article"
  },
  {
    "title": "PCantorSim",
    "doi": "https://doi.org/10.1145/2541228.2555305",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Chuntao Jiang; Zhibin Yu; Hai Jin; Chengzhong Xu; Lieven Eeckhout; Wim Heirman; Trevor E. Carlson; Xiaofei Liao",
    "corresponding_authors": "",
    "abstract": "Computer architects rely heavily on microarchitecture simulation to evaluate design alternatives. Unfortunately, cycle-accurate simulation is extremely slow, being at least 4 to 6 orders of magnitude slower than real hardware. This longstanding problem is further exacerbated in the multi-/many-core era, because single-threaded simulation performance has not improved much, while the design space has expanded substantially. Parallel simulation is a promising approach, yet does not completely solve the simulation challenge. Furthermore, existing sampling techniques, which are widely used for single-threaded applications, do not readily apply to multithreaded applications as thread interaction and synchronization must now be taken into account. This work presents PCantorSim , a novel Cantor set (a classic fractal)--based sampling scheme to accelerate parallel simulation of multithreaded applications. Through the use of the proposed methodology, only less than 5% of an application's execution time is simulated in detail. We have implemented our approach in Sniper (a parallel multicore simulator) and evaluated it by running the PARSEC benchmarks on a simulated 8-core system. The results show that PCantorSim increases simulation speed over detailed parallel simulation by a factor of 20×, on average, with an average absolute execution time prediction error of 5.3%.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2086646109",
    "type": "article"
  },
  {
    "title": "Significance-Aware Program Execution on Unreliable Hardware",
    "doi": "https://doi.org/10.1145/3058980",
    "publication_date": "2017-04-28",
    "publication_year": 2017,
    "authors": "Konstantinos Parasyris; Vassilis Vassiliadis; Christos D. Antonopoulos; Spyros Lalis; Nikolaos Bellas",
    "corresponding_authors": "",
    "abstract": "This article introduces a significance-centric programming model and runtime support that sets the supply voltage in a multicore CPU to sub-nominal values to reduce the energy footprint and provide mechanisms to control output quality. The developers specify the significance of application tasks respecting their contribution to the output quality and provide check and repair functions for handling faults. On a multicore system, we evaluate five benchmarks using an energy model that quantifies the energy reduction. When executing the least-significant tasks unreliably, our approach leads to 20% CPU energy reduction with respect to a reliable execution and has minimal quality degradation.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2608135450",
    "type": "article"
  },
  {
    "title": "Fuse",
    "doi": "https://doi.org/10.1145/3148054",
    "publication_date": "2017-12-05",
    "publication_year": 2017,
    "authors": "Richard Neill; Andi Drebes; Antoniu Pop",
    "corresponding_authors": "",
    "abstract": "Collecting hardware event counts is essential to understanding program execution behavior. Contemporary systems offer few Performance Monitoring Counters (PMCs), thus only a small fraction of hardware events can be monitored simultaneously. We present new techniques to acquire counts for all available hardware events with high accuracy by multiplexing PMCs across multiple executions of the same program, then carefully reconciling and merging the multiple profiles into a single, coherent profile. We present a new metric for assessing the similarity of statistical distributions of event counts and show that our execution profiling approach performs significantly better than Hardware Event Multiplexing.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2623247943",
    "type": "article"
  },
  {
    "title": "Energy-Efficient Compilation of Irregular Task-Parallel Loops",
    "doi": "https://doi.org/10.1145/3136063",
    "publication_date": "2017-11-14",
    "publication_year": 2017,
    "authors": "Rahul Shrivastava; V. Krishna Nandivada",
    "corresponding_authors": "",
    "abstract": "Energy-efficient compilation is an important problem for multi-core systems. In this context, irregular programs with task-parallel loops present interesting challenges: the threads with lesser work-loads ( non-critical -threads) wait at the join-points for the thread with maximum work-load ( critical -thread); this leads to significant energy wastage. This problem becomes more interesting in the context of multi-socket-multi-core (MSMC) systems, where different sockets may run at different frequencies, but all the cores connected to a socket run at a single frequency. In such a configuration, even though the load-imbalance among the cores may be significant, an MSMC-oblivious technique may miss the opportunities to reduce energy consumption, if the load-imbalance across the sockets is minimal. This problem becomes further challenging in the presence of mutual-exclusion, where scaling the frequencies of a socket executing the non-critical-threads can impact the execution time of the critical-threads. In this article, we propose a scheme (X10Ergy) to obtain energy gains with minimal impact on the execution time, for task-parallel languages, such as X10, HJ, and so on. X10Ergy takes as input a loop-chunked program (parallel-loop iterations divided into chunks and each chunk is executed by a unique thread). X10Ergy follows a mixed compile-time + runtime approach that (i) uses static analysis to efficiently compute the work-load of each chunk at runtime, (ii) computes the “remaining” work-load of the chunks running on the cores of each socket at regular intervals and tunes the frequency of the sockets accordingly, (iii) groups the threads into different sockets (based on the remaining work-load of their respective chunks), and (iv) in the presence of atomic-blocks, models the effect of frequency-scaling on the critical-thread. We implemented X10Ergy for X10 and have obtained encouraging results for the IMSuite kernels.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2770722083",
    "type": "article"
  },
  {
    "title": "A Framework for Automated and Controlled Floating-Point Accuracy Reduction in Graphics Applications on GPUs",
    "doi": "https://doi.org/10.1145/3151032",
    "publication_date": "2017-12-05",
    "publication_year": 2017,
    "authors": "Alexandra Angerd; Erik Sintorn; Per Stenström",
    "corresponding_authors": "",
    "abstract": "Reducing the precision of floating-point values can improve performance and/or reduce energy expenditure in computer graphics, among other, applications. However, reducing the precision level of floating-point values in a controlled fashion needs support both at the compiler and at the microarchitecture level. At the compiler level, a method is needed to automate the reduction of precision of each floating-point value. At the microarchitecture level, a lower precision of each floating-point register can allow more floating-point values to be packed into a register file. This, however, calls for new register file organizations. This article proposes an automated precision-selection method and a novel GPU register file organization that can store floating-point register values at arbitrary precisions densely. The automated precision-selection method uses a data-driven approach for setting the precision level of floating-point values, given a quality threshold and a representative set of input data. By allowing a small, but acceptable, degradation in output quality, our method can remove a significant amount of the bits needed to represent floating-point values in the investigated kernels (between 28% and 60%). Our proposed register file organization exploits these lower-precision floating-point values by packing several of them into the same physical register. This reduces the register pressure per thread by up to 48%, and by 27% on average, for a negligible output-quality degradation. This can enable GPUs to keep up to twice as many threads in flight simultaneously.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2771613338",
    "type": "article"
  },
  {
    "title": "SLOOP",
    "doi": "https://doi.org/10.1145/3148053",
    "publication_date": "2017-12-05",
    "publication_year": 2017,
    "authors": "Muhammad Waqar Azhar; Per Stenström; Vassilis Papaefstathiou",
    "corresponding_authors": "",
    "abstract": "Most systems allocate computational resources to each executing task without any actual knowledge of the application’s Quality-of-Service (QoS) requirements. Such best-effort policies lead to overprovisioning of the resources and increase energy loss. This work assumes applications with soft QoS requirements and exploits the inherent timing slack to minimize the allocated computational resources to reduce energy consumption. We propose a lightweight progress-tracking methodology based on the outer loops of application kernels. It builds on online history and uses it to estimate the total execution time. The prediction of the execution time and the QoS requirements are then used to schedule the application on a heterogeneous architecture with big out-of-order cores and small (LITTLE) in-order cores and select the minimum operating frequency, using DVFS, that meets the deadline. Our scheme is effective in exploiting the timing slack of each application. We show that it can reduce the energy consumption by more than 20% without missing any computational deadlines.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2772163480",
    "type": "article"
  },
  {
    "title": "SIMPO",
    "doi": "https://doi.org/10.1145/3167972",
    "publication_date": "2018-03-22",
    "publication_year": 2018,
    "authors": "Mingzhe Zhang; King Tin Lam; Xin Yao; Cho‐Li Wang",
    "corresponding_authors": "",
    "abstract": "While CPU architectures are incorporating many more cores to meet ever-bigger workloads, advance in fault-tolerance support is indispensable for sustaining system performance under reliability constraints. Emerging non-volatile memory technologies are yielding fast, dense, and energy-efficient NVRAM that can dethrone SSD drives for persisting data. Research on using NVRAM to enable fast in-memory data persistence is ongoing. In this work, we design and implement a persistent object framework, dubbed scalable in-memory persistent object (SIMPO) , which exploits NVRAM, alongside DRAM, to support efficient object persistence in highly threaded big data applications. Based on operation logging, we propose a new programming model that classifies functions into instant and deferrable groups. SIMPO features a streamlined execution model, which allows lazy evaluation of deferrable functions and is well suited to big data computing workloads that would see improved data locality and concurrency. Our log recording and checkpointing scheme is effectively optimized towards NVRAM, mitigating its long write latency through write-combining and consolidated flushing techniques. Efficient persistent object management with features including safe references and memory leak prevention is also implemented and tailored to NVRAM. We evaluate a wide range of SIMPO-enabled applications with machine learning, high-performance computing, and database workloads on an emulated hybrid memory architecture and a real hybrid memory machine with NVDIMM. Compared with native applications without persistence, experimental results show that SIMPO incurs less than 5% runtime overhead on both platforms and even gains up to 2.5× speedup and 84% increase in throughput in highly threaded situations on the two platforms, respectively, thanks to the streamlined execution model.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2790090124",
    "type": "article"
  },
  {
    "title": "Benzene",
    "doi": "https://doi.org/10.1145/3177963",
    "publication_date": "2018-03-22",
    "publication_year": 2018,
    "authors": "Namhyung Kim; Junwhan Ahn; Ki‐Young Choi; Daniel Sánchez; Donghoon Yoo; Soojung Ryu",
    "corresponding_authors": "",
    "abstract": "This article proposes Benzene, an energy-efficient distributed SRAM/STT-RAM hybrid cache for manycore systems running multiple applications. It is based on the observation that a naïve application of hybrid cache techniques to distributed caches in a manycore architecture suffers from limited energy reduction due to uneven utilization of scarce SRAM. We propose two-level optimization techniques: intra-bank and inter-bank. Intra-bank optimization leverages highly associative cache design, achieving more uniform distribution of writes within a bank. Inter-bank optimization evenly balances the amount of write-intensive data across the banks. Our evaluation results show that Benzene significantly reduces energy consumption of distributed hybrid caches.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2790582756",
    "type": "article"
  },
  {
    "title": "Extending Moore’s Law via Computationally Error-Tolerant Computing",
    "doi": "https://doi.org/10.1145/3177837",
    "publication_date": "2018-03-22",
    "publication_year": 2018,
    "authors": "Bobin Deng; Sriseshan Srikanth; Eric R. Hein; Thomas M. Conte; Erik P. DeBenedictis; Jeanine Cook; Michael P. Frank",
    "corresponding_authors": "",
    "abstract": "Dennard scaling has ended. Lowering the voltage supply ( V dd ) to sub-volt levels causes intermittent losses in signal integrity, rendering further scaling (down) no longer acceptable as a means to lower the power required by a processor core. However, it is possible to correct the occasional errors caused due to lower V dd in an efficient manner and effectively lower power. By deploying the right amount and kind of redundancy, we can strike a balance between overhead incurred in achieving reliability and energy savings realized by permitting lower V dd . One promising approach is the Redundant Residue Number System (RRNS) representation. Unlike other error correcting codes, RRNS has the important property of being closed under addition, subtraction and multiplication, thus enabling computational error correction at a fraction of an overhead compared to conventional approaches. We use the RRNS scheme to design a Computationally-Redundant, Energy-Efficient core, including the microarchitecture, Instruction Set Architecture (ISA) and RRNS centered algorithms. From the simulation results, this RRNS system can reduce the energy-delay-product by about 3× for multiplication intensive workloads and by about 2× in general, when compared to a non-error-correcting binary core.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2792850741",
    "type": "article"
  },
  {
    "title": "Efficient and Scalable Graph Parallel Processing With Symbolic Execution",
    "doi": "https://doi.org/10.1145/3170434",
    "publication_date": "2018-03-22",
    "publication_year": 2018,
    "authors": "Long Zheng; Xiaofei Liao; Hai Jin",
    "corresponding_authors": "",
    "abstract": "Existing graph processing essentially relies on the underlying iterative execution with synchronous (Sync) and/or asynchronous (Async) engine. Nevertheless, they both suffer from a wide class of inherent serialization arising from data interdependencies within a graph. In this article, we present SymGraph, a judicious graph engine with symbolic iteration that enables the parallelism of dependent computation on vertices. SymGraph allows using abstract symbolic value (instead of the concrete value) for the computation if the desired data is unavailable. To maximize the potential of symbolic iteration, we propose a chain of tailored sophisticated techniques, enabling SymGraph to scale out with a new milestone of efficiency for large-scale graph processing. We evaluate SymGraph in comparison to Sync, Async, and a hybrid of Sync and Async engines. Our results on 12 nodes show that SymGraph outperforms all three graph engines by 1.93x (vs. Sync), 1.98x (vs. Async), and 1.57x (vs. Hybrid) on average. In particular, the performance for PageRank on 32 nodes can be dramatically improved by 16.5x (vs. Sync), 23.3x (vs. Async), and 12.1x (vs. Hybrid), respectively. The efficiency of SymGraph is also validated with at least one order of magnitude improvement in contrast to three specialized graph systems (Naiad, GraphX, and PGX.D).",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2793508496",
    "type": "article"
  },
  {
    "title": "Cross-Layer Memory Management to Improve DRAM Energy Efficiency",
    "doi": "https://doi.org/10.1145/3196886",
    "publication_date": "2018-05-01",
    "publication_year": 2018,
    "authors": "M. Ben Olson; Joseph Townley Teague; D. Ankamma Rao; Michael R. Jantz; Kshitij Doshi; Prasad A. Kulkarni",
    "corresponding_authors": "",
    "abstract": "Controlling the distribution and usage of memory power is often difficult, because these effects typically depend on activity across multiple layers of the vertical execution stack. To address this challenge, we construct a novel and collaborative framework that employs object placement, cross-layer communication, and page-level management to effectively distribute application objects in the DRAM hardware to achieve desired power/performance goals. This work describes the design and implementation of our framework, which is the first to integrate automatic object profiling and analysis at the application layer with fine-grained management of memory hardware resources in the operating system. We demonstrate the utility of this framework by employing it to control memory power consumption more effectively. First, we design a custom memory-intensive workload to show the potential of this approach to reduce DRAM energy consumption. Next, we develop sampling and profiling-based analyses and modify the code generator in the HotSpot VM to understand object usage patterns and automatically control the placement of hot and cold objects in a partitioned VM heap. This information is communicated to the operating system, which uses it to map the logical application pages to the appropriate DRAM modules according to user-defined provisioning goals. The evaluation shows that our Java VM-based framework achieves our goal of significant DRAM energy savings across a variety of workloads, without any source code modifications or recompilations.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2802330870",
    "type": "article"
  },
  {
    "title": "An Alternative TAGE-like Conditional Branch Predictor",
    "doi": "https://doi.org/10.1145/3226098",
    "publication_date": "2018-08-28",
    "publication_year": 2018,
    "authors": "Pierre Michaud",
    "corresponding_authors": "Pierre Michaud",
    "abstract": "TAGE is one of the most accurate conditional branch predictors known today. However, TAGE does not exploit its input information perfectly, as it is possible to obtain significant prediction accuracy improvements by complementing TAGE with a statistical corrector using the same input information. This article proposes an alternative TAGE-like predictor making statistical correction practically superfluous.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2806279293",
    "type": "article"
  },
  {
    "title": "LAPPS",
    "doi": "https://doi.org/10.1145/3233299",
    "publication_date": "2018-08-28",
    "publication_year": 2018,
    "authors": "Engin Kayraklioglu; Michael P. Ferguson; Tarek El‐Ghazawi",
    "corresponding_authors": "",
    "abstract": "Prefetching is a well-known technique to mitigate scalability challenges in the Partitioned Global Address Space (PGAS) model. It has been studied as either an automated compiler optimization or a manual programmer optimization. Using the PGAS locality awareness, we define a hybrid tradeoff. Specifically, we introduce locality-aware productive prefetching support for PGAS. Our novel, user-driven approach strikes a balance between the ease-of-use of compiler-based automated prefetching and the high performance of the laborious manual prefetching. Our prototype implementation in Chapel shows that significant scalability and performance improvements can be achieved with minimal effort in common applications.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2888871693",
    "type": "article"
  },
  {
    "title": "SketchDLC",
    "doi": "https://doi.org/10.1145/3312570",
    "publication_date": "2019-04-18",
    "publication_year": 2019,
    "authors": "Yemao Xu; Dezun Dong; Weixia Xu; Xiangke Liao",
    "corresponding_authors": "",
    "abstract": "With the fast development of deep learning (DL), the communication is increasingly a bottleneck for distributed workloads, and a series of optimization works have been done to scale out successfully. Nevertheless, the network behavior has not been investigated much yet. We intend to analyze the network behavior and then carry out some research through network simulation. Under this circumstance, an accurate communication measurement is necessary, as it is an effective way to study the network behavior and the basis for accurate simulation. Therefore, we propose to capture the deep learning communication (DLC) trace to achieve the measurement. To the best of our knowledge, we make the first attempt to capture the communication trace for DL training. In this article, we first provide detailed analyses about the communication mechanism of MXNet, which is a representative framework for distributed DL. Secondly, we define the DLC trace format to describe and record the communication behaviors. Third, we present the implementation of method for trace capturing. Finally, we make some statistics and analyses about the distributed DL training, including communication pattern, overlap ratio between computation and communication, computation overhead, synchronization overhead, update overhead, and so forth. Both the statistics and analyses are based on the trace files captured in a cluster with six machines. On the one hand, our trace files provide a sketch on the DLC, which contributes to understanding the communication details. On the other hand, the captured trace files can be used for figuring out various overheads, as they record the communication behaviors of each node.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2938693278",
    "type": "article"
  },
  {
    "title": "Coordinated CTA Combination and Bandwidth Partitioning for GPU Concurrent Kernel Execution",
    "doi": "https://doi.org/10.1145/3326124",
    "publication_date": "2019-06-17",
    "publication_year": 2019,
    "authors": "Zhen Lin; Hongwen Dai; Michael Mantor; Huiyang Zhou",
    "corresponding_authors": "",
    "abstract": "Contemporary GPUs support multiple kernels to run concurrently on the same streaming multiprocessors (SMs). Recent studies have demonstrated that such concurrent kernel execution (CKE) improves both resource utilization and computational throughput. Most of the prior works focus on partitioning the GPU resources at the cooperative thread array (CTA) level or the warp scheduler level to improve CKE. However, significant performance slowdown and unfairness are observed when latency-sensitive kernels co-run with bandwidth-intensive ones. The reason is that bandwidth over-subscription from bandwidth-intensive kernels leads to much aggravated memory access latency, which is highly detrimental to latency-sensitive kernels. Even among bandwidth-intensive kernels, more intensive kernels may unfairly consume much higher bandwidth than less-intensive ones. In this article, we first make a case that such problems cannot be sufficiently solved by managing CTA combinations alone and reveal the fundamental reasons. Then, we propose a coordinated approach for CTA combination and bandwidth partitioning. Our approach dynamically detects co-running kernels as latency sensitive or bandwidth intensive. As both the DRAM bandwidth and L2-to-L1 Network-on-Chip (NoC) bandwidth can be the critical resource, our approach partitions both bandwidth resources coordinately along with selecting proper CTA combinations. The key objective is to allocate more CTA resources for latency-sensitive kernels and more NoC/DRAM bandwidth resources to NoC-/DRAM-intensive kernels. We achieve it using a variation of dominant resource fairness (DRF). Compared with two state-of-the-art CKE optimization schemes, SMK [52] and WS [55], our approach improves the average harmonic speedup by 78% and 39%, respectively. Even compared to the best possible CTA combinations, which are obtained from an exhaustive search among all possible CTA combinations, our approach improves the harmonic speedup by up to 51% and 11% on average.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2952562588",
    "type": "article"
  },
  {
    "title": "The Power-optimised Software Envelope",
    "doi": "https://doi.org/10.1145/3321551",
    "publication_date": "2019-06-17",
    "publication_year": 2019,
    "authors": "Stephen I. Roberts; Steven A. Wright; Suhaib A. Fahmy; Stephen A. Jarvis",
    "corresponding_authors": "",
    "abstract": "Advances in processor design have delivered performance improvements for decades. As physical limits are reached, refinements to the same basic technologies are beginning to yield diminishing returns. Unsustainable increases in energy consumption are forcing hardware manufacturers to prioritise energy efficiency in their designs. Research suggests that software modifications may be needed to exploit the resulting improvements in current and future hardware. New tools are required to capitalise on this new class of optimisation. In this article, we present the Power Optimised Software Envelope (POSE) model, which allows developers to assess the potential benefits of power optimisation for their applications. The POSE model is metric agnostic and in this article, we provide derivations using the established Energy-Delay Product metric and the novel Energy-Delay Sum and Energy-Delay Distance metrics that we believe are more appropriate for energy-aware optimisation efforts. We demonstrate POSE on three platforms by studying the optimisation characteristics of applications from the Mantevo benchmark suite. Our results show that the Pathfinder application has very little scope for power optimisation while TeaLeaf has the most, with all other applications in the benchmark suite falling between the two. Finally, we extend our POSE model with a formulation known as System Summary POSE—a meta-heuristic that allows developers to assess the scope a system has for energy-aware software optimisation independent of the code being run.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2952713505",
    "type": "article"
  },
  {
    "title": "Layup",
    "doi": "https://doi.org/10.1145/3357238",
    "publication_date": "2019-10-11",
    "publication_year": 2019,
    "authors": "Wenbin Jiang; Yang Ma; Bo Liu; Haikun Liu; Bing Zhou; Jian Zhu; Song Wu; Hai Jin",
    "corresponding_authors": "",
    "abstract": "Although GPUs have emerged as the mainstream for the acceleration of convolutional neural network (CNN) training processes, they usually have limited physical memory, meaning that it is hard to train large-scale CNN models. Many methods for memory optimization have been proposed to decrease the memory consumption of CNNs and to mitigate the increasing scale of these networks; however, this optimization comes at the cost of an obvious drop in time performance. We propose a new memory optimization strategy named Layup that realizes both better memory efficiency and better time performance. First, a fast layer-type-specific method for memory optimization is presented, based on the new finding that a single memory optimization often shows dramatic differences in time performance for different types of layers. Second, a new memory reuse method is presented in which greater attention is paid to multi-type intermediate data such as convolutional workspaces and cuDNN handle data. Experiments show that Layup can significantly increase the scale of extra-deep network models on a single GPU with lower performance loss. It even can train ResNet with 2,504 layers using 12GB memory, outperforming the state-of-the-art work of SuperNeurons with 1,920 layers (batch size = 16).",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2979816092",
    "type": "article"
  },
  {
    "title": "<i>Zeroploit</i>",
    "doi": "https://doi.org/10.1145/3394284",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "Ram Rangan; Mark W. Stephenson; Aditya Ukarande; Shyam Murthy; Virat Agarwal; Marc Blackstein",
    "corresponding_authors": "",
    "abstract": "In this article, we first characterize register operand value locality in shader programs of modern gaming applications and observe that there is a high likelihood of one of the register operands of several multiply, logical-and, and similar operations being zero, dynamically. We provide intuition, examples, and a quantitative characterization for how zeros originate dynamically in these programs. Next, we show that this dynamic behavior can be gainfully exploited with a profile-guided code optimization called Zeroploit that transforms targeted code regions into a zero-(value-)specialized fast path and a default slow path. The fast path benefits from zero-specialization in two ways, namely: (a) the backward slice of the other operand of a given multiply or logical-and can be skipped dynamically, provided the only use of that other operand is in the given instruction, and (b) the forward slice of instructions originating at the given instruction can be zero-specialized, potentially triggering further backward slice specializations from operations of that forward slice as well. Such specialization helps the fast path avoid redundant dynamic computations as well as memory fetches, while the fast-slow versioning transform helps preserve functional correctness. With an offline value profiler and manually optimized shader programs, we demonstrate that Zeroploit is able to achieve an average speedup of 35.8% for targeted shader programs, amounting to an average frame-rate speedup of 2.8% across a collection of modern gaming applications on an NVIDIA® GeForce RTX™ 2080 GPU.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W3040989161",
    "type": "article"
  },
  {
    "title": "Cooperative Software-hardware Acceleration of K-means on a Tightly Coupled CPU-FPGA System",
    "doi": "https://doi.org/10.1145/3406114",
    "publication_date": "2020-08-17",
    "publication_year": 2020,
    "authors": "Tarek S. Abdelrahman",
    "corresponding_authors": "Tarek S. Abdelrahman",
    "abstract": "We consider software-hardware acceleration of K-means clustering on the Intel Xeon+FPGA platform. We design a pipelined accelerator for K-means and combine it with CPU threads to assess performance benefits of (1) acceleration when data are only accessed from system memory and (2) cooperative CPU-FPGA acceleration. Our evaluation shows that the accelerator is up to 12.7×/2.4× faster than a single CPU thread for the assignment/update step of K-means. The cooperative use of threads and FPGA is roughly 1.9× faster than CPU threads alone or the FPGA by itself. Our approach delivers 4×–5× higher throughput compared to existing offload processing approaches.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W3080250604",
    "type": "article"
  },
  {
    "title": "GraphPEG",
    "doi": "https://doi.org/10.1145/3450440",
    "publication_date": "2021-05-10",
    "publication_year": 2021,
    "authors": "Yashuai Lü; Hui Guo; Libo Huang; Qi Yu; Li Shen; Nong Xiao; Zhiying Wang",
    "corresponding_authors": "",
    "abstract": "Due to massive thread-level parallelism, GPUs have become an attractive platform for accelerating large-scale data parallel computations, such as graph processing. However, achieving high performance for graph processing with GPUs is non-trivial. Processing graphs on GPUs introduces several problems, such as load imbalance, low utilization of hardware unit, and memory divergence. Although previous work has proposed several software strategies to optimize graph processing on GPUs, there are several issues beyond the capability of software techniques to address. In this article, we present GraphPEG, a graph processing engine for efficient graph processing on GPUs. Inspired by the observation that many graph algorithms have a common pattern on graph traversal, GraphPEG improves the performance of graph processing by coupling automatic edge gathering with fine-grain work distribution. GraphPEG can also adapt to various input graph datasets and simplify the software design of graph processing with hardware-assisted graph traversal. Simulation results show that, in comparison with two representative highly efficient GPU graph processing software framework Gunrock and SEP-Graph, GraphPEG improves graph processing throughput by 2.8× and 2.5× on average, and up to 7.3× and 7.0× for six graph algorithm benchmarks on six graph datasets, with marginal hardware cost.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W3160477826",
    "type": "article"
  },
  {
    "title": "Iterative Compilation Optimization Based on Metric Learning and Collaborative Filtering",
    "doi": "https://doi.org/10.1145/3480250",
    "publication_date": "2021-12-06",
    "publication_year": 2021,
    "authors": "Hongzhi Liu; Jie Luo; Ying Li; Zhonghai Wu",
    "corresponding_authors": "",
    "abstract": "Pass selection and phase ordering are two critical compiler auto-tuning problems. Traditional heuristic methods cannot effectively address these NP-hard problems especially given the increasing number of compiler passes and diverse hardware architectures. Recent research efforts have attempted to address these problems through machine learning. However, the large search space of candidate pass sequences, the large numbers of redundant and irrelevant features, and the lack of training program instances make it difficult to learn models well. Several methods have tried to use expert knowledge to simplify the problems, such as using only the compiler passes or subsequences in the standard levels (e.g., -O1, -O2, and -O3) provided by compiler designers. However, these methods ignore other useful compiler passes that are not contained in the standard levels. Principal component analysis (PCA) and exploratory factor analysis (EFA) have been utilized to reduce the redundancy of feature data. However, these unsupervised methods retain all the information irrelevant to the performance of compilation optimization, which may mislead the subsequent model learning. To solve these problems, we propose a compiler pass selection and phase ordering approach, called Iterative Compilation based on Metric learning and Collaborative filtering (ICMC) . First, we propose a data-driven method to construct pass subsequences according to the observed collaborative interactions and dependency among passes on a given program set. Therefore, we can make use of all available compiler passes and prune the search space. Then, a supervised metric learning method is utilized to retain useful feature information for compilation optimization while removing both the irrelevant and the redundant information. Based on the learned similarity metric, a neighborhood-based collaborative filtering method is employed to iteratively recommend a few superior compiler passes for each target program. Last, an iterative data enhancement method is designed to alleviate the problem of lacking training program instances and to enhance the performance of iterative pass recommendations. The experimental results using the LLVM compiler on all 32 cBench programs show the following: (1) ICMC significantly outperforms several state-of-the-art compiler phase ordering methods, (2) it performs the same or better than the standard level -O3 on all the test programs, and (3) it can reach an average performance speedup of 1.20 (up to 1.46) compared with the standard level -O3.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4200074466",
    "type": "article"
  },
  {
    "title": "<scp>ReuseTracker</scp> : Fast Yet Accurate Multicore Reuse Distance Analyzer",
    "doi": "https://doi.org/10.1145/3484199",
    "publication_date": "2021-12-06",
    "publication_year": 2021,
    "authors": "Muhammad Aditya Sasongko; Milind Chabbi; Mandana Bagheri Marzijarani; Didem Unat",
    "corresponding_authors": "",
    "abstract": "One widely used metric that measures data locality is reuse distance —the number of unique memory locations that are accessed between two consecutive accesses to a particular memory location. State-of-the-art techniques that measure reuse distance in parallel applications rely on simulators or binary instrumentation tools that incur large performance and memory overheads. Moreover, the existing sampling-based tools are limited to measuring reuse distances of a single thread and discard interactions among threads in multi-threaded programs. In this work, we propose ReuseTracker —a fast and accurate reuse distance analyzer that leverages existing hardware features in commodity CPUs. ReuseTracker is designed for multi-threaded programs and takes cache-coherence effects into account. By utilizing hardware features like performance monitoring units and debug registers, ReuseTracker can accurately profile reuse distance in parallel applications with much lower overheads than existing tools. It introduces only 2.9× runtime and 2.8× memory overheads. Our tool achieves 92% accuracy when verified against a newly developed configurable benchmark that can generate a variety of different reuse distance patterns. We demonstrate the tool’s functionality with two use-case scenarios using PARSEC, Rodinia, and Synchrobench benchmark suites where ReuseTracker guides code refactoring in these benchmarks by detecting spatial reuses in shared caches that are also false sharing and successfully predicts whether some benchmarks in these suites can benefit from adjacent cache line prefetch optimization.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4200525217",
    "type": "article"
  },
  {
    "title": "Aging-Aware Compilation for GP-GPUs",
    "doi": "https://doi.org/10.1145/2778984",
    "publication_date": "2015-07-08",
    "publication_year": 2015,
    "authors": "Atieh Lotfi; Abbas Rahimi; Luca Benini; Rajesh K. Gupta",
    "corresponding_authors": "",
    "abstract": "General-purpose graphic processing units (GP-GPUs) offer high computational throughput using thousands of integrated processing elements (PEs). These PEs are stressed during workload execution, and negative bias temperature instability (NBTI) adversely affects their reliability by introducing new delay-induced faults. However, the effect of these delay variations is not uniformly spread across the PEs: some are affected more—hence less reliable—than others. This variation causes significant reduction in the lifetime of GP-GPU parts. In this article, we address the problem of “wear leveling” across processing units to mitigate lifetime uncertainty in GP-GPUs. We propose innovations in the static compiled code that can improve healing in PEs and stream cores (SCs) based on their degradation status. PE healing is a fine-grained very long instruction word (VLIW) slot assignment scheme that balances the stress of instructions across the PEs within an SC. SC healing is a coarse-grained workload allocation scheme that distributes workload across SCs in GP-GPUs. Both schemes share a common property: they adaptively shift workload from less reliable units to more reliable units, either spatially or temporally. These software schemes are based on online calibration with NBTI monitoring that equalizes the expected lifetime of PEs and SCs by regenerating adaptive compiled codes to respond to the specific health state of the GP-GPUs. We evaluate the effectiveness of the proposed schemes for various OpenCL kernels from the AMD APP SDK on Evergreen and Southern Island GPU architectures. The aging-aware healthy kernels generated by the PE (or SC) healing scheme reduce NBTI-induced voltage threshold shift by 30% (77% in the case of SCs), with no (moderate) performance penalty compared to the naive kernels.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W1459415741",
    "type": "article"
  },
  {
    "title": "FLARES",
    "doi": "https://doi.org/10.1145/2631919",
    "publication_date": "2014-07-31",
    "publication_year": 2014,
    "authors": "Stefano Di Carlo; Salvatore Galfano; Marco Indaco; P. Prinetto; Davide Bertozzi; P. Olivo; Cristian Zambelli",
    "corresponding_authors": "",
    "abstract": "With the advent of solid-state storage systems, NAND flash memories are becoming a key storage technology. However, they suffer from serious reliability and endurance issues during the operating lifetime that can be handled by the use of appropriate error correction codes (ECCs) in order to reconstruct the information when needed. Adaptable ECCs may provide the flexibility to avoid worst-case reliability design, thus leading to improved performance. However, a way to control such adaptable ECCs' strength is required. This article proposes FLARES, an algorithm able to adapt the ECC correction capability of each page of a flash based on a flash RBER prediction model and on a measurement of the number of errors detected in a given time window. FLARES has been fully implemented within the YAFFS 2 filesystem under the Linux operating system. This allowed us to perform an extensive set of simulations on a set of standard benchmarks that highlighted the benefit of FLARES on the overall storage subsystem performances.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2041662986",
    "type": "article"
  },
  {
    "title": "Predicate-aware, makespan-preserving software pipelining of scheduling tables",
    "doi": "https://doi.org/10.1145/2579676",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "Thomas Carle; Dumitru Potop‐Butucaru",
    "corresponding_authors": "",
    "abstract": "We propose a software pipelining technique adapted to specific hard real-time scheduling problems. Our technique optimizes both computation throughput and execution cycle makespan, with makespan being prioritary. It also takes advantage of the predicated execution mechanisms of our embedded execution platform. To do so, it uses a reservation table formalism allowing the manipulation of the execution conditions of operations. Our reservation tables allow the double reservation of a resource at the same dates by two different operations, if the operations have exclusive execution conditions. Our analyses can determine when double reservation is possible even for operations belonging to different iterations.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2047686075",
    "type": "article"
  },
  {
    "title": "Optimizing Memory Translation Emulation in Full System Emulators",
    "doi": "https://doi.org/10.1145/2686034",
    "publication_date": "2015-01-09",
    "publication_year": 2015,
    "authors": "Xin Tong; Toshihiko Koju; Motohiro Kawahito; Andreas Moshovos",
    "corresponding_authors": "",
    "abstract": "The emulation speed of a full system emulator (FSE) determines its usefulness. This work quantitatively measures where time is spent in QEMU [Bellard 2005], an industrial-strength FSE. The analysis finds that memory emulation is one of the most heavily exercised emulator components. For workloads studied, 38.1% of the emulation time is spent in memory emulation on average, even though QEMU implements a software translation lookaside buffer (STLB) to accelerate dynamic address translation. Despite the amount of time spent in memory emulation, there has been no study on how to further improve its speed. This work analyzes where time is spent in memory emulation and studies the performance impact of a number of STLB optimizations. Although there are several performance optimization techniques for hardware TLBs, this work finds that the trade-offs with an STLB are quite different compared to those with hardware TLBs. As a result, not all hardware TLB performance optimization techniques are applicable to STLBs and vice versa. The evaluated STLB optimizations target STLB lookups, as well as refills, and result in an average emulator performance improvement of 24.4% over the baseline.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2091873788",
    "type": "article"
  },
  {
    "title": "EECache",
    "doi": "https://doi.org/10.1145/2756552",
    "publication_date": "2015-07-08",
    "publication_year": 2015,
    "authors": "Hsiang-Yun Cheng; Matt Poremba; Narges Shahidi; Ivan Stalev; M.J. Irwin; Mahmut Kandemir; Jack Sampson; Yuan Xie",
    "corresponding_authors": "",
    "abstract": "Power management for large last-level caches (LLCs) is important in chip multiprocessors (CMPs), as the leakage power of LLCs accounts for a significant fraction of the limited on-chip power budget. Since not all workloads running on CMPs need the entire cache, portions of a large, shared LLC can be disabled to save energy. In this article, we explore different design choices, from circuit-level cache organization to microarchitectural management policies, to propose a low-overhead runtime mechanism for energy reduction in the large, shared LLC. We first introduce a slice-based cache organization that can shut down parts of the shared LLC with minimal circuit overhead. Based on this slice-based organization, part of the shared LLC can be turned off according to the spatial and temporal cache access behavior captured by low-overhead sampling-based hardware. In order to eliminate the performance penalties caused by flushing data before powering off a cache slice, we propose data migration policies to prevent the loss of useful data in the LLC. Results show that our energy-efficient cache design (EECache) provides 14.1% energy savings at only 1.2% performance degradation and consumes negligible hardware overhead compared to prior work.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2250920975",
    "type": "article"
  },
  {
    "title": "JavaScript Parallelizing Compiler for Exploiting Parallelism from Data-Parallel HTML5 Applications",
    "doi": "https://doi.org/10.1145/2846098",
    "publication_date": "2016-01-04",
    "publication_year": 2016,
    "authors": "Yeoul Na; Seon Wook Kim; Youngsun Han",
    "corresponding_authors": "",
    "abstract": "With the advent of the HTML5 standard, JavaScript is increasingly processing computationally intensive, data-parallel workloads. Thus, the enhancement of JavaScript performance has been emphasized because the performance gap between JavaScript and native applications is still substantial. Despite this urgency, conventional JavaScript compilers do not exploit much of parallelism even from data-parallel JavaScript applications, despite contemporary mobile devices being equipped with expensive parallel hardware platforms, such as multicore processors and GPGPUs. In this article, we propose an automatically parallelizing JavaScript compiler that targets emerging, data-parallel HTML5 applications by leveraging the mature affine loop analysis of conventional static compilers. We identify that the most critical issues when parallelizing JavaScript with a conventional static analysis are ensuring correct parallelization, minimizing compilation overhead, and conducting low-cost recovery when there is a speculation failure during parallel execution. We propose a mechanism for safely handling the failure at a low cost, based on compiler techniques and the property of idempotence. Our experiment shows that the proposed JavaScript parallelizing compiler detects most affine parallel loops. Also, we achieved a maximum speedup of 3.22 times on a quad-core system, while incurring negligible compilation and recovery overheads with various sets of data-parallel HTML5 applications.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2295664835",
    "type": "article"
  },
  {
    "title": "Thread-Aware Adaptive Prefetcher on Multicore Systems",
    "doi": "https://doi.org/10.1145/2890505",
    "publication_date": "2016-03-28",
    "publication_year": 2016,
    "authors": "Peng Liu; Jiyang Yu; Michael Huang",
    "corresponding_authors": "",
    "abstract": "Most processors employ hardware data prefetching techniques to hide memory access latencies. However, the prefetching requests from different threads on a multicore processor can cause severe interference with prefetching and/or demand requests of others. The data prefetching can lead to significant performance degradation due to shared resource contention on shared memory multicore systems. This article proposes a thread-aware data prefetching mechanism based on low-overhead runtime information to tune prefetching modes and aggressiveness, mitigating the resource contention in the memory system. Our solution has three new components: (1) a self-tuning prefetcher that uses runtime feedback to dynamically adjust data prefetching modes and arguments of each thread, (2) a filtering mechanism that informs the hardware about which prefetching request can cause shared data invalidation and should be discarded, and (3) a limiter thread acceleration mechanism to estimate and accelerate the critical thread which has the longest completion time in the parallel region of execution. On a set of multithreaded parallel benchmarks, our thread-aware data prefetching mechanism improves the overall performance of 64-core system by 13% over a multimode prefetch baseline system with two-level cache organization and conventional modified, exclusive, shared, and invalid-based directory coherence protocol. We compare our approach with the feedback directed prefetching technique and find that it provides 9% performance improvement on multicore systems, while saving the memory bandwidth consumption.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2333944373",
    "type": "article"
  },
  {
    "title": "Concurrent JavaScript Parsing for Faster Loading of Web Apps",
    "doi": "https://doi.org/10.1145/3004281",
    "publication_date": "2016-11-19",
    "publication_year": 2016,
    "authors": "Hyukwoo Park; Myungsu Cha; Soo‐Mook Moon",
    "corresponding_authors": "",
    "abstract": "JavaScript is a dynamic language mainly used as a client-side web script. Nowadays, web is evolving into an application platform with its web apps , and JavaScript increasingly undertakes complex computations and interactive user interfaces, requiring a high-performance JavaScript engine. There have been many optimizations for efficient JavaScript engines, but one component that has not been optimized much is JavaScript parsing . A JavaScript function needs to be parsed before being executed, and the parsing overhead takes a substantial portion of JavaScript execution time for web apps, especially during app loading . This article proposes concurrent parsing of JavaScript, which performs the parsing of JavaScript functions in advance on different threads, while the main thread is executing the parsed JavaScript functions. This can hide the parsing overhead from the main execution thread, reducing the JavaScript execution time, thus reducing the overall app loading time. More specifically, we separated JavaScript parsing and made it run on different threads without violating the execution semantics of JavaScript. We also designed an efficient multi-threaded parsing architecture, which reduces the synchronization overhead and schedules the parsing requests appropriately. Finally, we explored two methods of choosing the target functions for concurrent parsing: one based on profiled information and the other based on speculative heuristics. We performed experiments on the WebKit browser with the JSC engine for real web apps. The result shows that the proposed concurrent parsing can improve the JavaScript performance during app loading by as much as 64% and by 39.7% on average. This improves the whole app loading performance tangibly, by as much as 32.7% and by 18.2%, on average.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2552265536",
    "type": "article"
  },
  {
    "title": "MaxPB",
    "doi": "https://doi.org/10.1145/3012007",
    "publication_date": "2016-12-12",
    "publication_year": 2016,
    "authors": "Zheng Li; Fang Wang; Dan Feng; Yu Hua; Jingning Liu; Wei Tong",
    "corresponding_authors": "",
    "abstract": "Phase Change Memory (PCM) is one of the promising memory technologies but suffers from some critical problems such as poor write performance and high write energy consumption. Due to the high write energy consumption and limited power supply, the size of concurrent bit-write is restricted inside one PCM chip. Typically, the size of concurrent bit-write is much less than the cache line size and it is normal that many serially executed write units are consumed to write down the data block to PCM when using it as the main memory. Existing state-of-the-art PCM write schemes, such as FNW (Flip-N-Write) and two-stage-write, address the problem of poor performance by improving the write parallelism under the power constraints. The parallelism is obtained via reducing the data amount and leveraging power as well as time asymmetries, respectively. However, due to the extremely pessimistic assumptions of current utilization (FNW) and optimistic assumptions of asymmetries (two-stage-write), these schemes fail to maximize the power supply utilization and hence improve the write parallelism. In this article, we propose a novel PCM write scheme, called MaxPB (Maximize the Power Budget utilization) to maximize the power budget utilization with minimum changes about the circuits design. MaxPB is a “think before acting” method. The main idea of MaxPB is to monitor the actual power needs of all data units first and then effectively package them into the least number of write units under the power constraints. Experimental results show the efficiency and performance improvements on MaxPB. For example, four-core PARSEC and SPEC experimental results show that MaxPB gets 32.0% and 20.3% more read latency reduction, 26.5% and 16.1% more write latency reduction, 24.3% and 15.6% more running time decrease, 1.32× and 0.92× more speedup, as well as 30.6% and 18.4% more energy consumption reduction on average compared with the state-of-the-art FNW and two-stage-write write schemes, respectively.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2565626557",
    "type": "article"
  },
  {
    "title": "Preserving Addressability Upon GC-Triggered Data Movements on Non-Volatile Memory",
    "doi": "https://doi.org/10.1145/3511706",
    "publication_date": "2022-01-31",
    "publication_year": 2022,
    "authors": "Chencheng Ye; Yuanchao Xu; Xipeng Shen; Hai Jin; Xiaofei Liao; Yan Solihin",
    "corresponding_authors": "",
    "abstract": "This article points out an important threat that application-level Garbage Collection (GC) creates to the use of non-volatile memory (NVM). Data movements incurred by GC may invalidate the pointers to objects on NVM and, hence, harm the reusability of persistent data across executions. The article proposes the concept of movement-oblivious addressing (MOA), and develops and compares three novel solutions to materialize the concept for solving the addressability problem. It evaluates the designs on five benchmarks and a real-world application. The results demonstrate the promise of the proposed solutions, especially hardware-supported Multi-Level GPointer, in addressing the problem in a space- and time-efficient manner.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4210561563",
    "type": "article"
  },
  {
    "title": "GiantVM: A Novel Distributed Hypervisor for Resource Aggregation with DSM-aware Optimizations",
    "doi": "https://doi.org/10.1145/3505251",
    "publication_date": "2022-03-07",
    "publication_year": 2022,
    "authors": "Xingguo Jia; Jin Zhang; Boshi Yu; Xingyue Qian; Zhengwei Qi; Haibing Guan",
    "corresponding_authors": "",
    "abstract": "We present GiantVM, 1 an open-source distributed hypervisor that provides the many-to-one virtualization to aggregate resources from multiple physical machines. We propose techniques to enable distributed CPU and I/O virtualization and distributed shared memory (DSM) to achieve memory aggregation. GiantVM is implemented based on the state-of-the-art type-II hypervisor QEMU-KVM, and it can currently host conventional OSes such as Linux. (1) We identify the performance bottleneck of GiantVM to be DSM, through a top-down performance analysis. Although GiantVM offers great opportunities for CPU-intensive applications to enjoy the aggregated CPU resources, memory-intensive applications could suffer from cross-node page sharing, which requires frequent DSM involvement and leads to performance collapse. We design the guest-level thread scheduler, DaS (DSM-aware Scheduler), to overcome the bottleneck. When benchmarking with NAS Parallel Benchmarks, the DaS could achieve a performance boost of up to 3.5×, compared to the default Linux kernel scheduler. (2) While evaluating DaS, we observe the advantage of GiantVM as a resource reallocation facility. Thanks to the SSI abstraction of GiantVM, migration could be done by guest-level scheduling. DSM allows standby pages in the migration destination, which need not be transferred through the network. The saved network bandwidth is 68% on average, compared to VM live migration. Resource reallocation with GiantVM increases the overall CPU utilization by 14.3% in a co-location experiment.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4220993462",
    "type": "article"
  },
  {
    "title": "Phronesis: Efficient Performance Modeling for High-dimensional Configuration Tuning",
    "doi": "https://doi.org/10.1145/3546868",
    "publication_date": "2022-07-11",
    "publication_year": 2022,
    "authors": "Yuhao Li; Benjamin C. Lee",
    "corresponding_authors": "",
    "abstract": "We present Phronesis, a learning framework for efficiently modeling the performance of data analytic workloads as a function of their high-dimensional software configuration parameters. Accurate performance models are useful for efficiently optimizing data analytic performance. Phronesis explicitly considers the error decomposition in statistical learning and implications for efficient data acquisition and model growth strategies in performance modeling. We demonstrate Phronesis with three popular machine learning models commonly used in performance tuning: neural network, random forest, and regression spline. We implement and evaluate it for Spark configuration parameters. We show that Phronesis significantly reduces data collection time for training predictive models by up to 57% and 37%, on average, compared to state-of-the-art techniques in building Spark performance models. Furthermore, we construct a configuration autotuning pipeline based on Phronesis. Our results indicate up to 30% gains in performance for Spark workloads over previous, state-of-the-art tuning strategies that use high-dimensional models.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4285004820",
    "type": "article"
  },
  {
    "title": "SpecTerminator: Blocking Speculative Side Channels Based on Instruction Classes on RISC-V",
    "doi": "https://doi.org/10.1145/3566053",
    "publication_date": "2022-11-12",
    "publication_year": 2022,
    "authors": "Hai Jin; Zhuo He; Weizhong Qiang",
    "corresponding_authors": "",
    "abstract": "In modern processors, speculative execution has significantly improved the performance of processors, but it has also introduced speculative execution vulnerabilities. Recent defenses are based on the delayed execution to block various speculative side channels, but we show that several of the current state-of-the-art defenses fail to block some of the available speculative side channels, and the current most secure defense introduces a performance overhead of up to 24.5%. We propose SpecTerminator, the first defense framework based on instruction classes that can comprehensively and precisely block all existing speculative side channels. In SpecTerminator, a novel speculative side channel classification scheme based on the features of secret transmission is proposed, and the sensitive instructions in the speculative window are classified and identified using optimized hardware taint tracking and instruction masking techniques to accurately determine the scope of leakage. Then, according to the execution characteristics of these instructions, dedicated delayed execution strategies, such as TLB request ignoring, selective issue, and extended delay-on-miss, are designed for each type of sensitive instruction to precisely control that these instructions are delayed only in pipeline stages that are at risk of leakage. In contrast to previous defenses based on the Gem5 simulator, we have innovatively implemented defenses against Spectre attacks based on the open-source instruction set RISC-V on an FPGA-accelerated simulation platform that is more similar to real hardware. To evaluate the security of SpecTerminator, we have replicated various existing x86-based Spectre variants on RISC-V. On SPEC 2006, SpecTerminator defends against Spectre attacks based on memory hierarchy side channels with a performance overhead of 2.6% and against all existing Spectre attacks with a performance overhead of 6.0%.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4309073410",
    "type": "article"
  },
  {
    "title": "Source Matching and Rewriting for MLIR Using String-Based Automata",
    "doi": "https://doi.org/10.1145/3571283",
    "publication_date": "2022-11-18",
    "publication_year": 2022,
    "authors": "Vinícius Couto Espindola; Luciano Zago; Hervé Yviquel; Guido Araújo",
    "corresponding_authors": "",
    "abstract": "A typical compiler flow relies on a uni-directional sequence of translation/optimization steps that lower the program abstract representation, making it hard to preserve higher-level program information across each transformation step. On the other hand, modern ISA extensions and hardware accelerators can benefit from the compiler’s ability to detect and raise program idioms to acceleration instructions or optimized library calls. Although recent works based on Multi-Level IR (MLIR) have been proposed for code raising, they rely on specialized languages, compiler recompilation, or in-depth dialect knowledge. This article presents Source Matching and Rewriting (SMR), a user-oriented source-code-based approach for MLIR idiom matching and rewriting that does not require a compiler expert’s intervention. SMR uses a two-phase automaton-based DAG-matching algorithm inspired by early work on tree-pattern matching. First, the idiom Control-Dependency Graph (CDG) is matched against the program’s CDG to rule out code fragments that do not have a control-flow structure similar to the desired idiom. Second, candidate code fragments from the previous phase have their Data-Dependency Graphs (DDGs) constructed and matched against the idiom DDG. Experimental results show that SMR can effectively match idioms from Fortran (FIR) and C (CIL) programs while raising them as BLAS calls to improve performance. Additional experiments also show performance improvements when using SMR to enable code replacement in areas like approximate computing and hardware acceleration.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4309468401",
    "type": "article"
  },
  {
    "title": "YaConv: Convolution with Low Cache Footprint",
    "doi": "https://doi.org/10.1145/3570305",
    "publication_date": "2022-12-02",
    "publication_year": 2022,
    "authors": "Ivan Korostelev; João P. L. de Carvalho; José E. Moreira; José Nelson Amaral",
    "corresponding_authors": "",
    "abstract": "This article introduces YaConv , a new algorithm to compute convolution using GEMM microkernels from a Basic Linear Algebra Subprograms library that is efficient for multiple CPU architectures. Previous approaches either create a copy of each image element for each filter element or reload these elements into cache for each GEMM call, leading to redundant instances of the image elements in cache. Instead, YaConv loads each image element once into the cache and maximizes the reuse of these elements. The output image is computed by scattering results of the GEMM microkernel calls to the correct locations in the output image. The main advantage of this new algorithm—which leads to better performance in comparison to the existing im2col approach on several architectures—is a more efficient use of the memory hierarchy. The experimental evaluation on convolutional layers from PyTorch, along with a parameterized study, indicates an average 24% speedup over im2col convolution. Increased performance comes as a result of 3× reduction in L3 cache accesses and 2× fewer branch instructions.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4311119077",
    "type": "article"
  },
  {
    "title": "Improving Computation and Memory Efficiency for Real-world Transformer Inference on GPUs",
    "doi": "https://doi.org/10.1145/3617689",
    "publication_date": "2023-08-26",
    "publication_year": 2023,
    "authors": "Jiangsu Du; Jiazhi Jiang; Zheng Jiang; Hongbin Zhang; Dan Huang; Yutong Lu",
    "corresponding_authors": "",
    "abstract": "Transformer models have emerged as a leading approach in the field of natural language processing (NLP) and are increasingly being deployed in production environments. Graphic processing units (GPUs) have become a popular choice for the transformer deployment and often rely on the batch processing technique to ensure high hardware performance. Nonetheless, the current practice for transformer inference encounters computational and memory redundancy due to the heavy-tailed distribution of sequence lengths in NLP scenarios, resulting in low practical performance. In this article, we propose a unified solution for improving both computation and memory efficiency of the real-world transformer inference on GPUs. The solution eliminates the redundant computation and memory footprint across a transformer model. At first, a GPU-oriented computation approach is proposed to process the self-attention module in a fine-grained manner, eliminating its redundant computation. Next, the multi-layer perceptron module continues to use the word-accumulation approach to eliminate its redundant computation. Then, to better unify the fine-grained approach and the word-accumulation approach, it organizes the data layout of the self-attention module in block granularity. Since aforementioned approaches make the required memory size largely reduce and constantly fluctuate, we propose the chunk-based approach to enable a better balance between memory footprint and allocation/free efficiency. Our experimental results show that our unified solution achieves a decrease of average latency by 28% on the entire transformer model, 63.8% on the self-attention module, and reduces memory footprint of intermediate results by 7.8×, compared with prevailing frameworks.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4386191499",
    "type": "article"
  },
  {
    "title": "RACE: An Efficient Redundancy-aware Accelerator for Dynamic Graph Neural Network",
    "doi": "https://doi.org/10.1145/3617685",
    "publication_date": "2023-08-30",
    "publication_year": 2023,
    "authors": "Hui Yu; Yu Zhang; Jin Zhao; Yujian Liao; Zhiying Huang; Donghao He; Lin Gu; Hai Jin; Xiaofei Liao; Haikun Liu; Bingsheng He; Jianhui Yue",
    "corresponding_authors": "",
    "abstract": "Dynamic Graph Neural Network (DGNN) has recently attracted a significant amount of research attention from various domains, because most real-world graphs are inherently dynamic. Despite many research efforts, for DGNN, existing hardware/software solutions still suffer significantly from redundant computation and memory access overhead, because they need to irregularly access and recompute all graph data of each graph snapshot. To address these issues, we propose an efficient redundancy-aware accelerator, RACE , which enables energy-efficient execution of DGNN models. Specifically, we propose a redundancy-aware incremental execution approach into the accelerator design for DGNN to instantly achieve the output features of the latest graph snapshot by correctly and incrementally refining the output features of the previous graph snapshot and also enable regular accesses of vertices’ input features. Through traversing the graph on the fly, RACE identifies the vertices that are not affected by graph updates between successive snapshots to reuse these vertices’ states (i.e., their output features) of the previous snapshot for the processing of the latest snapshot. The vertices affected by graph updates are also tracked to incrementally recompute their new states using their neighbors’ input features of the latest snapshot for correctness. In this way, the processing and accessing of many graph data that are not affected by graph updates can be correctly eliminated, enabling smaller redundant computation and memory access overhead. Besides, the input features, which are accessed more frequently, are dynamically identified according to graph topology and are preferentially resident in the on-chip memory for less off-chip communications. Experimental results show that RACE achieves on average 1139× and 84.7× speedups for DGNN inference, with average 2242× and 234.2× energy savings, in comparison with the state-of-the-art software DGNN running on Intel Xeon CPU and NVIDIA A100 GPU, respectively. Moreover, for DGNN inference, RACE obtains on average 13.1×, 11.7×, 10.4×, and 7.9× speedup and 14.8×, 12.9×, 11.5×, and 8.9× energy savings over the state-of-the-art Graph Neural Network accelerators, i.e., AWB-GCN, GCNAX, ReGNN, and I-GCN, respectively.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4386290320",
    "type": "article"
  },
  {
    "title": "gPPM: A Generalized Matrix Operation and Parallel Algorithm to Accelerate the Encoding/Decoding Process of Erasure Codes",
    "doi": "https://doi.org/10.1145/3625005",
    "publication_date": "2023-09-21",
    "publication_year": 2023,
    "authors": "Shiyi Li; Qiang Cao; Shenggang Wan; Wen Xia; Changsheng Xie",
    "corresponding_authors": "",
    "abstract": "Erasure codes are widely deployed in modern storage systems, leading to frequent usage of their encoding/decoding operations. The encoding/decoding process for erasure codes is generally carried out using the parity-check matrix approach. However, this approach is serial and computationally expensive, mainly due to dealing with matrix operations, which results in low encoding/decoding performance. These drawbacks are particularly evident for newer erasure codes, including SD and LRC codes. To address these limitations, this article introduces the Partitioned and Parallel Matrix ( PPM ) algorithm. This algorithm partitions the parity-check matrix, parallelizes encoding/decoding operations, and optimizes calculation sequence to facilitate fast encoding/decoding of these codes. Furthermore, we present a generalized PPM ( gPPM ) algorithm that surpasses PPM in performance by employing fine-grained dynamic matrix calculation sequence selection. Unlike PPM, gPPM is also applicable to erasure codes such as RS code. Experimental results demonstrate that PPM improves the encoding/decoding speed of SD and LRC codes by up to 210.81%. Besides, gPPM achieves up to 102.41% improvement over PPM and 32.25% improvement over RS regarding encoding/decoding speed.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4386941882",
    "type": "article"
  },
  {
    "title": "At the Locus of Performance: Quantifying the Effects of Copious 3D-Stacked Cache on HPC Workloads",
    "doi": "https://doi.org/10.1145/3629520",
    "publication_date": "2023-10-25",
    "publication_year": 2023,
    "authors": "Jens Domke; Emil Vatai; Balazs Gerofi; Yuetsu Kodama; Mohamed Wahib; Artur Podobas; Sparsh Mittal; Miquel Pericàs; Lingqi Zhang; Peng Chen; Aleksandr Drozd; Satoshi Matsuoka",
    "corresponding_authors": "",
    "abstract": "Over the last three decades, innovations in the memory subsystem were primarily targeted at overcoming the data movement bottleneck. In this paper, we focus on a specific market trend in memory technology: 3D-stacked memory and caches. We investigate the impact of extending the on-chip memory capabilities in future HPC-focused processors, particularly by 3D-stacked SRAM. First, we propose a method oblivious to the memory subsystem to gauge the upper-bound in performance improvements when data movement costs are eliminated. Then, using the gem5 simulator, we model two variants of a hypothetical LARge Cache processor (LARC), fabricated in 1.5 nm and enriched with high-capacity 3D-stacked cache. With a volume of experiments involving a broad set of proxy-applications and benchmarks, we aim to reveal how HPC CPU performance will evolve, and conclude an average boost of 9.56× for cache-sensitive HPC applications, on a per-chip basis. Additionally, we exhaustively document our methodological exploration to motivate HPC centers to drive their own technological agenda through enhanced co-design.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4388272487",
    "type": "article"
  },
  {
    "title": "Implementing branch-predictor decay using quasi-static memory cells",
    "doi": "https://doi.org/10.1145/1011528.1011531",
    "publication_date": "2004-06-01",
    "publication_year": 2004,
    "authors": "Philo Juang; Kevin Skadron; Margaret Martonosi; Zhigang Hu; Douglas W. Clark; Philip W. Diodato; Stefanos Kaxiras",
    "corresponding_authors": "",
    "abstract": "With semiconductor technology advancing toward deep submicron, leakage energy is of increasing concern, especially for large on-chip array structures such as caches and branch predictors. Recent work has suggested that larger, aggressive branch predictors can and should be used in order to improve microprocessor performance. A further consideration is that more aggressive branch predictors, especially multiported predictors for multiple branch prediction, may be thermal hot spots, thus further increasing leakage. Moreover, as the branch predictor holds state that is transient and predictive, elements can be discarded without adverse effect. For these reasons, it is natural to consider applying decay techniques---already shown to reduce leakage energy for caches---to branch-prediction structures.Due to the structural difference between caches and branch predictors, applying decay techniques to branch predictors is not straightforward. This paper explores the strategies for exploiting spatial and temporal locality to make decay effective for bimodal, gshare, and hybrid predictors, as well as the branch target buffer (BTB). Furthermore, the predictive behavior of branch predictors steers them towards decay based not on state-preserving, static storage cells, but rather quasi-static, dynamic storage cells. This paper will examine the results of implementing decaying branch-predictor structures with dynamic---appropriately, decaying ---cells rather than the standard static SRAM cell.Overall, this paper demonstrates that decay techniques can apply to more than just caches, with the branch predictor and BTB as an example. We show decay can either be implemented at the architectural level, or with a wholesale replacement of static storage cells with quasi-static storage cells, which naturally implement decay. More importantly, decay techniques can be applied and should be applied to other such transient and/or predictive structures.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2024945307",
    "type": "article"
  },
  {
    "title": "Reducing cache misses through programmable decoders",
    "doi": "https://doi.org/10.1145/1328195.1328200",
    "publication_date": "2008-01-01",
    "publication_year": 2008,
    "authors": "Chuanjun Zhang",
    "corresponding_authors": "Chuanjun Zhang",
    "abstract": "Level-one caches normally reside on a processor's critical path, which determines clock frequency. Therefore, fast access to level-one cache is important. Direct-mapped caches exhibit faster access time, but poor hit rates, compared with same sized set-associative caches because of nonuniform accesses to the cache sets. The nonuniform accesses generate more cache misses in some sets, while other sets are underutilized. We propose to increase the decoder length and, hence, reduce the accesses to heavily used sets without dynamically detecting the cache set usage information. We increase the access to the underutilized cache sets by incorporating a replacement policy into the cache design using programmable decoders. On average, the proposed techniques achieve as low a miss rate as a traditional 4-way cache on all 26 SPEC2K benchmarks for the instruction and data caches, respectively. This translates into an average IPC improvement of 21.5 and 42.4% for SPEC2K integer and floating-point benchmarks, respectively. The B-Cache consumes 10.5% more power per access, but exhibits a 12% total memory access-related energy savings as a result of the miss rate reductions, and, hence, the reduction to applications' execution time. Compared with previous techniques that aim at reducing the miss rate of direct-mapped caches, our technique requires only one cycle to access all cache hits and has the same access time of a direct-mapped cache.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2071328833",
    "type": "article"
  },
  {
    "title": "Architecting a reliable CMP switch architecture",
    "doi": "https://doi.org/10.1145/1216544.1216545",
    "publication_date": "2007-03-01",
    "publication_year": 2007,
    "authors": "Kypros Constantinides; Stephen M. Plaza; Jason Blome; Valeria Bertacco; Scott Mahlke; Todd Austin; Bin Zhang; Michael Orshansky",
    "corresponding_authors": "",
    "abstract": "As silicon technologies move into the nanometer regime, transistor reliability is expected to wane as devices become subject to extreme process variation, particle-induced transient errors, and transistor wear-out. Unless these challenges are addressed, computer vendors can expect low yields and short mean-times-to-failure. In this article, we examine the challenges of designing complex computing systems in the presence of transient and permanent faults. We select one small aspect of a typical chip multiprocessor (CMP) system to study in detail, a single CMP router switch. Our goal is to design a BulletProof CMP switch architecture capable of tolerating significant levels of various types of defects. We first assess the vulnerability of the CMP switch to transient faults. To better understand the impact of these faults, we evaluate our CMP switch designs using circuit-level timing on detailed physical layouts. Our infrastructure represents a new level of fidelity in architectural-level fault analysis, as we can accurately track faults as they occur, noting whether they manifest or not, because of masking in the circuits, logic, or architecture. Our experimental results are quite illuminating. We find that transient faults, because of their fleeting nature, are of little concern for our CMP switch, even within large switch fabrics with fast clocks. Next, we develop a unified model of permanent faults, based on the time-tested bathtub curve. Using this convenient abstraction, we analyze the reliability versus area tradeoff across a wide spectrum of CMP switch designs, ranging from unprotected designs to fully protected designs with on-line repair and recovery capabilities. Protection is considered at multiple levels from the entire system down through arbitrary partitions of the design. We find that designs are attainable that can tolerate a larger number of defects with less overhead than naïve triple-modular redundancy, using domain-specific techniques, such as end-to-end error detection, resource sparing, automatic circuit decomposition, and iterative diagnosis and reconfiguration.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2127703742",
    "type": "article"
  },
  {
    "title": "Quality of service shared cache management in chip multiprocessor architecture",
    "doi": "https://doi.org/10.1145/1880037.1880039",
    "publication_date": "2010-12-01",
    "publication_year": 2010,
    "authors": "Fei Guo; Yan Solihin; Li Zhao; Ravishankar Iyer",
    "corresponding_authors": "",
    "abstract": "The trends in enterprise IT toward service-oriented computing, server consolidation, and virtual computing point to a future in which workloads are becoming increasingly diverse in terms of performance, reliability, and availability requirements. It can be expected that more and more applications with diverse requirements will run on a Chip Multi-Processor (CMP) and share platform resources such as the lowest level cache and off-chip bandwidth. In this environment, it is desirable to have microarchitecture and software support that can provide a guarantee of a certain level of performance, which we refer to as performance Quality of Service . In this article, we investigated a framework would be needed to manage the shared cache resource for fully providing QoS in a CMP. We found in order to fully provide QoS, we need to specify an appropriate QoS target for each job and apply an admission control policy to accept jobs only when their QoS targets can be satisfied. We also found that providing strict QoS often leads to a significant reduction in throughput due to resource fragmentation. We proposed throughput optimization techniques that include: (1) exploiting various QoS execution modes, and (2) a microarchitecture technique, which we refer to as resource stealing, that detects and reallocates excess cache capacity from a job while preserving its QoS target. We designed and evaluated three algorithms for performing resource stealing, which differ in how aggressive they are in stealing excess cache capacity, and in the degree of confidence in meeting QoS targets. In addition, we proposed a mechanism to dynamically enable or disable resource stealing depending on whether other jobs can benefit from additional cache capacity. We evaluated our QoS framework with a full system simulation of a 4-core CMP and a recent version of the Linux Operating System. We found that compared to an unoptimized scheme, the throughput can be improved by up to 47%, making the throughput significantly closer to a non-QoS CMP.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2059992278",
    "type": "article"
  },
  {
    "title": "SYRANT: SYmmetric resource allocation on not-taken and taken paths",
    "doi": "https://doi.org/10.1145/2086696.2086722",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Nathanaël Prémillieu; André Seznec",
    "corresponding_authors": "",
    "abstract": "In the multicore era, achieving ultimate single process performance is still an issue e.g. for single process workload or for sequential sections in parallel applications. Unfortunately, despite tremendous research effort on branch prediction, substantial performance potential is still wasted due to branch mispredictions. On a branch misprediction resolution, instruction treatment on the wrong path is essentially thrown away. However, in most cases after a conditional branch, the taken and the not-taken paths of execution merge after a few instructions. Instructions that follow the reconvergence point are executed whatever the branch outcome is. We present SYRANT (SYmmetric Resource Allocation on Not-taken and Taken paths), a new technique for exploiting control independence. SYRANT essentially uses the same pipeline structure as a conventional processor. SYRANT tries to enforce the allocation of the exact same resources on the out-of-order execution mechanism (physical register, load/store queue and reorder buffer) for both the taken and not-taken paths. Thus, on a branch misprediction, the result of an instruction already executed on the wrong path after the reconvergence point can be conserved in the same structure when it is data independent. Adding SYRANT on top of an aggressive superscalar execution core allows to improve performance for applications suffering a significant branch misprediction rate. As a side, but important extra contribution, we introduce ABL/SBL a simple and non-intrusive hardware reconvergence detection mechanism. ABL/SBL can be used in a conventional superscalar processor to improve branch prediction accuracy by exploiting the execution of branches along the wrong path.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2060760674",
    "type": "article"
  },
  {
    "title": "Hardware transactional memory with software-defined conflicts",
    "doi": "https://doi.org/10.1145/2086696.2086710",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Rubén Titos-Gil; Manuel E. Acacio; José M. Garcı́a; Tim Harris; Adrián Cristal; Osman Ünsal; Ibrahim Hur; Mateo Valero",
    "corresponding_authors": "",
    "abstract": "In this paper we investigate the benefits of turning the concept of transactional conflict from its traditionally fixed definition into a variable one that can be dynamically controlled in software. We propose the extension of the atomic language construct with an attribute that specifies the definition of conflict, so that programmers can write code which adjusts what kinds of conflicts are to be detected, relaxing or tightening the conditions according to the forms of interference that can be tolerated by a particular algorithm. Using this performance-motivated construct, specific conflict information can be associated with portions of code, as each transaction is provided with a local definition that applies while it executes. We find that defining conflicts in software makes possible the removal of dependencies which arise as a result of the coarse synchronization style encouraged by the TM programming model. We illustrate the use of the proposed construct in a variety of use cases with real applications, showing how programmers can take advantage of their knowledge about the problem and other global information not available at run-time. We describe how to implement a hardware TM design that utilizes this software construct. Our experiments reveal that leveraging software-defined conflicts, the programmer is able to achieve significant reductions in the number of aborts--over 50% for most applications. At 16 threads, our system with software-defined conflicts outperforms LogTM-SE in nearly all benchmarks, reaching an average reduction in execution time of 18%.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2071558308",
    "type": "article"
  },
  {
    "title": "Designing a practical data filter cache to improve both energy efficiency and performance",
    "doi": "https://doi.org/10.1145/2541228.2555310",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Alen Bardizbanyan; Magnus Själander; David Whalley; Per Larsson-Edefors",
    "corresponding_authors": "",
    "abstract": "Conventional Data Filter Cache (DFC) designs improve processor energy efficiency, but degrade performance. Furthermore, the single-cycle line transfer suggested in prior studies adversely affects Level-1 Data Cache (L1 DC) area and energy efficiency. We propose a practical DFC that is accessed early in the pipeline and transfers a line over multiple cycles. Our DFC design improves performance and eliminates a substantial fraction of L1 DC accesses for loads, L1 DC tag checks on stores, and data translation lookaside buffer accesses for both loads and stores. Our evaluation shows that the proposed DFC can reduce the data access energy by 42.5% and improve execution time by 4.2%.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2072880927",
    "type": "article"
  },
  {
    "title": "Reducing DRAM row activations with eager read/write clustering",
    "doi": "https://doi.org/10.1145/2541228.2555300",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Myeongjae Jeon; Conglong Li; Alan L. Cox; Scott Rixner",
    "corresponding_authors": "",
    "abstract": "This article describes and evaluates a new approach to optimizing DRAM performance and energy consumption that is based on eagerly writing dirty cache lines to DRAM. Under this approach, many dirty cache lines are written to DRAM before they are evicted. In particular, dirty cache lines that have not been recently accessed are eagerly written to DRAM when the corresponding row has been activated by an ordinary, noneager access, such as a read. This approach enables clustering of reads and writes that target the same row, resulting in a significant reduction in row activations. Specifically, for a variety of applications, it reduces the number of DRAM row activations by an average of 42% and a maximum of 82%. Moreover, the results from a full-system simulator show compelling performance improvements and energy consumption reductions. Out of 23 applications, 6 have overall performance improvements between 10% and 20%, and 3 have improvements in excess of 20%. Furthermore, 12 consume between 10% and 20% less DRAM energy, and 7 have energy consumption reductions in excess of 20%.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2077178780",
    "type": "article"
  },
  {
    "title": "Easy, fast, and energy-efficient object detection on heterogeneous on-chip architectures",
    "doi": "https://doi.org/10.1145/2541228.2555302",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Ehsan Totoni; Mert Dikmen; María Jesús Garzarán",
    "corresponding_authors": "",
    "abstract": "We optimize a visual object detection application (that uses Vision Video Library kernels) and show that OpenCL is a unified programming paradigm that can provide high performance when running on the Ivy Bridge heterogeneous on-chip architecture. We evaluate different mapping techniques and show that running each kernel where it fits the best and using software pipelining can provide 1.91 times higher performance and 42% better energy efficiency. We also show how to trade accuracy for energy at runtime. Overall, our application can perform accurate object detection at 40 frames per second (fps) in an energy-efficient manner.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2087767222",
    "type": "article"
  },
  {
    "title": "LIGERO",
    "doi": "https://doi.org/10.1145/2400682.2400696",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Pablo Garrido Abad; Valentín Puente; J.A. Gregorio",
    "corresponding_authors": "",
    "abstract": "Although abstraction is the best approach to deal with computing system complexity, sometimes implementation details should be considered. Considering on-chip interconnection networks in particular, underestimating the underlying system specificity could have nonnegligible impact on performance, cost, or correctness. This article presents a very efficient router that has been devised to deal with cache-coherent chip multiprocessor particularities in a balanced way. Employing the same principles of packet rotation structures as in the rotary router, we present a router configuration with the following novel features: (1) reduced buffering requirements, (2) optimized pipeline under contentionless conditions, (3) more efficient deadlock avoidance mechanism, and (4) optimized in-order delivery guarantee. Putting it all together, our proposal provides a set of features that no other router, to the best of our knowledge, has achieved previously. These are: (1') low implementation cost, (2') low pass-through latency under low load, (3') improved resource utilization through adaptive routing and a buffering scheme free of head-of-line blocking, (4') guarantee of coherence protocol correctness via end-to-end deadlock avoidance and in-order delivery, and (5') improvement of coherence protocol responsiveness through adaptive in-network multicast support. We conduct a thorough evaluation that includes hardware cost estimation and performance evaluation under a wide spectrum of realistic workloads and coherence protocols. Comparing our proposal with VCTM, an optimized state-of-the-art wormhole router, it requires 50% less area, reduces on-chip cache hierarchy energy delay product on average by 20%, and improves the cache-coherency chip multiprocessor performance under realistic working conditions by up to 20%.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2092665266",
    "type": "article"
  },
  {
    "title": "Exploring the limits of GPGPU scheduling in control flow bound applications",
    "doi": "https://doi.org/10.1145/2086696.2086708",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Roman Malits; Evgeny Bolotin; Avinoam Kolodny; Avi Mendelson",
    "corresponding_authors": "",
    "abstract": "GPGPUs are optimized for graphics, for that reason the hardware is optimized for massively data parallel applications characterized by predictable memory access patterns and little control flow. For such applications' e.g., matrix multiplication, GPGPU based system can achieve very high performance. However, many general purpose data parallel applications are characterized as having intensive control flow and unpredictable memory access patterns. Optimizing the code in such problems for current hardware is often ineffective and even impractical since it exhibits low hardware utilization leading to relatively low performance. This work tracks the root causes of execution inefficacies when running control flow intensive CUDA applications on NVIDIA GPGPU hardware. We show both analytically and by simulations of various benchmarks that local thread scheduling has inherent limitations when dealing with applications that have high rate of branch divergence. To overcome those limitations we propose to use hierarchical warp scheduling and global warps reconstruction. We implement an ideal hierarchical warp scheduling mechanism we term ODGS (Oracle Dynamic Global Scheduling) designed to maximize machine utilization via global warp reconstruction. We show that in control flow bound applications that make no use of shared memory (1) there is still a substantial potential for performance improvement (2) we demonstrate, based on various synthetic and real benchmarks the feasible performance improvement. For example, MUM and BFS are parallel graph algorithms suffering from significant branch divergence. We show that in those algorithms it's possible to achieve performance gain of up to x4.4 and x2.6 relative to previously applied scheduling methods.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2106815796",
    "type": "article"
  },
  {
    "title": "Temporal-based multilevel correlating inclusive cache replacement",
    "doi": "https://doi.org/10.1145/2541228.2555290",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Yingying Tian; Samira Khan; Daniel A. Jiménez",
    "corresponding_authors": "",
    "abstract": "Inclusive caches have been widely used in Chip Multiprocessors (CMPs) to simplify cache coherence. However, they have poor performance compared with noninclusive caches not only because of the limited capacity of the entire cache hierarchy but also due to ignorance of temporal locality of the Last-Level Cache (LLC). Blocks that are highly referenced (referred to as hot blocks ) are always hit in higher-level caches (e.g., L1 cache) and are rarely referenced in the LLC. Therefore, they become replacement victims in the LLC. Due to the inclusion property, blocks evicted from the LLC have to also be invalidated from higher-level caches. Invalidation of hot blocks from the entire cache hierarchy introduces costly off-chip misses that makes the inclusive cache perform poorly. Neither blocks that are highly referenced in the LLC nor blocks that are highly referenced in higher-level caches should be the LLC replacement victims. We propose temporal-based multilevel correlating cache replacement for inclusive caches to evict blocks in the LLC that are also not hot in higher-level caches using correlated temporal information acquired from all levels of a cache hierarchy with minimal overhead. Invalidation of these blocks does not hurt the performance. By contrast, replacing them as early as possible with useful blocks helps improve cache performance. Based on our experiments, in a dual-core CMP, an inclusive cache with temporal-based multilevel correlating cache replacement significantly outperforms an inclusive cache with traditional LRU replacement by yielding an average speedup of 12.7%, which is comparable to an enhanced noninclusive cache, while requiring less than 1% of storage overhead.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2170510345",
    "type": "article"
  },
  {
    "title": "WCET-Aware Dynamic I-Cache Locking for a Single Task",
    "doi": "https://doi.org/10.1145/3046683",
    "publication_date": "2017-03-13",
    "publication_year": 2017,
    "authors": "Wenguang Zheng; Hui Wu; Qing Yang",
    "corresponding_authors": "",
    "abstract": "Caches are widely used in embedded systems to bridge the increasing speed gap between processors and off-chip memory. However, caches make it significantly harder to compute the worst-case execution time (WCET) of a task. To alleviate this problem, cache locking has been proposed. We investigate the WCET-aware I-cache locking problem and propose a novel dynamic I-cache locking heuristic approach for reducing the WCET of a task. For a nonnested loop, our approach aims at selecting a minimum set of memory blocks of the loop as locked cache contents by using the min-cut algorithm. For a loop nest, our approach not only aims at selecting a minimum set of memory blocks of the loop nest as locked cache contents but also finds a good loading point for each selected memory block. We propose two algorithms for finding a good loading point for each selected memory block, a polynomial-time heuristic algorithm and an integer linear programming (ILP)-based algorithm, further reducing the WCET of each loop nest. We have implemented our approach and compared it to two state-of-the-art I-cache locking approaches by using a set of benchmarks from the MRTC benchmark suite. The experimental results show that the polynomial-time heuristic algorithm for finding a good loading point for each selected memory block performs almost equally as well as the ILP-based algorithm. Compared to the partial locking approach proposed in Ding et al. [2012], our approach using the heuristic algorithm achieves the average improvements of 33%, 15%, 9%, 3%, 8%, and 11% for the 256B, 512B, 1KB, 4KB, 8KB, and 16KB caches, respectively. Compared to the dynamic locking approach proposed in Puaut [2006], it achieves the average improvements of 9%, 19%, 18%, 5%, 11%, and 16% for the 256B, 512B, 1KB, 4KB, 8KB, and 16KB caches, respectively.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2602228997",
    "type": "article"
  },
  {
    "title": "Band-Pass Prefetching",
    "doi": "https://doi.org/10.1145/3090635",
    "publication_date": "2017-06-28",
    "publication_year": 2017,
    "authors": "Aswinkumar Sridharan; Biswabandan Panda; André Seznec",
    "corresponding_authors": "",
    "abstract": "In multi-core systems, an application’s prefetcher can interfere with the memory requests of other applications using the shared resources, such as last level cache and memory bandwidth. In order to minimize prefetcher-caused interference, prior mechanisms have been proposed to dynamically control prefetcher aggressiveness at runtime. These mechanisms use several parameters to capture prefetch usefulness as well as prefetcher-caused interference, performing aggressive control decisions. However, these mechanisms do not capture the actual interference at the shared resources and most often lead to incorrect aggressiveness control decisions. Therefore, prior works leave scope for performance improvement. Toward this end, we propose a solution to manage prefetching in multicore systems. In particular, we make two fundamental observations: First, a positive correlation exists between the accuracy of a prefetcher and the amount of prefetch requests it generates relative to an application’s total (demand and prefetch) requests. Second, a strong positive correlation exists between the ratio of total prefetch to demand requests and the ratio of average last level cache miss service times of demand to prefetch requests. In this article, we propose Band-pass prefetching that builds on those two observations, a simple and low-overhead mechanism to effectively manage prefetchers in multicore systems. Our solution consists of local and global prefetcher aggressiveness control components, which altogether, control the flow of prefetch requests between a range of prefetch to demand requests ratios. From our experiments on 16-core multi-programmed workloads, on systems using stream prefetching, we observe that Band-pass prefetching achieves 12.4% (geometric-mean) improvement on harmonic speedup over the baseline that implements no prefetching, while aggressive prefetching without prefetcher aggressiveness control and state-of-the-art HPAC, P-FST, and CAFFEINE achieve 8.2%, 8.4%, 1.4%, and 9.7%, respectively. Further evaluation of the proposed Band-pass prefetching mechanism on systems using AMPM prefetcher shows similar performance trends. For a 16-core system, Band-pass prefetching requires only a modest hardware cost of 239 bytes.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2613283514",
    "type": "article"
  },
  {
    "title": "HAP",
    "doi": "https://doi.org/10.1145/3106340",
    "publication_date": "2017-09-06",
    "publication_year": 2017,
    "authors": "Wei Wei; Dejun Jiang; Jin Xiong; Mingyu Chen",
    "corresponding_authors": "",
    "abstract": "Data-center servers benefit from large-capacity memory systems to run multiple processes simultaneously. Hybrid DRAM-NVM memory is attractive for increasing memory capacity by exploiting the scalability of Non-Volatile Memory (NVM). However, current LLC policies are unaware of hybrid memory. Cache misses to NVM introduce high cost due to long NVM latency. Moreover, evicting dirty NVM data suffer from long write latency. We propose hybrid memory aware cache partitioning to dynamically adjust cache spaces and give NVM dirty data more chances to reside in LLC. Experimental results show Hybrid-memory-Aware Partition (HAP) improves performance by 46.7% and reduces energy consumption by 21.9% on average against LRU management. Moreover, HAP averagely improves performance by 9.3% and reduces energy consumption by 6.4% against a state-of-the-art cache mechanism.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2752650511",
    "type": "article"
  },
  {
    "title": "MBZip",
    "doi": "https://doi.org/10.1145/3151033",
    "publication_date": "2017-12-05",
    "publication_year": 2017,
    "authors": "Raghavendra Kanakagiri; Biswabandan Panda; Madhu Mutyam",
    "corresponding_authors": "",
    "abstract": "Compression techniques at the last-level cache and the DRAM play an important role in improving system performance by increasing their effective capacities. A compressed block in DRAM also reduces the transfer time over the memory bus to the caches, reducing the latency of a LLC cache miss. Usually, compression is achieved by exploiting data patterns present within a block. But applications can exhibit data locality that spread across multiple consecutive data blocks. We observe that there is significant opportunity available for compressing multiple consecutive data blocks into one single block, both at the LLC and DRAM. Our studies using 21 SPEC CPU applications show that, at the LLC, around 25% (on average) of the cache blocks can be compressed into one single cache block when grouped together in groups of 2 to 8 blocks. In DRAM, more than 30% of the columns residing in a single DRAM page can be compressed into one DRAM column, when grouped together in groups of 2 to 6. Motivated by these observations, we propose a mechanism, namely, MBZip, that compresses multiple data blocks into one single block (called a zipped block), both at the LLC and DRAM. At the cache, MBZip includes a simple tag structure to index into these zipped cache blocks and the indexing does not incur any redirectional delay. At the DRAM, MBZip does not need any changes to the address computation logic and works seamlessly with the conventional/existing logic. MBZip is a synergistic mechanism that coordinates these zipped blocks at the LLC and DRAM. Further, we also explore silent writes at the DRAM and show that certain writes need not access the memory when blocks are zipped. MBZip improves the system performance by 21.9%, with a maximum of 90.3% on a 4-core system.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2771609513",
    "type": "article"
  },
  {
    "title": "Data-Driven Concurrency for High Performance Computing",
    "doi": "https://doi.org/10.1145/3162014",
    "publication_date": "2017-12-20",
    "publication_year": 2017,
    "authors": "George Matheou; Paraskevas Evripidou",
    "corresponding_authors": "",
    "abstract": "In this work, we utilize dynamic dataflow/data-driven techniques to improve the performance of high performance computing (HPC) systems. The proposed techniques are implemented and evaluated through an efficient, portable, and robust programming framework that enables data-driven concurrency on HPC systems. The proposed framework is based on data-driven multithreading (DDM), a hybrid control-flow/dataflow model that schedules threads based on data availability on sequential processors. The proposed framework was evaluated using several benchmarks, with different characteristics, on two different systems: a 4-node AMD system with a total of 128 cores and a 64-node Intel HPC system with a total of 768 cores. The performance evaluation shows that the proposed framework scales well and tolerates scheduling overheads and memory latencies effectively. We also compare our framework to MPI, DDM-VM, and OmpSs@Cluster. The comparison results show that the proposed framework obtains comparable or better performance.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2780488395",
    "type": "article"
  },
  {
    "title": "GPU Performance vs. Thread-Level Parallelism",
    "doi": "https://doi.org/10.1145/3177964",
    "publication_date": "2018-03-22",
    "publication_year": 2018,
    "authors": "Zhen Lin; Michael Mantor; Huiyang Zhou",
    "corresponding_authors": "",
    "abstract": "Graphics Processing Units (GPUs) leverage massive thread-level parallelism (TLP) to achieve high computation throughput and hide long memory latency. However, recent studies have shown that the GPU performance does not scale with the GPU occupancy or the degrees of TLP that a GPU supports, especially for memory-intensive workloads. The current understanding points to L1 D-cache contention or off-chip memory bandwidth. In this article, we perform a novel scalability analysis from the perspective of throughput utilization of various GPU components, including off-chip DRAM, multiple levels of caches, and the interconnect between L1 D-caches and L2 partitions. We show that the interconnect bandwidth is a critical bound for GPU performance scalability. For the applications that do not have saturated throughput utilization on a particular resource, their performance scales well with increased TLP. To improve TLP for such applications efficiently, we propose a fast context switching approach. When a warp/thread block (TB) is stalled by a long latency operation, the context of the warp/TB is spilled to spare on-chip resource so that a new warp/TB can be launched. The switched-out warp/TB is switched back when another warp/TB is completed or switched out. With this fine-grain fast context switching, higher TLP can be supported without increasing the sizes of critical resources like the register file. Our experiment shows that the performance can be improved by up to 47% and a geometric mean of 22% for a set of applications with unsaturated throughput utilization. Compared to the state-of-the-art TLP improvement scheme, our proposed scheme achieves 12% higher performance on average and 16% for unsaturated benchmarks.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2791833900",
    "type": "article"
  },
  {
    "title": "Improving Parallelism in Hardware Transactional Memory",
    "doi": "https://doi.org/10.1145/3177962",
    "publication_date": "2018-03-22",
    "publication_year": 2018,
    "authors": "Dave Dice; Maurice Herlihy; Alex Kogan",
    "corresponding_authors": "",
    "abstract": "Today’s hardware transactional memory (HTM) systems rely on existing coherence protocols, which implement a requester-wins strategy. This, in turn, leads to poor performance when transactions frequently conflict, causing them to resort to a non-speculative fallback path. Often, such a path severely limits parallelism. In this article, we propose very simple architectural changes to the existing requester-wins HTM implementations that enhance conflict resolution between hardware transactions and thus improve their parallelism. Our idea is compatible with existing HTM systems, requires no changes to target applications that employ traditional lock synchronization, and is shown to provide robust performance benefits.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2792782933",
    "type": "article"
  },
  {
    "title": "Enabling SIMT Execution Model on Homogeneous Multi-Core System",
    "doi": "https://doi.org/10.1145/3177960",
    "publication_date": "2018-03-22",
    "publication_year": 2018,
    "authors": "Kuan‐Chung Chen; Chung‐Ho Chen",
    "corresponding_authors": "",
    "abstract": "Single-instruction multiple-thread (SIMT) machine emerges as a primary computing device in high-perfor-mance computing, since the SIMT execution paradigm can exploit data-level parallelism effectively. This article explores the SIMT execution potential on homogeneous multi-core processors, which generally run in multiple-instruction multiple-data (MIMD) mode when utilizing the multi-core resources. We address three architecture issues in enabling SIMT execution model on multi-core processor, including multithreading execution model, kernel thread context placement, and thread divergence. For the SIMT execution model, we propose a fine-grained multithreading mechanism on an ARM-based multi-core system. Each of the processor cores stores the kernel thread contexts in its L1 data cache for per-cycle thread-switching requirement. For divergence-intensive kernels, an Inner Conditional Statement First (ICS-First) mechanism helps early re-convergence to occur and significantly improves the performance. The experiment results show that effectiveness in data-parallel processing reduces on average 36% dynamic instructions, and boosts the SIMT executions to achieve on average 1.52× and up to 5× speedups over the MIMD counterpart for OpenCL benchmarks for single issue in-order processor cores. By using the explicit vectorization optimization on the kernels, the SIMT model gains further benefits from the SIMD extension and achieves 1.71× speedup over the MIMD approach. The SIMT model using in-order superscalar processor cores outperforms the MIMD model that uses superscalar out-of-order processor cores by 40%. The results show that, to exploit data-level parallelism, enabling the SIMT model on homogeneous multi-core processors is important.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2794160862",
    "type": "article"
  },
  {
    "title": "Bandwidth and Locality Aware Task-stealing for Manycore Architectures with Bandwidth-Asymmetric Memory",
    "doi": "https://doi.org/10.1145/3291058",
    "publication_date": "2018-12-08",
    "publication_year": 2018,
    "authors": "Han Zhao; Quan Chen; Yuxian Qiu; Ming Wu; Yao Shen; Jingwen Leng; Chao Li; Minyi Guo",
    "corresponding_authors": "",
    "abstract": "Parallel computers now start to adopt Bandwidth-Asymmetric Memory architecture that consists of traditional DRAM memory and new High Bandwidth Memory (HBM) for high memory bandwidth. However, existing task schedulers suffer from low bandwidth usage and poor data locality problems in bandwidth-asymmetric memory architectures. To solve the two problems, we propose a Bandwidth and Locality Aware Task-stealing (BATS) system, which consists of an HBM-aware data allocator, a bandwidth-aware traffic balancer, and a hierarchical task-stealing scheduler. Leveraging compile-time code transformation and run-time data distribution, the data allocator enables HBM usage automatically without user interference. According to data access hotness, the traffic balancer migrates data to balance memory traffic across memory nodes proportional to their bandwidth. The hierarchical scheduler improves data locality at runtime without a priori program knowledge. Experiments on an Intel Knights Landing server that adopts bandwidth-asymmetric memory show that BATS reduces the execution time of memory-bound programs up to 83.5% compared with traditional task-stealing schedulers.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2904487346",
    "type": "article"
  },
  {
    "title": "Building a Polyhedral Representation from an Instrumented Execution",
    "doi": "https://doi.org/10.1145/3363785",
    "publication_date": "2019-12-17",
    "publication_year": 2019,
    "authors": "Manuel Selva; Fabian M. Gruber; Diogo Sampaio; Christophe Guillon; Louis-Noël Pouchet; Fabrice Rastello",
    "corresponding_authors": "",
    "abstract": "The polyhedral model has been successfully used in production compilers. Nevertheless, only a very restricted class of applications can benefit from it. Recent proposals investigated how runtime information could be used to apply polyhedral optimization on applications that do not statically fit the model. In this work, we go one step further in that direction. We propose the folding-based analysis that, from the output of an instrumented program execution, builds a compact polyhedral representation. It is able to accurately detect affine dependencies, fixed-stride memory accesses, and induction variables in programs. It scales to real-life applications, which often include some nonaffine dependencies and accesses in otherwise affine code. This is enabled by a safe fine-grained polyhedral overapproximation mechanism. We evaluate our analysis on the entire Rodinia benchmark suite, enabling accurate feedback about the potential for complex polyhedral transformations.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2908475304",
    "type": "article"
  },
  {
    "title": "Dual-Page Checkpointing",
    "doi": "https://doi.org/10.1145/3291057",
    "publication_date": "2018-12-31",
    "publication_year": 2018,
    "authors": "Song Wu; Fang Zhou; Xiang Gao; Hai Jin; Jinglei Ren",
    "corresponding_authors": "",
    "abstract": "Data persistence is necessary for many in-memory applications. However, the disk-based data persistence largely slows down in-memory applications. Emerging non-volatile memory (NVM) offers an opportunity to achieve in-memory data persistence at the DRAM-level performance. Nevertheless, NVM typically requires a software library to operate NVM data, which brings significant overhead. This article demonstrates that a hardware-based high-frequency checkpointing mechanism can be used to achieve efficient in-memory data persistence on NVM. To maintain checkpoint consistency, traditional logging and copy-on-write techniques incur excessive NVM writes that impair both performance and endurance of NVM; recent work attempts to solve the issue but requires a large amount of metadata in the memory controller. Hence, we design a new dual-page checkpointing system, which achieves low metadata cost and eliminates most excessive NVM writes at the same time. It breaks the traditional trade-off between metadata space cost and extra data writes. Our solution outperforms the state-of-the-art NVM software libraries by 13.6× in throughput, and leads to 34% less NVM wear-out and 1.28× higher throughput than state-of-the-art hardware checkpointing solutions, according to our evaluation with OLTP, graph computing, and machine-learning workloads.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2910726692",
    "type": "article"
  },
  {
    "title": "A Self-aware Resource Management Framework for Heterogeneous Multicore SoCs with Diverse QoS Targets",
    "doi": "https://doi.org/10.1145/3319804",
    "publication_date": "2019-04-09",
    "publication_year": 2019,
    "authors": "Yang Song; Olivier Alavoine; Bill Lin",
    "corresponding_authors": "",
    "abstract": "In modern heterogeneous MPSoCs, the management of shared memory resources is crucial in delivering end-to-end QoS. Previous frameworks have either focused on singular QoS targets or the allocation of partitionable resources among CPU applications at relatively slow timescales. However, heterogeneous MPSoCs typically require instant response from the memory system where most resources cannot be partitioned. Moreover, the health of different cores in a heterogeneous MPSoC is often measured by diverse performance objectives. In this work, we propose the Self-Aware Resource Allocation framework for heterogeneous MPSoCs. Priority-based adaptation allows cores to use different target performance and self-monitor their own intrinsic health. In response, the system allocates non-partitionable resources based on priorities. The proposed framework meets a diverse range of QoS demands from heterogeneous cores. Moreover, we present a runtime scheme to configure priority-based adaptation so that distinct sensitivities of heterogeneous QoS targets with respect to memory allocation can be accommodated. In addition, the priority of best-effort cores can also be regulated.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2936689759",
    "type": "article"
  },
  {
    "title": "Transparent Acceleration for Heterogeneous Platforms With Compilation to OpenCL",
    "doi": "https://doi.org/10.1145/3319423",
    "publication_date": "2019-04-18",
    "publication_year": 2019,
    "authors": "Heinrich Riebler; Gavin Vaz; Tobias Kenter; Christian Plessl",
    "corresponding_authors": "",
    "abstract": "Multi-accelerator platforms combine CPUs and different accelerator architectures within a single compute node. Such systems are capable of processing parallel workloads very efficiently while being more energy efficient than regular systems consisting of CPUs only. However, the architectures of such systems are diverse, forcing developers to port applications to each accelerator using different programming languages, models, tools, and compilers. Developers not only require domain-specific knowledge but also need to understand the low-level accelerator details, leading to an increase in the design effort and costs. To tackle this challenge, we propose a compilation approach and a practical realization called HT r OP that is completely transparent to the user. HT r OP is able to automatically analyze a sequential CPU application, detect computational hotspots, and generate parallel OpenCL host and kernel code. The potential of HT r OP is demonstrated by offloading hotspots to different OpenCL-enabled resources (currently the CPU, the general-purpose GPU, and the manycore Intel Xeon Phi) for a broad set of benchmark applications. We present an in-depth evaluation of our approach in terms of performance gains and energy savings, taking into account all static and dynamic overheads. We are able to achieve speedups and energy savings of up to two orders of magnitude, if an application has sufficient computational intensity, when compared to a natively compiled application.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2938200218",
    "type": "article"
  },
  {
    "title": "MH Cache",
    "doi": "https://doi.org/10.1145/3328520",
    "publication_date": "2019-07-18",
    "publication_year": 2019,
    "authors": "Jungwoo Park; Myoungjun Lee; Soontae Kim; Minho Ju; Jeongkyu Hong",
    "corresponding_authors": "",
    "abstract": "Mobile devices have become the most important devices in our life. However, they are limited in battery capacity. Therefore, low-power computing is crucial for their long lifetime. A spin-transfer torque RAM (STT-RAM) has become emerging memory technology because of its low leakage power consumption. We herein propose MH cache, a multi-retention STT-RAM-based cache management scheme for last-level caches (LLC) to reduce their power consumption for mobile hardware rendering systems. We analyzed the memory access patterns of processes and observed how rendering methods affect process behaviors. We propose a cache management scheme that measures write-intensity of each process dynamically and exploits it to manage a power-efficient multi-retention STT-RAM-based cache. Our proposed scheme uses variable threshold for a process’ write-intensity to determine cache line placement. We explain how to deal with the following issue to implement our proposed scheme. Our experimental results show that our techniques significantly reduce the LLC power consumption by 32% and 32.2% in single- and quad-core systems, respectively, compared to a full STT-RAM LLC.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2964226733",
    "type": "article"
  },
  {
    "title": "Evaluating Auto-Vectorizing Compilers through Objective Withdrawal of Useful Information",
    "doi": "https://doi.org/10.1145/3356842",
    "publication_date": "2019-10-11",
    "publication_year": 2019,
    "authors": "Sergi Siso; Wesley Armour; Jeyan Thiyagalingam",
    "corresponding_authors": "",
    "abstract": "The need for compilers to generate highly vectorized code is at an all-time high with the increasing vectorization capabilities of modern processors. To this end, the information that compilers have at their disposal, either through code analysis or via user annotations, is instrumental for auto-vectorization, and hence for the overall performance. However, the information that is available to compilers at compile time and its accuracy varies greatly, as does the resulting performance of vectorizing compilers. Benchmarks like the Test Suite for Vectorizing Compilers (TSVC) have been developed to evaluate the vectorization capability of such compilers. The overarching approach of TSVC and similar benchmarks is to evaluate the compilers under the best possible scenario (i.e., assuming that compilers have access to all useful contextual information at compile time). Although this idealistic view is useful to observe the capability of compilers for auto-vectorization, it is not a true reflection of the conditions found in real-world applications. In this article, we propose a novel method for evaluating the auto-vectorization capability of compilers. Instead of assuming that compilers have access to a wealth of information at compile time, we formulate a method to objectively supply or withdraw information that would otherwise aid the compiler in the auto-vectorization process. This method is orthogonal to the approach adopted by TSVC, and as such, it provides the means of assessing the capabilities of modern vectorizing compilers in a more detailed way. Using this new method, we exhaustively evaluated five industry-grade compilers (GNU, Intel, Clang, PGI, and IBM) on four representative vector platforms (AVX-2, AVX-512 (Skylake), AVX-512 (KNL), and AltiVec) using the modified version of TSVC and application-level proxy kernels. The results show the impact that withdrawing information has on the vectorization capabilities of each compiler and also prove the validity of the presented technique.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2979513934",
    "type": "article"
  },
  {
    "title": "Tiling Optimizations for Stencil Computations Using Rewrite Rules in L <scp>ift</scp>",
    "doi": "https://doi.org/10.1145/3368858",
    "publication_date": "2019-12-26",
    "publication_year": 2019,
    "authors": "Larisa Stoltzfus; Bastian Hagedorn; Michel Steuwer; Sergei Gorlatch; Christophe Dubach",
    "corresponding_authors": "",
    "abstract": "Stencil computations are a widely used type of algorithm, found in applications from physical simulations to machine learning. Stencils are embarrassingly parallel, therefore fit on modern hardware such as Graphic Processing Units perfectly. Although stencil computations have been extensively studied, optimizing them for increasingly diverse hardware remains challenging. Domain-specific Languages (DSLs) have raised the programming abstraction and offer good performance; however, this method places the burden on DSL implementers to write almost full-fledged parallelizing compilers and optimizers. Lift has recently emerged as a promising approach to achieve performance portability by using a small set of reusable parallel primitives that DSL or library writers utilize. L ift ’s key novelty is in its encoding of optimizations as a system of extensible rewrite rules which are used to explore the optimization space. This article demonstrates how complex multi-dimensional stencil code and optimizations are expressed using compositions of simple 1D L ift primitives and rewrite rules. We introduce two optimizations that provide high performance for stencils in particular: classical overlapped tiling for multi-dimensional stencils and 2.5D tiling specifically for 3D stencils. We provide an in-depth analysis on how the tiling optimizations affects stencils of different shapes and sizes across different applications. Our experimental results show that our approach outperforms existing compiler approaches and hand-tuned codes.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2997495048",
    "type": "article"
  },
  {
    "title": "A Conflict-free Scheduler for High-performance Graph Processing on Multi-pipeline FPGAs",
    "doi": "https://doi.org/10.1145/3390523",
    "publication_date": "2020-05-29",
    "publication_year": 2020,
    "authors": "Qinggang Wang; Long Zheng; Jieshan Zhao; Xiaofei Liao; Hai Jin; Jingling Xue",
    "corresponding_authors": "",
    "abstract": "FPGA-based graph processing accelerators are nowadays equipped with multiple pipelines for hardware acceleration of graph computations. However, their multi-pipeline efficiency can suffer greatly from the considerable overheads caused by the read/write conflicts in their on-chip BRAM from different pipelines, leading to significant performance degradation and poor scalability. In this article, we investigate the underlying causes behind such inter-pipeline read/write conflicts by focusing on multi-pipeline FPGAs for accelerating Sparse Matrix Vector Multiplication (SpMV) arising in graph processing. We exploit our key insight that the problem of eliminating inter-pipeline read/write conflicts for SpMV can be formulated as one of solving a row- and column-wise tiling problem for its associated adjacency matrix. However, how to partition a sparse adjacency matrix obtained from any graph with respect to a set of pipelines by both eliminating all the inter-pipeline read/write conflicts and keeping all the pipelines reasonably load-balanced is challenging. We present a conflict-free scheduler, WaveScheduler, that can dispatch different sub-matrix tiles to different pipelines without any read/write conflict. We also introduce two optimizations that are specifically tailored for graph processing, “degree-aware vertex index renaming” for improving load balancing and “data re-organization” for enabling sequential off-chip memory access, for all the pipelines. Our evaluation on Xilinx®Alveo™ U250 accelerator card with 16 pipelines shows that WaveScheduler can achieve up to 3.57 GTEPS, running much faster than native scheduling and two state-of-the-art FPGA-based graph accelerators (by 6.48× for “native,” 2.54× for HEGP, and 2.11× for ForeGraph), on average. In particular, these performance gains also scale up significantly as the number of pipelines increases.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3042639142",
    "type": "article"
  },
  {
    "title": "Locality-Aware CTA Scheduling for Gaming Applications",
    "doi": "https://doi.org/10.1145/3477497",
    "publication_date": "2021-12-06",
    "publication_year": 2021,
    "authors": "Aditya Ukarande; Suryakant Patidar; Ram Rangan",
    "corresponding_authors": "",
    "abstract": "The compute work rasterizer or the GigaThread Engine of a modern NVIDIA GPU focuses on maximizing compute work occupancy across all streaming multiprocessors in a GPU while retaining design simplicity. In this article, we identify the operational aspects of the GigaThread Engine that help it meet those goals but also lead to less-than-ideal cache locality for texture accesses in 2D compute shaders, which are an important optimization target for gaming applications. We develop three software techniques, namely LargeCTAs , Swizzle , and Agents , to show that it is possible to effectively exploit the texture data working set overlap intrinsic to 2D compute shaders. We evaluate these techniques on gaming applications across two generations of NVIDIA GPUs, RTX 2080 and RTX 3080, and find that they are effective on both GPUs. We find that the bandwidth savings from all our software techniques on RTX 2080 is much higher than the bandwidth savings on baseline execution from inter-generational cache capacity increase going from RTX 2080 to RTX 3080. Our best-performing technique, Agents , records up to a 4.7% average full-frame speedup by reducing bandwidth demand of targeted shaders at the L1-L2 and L2-DRAM interfaces by 23% and 32%, respectively, on the latest generation RTX 3080. These results acutely highlight the sensitivity of cache locality to compute work rasterization order and the importance of locality-aware cooperative thread array scheduling for gaming applications.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4200051960",
    "type": "article"
  },
  {
    "title": "SecNVM: An Efficient and Write-Friendly Metadata Crash Consistency Scheme for Secure NVM",
    "doi": "https://doi.org/10.1145/3488724",
    "publication_date": "2021-12-06",
    "publication_year": 2021,
    "authors": "Mengya Lei; Fan Li; Fang Wang; Dan Feng; Xiaomin Zou; Renzhi Xiao",
    "corresponding_authors": "",
    "abstract": "Data security is an indispensable part of non-volatile memory (NVM) systems. However, implementing data security efficiently on NVM is challenging, since we have to guarantee the consistency of user data and the related security metadata. Existing consistency schemes ignore the recoverability of the SGX style integrity tree (SIT) and the access correlation between metadata blocks, thereby generating unnecessary NVM write traffic. In this article, we propose SecNVM, an efficient and write-friendly metadata crash consistency scheme for secure NVM. SecNVM utilizes the observation that for a lazily updated SIT, the lost tree nodes after a crash can be recovered by the corresponding child nodes in NVM. It reduces the SIT persistency overhead through a restrained write-back metadata cache and exploits the SIT inter-layer dependency for recovery. Next, leveraging the strong access correlation between the counter and DMAC, SecNVM improves the efficiency of security metadata access through a novel collaborative counter-DMAC scheme. In addition, it adopts a lightweight address tracker to reduce the cost of address tracking for fast recovery. Experiments show that compared to the state-of-the-art schemes, SecNVM improves the performance and decreases write traffic a lot, and achieves an acceptable recovery time.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4200499094",
    "type": "article"
  },
  {
    "title": "CAFFEINE",
    "doi": "https://doi.org/10.1145/2806891",
    "publication_date": "2015-08-31",
    "publication_year": 2015,
    "authors": "Biswabandan Panda; Shankar Balachandran",
    "corresponding_authors": "",
    "abstract": "Aggressive prefetching improves system performance by hiding and tolerating off-chip memory latency. However, on a multicore system, prefetchers of different cores contend for shared resources and aggressive prefetching can degrade the overall system performance. The role of a prefetcher aggressiveness engine is to select appropriate aggressiveness levels for each prefetcher such that shared resource contention caused by prefetchers is reduced, thereby improving system performance. State-of-the-art prefetcher aggressiveness engines monitor metrics such as prefetch accuracy, bandwidth consumption, and last-level cache pollution. They use carefully tuned thresholds for these metrics, and when the thresholds are crossed, they trigger aggressiveness control measures. These engines have three major shortcomings: (1) thresholds are dependent on the system configuration (cache size, DRAM scheduling policy, and cache replacement policy) and have to be tuned appropriately, (2) there is no single threshold that works well across all the workloads, and (3) thresholds are oblivious to the phase change of applications. To overcome these shortcomings, we propose CAFFEINE, a model-based approach that analyzes the effectiveness of a prefetcher and uses a metric called net utility to control the aggressiveness. Our metric provides net processor cycles saved because of prefetching by approximating the cycles saved across the memory subsystem, from last-level cache to DRAM. We evaluate CAFFEINE across a wide range of workloads and compare it with the state-of-the-art prefetcher aggressiveness engine. Experimental results demonstrate that, on average (geomean), CAFFEINE achieves 9.5% (as much as 38.29%) and 11% (as much as 20.7%) better performance than the best-performing aggressiveness engine for four-core and eight-core systems, respectively.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2014790243",
    "type": "article"
  },
  {
    "title": "Shared-port register file architecture for low-energy VLIW processors",
    "doi": "https://doi.org/10.1145/2533397",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "Neeraj Goel; Anshul Kumar; Preeti Ranjan Panda",
    "corresponding_authors": "",
    "abstract": "We propose a reduced-port Register File (RF) architecture for reducing RF energy in a VLIW processor. With port reduction, RF ports need to be shared among Function Units (FUs), which may lead to access conflicts, and thus, reduced performance. Our solution includes (i) a carefully designed RF-FU interconnection network that permits port sharing with minimum conflicts and without any delay/energy overheads, and (ii) a novel scheduling and binding algorithm that reduces the performance penalty. With our solution, we observed as much as 83% RF energy savings with no more than a 10% loss in performance for a set of Mediabench and Mibench benchmarks.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2029077128",
    "type": "article"
  },
  {
    "title": "NUCA-L1",
    "doi": "https://doi.org/10.1145/2631918",
    "publication_date": "2014-10-27",
    "publication_year": 2014,
    "authors": "Farrukh Hijaz; Omer Khan",
    "corresponding_authors": "",
    "abstract": "Research has shown that operating in the near-threshold region is expected to provide up to 10× energy efficiency for future processors. However, reliable operation below a minimum voltage (Vccmin) cannot be guaranteed due to process variations. Because SRAM margins can easily be violated at near-threshold voltages, their bit-cell failure rates are expected to rise steeply. Multicore processors rely on fast private L1 caches to exploit data locality and achieve high performance. In the presence of high bit-cell fault rates, traditionally an L1 cache either sacrifices capacity or incurs additional latency to correct the faults. We observe that L1 cache sensitivity to hit latency offers a design trade-off between capacity and latency. When fault rate is high at extreme Vccmin, it is beneficial to recover L1 cache capacity, even if it comes at the cost of additional latency. However, at low fault rates, the additional constant latency to recover cache capacity degrades performance. With this trade-off in mind, we propose a Non-Uniform Cache Access L1 architecture (NUCA-L1) that avoids additional latency on accesses to fault-free cache lines. To mitigate the capacity bottleneck, it deploys a correction mechanism to recover capacity at the cost of additional latency. Using extensive simulations of a 64-core multicore, we demonstrate that at various bit-cell fault rates, our proposed private NUCA-L1 cache architecture performs better than state-of-the-art schemes, along with a significant reduction in energy consumption.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2047216488",
    "type": "article"
  },
  {
    "title": "Performance-Energy Considerations for Shared Cache Management in a Heterogeneous Multicore Processor",
    "doi": "https://doi.org/10.1145/2710019",
    "publication_date": "2015-03-09",
    "publication_year": 2015,
    "authors": "Anup Holey; Vineeth Mekkat; Pen-Chung Yew; Antonia Zhai",
    "corresponding_authors": "",
    "abstract": "Heterogeneous multicore processors that integrate CPU cores and data-parallel accelerators such as graphic processing unit (GPU) cores onto the same die raise several new issues for sharing various on-chip resources. The shared last-level cache (LLC) is one of the most important shared resources due to its impact on performance. Accesses to the shared LLC in heterogeneous multicore processors can be dominated by the GPU due to the significantly higher number of concurrent threads supported by the architecture. Under current cache management policies, the CPU applications’ share of the LLC can be significantly reduced in the presence of competing GPU applications. For many CPU applications, a reduced share of the LLC could lead to significant performance degradation. On the contrary, GPU applications can tolerate increase in memory access latency when there is sufficient thread-level parallelism (TLP). In addition to the performance challenge, introduction of diverse cores onto the same die changes the energy consumption profile and, in turn, affects the energy efficiency of the processor. In this work, we propose heterogeneous LLC management (HeLM), a novel shared LLC management policy that takes advantage of the GPU’s tolerance for memory access latency. HeLM is able to throttle GPU LLC accesses and yield LLC space to cache-sensitive CPU applications. This throttling is achieved by allowing GPU accesses to bypass the LLC when an increase in memory access latency can be tolerated. The latency tolerance of a GPU application is determined by the availability of TLP, which is measured at runtime as the average number of threads that are available for issuing. For a baseline configuration with two CPU cores and four GPU cores, modeled after existing heterogeneous processor designs, HeLM outperforms least recently used (LRU) policy by 10.4%. Additionally, HeLM also outperforms competing policies. Our evaluations show that HeLM is able to sustain performance with varying core mix. In addition to the performance benefit, bypassing also reduces total accesses to the LLC, leading to a reduction in the energy consumption of the LLC module. However, LLC bypassing has the potential to increase off-chip bandwidth utilization and DRAM energy consumption. Our experiments show that HeLM exhibits better energy efficiency by reducing the ED 2 by 18% over LRU while impacting only a 7% increase in off-chip bandwidth utilization.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2054065039",
    "type": "article"
  },
  {
    "title": "SECRET",
    "doi": "https://doi.org/10.1145/2747876",
    "publication_date": "2015-06-24",
    "publication_year": 2015,
    "authors": "Chung-Hsiang Lin; De-Yu Shen; Yi-Jung Chen; Chia-Lin Yang; Cheng-Yuan Michael Wang",
    "corresponding_authors": "",
    "abstract": "DRAMs are used as the main memory in most computing systems today. Studies show that DRAMs contribute to a significant part of overall system power consumption. One of the main challenges in low-power DRAM design is the inevitable refresh process. Due to process variation, memory cells exhibit retention time variations. Current DRAMs use a single refresh period determined by the cell with the largest leakage. Since prolonging refresh intervals introduces retention errors, a set of previous works adopt conventional error-correcting code (ECC) to correct retention errors. However, these approaches introduce significant area and energy overheads. In this article, we propose a novel error correction framework for retention errors in DRAMs, called SECRET (selective error correction for refresh energy reduction). The key observations we make are that retention errors are hard errors rather than soft errors, and only few DRAM cells have large leakage. Therefore, instead of equipping error correction capability for all memory cells as existing ECC schemes, we only allocate error correction information to leaky cells under a refresh interval. Our SECRET framework contains two parts: an offline phase to identify memory cells with retention errors given a target error rate and a low-overhead error correction mechanism. The experimental results show that among all test cases performed, the proposed SECRET framework can reduce refresh power by 87.2% and overall DRAM power up to 18.57% with negligible area and performance overheads.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2247473009",
    "type": "article"
  },
  {
    "title": "Dynamic Shared SPM Reuse for Real-Time Multicore Embedded Systems",
    "doi": "https://doi.org/10.1145/2738051",
    "publication_date": "2015-05-11",
    "publication_year": 2015,
    "authors": "Morteza Mohajjel Kafshdooz; Alireza Ejlali",
    "corresponding_authors": "",
    "abstract": "Allocating the scratchpad memory (SPM) space to tasks is a challenging problem in real-time multicore embedded systems that use shared SPM. Proper SPM space allocation is important, as it considerably influences the application worst-case execution time (WCET), which is of great importance in real-time applications. To address this problem, in this article we present a dynamic SPM reuse scheme, where SPM space can be reused by other tasks during runtime without requiring any static SPM partitioning. Although the proposed scheme is applied dynamically at runtime, the required decision making is fairly complex and hence cannot be performed at runtime. We have developed techniques to perform the decision making offline at design time in the form of optimization problems combined with task scheduling/mapping. The proposed work is unlike previous works that either exploit static schemes for SPM space allocation or perform task scheduling/mapping and SPM space allocation incoherently. The experimental results show that our dynamic SPM reuse scheme can reduce WCET by up to 55% as compared to recent previous works on SPM allocation in real-time multicore embedded systems.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2258033970",
    "type": "article"
  },
  {
    "title": "Enabling PGAS Productivity with Hardware Support for Shared Address Mapping",
    "doi": "https://doi.org/10.1145/2842686",
    "publication_date": "2015-12-22",
    "publication_year": 2015,
    "authors": "Olivier Serres; Abdullah Kayi; Ahmad Anbar; Tarek El‐Ghazawi",
    "corresponding_authors": "",
    "abstract": "Due to its rich memory model, the partitioned global address space (PGAS) parallel programming model strikes a balance between locality-awareness and the ease of use of the global address space model. Although locality-awareness can lead to high performance, supporting the PGAS memory model is associated with penalties that can hinder PGAS’s potential for scalability and speed of execution. This is because mapping the PGAS memory model to the underlying system requires a mapping process that is done in software, thereby introducing substantial overhead for shared accesses even when they are local. Compiler optimizations have not been sufficient to offset this overhead. On the other hand, manual code optimizations can help, but this eliminates the productivity edge of PGAS. This article proposes a processor microarchitecture extension that can perform such address mapping in hardware with nearly no performance overhead. These extensions are then availed to compilers through extensions to the processor instructions. Thus, the need for manual optimizations is eliminated and the productivity of PGAS languages is unleashed. Using Unified Parallel C (UPC), a PGAS language, we present a case study of a prototype compiler and architecture support. Two different implementations of the system were realized. The first uses a full-system simulator, gem5, which evaluates the overall performance gain of the new hardware support. The second uses an FPGA Leon3 soft-core processor to verify implementation feasibility and to parameterize the cost of the new hardware. The new instructions show promising results on all tested codes, including the NAS Parallel Benchmark kernels in UPC. Performance improvements of up to 5.5× for unmodified codes, sometimes surpassing hand-optimized performance, were demonstrated. We also show that our four-core FPGA prototype requires less than 2.4% of the overall chip’s area.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2263688137",
    "type": "article"
  },
  {
    "title": "Reliable Integrity Checking in Multicore Processors",
    "doi": "https://doi.org/10.1145/2738052",
    "publication_date": "2015-05-11",
    "publication_year": 2015,
    "authors": "Arun K. Kanuparthi; Ramesh Karri",
    "corresponding_authors": "",
    "abstract": "Security and reliability have become important concerns in the design of computer systems. On one hand, microarchitectural enhancements for security (such as for dynamic integrity checking of code at runtime) have been proposed. On the other hand, independently, microarchitectural enhancements for reliability to detect and tolerate natural faults have also been proposed. A fault in these security enhancements due to alpha particles or aging might potentially pass off maliciously modified instructions as safe, rendering the security enhancements useless. Deliberate fault attacks by attackers can be launched to disable the security enhancements and then launch the well-known security attacks that would otherwise have been detected by these enhancements. We report an integrated microarchitecture support for security and reliability in multicore processors. Specifically, we add integrity checkers to protect the code running on the multiple cores in a multicore processor. We then adapt these checkers to check one another periodically to ensure reliable operation. These checkers naturally can check the other parts of the core. The average performance, power, and area costs for these security-reliability enhancements are 6.42%, 0.73%, and 0.53%, respectively.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2294522909",
    "type": "article"
  },
  {
    "title": "Variable Liberalization",
    "doi": "https://doi.org/10.1145/2963101",
    "publication_date": "2016-08-17",
    "publication_year": 2016,
    "authors": "Sanyam Mehta; Pen-Chung Yew",
    "corresponding_authors": "",
    "abstract": "In the wake of the current trend of increasing the number of cores on a chip, compiler optimizations for improving the memory performance have assumed increased importance. Loop fusion is one such key optimization that can alleviate memory and bandwidth wall and thus improve parallel performance. However, we find that loop fusion in interesting memory-intensive applications is prevented by the existence of dependences between temporary variables that appear in different loop nests. Furthermore, known techniques of allowing useful transformations in the presence of temporary variables, such as privatization and expansion, prove insufficient in such cases. In this work, we introduce variable liberalization , a technique that selectively removes dependences on temporary variables in different loop nests to achieve loop fusion while preserving the semantical correctness of the optimized program. This removal of extra-stringent dependences effectively amounts to variable expansion, thus achieving the benefit of an increased degree of freedom for program transformation but without an actual expansion. Hence, there is no corresponding increase in the memory footprint incurred. We implement liberalization in the Pluto polyhedral compiler and evaluate its performance on nine hot regions in five real applications. Results demonstrate parallel performance improvement of 1.92 × over the Intel compiler, averaged over the nine hot regions, and an overall improvement of as much as 2.17 × for an entire application, on an eight-core Intel Xeon processor.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2511242544",
    "type": "article"
  },
  {
    "title": "Task-RM: A Resource Manager for Energy Reduction in Task-Parallel Applications under Quality of Service Constraints",
    "doi": "https://doi.org/10.1145/3494537",
    "publication_date": "2022-01-23",
    "publication_year": 2022,
    "authors": "Muhammad Waqar Azhar; Miquel Pericàs; Per Stenström",
    "corresponding_authors": "",
    "abstract": "Improving energy efficiency is an important goal of computer system design. This article focuses on a general model of task-parallel applications under quality-of-service requirements on the completion time. Our technique, called Task-RM , exploits the variance in task execution-times and imbalance between tasks to allocate just enough resources in terms of voltage-frequency and core-allocation so that the application completes before the deadline. Moreover, we provide a solution that can harness additional energy savings with the availability of additional processors. We observe that, for the proposed run-time resource manager to allocate resources, it requires specification of the soft deadlines to the tasks. This is accomplished by analyzing the energy-saving scenarios offline and by providing Task-RM with the performance requirements of the tasks. The evaluation shows an energy saving of 33% compared to race-to-idle and 22% compared to dynamic slack allocation (DSA) with an overhead of less than 1%.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4206921211",
    "type": "article"
  },
  {
    "title": "Building a Fast and Efficient LSM-tree Store by Integrating Local Storage with Cloud Storage",
    "doi": "https://doi.org/10.1145/3527452",
    "publication_date": "2022-03-31",
    "publication_year": 2022,
    "authors": "Peng Xu; Nannan Zhao; Jiguang Wan; Wei Liu; Shuning Chen; Yuanhui Zhou; Hadeel Albahar; Hanyang Liu; Liu Tang; Zhihu Tan",
    "corresponding_authors": "",
    "abstract": "The explosive growth of modern web-scale applications has made cost-effectiveness a primary design goal for their underlying databases. As a backbone of modern databases, LSM-tree based key–value stores (LSM store) face limited storage options. They are either designed for local storage that is relatively small, expensive, and fast or for cloud storage that offers larger capacities at reduced costs but slower. Designing an LSM store by integrating local storage with cloud storage services is a promising way to balance the cost and performance. However, such design faces challenges such as data reorganization, metadata overhead, and reliability issues. In this article, we propose RocksMash , a fast and efficient LSM store that uses local storage to store frequently accessed data and metadata while using cloud to hold the rest of the data to achieve cost-effectiveness. To improve metadata space-efficiency and read performance, RocksMash uses an LSM-aware persistent cache that stores metadata in a space-efficient way and stores popular data blocks by using compaction-aware layouts. Moreover, RocksMash uses an extended write-ahead log for fast parallel data recovery. We implemented RocksMash by embedding these designs into RocksDB. The evaluation results show that RocksMash improves the performance by up to 1.7 \\( \\times \\) compared to the state-of-the-art schemes and delivers high reliability, cost-effectiveness, and fast recovery.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4226504908",
    "type": "article"
  },
  {
    "title": "Energy-efficient In-Memory Address Calculation",
    "doi": "https://doi.org/10.1145/3546071",
    "publication_date": "2022-06-30",
    "publication_year": 2022,
    "authors": "Amirreza Yousefzadeh; Jan Stuijt; Martijn Hijdra; Hsiao-Hsuan Liu; Anteneh Gebregiorgis; Abhairaj Singh; Said Hamdioui; Francky Catthoor",
    "corresponding_authors": "",
    "abstract": "Computation-in-Memory (CIM) is an emerging computing paradigm to address memory bottleneck challenges in computer architecture. A CIM unit cannot fully replace a general-purpose processor. Still, it significantly reduces the amount of data transfer between a traditional memory unit and the processor by enriching the transferred information. Data transactions between processor and memory consist of memory access addresses and values. While the main focus in the field of in-memory computing is to apply computations on the content of the memory (values), the importance of CPU-CIM address transactions and calculations for generating the sequence of access addresses for data-dominated applications is generally overlooked. However, the amount of information transactions used for “address” can easily be even more than half of the total transferred bits in many applications. In this article, we propose a circuit to perform the in-memory Address Calculation Accelerator. Our simulation results showed that calculating address sequences inside the memory (instead of the CPU) can significantly reduce the CPU-CIM address transactions and therefore contribute to considerable energy saving, latency, and bus traffic. For a chosen application of guided image filtering, in-memory address calculation results in almost two orders of magnitude reduction in address transactions over the memory bus.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4283759689",
    "type": "article"
  },
  {
    "title": "Removing communications in clustered microarchitectures through instruction replication",
    "doi": "https://doi.org/10.1145/1011528.1011529",
    "publication_date": "2004-06-01",
    "publication_year": 2004,
    "authors": "Alex Aletà; Josep Codina; Antonio González; David Kaeli",
    "corresponding_authors": "",
    "abstract": "The need to communicate values between clusters can result in a significant performance loss for clustered microarchitectures. In this work, we describe an optimization technique that removes communications by selectively replicating an appropriate set of instructions. Instruction replication is done carefully because it might degrade performance due to the increased contention it can place on processor resources. The proposed scheme is built on top of a previously proposed state-of-the-art modulo-scheduling algorithm. Though this algorithm has been proved to be very effective at reducing communications, results show that the number of communications can be further decreased by around one-third through replication, which results in a significant speedup. IPC is increased by 25% on average for a four-cluster microarchitecture and by as much as 70% for selected programs. We also show that replicating appropriate sets of instructions is more effective than doubling the intercluster connection network bandwidth.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W1990840846",
    "type": "article"
  },
  {
    "title": "Analysis of cache-coherence bottlenecks with hybrid hardware/software techniques",
    "doi": "https://doi.org/10.1145/1187976.1187978",
    "publication_date": "2006-12-01",
    "publication_year": 2006,
    "authors": "Jaydeep Marathe; Frank Mueller; Bronis R. de Supinski",
    "corresponding_authors": "",
    "abstract": "Application performance on high-performance shared-memory systems is often limited by sharing patterns resulting in cache-coherence bottlenecks. Current approaches to identify coherence bottlenecks incur considerable run-time overhead and do not scale. We present two novel hardware-assisted coherence-analysis techniques that reduce trace sizes by two orders of magnitude over full traces. First, hardware performance monitoring is combined with capturing stores in software to provide a lossy-trace mechanism, which is an order of magnitude faster than software-instrumentation-based full-tracing and retains accuracy. Second, selected long-latency loads are instrumented via binary rewriting, which provides even higher accuracy and control over tracing, but requires additional overhead.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2074228071",
    "type": "article"
  },
  {
    "title": "Profile-based adaptation for cache decay",
    "doi": "https://doi.org/10.1145/1022969.1022972",
    "publication_date": "2004-09-01",
    "publication_year": 2004,
    "authors": "Karthik Sankaranarayanan; Kevin Skadron",
    "corresponding_authors": "",
    "abstract": "\"Cache decay\" is a set of leakage-reduction mechanisms that put cache lines that have not been accessed for a specific duration into a low-leakage standby mode. This duration is called the decay interval, and its optimal value varies across applications. This paper describes an adaptation technique that analytically finds the optimal decay interval through profiling, and shows that the most important variables required for finding the optimal decay interval can be estimated with a reasonable degree of accuracy using profiling. This work explicitly trades off the leakage power saved in putting both the \"live\" and \"dead\" lines into standby mode, against its performance and energy costs. It achieves energy savings close to what can be obtained with an omniscient choice of per-benchmark optimal decay interval.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2085904822",
    "type": "article"
  },
  {
    "title": "Service level agreement for multithreaded processors",
    "doi": "https://doi.org/10.1145/1543753.1543755",
    "publication_date": "2009-06-01",
    "publication_year": 2009,
    "authors": "Ron Gabor; Avi Mendelson; Shlomo Weiss",
    "corresponding_authors": "",
    "abstract": "Multithreading is widely used to increase processor throughput. As the number of shared resources increase, managing them while guaranteeing predicted performance becomes a major problem. Attempts have been made in previous work to ease this via different fairness mechanisms. In this article, we present a new approach to control the resource allocation and sharing via a service level agreement (SLA)-based mechanism; that is, via an agreement in which multithreaded processors guarantee a minimal level of service to the running threads. We introduce a new metric, C SLA , for conformance to SLA in multithreaded processors and show that controlling resources using with SLA allows for higher gains than are achievable by previously suggested fairness techniques. It also permits improving one metric (e.g., power) while maintaining SLA in another (e.g., performance). We compare SLA enforcement to schemes based on other fairness metrics, which are mostly targeted at equalizing execution parameters. We show that using SLA rather than fairness based algorithms provides a range of acceptable execution points from which we can select the point that best fits our optimization target, such as maximizing the weighted speedup (sum of the speedups of the individual threads) or reducing power. We demonstrate the effectiveness of the new SLA approach using switch-on-event (coarse-grained) multithreading. Our weighted speedup improvement scheme successfully enforces SLA while improving the weighted speedup by an average of 10% for unbalanced threads. This result is significant when compared with performance losses that may be incurred by fairness enforcement methods. When optimizing for power reduction in unbalanced threads SLA enforcement reduces the power by an average of 15%. SLA may be complemented by other power reduction methods to achieve further power savings and maintain the same service level for the threads. We also demonstrate differentiated SLA, where weighted speedup is maximized while each thread may have a different throughput constraint.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2027783656",
    "type": "article"
  },
  {
    "title": "Dynamically dispatching speculative threads to improve sequential execution",
    "doi": "https://doi.org/10.1145/2355585.2355586",
    "publication_date": "2012-09-01",
    "publication_year": 2012,
    "authors": "Yangchun Luo; Antonia Zhai",
    "corresponding_authors": "",
    "abstract": "Efficiently utilizing multicore processors to improve their performance potentials demands extracting thread-level parallelism from the applications. Various novel and sophisticated execution models have been proposed to extract thread-level parallelism from sequential programs. One such execution model, Thread-Level Speculation (TLS), allows potentially dependent threads to execute speculatively in parallel. However, TLS execution is inherently unpredictable, and consequently incorrect speculation could degrade performance for the multicore systems. Existing approaches have focused on using the compilers to select sequential program regions to apply TLS. Our research shows that even the state-of-the-art compiler makes suboptimal decisions, due to the unpredictability of TLS execution. Thus, we propose to dynamically optimize TLS performance. This article describes the design, implementation, and evaluation of a runtime thread dispatching mechanism that adjusts the behaviors of speculative threads based on their efficiency. In the proposed system, speculative threads are monitored by hardware-based performance counters and their performance impact is evaluated with a novel methodology that takes into account various unique TLS characteristics. Thread dispatching policies are devised to adjust the behaviors of speculative threads accordingly. With the help of the runtime evaluation, where and how to create speculative threads is better determined. Evaluated with all the SPEC CPU2000 benchmark programs written in C, the dynamic dispatching system outperforms the state-of-the-art compiler-based thread management techniques by 9.4% on average. Comparing to sequential execution, we achieve 1.37X performance improvement on a four-core CMP-based system.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W1979593478",
    "type": "article"
  },
  {
    "title": "Compiler techniques to improve dynamic branch prediction for indirect jump and call instructions",
    "doi": "https://doi.org/10.1145/2086696.2086703",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Jason McCandless; David Gregg",
    "corresponding_authors": "",
    "abstract": "Indirect jump instructions are used to implement multiway branch statements and virtual function calls in object-oriented languages. Branch behavior can have significant impact on program performance, but fortunately hardware predictors can alleviate much of the risk. Modern processors include indirect branch predictors which use part of the target address to update a global history. We present a code generation technique to maximize the branch history information available to the predictor. We implement our optimization as an assembly language transformation, and evaluate it for SPEC benchmarks and interpreters using simulated and real hardware, showing indirect branch misprediction decreases.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W1989891739",
    "type": "article"
  },
  {
    "title": "Deterministic Replay Using Global Clock",
    "doi": "https://doi.org/10.1145/2445572.2445573",
    "publication_date": "2013-04-01",
    "publication_year": 2013,
    "authors": "Yunji Chen; Tianshi Chen; Ling Li; Ruiyang Wu; Daofu Liu; Weiwu Hu",
    "corresponding_authors": "",
    "abstract": "Debugging parallel programs is a well-known difficult problem. A promising method to facilitate debugging parallel programs is using hardware support to achieve deterministic replay on a Chip Multi-Processor (CMP). As a Design-For-Debug (DFD) feature, a practical hardware-assisted deterministic replay scheme should have low design and verification costs, as well as a small log size. To achieve these goals, we propose a novel and succinct hardware-assisted deterministic replay scheme named LReplay. The key innovation of LReplay is that instead of recording the logical time orders between instructions or instruction blocks as previous investigations, LReplay is built upon recording the pending period information infused by the global clock. By the recorded pending period information, about 99% execution orders are inferrable, implying that LReplay only needs to record directly the residual 1% noninferrable execution orders in production run. The 1% noninferrable orders can be addressed by a simple yet cost-effective direction prediction technique, which further reduces the log size of LReplay. Benefiting from the preceding innovations, the overall log size of LReplay over SPLASH-2 benchmarks is about 0.17B/K-Inst (byte per k-instruction) for the sequential consistency, and 0.57B/K-Inst for the Godson-3 consistency. Such log sizes are smaller in an order of magnitude than previous deterministic replay schemes incurring no performance loss. Furthermore, LReplay only consumes about 0.5% area of the Godson-3 CMP, since it requires only trivial modifications to existing components of Godson-3. The features of LReplay demonstrate the potential of integrating hardware support for deterministic replay into future industrial processors.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W1993766354",
    "type": "article"
  },
  {
    "title": "Improved bitwidth-aware variable packing",
    "doi": "https://doi.org/10.1145/2509420.2509427",
    "publication_date": "2013-09-16",
    "publication_year": 2013,
    "authors": "V. Krishna Nandivada; Rajkishore Barik",
    "corresponding_authors": "",
    "abstract": "Bitwidth-aware register allocation has caught the attention of researchers aiming to effectively reduce the number of variables spilled into memory. For general-purpose processors, this improves the execution time performance and reduces runtime memory requirements (which in turn helps in the compilation of programs targeted to systems with constrained memory). Additionally, bitwidth-aware register allocation has been effective in reducing power consumption in embedded processors. One of the key components of bitwidth-aware register allocation is the variable packing algorithm that packs multiple narrow-width variables into one physical register. Tallam and Gupta [2003] have proved that optimal variable packing is an NP-complete problem for arbitrary-width variables and have proposed an approximate solution. In this article, we analyze the complexity of the variable packing problem and present three enhancements that improve the overall packing of variables. In particular, the improvements we describe are: (a) Width Static Single Assignment (W-SSA) form representation that splits the live range of a variable into several fixed-width live ranges (W-SSA) variables); (b) PoTR Representation - use of powers-of-two representation for bitwidth information for W-SSA variables. Our empirical results have shown that the associated bit wastage resulting from the overapproximation of the widths of variables to the nearest next power of two is a small fraction compared to the total number of bits in use (≈13%). The main advantage of this representation is that it leads to optimal variable packing in polynomial time; (c) Combined Packing and Coalescing - we discuss the importance of coalescing (combining variables whose live ranges do not interfere) in the context of variable packing and present an iterative algorithm to perform coalescing and packing of W-SSA variables represented in PoTR. Our experimental results show up to 76.00% decrease in the number of variables compared to the number of variables in the input program in Single Static Assignment (SSA) form. This reduction in the number of variables led to a significant reduction in dynamic spilling, packing, and unpacking instructions.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2041582885",
    "type": "article"
  },
  {
    "title": "Integer affine transformations of parametric ℤ-polytopes and applications to loop nest optimization",
    "doi": "https://doi.org/10.1145/2207222.2207224",
    "publication_date": "2012-06-01",
    "publication_year": 2012,
    "authors": "Rachid Seghir; Vincent Loechner; Benoît Meister",
    "corresponding_authors": "",
    "abstract": "The polyhedral model is a well-known compiler optimization framework for the analysis and transformation of affine loop nests. We present a new method to solve a difficult geometric operation that is raised by this model: the integer affine transformation of parametric ℤ-polytopes. The result of such a transformation is given by a worst-case exponential union of ℤ-polytopes. We also propose a polynomial algorithm (for fixed dimension), to count points in arbitrary unions of a fixed number of parametric ℤ-polytopes. We implemented these algorithms and compared them to other existing algorithms, for a set of applications to loop nest analysis and optimization.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2059347275",
    "type": "article"
  },
  {
    "title": "Comparability Graph Coloring for Optimizing Utilization of Software-Managed Stream Register Files for Stream Processors",
    "doi": "https://doi.org/10.1145/2133382.2133387",
    "publication_date": "2012-03-01",
    "publication_year": 2012,
    "authors": "Xuejun Yang; Li Wang; Jingling Xue; Qingbo Wu",
    "corresponding_authors": "",
    "abstract": "The stream processors represent a promising alternative to traditional cache-based general-purpose processors in achieving high performance in stream applications (media and some scientific applications). In a stream programming model for stream processors, an application is decomposed into a sequence of kernels operating on streams of data. During the execution of a kernel on a stream processor, all streams accessed must be communicated through a nonbypassing software-managed on-chip memory, the SRF (Stream Register File). Optimizing utilization of the scarce on-chip memory is crucial for good performance. The key insight is that the interference graphs (IGs) formed by the streams in stream applications tend to be comparability graphs or decomposable into a set of comparability graphs. We present a compiler algorithm for finding optimal or near-optimal colorings, that is, SRF allocations in stream IGs, by computing a maximum spanning forest of the sub-IG formed by long live ranges, if necessary. Our experimental results validate the optimality and near-optimality of our algorithm by comparing it with an ILP solver, and show that our algorithm yields improved SRF utilization over the First-Fit bin-packing algorithm, the best in the literature.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2059659006",
    "type": "article"
  },
  {
    "title": "Utilizing RF-I and intelligent scheduling for better throughput/watt in a mobile GPU memory system",
    "doi": "https://doi.org/10.1145/2086696.2086730",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Kanit Therdsteerasukdi; Gyung-Su Byun; Jason Cong; Mau-Chung Frank Chang; Glenn Reinman",
    "corresponding_authors": "",
    "abstract": "Smartphones and tablets are becoming more and more powerful, replacing desktops and laptops as the users' main computing system. As these systems support higher and higher resolutions with more complex 3D graphics, a high-throughput and low-power memory system is essential for the mobile GPU. In this article, we propose to improve throughput/watt in a mobile GPU memory system by using intelligent scheduling to reduce power and multi-band radio frequency interconnect (MRF-I) to offset any throughput degradation caused by our intelligent scheduling. Overall, we are able to improve throughput 17% up to 66% while increasing throughput per watt by an average of 18% up to 26%.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2076370206",
    "type": "article"
  },
  {
    "title": "Significantly reducing MPI intercommunication latency and power overhead in both embedded and HPC systems",
    "doi": "https://doi.org/10.1145/2400682.2400710",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Pavlos M. Mattheakis; Ioannis Papaefstathiou",
    "corresponding_authors": "",
    "abstract": "Highly parallel systems are becoming mainstream in a wide range of sectors ranging from their traditional stronghold high-performance computing, to data centers and even embedded systems. However, despite the quantum leaps of improvements in cost and performance of individual components over the last decade (e.g., processor speeds, memory/interconnection bandwidth, etc.), system manufacturers are still struggling to deliver low-latency, highly scalable solutions. One of the main reasons is that the intercommunication latency grows significantly with the number of processor nodes. This article presents a novel way to reduce this intercommunication delay by implementing, in custom hardware, certain communication tasks. In particular, the proposed novel device implements the two most widely used procedures of the most popular communication protocol in parallel systems the Message Passing Interface (MPI). Our novel approach has initially been simulated within a pioneering parallel systems simulation framework and then synthesized directly from a high-level description language (i.e., SystemC) using a state-of-the-art synthesis tool. To the best of our knowledge, this is the first article presenting the complete hardware implementation of such a system. The proposed novel approach triggers a speedup from one to four orders of magnitude when compared with conventional software-based solutions and from one to three orders of magnitude when compared with a sophisticated software-based approach. Moreover, the performance of our system is from one to two orders of magnitude higher than the simulated performance of a similar but, relatively simpler hardware architecture; at the same time the power consumption of our device is about two orders of magnitude lower than that of a low-power CPU when executing the exact same intercommunication tasks.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2079837974",
    "type": "article"
  },
  {
    "title": "A transpose-free in-place SIMD optimized FFT",
    "doi": "https://doi.org/10.1145/2355585.2355596",
    "publication_date": "2012-09-01",
    "publication_year": 2012,
    "authors": "James R. Geraci; Sharon M. Sacco",
    "corresponding_authors": "",
    "abstract": "A transpose-free in-place SIMD optimized algorithm for the computation of large FFTs is introduced and implemented on the Cell Broadband Engine. Six different FFT implementations of the algorithm using six different data movement methods are described. Their relative performance is compared for input sizes from 217 to 221 complex floating point samples. Large differences in performance are observed among even theoretically equivalent data movement patterns. All six implementations compare favorably with FFTW and other previous FFT implementations.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2084038658",
    "type": "article"
  },
  {
    "title": "A circuit-architecture co-optimization framework for exploring nonvolatile memory hierarchies",
    "doi": "https://doi.org/10.1145/2541228.2541230",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Xiangyu Dong; Norman P. Jouppi; Yuan Xie",
    "corresponding_authors": "",
    "abstract": "Many new memory technologies are available for building future energy-efficient memory hierarchies. It is necessary to have a framework that can quickly find the optimal memory technology at each hierarchy level. In this work, we first build a circuit-architecture joint design space exploration framework by combining RC circuit analysis and Artificial Neural Network (ANN)-based performance modeling. Then, we use this framework to evaluate some emerging nonvolatile memory hierarchies. We demonstrate that a Resistive RAM (ReRAM)-based cache hierarchy on an 8-core Chip-Multiprocessor (CMP) system can achieve a 24% Energy Delay Product (EDP) improvement and a 36% Energy Delay Area Product (EDAP) improvement compared to a conventional hierarchy with SRAM on-chip caches and DRAM main memory.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2116703387",
    "type": "article"
  },
  {
    "title": "TornadoNoC",
    "doi": "https://doi.org/10.1145/2541228.2555312",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Jung­hee Lee; Chrysostomos Nicopoulos; Hyung Gyu Lee; Jong Man Kim",
    "corresponding_authors": "",
    "abstract": "The rapid emergence of Chip Multi-Processors (CMP) as the de facto microprocessor archetype has highlighted the importance of scalable and efficient on-chip networks. Packet-based Networks-on-Chip (NoC) are gradually cementing themselves as the medium of choice for the multi-/many-core systems of the near future, due to their innate scalability. However, the prominence of the debilitating power wall requires the NoC to also be as energy efficient as possible. To achieve these two antipodal requirements—scalability and energy efficiency—we propose TornadoNoC, an interconnect architecture that employs a novel flow control mechanism. To prevent livelocks and deadlocks, a sequence numbering scheme and a dynamic ring inflation technique are proposed, and their correctness formally proven. The primary objective of TornadoNoC is to achieve substantial gains in (a) scalability to many-core systems and (b) the area/power footprint, as compared to current state-of-the-art router implementations. The new router is demonstrated to provide better scalability to hundreds of cores than an ideal single-cycle wormhole implementation and other scalability-enhanced low-cost routers. Extensive simulations using both synthetic traffic patterns and real applications running in a full-system simulator corroborate the efficacy of the proposed design. Finally, hardware synthesis analysis using commercial 65nm standard-cell libraries indicates that the area and power budgets of the new router are reduced by up to 53% and 58%, respectively, as compared to existing state-of-the-art low-cost routers.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2163975534",
    "type": "article"
  },
  {
    "title": "Stream arbitration",
    "doi": "https://doi.org/10.1145/2400682.2400719",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Chunhua Xiao; Mau-Chung Frank Chang; Jason Cong; Michael Gill; Zhangqin Huang; Chunyue Liu; Glenn Reinman; Hao Wu",
    "corresponding_authors": "",
    "abstract": "Alternative interconnects are attractive for scaling on-chip communication bandwidth in a power-efficient manner. However, efficient utilization of the bandwidth provided by these emerging interconnects still remains an open problem due to the spatial and temporal communication heterogeneity. In this article, a Stream Arbitration scheme is proposed, where at runtime any source can compete for any communication channel of the interconnect to talk to any destination. We apply stream arbitration to radio frequency interconnect (RF-I). Experimental results show that compared to the representative token arbitration scheme, stream arbitration can provide an average 20% performance improvement and 12% power reduction.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2170170114",
    "type": "article"
  },
  {
    "title": "DiagSim",
    "doi": "https://doi.org/10.1145/3177959",
    "publication_date": "2018-03-22",
    "publication_year": 2018,
    "authors": "Jae-Eon Jo; Gyu-hyeon Lee; Hanhwi Jang; Jaewon Lee; Mohammadamin Ajdari; Jangwoo Kim",
    "corresponding_authors": "",
    "abstract": "Simulators are the most popular and useful tool to study computer architecture and examine new ideas. However, modern simulators have become prohibitively complex (e.g., 200K+ lines of code) to fully understand and utilize. Users therefore end up analyzing and modifying only the modules of interest (e.g., branch predictor, register file) when performing simulations. Unfortunately, hidden details and inter-module interactions of simulators create discrepancies between the expected and actual module behaviors. Consequently, the effect of modifying the target module may be amplified or masked and the users get inaccurate insights from expensive simulations. In this article, we propose DiagSim, an efficient and systematic method to diagnose simulators. It ensures the target modules behave as expected to perform simulation in a healthy (i.e., accurate and correct) way. DiagSim is efficient in that it quickly pinpoints the modules showing discrepancies and guides the users to inspect the behavior without investigating the whole simulator. DiagSim is systematic in that it hierarchically tests the modules to guarantee the integrity of individual diagnosis and always provide reliable results. We construct DiagSim based on generic category-based diagnosis ideas to encourage easy expansion of the diagnosis. We diagnose three popular open source simulators and discover hidden details including implicitly reserved resources, un-documented latency factors, and hard-coded module parameter values. We observe that these factors have large performance impacts (up to 156%) and illustrate that our diagnosis can correctly detect and eliminate them.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2791289333",
    "type": "article"
  },
  {
    "title": "ReveNAND",
    "doi": "https://doi.org/10.1145/3184744",
    "publication_date": "2018-05-01",
    "publication_year": 2018,
    "authors": "Mustafa Shihab; Jie Zhang; Myoungsoo Jung; Mahmut Kandemir",
    "corresponding_authors": "",
    "abstract": "The paradigm shift from planar (two dimensional (2D)) to vertical (three-dimensional (3D)) models has placed the NAND flash technology on the verge of a design evolution that can handle the demands of next-generation storage applications. However, it also introduces challenges that may obstruct the realization of such 3D NAND flash. Specifically, we observed that the fast threshold drift (fast-drift) in a charge-trap flash-based 3D NAND cell can make it lose a critical fraction of the stored charge relatively soon after programming and generate errors. In this work, we first present an elastic read reference ( V Ref ) scheme (ERR) for reducing such errors in ReveNAND—our fast-drift aware 3D NAND design. To address the inherent limitation of the adaptive V Ref , we introduce a new intra-block page organization (hitch-hike) that can enable stronger error correction for the error-prone pages. In addition, we propose a novel reinforcement-learning-based smart data refill scheme (iRefill) to counter the impact of fast-drift with minimum performance and hardware overhead. Finally, we present the first analytic model to characterize fast-drift and evaluate its system-level impact. Our results show that, compared to conventional 3D NAND design, our ReveNAND can reduce fast-drift errors by 87%, on average, and can lower the ECC latency and energy overheads by 13× and 10×, respectively.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2800250775",
    "type": "article"
  },
  {
    "title": "Polyhedral Search Space Exploration in the ExaStencils Code Generator",
    "doi": "https://doi.org/10.1145/3274653",
    "publication_date": "2018-10-10",
    "publication_year": 2018,
    "authors": "Stefan Kronawitter; Christian Lengauer",
    "corresponding_authors": "",
    "abstract": "Performance optimization of stencil codes requires data locality improvements. The polyhedron model for loop transformation is well suited for such optimizations with established techniques, such as the PLuTo algorithm and diamond tiling. However, in the domain of our project ExaStencils, stencil codes, it fails to yield optimal results. As an alternative, we propose a new, optimized, multi-dimensional polyhedral search space exploration and demonstrate its effectiveness: we obtain better results than existing approaches in several cases. We also propose how to specialize the search for the domain of stencil codes, which dramatically reduces the exploration effort without significantly impairing performance.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2899298108",
    "type": "article"
  },
  {
    "title": "An Autotuning Framework for Scalable Execution of Tiled Code via Iterative Polyhedral Compilation",
    "doi": "https://doi.org/10.1145/3293449",
    "publication_date": "2018-12-31",
    "publication_year": 2018,
    "authors": "Yukinori Sato; Tomoya Yuki; Toshio Endo",
    "corresponding_authors": "",
    "abstract": "On modern many-core CPUs, performance tuning against complex memory subsystems and scalability for parallelism is mandatory to achieve their potential. In this article, we focus on loop tiling, which plays an important role in performance tuning, and develop a novel framework that analytically models the load balance and empirically autotunes unpredictable cache behaviors through iterative polyhedral compilation using LLVM/Polly. From an evaluation on many-core CPUs, we demonstrate that our autotuner achieves a performance superior to those that use conventional static approaches and well-known autotuning heuristics. Moreover, our autotuner achieves almost the same performance as a brute-force search-based approach.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2908959276",
    "type": "article"
  },
  {
    "title": "Nested MIMD-SIMD Parallelization for Heterogeneous Microprocessors",
    "doi": "https://doi.org/10.1145/3368304",
    "publication_date": "2019-12-17",
    "publication_year": 2019,
    "authors": "Daniel Gerzhoy; Xiaowu Sun; Michael Zuzak; Donald Yeung",
    "corresponding_authors": "",
    "abstract": "Heterogeneous microprocessors integrate a CPU and GPU on the same chip, providing fast CPU-GPU communication and enabling cores to compute on data “in place.” This permits exploiting a finer granularity of parallelism on the integrated GPUs, and enables the use of GPUs for accelerating more complex and irregular codes. One challenge, however, is exposing enough parallelism such that both the CPU and GPU are effectively utilized to achieve maximum gain. In this article, we propose exploiting nested parallelism for integrated CPU-GPU chips. We look for loop structures in which one or more regular data parallel loops are nested within a parallel outer loop that can contain irregular code (e.g., with control divergence). By scheduling the outer loop on multiple CPU cores, multiple dynamic instances of the inner regular loop(s) can be scheduled on the GPU cores. This boosts GPU utilization and parallelizes the outer loop. We find that such nested MIMD-SIMD parallelization provides greater levels of parallelism for integrated CPU-GPU chips, and additionally there is ample opportunity to perform such parallelization in OpenMP programs. Our results show nested MIMD-SIMD parallelization provides a 16.1x and 8.67x speedup over sequential execution on a simulator and a physical machine, respectively. Our technique beats CPU-only parallelization by 4.13x and 2.40x, respectively, and GPU-only parallelization by 2.74x and 2.26x, respectively. Compared to the next-best scheme (either CPU- or GPU-only parallelization) per benchmark, our approach provides a 1.46x and 1.23x speedup for the simulator and physical machine, respectively.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2996123223",
    "type": "article"
  },
  {
    "title": "SIMT-X",
    "doi": "https://doi.org/10.1145/3392032",
    "publication_date": "2020-05-29",
    "publication_year": 2020,
    "authors": "Anita Tino; Caroline Collange; André Seznec",
    "corresponding_authors": "",
    "abstract": "This work introduces Single Instruction Multi-Thread Express (SIMT-X), a general-purpose Central Processing Unit (CPU) microarchitecture that enables Graphics Processing Units (GPUs)-style SIMT execution across multiple threads of the same program for high throughput, while retaining the latency benefits of out-of-order execution, and the programming convenience of homogeneous multi-thread processors. SIMT-X leverages the existing Single Instruction Multiple Data (SIMD) back-end to provide CPU/GPU-like processing on a single core with minimal overhead. We demonstrate that although SIMT-X invokes a restricted form of Out-of-Order (OoO), the microarchitecture successfully captures a majority of the benefits of aggressive OoO execution using at most two concurrent register mappings per architectural register, while addressing issues of partial dependencies and supporting a general-purpose Instruction Set Architecture (ISA).",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3017116334",
    "type": "article"
  },
  {
    "title": "Building and Optimizing MRAM-Based Commodity Memories",
    "doi": "https://doi.org/10.1145/2667105",
    "publication_date": "2014-12-08",
    "publication_year": 2014,
    "authors": "Jue Wang; Xiangyu Dong; Yuan Xie",
    "corresponding_authors": "",
    "abstract": "Emerging non-volatile memory technologies such as MRAM are promising design solutions for energy-efficient memory architecture, especially for mobile systems. However, building commodity MRAM by reusing DRAM designs is not straightforward. The existing memory interfaces are incompatible with MRAM small page size, and they fail to leverage MRAM unique properties, causing unnecessary performance and energy overhead. In this article, we propose four techniques to enable and optimize an LPDDRx-compatible MRAM solution: ComboAS to solve the pin incompatibility; DynLat to avoid unnecessary access latencies; and EarlyPA and BufW to further improve performance by exploiting the MRAM unique features of non-destructive read and independent write path. Combining all these techniques together, we boost the MRAM performance by 17% and provide a DRAM-compatible MRAM solution consuming 21% less energy.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W1984195439",
    "type": "article"
  },
  {
    "title": "Multiprogram Throughput Metrics",
    "doi": "https://doi.org/10.1145/2663346",
    "publication_date": "2014-10-27",
    "publication_year": 2014,
    "authors": "Stijn Eyerman; Pierre Michaud; Wouter Rogiest",
    "corresponding_authors": "",
    "abstract": "Running multiple programs on a processor aims at increasing the throughput of that processor. However, defining meaningful throughput metrics in a simulation environment is not as straightforward as reporting execution time. This has led to an ongoing debate on what forms a meaningful throughput metric for multiprogram workloads. We present a method to construct throughput metrics in a systematic way: we start by expressing assumptions on job size, job distribution, scheduling, and so forth that together define a theoretical throughput experiment. The throughput metric is then the average throughput of this experiment. Different assumptions lead to different metrics, so one should be aware of these assumptions when making conclusions based on results using a specific metric. Throughput metrics should always be defined from explicit assumptions, because this leads to a better understanding of the implications and limits of the results obtained with that metric. We elaborate multiple metrics based on different assumptions. In particular, we identify the assumptions that lead to the commonly used weighted speedup and harmonic mean of speedups. Our study clarifies that they are actual throughput metrics, which was recently questioned. We also propose some new throughput metrics, which cannot always be expressed as a closed formula. We use real experimental data to characterize metrics and show how they relate to each other.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2070744914",
    "type": "article"
  },
  {
    "title": "Trace transitioning and exception handling in a trace-based JIT compiler for java",
    "doi": "https://doi.org/10.1145/2579673",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "Christian Häubl; Christian Wimmer; Hanspeter Mössenböck",
    "corresponding_authors": "",
    "abstract": "Trace-based Just-In-Time (JIT) compilation generates machine code for frequently executed paths (so-called traces) instead of whole methods. While this has several advantages, it complicates invocation of compiled traces as well as exception handling, so that previous trace-based compilers limited the way in which traces could be invoked. We present a significantly enhanced trace-based compiler where arbitrary transitions between interpreted and compiled traces are possible. For that, we introduce suitable trace calling conventions and extend exception handling to work both within traces and across trace boundaries. Furthermore, we use the recorded trace information for optimizations and combine the tracing ideas with ideas from partial-method compilation to avoid code bloat. An extensive evaluation with the benchmark suites DaCapo 9.12 Bach and SPECjvm2008 shows that our trace-based compiler achieves up to 59% higher peak performance than the method-based Java HotSpot client compiler. On a few benchmarks, our fairly simple trace-based compiler shows a higher peak performance than the Java HotSpot server compiler, which is one of today's best optimizing JIT compilers for Java.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2080508069",
    "type": "article"
  },
  {
    "title": "Four Metrics to Evaluate Heterogeneous Multicores",
    "doi": "https://doi.org/10.1145/2829950",
    "publication_date": "2015-11-16",
    "publication_year": 2015,
    "authors": "Erik Tomusk; Christophe Dubach; Michael O’Boyle",
    "corresponding_authors": "",
    "abstract": "Semiconductor device scaling has made single-ISA heterogeneous processors a reality. Heterogeneous processors contain a number of different CPU cores that all implement the same Instruction Set Architecture (ISA). This enables greater flexibility and specialization, as runtime constraints and workload characteristics can influence which core a given workload is run on. A major roadblock to the further development of heterogeneous processors is the lack of appropriate evaluation metrics. Existing metrics can be used to evaluate individual cores, but to evaluate a heterogeneous processor, the cores must be considered as a collective. Without appropriate metrics, it is impossible to establish design goals for processors, and it is difficult to accurately compare two different heterogeneous processors. We present four new metrics to evaluate user-oriented aspects of sets of heterogeneous cores: localized nonuniformity , gap overhead , set overhead , and generality . The metrics consider sets rather than individual cores. We use examples to demonstrate each metric, and show that the metrics can be used to quantify intuitions about heterogeneous cores.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2174864330",
    "type": "article"
  },
  {
    "title": "Tumbler",
    "doi": "https://doi.org/10.1145/2827698",
    "publication_date": "2015-11-16",
    "publication_year": 2015,
    "authors": "Kishore Kumar Pusukuri; Rajiv Gupta; Laxmi N. Bhuyan",
    "corresponding_authors": "",
    "abstract": "Schedulers used by modern OSs (e.g., Oracle Solaris 11™ and GNU/Linux) balance load by balancing the number of threads in run queues of different cores. While this approach is effective for a single CPU multicore system, we show that it can lead to a significant load imbalance across CPUs of a multi-CPU multicore system. Because different threads of a multithreaded application often exhibit different levels of CPU utilization, load cannot be measured in terms of the number of threads alone. We propose Tumbler that migrates the threads of a multithreaded program across multiple CPUs to balance the load across the CPUs. While Tumbler distributes the threads equally across the CPUs, its assignment of threads to CPUs is aimed at minimizing the variation in utilization of different CPUs to achieve load balance. We evaluated Tumbler using a wide variety of 35 multithreaded applications, and our experimental results show that Tumbler outperforms both Oracle Solaris 11™ and GNU/Linux.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2181032569",
    "type": "article"
  },
  {
    "title": "Locality-Aware Work Stealing Based on Online Profiling and Auto-Tuning for Multisocket Multicore Architectures",
    "doi": "https://doi.org/10.1145/2766450",
    "publication_date": "2015-07-08",
    "publication_year": 2015,
    "authors": "Quan Chen; Minyi Guo",
    "corresponding_authors": "",
    "abstract": "Modern mainstream powerful computers adopt multisocket multicore CPU architecture and NUMA-based memory architecture. While traditional work-stealing schedulers are designed for single-socket architectures, they incur severe shared cache misses and remote memory accesses in these computers. To solve the problem, we propose a locality-aware work-stealing (LAWS) scheduler, which better utilizes both the shared cache and the memory system. In LAWS, a load-balanced task allocator is used to evenly split and store the dataset of a program to all the memory nodes and allocate a task to the socket where the local memory node stores its data for reducing remote memory accesses. Then, an adaptive DAG packer adopts an auto-tuning approach to optimally pack an execution DAG into cache-friendly subtrees. After cache-friendly subtrees are created, every socket executes cache-friendly subtrees sequentially for optimizing shared cache usage. Meanwhile, a triple-level work-stealing scheduler is applied to schedule the subtrees and the tasks in each subtree. Through theoretical analysis, we show that LAWS has comparable time and space bounds compared with traditional work-stealing schedulers. Experimental results show that LAWS can improve the performance of memory-bound programs up to 54.2% on AMD-based experimental platforms and up to 48.6% on Intel-based experimental platforms compared with traditional work-stealing schedulers.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2202621926",
    "type": "article"
  },
  {
    "title": "Rethinking Memory Permissions for Protection Against Cross-Layer Attacks",
    "doi": "https://doi.org/10.1145/2842621",
    "publication_date": "2015-12-08",
    "publication_year": 2015,
    "authors": "Jesse Elwell; Ryan Riley; Nael Abu‐Ghazaleh; Dmitry Ponomarev; Iliano Cervesato",
    "corresponding_authors": "",
    "abstract": "The inclusive permissions structure (e.g., the Intel ring model) of modern commodity CPUs provides privileged system software layers with arbitrary permissions to access and modify client processes, allowing them to manage these clients and the system resources efficiently. Unfortunately, these inclusive permissions allow a compromised high-privileged software layer to perform arbitrary malicious activities. In this article, our goal is to prevent attacks that cross system layers while maintaining the abilities of system software to manage the system and allocate resources. In particular, we present a hardware-supported page permission framework for physical pages that is based on the concept of noninclusive sets of memory permissions for different layers of system software (such as hypervisors, operating systems, and user-level applications). Instead of viewing privilege levels as an ordered hierarchy with each successive level being more privileged, we view them as distinct levels each with its own set of permissions. In order to enable system software to manage client processes, we define a set of legal permission transitions that support resource allocation but preserve security. We show that the model prevents a range of recent attacks. We also show that it can be implemented with negligible performance overhead (both at load time and at runtime), low hardware complexity, and minimal changes to the commodity OS and hypervisor code.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2203543290",
    "type": "article"
  },
  {
    "title": "Scalable Energy Efficiency with Resilience for High Performance Computing Systems",
    "doi": "https://doi.org/10.1145/2822893",
    "publication_date": "2015-11-16",
    "publication_year": 2015,
    "authors": "Li Tan; Zizhong Chen; Shuaiwen Leon Song",
    "corresponding_authors": "",
    "abstract": "Ever-growing performance of supercomputers nowadays brings demanding requirements of energy efficiency and resilience, due to rapidly expanding size and duration in use of the large-scale computing systems. Many application/architecture-dependent parameters that determine energy efficiency and resilience individually have causal effects with each other, which directly affect the trade-offs among performance, energy efficiency and resilience at scale. To enable high-efficiency management for large-scale High-Performance Computing (HPC) systems nowadays, quantitatively understanding the entangled effects among performance, energy efficiency, and resilience is thus required. While previous work focuses on exploring energy-saving and resilience-enhancing opportunities separately, little has been done to theoretically and empirically investigate the interplay between energy efficiency and resilience at scale. In this article, by extending the Amdahl’s Law and the Karp-Flatt Metric, taking resilience into consideration, we quantitatively model the integrated energy efficiency in terms of performance per Watt and showcase the trade-offs among typical HPC parameters, such as number of cores, frequency/voltage, and failure rates. Experimental results for a wide spectrum of HPC benchmarks on two HPC systems show that the proposed models are accurate in extrapolating resilience-aware performance and energy efficiency, and capable of capturing the interplay among various energy-saving and resilience factors. Moreover, the models can help find the optimal HPC configuration for the highest integrated energy efficiency, in the presence of failures and applied resilience techniques.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2250845294",
    "type": "article"
  },
  {
    "title": "Optimizing Control Transfer and Memory Virtualization in Full System Emulators",
    "doi": "https://doi.org/10.1145/2837027",
    "publication_date": "2015-12-08",
    "publication_year": 2015,
    "authors": "Ding‐Yong Hong; Chun-Chen Hsu; Cheng-Yi Chou; Wei‐Chung Hsu; Pangfeng Liu; Jan‐Jan Wu",
    "corresponding_authors": "",
    "abstract": "Full system emulators provide virtual platforms for several important applications, such as kernel and system software development, co-verification with cycle accurate CPU simulators, or application development for hardware still in development. Full system emulators usually use dynamic binary translation to obtain reasonable performance. This paper focuses on optimizing the performance of full system emulators. First, we optimize performance by enabling classic control transfer optimizations of dynamic binary translation in full system emulation, such as indirect branch target caching and block chaining. Second, we improve the performance of memory virtualization of cross-ISA virtual machines by improving the efficiency of the software translation lookaside buffer (software TLB). We implement our optimizations on QEMU, an industrial-strength full system emulator, along with the Android emulator. Experimental results show that our optimizations achieve an average speedup of 1.98X for ARM-to-X86-64 QEMU running SPEC CINT2006 benchmarks with train inputs. Our optimizations also achieve an average speedup of 1.44X and 1.40X for IA32-to-X86-64 QEMU and AArch64-to-X86-64 QEMU on SPEC CINT2006. We use a set of real applications downloaded from Google Play as benchmarks for the Android emulator. Experimental results show that our optimizations achieve an average speedup of 1.43X for the Android emulator running these applications.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2264988905",
    "type": "article"
  },
  {
    "title": "Iteration Interleaving--Based SIMD Lane Partition",
    "doi": "https://doi.org/10.1145/2847253",
    "publication_date": "2016-01-04",
    "publication_year": 2016,
    "authors": "Yaohua Wang; Dong Wang; Shuming Chen; Zonglin Liu; Shenggang Chen; Xiaowen Chen; Xu Zhou",
    "corresponding_authors": "",
    "abstract": "The efficacy of single instruction, multiple data (SIMD) architectures is limited when handling divergent control flows. This circumstance results in SIMD fragments using only a subset of the available lanes. We propose an iteration interleaving--based SIMD lane partition (IISLP) architecture that interleaves the execution of consecutive iterations and dynamically partitions SIMD lanes into branch paths with comparable execution time. The benefits are twofold: SIMD fragments under divergent branches can execute in parallel, and the pathology of fragment starvation can also be well eliminated. Our experiments show that IISLP doubles the performance of a baseline mechanism and provides a speedup of 28% versus instruction shuffle.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2291920536",
    "type": "article"
  },
  {
    "title": "Maximizing Heterogeneous Processor Performance Under Power Constraints",
    "doi": "https://doi.org/10.1145/2976739",
    "publication_date": "2016-09-17",
    "publication_year": 2016,
    "authors": "Almutaz Adileh; Stijn Eyerman; Aamer Jaleel; Lieven Eeckhout",
    "corresponding_authors": "",
    "abstract": "Heterogeneous processors (e.g., ARM’s big.LITTLE) improve performance in power-constrained environments by executing applications on the ‘little’ low-power core and move them to the ‘big’ high-performance core when there is available power budget. The total time spent on the big core depends on the rate at which the application dissipates the available power budget. When applications with different big-core power consumption characteristics concurrently execute on a heterogeneous processor, it is best to give a larger share of the power budget to applications that can run longer on the big core, and a smaller share to applications that run for a very short duration on the big core. This article investigates mechanisms to manage the available power budget on power-constrained heterogeneous processors. We show that existing proposals that schedule applications onto a big core based on various performance metrics are not high performing, as these strategies do not optimize over an entire power period and are unaware of the applications’ power/performance characteristics. We use linear programming to design the DPDP power management technique, which guarantees optimal performance on heterogeneous processors. We mathematically derive a metric (Delta Performance by Delta Power) that takes into account the power/performance characteristics of each running application and allows our power-management technique to decide how best to distribute the available power budget among the co-running applications at minimal overhead. Our evaluations with a 4-core heterogeneous processor consisting of big.LITTLE pairs show that DPDP improves performance by 16% on average and up to 40% compared to a strategy that globally and greedily optimizes the power budget. We also show that DPDP outperforms existing heterogeneous scheduling policies that use performance metrics to decide how best to schedule applications on the big core.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2521762357",
    "type": "article"
  },
  {
    "title": "An Accurate Cross-Layer Approach for Online Architectural Vulnerability Estimation",
    "doi": "https://doi.org/10.1145/2975588",
    "publication_date": "2016-09-17",
    "publication_year": 2016,
    "authors": "Bagus Wibowo; Abhinav Agrawal; Thomas H. Stanton; James Tuck",
    "corresponding_authors": "",
    "abstract": "Processor soft-error rates are projected to increase as feature sizes scale down, necessitating the adoption of reliability-enhancing techniques, but power and performance overhead remain a concern of such techniques. Dynamic cross-layer techniques are a promising way to improve the cost-effectiveness of resilient systems. As a foundation for making such a system, we propose a cross-layer approach for estimating the architectural vulnerability of a processor core online that works by combining information from software, compiler, and microarchitectural layers at runtime. The hardware layer combines the metadata from software and compiler layers with microarchitectural measurements to estimate architectural vulnerability online. We describe our design and evaluate it in detail on a set of SPEC CPU 2006 applications. We find that our online AVF estimate is highly accurate with respect to a postmortem AVF analysis, with only 0.46% average absolute error. Also, our design incurs negligible performance impact for SPEC2006 applications and about 1.2% for a Monte Carlo application, requires approximately 1.4% area overhead, and costs about 3.3% more power on average. We compare our technique against two prior online AVF estimation techniques, one using a linear regression to estimate AVF and another based on PVF-HVF; our evaluation finds that our approach, on average, is more accurate. Our case study of a Monte Carlo simulation shows that our AVF estimate can adapt to the inherent resiliency of the algorithm. Finally, we demonstrate the effectiveness of our approach using a dynamic protection scheme that limits vulnerability to soft errors while reducing the energy consumption by an average of 4.8%, and with a target normalized SER of 10%, compared to enabling a simple parity+ECC protection at all times.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2522581075",
    "type": "article"
  },
  {
    "title": "AIM",
    "doi": "https://doi.org/10.1145/2994149",
    "publication_date": "2016-10-25",
    "publication_year": 2016,
    "authors": "Junwhan Ahn; Sungjoo Yoo; Ki‐Young Choi",
    "corresponding_authors": "",
    "abstract": "In this article, we propose Aggregation-in-Memory (AIM), a new processing-in-memory system designed for energy efficiency and near-term adoption. In order to efficiently perform aggregation, we implement simple aggregation operations in main memory and develop a locality-adaptive host architecture for in-memory aggregation, called cache-conscious aggregation. Through this, AIM executes aggregation at the most energy-efficient location among all levels of the memory hierarchy. Moreover, AIM minimally changes existing sequential programming models and provides fully automated compiler toolchain, thereby allowing unmodified legacy software to use AIM. Evaluations show that AIM greatly improves the energy efficiency of main memory and the system performance.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2532763869",
    "type": "article"
  },
  {
    "title": "LDAC",
    "doi": "https://doi.org/10.1145/2983632",
    "publication_date": "2016-11-15",
    "publication_year": 2016,
    "authors": "Qingchuan Shi; George Thomas Kurian; Farrukh Hijaz; Srinivas Devadas; Omer Khan",
    "corresponding_authors": "",
    "abstract": "The trend of increasing the number of cores to achieve higher performance has challenged efficient management of on-chip data. Moreover, many emerging applications process massive amounts of data with varying degrees of locality. Therefore, exploiting locality to improve on-chip traffic and resource utilization is of fundamental importance. Conventional multicore cache management schemes either manage the private cache (L1) or the Last-Level Cache (LLC), while ignoring the other. We propose a holistic locality-aware cache hierarchy management protocol for large-scale multicores. The proposed scheme improves on-chip data access latency and energy consumption by intelligently bypassing cache line replication in the L1 caches, and/or intelligently replicating cache lines in the LLC. The approach relies on low overhead yet highly accurate in-hardware runtime classification of data locality at both L1 cache and the LLC. The decision to bypass L1 and/or replicate in LLC is then based on the measured reuse at the fine granularity of cache lines. The locality tracking mechanism is decoupled from the sharer tracking structures that cause scalability concerns in traditional cache coherence protocols. Moreover, the complexity of the protocol is low since no additional coherence states are created. However, the proposed classifier incurs a 5.6 KB per-core storage overhead. On a set of parallel benchmarks, the locality-aware protocol reduces average energy consumption by 26% and completion time by 16%, when compared to the state-of-the-art Reactive-NUCA multicore cache management scheme.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2554529045",
    "type": "article"
  },
  {
    "title": "FlowPix: Accelerating Image Processing Pipelines on an FPGA Overlay using a Domain Specific Compiler",
    "doi": "https://doi.org/10.1145/3629523",
    "publication_date": "2023-10-25",
    "publication_year": 2023,
    "authors": "Ziaul Choudhury; A. Gulati; Suresh Purini",
    "corresponding_authors": "",
    "abstract": "The exponential performance growth guaranteed by Moore’s law has started to taper in recent years. At the same time, emerging applications like image processing demand heavy computational performance. These factors inevitably lead to the emergence of domain-specific accelerators (DSA) to fill the performance void left by conventional architectures. FPGAs are rapidly evolving towards becoming an alternative to custom ASICs for designing DSAs because of their low power consumption and a higher degree of parallelism. DSA design on FPGAs requires careful calibration of the FPGA compute and memory resources towards achieving optimal throughput. Hardware Descriptive Languages (HDL) like Verilog have been traditionally used to design FPGA hardware. HDLs are not geared towards any domain, and the user has to put in much effort to describe the hardware at the register transfer level. Domain Specific Languages (DSLs) and compilers have been recently used to weave together handwritten HDLs templates targeting a specific domain. Recent efforts have designed DSAs with image-processing DSLs targeting FPGAs. Image computations in the DSL are lowered to pre-existing templates or lower-level languages like HLS-C. This approach requires expensive FPGA re-flashing for every new workload. In contrast to this fixed-function hardware approach, overlays are gaining traction. Overlays are DSAs resembling a processor, which is synthesized and flashed on the FPGA once but is flexible enough to process a broad class of computations through soft reconfiguration. Less work has been reported in the context of image processing overlays. Image processing algorithms vary in size and shape, ranging from simple blurring operations to complex pyramid systems. The primary challenge in designing an image-processing overlay is maintaining flexibility in mapping different algorithms. This paper proposes a DSL-based overlay accelerator called FlowPix for image processing applications. The DSL programs are expressed as pipelines, with each stage representing a computational step in the overall algorithm. We implement 15 image-processing benchmarks using FlowPix on a Virtex-7-690t FPGA. The benchmarks range from simple blur operations to complex pipelines like Lucas-Kande optical flow. We compare FlowPix against existing DSL-to-FPGA frameworks like Hetero-Halide and Vitis Vision library that generate fixed-function hardware. On most benchmarks, we see up to 25% degradation in latency with approximately a 1.7x to 2x increase in the FPGA LUT consumption. Our ability to execute any benchmark without incurring the high costs of hardware synthesis, place-and-route, and FPGA re-flashing justifies the slight performance loss and increased resource consumption that we experience. FlowPix achieves an average frame rate of 170 FPS on HD frames of 1920 × 1080 pixels in the implemented benchmarks.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4387933363",
    "type": "article"
  },
  {
    "title": "The design, implementation, and evaluation of adaptive code unloading for resource-constrained devices",
    "doi": "https://doi.org/10.1145/1071604.1071606",
    "publication_date": "2005-06-01",
    "publication_year": 2005,
    "authors": "Lingli Zhang; Chandra Krintz",
    "corresponding_authors": "",
    "abstract": "Java Virtual Machines (JVMs) for resource-constrained devices, e.g., hand-helds and cell phones, commonly employ interpretation for program translation. However, compilers are able to produce significantly better code quality, and, hence, use device resources more efficiently than interpreters, since compilers can consider large sections of code concurrently and exploit optimization opportunities. Moreover, compilation-based systems store code for reuse by future invocations obviating the redundant computation required for reinterpretation of repeatedly executed code.However, code storage required for compilation can increase the memory footprint of the virtual machine (VM) significantly. As a result, for devices with limited memory resources, this additional code storage may preclude some programs from executing, significantly increase memory management overhead, and substantially reduce the amount of memory available for use by the application.To address the limitations of native code storage, we present the design, implementation, and empirical evaluation of a compiled-code management system that can be integrated into any compilation-based JVM. The system unloads compiled code to reduce the memory footprint of the VM. It does so by dynamically identifying and unloading dead or infrequently used code; if the code is later reused, it is recompiled by the system. As such, our system adaptively trades off memory footprint and its associated memory management costs, with recompilation overhead. Our empirical evaluation shows that our code management system significantly reduces the memory requirements of a compile-only JVM, while maintaining the performance benefits enabled by compilation.We investigate a number of implementation alternatives that use dynamic program behavior and system resource availability to determine when to unload as well as what code to unload. From our empirical evaluation of these alternatives, we identify a set of strategies that enable significant reductions in the memory overhead required for application code. Our system reduces code size by 36--62%, on average, which translates into significant execution-time benefits for the benchmarks and JVM configurations that we studied.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2130959338",
    "type": "article"
  },
  {
    "title": "Instruction packing",
    "doi": "https://doi.org/10.1145/1138035.1138037",
    "publication_date": "2006-06-01",
    "publication_year": 2006,
    "authors": "Joseph Sharkey; Dmitry Ponomarev; Kanad Ghose; Oğuz Ergin",
    "corresponding_authors": "",
    "abstract": "Traditional dynamic scheduler designs use one issue queue entry per instruction, regardless of the actual number of operands actively involved in the wakeup process. We propose Instruction Packing---a novel microarchitectural technique that reduces both delay and power consumption of the issue queue by sharing the associative part of an issue queue entry between two instructions, each with, at most, one nonready register source operand at the time of dispatch. Our results show that this technique results in 40% reduction of the IQ power and 14% reduction in scheduling delay with negligible IPC degradations.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2055121996",
    "type": "article"
  },
  {
    "title": "Java object header elimination for reduced memory consumption in 64-bit virtual machines",
    "doi": "https://doi.org/10.1145/1275937.1275941",
    "publication_date": "2007-09-01",
    "publication_year": 2007,
    "authors": "Kris Venstermans; Lieven Eeckhout; Koen De Bosschere",
    "corresponding_authors": "",
    "abstract": "Memory performance is an important design issue for contemporary computer systems given the huge processor/memory speed gap. This paper proposes a space-efficient Java object model for reducing the memory consumption of 64-bit Java virtual machines. We completely eliminate the object header through typed virtual addressing (TVA) or implicit typing. TVA encodes the object type in the object's virtual address by allocating all objects of a given type in a contiguous memory segment. This allows for removing the type information as well as the status field from the object header. Whenever type and status information is needed, masking is applied to the object's virtual address for obtaining an offset into type and status information structures. Unlike previous work on implicit typing, we apply TVA to a selected number of frequently allocated object types, hence, the name selective TVA (STVA); this limits the amount of memory fragmentation. In addition to applying STVA, we also compress the type information block (TIB) pointers for all objects that do not fall under TVA. We implement the space-efficient Java object model in the 64-bit version of the Jikes RVM on an AIX IBM platform and compare its performance against the traditionally used Java object model using a multitude of Java benchmarks. We conclude that the space-efficient Java object model reduces memory consumption by on average 15% (and up to 45% for some benchmarks). About one-half the reduction comes from TIB pointer compression; the other one-half comes from STVA. In terms of performance, the space-efficient object model generally does not affect performance; however, for some benchmarks we observe statistically significant performance speedups, up to 20%.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2104117376",
    "type": "article"
  },
  {
    "title": "SSA-based mobile code",
    "doi": "https://doi.org/10.1145/1250727.1250733",
    "publication_date": "2007-06-01",
    "publication_year": 2007,
    "authors": "Wolfram Amme; Jeffery von Ronne; Michael Franz",
    "corresponding_authors": "",
    "abstract": "Although one might expect transportation formats based on static single-assignment form (SSA) to yield faster just-in-time compilation times than those based on stack-based virtual machines, this claim has not previously been validated, in practice. We attempt to quantify the effect of using an SSA-based mobile code representation by integrating support for a verifiable SSA-based IR into Jikes RVM. Performance results, measured with various optimizations and on both the IA32 and PowerPC, show improvements in both compilation time and code quality.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2109160899",
    "type": "article"
  },
  {
    "title": "Monolithically Integrating Non-Volatile Main Memory over the Last-Level Cache",
    "doi": "https://doi.org/10.1145/3462632",
    "publication_date": "2021-07-17",
    "publication_year": 2021,
    "authors": "Candace Walden; Devesh Singh; Meenatchi Jagasivamani; Li Shang; Luyi Kang; Mehdi Asnaashari; Sylvain Dubois; Bruce Jacob; Donald Yeung",
    "corresponding_authors": "",
    "abstract": "Many emerging non-volatile memories are compatible with CMOS logic, potentially enabling their integration into a CPU’s die. This article investigates such monolithically integrated CPU–main memory chips. We exploit non-volatile memories employing 3D crosspoint subarrays, such as resistive RAM (ReRAM), and integrate them over the CPU’s last-level cache (LLC). The regular structure of cache arrays enables co-design of the LLC and ReRAM main memory for area efficiency. We also develop a streamlined LLC/main memory interface that employs a single shared internal interconnect for both the cache and main memory arrays, and uses a unified controller to service both LLC and main memory requests. We apply our monolithic design ideas to a many-core CPU by integrating 3D ReRAM over each core’s LLC slice. We find that co-design of the LLC and ReRAM saves 27% of the total LLC–main memory area at the expense of slight increases in delay and energy. The streamlined LLC/main memory interface saves an additional 12% in area. Our simulation results show monolithic integration of CPU and main memory improves performance by 5.3× and 1.7× over HBM2 DRAM for several graph and streaming kernels, respectively. It also reduces the memory system’s energy by 6.0× and 1.7×, respectively. Moreover, we show that the area savings of co-design permits the CPU to have 23% more cores and main memory, and that streamlining the LLC/main memory interface incurs a small 4% performance penalty.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3185867486",
    "type": "article"
  },
  {
    "title": "Spiking Neural Networks in Spintronic Computational RAM",
    "doi": "https://doi.org/10.1145/3475963",
    "publication_date": "2021-09-29",
    "publication_year": 2021,
    "authors": "Hüsrev Cılasun; Salonik Resch; Zamshed I. Chowdhury; Erin Olson; Masoud Zabihi; Zhengyang Zhao; T. Peterson; Keshab K. Parhi; Jian‐Ping Wang; Sachin S. Sapatnekar; Ulya R. Karpuzcu",
    "corresponding_authors": "",
    "abstract": "Spiking Neural Networks (SNNs) represent a biologically inspired computation model capable of emulating neural computation in human brain and brain-like structures. The main promise is very low energy consumption. Classic Von Neumann architecture based SNN accelerators in hardware, however, often fall short of addressing demanding computation and data transfer requirements efficiently at scale. In this article, we propose a promising alternative to overcome scalability limitations, based on a network of in-memory SNN accelerators, which can reduce the energy consumption by up to 150.25= when compared to a representative ASIC solution. The significant reduction in energy comes from two key aspects of the hardware design to minimize data communication overheads: (1) each node represents an in-memory SNN accelerator based on a spintronic Computational RAM array, and (2) a novel, De Bruijn graph based architecture establishes the SNN array connectivity.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3201969852",
    "type": "article"
  },
  {
    "title": "DAPSCO",
    "doi": "https://doi.org/10.1145/2086696.2086704",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Antonio García‐Guirado; Ricardo Fernández‐Pascual; Alberto Ros; José M. Garcı́a",
    "corresponding_authors": "",
    "abstract": "Many-core tiled CMP proposals often assume a partially shared last level cache (LLC) since this provides a good compromise between access latency and cache utilization. In this paper, we propose a novel way to map memory addresses to LLC banks that takes into account the average distance between the banks and the tiles that access them. Contrary to traditional approaches, our mapping does not group the tiles in clusters within which all the cores access the same bank for the same addresses. Instead, two neighboring cores access different sets of banks minimizing the average distance travelled by the cache requests. Results for a 64-core CMP show that our proposal improves both execution time and the energy consumed by the network by 13% when compared to a traditional mapping. Moreover, our proposal comes at a negligible cost in terms of hardware and its benefits in both energy and execution time increase with the number of cores.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W1972784174",
    "type": "article"
  },
  {
    "title": "Fair CPU time accounting in CMP+SMT processors",
    "doi": "https://doi.org/10.1145/2400682.2400709",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Carlos Luque; Miquel Moretó; Francisco J. Cazorla; Mateo Valero",
    "corresponding_authors": "",
    "abstract": "Processor architectures combining several paradigms of Thread-Level Parallelism (TLP), such as CMP processors in which each core is SMT, are becoming more and more popular as a way to improve performance at a moderate cost. However, the complex interaction between running tasks in hardware shared resources in multi-TLP architectures introduces complexities when accounting CPU time (or CPU utilization) to tasks. The CPU utilization accounted to a task depends on both the time it runs in the processor and the amount of processor hardware resources it receives. Deploying systems with accurate CPU accounting mechanisms is necessary to increase fairness. Moreover, it will allow users to be fairly charged on a shared data center, facilitating server consolidation in future systems. In this article we analyze the accuracy and hardware cost of previous CPU accounting mechanisms for pure-CMP and pure-SMT processors and we show that they are not adequate for CMP+SMT processors. Consequently, we propose a new accounting mechanism for CMP+SMT processors which: (1) increases the accuracy of accounted CPU utilization; (2) provides much more stable results over a wide range of processor setups; and (3) does not require tracking all hardware shared resources, significantly reducing its implementation cost. In particular, previous proposals lead to inaccuracies between 21% and 79% when measuring CPU utilization in an 8-core 2-way SMT processor, while our proposal reduces this inaccuracy to less than 5.0%.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2056007740",
    "type": "article"
  },
  {
    "title": "Accelerating an application domain with specialized functional units",
    "doi": "https://doi.org/10.1145/2541228.2555303",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Cecilia González-Álvarez; Jennifer B. Sartor; Carlos Álvarez; Daniel Jiménez-González; Lieven Eeckhout",
    "corresponding_authors": "",
    "abstract": "Hardware specialization has received renewed interest recently as chips are hitting power limits. Chip designers of traditional processor architectures have primarily focused on general-purpose computing, partially due to time-to-market pressure and simpler design processes. But new power limits require some chip specialization. Although hardware configured for a specific application yields large speedups for low-power dissipation, its design is more complex and less reusable. We instead explore domain-based specialization, a scalable approach that balances hardware’s reusability and performance efficiency. We focus on specialization using customized compute units that accelerate particular operations. In this article, we develop automatic techniques to identify code sequences from different applications within a domain that can be targeted to a new custom instruction that will be run inside a configurable specialized functional unit (SFU). We demonstrate that using a canonical representation of computations finds more common code sequences among applications that can be mapped to the same custom instruction, leading to larger speedups while specializing a smaller core area than previous pattern-matching techniques. We also propose new heuristics to narrow the search space of domain-specific custom instructions, finding those that achieve the best performance across applications. We estimate the overall performance achieved with our automatic techniques using hardware models on a set of nine media benchmarks, showing that when limiting the core area devoted to specialization, the SFU customization with the largest speedups includes both application- and domain-specific custom instructions. We demonstrate that exploring domain-specific hardware acceleration is key to continued computing system performance improvements.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2058884937",
    "type": "article"
  },
  {
    "title": "Fast modulo scheduler utilizing patternized routes for coarse-grained reconfigurable architectures",
    "doi": "https://doi.org/10.1145/2541228.2555314",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Wonsub Kim; Yoonseo Choi; Hae-woo Park",
    "corresponding_authors": "",
    "abstract": "Coarse-Grained Reconfigurable Architectures (CGRAs) present a potential of high compute throughput with energy efficiency. A CGRA consists of an array of Functional Units (FUs), which communicate with each other through an interconnect network containing transmission nodes and register files. To achieve high performance from the software solutions mapped onto CGRAs, modulo scheduling of loops is generally employed. One of the key challenges in modulo scheduling for CGRAs is to explicitly handle routings of operands from a source to a destination operations through various routing resources. Existing modulo schedulers for CGRAs are slow because finding a valid routing is generally a searching problem over a large space, even with the guidance of well-defined cost metrics. Applications in traditional embedded multimedia domains are regarded as relatively tolerant to a slow compile time in exchange for a high-quality solution. However, many rapidly growing domains of applications, such as 3D graphics, require a fast compilation. Entrances of CGRAs to these domains have been blocked mainly due to their long compile time. We attack this problem by utilizing patternized routes, for which resources and time slots for a success can be estimated in advance when a source operation is placed. By conservatively reserving predefined resources at predefined time slots, future routings originating from the source operation are guaranteed. Experiments on a real-world 3D graphics benchmark suite show that our scheduler improves the compile time up to 6,000 times while achieving an average 70% throughputs of the state-of-the-art CGRA modulo scheduler, the Edge-centric Modulo Scheduler (EMS).",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2084467531",
    "type": "article"
  },
  {
    "title": "Probabilistic modeling for job symbiosis scheduling on SMT processors",
    "doi": "https://doi.org/10.1145/2207222.2207223",
    "publication_date": "2012-06-01",
    "publication_year": 2012,
    "authors": "Stijn Eyerman; Lieven Eeckhout",
    "corresponding_authors": "",
    "abstract": "Symbiotic job scheduling improves simultaneous multithreading (SMT) processor performance by coscheduling jobs that have “compatible” demands on the processor's shared resources. Existing approaches however require a sampling phase, evaluate a limited number of possible coschedules, use heuristics to gauge symbiosis, are rigid in their optimization target, and do not preserve system-level priorities/shares. This article proposes probabilistic job symbiosis modeling, which predicts whether jobs will create positive or negative symbiosis when coscheduled without requiring the coschedule to be evaluated. The model, which uses per-thread cycle stacks computed through a previously proposed cycle accounting architecture, is simple enough to be used in system software. Probabilistic job symbiosis modeling provides six key innovations over prior work in symbiotic job scheduling: (i) it does not require a sampling phase, (ii) it readjusts the job coschedule continuously, (iii) it evaluates a large number of possible coschedules at very low overhead, (iv) it is not driven by heuristics, (v) it can optimize a performance target of interest (e.g., system throughput or job turnaround time), and (vi) it preserves system-level priorities/shares. These innovations make symbiotic job scheduling both practical and effective. Our experimental evaluation, which assumes a realistic scenario in which jobs come and go, reports an average 16% (and up to 35%) reduction in job turnaround time compared to the previously proposed SOS (sample, optimize, symbios) approach for a two-thread SMT processor, and an average 19% (and up to 45%) reduction in job turnaround time for a four-thread SMT processor.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2102382208",
    "type": "article"
  },
  {
    "title": "Sabrewing",
    "doi": "https://doi.org/10.1145/2086696.2086720",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Tom M. Bruintjes; K.H.G. Walters; Sabih H. Gerez; Bert Molenkamp; Gerard J.M. Smit",
    "corresponding_authors": "",
    "abstract": "In spite of the fact that floating-point arithmetic is costly in terms of silicon area, the joint design of hardware for floating-point and integer arithmetic is seldom considered. While components like multipliers and adders can potentially be shared, floating-point and integer units in contemporary processors are practically disjoint. This work presents a new architecture which tightly integrates floating-point and integer arithmetic in a single datapath. It is mainly intended for use in low-power embedded digital signal processors and therefore the following design constraints were important: limited use of pipelining for the convenience of the compiler; maintaining compatibility with existing technology; minimal area and power consumption for applicability in embedded systems. The architecture is tailored to digital signal processing by combining floating-point fused multiply-add and integer multiply-accumulate . It could be deployed in a multi-core system-on-chip designed to support applications with and without dominance of floating-point calculations. The VHDL structural description of this architecture is available for download under BSD license. Besides being configurable at design time, it has been thoroughly checked for IEEE-754 compliance by means of a floating-point test suite originating from the IBM Research Labs. A proof-of-concept has also been implemented using STMicroelectronics 65nm technology. This prototype supports 32-bit signed two's complement integers and 41-bit (8-bit exponent and 32-bit significand) floating-point numbers. Our evaluations show that over 67% energy and 19% area can be saved compared to a reference design in which floating-point and integer arithmetic are implemented separately. The area overhead caused by combining floating-point and integer is less than 5%. Implemented in ST's general-purpose CMOS technology, the design can operate at a frequency of 1.35GHz, while 667MHz can be achieved in low-power CMOS. Considering that the entire datapath is partitioned in just three pipeline stages, and the fact that the design is intended for use in the low-power domain, these frequencies are adequate. They are in fact competitive with current technology low-power floating-point units. Post-layout estimates indicate that the required area of a low-power implementation can be as small as 0.04mm 2 . Power consumption is on the order of several milliwatts. Strengthened by the fact that clock gating could reduce power consumption even further, we think that a shared floating-point and integer architecture is a good choice for signal processing in low-power embedded systems.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2143263632",
    "type": "article"
  },
  {
    "title": "Mixed speculative multithreaded execution models",
    "doi": "https://doi.org/10.1145/2355585.2355591",
    "publication_date": "2012-09-01",
    "publication_year": 2012,
    "authors": "Polychronis Xekalakis; Nikolas Ioannou; Marcelo Cintra",
    "corresponding_authors": "",
    "abstract": "The current trend toward multicore architectures has placed great pressure on programmers and compilers to generate thread-parallel programs. Improved execution performance can no longer be obtained via traditional single-thread instruction level parallelism (ILP), but, instead, via multithreaded execution. One notable technique that facilitates the extraction of parallel threads from sequential applications is thread-level speculation (TLS). This technique allows programmers/compilers to generate threads without checking for inter-thread data and control dependences, which are then transparently enforced by the hardware. Most prior work on TLS has concentrated on thread selection and mechanisms to efficiently support the main TLS operations, such as squashes, data versioning, and commits. This article seeks to enhance TLS functionality by combining it with other speculative multithreaded execution models. The main idea is that TLS already requires extensive hardware support, which when slightly augmented can accommodate other speculative multithreaded techniques. Recognizing that for different applications, or even program phases, the application bottlenecks may be different, it is reasonable to assume that the more versatile a system is, the more efficiently it will be able to execute the given program. Toward this direction, we first show that mixed execution models that combine TLS with Helper Threads (HT), RunAhead execution (RA) and MultiPath execution (MP) perform better than any of the models alone. Based on a simple model that we propose, we show that benefits come from being able to extract additional ILP without harming the TLP extracted by TLS. We then show that by combining all the execution models in a unified one that combines all these speculative multithreaded models, ILP can be further enhanced with only minimal additional cost in hardware.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2152698516",
    "type": "article"
  },
  {
    "title": "An Architecture for Integrated Near-Data Processors",
    "doi": "https://doi.org/10.1145/3127069",
    "publication_date": "2017-09-06",
    "publication_year": 2017,
    "authors": "Erik Vermij; Leandro Fiorin; Rik Jongerius; Christoph Hagleitner; Jan van Lunteren; Koen Bertels",
    "corresponding_authors": "",
    "abstract": "To increase the performance of data-intensive applications, we present an extension to a CPU architecture that enables arbitrary near-data processing capabilities close to the main memory. This is realized by introducing a component attached to the CPU system-bus and a component at the memory side. Together they support hardware-managed coherence and virtual memory support to integrate the near-data processors in a shared-memory environment. We present an implementation of the components, as well as a system-simulator, providing detailed performance estimations. With a variety of synthetic workloads we demonstrate the performance of the memory accesses, the mixed fine- and coarse-grained coherence mechanisms, and the near-data processor communication mechanism. Furthermore, we quantify the inevitable start-up penalty regarding coherence and data writeback, and argue that near-data processing workloads should access data several times to offset this penalty. A case study based on the Graph500 benchmark confirms the small overhead for the proposed coherence mechanisms and shows the ability to outperform a real CPU by a factor of two.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2751182785",
    "type": "article"
  },
  {
    "title": "Power Consumption Models for Multi-Tenant Server Infrastructures",
    "doi": "https://doi.org/10.1145/3148965",
    "publication_date": "2017-11-14",
    "publication_year": 2017,
    "authors": "Matteo Ferroni; Andrea Corna; Andrea Damiani; Rolando Brondolin; Juan A. Colmenares; Steven Hofmeyr; John Kubiatowicz; Marco D. Santambrogio",
    "corresponding_authors": "",
    "abstract": "Multi-tenant virtualized infrastructures allow cloud providers to minimize costs through workload consolidation. One of the largest costs is power consumption, which is challenging to understand in heterogeneous environments. We propose a power modeling methodology that tackles this complexity using a divide-and-conquer approach. Our results outperform previous research work, achieving a relative error of 2% on average and under 4% in almost all cases. Models are portable across similar architectures, enabling predictions of power consumption before migrating a tenant to a different hardware platform. Moreover, we show the models allow us to evaluate colocations of tenants to reduce overall consumption.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2770126384",
    "type": "article"
  },
  {
    "title": "Hardware support for accurate per-task energy metering in multicore systems",
    "doi": "https://doi.org/10.1145/2555289.2555291",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Qixiao Liu; Miquel Moretó; Víctor Jiménez; Jaume Abella; Francisco J. Cazorla; Mateo Valero",
    "corresponding_authors": "",
    "abstract": "Accurately determining the energy consumed by each task in a system will become of prominent importance in future multicore-based systems because it offers several benefits, including (i) better application energy/performance optimizations, (ii) improved energy-aware task scheduling, and (iii) energy-aware billing in data centers. Unfortunately, existing methods for energy metering in multicores fail to provide accurate energy estimates for each task when several tasks run simultaneously.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4237656017",
    "type": "article"
  },
  {
    "title": "Optimal Parallelogram Selection for Hierarchical Tiling",
    "doi": "https://doi.org/10.1145/2687414",
    "publication_date": "2015-01-09",
    "publication_year": 2015,
    "authors": "Xing Zhou; María Jesús Garzarán; David Padua",
    "corresponding_authors": "",
    "abstract": "Loop tiling is an effective optimization to improve performance of multiply nested loops, which are the most time-consuming parts in many programs. Most massively parallel systems today are organized hierarchically, and different levels of the hierarchy differ in the organization of parallelism and the memory models they adopt. To make better use of these machines, it is clear that loop nests should be tiled hierarchically to fit the hierarchical organization of the machine; however, it is not so clear what should be the exact form of these hierarchical tiles. In particular, tile shape selection is of critical importance to expose parallelism of the tiled loop nests. Although loop tiling is a well-known optimization, not much is known about tile shape selection. In this article, we study tile shape selection when the shapes are any type of parallelograms and introduce a model to relate the tile shape of the hierarchy to the execution time. Using this model, we implement a system that automatically finds the tile shapes that minimize the execution time in a hierarchical system. Our experimental results show that in several cases, the tiles automatically selected by our system outperform the most intuitive tiling schemes usually adopted by programmers because of their simplicity.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2019557157",
    "type": "article"
  },
  {
    "title": "On Using the Roofline Model with Lower Bounds on Data Movement",
    "doi": "https://doi.org/10.1145/2693656",
    "publication_date": "2015-01-09",
    "publication_year": 2015,
    "authors": "Venmugil Elango; Naser Sedaghati; Fabrice Rastello; Louis-Noël Pouchet; J. Ramanujam; Radu Teodorescu; P. Sadayappan",
    "corresponding_authors": "",
    "abstract": "The roofline model is a popular approach for “bound and bottleneck” performance analysis. It focuses on the limits to the performance of processors because of limited bandwidth to off-chip memory. It models upper bounds on performance as a function of operational intensity, the ratio of computational operations per byte of data moved from/to memory. While operational intensity can be directly measured for a specific implementation of an algorithm on a particular target platform, it is of interest to obtain broader insights on bottlenecks, where various semantically equivalent implementations of an algorithm are considered, along with analysis for variations in architectural parameters. This is currently very cumbersome and requires performance modeling and analysis of many variants. In this article, we address this problem by using the roofline model in conjunction with upper bounds on the operational intensity of computations as a function of cache capacity, derived from lower bounds on data movement. This enables bottleneck analysis that holds across all dependence-preserving semantically equivalent implementations of an algorithm. We demonstrate the utility of the approach in assessing fundamental limits to performance and energy efficiency for several benchmark algorithms across a design space of architectural variations.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2091421236",
    "type": "article"
  },
  {
    "title": "Buddy SM",
    "doi": "https://doi.org/10.1145/2744202",
    "publication_date": "2015-05-11",
    "publication_year": 2015,
    "authors": "Tao Zhang; Naifeng Jing; Kaiming Jiang; Wei Shu; Min‐You Wu; Xiaoyao Liang",
    "corresponding_authors": "",
    "abstract": "A modern general-purpose graphics processing unit (GPGPU) usually consists of multiple streaming multiprocessors (SMs), each having a pipeline that incorporates a group of threads executing a common instruction flow. Although SMs are designed to work independently, we observe that they tend to exhibit very similar behavior for many workloads. If multiple SMs can be grouped and work in the lock-step manner, it is possible to save energy by sharing the front-end units among multiple SMs, including the instruction fetch, decode, and schedule components. However, such sharing brings architectural challenges and sometime causes performance degradation. In this article, we show our design, implementation, and evaluation for such an architecture, which we call Buddy SM . Specifically, multiple SMs can be opportunistically grouped into a buddy cluster. One SM becomes the master, and the rest become the slaves. The front-end unit of the master works actively for itself as well as for the slaves, whereas the front-end logics of the slaves are power gated. For efficient flow control and program correctness, the proposed architecture can identify unfavorable conditions and ungroup the buddy cluster when necessary. We analyze various techniques to improve the performance and energy efficiency of Buddy SM. Detailed experiments manifest that 37.2% front-end and 7.5% total GPU energy reduction can be achieved.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2203504382",
    "type": "article"
  },
  {
    "title": "R-GPU",
    "doi": "https://doi.org/10.1145/2890506",
    "publication_date": "2016-03-07",
    "publication_year": 2016,
    "authors": "Gert-Jan van den Braak; Henk Corporaal",
    "corresponding_authors": "",
    "abstract": "Over the last decade, Graphics Processing Unit (GPU) architectures have evolved from a fixed-function graphics pipeline to a programmable, energy-efficient compute accelerator for massively parallel applications. The compute power arises from the GPU’s Single Instruction/Multiple Threads architecture: concurrently running many threads and executing them as Single Instruction/Multiple Data--style vectors. However, compute power is still lost due to cycles spent on data movement and control instructions instead of data computations. Even more cycles are lost on pipeline stalls resulting from long latency (memory) operations. To improve not only performance but also energy efficiency, we introduce R-GPU: a reconfigurable GPU architecture with communicating cores. R-GPU is an addition to a GPU, which can still be used as such, but also has the ability to reorganize the cores of a GPU in a reconfigurable network. In R-GPU data movement and control is implicit in the configuration of the network. Each core executes a fixed instruction, reducing instruction decode count and increasing energy efficiency. On a number of benchmarks we show an average performance improvement of 2.1 × over the same GPU without modifications. We further make a conservative power estimation of R-GPU which shows that power consumption can be reduced by 6%, leading to an energy consumption reduction of 55%, while area only increases by a mere 4%.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2300715507",
    "type": "article"
  },
  {
    "title": "Optimization Models for Three On-Chip Network Problems",
    "doi": "https://doi.org/10.1145/2943781",
    "publication_date": "2016-09-17",
    "publication_year": 2016,
    "authors": "Nilay Vaish; Michael C. Ferris; David A. Wood",
    "corresponding_authors": "",
    "abstract": "We model three on-chip network design problems—memory controller placement, resource allocation in heterogeneous on-chip networks, and their combination—as mathematical optimization problems. We model the first two problems as mixed integer linear programs. We model the third problem as a mixed integer nonlinear program, which we then linearize exactly. Sophisticated optimization algorithms enable solutions to be obtained much more efficiently. Detailed simulations using synthetic traffic and benchmark applications validate that our designs provide better performance than solutions proposed previously. Our work provides further evidence toward suitability of optimization models in searching/pruning architectural design space.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2523279835",
    "type": "article"
  },
  {
    "title": "Memory Access Scheduling Based on Dynamic Multilevel Priority in Shared DRAM Systems",
    "doi": "https://doi.org/10.1145/3007647",
    "publication_date": "2016-12-02",
    "publication_year": 2016,
    "authors": "Dongliang Xiong; Kai Huang; Xiaowen Jiang; Xiaolang Yan",
    "corresponding_authors": "",
    "abstract": "Interapplication interference at shared main memory severely degrades performance and increasing DRAM frequency calls for simple memory schedulers. Previous memory schedulers employ a per-application ranking scheme for high system performance or a per-group ranking scheme for low hardware cost, but few provide a balance. We propose DMPS, a memory scheduler based on dynamic multilevel priority. First, DMPS uses “memory occupancy” to measure interference quantitatively. Second, DMPS groups applications, favors latency-sensitive groups, and dynamically prioritizes applications by employing a per-level ranking scheme. The simulation results show that DMPS has 7.2% better system performance and 22% better fairness over FRFCFS at low hardware complexity and cost.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2558455683",
    "type": "article"
  },
  {
    "title": "Pot",
    "doi": "https://doi.org/10.1145/3017993",
    "publication_date": "2016-12-16",
    "publication_year": 2016,
    "authors": "Tiago M. Vale; João A. Silva; Ricardo J. Dias; João Lourenço",
    "corresponding_authors": "",
    "abstract": "This article presents Pot, a system that leverages the concept of preordered transactions to achieve deterministic multithreaded execution of programs that use Transactional Memory. Preordered transactions eliminate the root cause of nondeterminism in transactional execution: they provide the illusion of executing in a deterministic serial order, unlike traditional transactions that appear to execute in a nondeterministic order that can change from execution to execution. Pot uses a new concurrency control protocol that exploits the serialization order to distinguish between fast and speculative transaction execution modes in order to mitigate the overhead of imposing a deterministic order. We build two Pot prototypes: one using STM and another using off-the-shelf HTM. To the best of our knowledge, Pot enables deterministic execution of programs using off-the-shelf HTM for the first time. An experimental evaluation shows that Pot achieves deterministic execution of TM programs with low overhead, sometimes even outperforming nondeterministic executions, and clearly outperforming the state of the art.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2561368256",
    "type": "article"
  },
  {
    "title": "Selecting Heterogeneous Cores for Diversity",
    "doi": "https://doi.org/10.1145/3014165",
    "publication_date": "2016-12-16",
    "publication_year": 2016,
    "authors": "Erik Tomusk; Christophe Dubach; Michael O’Boyle",
    "corresponding_authors": "",
    "abstract": "Mobile devices with heterogeneous processors are becoming mainstream. With a heterogeneous processor, the runtime scheduler can pick the best CPU core for a given task based on program characteristics, performance requirements, and power limitations. For a heterogeneous processor to be effective, it must contain a diverse set of cores to match a range of runtime requirements and program behaviors. Selecting a diverse set of cores is, however, a non-trivial problem. Power and performance are dependent on both program features and the microarchitectural features of cores, and a selection of cores must satisfy the competing demands of different types of programs. We present a method of core selection that chooses cores at a range of power-performance points. Our algorithm is based on the observation that it is not necessary for a core to consistently have high performance or low power; one type of core can fulfill different roles for different types of programs. Given a power budget, cores selected with our method provide an average speedup of 6% on EEMBC mobile benchmarks and a 24% speedup on SPEC 2006 integer benchmarks over the state-of-the-art core selection method.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2561945387",
    "type": "article"
  },
  {
    "title": "An FPGA-based Approach to Evaluate Thermal and Resource Management Strategies of Many-core Processors",
    "doi": "https://doi.org/10.1145/3516825",
    "publication_date": "2022-02-23",
    "publication_year": 2022,
    "authors": "Marcel Mettler; Martin Rapp; Heba Khdr; Daniel Mueller-Gritschneder; Jörg Henkel; Ulf Schlichtmann",
    "corresponding_authors": "",
    "abstract": "The continuous technology scaling of integrated circuits results in increasingly higher power densities and operating temperatures. Hence, modern many-core processors require sophisticated thermal and resource management strategies to mitigate these undesirable side effects. A simulation-based evaluation of these strategies is limited by the accuracy of the underlying processor model and the simulation speed. Therefore, we present, for the first time, an field-programmable gate array (FPGA)-based evaluation approach to test and compare thermal and resource management strategies using the combination of benchmark generation, FPGA-based application-specific integrated circuit (ASIC) emulation, and run-time monitoring. The proposed benchmark generation method enables an evaluation of run-time management strategies for applications with various run-time characteristics. Furthermore, the ASIC emulation platform features a novel distributed temperature emulator design, whose overhead scales linearly with the number of integrated cores, and a novel dynamic voltage frequency scaling emulator design, which precisely models the timing and energy overhead of voltage and frequency transitions. In our evaluations, we demonstrate the proposed approach for a tiled many-core processor with 80 cores on four Virtex-7 FPGAs. Additionally, we present the suitability of the platform to evaluate state-of-the-art run-time management techniques with a case study.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4213427189",
    "type": "article"
  },
  {
    "title": "PowerMorph: QoS-Aware Server Power Reshaping for Data Center Regulation Service",
    "doi": "https://doi.org/10.1145/3524129",
    "publication_date": "2022-03-21",
    "publication_year": 2022,
    "authors": "Ali Jahanshahi; Nanpeng Yu; Daniel Wong",
    "corresponding_authors": "",
    "abstract": "Adoption of renewable energy in power grids introduces stability challenges in regulating the operation frequency of the electricity grid. Thus, electrical grid operators call for provisioning of frequency regulation services from end-user customers, such as data centers, to help balance the power grid’s stability by dynamically adjusting their energy consumption based on the power grid’s need. As renewable energy adoption grows, the average reward price of frequency regulation services has become much higher than that of the electricity cost. Therefore, there is a great cost incentive for data centers to provide frequency regulation service. Many existing techniques modulating data center power result in significant performance slowdown or provide a low amount of frequency regulation provision. We present PowerMorph , a tight QoS-aware data center power-reshaping framework, which enables commodity servers to provide practical frequency regulation service. The key behind PowerMorph is using “complementary workload” as an additional knob to modulate server power, which provides high provision capacity while satisfying tight QoS constraints of latency-critical workloads. We achieve up to 58% improvement to TCO under common conditions, and in certain cases can even completely eliminate the data center electricity bill and provide a net profit.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4220823244",
    "type": "article"
  },
  {
    "title": "Triangle Dropping: An Occluded-geometry Predictor for Energy-efficient Mobile GPUs",
    "doi": "https://doi.org/10.1145/3527861",
    "publication_date": "2022-04-01",
    "publication_year": 2022,
    "authors": "David Corbalán-Navarro; Juan L. Aragón; Martí Anglada; Joan-Manuel Parcerisa; Antonio González",
    "corresponding_authors": "",
    "abstract": "This article proposes a novel micro-architecture approach for mobile GPUs aimed at early removing the occluded geometry in a scene by leveraging frame-to-frame coherence, thus reducing the overall energy consumption. Mobile GPUs commonly implement a Tile-Based Rendering (TBR) architecture that differentiates two main phases: the Geometry Pipeline , where all the geometry of a scene is processed; and the Raster Pipeline , where primitives are rendered in a framebuffer. After the Geometry Pipeline, only non-culled primitives inside the camera’s frustum are stored into the Parameter Buffer , a data structure stored in DRAM. However, among the non-culled primitives there is a significant amount that are rendered but non-visible at all , resulting in useless computations. On average, 60% of those primitives are completely occluded in our benchmarks. Despite TBR architectures use on-chip caches for the Parameter Buffer, about 46% of the DRAM traffic still comes from accesses to such buffer. The proposed Triangle Dropping technique leverages the visibility information computed along the Raster Pipeline to predict the primitives’ visibility in the next frame to early discard those that will be totally occluded, drastically reducing Parameter Buffer accesses. On average, our approach achieves overall 14.5% energy savings, 28.2% energy-delay product savings, and a speedup of 20.2%.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4221070100",
    "type": "article"
  },
  {
    "title": "A Pressure-Aware Policy for Contention Minimization on Multicore Systems",
    "doi": "https://doi.org/10.1145/3524616",
    "publication_date": "2022-05-25",
    "publication_year": 2022,
    "authors": "Shivam Kundan; Theodoros Marinakis; Iraklis Anagnostopoulos; Dimitri Kagaris",
    "corresponding_authors": "",
    "abstract": "Modern Chip Multiprocessors (CMPs) are integrating an increasing amount of cores to address the continually growing demand for high-application performance. The cores of a CMP share several components of the memory hierarchy, such as Last-Level Cache (LLC) and main memory. This allows for considerable gains in multithreaded applications while also helping to maintain architectural simplicity. However, sharing resources can also result in performance bottleneck due to contention among concurrently executing applications. In this work, we formulate a fine-grained application characterization methodology that leverages Performance Monitoring Counters (PMCs) and Cache Monitoring Technology (CMT) in Intel processors. We utilize this characterization methodology to develop two contention-aware scheduling policies, one static and one dynamic , that co-schedule applications based on their resource-interference profiles. Our approach focuses on minimizing contention on both the main-memory bandwidth and the LLC by monitoring the pressure that each application inflicts on these resources. We achieve performance benefits for diverse workloads, outperforming Linux and three state-of-the-art contention-aware schedulers in terms of system throughput and fairness for both single and multithreaded workloads. Compared with Linux, our policy achieves up to 16% greater throughput for single-threaded and up to 40% greater throughput for multithreaded applications. Additionally, the policies increase fairness by up to 65% for single-threaded and up to 130% for multithreaded ones.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4281476394",
    "type": "article"
  },
  {
    "title": "COX : Exposing CUDA Warp-level Functions to CPUs",
    "doi": "https://doi.org/10.1145/3554736",
    "publication_date": "2022-08-02",
    "publication_year": 2022,
    "authors": "Ruobing Han; Jaewon Lee; Jaewoong Sim; Hyesoon Kim",
    "corresponding_authors": "",
    "abstract": "As CUDA becomes the de facto programming language among data parallel applications such as high-performance computing or machine learning applications, running CUDA on other platforms becomes a compelling option. Although several efforts have attempted to support CUDA on devices other than NVIDIA GPUs, due to extra steps in the translation, the support is always a few years behind CUDA’s latest features. In particular, the new CUDA programming model exposes the warp concept in the programming language, which greatly changes the way the CUDA code should be mapped to CPU programs. In this article, hierarchical collapsing that correctly supports CUDA warp-level functions on CPUs is proposed. To verify hierarchical collapsing , we build a framework, COX , that supports executing CUDA source code on the CPU backend. With hierarchical collapsing , 90% of kernels in CUDA SDK samples can be executed on CPUs, much higher than previous works (68%). We also evaluate the performance with benchmarks for real applications and show that hierarchical collapsing can generate CPU programs with comparable or even higher performance than previous projects in general.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4289522463",
    "type": "article"
  },
  {
    "title": "ASA: <u>A</u> ccelerating <u>S</u> parse <u>A</u> ccumulation in Column-wise SpGEMM",
    "doi": "https://doi.org/10.1145/3543068",
    "publication_date": "2022-06-11",
    "publication_year": 2022,
    "authors": "Chao Zhang; Maximilian Bremer; Cy Chan; John Shalf; Xiaochen Guo",
    "corresponding_authors": "",
    "abstract": "Sparse linear algebra is an important kernel in many different applications. Among various sparse general matrix-matrix multiplication (SpGEMM) algorithms, Gustavson’s column-wise SpGEMM has good locality when reading input matrix and can be easily parallelized by distributing the computation of different columns of an output matrix to different processors. However, the sparse accumulation (SPA) step in column-wise SpGEMM, which merges partial sums from each of the multiplications by the row indices, is still a performance bottleneck. The state-of-the-art software implementation uses a hash table for partial sum search in the SPA, which makes SPA the largest contributor to the execution time of SpGEMM. There are three reasons that cause the SPA to become the bottleneck: (1) hash probing requires data-dependent branches that are difficult for a branch predictor to predict correctly; (2) the accumulation of partial sum is dependent on the results of the hash probing, which makes it difficult to hide the hash probing latency; and (3) hash collision requires time-consuming linear search and optimizations to reduce these collisions require an accurate estimation of the number of non-zeros in each column of the output matrix. This work proposes ASA architecture to accelerate the SPA. ASA overcomes the challenges of SPA by (1) executing the partial sum search and accumulate with a single instruction through ISA extension to eliminate data-dependent branches in hash probing, (2) using a dedicated on-chip cache to perform the search and accumulation in a pipelined fashion, (3) relying on the parallel search capability of a set-associative cache to reduce search latency, and (4) delaying the merging of overflowed entries. As a result, ASA achieves an average of 2.25× and 5.05× speedup as compared to the state-of-the-art software implementation of a Markov clustering application and its SpGEMM kernel, respectively. As compared to a state-of-the-art hashing accelerator design, ASA achieves an average of 1.95× speedup in the SpGEMM kernel.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4293084046",
    "type": "article"
  },
  {
    "title": "Design and Implementation for Nonblocking Execution in GraphBLAS: Tradeoffs and Performance",
    "doi": "https://doi.org/10.1145/3561652",
    "publication_date": "2022-09-06",
    "publication_year": 2022,
    "authors": "Aristeidis Mastoras; Sotiris Anagnostidis; Albert-Jan N. Yzelman",
    "corresponding_authors": "",
    "abstract": "GraphBLASis a recent standard that allows the expression of graph algorithms in the language of linear algebra and enables automatic code parallelization and optimization. GraphBLAS operations are memory bound and may benefit from data locality optimizations enabled by nonblocking execution. However, nonblocking execution remains under-evaluated. In this article, we present a novel design and implementation that investigates nonblocking execution in GraphBLAS. Lazy evaluation enables runtime optimizations that improve data locality, and dynamic data dependence analysis identifies operations that may reuse data in cache. The nonblocking execution of an arbitrary number of operations results in dynamic parallelism, and the performance of the nonblocking execution depends on two parameters, which are automatically determined, at run-time, based on a proposed analytic model. The evaluation confirms the importance of nonblocking execution for various matrices of three algorithms, by showing up to 4.11× speedup over blocking execution as a result of better cache utilization. The proposed analytic model makes the nonblocking execution reach up to 5.13× speedup over the blocking execution. The fully automatic performance is very close to that obtained by using the best manual configuration for both small and large matrices. Finally, the evaluation includes a comparison with other state-of-the-art frameworks for numerical linear algebra programming that employ parallel execution and similar optimizations to those discussed in this work, and the presented nonblocking execution reaches up to 16.1× speedup over the state of the art.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4294734199",
    "type": "article"
  },
  {
    "title": "Cache Programming for Scientific Loops Using Leases",
    "doi": "https://doi.org/10.1145/3600090",
    "publication_date": "2023-05-31",
    "publication_year": 2023,
    "authors": "Benjamin Reber; Matthew Gould; Alexander H. Kneipp; Fangzhou Liu; Ian Prechtl; Chen Ding; Linlin Chen; Dorin Patru",
    "corresponding_authors": "",
    "abstract": "Cache management is important in exploiting locality and reducing data movement. This article studies a new type of programmable cache called the lease cache. By assigning leases, software exerts the primary control on when and how long data stays in the cache. Previous work has shown an optimal solution for an ideal lease cache. This article develops and evaluates a set of practical solutions for a physical lease cache emulated in FPGA with the full suite of PolyBench benchmarks. Compared to automatic caching, lease programming can further reduce data movement by 10% to over 60% when the data size is 16 times to 3,000 times the cache size, and the techniques in this article realize over 80% of this potential. Moreover, lease programming can reduce data movement by another 0.8% to 20% after polyhedral locality optimization.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4378904312",
    "type": "article"
  },
  {
    "title": "MFFT: A GPU Accelerated Highly Efficient Mixed-Precision Large-Scale FFT Framework",
    "doi": "https://doi.org/10.1145/3605148",
    "publication_date": "2023-06-19",
    "publication_year": 2023,
    "authors": "Yuwen Zhao; Fangfang Liu; Wenjing Ma; Huiyuan Li; Yuanchi Peng; Cui Wang",
    "corresponding_authors": "",
    "abstract": "Fast Fourier transform (FFT) is widely used in computing applications in large-scale parallel programs, and data communication is the main performance bottleneck of FFT and seriously affects its parallel efficiency. To tackle this problem, we propose a new large-scale FFT framework, MFFT, which optimizes parallel FFT with a new mixed-precision optimization technique, adopting the “high precision computation, low precision communication” strategy. To enable “low precision communication”, we propose a shared-exponent floating-point number compression technique, which reduces the volume of data communication, while maintaining higher accuracy. In addition, we apply a two-phase normalization technique to further reduce the round-off error. Based on the mixed-precision MFFT framework, we apply several optimization techniques to improve the performance, such as streaming of GPU kernels, MPI message combination, kernel optimization, and memory optimization. We evaluate MFFT on a system with 4,096 GPUs. The results show that shared-exponent MFFT is 1.23 × faster than that of double-precision MFFT on average, and double-precision MFFT achieves performance 3.53× and 9.48× on average higher than open source library 2Decomp&amp;FFT (CPU-based version) and heFFTe (AMD GPU-based version), respectively. The parallel efficiency of double-precision MFFT increased from 53.2% to 78.1% compared with 2Decomp&amp;FFT, and shared-exponent MFFT further increases the parallel efficiency to 83.8%.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4381251538",
    "type": "article"
  },
  {
    "title": "Extension VM: Interleaved Data Layout in Vector Memory",
    "doi": "https://doi.org/10.1145/3631528",
    "publication_date": "2023-11-07",
    "publication_year": 2023,
    "authors": "Dunbo Zhang; Qingjie Lang; Ruoxi Wang; Li Shen",
    "corresponding_authors": "",
    "abstract": "While vector architecture is widely employed in processors for neural networks, signal processing, and high-performance computing; however, its performance is limited by inefficient column-major memory access. The column-major access limitation originates from the unsuitable mapping of multidimensional data structures to two-dimensional vector memory spaces. In addition, the traditional data layout mapping method creates an irreconcilable conflict between row- and column-major accesses. Ideally, both row- and column-major accesses can take advantage of the bank parallelism of vector memory. To this end, we propose the Interleaved Data Layout (IDL) method in vector memory, which can distribute vector elements into different banks regardless of whether they are in the row- or column-major category, so that any vector memory access can benefit from bank parallelism. Additionally, we propose an Extension Vector Memory (EVM) architecture to achieve IDL in vector memory. EVM can support two data layout methods and vector memory access modes simultaneously. The key idea is to continuously distribute the data that needs to be accessed from the main memory to different banks during the loading period. Thus, EVM can provide a larger spatial locality level through careful programming and the extension ISA support. The experimental results showed a 1.43-fold improvement of state-of-the-art vector processors by the proposed architecture, with an area cost of only 1.73%. Furthermore, the energy consumption was reduced by 50.1%.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4388463752",
    "type": "article"
  },
  {
    "title": "Fine-Grain Quantitative Analysis of Demand Paging in Unified Virtual Memory",
    "doi": "https://doi.org/10.1145/3632953",
    "publication_date": "2023-11-14",
    "publication_year": 2023,
    "authors": "Tyler Allen; Bennett Cooper; Rong Ge",
    "corresponding_authors": "",
    "abstract": "The abstraction of a shared memory space over separate CPU and GPU memory domains has eased the burden of portability for many HPC codebases. However, users pay for ease of use provided by system-managed memory with a moderate-to-high performance overhead. NVIDIA Unified Virtual Memory (UVM) is currently the primary real-world implementation of such abstraction and offers a functionally equivalent testbed for in-depth performance study for both UVM and future Linux Heterogeneous Memory Management (HMM) compatible systems. The continued advocacy for UVM and HMM motivates improvement of the underlying system. We focus on UVM-based systems and investigate the root causes of UVM overhead, a non-trivial task due to complex interactions of multiple hardware and software constituents and the desired cost granularity. In our prior work, we delved deeply into UVM system architecture and showed internal behaviors of page fault servicing in batches. We provided quantitative evaluation of batch handling for various applications under different scenarios, including prefetching and oversubscription. We revealed that the driver workload depends on the interactions among application access patterns, GPU hardware constraints, and host OS components. Host OS components have significant overhead present across implementations, warranting close attention. This extension furthers our prior study in three aspects: fine-grain cost analysis and breakdown, extension to multiple GPUs, and investigation of platforms with different GPU-GPU interconnects. We take a top-down approach to quantitative batch analysis and uncover how constituent component costs accumulate and overlap, governed by synchronous and asynchronous operations. Our multi-GPU analysis shows reduced cost of GPU-GPU batch workloads compared to CPU-GPU workloads. We further demonstrate that while specialized interconnects, NVLink, can improve batch cost, their benefits are limited by host OS software overhead and GPU oversubscription. This study serves as a proxy for future shared memory systems, such as those that interface with HMM, and the development of interconnects.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4388671996",
    "type": "article"
  },
  {
    "title": "COWS for High Performance: Cost Aware Work Stealing for Irregular Parallel Loop",
    "doi": "https://doi.org/10.1145/3633331",
    "publication_date": "2023-11-18",
    "publication_year": 2023,
    "authors": "Prasoon Mishra; V. Krishna Nandivada",
    "corresponding_authors": "",
    "abstract": "Parallel libraries such as OpenMP distribute the iterations of parallel-for-loops among the threads, using a programmer-specified scheduling policy. While the existing scheduling policies perform reasonably well in the context of balanced workloads, in computations that involve highly imbalanced workloads it is extremely non-trivial to obtain an efficient distribution of work (even using non-static scheduling methods like dynamic and guided). In this paper, we present a scheme called COst aware Work Stealing (COWS) to efficiently extend the idea of work-stealing to OpenMP. In contrast to the traditional work-stealing schedulers, COWS takes into consideration that (i) not all iterations of a parallel-for-loops may take the same amount of time. (ii) identifying a suitable victim for stealing is important for load-balancing, and (iii) queues lead to significant overheads in traditional work-stealing and should be avoided. We present two variations of COWS: WSRI (a naive work-stealing scheme based on the number of remaining iterations) and WSRW (work-stealing scheme based on the amount of remaining workload). Since in irregular loops like those found in graph analytics it is not possible to statically compute the cost of the iterations of the parallel-for-loops, we use a combined compile-time + runtime approach, where the remaining workload of a loop is computed efficiently at runtime by utilizing the code generated by our compile-time component. We have performed an evaluation over seven different benchmark programs, using five different input datasets, on two different hardware across a varying number of threads; leading to a total number of 275 configurations. We show that in 225 out of 275 configurations, compared to the best OpenMP scheduling scheme for that configuration, our approach achieves clear performance gains.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4388798965",
    "type": "article"
  },
  {
    "title": "Exploring Data Layout for Sparse Tensor Times Dense Matrix on GPUs",
    "doi": "https://doi.org/10.1145/3633462",
    "publication_date": "2023-12-28",
    "publication_year": 2023,
    "authors": "Khalid Ahmad; Cris Cecka; Michael Garland; Mary Hall",
    "corresponding_authors": "",
    "abstract": "An important sparse tensor computation is sparse-tensor-dense-matrix multiplication (SpTM), which is used in tensor decomposition and applications. SpTM is a multi-dimensional analog to sparse-matrix-dense-matrix multiplication (SpMM). In this article, we employ a hierarchical tensor data layout that can unfold a multidimensional tensor to derive a 2D matrix, making it possible to compute SpTM using SpMM kernel implementations for GPUs. We compare two SpMM implementations to the state-of-the-art PASTA sparse tensor contraction implementation using: (1) SpMM with hierarchical tensor data layout; and, (2) unfolding followed by an invocation of cuSPARSE’s SpMM. Results show that SpMM can outperform PASTA 70.9% of the time, but none of the three approaches is best overall. Therefore, we use a decision tree classifier to identify the best performing sparse tensor contraction kernel based on precomputed properties of the sparse tensor.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4390317766",
    "type": "article"
  },
  {
    "title": "A low-complexity fetch architecture for high-performance superscalar processors",
    "doi": "https://doi.org/10.1145/1011528.1011532",
    "publication_date": "2004-06-01",
    "publication_year": 2004,
    "authors": "Oliverio J. Santana; Alex Ramírez; Josep-L. Larriba-Pey; Mateo Valero",
    "corresponding_authors": "",
    "abstract": "Fetch engine performance is a key topic in superscalar processors, since it limits the instruction-level parallelism that can be exploited by the execution core. In the search of high performance, the fetch engine has evolved toward more efficient designs, but its complexity has also increased.In this paper, we present the stream fetch engine, a novel architecture based on the execution of long streams of sequential instructions, taking maximum advantage of code layout optimizations. We describe our design in detail, showing that it achieves high fetch performance, while requiring less complexity than other state-of-the-art fetch architectures.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2081487206",
    "type": "article"
  },
  {
    "title": "An approach toward profit-driven optimization",
    "doi": "https://doi.org/10.1145/1162690.1162691",
    "publication_date": "2006-09-01",
    "publication_year": 2006,
    "authors": "Min Zhao; Bruce R. Childers; Mary Lou Soffa",
    "corresponding_authors": "",
    "abstract": "Although optimizations have been applied for a number of years to improve the performance of software, problems with respect to the application of optimizations have not been adequately addressed. For example, in certain circumstances, optimizations may degrade performance. However, there is no efficient way to know when a degradation will occur. In this research, we investigate the profitability of optimizations, which is useful for determining the benefit of applying optimizations. We develop a framework that enables us to predict profitability using analytic models. The profitability of an optimization depends on code context, the particular optimization, and machine resources. Thus, our framework has analytic models for each of these components. As part of the framework, there is also a profitability engine that uses models to predict the profit. In this paper, we target scalar optimizations and, in particular, describe the models for partial redundancy elimination (PRE), loop invariant code motion (LICM), and value numbering (VN). We implemented the framework for predicting the profitability of these optimizations. Based on the predictions, we can selectively apply profitable optimizations. We compared the profit-driven approach with an approach that uses a heuristic in deciding when optimizations should be applied. Our experiments demonstrate that the profitability of scalar optimizations can be accurately predicted by using models. That is, without actually applying a scalar optimization, we can determine if an optimization is beneficial and should be applied.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2046311694",
    "type": "article"
  },
  {
    "title": "Hiding the misprediction penalty of a resource-efficient high-performance processor",
    "doi": "https://doi.org/10.1145/1328195.1328201",
    "publication_date": "2008-01-01",
    "publication_year": 2008,
    "authors": "Amit Golander; Shlomo Weiss",
    "corresponding_authors": "",
    "abstract": "Misprediction is a major obstacle for increasing speculative out-of-order processors performance. Performance degradation depends on both the number of misprediction events and the recovery time associated with each one of them. In recent years a few checkpoint based microarchitectures have been proposed. In comparison with ROB-based processors, checkpoint processors are scalable and highly resource efficient. Unfortunately, in these proposals the misprediction recovery time is proportional to the instruction queue size. In this paper we analyze methods to reduce the misprediction recovery time. We propose a new register file management scheme and techniques to selectively flush the instruction queue and the load store queue, and to isolate deeply pipelined execution units. The result is a novel checkpoint processor with Constant misprediction RollBack time (CRB). We further present a streamlined, cost-efficient solution, which saves complexity at the price of slightly lower performance.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W1994386599",
    "type": "article"
  },
  {
    "title": "Software-Directed Techniques for Improved GPU Register File Utilization",
    "doi": "https://doi.org/10.1145/3243905",
    "publication_date": "2018-09-24",
    "publication_year": 2018,
    "authors": "Dani Voitsechov; Arslan Zulfiqar; Mark W. Stephenson; Mark Gebhart; Stephen W. Keckler",
    "corresponding_authors": "",
    "abstract": "Throughput architectures such as GPUs require substantial hardware resources to hold the state of a massive number of simultaneously executing threads. While GPU register files are already enormous, reaching capacities of 256KB per streaming multiprocessor (SM), we find that nearly half of real-world applications we examined are register-bound and would benefit from a larger register file to enable more concurrent threads. This article seeks to increase the thread occupancy and improve performance of these register-bound applications by making more efficient use of the existing register file capacity. Our first technique eagerly deallocates register resources during execution. We show that releasing register resources based on value liveness as proposed in prior states of the art leads to unreliable performance and undue design complexity. To address these deficiencies, our article presents a novel compiler-driven approach that identifies and exploits last use of a register name (instead of the value contained within) to eagerly release register resources. Furthermore, while previous works have leveraged “scalar” and “narrow” operand properties of a program for various optimizations, their impact on thread occupancy has been relatively unexplored. Our article evaluates the effectiveness of these techniques in improving thread occupancy and demonstrates that while any one approach may fail to free very many registers, together they synergistically free enough registers to launch additional parallel work. An in-depth evaluation on a large suite of applications shows that just our early register technique outperforms previous work on dynamic register allocation, and together these approaches, on average, provide 12% performance speedup (23% higher thread occupancy) on register bound applications not already saturating other GPU resources.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2892817096",
    "type": "article"
  },
  {
    "title": "SCP",
    "doi": "https://doi.org/10.1145/3274654",
    "publication_date": "2018-10-10",
    "publication_year": 2018,
    "authors": "Xing Su; Xiangke Liao; Hao Jiang; Canqun Yang; Jingling Xue",
    "corresponding_authors": "",
    "abstract": "GEneral Matrix Multiply (GEMM) is the most fundamental computational kernel routine in the BLAS library. To achieve high performance, in-memory data must be prefetched into fast on-chip caches before they are used. Two techniques, software prefetching and data packing, have been used to effectively exploit the capability of on-chip least recent used (LRU) caches, which are popular in traditional high-performance processors used in high-end servers and supercomputers. However, the market has recently witnessed a new diversity in processor design, resulting in high-performance processors equipped with shared caches with non-LRU replacement policies. This poses a challenge to the development of high-performance GEMM in a multithreaded context. As several threads try to load data into a shared cache simultaneously, interthread cache conflicts will increase significantly. We present a Shared Cache Partitioning (SCP) method to eliminate interthread cache conflicts in the GEMM routines, by partitioning a shared cache into physically disjoint sets and assigning different sets to different threads. We have implemented SCP in the OpenBLAS library and evaluated it on Phytium 2000+, a 64-core AArch64 processor with private LRU L1 caches and shared pseudo-random L2 caches (per four-core cluster). Our evaluation shows that SCP has effectively reduced the conflict misses in both L1 and L2 caches in a highly optimized GEMM implementation, resulting in an improvement of its performance by 2.75% to 6.91%.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2898890919",
    "type": "article"
  },
  {
    "title": "MetaStrider",
    "doi": "https://doi.org/10.1145/3355396",
    "publication_date": "2019-10-11",
    "publication_year": 2019,
    "authors": "Sriseshan Srikanth; Anirudh Jain; Joseph M. Lennon; Thomas M. Conte; Erik P. DeBenedictis; Jeanine Cook",
    "corresponding_authors": "",
    "abstract": "Reduction is an operation performed on the values of two or more key-value pairs that share the same key. Reduction of sparse data streams finds application in a wide variety of domains such as data and graph analytics, cybersecurity, machine learning, and HPC applications. However, these applications exhibit low locality of reference, rendering traditional architectures and data representations inefficient. This article presents MetaStrider, a significant algorithmic and architectural enhancement to the state-of-the-art, SuperStrider. Furthermore, these enhancements enable a variety of parallel, memory-centric architectures that we propose, resulting in demonstrated performance that scales near-linearly with available memory-level parallelism.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2979707853",
    "type": "article"
  },
  {
    "title": "Compiler-support for Critical Data Persistence in NVM",
    "doi": "https://doi.org/10.1145/3371236",
    "publication_date": "2019-12-26",
    "publication_year": 2019,
    "authors": "Reem Elkhouly; Mohammad Alshboul; Akihiro Hayashi; Yan Solihin; Keiji Kimura",
    "corresponding_authors": "",
    "abstract": "Non-volatile Main Memories (NVMs) offer a promising way to preserve data persistence and enable computation recovery in case of failure. While the use of NVMs can significantly reduce the overhead of failure recovery, which is the case with High-Performance Computing (HPC) kernels, rewriting existing programs or writing new applications for NVMs is non-trivial. In this article, we present a compiler-support that automatically inserts complex instructions into kernels to achieve NVM data-persistence based on a simple programmer directive. Unlike checkpointing techniques that store the whole system state, our technique only persists user-designated objects as well as some parameters required for safe recovery such as loop induction variables. Also, our technique can reduce the number of data transfer operations, because our compiler coalesces consecutive memory-persisting operations into a single memory transaction per cache line when possible. Our compiler-support is implemented in the LLVM tool-chain and introduces the necessary modifications to loop-intensive computational kernels (e.g., TMM, LU, Gauss, and FFT) to force data persistence. The experiments show that our proposed compiler-support outperforms the most recent checkpointing techniques while its performance overheads are insignificant.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2998229282",
    "type": "article"
  },
  {
    "title": "On Architectural Support for Instruction Set Randomization",
    "doi": "https://doi.org/10.1145/3419841",
    "publication_date": "2020-11-10",
    "publication_year": 2020,
    "authors": "George Christou; Giorgos Vasiliadis; Vassilis Papaefstathiou; Antonis Papadogiannakis; Sotiris Ioannidis",
    "corresponding_authors": "",
    "abstract": "Instruction Set Randomization (ISR) is able to protect against remote code injection attacks by randomizing the instruction set of each process. Thereby, even if an attacker succeeds to inject code, it will fail to execute on the randomized processor. The majority of existing ISR implementations is based on emulators and binary instrumentation tools that unfortunately: (i) incur significant runtime performance overheads, (ii) limit the ease of deployment, (iii) cannot protect the underlying operating system kernel, and (iv) are vulnerable to evasion attempts that bypass the ISR protection itself. To address these issues, we present the design and implementation of ASIST, an architecture with both hardware and operating system support for ISR. ASIST uses our extended SPARC processor that is mapped onto a FPGA board and runs our modified Linux kernel to support the new features. In particular, before executing a new user-level process, the operating system loads its randomization key into a newly defined register, and the modified processor decodes the process’s instructions with this key. Besides that, ASIST uses a separate randomization key for the operating system to protect the base system against attacks that exploit kernel vulnerabilities to run arbitrary code with elevated privileges. Our evaluation shows that ASIST can transparently protect both user-land applications and the operating system kernel from code injection and code reuse attacks, with about 1.5% runtime overhead when using simple encryption schemes, such as XOR and Transposition; more secure ciphers, such as AES, even though they are much more complicated for mapping them to hardware, they are still within acceptable margins,with approximately 10% runtime overhead, when efficiently leveraging the spatial locality of code through modern instruction cache configurations.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3101626044",
    "type": "article"
  },
  {
    "title": "FastPath_MP",
    "doi": "https://doi.org/10.1145/3423134",
    "publication_date": "2020-11-25",
    "publication_year": 2020,
    "authors": "Athanasios Stratikopoulos; Christos Kotselidis; John Goodacre; Mikel Luján",
    "corresponding_authors": "",
    "abstract": "In this article, we present FastPath_MP, a novel low-overhead and energy-efficient storage multi-path architecture that leverages FPGAs to operate transparently to the main processor and improve the performance and energy efficiency of accessing storage devices. We prototyped FastPath_MP on both Arm-FPGA Zynq 7000 SoC and Zynq UltraScale+ MPSoC and evaluated its performance against standard microbenchmarks as well as the real-world in-memory Redis database. Our results show that FastPath_MP achieves up to 82% lower latency, up to 12× higher throughput, and up to 10× more energy efficiency against the baseline storage path of the Linux kernel.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3108735728",
    "type": "article"
  },
  {
    "title": "Reliability-aware Garbage Collection for Hybrid HBM-DRAM Memories",
    "doi": "https://doi.org/10.1145/3431803",
    "publication_date": "2021-01-20",
    "publication_year": 2021,
    "authors": "Wenjie Liu; Shoaib Akram; Jennifer B. Sartor; Lieven Eeckhout",
    "corresponding_authors": "",
    "abstract": "Emerging workloads in cloud and data center infrastructures demand high main memory bandwidth and capacity. Unfortunately, DRAM alone is unable to satisfy contemporary main memory demands. High-bandwidth memory (HBM) uses 3D die-stacking to deliver 4–8× higher bandwidth. HBM has two drawbacks: (1) capacity is low, and (2) soft error rate is high. Hybrid memory combines DRAM and HBM to promise low fault rates, high bandwidth, and high capacity. Prior OS approaches manage HBM by mapping pages to HBM versus DRAM based on hotness (access frequency) and risk (susceptibility to soft errors). Unfortunately, these approaches operate at a coarse-grained page granularity, and frequent page migrations hurt performance. This article proposes a new class of reliability-aware garbage collectors for hybrid HBM-DRAM systems that place hot and low-risk objects in HBM and the rest in DRAM. Our analysis of nine real-world Java workloads shows that: (1) newly allocated objects in the nursery are frequently written, making them both hot and low-risk, (2) a small fraction of the mature objects are hot and low-risk, and (3) allocation site is a good predictor for hotness and risk. We propose RiskRelief, a novel reliability-aware garbage collector that uses allocation site prediction to place hot and low-risk objects in HBM. Allocation sites are profiled offline and RiskRelief uses heuristics to classify allocation sites as DRAM and HBM. The proposed heuristics expose Pareto-optimal trade-offs between soft error rate (SER) and execution time. RiskRelief improves SER by 9× compared to an HBM-Only system while at the same time improving performance by 29% compared to a DRAM-Only system. Compared to a state-of-the-art OS approach for reliability-aware data placement, RiskRelief eliminates all page migration overheads, which substantially improves performance while delivering similar SER. Reliability-aware garbage collection opens up a new opportunity to manage emerging HBM-DRAM memories at fine granularity while requiring no extra hardware support and leaving the programming model unchanged.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3125828735",
    "type": "article"
  },
  {
    "title": "GPU Domain Specialization via Composable On-Package Architecture",
    "doi": "https://doi.org/10.1145/3484505",
    "publication_date": "2021-12-06",
    "publication_year": 2021,
    "authors": "Yaosheng Fu; Evgeny Bolotin; Niladrish Chatterjee; David Nellans; Stephen W. Keckler",
    "corresponding_authors": "",
    "abstract": "As GPUs scale their low-precision matrix math throughput to boost deep learning (DL) performance, they upset the balance between math throughput and memory system capabilities. We demonstrate that a converged GPU design trying to address diverging architectural requirements between FP32 (or larger)-based HPC and FP16 (or smaller)-based DL workloads results in sub-optimal configurations for either of the application domains. We argue that a C omposable O n- PA ckage GPU (COPA-GPU) architecture to provide domain-specialized GPU products is the most practical solution to these diverging requirements. A COPA-GPU leverages multi-chip-module disaggregation to support maximal design reuse, along with memory system specialization per application domain. We show how a COPA-GPU enables DL-specialized products by modular augmentation of the baseline GPU architecture with up to 4× higher off-die bandwidth, 32× larger on-package cache, and 2.3× higher DRAM bandwidth and capacity, while conveniently supporting scaled-down HPC-oriented designs. This work explores the microarchitectural design necessary to enable composable GPUs and evaluates the benefits composability can provide to HPC, DL training, and DL inference. We show that when compared to a converged GPU design, a DL-optimized COPA-GPU featuring a combination of 16× larger cache capacity and 1.6× higher DRAM bandwidth scales per-GPU training and inference performance by 31% and 35%, respectively, and reduces the number of GPU instances by 50% in scale-out training scenarios.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3142875983",
    "type": "article"
  },
  {
    "title": "MC-DeF",
    "doi": "https://doi.org/10.1145/3447970",
    "publication_date": "2021-04-14",
    "publication_year": 2021,
    "authors": "George Charitopoulos; Dionisios Pnevmatikatos; Georgi Gaydadjiev",
    "corresponding_authors": "",
    "abstract": "Executing complex scientific applications on Coarse-Grain Reconfigurable Arrays ( CGRAs ) promises improvements in execution time and/or energy consumption compared to optimized software implementations or even fully customized hardware solutions. Typical CGRA architectures contain of multiple instances of the same compute module that consist of simple and general hardware units such as ALUs, simple processors. However, generality in the cell contents, while convenient for serving a wide variety of applications, penalizes performance and energy efficiency. To that end, a few proposed CGRAs use custom logic tailored to a particular application’s specific characteristics in the compute module. This approach, while much more efficient, restricts the versatility of the array. To date, versatility at hardware speeds is only supported with Field programmable gate arrays (FPGAs), that are reconfigurable at a very fine grain. This work proposes MC-DeF, a novel Mixed-CGRA Definition Framework targeting a Mixed-CGRA architecture that leverages the advantages of CGRAs by utilizing a customized cell array, and those of FPGAs by incorporating a separate LUT array used for adaptability. The framework presented aims to develop a complete CGRA architecture. First, a cell structure and functionality definition phase creates highly customized application/domain specific CGRA cells. Then, mapping and routing phases define the CGRA connectivity and cell-LUT array transactions. Finally, an energy and area estimation phase presents the user with area occupancy and energy consumption estimations of the final design. MC-DeF uses novel algorithms and cost functions driven by user defined metrics, threshold values, and area/energy restrictions. The benefits of our framework, besides creating fast and efficient CGRA designs, include design space exploration capabilities offered to the user. The validity of the presented framework is demonstrated by evaluating and creating CGRA designs of nine applications. Additionally, we provide comparisons of MC-DeF with state-of-the-art related works, and show that MC-DeF offers competitive performance (in terms of internal bandwidth and processing throughput) even compared against much larger designs, and requires fewer physical resources to achieve this level of performance. Finally, MC-DeF is able to better utilize the underlying FPGA fabric and achieves the best efficiency (measured in LUT/GOPs).",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3152726680",
    "type": "article"
  },
  {
    "title": "CATCH",
    "doi": "https://doi.org/10.1145/2019608.2019610",
    "publication_date": "2011-10-01",
    "publication_year": 2011,
    "authors": "Marios Kleanthous; Yiannakis Sazeides",
    "corresponding_authors": "",
    "abstract": "Cache-content-duplication (CCD) occurs when there is a miss for a block in a cache and the entire content of the missed block is already in the cache in a block with a different tag. Caches aware of content-duplication can have lower miss penalty by fetching, on a miss to a duplicate block, directly from the cache instead of accessing lower in the memory hierarchy, and can have lower miss rates by allowing only blocks with unique content to enter a cache. This work examines the potential of CCD for instruction caches. We show that CCD is a frequent phenomenon and that an idealized duplication-detection mechanism for instruction caches has the potential to increase performance of an out-of-order processor, with a 16KB, 8-way, 8 instructions per block instruction cache, often by more than 10% and up to 36%. This work also proposes CATCH, a hardware mechanism for dynamically detecting CCD for instruction caches. Experimental results for an out-of-order processor show that a duplication-detection mechanism with a 1.38KB cost captures on average 58% of the CCD's idealized potential.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W1985671208",
    "type": "article"
  },
  {
    "title": "API compilation for image hardware accelerators",
    "doi": "https://doi.org/10.1145/2400682.2400708",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Fabien Coelho; François Irigoin",
    "corresponding_authors": "",
    "abstract": "We present an API-based compilation strategy to optimize image applications, developed using a high-level image processing library, onto three different image processing hardware accelerators. We demonstrate that such a strategy is profitable for both development cost and overall performance, especially as it takes advantage of optimization opportunities across library calls otherwise beyond reach. The library API provides the semantics of the image computations. The three image accelerator targets are quite distinct: the first one uses a vector architecture; the second one presents an SIMD architecture; the last one runs both on GPGPU and multicores through OpenCL. We have adapted standard compilation techniques to perform these compilation and code generation tasks automatically. Our strategy is implemented in PIPS, a source-to-source compiler which greatly reduces the development cost as standard phases are reused and parameterized. We carried out experiments with applications on hardware functional simulators and GPUs. Our contributions include: (1) a general low-cost compilation strategy for image processing applications, based on the semantics provided by library calls, which improves locality by an order of magnitude; (2) specific heuristics to minimize execution time on the target accelerators; (3) numerous experiments that show the effectiveness of our strategies. We also discuss the conditions required to extend this approach to other application domains.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2000809992",
    "type": "article"
  },
  {
    "title": "FlexSig",
    "doi": "https://doi.org/10.1145/2086696.2086709",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Lois Orosa; E. Antelo; J.D. Bruguera",
    "corresponding_authors": "",
    "abstract": "With the advent of chip multiprocessors, new techniques have been developed to make parallel programing easier and more reliable. New parallel programing paradigms and new methods of making the execution of programs more efficient and more reliable have been developed. Usually, these improvements require hardware support to avoid a system slowdown. Signatures based on Bloom filters are widely used as hardware support for parallel programing in chip multiprocessors. Signatures are used in Transactional Memory, thread-level speculation, parallel debugging, deterministic replay and other tools and applications. The main limitation of hardware signatures is the lack of flexibility: if signatures are designed with a given configuration, tailored to the requirements of a specific tool or application, it is likely that they do not fit well for other different requirements. In this paper a new hardware signature organization, called Flexible Signatures ( FlexSig ), is proposed. FlexSig can change dynamically the resources assigned to a given signature and the number of signatures in the system, by redistributing the available hardware resources according to the system requirements. This allows higher flexibility than with traditional fixed-resources signatures based on Bloom filters, while maintaining a low false positive rate. FlexSig has been evaluated by comparing it with signatures based on parallel Bloom filters, and we conclude that FlexSig outperforms (in terms of false positive rate) conventional parallel Bloom filters in most cases, due to its ability to use all the signature resources available.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2003488473",
    "type": "article"
  },
  {
    "title": "GPU code generation for ODE-based applications with phased shared-data access patterns",
    "doi": "https://doi.org/10.1145/2541228.2555311",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Andrei Hagiescu; Bing Liu; Ravishankar Ramanathan; Sucheendra K. Palaniappan; Zheng Cui; Bipasa Chattopadhyay; P. S. Thiagarajan; Weng‐Fai Wong",
    "corresponding_authors": "",
    "abstract": "We present a novel code generation scheme for GPUs. Its key feature is the platform-aware generation of a heterogeneous pool of threads. This exposes more data-sharing opportunities among the concurrent threads and reduces the memory requirements that would otherwise exceed the capacity of the on-chip memory. Instead of the conventional strategy of focusing on exposing as much parallelism as possible, our scheme leverages on the phased nature of memory access patterns found in many applications that exhibit massive parallelism. We demonstrate the effectiveness of our code generation strategy on a computational systems biology application. This application consists of computing a Dynamic Bayesian Network (DBN) approximation of the dynamics of signalling pathways described as a system of Ordinary Differential Equations (ODEs). The approximation algorithm involves (i) sampling many (of the order of a few million) times from the set of initial states, (ii) generating trajectories through numerical integration, and (iii) storing the statistical properties of this set of trajectories in Conditional Probability Tables (CPTs) of a DBN via a prespecified discretization of the time and value domains. The trajectories can be computed in parallel. However, the intermediate data needed for computing them, as well as the entries for the CPTs, are too large to be stored locally. Our experiments show that the proposed code generation scheme scales well, achieving significant performance improvements on three realistic signalling pathways models. These results suggest how our scheme could be extended to deal with other applications involving systems of ODEs.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2064496884",
    "type": "article"
  },
  {
    "title": "Memory optimization of dynamic binary translators for embedded systems",
    "doi": "https://doi.org/10.1145/2355585.2355595",
    "publication_date": "2012-09-01",
    "publication_year": 2012,
    "authors": "Apala Guha; Kim Hazelwood; Mary Lou Soffa",
    "corresponding_authors": "",
    "abstract": "Dynamic binary translators (DBTs) are becoming increasingly important because of their power and flexibility. DBT-based services are valuable for all types of platforms. However, the high memory demands of DBTs present an obstacle for embedded systems. Most research on DBT design has a performance focus, which often drives up the DBT memory demand. In this article, we present a memory-oriented approach to DBT design. We consider the class of translation-based DBTs and their sources of memory demand; cached translated code, cached auxiliary code and DBT data structures. We explore aspects of DBT design that impact these memory demand sources and present strategies to mitigate memory demand. We also explore performance optimizations for DBTs that handle memory demand by placing a limit on it, and repeatedly flush translations to stay within the limit, thereby replacing the memory demand problem with a performance degradation problem. Our optimizations that mitigate memory demand improve performance.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2072554713",
    "type": "article"
  },
  {
    "title": "Profile-guided transaction coalescing—lowering transactional overheads by merging transactions",
    "doi": "https://doi.org/10.1145/2541228.2555306",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Srđan Stipić; Vesna Smiljković; Osman Ünsal; Adrián Cristal; Mateo Valero",
    "corresponding_authors": "",
    "abstract": "Previous studies in software transactional memory mostly focused on reducing the overhead of transactional read and write operations. In this article, we introduce transaction coalescing , a profile-guided compiler optimization technique that attempts to reduce the overheads of starting and committing a transaction by merging two or more small transactions into one large transaction. We develop a profiling tool and a transaction coalescing heuristic to identify candidate transactions suitable for coalescing. We implement a compiler extension to automatically merge the candidate transactions at the compile time. We evaluate the effectiveness of our technique using the hash table micro-benchmark and the STAMP benchmark suite. Transaction coalescing improves the performance of the hash table significantly and the performance of Vacation and SSCA2 benchmarks by 19.4% and 36.4%, respectively, when running with 12 threads.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2120473403",
    "type": "article"
  },
  {
    "title": "On the Interactions Between Value Prediction and Compiler Optimizations in the Context of EOLE",
    "doi": "https://doi.org/10.1145/3090634",
    "publication_date": "2017-06-30",
    "publication_year": 2017,
    "authors": "Fernando A. Endo; Arthur Pérais; André Seznec",
    "corresponding_authors": "",
    "abstract": "Increasing instruction-level parallelism is regaining attractiveness within the microprocessor industry. The {Early | Out-of-order | Late} Execution (EOLE) microarchitecture and Differential Value TAgged GEometric (D-VTAGE) value predictor were recently introduced to solve practical issues of Value Prediction (VP). In particular, they remove the most significant difficulties that forbade an effective VP hardware. In this study, we present a detailed evaluation of the potential of VP in the context of EOLE/D-VTAGE and different compiler options. Our study shows that if no single general rule always applies—more optimization might sometimes lead to more performance—unoptimized codes often get a large benefit from the prediction of redundant loads.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2614012776",
    "type": "article"
  },
  {
    "title": "An Integrated Vector-Scalar Design on an In-Order ARM Core",
    "doi": "https://doi.org/10.1145/3075618",
    "publication_date": "2017-05-26",
    "publication_year": 2017,
    "authors": "Milan Stanić; Oscar Palomar; Timothy Hayes; Ivan Ratković; Adrián Cristal; Osman Ünsal; Mateo Valero",
    "corresponding_authors": "",
    "abstract": "In the low-end mobile processor market, power, energy, and area budgets are significantly lower than in the server/desktop/laptop/high-end mobile markets. It has been shown that vector processors are a highly energy-efficient way to increase performance; however, adding support for them incurs area and power overheads that would not be acceptable for low-end mobile processors. In this work, we propose an integrated vector-scalar design for the ARM architecture that mostly reuses scalar hardware to support the execution of vector instructions. The key element of the design is our proposed block-based model of execution that groups vector computational instructions together to execute them in a coordinated manner. We implemented a classic vector unit and compare its results against our integrated design. Our integrated design improves the performance (more than 6×) and energy consumption (up to 5×) of a scalar in-order core with negligible area overhead (only 4.7% when using a vector register with 32 elements). In contrast, the area overhead of the classic vector unit can be significant (around 44%) if a dedicated vector floating-point unit is incorporated. Our block-based vector execution outperforms the classic vector unit for all kernels with floating-point data and also consumes less energy. We also complement the integrated design with three energy/performance-efficient techniques that further reduce power and increase performance. The first proposal covers the design and implementation of chaining logic that is optimized to work with the cache hierarchy through vector memory instructions, the second proposal reduces the number of reads/writes from/to the vector register file, and the third idea optimizes complex memory access patterns with the memory shape instruction and unified indexed vector load.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2619705232",
    "type": "article"
  },
  {
    "title": "Providing Predictable Performance via a Slowdown Estimation Model",
    "doi": "https://doi.org/10.1145/3124451",
    "publication_date": "2017-08-22",
    "publication_year": 2017,
    "authors": "Dongliang Xiong; Kai Huang; Xiaowen Jiang; Xiaolang Yan",
    "corresponding_authors": "",
    "abstract": "Interapplication interference at shared main memory slows down different applications differently. A few slowdown estimation models have been proposed to provide predictable performance by quantifying memory interference, but they have relatively low accuracy. Thus, we propose a more accurate slowdown estimation model called SEM at main memory. First, SEM unifies the slowdown estimation model by measuring IPC directly. Second, SEM uses the per-bank structure to monitor memory interference and improves estimation accuracy by considering write interference, row-buffer interference, and data bus interference. The evaluation results show that SEM has significantly lower slowdown estimation error (4.06%) compared to STFM (30.15%) and MISE (10.1%).",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2746476565",
    "type": "article"
  },
  {
    "title": "A system architecture, processor, and communication protocol for secure implants",
    "doi": "https://doi.org/10.1145/2555289.2555313",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Christos Strydis; Robert M. Seepers; Pedro Peris-López; Dimitrios Siskos; Ioannis Sourdis",
    "corresponding_authors": "",
    "abstract": "Secure and energy-efficient communication between Implantable Medical Devices (IMDs) and authorized external users is attracting increasing attention these days. However, there currently exists no systematic approach to the problem, while solutions from neighboring fields, such as wireless sensor networks, are not directly transferable due to the peculiarities of the IMD domain. This work describes an original, efficient solution for secure IMD communication. A new implant system architecture is proposed, where security and main-implant functionality are made completely decoupled by running the tasks onto two separate cores. Wireless communication goes through a custom security ASIP, called SISC (Smart-Implant Security Core), which runs an energy-efficient security protocol. The security core is powered by RF-harvested energy until it performs external-reader authentication, providing an elegant defense mechanism against battery Denial-of-Service (DoS) and other, more common attacks. The system has been evaluated based on a realistic case study involving an artificial pancreas implant. When synthesized for a UMC 90nm CMOS ASIC technology, our system architecture achieves defense against unauthorized accesses having zero energy cost, running entity authentication through harvesting only 7.45μJ of RF energy from the requesting entity. In all other successfully authenticated accesses, our architecture achieves secure data exchange without affecting the performance of the main IMD functionality, adding less than 1‰ (1.3mJ) to the daily energy consumption of a typical implant. Compared to a singe-core, secure reference IMD, which would still be more vulnerable to some types of attacks, our secure system on chip (SoC) achieves high security levels at 56% energy savings and at an area overhead of less than 15%.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4235595569",
    "type": "article"
  },
  {
    "title": "Symmetry-Agnostic Coordinated Management of the Memory Hierarchy in Multicore Systems",
    "doi": "https://doi.org/10.1145/2847254",
    "publication_date": "2016-01-04",
    "publication_year": 2016,
    "authors": "Miao Zhou; Yu Du; Bruce R. Childers; Daniel Mossé; Rami Melhem",
    "corresponding_authors": "",
    "abstract": "In a multicore system, many applications share the last-level cache (LLC) and memory bandwidth. These resources need to be carefully managed in a coordinated way to maximize performance. DRAM is still the technology of choice in most systems. However, as traditional DRAM technology faces energy, reliability, and scalability challenges, nonvolatile memory (NVM) technologies are gaining traction. While DRAM is read/write symmetric (a read operation has comparable latency and energy consumption as a write operation), many NVM technologies (such as Phase-Change Memory, PCM) experience read/write asymmetry: write operations are typically much slower and more power hungry than read operations. Whether the memory’s characteristics are symmetric or asymmetric influences the way shared resources are managed. We propose two symmetry-agnostic schemes to manage a shared LLC through way partitioning and memory through bandwidth allocation. The proposals work well for both symmetric and asymmetric memory. First, an exhaustive search is proposed to find the best combination of a cache way partition and bandwidth allocation. Second, an approximate scheme, derived from a theoretical model, is proposed without the overhead of exhaustive search. Simulation results show that the approximate scheme improves weighted speedup by at least 14% on average (regardless of the memory symmetry) over a state-of-the-art way partitioning and memory bandwidth allocation. Simulation results also show that the approximate scheme achieves comparable weighted speedup as a state-of-the-art multiple resource management scheme, XChange, for symmetric memory, and outperforms it by an average of 10% for asymmetric memory.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2293274043",
    "type": "article"
  },
  {
    "title": "Examining and Reducing the Influence of Sampling Errors on Feedback-Driven Optimizations",
    "doi": "https://doi.org/10.1145/2851502",
    "publication_date": "2016-04-05",
    "publication_year": 2016,
    "authors": "Mingzhou Zhou; Bo Wu; Xipeng Shen; Yaoqing Gao; Graham Yiu",
    "corresponding_authors": "",
    "abstract": "Feedback-driven optimization (FDO) is an important component in mainstream compilers. By allowing the compiler to reoptimize the program based on some profiles of the program's dynamic behaviors, it often enhances the quality of the generated code substantially. A barrier for using FDO is that it often requires many training runs to collect enough profiles to amortize the sensitivity of program optimizations to program input changes. Various sampling techniques have been explored to alleviate this time-consuming process. However, the lowered profile accuracy caused by sampling often hurts the benefits of FDO. This article gives the first systematic study in how sampling rates affect the accuracy of collected profiles and how the accuracy correlates with the usefulness of the profile for modern FDO. Studying basic block and edge profiles for FDO in two mature compilers reveals several counterintuitive observations, one of which is that profiling accuracy does not strongly correlate with the benefits of the FDO. A detailed analysis identifies three types of sampling-caused errors that critically impair the quality of the profiles for FDO. It then introduces a simple way to rectify profiles based on the findings. Experiments demonstrate that the simple rectification fixes most of those critical errors in sampled profiles and significantly enhances the effectiveness of FDO.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2330657922",
    "type": "article"
  },
  {
    "title": "Effective management of multiple configurable units using dynamic optimization",
    "doi": "https://doi.org/10.1145/1187976.1187981",
    "publication_date": "2006-12-01",
    "publication_year": 2006,
    "authors": "Shiwen Hu; Madhavi Valluri; Lizy K. John",
    "corresponding_authors": "",
    "abstract": "As one of the promising efforts to minimize the surging microprocessor power consumption, adaptive computing environments (ACEs), where microarchitectural resources can be dynamically tuned to match a program's run-time requirement and characteristics, are becoming increasingly common. In an ACE, efficient management of the configurable units (CUs) is vital for maximizing the benefit of resource adaptation. ACEs usually have multiple configurable hardware units, necessitating exploration of a large number of combinatorial configurations in order to identify the most energy-efficient configuration. In this paper, we propose an ACE management framework for efficient management of multiple CUs, utilizing dynamic optimization systems' inherent capabilities of detecting and optimizing program hotspots, i.e., dominate code regions. We develop a scheme where hotpot boundaries are used for phase detection and adaptation. The framework achieves good energy reduction on managing multiple CUs with minimal hardware requirements and low implement cost by leveraging the existing infrastructure of a dynamic optimization system. The proposed framework is evaluated by dynamically adapting five CUs with distinct reconfiguration latencies and overheads. Those CUs are issue queue, reorder buffer, level-one data and instruction caches, and level-two cache. Previous research indicates that those five components dominate the energy consumption of a microprocessor. Despite the growing complexity and overhead of adapting five CUs, our technique reduces the energy consumption of those CUs by as much as 45%, while one of the best techniques provided by prior literature achieves less than 15% energy reduction for all CUs.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W1980309616",
    "type": "article"
  },
  {
    "title": "Evaluating trace cache energy efficiency",
    "doi": "https://doi.org/10.1145/1187976.1187980",
    "publication_date": "2006-12-01",
    "publication_year": 2006,
    "authors": "Michele Co; Dee A. B. Weikle; Kevin Skadron",
    "corresponding_authors": "",
    "abstract": "Future fetch engines need to be energy efficient. Much research has focused on improving fetch bandwidth. In particular, previous research shows that storing concatenated basic blocks to form instruction traces can significantly improve fetch performance. This work evaluates whether this concatenating of basic blocks translates to significant energy-efficiency gains. We compare processor performance and energy efficiency in trace caches compared to instruction caches. We find that, although trace caches modestly outperform instruction cache only alternatives, it is branch-prediction accuracy that really determines performance and energy efficiency. When access delay and area restrictions are considered, our results show that sequential trace caches achieve very similar performance and energy efficiency results compared to instruction cache-based fetch engines and show that the trace cache's failure to significantly outperform the instruction cache-based fetch organizations stems from the poorer implicit branch prediction from the next-trace predictor at smaller areas. Because access delay limits the theoretical performance of the evaluated fetch engines, we also propose a novel ahead-pipelined next-trace predictor. Our results show that an STC fetch organization with a three-stage, ahead-pipelined next-trace predictor can achieve 5--17% IPC and 29% ED 2 improvements over conventional, unpipelined organizations.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2046196430",
    "type": "article"
  },
  {
    "title": "Spectral prefetcher",
    "doi": "https://doi.org/10.1145/1113841.1113845",
    "publication_date": "2005-12-01",
    "publication_year": 2005,
    "authors": "Saurabh Sharma; Jesse Beu; Thomas M. Conte",
    "corresponding_authors": "",
    "abstract": "Effective data prefetching requires accurate mechanisms to predict embedded patterns in the miss reference behavior. This paper proposes a novel prefetching mechanism, called the spectral prefetcher (SP), that accurately identifies the pattern by dynamically adjusting to its frequency. The proposed mechanism divides the memory address space into tag concentration zones (TCzones) and detects either the pattern of tags (higher order bits) or the pattern of strides (differences between consecutive tags) within each TCzone. The prefetcher dynamically determines whether the pattern of tags or strides will increase the effectiveness of prefetching and switches accordingly. To measure the performance of our scheme, we use a cycle-accurate aggressive out-of-order simulator that models bus occupancy, bus protocol, and limited bandwidth. Our experimental results show performance improvement of 1.59, on average, and at best 2.10 for the memory-intensive benchmarks we studied. Further, we show that SP outperforms the previously proposed scheme, with twice the size of SP, by 39% and a larger L2 cache, with equivalent storage area by 31%.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2107899636",
    "type": "article"
  },
  {
    "title": "Towards update-conscious compilation for energy-efficient code dissemination in WSNs",
    "doi": "https://doi.org/10.1145/1596510.1596512",
    "publication_date": "2009-10-01",
    "publication_year": 2009,
    "authors": "Weijia Li; Youtao Zhang; Jun Yang; Zheng Jiang",
    "corresponding_authors": "",
    "abstract": "Postdeployment code dissemination in wireless sensor networks (WSN) is challenging, as the code has to be transmitted via energy-expensive wireless communication. In this article, we propose novel update-conscious compilation (UCC) techniques to achieve energy efficiency. By integrating the compilation decisions in generating the old binary, an update-conscious compiler strives to match the old decisions, which improves the binary code similarity, reduces the amount of transmitted data to remote sensors, and thus, consumes less energy. In this article, we develop update-conscious register allocation and data layout algorithms. Our experimental results show great improvements over the traditional, update-oblivious approaches.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W1967063603",
    "type": "article"
  },
  {
    "title": "Thread-management techniques to maximize efficiency in multicore and simultaneous multithreaded microprocessors",
    "doi": "https://doi.org/10.1145/1839667.1839671",
    "publication_date": "2010-09-01",
    "publication_year": 2010,
    "authors": "Ryan Rakvic; Qian Cai; J. M. Gonzalez Castro; Grigorios Magklis; Pedro Chaparro; Antonio González",
    "corresponding_authors": "",
    "abstract": "We provide an analysis of thread-management techniques that increase performance or reduce energy in multicore and Simultaneous Multithreaded (SMT) cores. Thread delaying reduces energy consumption by running the core containing the critical thread at maximum frequency while scaling down the frequency and voltage of the cores containing noncritical threads. In this article, we provide an insightful breakdown of thread delaying on a simulated multi-core microprocessor. Thread balancing improves overall performance by giving higher priority to the critical thread in the issue queue of an SMT core. We provide a detailed breakdown of performance results for thread-balancing, identifying performance benefits and limitations. For those benchmarks where a performance benefit is not possible, we introduce a novel thread-balancing mechanism on an SMT core that can reduce energy consumption. We have performed a detailed study on an Intel microprocessor simulator running parallel applications. Thread delaying can reduce energy consumption by 4% to 44% with negligible performance loss. Thread balancing can increase performance by 20% or can reduce energy consumption by 23%.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W1972995899",
    "type": "article"
  },
  {
    "title": "Generalizing neural branch prediction",
    "doi": "https://doi.org/10.1145/1498690.1498692",
    "publication_date": "2009-03-01",
    "publication_year": 2009,
    "authors": "Daniel A. Jiménez",
    "corresponding_authors": "Daniel A. Jiménez",
    "abstract": "Improved branch prediction accuracy is essential to sustaining instruction throughput with today's deep pipelines. Traditional branch predictors exploit correlations between pattern history and branch outcome to predict branches, but there is a stronger and more natural correlation between path history and branch outcome. We explore the potential for exploiting this correlation. We introduce piecewise linear branch prediction , an idealized branch predictor that develops a set of linear functions, one for each program path to the branch to be predicted, that separate predicted taken from predicted not taken branches. Taken together, all of these linear functions form a piecewise linear decision surface. We present a limit study of this predictor showing its potential to greatly improve predictor accuracy. We then introduce a practical implementable branch predictor based on piecewise linear branch prediction. In making our predictor practical, we show how a parameterized version of it unifies the previously distinct concepts of perceptron prediction and path-based neural prediction. Our new branch predictor has implementation costs comparable to current prominent predictors in the literature while significantly improving accuracy. For a deeply pipelined simulated microarchitecture our predictor with a 256-KB hardware budget improves the harmonic mean normalized instructions-per-cycle rate by 8% over both the original path-based neural predictor and 2Bc- gskew . The average misprediction rate is decreased by 16% over the path-based neural predictor and by 22% over 2Bc- gskew .",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2075943024",
    "type": "article"
  },
  {
    "title": "Chameleon",
    "doi": "https://doi.org/10.1145/1736065.1736068",
    "publication_date": "2010-04-01",
    "publication_year": 2010,
    "authors": "Dong Hyuk Woo; Joshua B. Fryman; Allan Knies; Hsien-Hsin S. Lee",
    "corresponding_authors": "",
    "abstract": "Heterogeneous multicore processors have emerged as an energy- and area-efficient architectural solution to improving performance for domain-specific applications such as those with a plethora of data-level parallelism. These processors typically contain a large number of small, compute-centric cores for acceleration while keeping one or two high-performance ILP cores on the die to guarantee single-thread performance. Although a major portion of the transistors are occupied by the acceleration cores, these resources will sit idle when running unparallelized legacy codes or the sequential part of an application. To address this underutilization issue, in this article, we introduce Chameleon, a flexible heterogeneous multicore architecture to virtualize these resources for enhancing memory performance when running sequential programs. The Chameleon architecture can dynamically virtualize the idle acceleration cores into a last-level cache, a data prefetcher, or a hybrid between these two techniques. In addition, Chameleon can operate in an adaptive mode that dynamically configures the acceleration cores between the hybrid mode and the prefetch-only mode by monitoring the effectiveness of the Chameleon cache mode. In our evaluation with SPEC2006 benchmark suite, different levels of performance improvements were achieved in different modes for different applications. In the case of the adaptive mode, Chameleon improves the performance of SPECint06 and SPECfp06 by 31% and 15%, on average. When considering only memory-intensive applications, Chameleon improves the system performance by 50% and 26% for SPECint06 and SPECfp06, respectively.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2105931145",
    "type": "article"
  },
  {
    "title": "Improving Energy Efficiency of Coarse-Grain Reconfigurable Arrays Through Modulo Schedule Compression/Decompression",
    "doi": "https://doi.org/10.1145/3162018",
    "publication_date": "2018-03-22",
    "publication_year": 2018,
    "authors": "Hochan Lee; Mansureh S. Moghaddam; Dongkwan Suh; Bernhard Egger",
    "corresponding_authors": "",
    "abstract": "Modulo-scheduled course-grain reconfigurable array (CGRA) processors excel at exploiting loop-level parallelism at a high performance per watt ratio. The frequent reconfiguration of the array, however, causes between 25% and 45% of the consumed chip energy to be spent on the instruction memory and fetches therefrom. This article presents a hardware/software codesign methodology for such architectures that is able to reduce both the size required to store the modulo-scheduled loops and the energy consumed by the instruction decode logic. The hardware modifications improve the spatial organization of a CGRA’s execution plan by reorganizing the configuration memory into separate partitions based on a statistical analysis of code. A compiler technique optimizes the generated code in the temporal dimension by minimizing the number of signal changes. The optimizations achieve, on average, a reduction in code size of more than 63% and in energy consumed by the instruction decode logic by 70% for a wide variety of application domains. Decompression of the compressed loops can be performed in hardware with no additional latency, rendering the presented method ideal for low-power CGRAs running at high frequencies. The presented technique is orthogonal to dictionary-based compression schemes and can be combined to achieve a further reduction in code size.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2789501424",
    "type": "article"
  },
  {
    "title": "Visual Program Manipulation in the Polyhedral Model",
    "doi": "https://doi.org/10.1145/3177961",
    "publication_date": "2018-03-22",
    "publication_year": 2018,
    "authors": "Oleksandr Zinenko; Stéphane Huot; Cédric Bastoul",
    "corresponding_authors": "",
    "abstract": "Parallelism is one of the key performance sources in modern computer systems. When heuristics-based automatic parallelization fails to improve performance, a cumbersome and error-prone manual transformation is often required. As a solution, we propose an interactive visual approach building on the polyhedral model that visualizes exact dependencies and parallelism; decomposes and replays a complex automatically computed transformation step by step; and allows for directly manipulating the visual representation as a means of transforming the program with immediate feedback. User studies suggest that our visualization is understood by experts and nonexperts alike, and that it may favor an exploratory approach.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2792716933",
    "type": "article"
  },
  {
    "title": "Improving Thread-level Parallelism in GPUs Through Expanding Register File to Scratchpad Memory",
    "doi": "https://doi.org/10.1145/3280849",
    "publication_date": "2018-11-16",
    "publication_year": 2018,
    "authors": "Chao Yu; Yuebin Bai; Qingxiao Sun; Hailong Yang",
    "corresponding_authors": "",
    "abstract": "Modern Graphic Processing Units (GPUs) have become pervasive computing devices in datacenters due to their high performance with massive thread level parallelism (TLP). GPUs are equipped with large register files (RF) to support fast context switch between massive threads and scratchpad memory (SPM) to support inter-thread communication within the cooperative thread array (CTA). However, the TLP of GPUs is usually limited by the inefficient resource management of register file and scratchpad memory. This inefficiency also leads to register file and scratchpad memory underutilization. To overcome the above inefficiency, we propose a new resource management approach EXPARS for GPUs. EXPARS provides a larger register file logically by expanding the register file to scratchpad memory. When the available register file becomes limited, our approach leverages the underutilized scratchpad memory to support additional register allocation. Therefore, more CTAs can be dispatched to SMs, which improves the GPU utilization. Our experiments on representative benchmark suites show that the number of CTAs dispatched to each SM increases by 1.28× on average. In addition, our approach improves the GPU resource utilization significantly, with the register file utilization improved by 11.64% and the scratchpad memory utilization improved by 48.20% on average. With better TLP, our approach achieves 20.01% performance improvement on average with negligible energy overhead.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2900482429",
    "type": "article"
  },
  {
    "title": "Reusing the Optimized Code for JavaScript Ahead-of-Time Compilation",
    "doi": "https://doi.org/10.1145/3291056",
    "publication_date": "2018-12-19",
    "publication_year": 2018,
    "authors": "Hyukwoo Park; Sungkook Kim; Jung‐Geun Park; Soo‐Mook Moon",
    "corresponding_authors": "",
    "abstract": "As web pages and web apps increasingly include heavy JavaScript code, JavaScript performance has been a critical issue. Modern JavaScript engines achieve a remarkable performance by employing tiered-execution architecture based on interpreter, baseline just-in-time compiler (JITC), and optimizing JITC. Unfortunately, they suffer from a substantial compilation overhead, which can take more than 50% of the whole running time. A simple idea to reduce the compilation overhead is ahead-of-time compilation (AOTC), which reuses the code generated in the previous run. In fact, existing studies that reuse the bytecode generated by the interpreter or the machine code generated by the baseline JITC have shown tangible performance benefits [12, 31, 41]. However, there has been no study to reuse the machine code generated by the optimizing JITC, which heavily uses profile-based optimizations, thus not easily reusable. We propose a novel AOTC that can reuse the optimized machine code for high-performance JavaScript engines. Unlike previous AOTCs, we need to resolve a few challenging issues related to reusing profile-based optimized code and relocating dynamic addresses. Our AOTC improves the performance of a commercial JavaScript engine by 6.36 times (max) and 1.99 times (average) for Octane benchmarks, by reducing the compilation overhead and by running the optimized code from the first invocation of functions. It also improves the loading time of six web apps by 1.28 times, on average.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2904527437",
    "type": "article"
  },
  {
    "title": "Exploring an Alternative Cost Function for Combinatorial Register-Pressure-Aware Instruction Scheduling",
    "doi": "https://doi.org/10.1145/3301489",
    "publication_date": "2019-02-27",
    "publication_year": 2019,
    "authors": "Ghassan Shobaki; Austin Kerbow; Christopher Pulido; William D. Dobson",
    "corresponding_authors": "",
    "abstract": "Multiple combinatorial algorithms have been proposed for doing pre-allocation instruction scheduling with the objective of minimizing register pressure or balancing register pressure and instruction-level parallelism. The cost function that is minimized in most of these algorithms is the peak register pressure (or the peak excess register pressure). In this work, we explore an alternative register-pressure cost function, which is the Sum of Live Interval Lengths (SLIL). Unlike the peak cost function, which captures register pressure only at the highest pressure point in the schedule, the proposed SLIL cost function captures register pressure at all points in the schedule. Minimizing register pressure at all points is desirable in larger scheduling regions with multiple high-pressure points. This article describes a Branch-and-Bound (B8B) algorithm for minimizing the SLIL cost function. The algorithm is based on two SLIL-specific dynamic lower bounds as well as the history utilization technique proposed in our previous work. The proposed algorithm is implemented into the LLVM Compiler and evaluated experimentally relative to our previously proposed B8B algorithm for minimizing the peak excess register pressure. The experimental results show that the proposed algorithm for minimizing the SLIL cost function produces substantially less spilling than the previous algorithm that minimizes the peak cost function. Execution-time results on various processors show that the proposed B8B algorithm significantly improves the performance of many CPU2006 benchmarks by up to 49% relative to LLVM's default scheduler. The geometric-mean improvement for FP2006 on Intel Core i7 is 4.22%.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2916770803",
    "type": "article"
  },
  {
    "title": "Efficient and Scalable Execution of Fine-Grained Dynamic Linear Pipelines",
    "doi": "https://doi.org/10.1145/3307411",
    "publication_date": "2019-04-18",
    "publication_year": 2019,
    "authors": "Aristeidis Mastoras; Thomas R. Gross",
    "corresponding_authors": "",
    "abstract": "We present Pipelite , a dynamic scheduler that exploits the properties of dynamic linear pipelines to achieve high performance for fine-grained workloads. The flexibility of Pipelite allows the stages and their data dependences to be determined at runtime. Pipelite unifies communication, scheduling, and synchronization algorithms with suitable data structures. This unified design introduces the local suspension mechanism and a wait-free enqueue operation, which allow efficient dynamic scheduling. The evaluation on a 44-core machine, using programs from three widely used benchmark suites, shows that Pipelite implies low overhead and significantly outperforms the state of the art in terms of speedup, scalability, and memory usage.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2937604920",
    "type": "article"
  },
  {
    "title": "Comprehensive Characterization of an Open Source Document Search Engine",
    "doi": "https://doi.org/10.1145/3320346",
    "publication_date": "2019-05-29",
    "publication_year": 2019,
    "authors": "Zacharias Hadjilambrou; Marios Kleanthous; Georgia Antoniou; Antoni Portero; Yiannakis Sazeides",
    "corresponding_authors": "",
    "abstract": "This work performs a thorough characterization and analysis of the open source Lucene search library. The article describes in detail the architecture, functionality, and micro-architectural behavior of the search engine, and investigates prominent online document search research issues. In particular, we study how intra-server index partitioning affects the response time and throughput, explore the potential use of low power servers for document search, and examine the sources of performance degradation ands the causes of tail latencies. Some of our main conclusions are the following: (a) intra-server index partitioning can reduce tail latencies but with diminishing benefits as incoming query traffic increases, (b) low power servers given enough partitioning can provide same average and tail response times as conventional high performance servers, (c) index search is a CPU-intensive cache-friendly application, and (d) C-states are the main culprits for performance degradation in document search.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2946839132",
    "type": "article"
  },
  {
    "title": "Toward On-chip Network Security Using Runtime Isolation Mapping",
    "doi": "https://doi.org/10.1145/3337770",
    "publication_date": "2019-07-20",
    "publication_year": 2019,
    "authors": "Mohammad Sadeghi; Siavash Bayat-Sarmadi; Shaahin Hessabi",
    "corresponding_authors": "",
    "abstract": "Many-cores execute a large number of diverse applications concurrently. Inter-application interference can lead to a security threat as timing channel attack in the on-chip network. A non-interference communication in the shared on-chip network is a dominant necessity for secure many-core platforms to leverage the concepts of the cloud and embedded system-on-chip. The current non-interference techniques are limited to static scheduling and need router modification at micro-architecture level. Mapping of applications can effectively determine the interference among applications in on-chip network. In this work, we explore non-interference approaches through run-time mapping at software and application level. We map the same group of applications in isolated domain(s) to meet non-interference flows. Through run-time mapping, we can maximize utilization of the system without leaking information. The proposed run-time mapping policy requires no router modification in contrast to the best known competing schemes, and the performance degradation is, on average, 16% compared to the state-of-the-art baselines.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2963060080",
    "type": "article"
  },
  {
    "title": "B <scp>it</scp> SAD v2",
    "doi": "https://doi.org/10.1145/3364999",
    "publication_date": "2019-11-18",
    "publication_year": 2019,
    "authors": "Kyle Daruwalla; Heng Zhuo; Rohit Shukla; Mikko H. Lipasti",
    "corresponding_authors": "",
    "abstract": "Computer vision and machine learning algorithms operating under a strict power budget require an alternate computing paradigm. While bitstream computing (BC) satisfies these constraints, creating BC systems is difficult. To address the design challenges, we propose compiler extensions to B it SAD, a DSL for BC. Our work enables bit-level software emulation and automated generation of hierarchical hardware, discusses potential optimizations, and proposes compiler phases to implement those optimizations in a hardware-aware manner. Finally, we introduce population coding, a parallelization scheme for stochastic computing that decreases latency without sacrificing accuracy, and provide theoretical and experimental guarantees on its effectiveness.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2984847816",
    "type": "article"
  },
  {
    "title": "DisGCo",
    "doi": "https://doi.org/10.1145/3414469",
    "publication_date": "2020-09-30",
    "publication_year": 2020,
    "authors": "Anchu Rajendran; V. Krishna Nandivada",
    "corresponding_authors": "",
    "abstract": "Graph algorithms are widely used in various applications. Their programmability and performance have garnered a lot of interest among the researchers. Being able to run these graph analytics programs on distributed systems is an important requirement. Green-Marl is a popular Domain Specific Language (DSL) for coding graph algorithms and is known for its simplicity. However, the existing Green-Marl compiler for distributed systems (Green-Marl to Pregel) can only compile limited types of Green-Marl programs (in Pregel canonical form). This severely restricts the types of parallel Green-Marl programs that can be executed on distributed systems. We present DisGCo , the first compiler to translate any general Green-Marl program to equivalent MPI program that can run on distributed systems. Translating Green-Marl programs to MPI (SPMD/MPMD style of computation, distributed memory) presents many other exciting challenges, besides the issues related to differences in syntax, as Green-Marl gives the programmer a unified view of the whole memory and allows the parallel and serial code to be inter-mixed. We first present the set of challenges involved in translating Green-Marl programs to MPI and then present a systematic approach to do the translation. We also present a few optimization techniques to improve the performance of our generated programs. DisGCo is the first graph DSL compiler that can handle all syntactic capabilities of a practical graph DSL like Green-Marl and generate code that can run on distributed systems. Our preliminary evaluation of DisGCo shows that our generated programs are scalable. Further, compared to the state-of-the-art DH-Falcon compiler that translates a subset of Falcon programs to MPI, our generated codes exhibit a geomean speedup of 17.32×.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3090198206",
    "type": "article"
  },
  {
    "title": "Design and Evaluation of an Ultra Low-power Human-quality Speech Recognition System",
    "doi": "https://doi.org/10.1145/3425604",
    "publication_date": "2020-11-10",
    "publication_year": 2020,
    "authors": "Dennis Pinto; José-María Arnau; Antonio González",
    "corresponding_authors": "",
    "abstract": "Automatic Speech Recognition (ASR) has experienced a dramatic evolution since pioneer development of Bell Lab’s single-digit recognizer more than 50 years ago. Current ASR systems have taken advantage of the tremendous improvements in AI during the past decade by incorporating Deep Neural Networks into the system and pushing their accuracy to levels comparable to that of humans. This article describes and characterizes a representative ASR system with state-of-the-art accuracy and proposes a hardware platform capable of decoding speech in real-time with a power dissipation close to 1 Watt. The software is based on the so-called hybrid approach with a vocabulary of 200K words and RNN-based language model re-scoring, whereas the hardware consists of a commercially available low-power processor along with two accelerators used for the most compute-intensive tasks. The article shows that high performance can be obtained with very low power, enabling the deployment of these systems in extremely power-constrained environments such as mobile and IoT devices.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3103348415",
    "type": "article"
  },
  {
    "title": "MemSZ",
    "doi": "https://doi.org/10.1145/3424668",
    "publication_date": "2020-11-10",
    "publication_year": 2020,
    "authors": "Albin Eldstål-Ahrens; Ioannis Sourdis",
    "corresponding_authors": "",
    "abstract": "This article describes Memory Squeeze (MemSZ), a new approach for lossy general-purpose memory compression. MemSZ introduces a low latency, parallel design of the Squeeze (SZ) algorithm offering aggressive compression ratios, up to 16:1 in our implementation. Our compressor is placed between the memory controller and the cache hierarchy of a processor to reduce the memory traffic of applications that tolerate approximations in parts of their data. Thereby, the available off-chip bandwidth is utilized more efficiently improving system performance and energy efficiency. Two alternative multi-core variants of the MemSZ system are described. The first variant has a shared last-level cache (LLC) on the processor-die, which is modified to store both compressed and uncompressed data. The second has a 3D-stacked DRAM cache with larger cache lines that match the granularity of the compressed memory blocks and stores only uncompressed data. For applications that tolerate aggressive approximation in large fractions of their data, MemSZ reduces baseline memory traffic by up to 81%, execution time by up to 62%, and energy costs by up to 25% introducing up to 1.8% error to the application output. Compared to the current state-of-the-art lossy memory compression design, MemSZ improves the execution time, energy, and memory traffic by up to 15%, 9%, and 64%, respectively.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3105912033",
    "type": "article"
  },
  {
    "title": "Refresh Triggered Computation",
    "doi": "https://doi.org/10.1145/3417708",
    "publication_date": "2020-12-30",
    "publication_year": 2020,
    "authors": "Syed M. A. H. Jafri; Hasan Hassan; Ahmed Hemani; Onur Mutlu",
    "corresponding_authors": "",
    "abstract": "To employ a Convolutional Neural Network (CNN) in an energy-constrained embedded system, it is critical for the CNN implementation to be highly energy efficient. Many recent studies propose CNN accelerator architectures with custom computation units that try to improve the energy efficiency and performance of CNNs by minimizing data transfers from DRAM-based main memory. However, in these architectures, DRAM is still responsible for half of the overall energy consumption of the system, on average. A key factor of the high energy consumption of DRAM is the refresh overhead , which is estimated to consume 40% of the total DRAM energy. In this article, we propose a new mechanism, Refresh Triggered Computation (RTC) , that exploits the memory access patterns of CNN applications to reduce the number of refresh operations . RTC uses two major techniques to mitigate the refresh overhead. First, Refresh Triggered Transfer (RTT) is based on our new observation that a CNN application accesses a large portion of the DRAM in a predictable and recurring manner. Thus, the read/write accesses of the application inherently refresh the DRAM, and therefore a significant fraction of refresh operations can be skipped. Second, Partial Array Auto-Refresh (PAAR) eliminates the refresh operations to DRAM regions that do not store any data. We propose three RTC designs (min-RTC, mid-RTC, and full-RTC), each of which requires a different level of aggressiveness in terms of customization to the DRAM subsystem. All of our designs have small overhead. Even the most aggressive RTC design (i.e., full-RTC) imposes an area overhead of only 0.18% in a 16 Gb DRAM chip and can have less overhead for denser chips. Our experimental evaluation on six well-known CNNs shows that RTC reduces average DRAM energy consumption by 24.4% and 61.3% for the least aggressive and the most aggressive RTC implementations, respectively. Besides CNNs, we also evaluate our RTC mechanism on three workloads from other domains. We show that RTC saves 31.9% and 16.9% DRAM energy for Face Recognition and Bayesian Confidence Propagation Neural Network (BCPNN) , respectively. We believe RTC can be applied to other applications whose memory access patterns remain predictable for a sufficiently long time.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3115728776",
    "type": "article"
  },
  {
    "title": "Leveraging Value Equality Prediction for Value Speculation",
    "doi": "https://doi.org/10.1145/3436821",
    "publication_date": "2020-12-30",
    "publication_year": 2020,
    "authors": "Kleovoulos Kalaitzidis; André Seznec",
    "corresponding_authors": "",
    "abstract": "Value Prediction (VP) has recently been gaining interest in the research community, since prior work has established practical solutions for its implementation that provide meaningful performance gains. A constant challenge of contemporary context-based value predictors is to sufficiently capture value redundancy and exploit the predictable execution paths. To do so, modern context-based VP techniques tightly associate recurring values with instructions and contexts by building confidence upon them after a plethora of repetitions. However, when execution monotony exists in the form of intervals, the potential prediction coverage is limited, since prediction confidence is reset at the beginning of each new interval. In this study, we address this challenge by introducing the notion of Equality Prediction (EP), which represents the binary facet of VP. Following a twofold decision scheme (similar to branch prediction), at fetch time, EP makes use of control-flow history to predict equality between the last committed result for this instruction and the result of the currently fetched occurrence. When equality is predicted with high confidence, the last committed value is used. Our simulation results show that this technique obtains the same level of performance as previously proposed state-of-the-art context-based value predictors. However, by virtue of exploiting equality patterns that are not captured by previous VP schemes, our design can improve the speedup of standard VP by 19% on average, when combined with contemporary prediction models.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3115822890",
    "type": "article"
  },
  {
    "title": "SPX64",
    "doi": "https://doi.org/10.1145/3436730",
    "publication_date": "2020-12-30",
    "publication_year": 2020,
    "authors": "Abhishek Singh; Shail Dave; Pantea Zardoshti; Robert Brotzman; Chao Zhang; Xiaochen Guo; Aviral Shrivastava; Gang Tan; Michael Spear",
    "corresponding_authors": "",
    "abstract": "General-purpose computing systems employ memory hierarchies to provide the appearance of a single large, fast, coherent memory. In special-purpose CPUs, programmers manually manage distinct, non-coherent scratchpad memories. In this article, we combine these mechanisms by adding a virtually addressed, set-associative scratchpad to a general purpose CPU. Our scratchpad exists alongside a traditional cache and is able to avoid many of the programming challenges associated with traditional scratchpads without sacrificing generality (e.g., virtualization). Furthermore, our design delivers increased security and improves performance, especially for workloads with high locality or that interact with nonvolatile memory.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3117042881",
    "type": "article"
  },
  {
    "title": "Hybrid type legalization for a sparse SIMD instruction set",
    "doi": "https://doi.org/10.1145/2509420.2509422",
    "publication_date": "2013-09-16",
    "publication_year": 2013,
    "authors": "Yosi Ben Asher; Nadav Rotem",
    "corresponding_authors": "",
    "abstract": "SIMD vector units implement only a subset of the operations used by vectorizing compilers, and there are multiple conflicting techniques to legalize arbitrary vector types into register-sized data types. Traditionally, type legalization is performed using a set of predefined rules, regardless of the operations used in the program. This method is not suitable to sparse SIMD instruction sets and often prevents the vectorization of programs. In this work we introduce a new technique for type legalization, namely vector element promotion, as well as a hybrid method for combining multiple techniques of type legalization. Our hybrid type legalization method makes decisions based on the knowledge of the available instruction set as well as the operations used in the program. Our experimental results demonstrate that program-dependent hybrid type legalization improves the execution time of vector programs, outperforms the existing legalization method, and allows the vectorization of workloads which were not vectorized before.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W1964349779",
    "type": "article"
  },
  {
    "title": "Benefits of selective packet discard in networks-on-chip",
    "doi": "https://doi.org/10.1145/2207222.2207228",
    "publication_date": "2012-06-01",
    "publication_year": 2012,
    "authors": "Andreas Lankes; Thomas Wild; Stefan Wallentowitz; Andreas Herkersdorf",
    "corresponding_authors": "",
    "abstract": "Today, Network on Chip concepts principally assume inherent lossless operation. Considering that future nanometer CMOS technologies will witness increased sensitivity to all forms of manufacturing and environmental variations (e.g., IR drop, soft errors due to radiation, transient temperature induced timing problems, device aging), efforts to cope with data corruption or packet loss will be unavoidable. Possible counter measures against packet loss are the extension of flits with ECC or the introduction of error detection with retransmission. We propose to make use of the perceived deficiency of packet loss as a feature. By selectively discarding stuck packets in the NoC, a proven practice in computer networks, all types of deadlocks can be resolved. This is especially advantageous for solving the problem of message-dependent deadlocks, which otherwise leads to high costs either in terms of throughput or chip area. Strict ordering, the most popular approach to this problem, results in a significant buffer overhead and a more complex router architecture. In addition, we will show that eliminating local network congestions by selectively discarding individual packets also can improve the effective throughput of the network. The end-to-end retransmission mechanism required for the reliable communication, then also provides lossless communication for the cores.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W1975882866",
    "type": "article"
  },
  {
    "title": "PLDS",
    "doi": "https://doi.org/10.1145/2086696.2086717",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Min Feng; Changhui Lin; Rajiv Gupta",
    "corresponding_authors": "",
    "abstract": "Recently, parallelization of computations in the presence of dynamic data structures has shown promising potential. In this paper, we present PLDS, a system for easily expressing and efficiently exploiting parallelism in computations that are based on dynamic linked data structures. PLDS improves the execution efficiency by providing support for data partitioning and then distributing computation across threads based on the partitioning. Such computations often require the use of speculation to exploit dynamic parallelism. PLDS supports a conditional speculation mechanism that reduces the cost of speculation. PLDS can be employed in the context of different forms of parallelism, which to cover a wide range of parallel applications. PLDS provides easy-to-use compiler directives, using enabling programmers to choose from among a variety of data partitionings to distribute computation across threads in a partitioning-sensitive fashion, and to use conditional speculation when required. We evaluate our implementation of PLDS using ten benchmarks, of which six are parallelized using speculation. PLDS achieves 1.3x--6.9x speedups on an 8-core machine.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W1982596120",
    "type": "article"
  },
  {
    "title": "Revisiting Clustered Microarchitecture for Future Superscalar Cores",
    "doi": "https://doi.org/10.1145/2800787",
    "publication_date": "2015-08-31",
    "publication_year": 2015,
    "authors": "Pierre Michaud; Andrea Mondelli; André Seznec",
    "corresponding_authors": "",
    "abstract": "During the past 10 years, the clock frequency of high-end superscalar processors has not increased. Performance keeps growing mainly by integrating more cores on the same chip and by introducing new instruction set extensions. However, this benefits only some applications and requires rewriting and/or recompiling these applications. A more general way to accelerate applications is to increase the IPC, the number of instructions executed per cycle. Although the focus of academic microarchitecture research moved away from IPC techniques, the IPC of commercial processors was continuously improved during these years. We argue that some of the benefits of technology scaling should be used to raise the IPC of future superscalar cores further. Starting from microarchitecture parameters similar to recent commercial high-end cores, we show that an effective way to increase the IPC is to allow the out-of-order engine to issue more micro-ops per cycle. But this must be done without impacting the clock cycle. We propose combining two techniques: clustering and register write specialization. Past research on clustered microarchitectures focused on narrow issue clusters, as the emphasis at that time was on allowing high clock frequencies. Instead, in this study, we consider wide issue clusters, with the goal of increasing the IPC under a constant clock frequency. We show that on a wide issue dual cluster, a very simple steering policy that sends 64 consecutive instructions to the same cluster, the next 64 instructions to the other cluster, and so forth, permits tolerating an intercluster delay of three cycles. We also propose a method for decreasing the energy cost of sending results from one cluster to the other cluster.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2001501073",
    "type": "article"
  },
  {
    "title": "NoCMsg",
    "doi": "https://doi.org/10.1145/2701426",
    "publication_date": "2015-03-09",
    "publication_year": 2015,
    "authors": "Christopher Zimmer; Frank Mueller",
    "corresponding_authors": "",
    "abstract": "The number of cores of contemporary processors is constantly increasing and thus continues to deliver ever higher peak performance (following Moore’s transistor law). Yet high core counts present a challenge to hardware and software alike. Following this trend, the network-on-chip (NoC) topology has changed from buses over rings and fully connected meshes to 2D meshes. This work contributes NoCMsg, a low-level message-passing abstraction over NoCs, which is specifically designed for large core counts in 2D meshes. NoCMsg ensures deadlock-free messaging for wormhole Manhattan-path routing over the NoC via a polling-based message abstraction and non--flow-controlled communication for selective communication patterns. Experimental results on the TilePro hardware platform show that NoCMsg can significantly reduce communication times by up to 86% for single packet messages and up to 40% for larger messages compared to other NoC-based message approaches. On the TilePro platform, NoCMsg outperforms shared memory abstractions by up to 93% as core counts and interprocess communication increase. Results for fully pipelined double-precision numerical codes show speedups of up to 64% for message passing over shared memory at 32 cores. Overall, we observe that shared memory scales up to about 16 cores on this platform, whereas message passing performs well beyond that threshold. These results generalize to similar NoC-based platforms.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2005163625",
    "type": "article"
  },
  {
    "title": "Reducing instruction fetch energy in multi-issue processors",
    "doi": "https://doi.org/10.1145/2541228.2555318",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Peter Gavin; David Whalley; Magnus Själander",
    "corresponding_authors": "",
    "abstract": "The need to minimize power while maximizing performance has led to recent developments of powerful superscalar designs targeted at embedded and portable use. Instruction fetch is responsible for a significant fraction of microprocessor power and energy, and is therefore an attractive target for architectural power optimization. We present novel techniques that take advantage of guarantees so that the instruction translation lookaside buffer, branch target buffer, and branch prediction buffer can frequently be disabled, reducing their energy usage, while simultaneously reducing branch predictor contention. These techniques require no changes to the instruction set and can easily be integrated into most single- and multiple-issue processors.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2006470695",
    "type": "article"
  },
  {
    "title": "An energy-efficient method of supporting flexible special instructions in an embedded processor with compact ISA",
    "doi": "https://doi.org/10.1145/2509420.2509426",
    "publication_date": "2013-09-16",
    "publication_year": 2013,
    "authors": "Dongrui She; Yifan He; Henk Corporaal",
    "corresponding_authors": "",
    "abstract": "In application-specific processor design, a common approach to improve performance and efficiency is to use special instructions that execute complex operation patterns. However, in a generic embedded processor with compact Instruction Set Architecture (ISA), these special instructions may lead to large overhead such as: ( i ) more bits are needed to encode the extra opcodes and operands, resulting in wider instructions; ( ii ) more Register File (RF) ports are required to provide the extra operands to the function units. Such overhead may increase energy consumption considerably. In this article, we propose to support flexible operation pair patterns in a processor with a compact 24-bit RISC-like ISA using: ( i ) a partially reconfigurable decoder that exploits the pattern locality to reduce opcode space requirement; ( ii ) a software-controlled bypass network to reduce operand encoding bit and RF port requirement. An energy-aware compiler backend is designed for the proposed architecture that performs pattern selection and bypass-aware scheduling to generate energy-efficient codes. Though the proposed design imposes extra constraints on the operation patterns, the experimental results show that for benchmark applications from different domains, the average dynamic instruction count is reduced by over 25%, which is only about 2% less than the architecture without such constraints. The proposed architecture reduces total energy by an average of 15.8% compared to the RISC baseline, while the one without constraints achieves almost no improvement due to its high overhead. When high performance is required, the proposed architecture is able to achieve a speedup of 13.8% with 13.1% energy reduction compared to the baseline by introducing multicycle SFU operations.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2016411588",
    "type": "article"
  },
  {
    "title": "A scalable and near-optimal representation of access schemes for memory management",
    "doi": "https://doi.org/10.1145/2579677",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "Angeliki Kritikakou; Francky Catthoor; Vasilios Kelefouras; Costas E. Goutis",
    "corresponding_authors": "",
    "abstract": "Memory management searches for the resources required to store the concurrently alive elements. The solution quality is affected by the representation of the element accesses: a sub-optimal representation leads to overestimation and a non-scalable representation increases the exploration time. We propose a methodology to near-optimal and scalable represent regular and irregular accesses. The representation consists of a set of pattern entries to compactly describe the behavior of the memory accesses and of pattern operations to consistently combine the pattern entries. The result is a final sequence of pattern entries which represents the global access scheme without unnecessary overestimation.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2016647231",
    "type": "article"
  },
  {
    "title": "Using in-flight chains to build a scalable cache coherence protocol",
    "doi": "https://doi.org/10.1145/2541228.2541235",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Samantika Subramaniam; Simon C. Steely; Will Hasenplaugh; Aamer Jaleel; Carl Beckmann; Tryggve Fossum; Joel Emer",
    "corresponding_authors": "",
    "abstract": "As microprocessor designs integrate more cores, scalability of cache coherence protocols becomes a challenging problem. Most directory-based protocols avoid races by using blocking tag directories that can impact the performance of parallel applications. In this article, we first quantitatively demonstrate that state-of-the-art blocking protocols significantly constrain throughput at large core counts for several parallel applications. Nonblocking protocols address this throughput concern at the expense of scalability in the interconnection network or in the required resource overheads. To address this concern, we enhance nonblocking directory protocols by migrating the point of service of responses. Our approach uses in-flight chains of cores making parallel memory requests to incorporate scalability while maintaining high-throughput. The proposed cache coherence protocol called chained cache coherence , can outperform blocking protocols by up to 20% on scientific and 12% on commercial applications. It also has low resource overheads and simple address ordering requirements making it both a high-performance and scalable protocol. Furthermore, in-flight chains provide a scalable solution to building hierarchical and nonblocking tag directories as well as optimize communication latencies.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2029111186",
    "type": "article"
  },
  {
    "title": "Efficient hosted interpreters on the JVM",
    "doi": "https://doi.org/10.1145/2532642",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "Gülfem Savrun-Yeniçeri; Wei Zhang; Huahan Zhang; Eric Seckler; Chen Li; Stefan Brunthaler; Per Larsen; Michael Franz",
    "corresponding_authors": "",
    "abstract": "Many guest languages are implemented using the Java Virtual Machine (JVM) as a host environment. There are two major implementation choices: custom compilers and so-called hosted interpreters. Custom compilers are complex to build but offer good performance. Hosted interpreters are comparatively simpler to implement but until now have suffered from poor performance. We studied the performance of hosted interpreters and identified common bottlenecks preventing their efficient execution. First, similar to interpreters written in C/C++, instruction dispatch is expensive on the JVM. Second, Java’s semantics require expensive runtime exception checks that negatively affect array performance essential to interpreters. We present two optimizations targeting these bottlenecks and show that the performance of optimized interpreters increases dramatically: we report speedups by a factor of up to 2.45 over the Jython interpreter, 3.57 over the Rhino interpreter, and 2.52 over the JRuby interpreter, respectively. The resulting performance is comparable with that of custom compilers. Our optimizations are enabled by a few simple annotations that require only modest implementation effort; in return, performance increases substantially.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2033197452",
    "type": "article"
  },
  {
    "title": "EFGR",
    "doi": "https://doi.org/10.1145/2656340",
    "publication_date": "2014-10-27",
    "publication_year": 2014,
    "authors": "Venkata Kalyan Tavva; Kasha Ravi; Madhu Mutyam",
    "corresponding_authors": "",
    "abstract": "High-density DRAM devices spend significant time refreshing the DRAM cells, leading to performance drop. The JEDEC DDR4 standard provides a Fine Granularity Refresh (FGR) feature to tackle refresh. Motivated by the observation that in FGR mode, only a few banks are involved, we propose an Enhanced FGR (EFGR) feature that introduces three optimizations to the basic FGR feature and exposes the bank-level parallelism within the rank even during the refresh. The first optimization decouples the nonrefreshing banks. The second and third optimizations determine the maximum number of nonrefreshing banks that can be active during refresh and selectively precharge the banks before refresh, respectively. Our simulation results show that the EFGR feature is able to recover almost 56.6% of the performance loss incurred due to refresh operations.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2033846827",
    "type": "article"
  },
  {
    "title": "A decoupled non-SSA global register allocation using bipartite liveness graphs",
    "doi": "https://doi.org/10.1145/2544101",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Rajkishore Barik; Jisheng Zhao; Vivek Sarkar",
    "corresponding_authors": "",
    "abstract": "Register allocation is an essential optimization for all compilers. A number of sophisticated register allocation algorithms have been developed over the years. The two fundamental classes of register allocation algorithms used in modern compilers are based on Graph Coloring (GC) and Linear Scan (LS). However, these two algorithms have fundamental limitations in terms of precision. For example, the key data structure used in GC-based algorithms, the interference graph, lacks information on the program points at which two variables may interfere. The LS-based algorithms make local decisions regarding spilling, and thereby trade off global optimization for reduced compile-time and space overheads. Recently, researchers have proposed Static Single Assignment (SSA)-based decoupled register allocation algorithms that exploit the live-range split points of the SSA representation to optimally solve the spilling problem. However, SSA-based register allocation often requires extra complexity in repairing register assignments during SSA elimination and in addressing architectural constraints such as aliasing and ABI encoding; this extra overhead can be prohibitively expensive in dynamic compilation contexts. This article proposes a decoupled non-SSA--based global register allocation algorithm for dynamic compilation. It addresses the limitations in current algorithms by introducing a Bipartite Liveness Graph (BLG)-based register allocation algorithm that models the spilling phase as an optimization problem on the BLG itself and the assignment phase as a separate optimization problem. Advanced register allocation optimizations such as move coalescing, live-range splitting, and register class handling are also performed along with the spilling and assignment phases. In the presence of register classes, we propose a bucket-based greedy heuristic for assignment that strikes a balance between spill-cost and register class constraints. We present experimental evaluation of our BLG-based register allocation algorithm and compare it with production-quality register allocators in Jikes RVM and LLVM.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2049496013",
    "type": "article"
  },
  {
    "title": "The design and implementation of heterogeneous multicore systems for energy-efficient speculative thread execution",
    "doi": "https://doi.org/10.1145/2541228.2541233",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Yangchun Luo; Wei‐Chung Hsu; Antonia Zhai",
    "corresponding_authors": "",
    "abstract": "With the emergence of multicore processors, various aggressive execution models have been proposed to exploit fine-grained thread-level parallelism, taking advantage of the fast on-chip interconnection communication. However, the aggressive nature of these execution models often leads to excessive energy consumption incommensurate to execution time reduction. In the context of Thread-Level Speculation, we demonstrated that on a same-ISA heterogeneous multicore system, by dynamically deciding how on-chip resources are utilized, speculative threads can achieve performance gain in an energy-efficient way. Through a systematic design space exploration, we built a multicore architecture that integrates heterogeneous components of processing cores and first-level caches. To cope with processor reconfiguration overheads, we introduced runtime mechanisms to mitigate their impacts. To match program execution with the most energy-efficient processor configuration, the system was equipped with a dynamic resource allocation scheme that characterizes program behaviors using novel processor counters. We evaluated the proposed heterogeneous system with a diverse set of benchmark programs from SPEC CPU2000 and CPU20006 suites. Compared to the most efficient homogeneous TLS implementation, we achieved similar performance but consumed 18% less energy. Compared to the most efficient homogeneous uniprocessor running sequential programs, we improved performance by 29% and reduced energy consumption by 3.6%, which is a 42% improvement in energy-delay-squared product.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2050419810",
    "type": "article"
  },
  {
    "title": "Dataflow Tomography",
    "doi": "https://doi.org/10.1145/2133382.2133385",
    "publication_date": "2012-03-01",
    "publication_year": 2012,
    "authors": "Bita Mazloom; Shashidhar Mysore; Mohit Tiwari; Banit Agrawal; Timothy Sherwood",
    "corresponding_authors": "",
    "abstract": "It is not uncommon for modern systems to be composed of a variety of interacting services, running across multiple machines in such a way that most developers do not really understand the whole system. As abstraction is layered atop abstraction, developers gain the ability to compose systems of extraordinary complexity with relative ease. However, many software properties, especially those that cut across abstraction layers, become very difficult to understand in such compositions. The communication patterns involved, the privacy of critical data, and the provenance of information, can be difficult to find and understand, even with access to all of the source code. The goal of Dataflow Tomography is to use the inherent information flow of such systems to help visualize the interactions between complex and interwoven components across multiple layers of abstraction. In the same way that the injection of short-lived radioactive isotopes help doctors trace problems in the cardiovascular system, the use of “data tagging” can help developers slice through the extraneous layers of software and pin-point those portions of the system interacting with the data of interest. To demonstrate the feasibility of this approach we have developed a prototype system in which tags are tracked both through the machine and in between machines over the network, and from which novel visualizations of the whole system can be derived. We describe the system-level challenges in creating a working system tomography tool and we qualitatively evaluate our system by examining several example real world scenarios.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2051742109",
    "type": "article"
  },
  {
    "title": "Studying Optimal Spilling in the Light of SSA",
    "doi": "https://doi.org/10.1145/2685392",
    "publication_date": "2015-01-09",
    "publication_year": 2015,
    "authors": "Quentin Colombet; Florian Brandner; Alain Darte",
    "corresponding_authors": "",
    "abstract": "Recent developments in register allocation, mostly linked to static single assignment (SSA) form, have shown the benefits of decoupling the problem in two phases: a first spilling phase places load and store instructions so that the register pressure at all program points is small enough, and a second assignment and coalescing phase maps the variables to physical registers and reduces the number of move instructions among registers. This article focuses on the first phase, for which many open questions remain: in particular, we study the notion of optimal spilling (what can be expressed?) and the impact of SSA form (does it help?). To identify the important features for optimal spilling on load-store architectures, we develop a new integer linear programming formulation, more accurate and expressive than previous approaches. Among other features, we can express SSA ϕ-functions, memory-to-memory copies, and the fact that a value can be stored simultaneously in a register and in memory. Based on this formulation, we present a thorough analysis of the results obtained for the SPECINT 2000 and EEMBC 1.1 benchmarks, from which we draw, among others, the following conclusions: (1) rematerialization is extremely important; (2) SSA complicates the formulation of optimal spilling, especially because of memory coalescing when the code is not in conventional SSA (CSSA); (3) microarchitectural features are significant and thus have to be accounted for; and (4) significant savings can be obtained in terms of static spill costs, cache miss rates, and dynamic instruction counts.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2057472978",
    "type": "article"
  },
  {
    "title": "sFtree",
    "doi": "https://doi.org/10.1145/2086696.2086734",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Bartosz Bogdański; Sven-Arne Reinemo; Frank Olaf Sem-Jacobsen; Ernst Gunnar Gran",
    "corresponding_authors": "",
    "abstract": "Existing fat-tree routing algorithms fully exploit the path diversity of a fat-tree topology in the context of compute node traffic, but they lack support for deadlock-free and fully connected switch-to-switch communication. Such support is crucial for efficient system management, for example, in InfiniBand (IB) systems. With the general increase in system management capabilities found in modern InfiniBand switches, the lack of deadlock-free switch-to-switch communication is a problem for fat-tree-based IB installations because management traffic might cause routing deadlocks that bring the whole system down. This lack of deadlock-free communication affects all system management and diagnostic tools using LID routing. In this paper, we propose the sFtree routing algorithm that guarantees deadlock-free and fully connected switch-to-switch communication in fat-trees while maintaining the properties of the current fat-tree algorithm. We prove that the algorithm is deadlock free and we implement it in OpenSM for evaluation. We evaluate the performance of the sFtree algorithm experimentally on a small cluster and we do a large-scale evaluation through simulations. The results confirm that the sFtree routing algorithm is deadlock-free and show that the impact of switch-to-switch management traffic on the end-node traffic is negligible.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2091091642",
    "type": "article"
  },
  {
    "title": "The Effects of Parameter Tuning in Software Thread-Level Speculation in JavaScript Engines",
    "doi": "https://doi.org/10.1145/2686036",
    "publication_date": "2015-01-09",
    "publication_year": 2015,
    "authors": "Jan Kasper Martinsen; Håkan Grahn; Anders Isberg",
    "corresponding_authors": "",
    "abstract": "JavaScript is a sequential programming language that has a large potential for parallel execution in Web applications. Thread-level speculation can take advantage of this, but it has a large memory overhead. In this article, we evaluate the effects of adjusting various parameters for thread-level speculation. Our results clearly show that thread-level speculation is a useful technique for taking advantage of multicore architectures for JavaScript in Web applications, that nested speculation is required in thread-level speculation, and that the execution characteristics of Web applications significantly reduce the needed memory, the number of threads, and the depth of our speculation.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2166503548",
    "type": "article"
  },
  {
    "title": "MetaSys: A Practical Open-source Metadata Management System to Implement and Evaluate Cross-layer Optimizations",
    "doi": "https://doi.org/10.1145/3505250",
    "publication_date": "2022-03-24",
    "publication_year": 2022,
    "authors": "Nandita Vijaykumar; Ataberk Olgun; Konstantinos Kanellopoulos; F. Nisa Bostancı; Hasan Hassan; Mehrshad Lotfi; Phillip B. Gibbons; Onur Mutlu",
    "corresponding_authors": "",
    "abstract": "This article introduces the first open-source FPGA-based infrastructure, MetaSys, with a prototype in a RISC-V system, to enable the rapid implementation and evaluation of a wide range of cross-layer techniques in real hardware. Hardware-software cooperative techniques are powerful approaches to improving the performance, quality of service, and security of general-purpose processors. They are, however, typically challenging to rapidly implement and evaluate in real hardware as they require full-stack changes to the hardware, system software, and instruction-set architecture (ISA). MetaSys implements a rich hardware-software interface and lightweight metadata support that can be used as a common basis to rapidly implement and evaluate new cross-layer techniques. We demonstrate MetaSys’s versatility and ease-of-use by implementing and evaluating three cross-layer techniques for: (i) prefetching in graph analytics; (ii) bounds checking in memory unsafe languages, and (iii) return address protection in stack frames; each technique requiring only ~100 lines of Chisel code over MetaSys. Using MetaSys, we perform the first detailed experimental study to quantify the performance overheads of using a single metadata management system to enable multiple cross-layer optimizations in CPUs. We identify the key sources of bottlenecks and system inefficiency of a general metadata management system. We design MetaSys to minimize these inefficiencies and provide increased versatility compared to previously proposed metadata systems. Using three use cases and a detailed characterization, we demonstrate that a common metadata management system can be used to efficiently support diverse cross-layer techniques in CPUs. MetaSys is completely and freely available at https://github.com/CMU-SAFARI/MetaSys .",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3160898425",
    "type": "article"
  },
  {
    "title": "Occam: Optimal Data Reuse for Convolutional Neural Networks",
    "doi": "https://doi.org/10.1145/3566052",
    "publication_date": "2022-12-16",
    "publication_year": 2022,
    "authors": "Ashish Gondimalla; Jianqiao Liu; Mithuna Thottethodi; T. N. Vijaykumar",
    "corresponding_authors": "",
    "abstract": "Convolutional neural networks (CNNs) are emerging as powerful tools for image processing in important commercial applications. We focus on the important problem of improving the latency of image recognition. While CNNs are highly amenable to prefetching and multithreading to avoid memory latency issues, CNNs’ large data – each layer’s input, filters, and output – poses a memory bandwidth problem. While previous work captures only some of the enormous data reuse, full reuse implies that the initial input image and filters are read once from off-chip and the final output is written once off-chip without spilling the intermediate layers’ data to off-chip. We propose Occam to capture full reuse via four contributions. First, we identify the necessary conditions for full reuse. Second, we identify the dependence closure as the sufficient condition to capture full reuse using the least on-chip memory. Third, because the dependence closure is often too large to fit in on-chip memory, we propose a dynamic programming algorithm that optimally partitions a given CNN to guarantee the least off-chip traffic at the partition boundaries for a given on-chip capacity. While tiling is well-known, our contribution determines the optimal cross-layer tiles. Occam’s partitions reside on different chips, forming a pipeline so that a partition’s filters and dependence closure remain on-chip as different images pass through (i.e., each partition incurs off-chip traffic only for its inputs and outputs). Finally, because the optimal partitions may result in an unbalanced pipeline, we propose staggered asynchronous pipelines (STAPs) that replicate bottleneck stages to improve throughput by staggering mini-batches across replicas. Importantly, STAPs achieve balanced pipelines without changing Occam’s optimal partitioning. Our simulations show that, on average, Occam cuts off-chip transfers by 21× and achieves 2.04× and 1.21× better performance, and 33% better energy than the base case, respectively. Using a field-programmable gate array (FPGA) implementation, Occam performs 6.1× and 1.5× better, on average, than the base case and Layer Fusion, respectively.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3177229224",
    "type": "article"
  },
  {
    "title": "<i>LiteCON</i> : An All-photonic Neuromorphic Accelerator for Energy-efficient Deep Learning",
    "doi": "https://doi.org/10.1145/3531226",
    "publication_date": "2022-06-28",
    "publication_year": 2022,
    "authors": "Dharanidhar Dang; Bill Lin; Debashis Sahoo",
    "corresponding_authors": "",
    "abstract": "Deep learning is highly pervasive in today's data-intensive era. In particular, convolutional neural networks (CNNs) are being widely adopted in a variety of fields for superior accuracy. However, computing deep CNNs on traditional CPUs and GPUs brings several performance and energy pitfalls. Several novel approaches based on ASIC, FPGA, and resistive-memory devices have been recently demonstrated with promising results. Most of them target only the inference (testing) phase of deep learning. There have been very limited attempts to design a full-fledged deep learning accelerator capable of both training and inference. It is due to the highly compute- and memory-intensive nature of the training phase. In this article, we propose LiteCON , a novel analog photonics CNN accelerator. LiteCON uses silicon microdisk-based convolution, memristor-based memory, and dense-wavelength-division-multiplexing for energy-efficient and ultrafast deep learning. We evaluate LiteCON using a commercial CAD framework (IPKISS) on deep learning benchmark models including LeNet and VGG-Net. Compared to the state of the art, LiteCON improves the CNN throughput, energy efficiency, and computational efficiency by up to 32×, 37×, and 5×, respectively, with trivial accuracy degradation.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4283652620",
    "type": "article"
  },
  {
    "title": "Lock-Free High-performance Hashing for Persistent Memory via PM-aware Holistic Optimization",
    "doi": "https://doi.org/10.1145/3561651",
    "publication_date": "2022-09-06",
    "publication_year": 2022,
    "authors": "Zhangyu Chen; Yu Hua; Luochangqi Ding; Bo Ding; Pengfei Zuo; Xue Liu",
    "corresponding_authors": "",
    "abstract": "Persistent memory (PM) provides large-scale non-volatile memory (NVM) with DRAM-comparable performance. The non-volatility and other unique characteristics of PM architecture bring new opportunities and challenges for the efficient storage system design. For example, some recent crash-consistent and write-friendly hashing schemes are proposed to provide fast queries for PM systems. However, existing PM hashing indexes suffer from the concurrency bottleneck due to the blocking resizing and expensive lock-based concurrency control for queries. Moreover, the lack of PM awareness and systematical design further increases the query latency. To address the concurrency bottleneck of lock contention in PM hashing, we propose clevel hashing, a lock-free concurrent level hashing scheme that provides non-blocking resizing via background threads and lock-free search/insertion/update/deletion using atomic primitives to enable high concurrency for PM hashing. By exploiting the PM characteristics, we present a holistic approach to building clevel hashing for high throughput and low tail latency via the PM-aware index/allocator co-design. The proposed volatile announcement array with a helping mechanism coordinates lock-free insertions and guarantees a strong consistency model. Our experiments using real-world YCSB workloads on Intel Optane DC PMM show that clevel hashing, respectively, achieves up to 5.7× and 1.6× higher throughput than state-of-the-art P-CLHT and Dash while guaranteeing low tail latency, e.g., 1.9×–7.2× speedup for the p99 latency with the insert-only workload.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4294733115",
    "type": "article"
  },
  {
    "title": "Puppeteer: A Random Forest Based Manager for Hardware Prefetchers Across the Memory Hierarchy",
    "doi": "https://doi.org/10.1145/3570304",
    "publication_date": "2022-10-31",
    "publication_year": 2022,
    "authors": "Furkan Eris; Marcia S. Louis; Kubra Eris; José Luis Abellán; Ajay Joshi",
    "corresponding_authors": "",
    "abstract": "Over the years, processor throughput has steadily increased. However, the memory throughput has not increased at the same rate, which has led to the memory wall problem in turn increasing the gap between effective and theoretical peak processor performance. To cope with this, there has been an abundance of work in the area of data/instruction prefetcher designs. Broadly, prefetchers predict future data/instruction address accesses and proactively fetch data/instructions in the memory hierarchy with the goal of lowering data/instruction access latency. To this end, one or more prefetchers are deployed at each level of the memory hierarchy, but typically, each prefetcher gets designed in isolation without comprehensively accounting for other prefetchers in the system. As a result, individual prefetchers do not always complement each other, and that leads to lower average performance gains and/or many negative outliers. In this work, we propose Puppeteer, which is a hardware prefetcher manager that uses a suite of random forest regressors to determine at runtime which prefetcher should be ON at each level in the memory hierarchy, such that the prefetchers complement each other and we reduce the data/instruction access latency. Compared to a design with no prefetchers, using Puppeteer we improve IPC by 46.0% in 1 one-core, 25.8% in four-core, and 11.9% in eight-core processors on average across traces generated from SPEC2017, SPEC2006, and Cloud suites with ~11-KB overhead. Moreover, we also reduce the number of negative outliers by more than 89%, and the performance loss of the worst-case negative outlier from 25% to only 5% compared to the state of the art.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4307884499",
    "type": "article"
  },
  {
    "title": "User-driven Online Kernel Fusion for SYCL",
    "doi": "https://doi.org/10.1145/3571284",
    "publication_date": "2022-11-18",
    "publication_year": 2022,
    "authors": "Víctor Pérez; Lukáš Sommer; Victor Lomüller; Kumudha Narasimhan; Mehdi Goli",
    "corresponding_authors": "",
    "abstract": "Heterogeneous programming models are becoming increasingly popular to support the ever-evolving hardware architectures, especially for new and emerging specialized accelerators optimizing specific tasks. While such programs provide performance portability of the existing applications across various heterogeneous architectures to some extent, short-running device kernels can affect an application performance due to overheads of data transfer, synchronization, and kernel launch. While in applications with one or two short-running kernels the overhead can be negligible, it can be noticeable when these short-running kernels dominate the overall number of kernels in an application, as it is the case in graph-based neural network models, where there are several small memory-bound nodes alongside few large compute-bound nodes. To reduce the overhead, combining several kernels into a single, more optimized kernel is an active area of research. However, this task can be time-consuming and error-prone given the huge set of potential combinations. This can push programmers to seek a tradeoff between (a) task-specific kernels with low overhead but hard to maintain and (b) smaller modular kernels with higher overhead but easier to maintain. While there are DSL-based approaches, such as those provided for machine learning frameworks, which offer the possibility of such a fusion, they are limited to a particular domain and exploit specific knowledge of that domain and, as a consequence, are hard to port elsewhere. This study explores the feasibility of a user-driven kernel fusion through an extension to the SYCL API to address the automation of kernel fusion. The proposed solution requires programmers to define the subgraph regions that are potentially suitable for fusion without any modification to the kernel code or the function signature. We evaluate the performance benefit of our approach on common neural networks and study the performance improvement in detail.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4309467950",
    "type": "article"
  },
  {
    "title": "ACTION: Adaptive Cache Block Migration in Distributed Cache Architectures",
    "doi": "https://doi.org/10.1145/3572911",
    "publication_date": "2022-11-29",
    "publication_year": 2022,
    "authors": "Chandra Sekhar Mummidi; Sandip Kundu",
    "corresponding_authors": "",
    "abstract": "Chip multiprocessors (CMP) with more cores have more traffic to the last-level cache (LLC) . Without a corresponding increase in LLC bandwidth, such traffic cannot be sustained, resulting in performance degradation. Previous research focused on data placement techniques to improve access latency in Non-Uniform Cache Architectures (NUCA) . Placing data closer to the referring core reduces traffic in cache interconnect. However, earlier data placement work did not account for the frequency with which specific memory references are accessed. The difficulty of tracking access frequency for all memory references is one of the main reasons why it was not considered in NUCA data placement. In this research, we present a hardware-assisted solution called ACTION ( A daptive C ache Block Migra tion ) to track the access frequency of individual memory references and prioritize placement of frequently referred data closer to the affine core. ACTION mechanism implements cache block migration when there is a detectable change in access frequencies due to a shift in the program phase. ACTION counts access references in the LLC stream using a simple and approximate method and uses a straightforward placement and migration solution to keep the hardware overhead low. We evaluate ACTION on a 4-core CMP with a 5x5 mesh LLC network implementing a partitioned D-NUCA against workloads exhibiting distinct asymmetry in cache block access frequency. Our simulation results indicate that ACTION can improve CMP performance by up to 7.5% over state-of-the-art (SOTA) D-NUCA solutions.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4310362092",
    "type": "article"
  },
  {
    "title": "An Instruction Inflation Analyzing Framework for Dynamic Binary Translators",
    "doi": "https://doi.org/10.1145/3640813",
    "publication_date": "2024-01-15",
    "publication_year": 2024,
    "authors": "B. P. Xie; Yue Yan; Chenghao Yan; S. Tao; Zhuangzhuang Zhang; Xinyu Li; Yanzhi Lan; Xiang Wu; Tianyi Liu; Tingting Zhang; Fuxin Zhang",
    "corresponding_authors": "",
    "abstract": "Dynamic binary translators (DBTs) are widely used to migrate applications between different instruction set architectures (ISAs). Despite extensive research to improve DBT performance, noticeable overhead remains, preventing near-native performance, especially when translating from complex instruction set computer (CISC) to reduced instruction set computer (RISC). For computational workloads, the main overhead stems from translated code quality. Experimental data show that state-of-the-art DBT products have dynamic code inflation of at least 1.46. This indicates that on average, more than 1.46 host instructions are needed to emulate one guest instruction. Worse, inflation closely correlates with translated code quality. However, the detailed sources of instruction inflation remain unclear. To understand the sources of inflation, we present Deflater , an instruction inflation analysis framework comprising a mathematical model, a collection of black-box unit tests called BenchMIAOes , and a trace-based simulator called InflatSim . The mathematical model calculates overall inflation based on the inflation of individual instructions and translation block optimizations. BenchMIAOes extract model parameters from DBTs without accessing DBT source code. InflatSim implements the model and uses the extracted parameters from BenchMIAOes to simulate a given DBT’s behavior. Deflater is a valuable tool to guide DBT analysis and improvement. Using Deflater, we simulated inflation for three state-of-the-art CISC-to-RISC DBTs: ExaGear, Rosetta2, and LATX, with inflation errors of 5.63%, 5.15%, and 3.44%, respectively for SPEC CPU 2017, gaining insights into these commercial DBTs. Deflater also efficiently models inflation for the open source DBT QEMU and suggests optimizations that can substantially reduce inflation. Implementing the suggested optimizations confirms Deflater’s effective guidance, with 4.65% inflation error, and gains 5.47x performance improvement.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4390880533",
    "type": "article"
  },
  {
    "title": "SLAP: Segmented Reuse-Time-Label Based Admission Policy for Content Delivery Network Caching",
    "doi": "https://doi.org/10.1145/3646550",
    "publication_date": "2024-02-09",
    "publication_year": 2024,
    "authors": "Ke Liu; Kan Wu; Hua Wang; Ke Zhou; Peng Wang; Ji Zhang; Cong Li",
    "corresponding_authors": "",
    "abstract": "‘‘Learned” admission policies have shown promise in improving Content Delivery Network (CDN) cache performance and lowering operational costs. Unfortunately, existing learned policies are optimized with a few fixed cache sizes while in reality, cache sizes often vary over time in an unpredictable manner. As a result, existing solutions cannot provide consistent benefits in production settings. We present SLAP , a learned CDN cache admission approach based on segmented object reuse time prediction. SLAP predicts an object’s reuse time range using the Long-Short-Term-Memory model and admits objects that will be reused (before eviction) given the current cache size. SLAP decouples model training from cache size, allowing it to adapt to arbitrary sizes. The key to our solution is a novel segmented labeling scheme that makes SLAP without requiring precise prediction on object reuse time. To further make SLAP a practical and efficient solution, we propose aggressive reusing of computation and training on sampled traces to optimize model training, and a specialized predictor architecture that overlaps prediction computation with miss object fetching to optimize model inference. Our experiments using production CDN traces show that SLAP achieves significantly lower write traffic (38%-59%), longer SSDs lifetime (104%-178%), a consistently higher hit rate (3.2%-11.7%), and requires no effort to adapt to changing cache sizes, outperforming existing policies.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4391680354",
    "type": "article"
  },
  {
    "title": "FASA-DRAM: Reducing DRAM Latency with Destructive Activation and Delayed Restoration",
    "doi": "https://doi.org/10.1145/3649135",
    "publication_date": "2024-02-23",
    "publication_year": 2024,
    "authors": "Haitao Du; Yuhan Qin; Chen Song; Yi Kang",
    "corresponding_authors": "",
    "abstract": "DRAM memory is a performance bottleneck for many applications, due to its high access latency. Previous work has mainly focused on data locality, introducing small-but-fast regions to cache frequently accessed data, thereby reducing the average latency. However, these locality-based designs have three challenges in modern multi-core systems: 1) Inter-application interference leads to random memory access traffic. 2) Fairness issues prevent the memory controller from over-prioritizing data locality. 3) Write-intensive applications have much lower locality and evict substantial dirty entries. With frequent data movement between the fast in-DRAM cache and slow regular arrays, the overhead induced by moving data may even offset the performance and energy benefits of in-DRAM caching. In this paper, we decouple the data movement process into two distinct phases. The first phase is Load-Reduced Destructive Activation (LRDA), which destructively promotes data into the in-DRAM cache. The second phase is Delayed Cycle-Stealing Restoration (DCSR), which restores the original data when DRAM bank is idle. LRDA decouples the most time-consuming restoration phase from activation, and DCSR hides the restoration latency through prevalent bank-level parallelism. We propose FASA-DRAM incorporating destructive activation and delayed restoration techniques to enable both in-DRAM caching and proactive latency-hiding mechanisms. Our evaluation shows that FASA-DRAM improves the average performance by 19.9% and reduces average DRAM energy consumption by 18.1% over DDR4 DRAM for four-core workloads, with less than 3.4% extra area overhead. Furthermore, FASA-DRAM outperforms state-of-the-art designs in both performance and energy efficiency.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4392121132",
    "type": "article"
  },
  {
    "title": "The Droplet Search Algorithm for Kernel Scheduling",
    "doi": "https://doi.org/10.1145/3650109",
    "publication_date": "2024-02-29",
    "publication_year": 2024,
    "authors": "Michael Canesche; Vanderson Martins do Rosário; Edson Borin; Fernando Magno Quintão Pereira",
    "corresponding_authors": "",
    "abstract": "Kernel scheduling is the problem of finding the most efficient implementation for a computational kernel. Identifying this implementation involves experimenting with the parameters of compiler optimizations, such as the size of tiling windows and unrolling factors. This article shows that it is possible to organize these parameters as points in a coordinate space. The function that maps these points to the running time of kernels, in general, will not determine a convex surface. However, this article provides empirical evidence that the origin of this surface (an unoptimized kernel) and its global optimum (the fastest kernel) reside on a convex region. We call this hypothesis the “droplet expectation.” Consequently, a search method based on the Coordinate Descent algorithm tends to find the optimal kernel configuration quickly if the hypothesis holds. This approach—called Droplet Search—has been available in Apache TVM since April of 2023. Experimental results with six large deep learning models on various computing devices (ARM, Intel, AMD, and NVIDIA) indicate that Droplet Search is not only as effective as other AutoTVM search techniques but also 2 to 10 times faster. Moreover, models generated by Droplet Search are competitive with those produced by TVM’s AutoScheduler (Ansor), despite the latter using 4 to 5 times more code transformations than AutoTVM.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4392295176",
    "type": "article"
  },
  {
    "title": "x <scp>Meta</scp> : SSD-HDD-hybrid Optimization for Metadata Maintenance of Cloud-scale Object Storage",
    "doi": "https://doi.org/10.1145/3652606",
    "publication_date": "2024-03-13",
    "publication_year": 2024,
    "authors": "Yan Chen; Qiwen Ke; Huiba Li; Yongwei Wu; Yiming Zhang",
    "corresponding_authors": "",
    "abstract": "Object storage has been widely used in the cloud. Traditionally, the size of object metadata is much smaller than that of object data, and thus existing object storage systems (such as Ceph and Oasis) can place object data and metadata, respectively, on hard disk drives (HDDs) and solid-state drives (SSDs) to achieve high I/O performance at a low monetary cost. Currently, however, a wide range of cloud applications organize their data as large numbers of small objects of which the data size is close to (or even smaller than) the metadata size, thus greatly increasing the cost if placing all metadata on expensive SSDs. This article presents x Meta , an SSD-HDD-hybrid optimization for metadata maintenance of cloud-scale object storage. We observed that a substantial portion of the metadata of small objects is rarely accessed and thus can be stored on HDDs with little performance penalty. Therefore, x Meta first classifies the hot and cold metadata based on the frequency of metadata accesses of upper-layer applications and then adaptively stores the hot metadata on SSDs and the cold metadata on HDDs. We also propose a merging mechanism for hot metadata to further improve the efficiency of SSD storage and optimize range key query and insertion for hot metadata by designing composite keys. We have integrated the x Meta metadata service with Ceph to realize a high-performance, low-cost object store (called xCeph). The extensive evaluation shows that xCeph outperforms the original Ceph by an order of magnitude in the space requirement of SSD storage, while improving the throughput by up to 2.7×.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4392740989",
    "type": "article"
  },
  {
    "title": "Orchard: Heterogeneous Parallelism and Fine-grained Fusion for Complex Tree Traversals",
    "doi": "https://doi.org/10.1145/3652605",
    "publication_date": "2024-03-15",
    "publication_year": 2024,
    "authors": "Vidush Singhal; Laith Sakka; Kirshanthan Sundararajah; Ryan Newton; Milind Kulkarni",
    "corresponding_authors": "",
    "abstract": "Many applications are designed to perform traversals on tree-like data structures. Fusing and parallelizing these traversals enhance the performance of applications. Fusing multiple traversals improves the locality of the application. The runtime of an application can be significantly reduced by extracting parallelism and utilizing multi-threading. Prior frameworks have tried to fuse and parallelize tree traversals using coarse-grained approaches, leading to missed fine-grained opportunities for improving performance. Other frameworks have successfully supported fine-grained fusion on heterogeneous tree types but fall short regarding parallelization. We introduce a new framework Orchard built on top of Grafter . Orchard ’s novelty lies in allowing the programmer to transform tree traversal applications by automatically applying fine-grained fusion and extracting heterogeneous parallelism. Orchard allows the programmer to write general tree traversal applications in a simple and elegant embedded Domain-Specific Language (eDSL). We show that the combination of fine-grained fusion and heterogeneous parallelism performs better than each alone when the conditions are met.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4392864418",
    "type": "article"
  },
  {
    "title": "Cerberus: Triple Mode Acceleration of Sparse Matrix and Vector Multiplication",
    "doi": "https://doi.org/10.1145/3653020",
    "publication_date": "2024-03-17",
    "publication_year": 2024,
    "authors": "Soojin Hwang; Daehyeon Baek; Jongse Park; Jaehyuk Huh",
    "corresponding_authors": "",
    "abstract": "The multiplication of sparse matrix and vector (SpMV) is one of the most widely used kernels in high-performance computing as well as machine learning acceleration for sparse neural networks. The design space of SpMV accelerators has two axes: algorithm and matrix representation. There have been two widely used algorithms and data representations. Two algorithms, scalar multiplication and dot product, can be combined with two sparse data representations, compressed sparse and bitmap formats for the matrix and vector. Although the prior accelerators adopted one of the possible designs, it is yet to be investigated which design is the best one across different hardware resources and workload characteristics. This paper first investigates the impact of design choices with respect to the algorithm and data representation. Our evaluation shows that no single design always outperforms the others across different workloads, but the two best designs (i.e., compressed sparse format and bitmap format with dot product) have complementary performance with trade-offs incurred by the matrix characteristics. Based on the analysis, this study proposes Cerberus, a triple-mode accelerator supporting two sparse operation modes in addition to the base dense mode. To allow such multi-mode operation, it proposes a prediction model based on matrix characteristics under a given hardware configuration, which statically selects the best mode for a given sparse matrix with its dimension and density information. Our experimental results show that Cerberus provides 12.1× performance improvements from a dense-only accelerator, and 1.5× improvements from a fixed best SpMV design.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4392894994",
    "type": "article"
  },
  {
    "title": "iSwap: A New Memory Page Swap Mechanism for Reducing Ineffective I/O Operations in Cloud Environments",
    "doi": "https://doi.org/10.1145/3653302",
    "publication_date": "2024-03-23",
    "publication_year": 2024,
    "authors": "Zhuohao Wang; Lei Liu; Limin Xiao",
    "corresponding_authors": "",
    "abstract": "This article proposes iSwap , a new memory page swap mechanism that reduces the ineffective I/O swap operations and improves the QoS for applications with a high priority in cloud environments. iSwap works in the OS kernel. iSwap accurately learns the reuse patterns for memory pages and makes the swap decisions accordingly to avoid ineffective operations. In the cases where memory pressure is high, iSwap compresses pages that belong to the latency-critical (LC) applications (or high-priority applications) and keeps them in main memory, avoiding I/O operations for these LC applications to ensure QoS, and iSwap evicts low-priority applications’ pages out of main memory. iSwap has a low overhead and works well for cloud applications with large memory footprints. We evaluate iSwap on Intel x86 and ARM platforms. The experimental results show that iSwap can significantly reduce ineffective swap operations (8.0%–19.2%) and improve the QoS for LC applications (36.8%–91.3%) in cases where memory pressure is high, compared with the latest LRU-based approach widely used in modern OSes.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4393118821",
    "type": "article"
  },
  {
    "title": "D <b> <sup>2</sup> </b> Comp: Efficient Offload of LSM-tree Compaction with Data Processing Units on Disaggregated Storage",
    "doi": "https://doi.org/10.1145/3656584",
    "publication_date": "2024-04-09",
    "publication_year": 2024,
    "authors": "Chen Ding; Jian Zhou; Kai Lü; Sicen Li; Yiqin Xiong; Jiguang Wan; Ling Zhan",
    "corresponding_authors": "",
    "abstract": "LSM-based key-value stores suffer from sub-optimal performance due to their slow and heavy background compactions. The compaction brings severe CPU and network overhead on high-speed disaggregated storage. This article further reveals that data-intensive compression in compaction consumes a significant portion of CPU power. Moreover, the multi-threaded compactions cause substantial CPU contention and network traffic during high-load periods. Based on the above observations, we propose fine-grained dynamical compaction offloading by leveraging the modern Data Processing Unit (DPU) to alleviate the CPU and network overhead. To achieve this, we first customized a file system to enable efficient data access for DPU. We then leverage the Arm cores on the DPU to meet the burst CPU and network requirements to reduce resource contention and data movement. We further employ dedicated hardware-based accelerators on the DPU to speed up the compression in compactions. We integrate our DPU-offloaded compaction with RocksDB and evaluate it with NVIDIA’s latest Bluefield-2 DPU on a real system. The evaluation shows that the DPU is an effective solution to solve the CPU bottleneck and reduce data traffic of compaction. The results show that compaction performance is accelerated by 2.86 to 4.03 times, system write and read throughput is improved by up to 3.2 times and 1.4 times respectively, and host CPU contention and network traffic are effectively reduced compared to the fine-tuned CPU-only baseline.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4394607202",
    "type": "article"
  },
  {
    "title": "ReHarvest: an ADC Resource-Harvesting Crossbar Architecture for ReRAM-Based DNN Accelerators",
    "doi": "https://doi.org/10.1145/3659208",
    "publication_date": "2024-04-17",
    "publication_year": 2024,
    "authors": "Jiahong Xu; Haikun Liu; Zhuohui Duan; Xiaofei Liao; Hai Jin; Xiaokang Yang; Huize Li; Cong Liu; Fubing Mao; Yu Zhang",
    "corresponding_authors": "",
    "abstract": "ReRAM-based Processing-In-Memory (PIM) architectures have been increasingly explored to accelerate various Deep Neural Network (DNN) applications because they can achieve extremely high performance and energy-efficiency for in-situ analog Matrix-Vector Multiplication (MVM) operations. However, since ReRAM crossbar arrays’ peripheral circuits– analog-to-digital converters (ADCs) often feature high latency and low area efficiency, AD conversion has become a performance bottleneck of in-situ analog MVMs. Moreover, since each crossbar array is tightly coupled with very limited ADCs in current ReRAM-based PIM architectures, the scarce ADC resource is often underutilized. In this article, we propose ReHarvest, an ADC-crossbar decoupled architecture to improve the utilization of ADC resource. Particularly, we design a many-to-many mapping structure between crossbars and ADCs to share all ADCs in a tile as a resource pool, and thus one crossbar array can harvest much more ADCs to parallelize the AD conversion for each MVM operation. Moreover, we propose a multi-tile matrix mapping (MTMM) scheme to further improve the ADC utilization across multiple tiles by enhancing data parallelism. To support fine-grained data dispatching for the MTMM, we also design a bus-based interconnection network to multicast input vectors among multiple tiles, and thus eliminate data redundancy and potential network congestion during multicasting. Extensive experimental results show that ReHarvest can improve the ADC utilization by 3.2×, and achieve 3.5× performance speedup while reducing the ReRAM resource consumption by 3.1× on average compared with the state-of-the-art PIM architecture–FORMS.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4394891385",
    "type": "article"
  },
  {
    "title": "GraphSER: Distance-Aware Stream-Based Edge Repartition for Many-Core Systems",
    "doi": "https://doi.org/10.1145/3661998",
    "publication_date": "2024-04-26",
    "publication_year": 2024,
    "authors": "J.G. Li; Yi Kang",
    "corresponding_authors": "",
    "abstract": "With the explosive growth of graph data, distributed graph processing has become popular, and many graph hardware accelerators use distributed frameworks. Graph partitioning is foundation in distributed graph processing. However, dynamic changes in graph make existing partitioning shifted from its optimized points and cause system performance degraded. Therefore, more efficient dynamic graph partition methods are needed. In this work, we propose GraphSER, a dynamic graph partition method for many-core systems. To improve the cross-node spatial locality and reduce the overhead of repartition, we propose a stream-based edge repartition, in which each computing node sequentially traverses its local edge list in parallel, then migrating edges based on distance and replica degree. GraphSER does not need costly searching and prioritizes nodes so it can avoid poor cross-node spatial locality. Our evaluation shows that compared to state-of-the-art edge repartition software methods, GraphSER has an average speedup of 1.52×, with the maximum up to 2×. Compared to the previous many-core hardware repartition method, GraphSER performance has an average of 40% improvement, with the maximum to 117%.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4395685215",
    "type": "article"
  },
  {
    "title": "Asynchronous Memory Access Unit: Exploiting Massive Parallelism for Far Memory Access",
    "doi": "https://doi.org/10.1145/3663479",
    "publication_date": "2024-05-09",
    "publication_year": 2024,
    "authors": "L. J. Wang; Xu Zhang; Songyue Wang; Zhuolun Jiang; Tianyue Lu; Mingyu Chen; Siwei Luo; Keji Huang",
    "corresponding_authors": "",
    "abstract": "The growing memory demands of modern applications have driven the adoption of far memory technologies in data centers to provide cost-effective, high-capacity memory solutions. However, far memory presents new performance challenges because its access latencies are significantly longer and more variable than local DRAM. For applications to achieve acceptable performance on far memory, a high degree of memory-level parallelism (MLP) is needed to tolerate the long access latency. While modern out-of-order processors are capable of exploiting a certain degree of MLP, they are constrained by resource limitations and hardware complexity. The key obstacle is the synchronous memory access semantics of traditional load/store instructions, which occupy critical hardware resources for a long time. The longer far memory latencies exacerbate this limitation. This article proposes a set of Asynchronous Memory Access Instructions (AMI) and its supporting function unit, Asynchronous Memory Access Unit (AMU), inside contemporary Out-of-Order Core. AMI separates memory request issuing from response handling to reduce resource occupation. Additionally, AMU architecture supports up to several hundreds of asynchronous memory requests through re-purposing a portion of L2 Cache as scratchpad memory (SPM) to provide sufficient temporal storage. Together with a coroutine-based programming framework, this scheme can achieve significantly higher MLP for hiding far memory latencies. Evaluation with a cycle-accurate simulation shows AMI achieves 2.42× speedup on average for memory-bound benchmarks with 1μs additional far memory latency. Over 130 outstanding requests are supported with 26.86× speedup for GUPS (random access) with 5 μs latency. These demonstrate how the techniques tackle far memory performance impacts through explicit MLP expression and latency adaptation.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4396773547",
    "type": "article"
  },
  {
    "title": "Stripe-schedule Aware Repair in Erasure-coded Clusters with Heterogeneous Star Networks",
    "doi": "https://doi.org/10.1145/3664926",
    "publication_date": "2024-05-13",
    "publication_year": 2024,
    "authors": "Hai Zhou; Dan Feng",
    "corresponding_authors": "",
    "abstract": "More and more storage systems use erasure code to tolerate faults. It takes pieces of data blocks as input and encodes a small number of parity blocks as output, where these blocks form a stripe. When reconsidering the recovery problem in the multi-stripe level and heterogeneous network clusters, quickly generating an efficient multi-stripe recovery solution that reduces recovery time remains a challenging and time-consuming task. Previous works either use a greedy algorithm that may fall into the local optimal and have low recovery performance or a meta-heuristic algorithm with a long running time and low solution generation efficiency. In this article, we propose a Stripe-schedule Aware Repair (SARepair) technique for multi-stripe recovery in heterogeneous erasure-coded clusters based on Reed–Solomon code. By carefully examining the metadata of blocks, SARepair intelligently adjusts the recovery solution for each stripe and obtains another multi-stripe solution with less recovery time in a computationally efficient manner. It then tolerates worse solutions to overcome the local optimal and uses a rollback mechanism to adjust search regions to reduce recovery time further. Moreover, instead of reading blocks sequentially from each node, SARepair also selectively schedules the reading order for each block to reduce the memory overhead. We extend SARepair to address the full-node recovery and adapt to the LRC code. We prototype SARepair and show via both simulations and Amazon EC2 experiments that the recovery performance can be improved by up to 59.97% over a state-of-the-art recovery approach while keeping running time and memory overhead low.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4396861065",
    "type": "article"
  },
  {
    "title": "Fixed-point Encoding and Architecture Exploration for Residue Number Systems",
    "doi": "https://doi.org/10.1145/3664923",
    "publication_date": "2024-05-14",
    "publication_year": 2024,
    "authors": "Bobin Deng; Bhargava Sai Nadendla; Kun Suo; Yixin Xie; Dan Lo",
    "corresponding_authors": "",
    "abstract": "Residue Number Systems (RNS) demonstrate the fascinating potential to serve integer addition/ multiplication-intensive applications. The complexity of Artificial Intelligence (AI) models has grown enormously in recent years. From a computer system’s perspective, ensuring the training of these large-scale AI models within an adequate time and energy consumption has become a big concern. Matrix multiplication is a dominant subroutine in many prevailing AI models, with an addition/multiplication-intensive attribute. However, the data type of matrix multiplication within machine learning training typically requires real numbers, which indicates that RNS benefits for integer applications cannot be directly gained by AI training. The state-of-the-art RNS real-number encodings, including floating-point and fixed-point, have defects and can be further enhanced. To transform default RNS benefits to the efficiency of large-scale AI training, we propose a low-cost and high-accuracy RNS fixed-point representation: Single RNS Logical Partition (S-RNS-Logic-P) representation with Scaling-down Postprocessing Multiplication (SD-Post-Mul) . Moreover, we extend the implementation details of the other two RNS fixed-point methods: Double RNS Concatenation and S-RNS-Logic-P representation with Scaling-down Preprocessing Multiplication . We also design the architectures of these three fixed-point multipliers. In empirical experiments, our S-RNS-Logic-P representation with SD-Post-Mul method achieves less latency and energy overhead while maintaining good accuracy. Furthermore, this method can easily extend to the Redundant Residue Number System to raise the efficiency of error-tolerant domains, such as improving the error correction efficiency of quantum computing.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4396893475",
    "type": "article"
  },
  {
    "title": "Optimization of Sparse Matrix Computation for Algebraic Multigrid on GPUs",
    "doi": "https://doi.org/10.1145/3664924",
    "publication_date": "2024-05-15",
    "publication_year": 2024,
    "authors": "Yizhuo Wang; Fangli Chang; Bingxin Wei; Jianhua Gao; Weixing Ji",
    "corresponding_authors": "",
    "abstract": "AMG is one of the most efficient and widely used methods for solving sparse linear systems. The computational process of AMG mainly consists of a series of iterative calculations of generalized sparse matrix-matrix multiplication (SpGEMM) and sparse matrix-vector multiplication (SpMV). Optimizing these sparse matrix calculations is crucial for accelerating solving linear systems. In this paper, we first focus on optimizing the SpGEMM algorithm in AmgX, a popular AMG library for GPUs. We propose a new algorithm called SpGEMM-upper, which achieves an average speedup of 2.02× on Tesla V100 and 1.96× on RTX 3090 against the original algorithm. Next, through experimental investigation, we conclude that no single SpGEMM library or algorithm performs optimally for most sparse matrices, and the same holds true for SpMV. Therefore, we build machine learning-based models to predict the optimal SpGEMM and SpMV used in the AMG calculation process. Finally, we integrate the prediction models, SpGEMM-upper, and other selected algorithms into a framework for adaptive sparse matrix computation in AMG. Our experimental results prove that the framework achieves promising performance improvements on the test set.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4396927956",
    "type": "article"
  },
  {
    "title": "ReIPE: Recycling Idle PEs in CNN Accelerator for Vulnerable Filters Soft-Error Detection",
    "doi": "https://doi.org/10.1145/3674909",
    "publication_date": "2024-06-28",
    "publication_year": 2024,
    "authors": "Xiaohui Wei; Chenyang Wang; Hengshan Yue; Jingweijia Tan; Zeyu Guan; Nan Jiang; Xinyang Zheng; Jianpeng Zhao; Meikang Qiu",
    "corresponding_authors": "",
    "abstract": "To satisfy prohibitively massive computational requirements of current deep Convolutional Neural Networks (CNNs), CNN-specific accelerators are widely deployed in large-scale systems. Caused by high-energy neutrons and α-particle strikes, soft error may lead to catastrophic failures when CNN is deployed on high integration density accelerators. As CNNs become ubiquitous in mission-critical domains, ensuring the reliable execution of CNN accelerators in the presence of soft errors is increasingly essential. In this article, we propose to Re cycle I dle P rocessing E lements (PEs) in the CNN accelerator for vulnerable filters soft error detection (ReIPE). Considering the error-sensitivity of filters, ReIPE first carries out a filter-level gradient analysis process to replace fault injection for fast filter-wise error resilience estimation. Then, to achieve maximal reliability benefits, combining the hardware-level systolic array idleness and software-level CNN filter-wise error resilience profile, ReIPE preferentially duplicated loads the most vulnerable filters onto systolic array to recycle idle-column PEs for opportunistically redundant execution (error detection). Exploiting the data reuse properties of accelerators, ReIPE incorporates the error detection process into the original computation flow of accelerators to perform real-time error detection. Once the error is detected, ReIPE will trigger a correction round to rectify the erroneous output. Experimental results performed on LeNet-5, Cifar-10-CNN, AlexNet, ResNet-20, VGG-16, and ResNet-50 exhibit that ReIPE can cover 96.40% of errors while reducing 75.06% performance degradation and 67.79% energy consumption of baseline dual modular redundancy on average. Moreover, to satisfy the reliability requirements of various application scenarios, ReIPE is also applicable for pruned, quantized, and Transformer-based models, as well as portable to other accelerator architectures.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4400123707",
    "type": "article"
  },
  {
    "title": "Optimization of Large-Scale Sparse Matrix-Vector Multiplication on Multi-GPU Systems",
    "doi": "https://doi.org/10.1145/3676847",
    "publication_date": "2024-07-08",
    "publication_year": 2024,
    "authors": "Jianhua Gao; Weixing Ji; Yizhuo Wang",
    "corresponding_authors": "",
    "abstract": "Sparse matrix-vector multiplication (SpMV) is one of the important kernels of many iterative algorithms for solving sparse linear systems. The limited storage and computational resources of individual GPUs restrict both the scale and speed of SpMV computing in problem-solving. As real-world engineering problems continue to increase in complexity, the imperative for collaborative execution of iterative solving algorithms across multiple GPUs is increasingly apparent. Although the multi-GPU-based SpMV takes less kernel execution time, it also introduces additional data transmission overhead, which diminishes the performance gains derived from parallelization across multi-GPUs. Based on the non-zero elements distribution characteristics of sparse matrices and the tradeoff between redundant computations and data transfer overhead, this article introduces a series of SpMV optimization techniques tailored for multi-GPU environments and effectively enhances the execution efficiency of iterative algorithms on multiple GPUs. First, we propose a two-level non-zero elements-based matrix partitioning method to increase the overlap of kernel execution and data transmission. Then, considering the irregular non-zero elements distribution in sparse matrices, a long-row-aware matrix partitioning method is proposed to hide more data transmissions. Finally, an optimization using redundant and inexpensive short-row execution to exchange costly data transmission is proposed. Our experimental evaluation demonstrates that, compared with the SpMV on a single GPU, the proposed method achieves an average speedup of 2.00× and 1.85× on platforms equipped with two RTX 3090 and two Tesla V100-SXM2, respectively. The average speedup of 2.65× is achieved on a platform equipped with four Tesla V100-SXM2.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4400422163",
    "type": "article"
  },
  {
    "title": "Time-Aware Spectrum-Based Bug Localization for Hardware Design Code with Data Purification",
    "doi": "https://doi.org/10.1145/3678009",
    "publication_date": "2024-07-12",
    "publication_year": 2024,
    "authors": "Jiang Wu; Zhuo Zhang; Deheng Yang; Jianjun Xu; Jiayu He; Xiaoguang Mao",
    "corresponding_authors": "",
    "abstract": "The verification of hardware design code is a critical aspect in ensuring the quality and reliability of hardware products. Finding bugs in hardware design code is important for hardware development and is frequently considered as a notoriously challenging and time-consuming activity while being an essential aspect of verification. Thus, bug localization techniques that could assist manual debugging have attracted much attention in the hardware community. However, there exists an unpredictable time span between the precise origin of a bug and its detected manifestation in prior work without costly formal verification. Locating the bug responsible for the exposed discrepancy between expected and exhibited design behavior remains a major challenge. In this work, we propose Tartan, a T ime- a ware spect r um-based bug localiza t ion with d a ta purificatio n for hardware design code to address these limitations. Tartan integrates hardware-specific timing information with the spectrum and captures the changes of executed statements when the state of the circuit changes to effectively locate bugs. Further, Tartan purifies the spectrum data from the simulation and evaluates the suspiciousness of the statements in the design to indicate the likelihood of being buggy. To evaluate the effectiveness of Tartan, we conduct large-scale experiments on 69 versions of 15 hardware projects by the state-of-the-art bug localization techniques. The experimental results clearly show that Tartan is statistically more effective than the baselines. It provides a new perspective on hardware design code bug localization and brings fresh insights to the community.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4400587308",
    "type": "article"
  },
  {
    "title": "EA4RCA: Efficient AIE accelerator design framework for regular Communication-Avoiding Algorithm",
    "doi": "https://doi.org/10.1145/3678010",
    "publication_date": "2024-07-15",
    "publication_year": 2024,
    "authors": "Wenbo Zhang; Yiqi Liu; Tianhao Zang; Zhenshan Bao",
    "corresponding_authors": "",
    "abstract": "With the introduction of the Adaptive Intelligence Engine (AIE), the Versal Adaptive Compute Acceleration Platform (Versal ACAP) has garnered great attention. However, the current focus of Vitis Libraries and limited research has mainly been on how to invoke AIE modules, without delving into a thorough discussion on effectively utilizing AIE in its typical use cases. As a result, the widespread adoption of Versal ACAP has been restricted. The Communication Avoidance (CA) algorithm is considered a typical application within the AIE architecture. Nevertheless, the effective utilization of AIE in CA applications remains an area that requires further exploration. We propose a top-down customized design framework, EA4RCA (Efficient AIE accelerator design framework for regular Communication-Avoiding Algorithm), specifically tailored for CA algorithms with regular communication patterns, and equipped with AIE Graph Code Generator software to accelerate the AIE design process. The primary objective of this framework is to maximize the performance of AIE while incorporating high-speed data streaming services. Experiments show that for the RCA algorithm Filter2D and Matrix Multiple (MM) with lower communication requirements and the RCA algorithm FFT with higher communication requirements, the accelerators implemented by the RA4RCA framework achieve the highest throughput improvements of 22.19×, 1.05×, and 3.88× compared with the current highest performance acceleration scheme (SOTA), and the highest energy efficiency improvements of 6.11×, 1.30× and 7.00×.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4400656480",
    "type": "article"
  },
  {
    "title": "Data Deduplication Based on Content Locality of Transactions to Enhance Blockchain Scalability",
    "doi": "https://doi.org/10.1145/3680547",
    "publication_date": "2024-07-25",
    "publication_year": 2024,
    "authors": "Cheng-Long Yi; J. Liu; Shenggang Wan; Juntao Fang; Bin Sun; Linhai Zhang",
    "corresponding_authors": "",
    "abstract": "Blockchain is a promising infrastructure for the internet and digital economy, but it has serious scalability problems, that is, long block synchronization time and high storage cost. Conventional coarse-grained data deduplication schemes (block or file level) are proved to be ineffective on improving the scalability of blockchains. Based on comprehensive analysis on typical blockchain workloads, we propose two new locality concepts (economic and argument locality) and a novel fine-grained data deduplication scheme (transaction level) named Alias-Chain. Specifically, Alias-Chain replaces frequently used data, for example, smart contract arguments, with much shorter aliases to reduce the block sizes, which results in both shorter synchronization time and lower storage cost. Furthermore, to solve the potential consistency issue in Alias-Chain, we propose two complementary techniques: one is generating aliases from history blocks with high consistency, and the other is speeding up the generation of aliases via a specific algorithm. Our simulation results show: (1) the average transfer and SC-call transaction (a transaction used to call the smart contracts in the blockchain) sizes can be significantly reduced by up to 11.03% and 79.44% in native Ethereum, and up to 39.29% and 81.84% in Ethereum optimized by state-of-the-art techniques; and (2) the two complementary techniques well address the inconsistency risk with very limited impact on the benefit of Alias-Chain. Prototyping-based experiments are further conducted on a testbed consisting of up to 3200 miners. The results demonstrate the effectiveness and efficiency of Alias-Chain on reducing block synchronization time and storage cost under typical real-world workloads.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4400984149",
    "type": "article"
  },
  {
    "title": "Pac-Sim: Simulation of Multi-threaded Workloads using Intelligent, Live Sampling",
    "doi": "https://doi.org/10.1145/3680548",
    "publication_date": "2024-08-27",
    "publication_year": 2024,
    "authors": "Changxi Liu; Alen Sabu; Akanksha Chaudhari; Qingxuan Kang; Trevor E. Carlson",
    "corresponding_authors": "",
    "abstract": "High-performance, multi-core processors are the key to accelerating workloads in several application domains. To continue to scale performance at the limit of Moore’s Law and Dennard scaling, software and hardware designers have turned to dynamic solutions that adapt to the needs of applications in a transparent, automatic way. For example, modern hardware improves its performance and power efficiency by changing the hardware configuration, like the frequency and voltage of cores, according to a number of parameters, such as the technology used or the workload running at the time. With this level of dynamism, it is essential to simulate next-generation multi-core processors in a way that can both respond to system changes and accurately determine system performance metrics. Currently, no sampled simulation platform can achieve these goals of dynamic, fast, and accurate simulation of multi-threaded workloads. In this work, we propose a solution that allows for fast, accurate simulation in the presence of both hardware and software dynamism. To accomplish this goal, we present Pac-Sim, a novel sampled simulation methodology for fast, accurate sampled simulation that requires no upfront analysis of the workload. With our proposed methodology, it is now possible to simulate long-running dynamically scheduled multi-threaded programs with significant simulation speedups, even in the presence of dynamic hardware events. We evaluate Pac-Sim using the SPEC CPU2017, NPB, and PARSEC multi-threaded benchmarks with both static and dynamic thread scheduling. The experimental results show that Pac-Sim achieves a very low sampling error of 1.63% and 3.81% on average for statically and dynamically scheduled benchmarks, respectively. Pac-Sim also demonstrates significant simulation speedups as high as 523.5× (210.3× on average) for the training input set of SPEC CPU2017 running eight threads.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4401917291",
    "type": "article"
  },
  {
    "title": "PMGraph: Accelerating Concurrent Graph Queries over Streaming Graphs",
    "doi": "https://doi.org/10.1145/3689337",
    "publication_date": "2024-08-20",
    "publication_year": 2024,
    "authors": "Fubing Mao; Xu Liu; Yu Zhang; Haikun Liu; Xiaofei Liao; Hai Jin; Wei Zhang; Jian Zhou; Yufei Wu; Longyu Nie; Yapu Guo; Zihan Jiang; Jingkang Liu",
    "corresponding_authors": "",
    "abstract": "There are usually a large number of concurrent graph queries (CGQs) requirements in streaming graphs. However, existing graph processing systems mainly optimize a single graph query in streaming graphs or CGQs in static graphs. They have a large number of redundant computations and expensive memory access overhead, and cannot process CGQs in streaming graphs efficiently. To address these issues, we propose PMGraph , a software-hardware collaborative accelerator for efficient processing of CGQs in streaming graphs. First, PMGraph centers on fine-grained data, selects graph queries that meet the requirements through vertex data, and utilizes the similarity between different graph queries to merge the same vertices they need to process to address the problem of a large amount of repeated access to the same data by different graph queries in CGQs, thereby reducing memory access overhead. Furthermore, it adopts the update strategy that regularizes the processing order of vertices in each graph query according to the order of the vertex dependence chain, consequently effectively reducing redundant computations. Second, we propose a CGQs-oriented scheduling strategy to increase the data overlap when different graph queries are processed, thereby further improving the performance. Finally, PMGraph prefetches the vertex information according to the global active vertex set Frontier of all graph queries, hiding the memory access latency. It also provides prefetching for the same vertices that need to be processed by different graph queries, reducing the memory access overhead. Compared with the state-of-the-art concurrent graph query software systems Kickstarter-C and Tripoline, PMGraph achieves average speedups of 5.57× and 4.58×, respectively. Compared with the state-of-the-art hardware accelerators Minnow, HATS, LCCG, and JetStream, PMGraph achieves the speedup of 3.65×, 3.41×, 1.73×, and 1.38× on average, respectively. Experimental results show that our proposed PMGraph outperforms the state-of-the-art concurrent graph processing systems and hardware accelerators.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4401981623",
    "type": "article"
  },
  {
    "title": "Access Characteristic-Guided Remote Swapping Across Mobile Devices",
    "doi": "https://doi.org/10.1145/3695870",
    "publication_date": "2024-09-16",
    "publication_year": 2024,
    "authors": "Wentong Li; Yina Lv; Longfei Luo; Yunpeng Song; Liang Shi",
    "corresponding_authors": "",
    "abstract": "Memory swapping ensures smooth application switching for mobile systems by caching applications in the background. To further play the role of memory swapping, remote swapping across mobile devices has been widely studied, which caches applications to nearby remote devices by remote paging. However, due to the massive remote I/Os and unguaranteed swap throughput, the current remote swapping is limited with an unsatisfactory user experience, especially under variable network conditions. This paper first studies the access characteristics of applications and clarifies the impact of various network traffic on remote swapping. Motivated by these, an efficient access characteristic-guided remote swapping framework (ACR-Swap + ) is proposed to optimize remote swapping across mobile devices with resilient remote paging. ACR-Swap + first performs selective remote paging based on the swap-in frequency of different processes and then prefetches data across devices based on the process running states. Finally, it conducts hierarchical remote paging to avoid the impact of network traffic on remote swapping. Evaluations on Google Pixel 6 show that ACR-Swap + reduces the application switching latency by 21.6% and achieves a negligible performance fluctuation under various network traffic compared to the state-of-the-art.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4402806952",
    "type": "article"
  },
  {
    "title": "Shining Light on the Inter-procedural Code Obfuscation: Keep Pace with Progress in Binary Diffing",
    "doi": "https://doi.org/10.1145/3701992",
    "publication_date": "2024-10-28",
    "publication_year": 2024,
    "authors": "Peihua Zhang; Chenggang Wu; H. B. Hu; Lichen Jia; Mingfan Peng; Jiali Xu; Mengyao Xie; Y. F. Lai; Yan Kang; Zhe Wang",
    "corresponding_authors": "",
    "abstract": "Software obfuscation techniques have lost their effectiveness due to the rapid development of binary diffing techniques, which can achieve accurate function matching and identification. In this paper, we propose a new inter-procedural code obfuscation mechanism KHaos , which moves the code across functions to obfuscate the function by using compilation optimizations. Three obfuscation primitives are proposed to separate, aggregate, and hide the function. They can be combined to enhance the obfuscation effect further. This paper also reveals distinguishing factors on obfuscation and compiler optimization and presents novel observations to gain insights into the impact of actively utilizing compiler optimization in obfuscation. A prototype of KHaos is implemented and evaluated on a large number of real-world programs. Experimental results show that KHaos outperforms existing code obfuscations and can significantly reduce the accuracy rates of six state-of-the-art binary diffing techniques with lower runtime overhead.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4403842065",
    "type": "article"
  },
  {
    "title": "MemoriaNova: Optimizing Memory-Aware Model Inference for Edge Computing",
    "doi": "https://doi.org/10.1145/3701997",
    "publication_date": "2024-10-28",
    "publication_year": 2024,
    "authors": "Ru‐hong Zhang; Tianming Zhang; Zinuo Cai; Dongmei Li; Ruhui Ma; Rajkumar Buyya",
    "corresponding_authors": "",
    "abstract": "In recent years, deploying deep learning models on edge devices has become pervasive, driven by the increasing demand for intelligent edge computing solutions across various industries. From industrial automation to intelligent surveillance and healthcare, edge devices are being leveraged for real-time analytics and decision-making. Existing methods face two challenges when deploying machine learning models on edge devices. The first challenge is handling the execution order of operators with a simple strategy, which can lead to a potential waste of memory resources when dealing with directed acyclic graph structure models. The second challenge is that they usually process operators of a model one by one to optimize the inference latency, which may lead to the optimization problem getting trapped in local optima. We present MemoriaNova, comprising BTSearch and GenEFlow, to solve these two problems. BTSearch is a graph state backtracking algorithm with efficient pruning and hashing strategies designed to minimize memory overhead during inference and enlarge latency optimization search space. GenEFlow, based on genetic algorithms, integrates latency modeling and memory constraints to optimize distributed inference latency. This innovative approach considers a comprehensive search space for model partitioning, ensuring robust and adaptable solutions. We implement BTSearch and GenEFlow and test them on eleven deep-learning models with different structures and scales. The results show that BTSearch can reach 12% memory optimization compared with the widely used random execution strategy. At the same time, GenEFlow reduces inference latency by 33.9% in distributed systems with four-edge devices.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4403842265",
    "type": "article"
  },
  {
    "title": "DTAP: Accelerating Strongly-Typed Programs with Data Type-Aware Hardware Prefetching",
    "doi": "https://doi.org/10.1145/3701994",
    "publication_date": "2024-10-29",
    "publication_year": 2024,
    "authors": "Yingshuai Dong; Chencheng Ye; Haikun Liu; Liting Tang; Xiaofei Liao; Hai Jin; Chen Cheng; Y. Li; Yi Wang",
    "corresponding_authors": "",
    "abstract": "Queries on linked data structures, such as trees and graphs, often suffer from frequent cache misses and significant performance loss due to dependent and random pointer-chasing memory accesses. In this paper, we propose a software-hardware co-designed solution for accelerating linked data structures implemented in strongly typed languages. The solution incorporates a compiler extension and a hardware prefetcher. The compiler extension extracts type information from the code, annotates each load instruction, and forwards the type information to the hardware prefetcher. The prefetcher leverages the type information to fetch the referred objects and identify the associated pointers in advance. By doing so, the program can find these objects in the cache when it follows the prefetched pointers, thus minimizing cache misses. In the evaluation, the proposed solution achieves an average speedup of 1.37 × over a set of memory-intensive benchmarks.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4403875551",
    "type": "article"
  },
  {
    "title": "An Efficient ReRAM-based Accelerator for Asynchronous Iterative Graph Processing",
    "doi": "https://doi.org/10.1145/3689335",
    "publication_date": "2024-11-02",
    "publication_year": 2024,
    "authors": "Jin Zhao; Yu Zhang; Donghao He; Qikun Li; Wei-Hang Yin; Hui Yu; Hao Qi; Xiaofei Liao; Hai Jin; Haikun Liu; Linchen Yu; Zhan Zhang",
    "corresponding_authors": "",
    "abstract": "Graph processing has become a central concern for many real-world applications and is well-known for its low compute-to-communication ratios and poor data locality. By integrating computing logic into memory, resistive random access memory (ReRAM) tackles the demand for high memory bandwidth in graph processing. Despite the years’ research efforts, existing ReRAM-based graph processing approaches still face the challenges of redundant computation overhead . It is because the vertices of many subgraphs are ineffectively and repeatedly processed over the ReRAM crossbars for lots of iterations so as to update their states according to the vertices of other subgraphs regardless of the dependencies among the subgraphs. In this paper, we propose ASGraph , a dependency-aware ReRAM-based graph processing accelerator that overcomes the aforementioned performance bottlenecks. Specifically, ASGraph dynamically constructs the subgraph based on the dependencies between vertices’ states and then detects constructed subgraph that owns high value (it is likely that it has accumulated many state propagations from its neighbors and is able to affect more other neighbors) to be preferentially processed. In this way, it makes the vertex states propagate along the dependencies between vertices as much as possible to reduce the redundant computation. Besides, ASGraph employs a hybrid processing scheme to accelerate the state propagations of the tightly connected subgraph, thereby minimizing the redundant computations. Experimental results show that ASGraph achieves 25.5 × and 4.8 × speedup and 70.8 × and 2.2 × energy saving on average compared with the state-of-the-art ReRAM-based graph processing accelerators, i.e., GraphR and GaaS-X, respectively.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4404006743",
    "type": "article"
  },
  {
    "title": "ATP: Achieving Throughput Peak for DNN Training via Smart GPU Memory Management",
    "doi": "https://doi.org/10.1145/3701996",
    "publication_date": "2024-11-13",
    "publication_year": 2024,
    "authors": "Weiduo Chen; Xiaoshe Dong; Fan Zhang; Bowen Li; Yufei Wang; Qiang Wang",
    "corresponding_authors": "",
    "abstract": "Due to the limited GPU memory, the performance of large DNNs training is constrained by the unscalable batch size. Existing researches partially address the issue of GPU memory limit through tensor recomputation and swapping, but overlook the exploration of optimal performance. In response, we propose ATP, a recomputation and swapping based GPU memory management framework that aims to maximize training performance by breaking GPU memory constraints. ATP utilizes a throughput model we proposed to evaluate the theoretical peak performance achievable by DNN training on GPU, and provide the optimum memory size required for recomputation and swapping. We optimize the mechanisms for GPU memory pool and CUDA stream control, employs an optimization method to search for specific tensors requiring recomputation and swapping, thereby bringing the actual DNN training performance on ATP closer to theoretical values. Evaluations with different types of large DNN models indicate that ATP achieve throughput improvements ranging from 1.14 ∼ 1.49 ×, while support model training exceeding the GPU memory limit by up to 9.2 ×.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4404331863",
    "type": "article"
  },
  {
    "title": "RT-GNN: Accelerating Sparse Graph Neural Networks by Tensor-CUDA Kernel Fusion",
    "doi": "https://doi.org/10.1145/3702001",
    "publication_date": "2024-11-15",
    "publication_year": 2024,
    "authors": "Jianrong Yan; Wenbin Jiang; Dong He; Suyang Wen; Yang Li; Hai Jin; Zhiyuan Shao",
    "corresponding_authors": "",
    "abstract": "Graph Neural Networks (GNNs) have achieved remarkable successes in various graph-based learning tasks, thanks to their ability to leverage advanced GPUs. However, GNNs currently face challenges arising from the concurrent use of advanced Tensor Cores (TCs) and CUDA Cores (CDs) in GPUs. These challenges are further exacerbated due to repeated, inefficient, and redundant aggregations in GNN that result from the high sparsity and irregular non-zero distribution of real-world graphs. We propose RT-GNN, a GNN framework based on the fusion of advanced TC and CD units, to eliminate the aforementioned redundancies by exploiting the properties of an adjacency matrix. First, a novel GNN representation technique, hierarchical embedding graph (HEG) is proposed to manage the intermediate aggregation results hierarchically, which can further avoid redundancy in intermediate aggregations elegantly. Next, to address the inherent sparsity of graphs, RT-GNN places the blocks (a.k.a tiles) in HEG onto TCs and CDs according to their sparsity by a new block-based row-wise multiplication approach, which assembles TCs and CDs to work concurrently. Experimental results demonstrate that HEG outperforms HAG by an average speedup of 19.3 × for redundancy elimination performance, especially up to 72 × speedup on the dataset of ARXIV. Moreover, for overall performance, RT-GNN outperforms state-of-the-art GNN frameworks (including DGL, HAG, GNNAdvisor, and TC-GNN) by an average factor of 3.1 × while maintaining or even improving the task accuracy.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4404417996",
    "type": "article"
  },
  {
    "title": "PRAGA: A Priority-Aware Hardware/Software Co-design for High-Throughput Graph Processing Acceleration",
    "doi": "https://doi.org/10.1145/3701998",
    "publication_date": "2024-11-26",
    "publication_year": 2024,
    "authors": "Long Zheng; Bing Zhu; Pengcheng Yao; Yinan Zhou; Chengzhi Pan; Wenju Zhao; Xiaofei Liao; Hai Jin; Jingling Xue",
    "corresponding_authors": "",
    "abstract": "Graph processing is pivotal in deriving insights from complex data structures but faces performance limitations due to the irregular nature of graphs. Traditional general-purpose processors often struggle with low instruction-level parallelism and energy inefficiency when handling graph data. In response, modern graph accelerators have embraced an intra-edge-parallel model to enhance parallelization, significantly outperforming conventional processors. However, the indiscriminate processing of edges in existing systems results in substantial computational redundancy, negatively impacting overall efficiency. This paper introduces PRAGA, an innovative graph accelerator designed to optimize efficiency by selectively processing edges that significantly contribute to final results while preserving high computational parallelism. PRAGA utilizes an intra-edge-sequential model, prioritizing edge processing to capitalize on coarse-grained vertex-level parallelism and minimize unnecessary computations. It incorporates a hot-value manager to alleviate network-on-chip congestion and a memory-aware coalescer to minimize redundant data accesses. Our experimental results, obtained using a Xilinx Alveo U280 FPGA accelerator card, demonstrate that PRAGA achieves speedups of 17.88 × and 5.86 × over state-of-the-art accelerators ScalaGraph and GraphDyns, respectively, and outperforms the advanced GPU-based system Gunrock by 22.52 × on average. This substantial improvement underscores PRAGA’s potential to redefine performance benchmarks in graph processing.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4404721955",
    "type": "article"
  },
  {
    "title": "COVER: Alleviating Crash-Consistency Error Amplification in Secure Persistent Memory Systems",
    "doi": "https://doi.org/10.1145/3708541",
    "publication_date": "2024-12-17",
    "publication_year": 2024,
    "authors": "Xueliang Wei; Dan Feng; Wei Tong; Bing Wu; Xu Jiang",
    "corresponding_authors": "",
    "abstract": "Data security (including confidentiality, integrity, and availability) and crash consistency guarantees are essential for building trusted persistent memory (PM) systems. Security and consistency metadata are added to enable the guarantees. Recent studies show that errors in security metadata have the amplified effect, which significantly affects data availability. However, the impact of consistency metadata errors on data availability has rarely been discussed. We identify the crash-consistency error amplification (CCEA) problem, several errors in consistency metadata can make a large portion of data in PM possibly inconsistent. The error sensitivity of consistency metadata is higher than data and security metadata, thus requiring special attention. It is inefficient to address this problem by using the methods that are proposed to alleviate the amplified effect of security metadata errors, because security metadata are generally designed for a single purpose (e.g., integrity verification), while consistency metadata are designed for multiple purposes, including inconsistency locating and recovery. To effectively and efficiently alleviate the CCEA problem, we propose a c rash c o nsistency ver ification approach (COVER) that decouples inconsistency locating and recovery. COVER provides three design options that support different tradeoffs between effectiveness and efficiency. Experimental results show that COVER effectively alleviates the problem with only about 1.0% performance degradation on average compared with the state-of-the-art secure PM design.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4405513849",
    "type": "article"
  },
  {
    "title": "VLIW instruction scheduling for minimal power variation",
    "doi": "https://doi.org/10.1145/1275937.1275942",
    "publication_date": "2007-09-01",
    "publication_year": 2007,
    "authors": "Shu Long Xiao; Edmund Lai",
    "corresponding_authors": "",
    "abstract": "The focus of this paper is on the minimization of the variation in power consumed by a VLIW processor during the execution of a target program through instruction scheduling. The problem is formulated as a mixed-integer program (MIP) and a problem-specific branch-and-bound algorithm has been developed to solve it more efficiently than generic MIP solvers. Simulation results based on the TMS320C6711 VLIW digital signal processor using benchmarks from Mediabench and Trimaran showed that over 40% average reduction in power variation can be achieved without sacrificing execution speed of these benchmarks. Computational requirements and convergence rates of our algorithm are also analyzed.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2002073007",
    "type": "article"
  },
  {
    "title": "Object co-location and memory reuse for Java programs",
    "doi": "https://doi.org/10.1145/1328195.1328199",
    "publication_date": "2008-01-01",
    "publication_year": 2008,
    "authors": "Zoe C. H. Yu; Francis C. M. Lau; Cho‐Li Wang",
    "corresponding_authors": "",
    "abstract": "We introduce a new memory management system, STEMA, which can improve the execution time of Java programs. STEMA detects prolific types on-the-fly and co-locates their objects in a special memory space which supports reuse of memory. We argue and show that memory reuse and co-location of prolific objects can result in improved cache locality, reduced memory fragmentation, reduced GC time, and faster object allocation. We evaluate STEMA using 16 benchmarks. Experimental results show that STEMA performs 2.7%, 4.0%, and 8.2% on average better than MarkSweep, CopyMS, and SemiSpace.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2012858817",
    "type": "article"
  },
  {
    "title": "Speculative return address stack management revisited",
    "doi": "https://doi.org/10.1145/1455650.1455654",
    "publication_date": "2008-11-01",
    "publication_year": 2008,
    "authors": "Hans Vandierendonck; André Seznec",
    "corresponding_authors": "",
    "abstract": "Branch prediction feeds a speculative execution processor core with instructions. Branch mispredictions are inevitable and have negative effects on performance and energy consumption. With the advent of highly accurate conditional branch predictors, nonconditional branch instructions are gaining importance. In this article, we address the prediction of procedure returns. On modern processors, procedure returns are predicted through a return address stack (RAS). The overwhelming majority of the return mispredictions are due to RAS overflows and/or overwriting the top entries of the RAS on a mispredicted path. These sources of misprediction were addressed by previously proposed speculative return address stacks [Jourdan et al. 1996; Skadron et al. 1998]. However, the remaining misprediction rate of these RAS designs is still significant when compared to state-of-the-art conditional predictors. We present two low-cost corruption detectors for RAS predictors. They detect RAS overflows and wrong path corruption with 100% coverage. As a consequence, when such a corruption is detected, another source can be used for predicting the return. On processors featuring a branch target buffer (BTB), this BTB can be used as a free backup predictor for predicting returns when corruption is detected. Our experiments show that our proposal can be used to improve the behavior of all previously proposed speculative RASs. For instance, without any specific management of the speculative states on the RAS, an 8-entry BTB-backed up RAS achieves the same performance level as a state-of-the-art, but complex, 64-entry self-checkpointing RAS [Jourdan et al. 1996]. Therefore, our proposal can be used either to improve the performance of the processor or to reduce its hardware complexity.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2046362374",
    "type": "article"
  },
  {
    "title": "SortCache",
    "doi": "https://doi.org/10.1145/3473332",
    "publication_date": "2021-09-03",
    "publication_year": 2021,
    "authors": "Sriseshan Srikanth; Anirudh Jain; Thomas M. Conte; Erik P. DeBenedictis; Jeanine Cook",
    "corresponding_authors": "",
    "abstract": "Sparse data applications have irregular access patterns that stymie modern memory architectures. Although hyper-sparse workloads have received considerable attention in the past, moderately-sparse workloads prevalent in machine learning applications, graph processing and HPC have not. Where the former can bypass the cache hierarchy, the latter fit in the cache. This article makes the observation that intelligent, near-processor cache management can improve bandwidth utilization for data-irregular accesses, thereby accelerating moderately-sparse workloads. We propose SortCache, a processor-centric approach to accelerating sparse workloads by introducing accelerators that leverage the on-chip cache subsystem, with minimal programmer intervention.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3197176963",
    "type": "article"
  },
  {
    "title": "GraphAttack",
    "doi": "https://doi.org/10.1145/3469846",
    "publication_date": "2021-09-03",
    "publication_year": 2021,
    "authors": "Aninda Manocha; Tyler Sorensen; Esin Türeci; Opeoluwa Matthews; Juan L. Aragón; Margaret Martonosi",
    "corresponding_authors": "",
    "abstract": "Graph structures are a natural representation of important and pervasive data. While graph applications have significant parallelism, their characteristic pointer indirect loads to neighbor data hinder scalability to large datasets on multicore systems. A scalable and efficient system must tolerate latency while leveraging data parallelism across millions of vertices. Modern Out-of-Order (OoO) cores inherently tolerate a fraction of long latencies, but become clogged when running severely memory-bound applications. Combined with large power/area footprints, this limits their parallel scaling potential and, consequently, the gains that existing software frameworks can achieve. Conversely, accelerator and memory hierarchy designs provide performant hardware specializations, but cannot support diverse application demands. To address these shortcomings, we present GraphAttack, a hardware-software data supply approach that accelerates graph applications on in-order multicore architectures. GraphAttack proposes compiler passes to (1) identify idiomatic long-latency loads and (2) slice programs along these loads into data Producer/ Consumer threads to map onto pairs of parallel cores. Each pair shares a communication queue; the Producer asynchronously issues long-latency loads, whose results are buffered in the queue and used by the Consumer. This scheme drastically increases memory-level parallelism (MLP) to mitigate latency bottlenecks. In equal-area comparisons, GraphAttack outperforms OoO cores, do-all parallelism, prefetching, and prior decoupling approaches, achieving a 2.87× speedup and 8.61× gain in energy efficiency across a range of graph applications. These improvements scale; GraphAttack achieves a 3× speedup over 64 parallel cores. Lastly, it has pragmatic design principles; it enhances in-order architectures that are gaining increasing open-source support.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3197380272",
    "type": "article"
  },
  {
    "title": "WaFFLe",
    "doi": "https://doi.org/10.1145/3471908",
    "publication_date": "2021-09-03",
    "publication_year": 2021,
    "authors": "Shounak Chakraborty; Magnus Själander",
    "corresponding_authors": "",
    "abstract": "Managing thermal imbalance in contemporary chip multi-processors (CMPs) is crucial in assuring functional correctness of modern mobile as well as server systems. Localized regions with high activity, e.g., register files, ALUs, FPUs, and so on, experience higher temperatures than the average across the chip and are commonly referred to as hotspots. Hotspots affect functional correctness of the underlying circuitry and a noticeable increase in leakage power, which in turn generates heat in a self-reinforced cycle. Techniques that reduce the severity of or completely eliminate hotspots can maintain functional correctness along with improving performance of CMPs. Conventional dynamic thermal management targets the cores to reduce hotspots but often ignores caches, which are known for their high leakage power consumption. This article presents WaFFLe , an approach that targets the leakage power of the last-level cache (LLC) and hotspots occurring at the cores. WaFFLe turns off LLC-ways to reduce leakage power and to generate on-chip thermal buffers. In addition, fine-grained DVFS is applied during long LLC miss induced stalls to reduce core temperature. Our results show that WaFFLe reduces peak and average temperature of a 16-core based homogeneous tiled CMP with up to 8.4 ֯ C and 6.2 ֯ C, respectively, with an average performance degradation of only 2.5 %. We also show that WaFFLe outperforms a state-of-the-art cache-based technique and a greedy DVFS policy.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3197485759",
    "type": "article"
  },
  {
    "title": "SMT-Based Contention-Free Task Mapping and Scheduling on 2D/3D SMART NoC with Mixed Dimension-Order Routing",
    "doi": "https://doi.org/10.1145/3487018",
    "publication_date": "2021-12-06",
    "publication_year": 2021,
    "authors": "Daeyeal Lee; Bill Lin; Chung‐Kuan Cheng",
    "corresponding_authors": "",
    "abstract": "SMART NoCs achieve ultra-low latency by enabling single-cycle multiple-hop transmission via bypass channels. However, contention along bypass channels can seriously degrade the performance of SMART NoCs by breaking the bypass paths. Therefore, contention-free task mapping and scheduling are essential for optimal system performance. In this article, we propose an SMT (Satisfiability Modulo Theories)-based framework to find optimal contention-free task mappings with minimum application schedule lengths on 2D/3D SMART NoCs with mixed dimension-order routing. On top of SMT’s fast reasoning capability for conditional constraints, we develop efficient search-space reduction techniques to achieve practical scalability. Experiments demonstrate that our SMT framework achieves 10× higher scalability than ILP (Integer Linear Programming) with 931.1× (ranges from 2.2× to 1532.1×) and 1237.1× (ranges from 4× to 4373.8×) faster average runtimes for finding optimum solutions on 2D and 3D SMART NoCs and our 2D and 3D extensions of the SMT framework with mixed dimension-order routing also maintain the improved scalability with the extended and diversified routing paths, resulting in reduced application schedule lengths throughout various application benchmarks.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4200024651",
    "type": "article"
  },
  {
    "title": "Energy- and area-efficient architectures through application clustering and architectural heterogeneity",
    "doi": "https://doi.org/10.1145/1509864.1509868",
    "publication_date": "2009-03-30",
    "publication_year": 2009,
    "authors": "Lukasz Strozek; David Brooks",
    "corresponding_authors": "",
    "abstract": "Customizing architectures for particular applications is a promising approach to yield highly energy-efficient designs for embedded systems. This work explores the benefits of architectural customization for a class of embedded architectures typically used in energy- and area-constrained application domains, such as sensor nodes and multimedia processing. We implement a process flow that performs an automatic synthesis and evaluation of the different architectures based on runtime profiles of applications and determines an efficient architecture, with consideration for both energy and area constraints. An expressive architectural model, used by our engine, is introduced that takes advantage of efficient opcode allocation, several memory addressing modes, and operand types. By profiling embedded benchmarks from a variety of sensor and multimedia applications, we show that the energy savings resulting from various architectural optimizations relative to the base architectures (e.g., MIPS and MSP430) are significant and can reach 50%, depending on the application. We then identify the set of architectures that achieves near-optimal savings for a group of applications. Finally, we propose the use of heterogeneous ISA processors implementing those architectures as a solution to capitalize on energy savings provided by application customization while executing a range of applications efficiently.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W1964322643",
    "type": "article"
  },
  {
    "title": "Micro-Sector Cache",
    "doi": "https://doi.org/10.1145/3046680",
    "publication_date": "2017-03-21",
    "publication_year": 2017,
    "authors": "Mainak Chaudhuri; Mukesh Agrawal; Jayesh Gaur; Sreenivas Subramoney",
    "corresponding_authors": "",
    "abstract": "Recent research proposals on DRAM caches with conventional allocation units (64 or 128 bytes) as well as large allocation units (512 bytes to 4KB) have explored ways to minimize the space/latency impact of the tag store and maximize the effective utilization of the bandwidth. In this article, we study sectored DRAM caches that exercise large allocation units called sectors, invest reasonably small storage to maintain tag/state, enable space- and bandwidth-efficient tag/state caching due to low tag working set size and large data coverage per tag element, and minimize main memory bandwidth wastage by fetching only the useful portions of an allocated sector. However, the sectored caches suffer from poor space utilization, since a large sector is always allocated even if the sector utilization is low. The recently proposed Unison cache addresses only a special case of this problem by not allocating the sectors that have only one active block. We propose Micro-sector cache, a locality-aware sectored DRAM cache architecture that features a flexible mechanism to allocate cache blocks within a sector and a locality-aware sector replacement algorithm. Simulation studies on a set of 30 16-way multi-programmed workloads show that our proposal, when incorporated in an optimized Unison cache baseline, improves performance (weighted speedup) by 8%, 14%, and 16% on average, respectively, for 1KB, 2KB, and 4KB sectors at 128MB capacity. These performance improvements result from significantly better cache space utilization, leading to 18%, 21%, and 22% average reduction in DRAM cache read misses, respectively, for 1KB, 2KB, and 4KB sectors at 128MB capacity. We evaluate our proposal for DRAM cache capacities ranging from 128MB to 1GB.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2599721687",
    "type": "article"
  },
  {
    "title": "SWITCHES",
    "doi": "https://doi.org/10.1145/3127068",
    "publication_date": "2017-09-06",
    "publication_year": 2017,
    "authors": "Andreas Diavastos; Pedro Trancoso",
    "corresponding_authors": "",
    "abstract": "SWITCHES is a task-based dataflow runtime that implements a lightweight distributed triggering system for runtime dependence resolution and uses static scheduling and compile-time assignment policies to reduce runtime overheads. Unlike other systems, the granularity of loop-tasks can be increased to favor data-locality, even when having dependences across different loops. SWITCHES introduces explicit task resource allocation mechanisms for efficient allocation of resources and adopts the latest OpenMP Application Programming Interface (API), as to maintain high levels of programming productivity. It provides a source-to-source tool that automatically produces thread-based code. Performance on an Intel Xeon-Phi shows good scalability and surpasses OpenMP by an average of 32%.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2751208315",
    "type": "article"
  },
  {
    "title": "Generating Fine-Grain Multithreaded Applications Using a Multigrain Approach",
    "doi": "https://doi.org/10.1145/3155288",
    "publication_date": "2017-12-13",
    "publication_year": 2017,
    "authors": "Jaime Muñoz Arteaga; Stéphane Zuckerman; Guang R. Gao",
    "corresponding_authors": "",
    "abstract": "The recent evolution in hardware landscape, aimed at producing high-performance computing systems capable of reaching extreme-scale performance, has reignited the interest in fine-grain multithreading, particularly at the intranode level. Indeed, popular parallel programming environments, such as OpenMP, which features a simple interface for the parallelization of programs, are now incorporating fine-grain constructs. However, since coarse-grain directives are still heavily used, the OpenMP runtime is forced to support both coarse- and fine-grain models of execution, potentially reducing the advantages obtained when executing an application in a fully fine-grain environment. To evaluate the type of applications that benefit from executing in a unified fine-grain program execution model, this article presents a multigrain parallel programming environment for the generation of fine-grain multithreaded applications from programs featuring OpenMP’s API, allowing OpenMP programs to be run on top of a fine-grain event-driven program execution model. Experimental results with five scientific benchmarks show that fine-grain applications, generated by and run on our environment with two runtimes implementing a fine-grain event-driven program execution model, are competitive and can outperform their OpenMP counterparts, especially for data-intensive workloads with irregular and dynamic parallelism, reaching speedups as high as 2.6× for Graph500 and 51× for NAS Data Cube.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2773150715",
    "type": "article"
  },
  {
    "title": "Spatiotemporal SIMT and Scalarization for Improving GPU Efficiency",
    "doi": "https://doi.org/10.1145/2811402",
    "publication_date": "2015-09-08",
    "publication_year": 2015,
    "authors": "Jan Lucas; Michael Andersch; Mauricio Álvarez-Mesa; Ben Juurlink",
    "corresponding_authors": "",
    "abstract": "Temporal SIMT (TSIMT) has been suggested as an alternative to conventional (spatial) SIMT for improving GPU performance on branch-intensive code. Although TSIMT has been briefly mentioned before, it was not evaluated. We present a complete design and evaluation of TSIMT GPUs, along with the inclusion of scalarization and a combination of temporal and spatial SIMT, named Spatiotemporal SIMT (STSIMT). Simulations show that TSIMT alone results in a performance reduction, but a combination of scalarization and STSIMT yields a mean performance enhancement of 19.6% and improves the energy-delay product by 26.2% compared to SIMT.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W1997432558",
    "type": "article"
  },
  {
    "title": "Efficient Out-of-Order Execution of Guarded ISAs",
    "doi": "https://doi.org/10.1145/2677037",
    "publication_date": "2014-12-08",
    "publication_year": 2014,
    "authors": "Nathanaël Prémillieu; André Seznec",
    "corresponding_authors": "",
    "abstract": "ARM ISA-based processors are no longer low-cost, low-power processors. Nowadays, ARM ISA-based processor manufacturers are striving to implement medium-end to high-end processor cores, which implies implementing a state-of-the-art out-of-order execution engine. Unfortunately, providing efficient out-of-order execution on legacy ARM codes may be quite challenging due to guarded instructions. Predicting the guarded instructions addresses the main serialization impact associated with guarded instructions execution and the multiple definition problem. Moreover, guard prediction allows one to use a global branch-and-guard history predictor to predict both branches and guards, often improving branch prediction accuracy. Unfortunately, such a global branch-and-guard history predictor requires the systematic use of guard predictions. In that case, poor guard prediction accuracy would lead to poor overall performance on some applications. Building on top of recent advances in branch prediction and confidence estimation, we propose a hybrid branch-and-guard predictor, combining a global branch history component and global branch-and-guard history component. The potential gain or loss due to the systematic use of guard prediction is dynamically evaluated at runtime. Two computing modes are enabled: systematic guard prediction use and high-confidence-only guard prediction use. Our experiments show that on most applications, an overwhelming majority of guarded instructions are predicted. Therefore, a simple but relatively inefficient hardware solution can be used to execute the few unpredicted guarded instructions. Significant performance benefits are observed on most applications, while applications with poorly predictable guards do not suffer from performance loss.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2049815754",
    "type": "article"
  },
  {
    "title": "A Joint SW/HW Approach for Reducing Register File Vulnerability",
    "doi": "https://doi.org/10.1145/2733378",
    "publication_date": "2015-05-11",
    "publication_year": 2015,
    "authors": "Hamed Tabkhi; Gunar Schirner",
    "corresponding_authors": "",
    "abstract": "The Register File (RF) is a particularly vulnerable component within processor core and at the same time a hotspot with high power density. To reduce RF vulnerability, conventional HW-only approaches such as Error Correction Codes (ECCs) or modular redundancies are not suitable due to their significant power overhead. Conversely, SW-only approaches either have limited improvement on RF reliability or require considerable performance overhead. As a result, new approaches are needed that reduce RF vulnerability with minimal power and performance overhead. This article introduces Application-guided Reliability-enhanced Register file Architecture (ARRA), a novel approach to reduce RF vulnerability of embedded processors. Taking advantage of uneven register utilization, ARRA mirrors, guided by a SW instrumentation, frequently used active registers into passive registers. ARRA is particularly suitable for control applications, as they have a high reliability demand with fairly low (uneven) RF utilization. ARRA is a cross-layer joint HW/SW approach based on an ARRA-extended RF microarchitecture, an ISA extension, as well as static binary analysis and instrumentation. We evaluate ARRA benefits using an ARRA-enhanced Blackfin processor executing a set of DSPBench and MiBench benchmarks. We quantify the benefits using RF Vulnerability Factor (RFVF) and Mean Work To Failure (MWTF). ARRA significantly reduces RFVF from 35% to 6.9% in cost of 0.5% performance lost for control applications. With ARRA’s register mirroring, it can also correct Multiple Bit Upsets (MBUs) errors, achieving an 8x increase in MWTF. Compared to a partially ECC-protected RF approach, ARRA demonstrates higher efficiency by achieving comparable vulnerability reduction at much lower power consumption.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2264351132",
    "type": "article"
  },
  {
    "title": "MInGLE",
    "doi": "https://doi.org/10.1145/2898356",
    "publication_date": "2016-06-14",
    "publication_year": 2016,
    "authors": "Cecilia González-Álvarez; Jennifer B. Sartor; Carlos Álvarez; Daniel Jiménez-González; Lieven Eeckhout",
    "corresponding_authors": "",
    "abstract": "The end of Dennard scaling leads to new research directions that try to cope with the utilization wall in modern chips, such as the design of specialized architectures. Processor customization utilizes transistors more efficiently, optimizing not only for performance but also for power. However, hardware specialization for each application is costly and impractical due to time-to-market constraints. Domain-specific specialization is an alternative that can increase hardware reutilization across applications that share similar computations. This article explores the specialization of low-power processors with custom instructions (CIs) that run on a specialized functional unit. We are the first, to our knowledge, to design CIs for an application domain and across basic blocks, selecting CIs that maximize both performance and energy efficiency improvements. We present the Merged Instructions Generator for Large Efficiency (MInGLE), an automated framework that identifies and selects CIs. Our framework analyzes large sequences of code (across basic blocks) to maximize acceleration potential while also performing partial matching across applications to optimize for reuse of the specialized hardware. To do this, we convert the code into a new canonical representation, the Merging Diagram, which represents the code’s functionality instead of its structure. This is key to being able to find similarities across such large code sequences from different applications with different coding styles. Groups of potential CIs are clustered depending on their similarity score to effectively reduce the search space. Additionally, we create new CIs that cover not only whole-body loops but also fragments of the code to optimize hardware reutilization further. For a set of 11 applications from the media domain, our framework generates CIs that significantly improve the energy-delay product (EDP) and performance speedup. CIs with the highest utilization opportunities achieve an average EDP improvement of 3.8 × compared to a baseline processor modeled after an Intel Atom. We demonstrate that we can efficiently accelerate a domain with partially matched CIs, and that their design time, from identification to selection, stays within tractable bounds.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2429120270",
    "type": "article"
  },
  {
    "title": "Aggregate Flow-Based Performance Fairness in CMPs",
    "doi": "https://doi.org/10.1145/3014429",
    "publication_date": "2016-12-28",
    "publication_year": 2016,
    "authors": "Zhonghai Lu; Yuan Yao",
    "corresponding_authors": "",
    "abstract": "In CMPs, multiple co-executing applications create mutual interference when sharing the underlying network-on-chip architecture. Such interference causes different performance slowdowns to different applications. To mitigate the unfairness problem, we treat traffic initiated from the same thread as an aggregate flow such that causal request/reply packet sequences can be allocated to resources consistently and fairly according to online profiled traffic injection rates. Our solution comprises three coherent mechanisms from rate profiling, rate inheritance, and rate-proportional channel scheduling to facilitate and realize unbiased workload-adaptive resource allocation. Full-system evaluations in GEM5 demonstrate that, compared to classic packet-centric and latest application-prioritization approaches, our approach significantly improves weighted speed-up for all multi-application mixtures and achieves nearly ideal performance fairness.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2562587134",
    "type": "article"
  },
  {
    "title": "FlexPointer: Fast Address Translation Based on Range TLB and Tagged Pointers",
    "doi": "https://doi.org/10.1145/3579854",
    "publication_date": "2023-02-01",
    "publication_year": 2023,
    "authors": "Dongwei Chen; Dong Tong; Chun Yang; Jiangfang Yi; Xu Cheng",
    "corresponding_authors": "",
    "abstract": "Page-based virtual memory relies on TLBs to accelerate the address translation. Nowadays, the gap between application workloads and the capacity of TLB continues to grow, bringing many costly TLB misses and making the TLB a performance bottleneck. Previous studies seek to narrow the gap by exploiting the contiguity of physical pages. One promising solution is to group pages that are both virtually and physically contiguous into a memory range. Recording range translations can greatly increase the TLB reach, but ranges are also hard to index because they have arbitrary bounds. The processor has to compare against all the boundaries to determine which range an address falls in, which restricts the usage of memory ranges. In this article, we propose a tagged-pointer-based scheme, FlexPointer, to solve the range indexing problem. The core insight of FlexPointer is that large memory objects are rare, so we can create memory ranges based on such objects and assign each of them a unique ID. With the range ID integrated into pointers, we can index the range TLB with IDs and greatly simplify its structure. Moreover, because the ID is stored in the unused bits of a pointer and is not manipulated by the address generation, we can shift the range lookup to an earlier stage, working in parallel with the address generation. According to our trace-based simulation results, FlexPointer can reduce nearly all the L1 TLB misses, and page walks for a variety of memory-intensive workloads. Compared with a 4K-page baseline system, FlexPointer shows a 14% performance improvement on average and up to 2.8x speedup in the best case. For other workloads, FlexPointer shows no performance degradation.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4318776735",
    "type": "article"
  },
  {
    "title": "TNT: A Modular Approach to Traversing Physically Heterogeneous NOCs at Bare-wire Latency",
    "doi": "https://doi.org/10.1145/3597611",
    "publication_date": "2023-05-22",
    "publication_year": 2023,
    "authors": "Gokul Subramanian Ravi; Tushar Krishna; Mikko H. Lipasti",
    "corresponding_authors": "",
    "abstract": "The ideal latency for on-chip network traversal would be the delay incurred from wire traversal alone. Unfortunately, in a realistic modular network, the latency for a packet to traverse the network is significantly higher than this wire delay. The main limiter to achieving lower latency is the modular quantization of network traversal into hops. Beyond this, the physical heterogeneity in real-world systems further complicate the ability to reach ideal wire-only delay. In this work, we propose TNT or Transparent Network Traversal . TNT targets ideal network latency by attempting source to destination network traversal as a single multi-cycle ‘long-hop’, bypassing the quantization effects of intermediate routers via transparent data/information flow. TNT is built in a modular tile-scalable manner via a novel control path performing neighbor-to-neighbor interactions but enabling end-to-end transparent flit traversal. Further, TNT’s fine grained on-the-fly delay tracking allows it to cope with physical NOC heterogeneity across the chip. Analysis on Ligra graph workloads shows that TNT can reduce NOC latency by as much as 43% compared to the state of the art and allows efficiency gains up to 38%. Further, it can achieve more than 3x the benefits of the best/closest alternative research proposal, SMART [ 43 ].",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4377292701",
    "type": "article"
  },
  {
    "title": "Hierarchical Model Parallelism for Optimizing Inference on Many-core Processor via Decoupled 3D-CNN Structure",
    "doi": "https://doi.org/10.1145/3605149",
    "publication_date": "2023-06-18",
    "publication_year": 2023,
    "authors": "Jiazhi Jiang; Zijian Huang; Dan Huang; Jiangsu Du; Lin Chen; Ziguan Chen; Yutong Lu",
    "corresponding_authors": "",
    "abstract": "The tremendous success of convolutional neural network (CNN) has made it ubiquitous in many fields of human endeavor. Many applications such as biomedical analysis and scientific data analysis involve analyzing volumetric data. This spawns huge demand for 3D-CNN. Although accelerators such as GPU may provide higher throughput on deep learning applications, they may not be available in all scenarios. CPU, especially many-core CPU with non-uniform memory access (NUMA) architecture, remains an attractive choice for deep learning inference in many scenarios. In this article, we propose a distributed inference solution for 3D-CNN that targets on the emerging ARM many-core CPU platform. A hierarchical partition approach is claimed to accelerate 3D-CNN inference by exploiting characteristics of memory and cache on ARM many-core CPU. Based on the hierarchical model partition approach, other optimization techniques such as NUMA-aware thread scheduling and optimization of 3D-img2row convolution are designed to exploit the potential of ARM many-core CPU for 3D-CNN. We evaluate our proposed inference solution with several classic 3D-CNNs: C3D, 3D-resnet34, 3D-resnet50, 3D-vgg11, and P3D. Our experimental results show that our solution can boost the performance of the 3D-CNN inference, and achieve much better scalability, with a negligible fluctuation in accuracy. When employing our 3D-CNN inference solution on ACL libraries, it can outperform naive ACL implementations by 11× to 50× on ARM many-core processor. When employing our 3D-CNN inference solution on NCNN libraries, it can outperform the naive NCNN implementations by 5.2× to 14.2× on ARM many-core processor.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4381149555",
    "type": "article"
  },
  {
    "title": "Approx-RM: Reducing Energy on Heterogeneous Multicore Processors under Accuracy and Timing Constraints",
    "doi": "https://doi.org/10.1145/3605214",
    "publication_date": "2023-06-22",
    "publication_year": 2023,
    "authors": "Muhammad Waqar Azhar; Madhavan Manivannan; Per Stenström",
    "corresponding_authors": "",
    "abstract": "Reducing energy consumption while providing performance and quality guarantees is crucial for computing systems ranging from battery-powered embedded systems to data centers. This article considers approximate iterative applications executing on heterogeneous multi-core platforms under user-specified performance and quality targets. We note that allowing a slight yet bounded relaxation in solution quality can considerably reduce the required iteration count and thereby can save significant amounts of energy. To this end, this article proposes Approx-RM , a resource management scheme that reduces energy expenditure while guaranteeing a specified performance as well as accuracy target. Approx-RM predicts the number of iterations required to meet the relaxed accuracy target at runtime. The time saved generates execution-time slack, which allows Approx-RM to allocate fewer resources on a heterogeneous multi-core platform in terms of DVFS, core type, and core count to save energy while meeting the performance target. Approx-RM contributes with lightweight methods for predicting the iteration count needed to meet the accuracy target and the resources needed to meet the performance target. Approx-RM uses the aforementioned predictions to allocate just enough resources to comply with quality of service constraints to save energy. Our evaluation shows energy savings of 31.6%, on average, compared to Race-to-idle when the accuracy is only relaxed by 1%. Approx-RM incurs timing and energy overheads of less than 0.1%.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4381687564",
    "type": "article"
  },
  {
    "title": "Smart-DNN+: A Memory-efficient Neural Networks Compression Framework for the Model Inference",
    "doi": "https://doi.org/10.1145/3617688",
    "publication_date": "2023-08-30",
    "publication_year": 2023,
    "authors": "Donglei Wu; Weihao Yang; Xiangyu Zou; Wen Xia; Shiyi Li; Zhenbo Hu; Weizhe Zhang; Binxing Fang",
    "corresponding_authors": "",
    "abstract": "Deep Neural Networks (DNNs) have achieved remarkable success in various real-world applications. However, running a Deep Neural Network (DNN) typically requires hundreds of megabytes of memory footprints, making it challenging to deploy on resource-constrained platforms such as mobile devices and IoT. Although mainstream DNNs compression techniques such as pruning, distillation, and quantization can reduce the memory overhead of model parameters during DNN inference, they suffer from three limitations: (i) low model compression ratio for the lightweight DNN structures with little redundancy, (ii) potential degradation in model inference accuracy, and (iii) inadequate memory compression ratio is attributable to ignoring the layering property of DNN inference. To address these issues, we propose a lightweight memory-efficient DNN inference framework called Smart-DNN+, which significantly reduces the memory costs of DNN inference without degrading the model quality. Specifically, ① Smart-DNN+ applies a layerwise binary-quantizer with a remapping mechanism to greatly reduce the model size by quantizing the typical floating-point DNN weights of 32-bit to the 1-bit signs layer by layer. To maintain model quality, ② Smart-DNN+ employs a bucket-encoder to keep the compressed quantization error by encoding the multiple similar floating-point residuals into the same integer bucket IDs. When running the compressed DNN in the user’s device, ③ Smart-DNN+ utilizes a partially decompressing strategy to greatly reduce the required memory overhead by first loading the compressed DNNs in memory and then dynamically decompressing the required materials for model inference layer by layer. Experimental results on popular DNNs and datasets demonstrate that Smart-DNN+ achieves lower 0.17%–0.92% memory costs at lower runtime overheads compared with the states of the art without degrading the inference accuracy. Moreover, Smart-DNN+ potentially reduces the inference runtime up to 2.04× that of conventional DNN inference workflow.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4386290345",
    "type": "article"
  },
  {
    "title": "A Compilation Tool for Computation Offloading in ReRAM-based CIM Architectures",
    "doi": "https://doi.org/10.1145/3617686",
    "publication_date": "2023-09-05",
    "publication_year": 2023,
    "authors": "Hai Jin; Bo Lei; Haikun Liu; Xiaofei Liao; Zhuohui Duan; Chencheng Ye; Yu Zhang",
    "corresponding_authors": "",
    "abstract": "Computing-in-Memory (CIM) architectures using Non-volatile Memories (NVMs) have emerged as a promising way to address the “memory wall” problem in traditional Von Neumann architectures. CIM accelerators can perform arithmetic or Boolean logic operations in NVMs by fully exploiting their high parallelism for bit-wise operations. These accelerators are often used in cooperation with general-purpose processors to speed up a wide variety of artificial neural network applications. In such a heterogeneous computing architecture, the legacy software should be redesigned and re-engineered to utilize new CIM accelerators. In this article, we propose a compilation tool to automatically migrate legacy programs to such heterogeneous architectures based on the low-level virtual machine (LLVM) compiler infrastructure. To accelerate some computations such as vector-matrix multiplication in CIM accelerators, we identify several typical computing patterns from LLVM intermediate representations , which are oblivious to high-level programming paradigms. Our compilation tool can modify accelerable LLVM IRs to offload them to CIM accelerators automatically, without re-engineering legacy software. Experimental results show that our compilation tool can translate many legacy programs to CIM-supported binary executables effectively, and improve application performance and energy efficiency by up to 51× and 309×, respectively, compared with general-purpose x86 processors.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4386440652",
    "type": "article"
  },
  {
    "title": "PARALiA: A Performance Aware Runtime for Auto-tuning Linear Algebra on Heterogeneous Systems",
    "doi": "https://doi.org/10.1145/3624569",
    "publication_date": "2023-09-15",
    "publication_year": 2023,
    "authors": "Petros Anastasiadis; Νικέλα Παπαδοπούλου; Georgios Goumas; Nectarios Koziris; Dennis Hoppe; Li Zhong",
    "corresponding_authors": "",
    "abstract": "Dense linear algebra operations appear very frequently in high-performance computing (HPC) applications, rendering their performance crucial to achieve optimal scalability. As many modern HPC clusters contain multi-GPU nodes, BLAS operations are frequently offloaded on GPUs, necessitating the use of optimized libraries to ensure good performance. Unfortunately, multi-GPU systems are accompanied by two significant optimization challenges: data transfer bottlenecks as well as problem splitting and scheduling in multiple workers (GPUs) with distinct memories. We demonstrate that the current multi-GPU BLAS methods for tackling these challenges target very specific problem and data characteristics, resulting in serious performance degradation for any slightly deviating workload. Additionally, an even more critical decision is omitted because it cannot be addressed using current scheduler-based approaches: the determination of which devices should be used for a certain routine invocation. To address these issues we propose a model-based approach: using performance estimation to provide problem-specific autotuning during runtime. We integrate this autotuning into an end-to-end BLAS framework named PARALiA. This framework couples autotuning with an optimized task scheduler, leading to near-optimal data distribution and performance-aware resource utilization. We evaluate PARALiA in an HPC testbed with 8 NVIDIA-V100 GPUs, improving the average performance of GEMM by 1.7× and energy efficiency by 2.5× over the state-of-the-art in a large and diverse dataset and demonstrating the adaptability of our performance-aware approach to future heterogeneous systems.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4386774053",
    "type": "article"
  },
  {
    "title": "DxPU: Large-scale Disaggregated GPU Pools in the Datacenter",
    "doi": "https://doi.org/10.1145/3617995",
    "publication_date": "2023-10-05",
    "publication_year": 2023,
    "authors": "Bowen He; Xiao Zheng; Yuan Chen; Weinan Li; Yajin Zhou; Xin Long; Pengcheng Zhang; Xiaowei Lu; Linquan Jiang; Qiang Liu; Dennis Cai; Xiantao Zhang",
    "corresponding_authors": "",
    "abstract": "The rapid adoption of AI and convenience offered by cloud services have resulted in the growing demands for GPUs in the cloud. Generally, GPUs are physically attached to host servers as PCIe devices. However, the fixed assembly combination of host servers and GPUs is extremely inefficient in resource utilization, upgrade, and maintenance. Due to these issues, the GPU disaggregation technique has been proposed to decouple GPUs from host servers. It aggregates GPUs into a pool and allocates GPU node(s) according to user demands. However, existing GPU disaggregation systems have flaws in software-hardware compatibility, disaggregation scope, and capacity. In this article, we present a new implementation of datacenter-scale GPU disaggregation, named DxPU. DxPU efficiently solves the above problems and can flexibly allocate as many GPU node(s) as users demand. To understand the performance overhead incurred by DxPU, we build up a performance model for AI specific workloads. With the guidance of modeling results, we develop a prototype system, which has been deployed into the datacenter of a leading cloud provider for a test run. We also conduct detailed experiments to evaluate the performance overhead caused by our system. The results show that the overhead of DxPU is less than 10%, compared with native GPU servers, in most of user scenarios.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4387377652",
    "type": "article"
  },
  {
    "title": "Fastensor: Optimise the Tensor I/O Path from SSD to GPU for Deep Learning Training",
    "doi": "https://doi.org/10.1145/3630108",
    "publication_date": "2023-10-25",
    "publication_year": 2023,
    "authors": "Jia Wei; Xingjun Zhang; Longxiang Wang; Zheng Wei",
    "corresponding_authors": "",
    "abstract": "In recent years, benefiting from the increase in model size and complexity, deep learning has achieved tremendous success in computer vision (CV) and (NLP). Training deep learning models using accelerators such as GPUs often requires much iterative data to be transferred from NVMe SSD to GPU memory. Much recent work has focused on data transfer during the pre-processing phase and has introduced techniques such as multiprocessing and GPU Direct Storage (GDS) to accelerate it. However, tensor data during training (such as Checkpoints, logs, and intermediate feature maps), which is also time-consuming, is often transferred using traditional serial, long-I/O-path transfer methods. In this article, based on GDS technology, we built Fastensor, an efficient tool for tensor data transfer between the NVMe SSDs and GPUs. To achieve higher tensor data I/O throughput, we optimized the traditional data I/O process. We also proposed a data and runtime context-aware tensor I/O algorithm. Fastensor can select the most suitable data transfer tool for the current tensor from a candidate set of tools during model training. The optimal tool is derived from a dictionary generated by our adaptive exploration algorithm in the first few training iterations. We used Fastensor’s unified interface to test the read/write bandwidth and energy consumption of different transfer tools for different sizes of tensor blocks. We found that the execution efficiency of different tensor transfer tools is related to both the tensor block size and the runtime context. We then deployed Fastensor in the widely applicable Pytorch deep learning framework. We showed that Fastensor could perform superior in typical scenarios of model parameter saving and intermediate feature map transfer with the same hardware configuration. Fastensor achieves a 5.37x read performance improvement compared to torch.save () when used for model parameter saving. When used for intermediate feature map transfer, Fastensor can increase the supported training batch size by 20x, while the total read and write speed is increased by 2.96x compared to the torch I/O API.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4387933214",
    "type": "article"
  },
  {
    "title": "DAG-Order: An Order-Based Dynamic DAG Scheduling for Real-Time Networks-on-Chip",
    "doi": "https://doi.org/10.1145/3631527",
    "publication_date": "2023-11-03",
    "publication_year": 2023,
    "authors": "Peng Chen; Hui Chen; Weichen Liu; Linbo Long; Wanli Chang; Nan Guan",
    "corresponding_authors": "",
    "abstract": "With the high-performance requirement of safety-critical real-time tasks, the platforms of many-core processors with high parallelism are widely utilized, where network-on-chip (NoC) is generally employed for inter-core communication due to its scalability and high efficiency. Unfortunately, large uncertainties are suffered on NoCs from both the overly parallel architecture and the distributed scheduling strategy (e.g., wormhole flow control), which complicates the response time upper bounds estimation (i.e., either unsafe or pessimistic). For DAG-based real-time parallel tasks, to solve this problem, we propose DAG-Order, an order-based dynamic DAG scheduling approach, which strictly guarantees NoC real-time services. First, rather than build the new analysis to fit the widely used best-effort wormhole NoC, DAG-Order is built upon a kind of advanced low-latency NoC with SLT ( S ingle-cycle L ong-range T raversal) to avoid the unpredictable parallel transmission on the shared source-destination link of wormhole NoCs. Second, DAG-Order is a non-preemptive dynamic scheduling strategy, which jointly considers communication as well as computation workloads, and fits SLT NoC. With such an order-based dynamic scheduling strategy, the provably bound safety is ensured by enforcing certain order constraints among DAG edges/vertices that eliminate the execution-timing anomaly at runtime. Third, the order constraints are further relaxed for higher average-case runtime performance without compromising bound safety. Finally, an effective heuristic algorithm seeking a proper schedule order is developed to tighten the bounds. Experiments on synthetic and realistic benchmarks demonstrate that DAG-Order performs better than the state-of-the-art related scheduling methods.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4388281920",
    "type": "article"
  },
  {
    "title": "Abakus: Accelerating <i>k</i> -mer Counting with Storage Technology",
    "doi": "https://doi.org/10.1145/3632952",
    "publication_date": "2023-11-21",
    "publication_year": 2023,
    "authors": "Lingxi Wu; Minxuan Zhou; Weihong Xu; Ashish Venkat; Tajana Rosing; Kevin Skadron",
    "corresponding_authors": "",
    "abstract": "This work seeks to leverage Processing-with-storage-technology (PWST) to accelerate a key bioinformatics kernel called k -mer counting, which involves processing large files of sequence data on the disk to build a histogram of fixed-size genome sequence substrings and thereby entails prohibitively high I/O overhead. In particular, this work proposes a set of accelerator designs called Abakus that offer varying degrees of tradeoffs in terms of performance, efficiency, and hardware implementation complexity. The key to these designs is a set of domain-specific hardware extensions to accelerate the key operations for k -mer counting at various levels of the SSD hierarchy, with the goal of enhancing the limited computing capabilities of conventional SSDs, while exploiting the parallelism of the multi-channel, multi-way SSDs. Our evaluation suggests that Abakus can achieve 8.42×, 6.91×, and 2.32× speedup over the CPU-, GPU-, and near-data processing solutions.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4388848769",
    "type": "article"
  },
  {
    "title": "Improving Utilization of Dataflow Unit for Multi-Batch Processing",
    "doi": "https://doi.org/10.1145/3637906",
    "publication_date": "2023-12-18",
    "publication_year": 2023,
    "authors": "Zhihua Fan; Wenming Li; Zhen Wang; Yang Yu; Xiaochun Ye; Dongrui Fan; Ninghui Sun; Xuejun An",
    "corresponding_authors": "",
    "abstract": "Dataflow architectures can achieve much better performance and higher efficiency than general-purpose core, approaching the performance of a specialized design while retaining programmability. However, advanced application scenarios place higher demands on the hardware in terms of cross-domain and multi-batch processing. In this article, we propose a unified scale-vector architecture that can work in multiple modes and adapt to diverse algorithms and requirements efficiently. First, a novel reconfigurable interconnection structure is proposed, which can organize execution units into different cluster typologies as a way to accommodate different data-level parallelism. Second, we decouple threads within each DFG node into consecutive pipeline stages and provide architectural support. By time-multiplexing during these stages, dataflow hardware can achieve much higher utilization and performance. In addition, the task-based program model can also exploit multi-level parallelism and deploy applications efficiently. Evaluated in a wide range of benchmarks, including digital signal processing algorithms, CNNs, and scientific computing algorithms, our design attains up to 11.95× energy efficiency (performance-per-watt) improvement over GPU (V100), and 2.01× energy efficiency improvement over state-of-the-art dataflow architectures.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4389921984",
    "type": "article"
  },
  {
    "title": "A Concise Concurrent B <sup>+</sup> -Tree for Persistent Memory",
    "doi": "https://doi.org/10.1145/3638717",
    "publication_date": "2023-12-25",
    "publication_year": 2023,
    "authors": "Yan Wei; Xingjun Zhang",
    "corresponding_authors": "",
    "abstract": "Persistent memory (PM) presents a unique opportunity for designing data management systems that offer improved performance, scalability, and instant restart capability. As a widely used data structure for managing data in such systems, B + -Tree must address the challenges presented by PM in both data consistency and device performance. However, existing studies suffer from significant performance degradation when maintaining data consistency on PM. To settle this problem, we propose a new concurrent B + -Tree, CC-Tree, optimized for PM. CC-Tree ensures data consistency while providing high concurrent performance, thanks to several technologies, including partitioned metadata, log-free split, and lock-free read. We conducted experiments using state-of-the-art indices, and the results demonstrate significant performance improvements, including approximately 1.2–1.6x search, 1.5–1.7x insertion, 1.5–2.8x update, 1.9–4x deletion, 0.9–10x range scan, and up to 1.55–1.82x in hybrid workloads.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4390195611",
    "type": "article"
  },
  {
    "title": "Reducing register pressure in SMT processors through L2-miss-driven early register release",
    "doi": "https://doi.org/10.1145/1455650.1455652",
    "publication_date": "2008-11-01",
    "publication_year": 2008,
    "authors": "Joseph Sharkey; Jason Loew; Dmitry Ponomarev",
    "corresponding_authors": "",
    "abstract": "The register file is one of the most critical datapath components limiting the number of threads that can be supported on a simultaneous multithreading (SMT) processor. To allow the use of smaller register files without degrading performance, techniques that maximize the efficiency of using registers through aggressive register allocation/deallocation can be considered. In this article, we propose a novel technique to early deallocate physical registers allocated to threads which experience L2 cache misses. This is accomplished by speculatively committing the load-independent instructions and deallocating the registers corresponding to the previous mappings of their destinations, without waiting for the cache miss request to be serviced. The early deallocated registers are then made immediately available for allocation to instructions within the same thread as well as within other threads, thus improving the overall processor throughput. On the average across the simulated mixes of multiprogrammed SPEC 2000 workloads, our technique results in 33% improvement in throughput and 25% improvement in terms of harmonic mean of weighted IPCs over the baseline SMT with the state-of-the-art DCRA policy. This is achieved without creating checkpoints, maintaining per-register counters of pending consumers, performing tag rebroadcasts, register remappings, and/or additional associative searches.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W1973060457",
    "type": "article"
  },
  {
    "title": "Exploiting the reuse supplied by loop-dependent stream references for stream processors",
    "doi": "https://doi.org/10.1145/1839667.1839673",
    "publication_date": "2008-10-05",
    "publication_year": 2008,
    "authors": "Xuejun Yang; Ying Zhang; Xicheng Lu; Jingling Xue; Ian Rogers; Gen Li; Guibin Wang; Xudong Fang",
    "corresponding_authors": "",
    "abstract": "Memory accesses limit the performance of stream processors. By exploiting the reuse of data held in the Stream Register File (SRF), an on-chip, software controlled storage, the number of memory accesses can be reduced. In current stream compilers, reuse exploitation is only attempted for simple stream references, those whose start and end are known. Compiler analysis, from outside of stream processors, does not directly enable the consideration of other more complex stream references. In this article, we propose a transformation to automatically optimize stream programs to exploit the reuse supplied by loop-dependent stream references. The transformation is based on three results: lemmas identifying the reuse supplied by stream references, a new abstract representation called the Stream Reuse Graph (SRG) depicting the identified reuse, and the optimization of the SRG for our transformation. Both the reuse between the whole sequences accessed by stream references and between partial sequences is exploited in the article. In particular, partial reuse and its treatment are quite new and have never, to the best of our knowledge, appeared in scalar and vector processing. At the same time, reusing streams increases the pressure on the SRF, and this presents a problem of which reuse should be exploited within limited SRF capacity. We extend our analysis to achieve this objective. Finally, we implement our techniques based on the StreamC/KernelC compiler that has been optimized with the best existing compilation techniques for stream processors. Experimental results show a resultant speed-up of 1.14 to 2.54 times using a range of benchmarks.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2013322167",
    "type": "article"
  },
  {
    "title": "Managing Heterogeneous Datacenters with Tokens",
    "doi": "https://doi.org/10.1145/3191821",
    "publication_date": "2018-05-23",
    "publication_year": 2018,
    "authors": "Seyed Majid Zahedi; Songchun Fan; Benjamin C. Lee",
    "corresponding_authors": "",
    "abstract": "Ensuring fairness in a system with scarce, preferred resources requires time sharing. We consider a heterogeneous system with a few “big” and many “small” processors. We allocate heterogeneous processors using a novel token mechanism, which frames the allocation problem as a repeated game. At each round, users request big processors and spend a token if their request is granted. We analyze the game and optimize users’ strategies to produce an equilibrium. In equilibrium, allocations balance performance and fairness. Our mechanism outperforms classical, fair mechanisms by 1.7×, on average, in performance gains, and is competitive with a performance maximizing mechanism.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2803251212",
    "type": "article"
  },
  {
    "title": "Block Cooperation",
    "doi": "https://doi.org/10.1145/3243906",
    "publication_date": "2018-08-28",
    "publication_year": 2018,
    "authors": "Mohammad Khavari Tavana; Amir Kavyan Ziabari; David Kaeli",
    "corresponding_authors": "",
    "abstract": "Block-level cooperation is an endurance management technique that operates on top of error correction mechanisms to extend memory lifetimes. Once an error recovery scheme fails to recover from faults in a data block, the entire physical page associated with that block is disabled and becomes unavailable to the physical address space. To reduce the page waste caused by early block failures, other blocks can be used to support the failed block, working cooperatively to keep it alive and extend the faulty page’s lifetime. We combine the proposed technique with existing error recovery schemes, such as Error Correction Pointers (ECP) and Aegis, to increase memory lifetimes. Block cooperation is realized through metadata sharing in ECP, where one data block shares its unused metadata with another data block. When combined with Aegis, block cooperation is realized through reorganizing data layout, where blocks possessing few faults come to the aid of failed blocks, bringing them back from the dead. Our evaluation using Monte Carlo simulation shows that block cooperation at a single level (or multiple levels) on top of ECP and Aegis, boosts memory lifetimes by 28% (37%) and 8% (14%) on average, respectively. Furthermore, using trace-driven benchmark evaluation shows that lifetime boost can reach to 68% (30%) exploiting metadata sharing (or data layout reorganization).",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2889258022",
    "type": "article"
  },
  {
    "title": "Global Dead-Block Management for Task-Parallel Programs",
    "doi": "https://doi.org/10.1145/3234337",
    "publication_date": "2018-09-04",
    "publication_year": 2018,
    "authors": "Madhavan Manivannan; Miquel Pericàs; Vassilis Papaefstathiou; Per Stenström",
    "corresponding_authors": "",
    "abstract": "Task-parallel programs inefficiently utilize the cache hierarchy due to the presence of dead blocks in caches. Dead blocks may occupy cache space in multiple cache levels for a long time without providing any utility until they are finally evicted. Existing dead-block prediction schemes take decisions locally for each cache level and do not efficiently manage the entire cache hierarchy. This article introduces runtime-orchestrated global dead-block management , in which static and dynamic information about tasks available to the runtime system is used to effectively detect and manage dead blocks across the cache hierarchy. In the proposed global management schemes, static information (e.g., when tasks start/finish, and what data regions tasks produce/consume) is combined with dynamic information to detect when/where blocks become dead. When memory regions are deemed dead at some cache level(s), all the associated cache blocks are evicted from the corresponding level(s). We extend the cache controllers at both private and shared cache levels to use the aforementioned information to evict dead blocks. The article does an extensive evaluation of both inclusive and non-inclusive cache hierarchies and shows that the proposed global schemes outperform existing local dead-block management schemes.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2892217358",
    "type": "article"
  },
  {
    "title": "Static Prediction of Silent Stores",
    "doi": "https://doi.org/10.1145/3280848",
    "publication_date": "2018-11-16",
    "publication_year": 2018,
    "authors": "Fernando Magno Quintão Pereira; Guilherme Leobas; Abdoulaye Gamatié",
    "corresponding_authors": "",
    "abstract": "A store operation is called “silent” if it writes in memory a value that is already there. The ability to detect silent stores is important, because they might indicate performance bugs, might enable code optimizations, and might reveal opportunities of automatic parallelization, for instance. Silent stores are traditionally detected via profiling tools. In this article, we depart from this methodology and instead explore the following question: is it possible to predict silentness by analyzing the syntax of programs? The process of building an answer to this question is interesting in itself, given the stochastic nature of silent stores, which depend on data and coding style. To build such an answer, we have developed a methodology to classify store operations in terms of syntactic features of programs. Based on such features, we develop different kinds of predictors, some of which go much beyond what any trivial approach could achieve. To illustrate how static prediction can be employed in practice, we use it to optimize programs running on nonvolatile memory systems.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2900428612",
    "type": "article"
  },
  {
    "title": "Speeding up Iterative Polyhedral Schedule Optimization with Surrogate Performance Models",
    "doi": "https://doi.org/10.1145/3291773",
    "publication_date": "2018-12-19",
    "publication_year": 2018,
    "authors": "Stefan Ganser; Armin Größlinger; Norbert Siegmund; Sven Apel; Christian Lengauer",
    "corresponding_authors": "",
    "abstract": "Iterative program optimization is known to be able to adapt more easily to particular programs and target hardware than model-based approaches. An approach is to generate random program transformations and evaluate their profitability by applying them and benchmarking the transformed program on the target hardware. This procedure’s large computational effort impairs its practicality tremendously, though. To address this limitation, we pursue the guidance of a genetic algorithm for program optimization via feedback from surrogate performance models. We train the models on program transformations that were evaluated during previous iterative optimizations. Our representation of programs and program transformations refers to the polyhedron model. The representation is particularly meaningful for an optimization of loop programs that profit a from coarse-grained parallelization for execution on modern multicore-CPUs. Our evaluation reveals that surrogate performance models can be used to speed up the optimization of loop programs. We demonstrate that we can reduce the benchmarking effort required for an iterative optimization and degrade the resulting speedups by an average of 15%.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2905538126",
    "type": "article"
  },
  {
    "title": "Exploiting SIMD Asymmetry in ARM-to-x86 Dynamic Binary Translation",
    "doi": "https://doi.org/10.1145/3301488",
    "publication_date": "2019-02-13",
    "publication_year": 2019,
    "authors": "Yuping Liu; Ding‐Yong Hong; Jan‐Jan Wu; Sheng‐Yu Fu; Wei‐Chung Hsu",
    "corresponding_authors": "",
    "abstract": "Single instruction multiple data (SIMD) has been adopted for decades because of its superior performance and power efficiency. The SIMD capability (i.e., width, number of registers, and advanced instructions) has diverged rapidly on different SIMD instruction-set architectures (ISAs). Therefore, migrating existing applications to another host ISA that has fewer but longer SIMD registers and more advanced instructions raises the issues of asymmetric SIMD capability. To date, this issue has been overlooked and the host SIMD capability is underutilized, resulting in suboptimal performance. In this article, we present a novel binary translation technique called spill-aware superword level parallelism (saSLP), which combines short ARMv8 instructions and registers in the guest binaries to exploit the x86 AVX2 host’s parallelism, register capacity, and gather instructions. Our experiment results show that saSLP improves the performance by 1.6× (2.3×) across a number of benchmarks and reduces spilling by 97% (99%) for ARMv8 to x86 AVX2 (AVX-512) translation. Furthermore, with AVX2 (AVX-512) gather instructions, saSLP speeds up several data-irregular applications that cannot be vectorized on ARMv8 NEON by up to 3.9× (4.2×).",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2911551207",
    "type": "article"
  },
  {
    "title": "Efficient Data Supply for Parallel Heterogeneous Architectures",
    "doi": "https://doi.org/10.1145/3310332",
    "publication_date": "2019-04-26",
    "publication_year": 2019,
    "authors": "Tae Jun Ham; Juan L. Aragón; Margaret Martonosi",
    "corresponding_authors": "",
    "abstract": "Decoupling techniques have been proposed to reduce the amount of memory latency exposed to high-performance accelerators as they fetch data. Although decoupled access-execute (DAE) and more recent decoupled data supply approaches offer promising single-threaded performance improvements, little work has considered how to extend them into parallel scenarios. This article explores the opportunities and challenges of designing parallel, high-performance, resource-efficient decoupled data supply systems. We propose M ercury , a parallel decoupled data supply system that utilizes thread-level parallelism for high-throughput data supply with good portability attributes. Additionally, we introduce some microarchitectural improvements for data supply units to efficiently handle long-latency indirect loads.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2943225788",
    "type": "article"
  },
  {
    "title": "A Model-Based Software Solution for Simultaneous Multiple Kernels on GPUs",
    "doi": "https://doi.org/10.1145/3377138",
    "publication_date": "2020-03-04",
    "publication_year": 2020,
    "authors": "Hao Wu; Weizhi Liu; Huanxin Lin; Cho‐Li Wang",
    "corresponding_authors": "",
    "abstract": "As a critical computing resource in multiuser systems such as supercomputers, data centers, and cloud services, a GPU contains multiple compute units (CUs). GPU Multitasking is an intuitive solution to underutilization in GPGPU computing. Recently proposed solutions of multitasking GPUs can be classified into two categories: (1) spatially partitioned sharing (SPS), which coexecutes different kernels on disjointed sets of compute units (CU), and (2) simultaneous multikernel (SMK), which runs multiple kernels simultaneously within a CU. Compared to SPS, SMK can improve resource utilization even further due to the interleaving of instructions from kernels with low dynamic resource contentions. However, it is hard to implement SMK on current GPU architecture, because (1) techniques for applying SMK on top of GPU hardware scheduling policy are scarce and (2) finding an efficient SMK scheme is difficult due to the complex interferences of concurrently executed kernels. In this article, we propose a lightweight and effective performance model to evaluate the complex interferences of SMK. Based on the probability of independent events, our performance model is built from a totally new angle and contains limited parameters. Then, we propose a metric, symbiotic factor , which can evaluate an SMK scheme so that kernels with complementary resource utilization can corun within a CU. Also, we analyze the advantages and disadvantages of kernel slicing and kernel stretching techniques and integrate them to apply SMK on GPUs instead of simulators. We validate our model on 18 benchmarks. Compared to the optimized hardware-based concurrent kernel execution whose kernel launching order brings fast execution time, the results of corunning kernel pairs show 11%, 18%, and 12% speedup on AMD R9 290X, RX 480, and Vega 64, respectively, on average. Compared to the Warped-Slicer, the results show 29%, 18%, and 51% speedup on AMD R9 290X, RX 480, and Vega 64, respectively, on average.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3017087102",
    "type": "article"
  },
  {
    "title": "Runtime Design Space Exploration and Mapping of DCNNs for the Ultra-Low-Power Orlando SoC",
    "doi": "https://doi.org/10.1145/3379933",
    "publication_date": "2020-05-29",
    "publication_year": 2020,
    "authors": "Ahmet Erdem; Cristina Silvano; Thomas Boesch; Andrea Carlo Ornstein; Surinder-Pal Singh; Giuseppe Desoli",
    "corresponding_authors": "",
    "abstract": "Recent trends in deep convolutional neural networks (DCNNs) impose hardware accelerators as a viable solution for computer vision and speech recognition. The Orlando SoC architecture from STMicroelectronics targets exactly this class of problems by integrating hardware-accelerated convolutional blocks together with DSPs and on-chip memory resources to enable energy-efficient designs of DCNNs. The main advantage of the Orlando platform is to have runtime configurable convolutional accelerators that can adapt to different DCNN workloads. This opens new challenges for mapping the computation to the accelerators and for managing the on-chip resources efficiently. In this work, we propose a runtime design space exploration and mapping methodology for runtime resource management in terms of on-chip memory, convolutional accelerators, and external bandwidth. Experimental results are reported in terms of power/performance scalability, Pareto analysis, mapping adaptivity, and accelerator utilization for the Orlando architecture mapping the VGG-16, Tiny-Yolo(v2), and MobileNet topologies.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3043387812",
    "type": "article"
  },
  {
    "title": "The migration prefetcher",
    "doi": "https://doi.org/10.1145/2086696.2086724",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Javier Lira; Timothy M. Jones; Carlos Molina; Antonio González",
    "corresponding_authors": "",
    "abstract": "The exponential increase in multicore processor (CMP) cache sizes accompanied by growing on-chip wire delays make it difficult to implement traditional caches with a single, uniform access latency. Non-Uniform Cache Architecture (NUCA) designs have been proposed to address this problem. A NUCA divides the whole cache memory into smaller banks and allows banks nearer a processor core to have lower access latencies than those further away, thus mitigating the effects of the cache's internal wires. Determining the best placement for data in the NUCA cache at any particular moment during program execution is crucial for exploiting the benefits that this architecture provides. Dynamic NUCA (D-NUCA) allows data to be mapped to multiple banks within the NUCA cache, and then uses data migration to adapt data placement to the program's behavior. Although the standard migration scheme is effective in moving data to its optimal position within the cache, half the hits still occur within non-optimal banks. This paper reduces this number by anticipating data migrations and moving data to the optimal banks in advance of being required. We introduce a prefetcher component to the NUCA cache that predicts the next memory request based on the past. We develop a realistic implementation of this prefetcher and, furthermore, experiment with a perfect prefetcher that always knows where the data resides, in order to evaluate the limits of this approach. We show that using our realistic data prefetching to anticipate data migrations in the NUCA cache can reduce the access latency by 15% on average and achieve performance improvements of up to 17%.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W1967788408",
    "type": "article"
  },
  {
    "title": "Profile-guided floating- to fixed-point conversion for hybrid FPGA-processor applications",
    "doi": "https://doi.org/10.1145/2400682.2400702",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Doris Chen; Deshanand P. Singh",
    "corresponding_authors": "",
    "abstract": "The key to enabling widespread use of FPGAs for algorithm acceleration is to allow programmers to create efficient designs without the time-consuming hardware design process. Programmers are used to developing scientific and mathematical algorithms in high-level languages (C/C++) using floating point data types. Although easy to implement, the dynamic range provided by floating point is not necessary in many applications; more efficient implementations can be realized using fixed point arithmetic. While this topic has been studied previously [Han et al. 2006; Olson et al. 1999; Gaffar et al. 2004; Aamodt and Chow 1999], the degree of full automation has always been lacking. We present a novel design flow for cases where FPGAs are used to offload computations from a microprocessor. Our LLVM-based algorithm inserts value profiling code into an unmodified C/C++ application to guide its automatic conversion to fixed point. This allows for fast and accurate design space exploration on a host microprocessor before any accelerators are mapped to the FPGA. Through experimental results, we demonstrate that fixed-point conversion can yield resource savings of up to 2x--3x reductions. Embedded RAM usage is minimized, and 13%--22% higher F max than the original floating-point implementation is observed. In a case study, we show that 17% reduction in logic and 24% reduction in register usage can be realized by using our algorithm in conjunction with a High-Level Synthesis (HLS) tool.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2059966556",
    "type": "article"
  },
  {
    "title": "Adaptive communication mechanism for accelerating MPI functions in NoC-based multicore processors",
    "doi": "https://doi.org/10.1145/2512434",
    "publication_date": "2013-09-16",
    "publication_year": 2013,
    "authors": "Libo Huang; Zhiying Wang; Nong Xiao; Yongwen Wang; Qiang Dou",
    "corresponding_authors": "",
    "abstract": "Multicore designs have emerged as the dominant organization for future high-performance microprocessors. Communication in such designs is often enabled by Networks-on-Chip (NoCs). A new trend in such architectures is to fit a Message Passing Interface (MPI) programming model on NoCs to achieve optimal parallel application performance. A key issue in designing MPI over NoCs is communication protocol, which has not been explored in previous research. This article advocates a hardware-supported communication mechanism using a protocol-adaptive approach to adjust to varying NoC configurations (e.g., number of buffers) and workload behavior (e.g., number of messages). We propose the ADaptive Communication Mechanism (ADCM), a hybrid protocol that involves behavior similar to buffered communication when sufficient buffer is available in the receiver to that similar to a synchronous protocol when buffers in the receiver are limited. ADCM adapts dynamically by deciding communication protocol on a per-request basis using a local estimate of recent buffer utilization. ADCM attempts to combine both the advantages of buffered and synchronous communication modes to achieve enhanced throughput and performance. Simulations of various workloads show that the proposed communication mechanism can be effectively used in future NoC designs.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2074036292",
    "type": "article"
  },
  {
    "title": "Low-latency adaptive mode transitions and hierarchical power management in asymmetric clustered cores",
    "doi": "https://doi.org/10.1145/2499901",
    "publication_date": "2013-09-16",
    "publication_year": 2013,
    "authors": "Eran Shifer; Shlomo Weiss",
    "corresponding_authors": "",
    "abstract": "Recently, engineering solutions that include asymmetric multicores have been fabricated for low form-factor computing devices, indicating a potential direction for future evolution of processors. In this article we propose an asymmetric clustered core architecture, exhibiting low-latency switching between modes relative to asymmetric multicores, and having similarities with the same asymmetric multicore architecture in the context of a wider dynamic range of the processor power-performance characteristic. Asymmetric clustered cores incur additional microarchitectural complexity and area cost inside a core but exhibit better chip-level integration characteristics compared to asymmetric multicores. Focusing on power efficiency of asymmetric clustered cores, we describe: (1) a hierarchical power management partitioning between the operating system and on-die firmware for coarse-grain switch policies, and (2) core-internal tracking hardware for fine-grain switching. The mode switch policies of the core's tracking hardware are dependent on higher-level directives and hints from the operating system, on-die firmware, and compiler or profiling software. We further explore the potential power management benefits of asymmetric clustered cores relative to asymmetric multicores, demonstrating that the ability of asymmetric clustered cores to use tight training periods for adaptive behavior, with low overhead switching between modes, results in a more efficient utilization of power management directives.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2077635370",
    "type": "article"
  },
  {
    "title": "Evaluator-executor transformation for efficient pipelining of loops with conditionals",
    "doi": "https://doi.org/10.1145/2541228.2555317",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Yeonghun Jeong; Seongseok Seo; Jongeun Lee",
    "corresponding_authors": "",
    "abstract": "Control divergence poses many problems in parallelizing loops. While predicated execution is commonly used to convert control dependence into data dependence, it often incurs high overhead because it allocates resources equally for both branches of a conditional statement regardless of their execution frequencies. For those loops with unbalanced conditionals, we propose a software transformation that divides a loop into two or three smaller loops so that the condition is evaluated only in the first loop, while the less frequent branch is executed in the second loop in a way that is much more efficient than in the original loop. To reduce the overhead of extra data transfer caused by the loop fission, we also present a hardware extension for a class of Coarse-Grained Reconfigurable Architectures (CGRAs). Our experiments using MiBench and computer vision benchmarks on a CGRA demonstrate that our techniques can improve the performance of loops over predicated execution by up to 65% (37.5%, on average), when the hardware extension is enabled. Without any hardware modification, our software-only version can improve performance by up to 64% (33%, on average), while simultaneously reducing the energy consumption of the entire CGRA including configuration and data memory by 22%, on average.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2105641776",
    "type": "article"
  },
  {
    "title": "Tile size selection revisited",
    "doi": "https://doi.org/10.1145/2555289.2555292",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Sanyam Mehta; Gautham Beeraka; Pen-Chung Yew",
    "corresponding_authors": "",
    "abstract": "Loop tiling is a widely used loop transformation to enhance data locality and allow data reuse. In the tiled code, however, tiles of different sizes can lead to significant variation in performance. Thus, selection of an optimal tile size is critical to performance of tiled codes.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4243958732",
    "type": "article"
  },
  {
    "title": "Designing a practical data filter cache to improve both energy efficiency and performance",
    "doi": "https://doi.org/10.1145/2555289.2555310",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Alen Bardizbanyan; Magnus Själander; David Whalley; Per Larsson-Edefors",
    "corresponding_authors": "",
    "abstract": "Conventional Data Filter Cache (DFC) designs improve processor energy efficiency, but degrade performance. Furthermore, the single-cycle line transfer suggested in prior studies adversely affects Level-1 Data Cache (L1 DC) area and energy efficiency. We propose a practical DFC that is accessed early in the pipeline and transfers a line over multiple cycles. Our DFC design improves performance and eliminates a substantial fraction of L1 DC accesses for loads, L1 DC tag checks on stores, and data translation lookaside buffer accesses for both loads and stores. Our evaluation shows that the proposed DFC can reduce the data access energy by 42.5% and improve execution time by 4.2%.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4248463787",
    "type": "article"
  },
  {
    "title": "Information flow tracking meets just-in-time compilation",
    "doi": "https://doi.org/10.1145/2555289.2555295",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Christoph Kerschbaumer; Eric Hennigan; Per Larsen; Stefan Brunthaler; Michael Franz",
    "corresponding_authors": "",
    "abstract": "Web applications are vulnerable to cross-site scripting attacks that enable data thefts. Information flow tracking in web browsers can prevent communication of sensitive data to unintended recipients and thereby stop such data thefts. Unfortunately, existing solutions have focused on incorporating information flow into browsers' JavaScript interpreters, rather than just-in-time compilers, rendering the resulting performance noncompetitive. Few users will switch to a safer browser if it comes at the cost of significantly degrading web application performance.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4253058387",
    "type": "article"
  },
  {
    "title": "Abstracting access patterns of dynamic memory using regular expressions",
    "doi": "https://doi.org/10.1145/1498690.1498693",
    "publication_date": "2009-03-01",
    "publication_year": 2009,
    "authors": "Jinseong Jeon; Keoncheol Shin; Hwansoo Han",
    "corresponding_authors": "",
    "abstract": "Unless the speed gap between CPU and memory disappears, efficient memory usage remains a decisive factor for performance. To optimize data usage of programs in the presence of the memory hierarchy, we are particularly interested in two compiler techniques: pool allocation and field layout restructuring . Since foreseeing runtime behaviors of programs at compile time is difficult, most of the previous work relied on profiling. On the contrary, our goal is to develop a fully automatic compiler that statically transforms input codes to use memory efficiently. Noticing that regular expressions , which denote repetition explicitly, are sufficient for memory access patterns, we describe how to extract memory access patterns as regular expressions in detail. Based on static patterns presented in regular expressions, we apply pool allocation to repeatedly accessed structures and exploit field layout restructuring according to field affinity relations of chosen structures. To make a scalable framework, we devise and apply new abstraction techniques, which build and interpret access patterns for the whole programs in a bottom-up fashion. We implement our analyses and transformations with the CIL compiler. To verify the effect and scalability of our scheme, we examine 17 benchmarks including 2 SPECINT 2000 benchmarks whose source lines of code are larger than 10,000. Our experiments demonstrate that the static layout transformations for dynamic memory can reduce L1D cache misses by 16% and execution times by 14% on average.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2040717438",
    "type": "article"
  },
  {
    "title": "Checkpoint allocation and release",
    "doi": "https://doi.org/10.1145/1582710.1582712",
    "publication_date": "2009-09-01",
    "publication_year": 2009,
    "authors": "Amit Golander; Shlomo Weiss",
    "corresponding_authors": "",
    "abstract": "Out-of-order speculative processors need a bookkeeping method to recover from incorrect speculation. In recent years, several microarchitectures that employ checkpoints have been proposed, either extending the reorder buffer or entirely replacing it. This work presents an in-dept-study of checkpointing in checkpoint-based microarchitectures, from the desired content of a checkpoint, via implementation trade-offs, and to checkpoint allocation and release policies. A major contribution of the article is a novel adaptive checkpoint allocation policy that outperforms known policies. The adaptive policy controls checkpoint allocation according to dynamic events, such as second-level cache misses and rollback history. It achieves 6.8% and 2.2% speedup for the integer and floating point benchmarks, respectively, and does not require a branch confidence estimator. The results show that the proposed adaptive policy achieves most of the potential of an oracle policy whose performance improvement is 9.8% and 3.9% for the integer and floating point benchmarks, respectively. We exploit known techniques for saving leakage power by adapting and applying them to checkpoint-based microarchitectures. The proposed applications combine to reduce the leakage power of the register file to about one half of its original value.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2066543901",
    "type": "article"
  },
  {
    "title": "Improving Loop Dependence Analysis",
    "doi": "https://doi.org/10.1145/3095754",
    "publication_date": "2017-08-22",
    "publication_year": 2017,
    "authors": "Nicklas Bo Jensen; Sven Karlsson",
    "corresponding_authors": "",
    "abstract": "Programmers can no longer depend on new processors to have significantly improved single-thread performance. Instead, gains have to come from other sources such as the compiler and its optimization passes. Advanced passes make use of information on the dependencies related to loops. We improve the quality of that information by reusing the information given by the programmer for parallelization. We have implemented a prototype based on GCC into which we also add a new optimization pass. Our approach improves the amount of correctly classified dependencies resulting in 46% average improvement in single-thread performance for kernel benchmarks compared to GCC 6.1.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2747413550",
    "type": "article"
  },
  {
    "title": "Triple Engine Processor (TEP)",
    "doi": "https://doi.org/10.1145/3155920",
    "publication_date": "2017-12-18",
    "publication_year": 2017,
    "authors": "Hong-Yeol Lim; Gi-Ho Park",
    "corresponding_authors": "",
    "abstract": "The advent of 3D memory stacking technology, which integrates a logic layer and stacked memories, is expected to be one of the most promising memory technologies to mitigate the memory wall problem by leveraging the concept of near-memory processing (NMP). With the ability to process data locally within the logic layer of stacked memory, a variety of emerging big data applications can achieve significant performance and energy-efficiency benefits. Various approaches to the NMP logic layer architecture have been studied to utilize the advantage of stacked memory. While significant acceleration of specific kernel operations has been derived from previous NMP studies, an NMP-based system using an NMP logic architecture capable of handling some specific kernel operations can suffer from performance and energy efficiency degradation caused by a significant communication overhead between the host processor and NMP stack. In this article, we first analyze the kernel operations that can greatly improve the performance of NMP-based systems in diverse emerging applications, and then we analyze the architecture to efficiently process the extracted kernel operations. This analysis confirms that three categories of processing engines for NMP logic are required for efficient processing of a variety of emerging applications, and thus we propose a Triple Engine Processor (TEP), a heterogeneous near-memory processor with three types of computing engines. These three types of engines are an in-order core, a coerce-grain reconfigurable processor (CGRA), and dedicated hardware. The proposed TEP provides about 3.4 times higher performance and 33% greater energy savings than the baseline 3D memory system.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2775998749",
    "type": "article"
  },
  {
    "title": "Optimization of Triangular and Banded Matrix Operations Using 2d-Packed Layouts",
    "doi": "https://doi.org/10.1145/3162016",
    "publication_date": "2017-12-18",
    "publication_year": 2017,
    "authors": "Toufik Baroudi; Rachid Seghir; Vincent Loechner",
    "corresponding_authors": "",
    "abstract": "Over the past few years, multicore systems have become increasingly powerful and thereby very useful in high-performance computing. However, many applications, such as some linear algebra algorithms, still cannot take full advantage of these systems. This is mainly due to the shortage of optimization techniques dealing with irregular control structures. In particular, the well-known polyhedral model fails to optimize loop nests whose bounds and/or array references are not affine functions. This is more likely to occur when handling sparse matrices in their packed formats. In this article, we propose using 2d-packed layouts and simple affine transformations to enable optimization of triangular and banded matrix operations. The benefit of our proposal is shown through an experimental study over a set of linear algebra benchmarks.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2779347689",
    "type": "article"
  },
  {
    "title": "HAShCache",
    "doi": "https://doi.org/10.1145/3158641",
    "publication_date": "2017-12-18",
    "publication_year": 2017,
    "authors": "Adarsh Patil; Ramaswamy Govindarajan",
    "corresponding_authors": "",
    "abstract": "Integrated Heterogeneous System (IHS) processors pack throughput-oriented General-Purpose Graphics Pprocessing Units (GPGPUs) alongside latency-oriented Central Processing Units (CPUs) on the same die sharing certain resources, e.g., shared last-level cache, Network-on-Chip (NoC), and the main memory. The demands for memory accesses and other shared resources from GPU cores can exceed that of CPU cores by two to three orders of magnitude. This disparity poses significant problems in exploiting the full potential of these architectures. In this article, we propose adding a large-capacity stacked DRAM, used as a shared last-level cache, for the IHS processors. However, adding the DRAMCache naively, leaves significant performance on the table due to the disparate demands from CPU and GPU cores for DRAMCache and memory accesses. In particular, the imbalance can significantly reduce the performance benefits that the CPU cores would have otherwise enjoyed with the introduction of the DRAMCache, necessitating a heterogeneity-aware management of this shared resource for improved performance. In this article, we propose three simple techniques to enhance the performance of CPU application while ensuring very little to no performance impact to the GPU. Specifically, we propose (i) PrIS , a prioritization scheme for scheduling CPU requests at the DRAMCache controller; (ii) ByE , a selective and temporal bypassing scheme for CPU requests at the DRAMCache; and (iii) Chaining , an occupancy controlling mechanism for GPU lines in the DRAMCache through pseudo-associativity. The resulting cache, Heterogeneity-Aware Shared DRAMCache (HAShCache), is heterogeneity-aware and can adapt dynamically to address the inherent disparity of demands in an IHS architecture. Experimental evaluation of the proposed HAShCache results in an average system performance improvement of 41% over a naive DRAMCache and over 200% improvement over a baseline system with no stacked DRAMCache.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2780898194",
    "type": "article"
  },
  {
    "title": "A Case for Fine-grain Coherence Specialization in Heterogeneous Systems",
    "doi": "https://doi.org/10.1145/3530819",
    "publication_date": "2022-04-25",
    "publication_year": 2022,
    "authors": "Johnathan Alsop; Weon Taek Na; Matthew D. Sinclair; Samuel Grayson; Sarita V. Adve",
    "corresponding_authors": "",
    "abstract": "Hardware specialization is becoming a key enabler of energy-efficient performance. Future systems will be increasingly heterogeneous, integrating multiple specialized and programmable accelerators, each with different memory demands. Traditionally, communication between accelerators has been inefficient, typically orchestrated through explicit DMA transfers between different address spaces. More recently, industry has proposed unified coherent memory which enables implicit data movement and more data reuse, but often these interfaces limit the coherence flexibility available to heterogeneous systems. This paper demonstrates the benefits of fine-grained coherence specialization for heterogeneous systems. We propose an architecture that enables low-complexity independent specialization of each individual coherence request in heterogeneous workloads by building upon a simple and flexible baseline coherence interface, Spandex. We then describe how to optimize individual memory requests to improve cache reuse and performance-critical memory latency in emerging heterogeneous workloads. Collectively, our techniques enable significant gains, reducing execution time by up to 61% or network traffic by up to 99% while adding minimal complexity to the Spandex protocol.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3157813827",
    "type": "article"
  },
  {
    "title": "CASHT: Contention Analysis in Shared Hierarchies with Thefts",
    "doi": "https://doi.org/10.1145/3494538",
    "publication_date": "2022-01-23",
    "publication_year": 2022,
    "authors": "Cesar Gomes; Maziar Amiraski; Mark Hempstead",
    "corresponding_authors": "",
    "abstract": "Cache management policies should consider workloads’ contention behavior when managing a shared cache. Prior art makes estimates about shared cache behavior by adding extra logic or time to isolate per workload cache statistics. These approaches provide per-workload analysis but do not provide a holistic understanding of the utilization and effectiveness of caches under the ever-growing contention that comes standard with scaling cores. We present Contention Analysis in Shared Hierarchies using Thefts, or CASHT, 1 a framework for capturing cache contention information both offline and online. CASHT takes advantage of cache statistics made richer by observing a consequence of cache contention: inter-core evictions, or what we call THEFTS. We use thefts to complement more familiar cache statistics to train a learning model based on Gradient-boosting Trees (GBT) to predict the best ways to partition the last-level cache. GBT achieves 90+% accuracy with trained models as small as 100 B and at least 95% accuracy at 1 kB model size when predicting the best way to partition two workloads. CASHT employs a novel run-time framework for collecting thefts-based metrics despite partition intervention, and enables per-access sampling rather than set sampling that could add overhead but may not capture true workload behavior. Coupling CASHT and GBT for use as a dynamic policy results in a very lightweight and dynamic partitioning scheme that performs within a margin of error of Utility-based Cache Partitioning at a 1/8 the overhead.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4207040076",
    "type": "article"
  },
  {
    "title": "Memory-Aware Functional IR for Higher-Level Synthesis of Accelerators",
    "doi": "https://doi.org/10.1145/3501768",
    "publication_date": "2022-01-31",
    "publication_year": 2022,
    "authors": "Christof Schlaak; Tzung-Han Juang; Christophe Dubach",
    "corresponding_authors": "",
    "abstract": "Specialized accelerators deliver orders of a magnitude of higher performance than general-purpose processors. The ever-changing nature of modern workloads is pushing the adoption of Field Programmable Gate Arrays (FPGAs) as the substrate of choice. However, FPGAs are hard to program directly using Hardware Description Languages (HDLs). Even modern high-level HDLs, e.g., Spatial and Chisel, still require hardware expertise. This article adopts functional programming concepts to provide a hardware-agnostic higher-level programming abstraction. During synthesis, these abstractions are mechanically lowered into a functional Intermediate Representation (IR) that defines a specific hardware design point. This novel IR expresses different forms of parallelism and standard memory features such as asynchronous off-chip memories or synchronous on-chip buffers. Exposing such features at the IR level is essential for achieving high performance. The viability of this approach is demonstrated on two stencil computations and by exploring the optimization space of matrix-matrix multiplication. Starting from a high-level representation for these algorithms, our compiler produces low-level VHSIC Hardware Description Language (VHDL) code automatically. Several design points are evaluated on an Intel Arria 10 FPGA, demonstrating the ability of the IR to exploit different hardware features. This article also shows that the designs produced are competitive with highly tuned OpenCL implementations and outperform hardware-agnostic OpenCL code.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4210570899",
    "type": "article"
  },
  {
    "title": "SIMD-Matcher: A SIMD-based Arbitrary Matching Framework",
    "doi": "https://doi.org/10.1145/3514246",
    "publication_date": "2022-02-11",
    "publication_year": 2022,
    "authors": "Ping Wang; Fei Wen; Paul V. Gratz; Alex Sprintson",
    "corresponding_authors": "",
    "abstract": "Packet classification methods rely upon matching packet content/header against pre-defined rules, which are generated by network applications and their configurations. With the rapid development of network technology and the fast-growing network applications, users seek more enhanced, secure, and diverse network services. Hence it becomes critical to improve the performance of arbitrary matching operations. This article presents SIMD-Matcher, an efficient Single Instruction Multiple Data (SIMD) and cache-friendly arbitrary matching framework. To further improve the arbitrary matching performance, SIMD-Matcher adopts a trie node with a fixed high fanout and a varying span for each node depending on the data distribution. The trie node layout leverages cache and modern processor features such as SIMD instructions. To support arbitrary matching, we first interpret arbitrary rules into three fields: value, mask, and priority. Second, to support insertion of randomly positioned wildcards to arbitrary rules, we propose the SIMD-Matcher extraction algorithm to process the wildcard bits. Third, we add an array of wildcard entries to the leaf entries, which store the wildcard rules and guarantee the correctness of matching results. Experiments show that SIMD-Matcher outperforms GenMatcher under large-scale ruleset and key set, in terms of search time, insert time, and memory cost. Specifically with 5M rules, our method achieves a 2.7X speedup on search time, and the insertion time takes \\( ~\\sim \\!\\! 7.3 \\) seconds, gaining a 1.38X speedup; meanwhile, the memory cost reduction is up to 6.17X.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4211205769",
    "type": "article"
  },
  {
    "title": "CARL: Compiler Assigned Reference Leasing",
    "doi": "https://doi.org/10.1145/3498730",
    "publication_date": "2022-03-17",
    "publication_year": 2022,
    "authors": "Chen Ding; Dong Chen; Fangzhou Liu; Benjamin Reber; Wesley Smith",
    "corresponding_authors": "",
    "abstract": "Data movement is a common performance bottleneck, and its chief remedy is caching. Traditional cache management is transparent to the workload: data that should be kept in cache are determined by the recency information only, while the program information, i.e., future data reuses, is not communicated to the cache. This has changed in a new cache design named Lease Cache . The program control is passed to the lease cache by a compiler technique called Compiler Assigned Reference Lease (CARL). This technique collects the reuse interval distribution for each reference and uses it to compute and assign the lease value to each reference. In this article, we prove that CARL is optimal under certain statistical assumptions. Based on this optimality, we prove miss curve convexity, which is useful for optimizing shared cache, and sub-partitioning monotonicity, which simplifies lease compilation. We evaluate the potential using scientific kernels from PolyBench and show that compiler insertions of up to 34 leases in program code achieve similar or better cache utilization (in variable size cache) than the optimal fixed-size caching policy, which has been unattainable with automatic caching but now within the potential of cache programming for all tested programs and most cache sizes.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4220931662",
    "type": "article"
  },
  {
    "title": "Just-In-Time Compilation on ARM—A Closer Look at Call-Site Code Consistency",
    "doi": "https://doi.org/10.1145/3546568",
    "publication_date": "2022-07-06",
    "publication_year": 2022,
    "authors": "Tim Hartley; Foivos S. Zakkak; Andy Nisbet; Christos Kotselidis; Mikel Luján",
    "corresponding_authors": "",
    "abstract": "The increase in computational capability of low-power Arm architectures has seen them diversify from their more traditional domain of portable battery powered devices into data center servers, personal computers, and even Supercomputers. Thus, managed languages (Java, Javascript, etc.) that require a managed runtime environment (MRE) need to be ported to the Arm architecture, requiring an understanding of different design tradeoffs. This article studies how the lack of strong hardware support for Self Modifying Code (SMC) in low-power architectures (e.g., absence of cache coherence between instruction cache and data caches), affects Just-In-Time (JIT) compilation and runtime behavior in MREs. Specifically, we focus on the implementation and treatment of call-sites, that must maintain code consistency in the face of concurrent execution and modification to redirect control (patching) by the MRE. The lack of coherence, is compounded with the maximum distance (reach of) a call-site can jump to as the reach is more constrained (smaller distance) in Arm when compared with Intel/AMD. We present four different robust implementations for call-sites and discuss their advantages and disadvantages in the absence of strong hardware support for SMC. Finally, we evaluate each approach using a microbenchmark, further evaluating the best three techniques using three JVM benchmark suites and the open source MaxineVM showcasing performance differences up to 12%. Based on these observations, we propose extending code-cache partitioning strategies for JIT compiled code to encourage more efficient local branching for architectures with limited direct branch ranges.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4283833760",
    "type": "article"
  },
  {
    "title": "Solving Sparse Assignment Problems on FPGAs",
    "doi": "https://doi.org/10.1145/3546072",
    "publication_date": "2022-09-14",
    "publication_year": 2022,
    "authors": "Erling Jellum; Milica Orlandić; Edmund Brekke; Tor Arne Johansen; Torleiv H. Bryne",
    "corresponding_authors": "",
    "abstract": "The assignment problem is a fundamental optimization problem and a crucial part of many systems. For example, in multiple object tracking, the assignment problem is used to associate object detections with hypothetical target tracks and solving the assignment problem is one of the most compute-intensive tasks. To enable low-latency real-time implementations, efficient solutions to the assignment problem is required. In this work, we present Sparse and Speculative (SaS) Auction, a novel implementation of the popular Auction algorithm for FPGAs. Two novel optimizations are proposed. First, the pipeline width and depth are reduced by exploiting sparsity in the input problems. Second, dependency speculation is employed to enable a fully pipelined design and increase the throughput. Speedups as high as 50 × are achieved relative to the state-of-the-art implementation for some input distributions. We evaluate the implementation both on randomly generated datasets and realistic datasets from multiple object tracking.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4295678633",
    "type": "article"
  },
  {
    "title": "XEngine: Optimal Tensor Rematerialization for Neural Networks in Heterogeneous Environments",
    "doi": "https://doi.org/10.1145/3568956",
    "publication_date": "2022-10-20",
    "publication_year": 2022,
    "authors": "Manuela Schuler; Richard Membarth; Philipp Slusallek",
    "corresponding_authors": "",
    "abstract": "Memory efficiency is crucial in training deep learning networks on resource-restricted devices. During backpropagation, forward tensors are used to calculate gradients. Despite the option of keeping those dependencies in memory until they are reused in backpropagation, some forward tensors can be discarded and recomputed later from saved tensors, so-called checkpoints. This allows, in particular, for resource-constrained heterogeneous environments to make use of all available compute devices. Unfortunately, the definition of these checkpoints is a non-trivial problem and poses a challenge to the programmer - improper or excessive recomputations negate the benefit of checkpointing. In this article, we present XEngine, an approach that schedules network operators to heterogeneous devices in low memory environments by determining checkpoints and recomputations of tensors. Our approach selects suitable resources per timestep and operator and optimizes the end-to-end time for neural networks taking the memory limitation of each device into account. For this, we formulate a mixed-integer quadratic program (MIQP) to schedule operators of deep learning networks on heterogeneous systems. We compare our MIQP solver XEngine against Checkmate, a mixed-integer linear programming (MILP) approach that solves recomputation on a single device. Our solver finds solutions that are up to 22.5 % faster than the fastest Checkmate schedule in which the network is computed exclusively on a single device. We also find valid schedules for networks making use of both central processing units and graphics processing units if memory limitations do not allow scheduling exclusively to the graphics processing unit.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4306873623",
    "type": "article"
  },
  {
    "title": "RegCPython: A Register-based Python Interpreter for Better Performance",
    "doi": "https://doi.org/10.1145/3568973",
    "publication_date": "2022-10-21",
    "publication_year": 2022,
    "authors": "Qiang Zhang; Lei Xu; Baowen Xu",
    "corresponding_authors": "",
    "abstract": "Interpreters are widely used in the implementation of many programming languages, such as Python, Perl, and Java. Even though various JIT compilers emerge in an endless stream, interpretation efficiency still plays a critical role in program performance. Does a stack-based interpreter or a register-based interpreter perform better? The pros and cons of the pair of architectures have long been discussed. The stack architecture is attractive for its concise model and compact bytecode, but our study finds that the register-based interpreter can also be implemented easily and that its bytecode size only grows by a small margin. Moreover, the latter turns out to be appreciably faster. Specifically, we implemented an open source Python interpreter named RegCPython based on CPython v3.10.1. The former is register based, while the latter is stack based. Without changes in syntax, Application Programming Interface, and Application Binary Interface, RegCPython is excellently compatible with CPython, as it does not break existing syntax or interfaces. It achieves a speedup of 1.287 on the most favorable benchmark and 0.977 even on the most unfavorable benchmark. For all Python-intensive benchmarks, the average speedup reaches 1.120 on x86 and 1.130 on ARM. Our evaluation work, which also serves as an empirical study, provides a detailed performance survey of both interpreters on modern hardware. It points out that the register-based interpreters are more efficient mainly due to the elimination of machine instructions needed, while changes in branch mispredictions and cache misses have a limited impact on performance. Additionally, it confirms that the register-based implementation is also satisfactory in terms of memory footprint, compilation cost, and implementation complexity.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4307168806",
    "type": "article"
  },
  {
    "title": "Optimal register reassignment for register stack overflow minimization",
    "doi": "https://doi.org/10.1145/1132462.1132467",
    "publication_date": "2006-03-01",
    "publication_year": 2006,
    "authors": "Yoonseo Choi; Hwansoo Han",
    "corresponding_authors": "",
    "abstract": "Architectures with a register stack can implement efficient calling conventions. Using the overlapping of callers' and callees' registers, callers are able to pass parameters to callees without a memory stack. The most recent instance of a register stack can be found in the Intel Itanium architecture. A hardware component called the register stack engine (RSE) provides an illusion of an infinite-length register stack using a memory-backed process to handle overflow and underflow for a physically limited number of registers. Despite such hardware support, some applications suffer from the overhead required to handle register stack overflow and underflow. The memory latency associated with the overflow and underflow of a register stack can be reduced by generating multiple register allocation instructions within a procedure [Settle et al. 2003]. Live analysis is utilized to find a set of registers that are not required to keep their values across procedure boundaries. However, among those dead registers, only the registers that are consecutively located in a certain part of the register stack frame can be removed. We propose a compiler-supported register reassignment technique that reduces RSE overflow/underflow further. By reassigning registers based on live analysis, our technique forces as many dead registers to be removed as possible. We define the problem of optimal register reassignment, which minimizes interprocedural register stack heights considering multiple call sites within a procedure. We present how this problem is related to a path-finding problem in a graph called a sequence graph . We also propose an efficient heuristic algorithm for the problem. Finally, we present the measurement of effects of the proposed techniques on SPEC CINT2000 benchmark suite and the analysis of the results. The result shows that our approach reduces the RSE cycles by 6.4% and total cpu cycles by 1.7% on average.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2010867025",
    "type": "article"
  },
  {
    "title": "A low-power in-order/out-of-order issue queue",
    "doi": "https://doi.org/10.1145/1011528.1011530",
    "publication_date": "2004-06-01",
    "publication_year": 2004,
    "authors": "Yu Bai; R. Iris Bahar",
    "corresponding_authors": "",
    "abstract": "To better address power concerns, a good design strategy should be flexible enough to dynamically reconfigure available resources according to the application's needs such that extra power is dissipated only when it is really needed. In this work, we focus on power-aware solutions for the issue queue (IQ) in an out-of-order superscalar processor. We propose two schemes that partition the IQ into FIFOs such that only the instructions at the head of each FIFO may request to issue. We then monitor the processor and dynamically vary the number and/or size of FIFOs in accordance with utilization. Experimenting with two different distributions in power dissipation, we show up to 69% reduction in power dissipation in the wakeup and arbitration loop, while constraining performance degradation to be no more than 5%.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2025301757",
    "type": "article"
  },
  {
    "title": "Snug set-associative caches",
    "doi": "https://doi.org/10.1145/1216544.1216549",
    "publication_date": "2007-03-01",
    "publication_year": 2007,
    "authors": "Yuan‐Shin Hwang; Jia-Jhe Li",
    "corresponding_authors": "",
    "abstract": "As transistors keep shrinking and on-chip caches keep growing, static power dissipation resulting from leakage of caches takes an increasing fraction of total power in processors. Several techniques have already been proposed to reduce leakage power by turning off unused cache lines. However, they all have to pay the price of performance degradation. This paper presents a cache architecture, the snug set-associative ( SSA ) cache, that cuts most of static power dissipation of caches without incuring performance penalties. The SSA cache reduces leakage power by implementing the minimum set-associative scheme, which only activates the minimal numbers of ways in each cache set, while the performance losses caused by this scheme are compensated by the base-offset load/store queues . The rationale of combining these two techniques is locality: as the contents of the cache blocks in the current working set are repeatedly accessed, same addresses would be computed again and again. The SSA cache architecture can be applied to data and instruction caches to reduce leakage power without incurring performance penalties. Experimental results show that SSA can cut static power consumption of the L1 data cache by 93%, on average, for SPECint2000 benchmarks, while the execution times are reduced by 5%. Similarly, SSA can cut leakage dissipation of the L1 instruction cache by 92%, on average, and improve performance over 3%. Furthermore, when SSA is adopted for both L1 data and instruction caches, the normalized leakage of L1 data and instruction caches is lowered to 8%, on average, while still accomplishing a 2% reduction in execution times.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W1989931770",
    "type": "article"
  },
  {
    "title": "POWAR",
    "doi": "https://doi.org/10.1145/3293445",
    "publication_date": "2018-12-31",
    "publication_year": 2018,
    "authors": "Francisco J. Andújar; Salvador Coll; Marina Alonso; Pedro López; Juan Rubio",
    "corresponding_authors": "",
    "abstract": "In order to save energy in HPC interconnection networks, one usual proposal is to switch idle links into a low-power mode after a certain time without any transmission, as IEEE Energy Efficient Ethernet standard proposes. Extending the low-power mode mechanism, we propose POW er- A ware R outing ( POWAR ), a simple power-aware routing and selection function for fat-tree and torus networks. POWAR adapts the amount of network links that can be used, taking into account the network load, and obtaining great energy savings in the network (55%--65%) and the entire system (9%--10%) with negligible performance overhead.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2909537693",
    "type": "article"
  },
  {
    "title": "Blaze-Tasks",
    "doi": "https://doi.org/10.1145/3293448",
    "publication_date": "2018-12-31",
    "publication_year": 2018,
    "authors": "Peter Pirkelbauer; Amalee Wilson; Christina Peterson; Damian Dechev",
    "corresponding_authors": "",
    "abstract": "Compared to threads, tasks are a more fine-grained alternative. The task parallel programming model offers benefits in terms of better performance portability and better load-balancing for problems that exhibit nonuniform workloads. A common scenario of task parallel programming is that a task is recursively decomposed into smaller sub-tasks. Depending on the problem domain, the number of created sub-tasks may be nonuniform, thereby creating potential for significant load imbalances in the system. Dynamic load-balancing mechanisms will distribute the tasks across available threads. The final result of a computation may be modeled as a reduction over the results of all sub-tasks. This article describes a simple, yet effective prototype framework, Blaze-Tasks, for task scheduling and task reductions on shared memory architectures. The framework has been designed with lock-free techniques and generic programming principles in mind. Blaze-Tasks is implemented entirely in C++17 and is thus portable. To load-balance the computation, Blaze-Tasks uses task stealing. To manage contention on a task pool, the number of lock-free attempts to steal a task depends on the distance between thief and pool owner and the estimated number of tasks in a victim’s pool. This article evaluates the Blaze framework on Intel and IBM dual-socket systems using nine benchmarks and compares its performance with other task parallel frameworks. While Cilk outperforms Blaze on Intel on most benchmarks, the evaluation shows that Blaze is competitive with OpenMP and other library-based implementations. On IBM, the experiments show that Blaze outperforms other approaches on most benchmarks.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2910837860",
    "type": "article"
  },
  {
    "title": "Memory-Side Protection With a Capability Enforcement Co-Processor",
    "doi": "https://doi.org/10.1145/3302257",
    "publication_date": "2019-03-08",
    "publication_year": 2019,
    "authors": "Leonid Azriel; Lukas Humbel; Reto Achermann; Alexander Richardson; Moritz Hoffmann; Avi Mendelson; Timothy Roscoe; Robert N. M. Watson; Paolo Faraboschi; Dejan Milojičić",
    "corresponding_authors": "",
    "abstract": "Byte-addressable nonvolatile memory (NVM) blends the concepts of storage and memory and can radically improve data-centric applications, from in-memory databases to graph processing. By enabling large-capacity devices to be shared across multiple computing elements, fabric-attached NVM changes the nature of rack-scale systems and enables short-latency direct memory access while retaining data persistence properties and simplifying the software stack. An adequate protection scheme is paramount when addressing shared and persistent memory, but mechanisms that rely on virtual memory paging suffer from the tension between performance (pushing toward large pages) and protection granularity (pushing toward small pages). To address this tension, capabilities are worth revisiting as a more powerful protection mechanism, but the long time needed to introduce new CPU features hampers the adoption of schemes that rely on instruction-set architecture support. This article proposes the Capability Enforcement Co-Processor (CEP), a programmable memory controller that implements fine-grain protection through the capability model without requiring instruction-set support in the application CPU. CEP decouples capabilities from the application CPU instruction-set architecture, shortens time to adoption, and can rapidly evolve to embrace new persistent memory technologies, from NVDIMMs to native NVM devices, either locally connected or fabric attached in rack-scale configurations. CEP exposes an application interface based on memory handles that get internally converted to extended-pointer capabilities. This article presents a proof of concept implementation of a distributed object store (Redis) with CEP. It also demonstrates a capability-enhanced file system (FUSE) implementation using CEP. Our proof of concept shows that CEP provides fine-grain protection while enabling direct memory access from application clients to the NVM, and that by doing so opens up important performance optimization opportunities (up to 4× reduction in latency in comparison to software-based security enforcement) without compromising security. Finally, we also sketch how a future hybrid model could improve the initial implementation by delegating some CEP functionality to a CHERI-enabled processor.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2921238769",
    "type": "article"
  },
  {
    "title": "Accelerating In-Memory Database Selections Using Latency Masking Hardware Threads",
    "doi": "https://doi.org/10.1145/3310229",
    "publication_date": "2019-04-09",
    "publication_year": 2019,
    "authors": "Prerna Budhkar; Ildar Absalyamov; Vasileios Zois; Skyler Windh; Walid Najjar; Vassilis J. Tsotras",
    "corresponding_authors": "",
    "abstract": "Inexpensive DRAMs have created new opportunities for in-memory data analytics. However, the major bottleneck in such systems is high memory access latency. Traditionally, this problem is solved with large cache hierarchies that only benefit regular applications. Alternatively, many data-intensive applications exhibit irregular behavior. Hardware multithreading can better cope with high latency seen in such applications. This article implements a multithreaded prototype (MTP) on FPGAs for the relational selection operator that exhibits control flow irregularity. On a standard TPC-H query evaluation, MTP achieves a bandwidth utilization of 83%, while the CPU and the GPU implementations achieve 61% and 64%, respectively. Besides being bandwidth efficient, MTP is also 14.2× and 4.2× more power efficient than CPU and GPU, respectively.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2937781567",
    "type": "article"
  },
  {
    "title": "Polyhedral Compilation for Multi-dimensional Stream Processing",
    "doi": "https://doi.org/10.1145/3330999",
    "publication_date": "2019-07-18",
    "publication_year": 2019,
    "authors": "Jakob Leben; George Tzanetakis",
    "corresponding_authors": "",
    "abstract": "We present a method for compilation of multi-dimensional stream processing programs from affine recurrence equations with unbounded domains into imperative code with statically allocated memory. The method involves a novel polyhedral schedule transformation called periodic tiling. It accommodates existing polyhedral optimizations to improve memory access patterns and expose parallelism. This enables efficient execution of programming languages with unbounded recurrence equations, as well as optimization of existing languages from which this form can be derived. The method is experimentally evaluated on 5 DSP algorithms with large problem sizes. Results show potential for improved throughput compared to hand-optimized C++ (speedups on a 6-core Intel Xeon CPU up to 10× with a geometric mean 3.3×). 1",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2963523287",
    "type": "article"
  },
  {
    "title": "FPD <scp>etect</scp>",
    "doi": "https://doi.org/10.1145/3402451",
    "publication_date": "2020-08-17",
    "publication_year": 2020,
    "authors": "Arnab Das; Sriram Krishnamoorthy; Ian Briggs; Ganesh Gopalakrishnan; Ramakrishna Tipireddy",
    "corresponding_authors": "",
    "abstract": "We present FPD etect , a low-overhead approach for detecting logical errors and soft errors affecting stencil computations without generating false positives. We develop an offline analysis that tightly estimates the number of floating-point bits preserved across stencil applications. This estimate rigorously bounds the values expected in the data space of the computation. Violations of this bound can be attributed with certainty to errors. FPD etect helps synthesize error detectors customized for user-specified levels of accuracy and coverage. FPD etect also enables overhead reduction techniques based on deploying these detectors coarsely in space and time. Experimental evaluations demonstrate the practicality of our approach.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3077206599",
    "type": "article"
  },
  {
    "title": "<scp>ECO</scp> TLB",
    "doi": "https://doi.org/10.1145/3409454",
    "publication_date": "2020-09-30",
    "publication_year": 2020,
    "authors": "Steffen Maaß; Mohan Kumar Kumar; Taesoo Kim; Tushar Krishna; Abhishek Bhattacharjee",
    "corresponding_authors": "",
    "abstract": "We propose ecoTLB —software-based eventual translation lookaside buffer (TLB) coherence—which eliminates the overhead of the synchronous TLB shootdown mechanism in operating systems that use address space identifiers (ASIDs). With an eventual TLB coherence, ecoTLB improves the performance of free and page swap operations by removing the inter-processor interrupt (IPI) overheads incurred to invalidate TLB entries. We show that the TLB shootdown has implications for page swapping in particular in emerging, disaggregated data centers and demonstrate that ecoTLB can improve both the performance and the specific swapping policy decisions using ecoTLB ’s asynchronous mechanism. We demonstrate that ecoTLB improves the performance of real-world applications, such as Memcached and Make, that perform page swapping using Infiniswap , a solution for next generation data centers that use disaggregated memory, by up to 17.2%. Moreover, ecoTLB improves the 99th percentile tail latency of Memcached by up to 70.8% due to its asynchronous scheme and improved policy decisions. Furthermore, we show that recent features to improve security in the Linux kernel, like kernel page table isolation (KPTI), can result in significant performance overheads on architectures without support for specific instructions to clear single entries in tagged TLBs, falling back to full TLB flushes. In this scenario, ecoTLB is able to recover the performance lost for supporting KPTI due to its asynchronous shootdown scheme and its support for tagged TLBs. Finally, we demonstrate that ecoTLB improves the performance of free operations by up to 59.1% on a 120-core machine and improves the performance of Apache on a 16-core machine by up to 13.7% compared to baseline Linux, and by up to 48.2% compared to ABIS, a recent state-of-the-art research prototype that reduces the number of IPIs.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3091205405",
    "type": "article"
  },
  {
    "title": "A Distributed Hardware Monitoring System for Runtime Verification on Multi-Tile MPSoCs",
    "doi": "https://doi.org/10.1145/3430699",
    "publication_date": "2020-12-30",
    "publication_year": 2020,
    "authors": "Marcel Mettler; Daniel Mueller-Gritschneder; Ulf Schlichtmann",
    "corresponding_authors": "",
    "abstract": "Exhaustive verification techniques do not scale with the complexity of today’s multi-tile Multi-processor Systems-on-chip (MPSoCs). Hence, runtime verification (RV) has emerged as a complementary method, which verifies the correct behavior of applications executed on the MPSoC during runtime. In this article, we propose a decentralized monitoring architecture for large-scale multi-tile MPSoCs. In order to minimize performance and power overhead for RV, we propose a lightweight and non-intrusive hardware solution. It features a new specialized tracing interconnect that distributes and sorts detected events according to their timestamps. Each tile monitor has a consistent view on a globally sorted trace of events on which the behavior of the target application can be verified using logical and timing requirements. Furthermore, we propose an integer linear programming-based algorithm for the assignment of requirements to monitors to exploit the local resources best. The monitoring architecture is demonstrated for a four-tiled MPSoC with 20 cores implemented on a Virtex-7 field-programmable gate array (FPGA).",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3113850555",
    "type": "article"
  },
  {
    "title": "Irregular Register Allocation for Translation of Test-pattern Programs",
    "doi": "https://doi.org/10.1145/3427378",
    "publication_date": "2020-12-30",
    "publication_year": 2020,
    "authors": "Minsu Kim; Jeong-Keun Park; Soo‐Mook Moon",
    "corresponding_authors": "",
    "abstract": "Test-pattern programs are for testing DRAM memory chips. They run on a special embedded system called automated test equipment (ATE). Each ATE manufacturer provides its own programming language, which is mostly low level, thus accessing the registers in the ATE directly. The register structure of each ATE is quite different and highly irregular. Since DRAM chipmakers are often equipped with diverse ATEs from different manufacturers, they employ automatic translation of a program developed for one ATE to a program for different ATEs. This raises an irregular register allocation problem during translation. This article proposes a solution based on partitioned Boolean quadratic programming (PBQP). PBQP has been used for a number of compiler optimizations, including paired register allocation , which our ATE register allocation also requires. Moreover, the interleaved processing in ATE incurs complex register constraints, which we could also formulate elegantly with PBQP. The original PBQP solver is not quite appropriate to use, though, since ATE register allocation does not allow spills, so we devised a more elaborate PBQP solver that trades off the allocation time and allocation search space, to find a solution in a reasonable amount of time. Our experimental results with product-level pattern programs show that the proposed register allocator successfully finds valid solutions in all cases, in the order of tenths of seconds.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3116960032",
    "type": "article"
  },
  {
    "title": "Deadline-Constrained Clustered Scheduling for VLIW Architectures using Power-Gated Register Files",
    "doi": "https://doi.org/10.1145/2632218",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "Zhibin Liang; Wei Zhang; Yung-Cheng Ma",
    "corresponding_authors": "",
    "abstract": "Designing energy-efficient Digital Signal Processor (DSP) cores has become a key concern in embedded systems development. This paper proposes an energy-proportional computing scheme for Very Long Instruction Word (VLIW) architectures. To make the processor power scales with adapted parallelism, we propose incorporating distributed Power-Gated Register Files (PGRF) into VLIW to achieve a PGRF-VLIW architecture. For energy efficiency, we also propose an instruction scheduling algorithm called the Deadline-Constrained Clustered Scheduling (DCCS) algorithm. The algorithm clusters the data dependence graph to reduce data transfer energy and makes optimal use of low-powered local registers for tree-structured data dependence graphs. The results of evaluations conducted using the MiBench and DSPstone benchmark suites substantiate the expected power saving and scaling effects.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W1994185234",
    "type": "article"
  },
  {
    "title": "Idiom recognition framework using topological embedding",
    "doi": "https://doi.org/10.1145/2512431",
    "publication_date": "2013-09-16",
    "publication_year": 2013,
    "authors": "Motohiro Kawahito; Hideaki Komatsu; Takao Moriyama; Hiroshi Inoue; Toshio Nakatani",
    "corresponding_authors": "",
    "abstract": "Modern processors support hardware-assist instructions (such as TRT and TROT instructions on the IBM System z) to accelerate certain functions such as delimiter search and character conversion. Such special instructions are often used in high-performance libraries, but their exploitation in optimizing compilers has been limited. We devised a new idiom recognition technique based on a topological embedding algorithm to detect idiom patterns in the input programs more aggressively than in previous approaches using exact pattern matching. Our approach can detect a pattern even if the code segment does not exactly match the idiom. For example, we can detect a code segment that includes additional code within the idiom pattern. We also propose an instruction simplification for the idiom recognition. This optimization analyzes all of the usages of the output of the optimized code for a specific idiom. If we find that we do not need an actual value for the output but only a value in a subrange, then we can assign a value in that subrange as the output. The code generation can generate faster code with this optimization. We implemented our new idiom recognition approach based on the Java Just-In-Time (JIT) compiler that is part of the J9 Java Virtual Machine, and we supported several important idioms for the special hardware-assist instructions on the IBM System z and on some models of the IBM System p. To demonstrate the effectiveness of our technique, we performed two experiments. The first experiment was to see how many more patterns we can detect compared to the previous approach. The second experiment measured the performance improvements over the previous approaches. For the first experiment, we used the Java Compatibility Kit (JCK) API tests. For the second experiment we used the IBM XML parser, SPECjvm98, and SPCjbb2000. In summary, relative to a baseline implementation using exact pattern matching, our algorithm converted 76% more loops in JCK tests. On a z9, we also observed significant average performance improvement of the XML parser by 54%, of SPECjvm98 by 1.9%, and of SPECjbb2000 by 4.4%. Finally, we observed that the JIT compilation time increased by only 0.32% to 0.44%.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2050733629",
    "type": "article"
  },
  {
    "title": "VLIW coprocessor for IEEE-754 quadruple-precision elementary functions",
    "doi": "https://doi.org/10.1145/2512430",
    "publication_date": "2013-09-16",
    "publication_year": 2013,
    "authors": "Yuanwu Lei; Yong Dou; Lei Guo; Jinbo Xu; Jie Zhou; Yazhuo Dong; Hongjian Li",
    "corresponding_authors": "",
    "abstract": "In this article, a unified VLIW coprocessor, based on a common group of atomic operation units, for Quad arithmetic and elementary functions (QP_VELP) is presented. The explicitly parallel scheme of VLIW instruction and Estrin's evaluation scheme for polynomials are used to improve the performance. A two-level VLIW instruction RAM scheme is introduced to achieve high scalability and customizability, even for more complex key program kernels. Finally, the Quad arithmetic accelerator (QAA) with the QP_VELP array is implemented on ASIC. Compared with hyper-thread software implementation on an Intel Xeon E5620, QAA with 8 QP_VELP units achieves improvement by a factor of 18X.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2057154761",
    "type": "article"
  },
  {
    "title": "A decoupled local memory allocator",
    "doi": "https://doi.org/10.1145/2400682.2400693",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Boubacar Diouf; Can Hantaş; Albert Cohen; Özcan Öztürk; Jens Palsberg",
    "corresponding_authors": "",
    "abstract": "Compilers use software-controlled local memories to provide fast, predictable, and power-efficient access to critical data. We show that the local memory allocation for straight-line, or linearized programs is equivalent to a weighted interval-graph coloring problem. This problem is new when allowing a color interval to “wrap around,” and we call it the submarine-building problem. This graph-theoretical decision problem differs slightly from the classical ship-building problem, and exhibits very interesting and unusual complexity properties. We demonstrate that the submarine-building problem is NP-complete, while it is solvable in linear time for not-so-proper interval graphs, an extension of the the class of proper interval graphs. We propose a clustering heuristic to approximate any interval graph into a not-so-proper interval graph, decoupling spill code generation from local memory assignment. We apply this heuristic to a large number of randomly generated interval graphs reproducing the statistical features of standard local memory allocation benchmarks, comparing with state-of-the-art heuristics.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2064305759",
    "type": "article"
  },
  {
    "title": "MINIME-GPU",
    "doi": "https://doi.org/10.1145/2818693",
    "publication_date": "2015-11-16",
    "publication_year": 2015,
    "authors": "Etem Deniz; Alper Şen",
    "corresponding_authors": "",
    "abstract": "We introduce MINIME-GPU, a novel automated benchmark synthesis framework for graphics processing units (GPUs) that serves to speed up architectural simulation of modern GPU architectures. Our framework captures important characteristics of original GPU applications and generates synthetic GPU benchmarks using the Open Computing Language (OpenCL) library from those applications. To the best of our knowledge, this is the first time synthetic OpenCL benchmarks for GPUs are generated from existing applications. We use several characteristics, including instruction throughput, compute unit occupancy, and memory efficiency, to compare the similarity of original applications and their corresponding synthetic benchmarks. The experimental results show that our synthetic benchmark generation framework is capable of generating synthetic benchmarks that have similar characteristics with the original applications from which they are generated. On average, the similarity (accuracy) is 96% and the speedup is 541 ×. In addition, our synthetic benchmarks use the OpenCL library, which allows us to obtain portable human readable benchmarks as opposed to using assembly-level code, and they are faster and smaller than the original applications from which they are generated. We experimentally validated that our synthetic benchmarks preserve the characteristics of the original applications across different architectures.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2233606187",
    "type": "article"
  },
  {
    "title": "Section-Based Program Analysis to Reduce Overhead of Detecting Unsynchronized Thread Communication",
    "doi": "https://doi.org/10.1145/2766451",
    "publication_date": "2015-06-24",
    "publication_year": 2015,
    "authors": "Madan Das; Gabriel Southern; Jose Renau",
    "corresponding_authors": "",
    "abstract": "Most systems that test and verify parallel programs, such as deterministic execution engines, data race detectors, and software transactional memory systems, require instrumenting loads and stores in an application. This can cause a very significant runtime and memory overhead compared to executing uninstrumented code. Multithreaded programming typically allows any thread to perform loads and stores to any location in the process’s address space independently, and such tools monitor all these memory accesses. However, many of the addresses in these unsynchronized memory accesses are only used by a single thread and do not affect other executing threads. We propose Section-Based Program Analysis (SBPA), a novel way to decompose the program into disjoint code sections to identify and eliminate instrumenting such loads and stores during program compilation so that the program runtime overhead is significantly reduced. Our analysis includes improvements to pointer analysis and uses a few user directives to increase the effectiveness of SBPA further. We implemented SBPA for a deterministic execution runtime environment and were able to eliminate 51% of dynamic memory access instrumentations. When combined with directives, such reduction increased to 63%. We also integrated SBPA with ThreadSanitizer, a state-of-the-art dynamic race detector, and achieved a speedup of 2.43 (2.74 with directives) on a geometric mean basis.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2247879939",
    "type": "article"
  },
  {
    "title": "Adaptive Correction of Sampling Bias in Dynamic Call Graphs",
    "doi": "https://doi.org/10.1145/2840806",
    "publication_date": "2015-12-08",
    "publication_year": 2015,
    "authors": "Byeongcheol Lee",
    "corresponding_authors": "Byeongcheol Lee",
    "abstract": "This article introduces a practical low-overhead adaptive technique of correcting sampling bias in profiling dynamic call graphs. Timer-based sampling keeps the overhead low but sampling bias lowers the accuracy when either observable call events or sampling actions are not equally spaced in time. To mitigate sampling bias, our adaptive correction technique weights each sample by monitoring time-varying spacing of call events and sampling actions. We implemented and evaluated our adaptive correction technique in Jikes RVM, a high-performance virtual machine. In our empirical evaluation, our technique significantly improved the sampling accuracy without measurable overhead and resulted in effective feedback directed inlining.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2248141908",
    "type": "article"
  },
  {
    "title": "Two-Level Hybrid Sampled Simulation of Multithreaded Applications",
    "doi": "https://doi.org/10.1145/2818353",
    "publication_date": "2015-11-16",
    "publication_year": 2015,
    "authors": "Chuntao Jiang; Zhibin Yu; Lieven Eeckhout; Hai Jin; Xiaofei Liao; Chengzhong Xu",
    "corresponding_authors": "",
    "abstract": "Sampled microarchitectural simulation of single-threaded applications is mature technology for over a decade now. Sampling multithreaded applications, on the other hand, is much more complicated. Not until very recently have researchers proposed solutions for sampled simulation of multithreaded applications. Time-Based Sampling (TBS) samples multithreaded application execution based on time—not instructions as is typically done for single-threaded applications—yielding estimates for a multithreaded application’s execution time. In this article, we revisit and analyze previously proposed TBS approaches (periodic and cantor fractal based sampling), and we obtain a number of novel and surprising insights, such as (i) accurately estimating fast-forwarding IPC , that is, performance in-between sampling units, is more important than accurately estimating sample IPC , that is, performance within the sampling units; (ii) fast-forwarding IPC estimation accuracy is determined by both the sampling unit distribution and how to use the sampling units to predict fast-forwarding IPC; and (iii) cantor sampling is more accurate at small sampling unit sizes, whereas periodic is more accurate at large sampling unit sizes. These insights lead to the development of Two-level Hybrid Sampling (THS) , a novel sampling methodology for multithreaded applications that combines periodic sampling’s accuracy at large time scales (i.e., uniformly selecting coarse-grain sampling units across the entire program execution) with cantor sampling’s accuracy at small time scales (i.e., the ability to accurately predict fast-forwarding IPC in-between small sampling units). The clustered occurrence of small sampling units under cantor sampling also enables shortened warmup and thus enhanced simulation speed. Overall, THS achieves an average absolute execution time prediction error of 4% while yielding an average simulation speedup of 40 × compared to detailed simulation, which is both more accurate and faster than the current state-of-the-art. Case studies illustrate THS’ ability to accurately predict relative performance differences across the design space.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2250590078",
    "type": "article"
  },
  {
    "title": "FluidCheck",
    "doi": "https://doi.org/10.1145/2842620",
    "publication_date": "2015-12-22",
    "publication_year": 2015,
    "authors": "Rajshekar Kalayappan; Smruti R. Sarangi",
    "corresponding_authors": "",
    "abstract": "Soft errors have become a serious cause of concern with reducing feature sizes. The ability to accommodate complex, Simultaneous Multithreading (SMT) cores on a single chip presents a unique opportunity to achieve reliable execution, safe from soft errors, with low performance penalties. In this context, we present FluidCheck , a checker architecture that allows highly flexible assignment and migration of checking duties across cores. In this article, we present a mechanism to dynamically use the resources of SMT cores for checking the results of other threads, and propose a variety of heuristics for migration of such checker threads across cores. Secondly, to make the process of checking more efficient, we propose a set of architectural enhancements that reduce power consumption, decrease the length of the critical path, and reduce the load on the Network-on-Chip (NoC). Based on our observations, we design a 16 core system for running SPEC2006 based bag-of-tasks applications. Our experiments demonstrate that fully reliable execution can be attained with a mere 27% slowdown, surpassing traditional redundant threading based techniques by roughly 42%.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2251869787",
    "type": "article"
  },
  {
    "title": "Sensible Energy Accounting with Abstract Metering for Multicore Systems",
    "doi": "https://doi.org/10.1145/2842616",
    "publication_date": "2015-12-22",
    "publication_year": 2015,
    "authors": "Qixiao Liu; Miquel Moretó; Jaume Abella; Francisco J. Cazorla; Daniel A. Jiménez; Mateo Valero",
    "corresponding_authors": "",
    "abstract": "Chip multicore processors (CMPs) are the preferred processing platform across different domains such as data centers, real-time systems, and mobile devices. In all those domains, energy is arguably the most expensive resource in a computing system. Accurately quantifying energy usage in a multicore environment presents a challenge as well as an opportunity for optimization. Standard metering approaches are not capable of delivering consistent results with shared resources, since the same task with the same inputs may have different energy consumption based on the mix of co-running tasks. However, it is reasonable for data-center operators to charge on the basis of estimated energy usage rather than time since energy is more correlated with their actual cost. This article introduces the concept of Sensible Energy Accounting (SEA). For a task running in a multicore system, SEA accurately estimates the energy the task would have consumed running in isolation with a given fraction of the CMP shared resources. We explain the potential benefits of SEA in different domains and describe two hardware techniques to implement it for a shared last-level cache and on-core resources in SMT processors. Moreover, with SEA, an energy-aware scheduler can find a highly efficient on-chip resource assignment, reducing by up to 39% the total processor energy for a 4-core system.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2271098896",
    "type": "article"
  },
  {
    "title": "Dynamic Process Migration Based on Block Access Patterns Occurring in Storage Servers",
    "doi": "https://doi.org/10.1145/2899002",
    "publication_date": "2016-06-14",
    "publication_year": 2016,
    "authors": "Jianwei Liao; François Trahay; Guoqiang Xiao",
    "corresponding_authors": "",
    "abstract": "An emerging trend in developing large and complex applications on today’s high-performance computers is to couple independent components into a comprehensive application. The components may employ the global file system to exchange their data when executing the application. In order to reduce the time required for input/output (I/O) data exchange and data transfer in the coupled systems or other applications, this article proposes a dynamic process migration mechanism on the basis of block access pattern similarity for utilizing the local file cache to exchange the data. We first introduce the scheme of the block access counting diagram to profile the process access pattern during a time period on the storage server. Next, we propose an algorithm that compares the access patterns of processes running on different computing nodes. Last, processes are migrated in order to group processes with similar access patterns. Consequently, the processes on the computing node can exchange their data by accessing the local file cache, instead of the global file system. The experimental results show that the proposed process migration mechanism can reduce the execution time required by the application because of the shorter I/O time, as well as yield attractive I/O throughput. In summary, this dynamic process migration technique can work fairly well for distributed applications whose data dependency rely on distributed file systems.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2433572455",
    "type": "article"
  },
  {
    "title": "Energy-Proportional Photonic Interconnects",
    "doi": "https://doi.org/10.1145/3018110",
    "publication_date": "2016-12-28",
    "publication_year": 2016,
    "authors": "Yigit Demir; Nikos Hardavellas",
    "corresponding_authors": "",
    "abstract": "Photonic interconnects have emerged as the prime candidate technology for efficient networks on chip at future process nodes. However, the high optical loss of many nanophotonic components coupled with the low efficiency of current laser sources results in exceedingly high total power requirements for the laser. As optical interconnects stay on even during periods of system inactivity, most of this power is wasted, which has prompted research on laser gating. Unfortunately, prior work has been complicated by the long laser turn-on delays and has failed to deliver the full savings. In this article, we propose ProLaser, a laser control mechanism that monitors the requests sent on the interconnect, the cache, and the coherence directory to detect highly correlated events and turn on proactively the lasers of a photonic interconnect. While ProLaser requires fast lasers with a turn-on delay of a few nanoseconds, a technology that is still experimental, several types of such lasers that are suitable for power gating have already been manufactured over the last decade. Overall, ProLaser saves 42% to 85% of the laser power, outperforms the current state of the art by 2× on average, and closely tracks (within 2%--6%) a perfect prediction scheme with full knowledge of future interconnect requests. Moreover, the power savings of ProLaser allow the cores to exploit a higher-power budget and run faster, achieving speedups of 1.5 to 1.7× (1.6× on average).",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2561851412",
    "type": "article"
  },
  {
    "title": "Designing a Tunable Nested Data-Parallel Programming System",
    "doi": "https://doi.org/10.1145/3012011",
    "publication_date": "2016-12-28",
    "publication_year": 2016,
    "authors": "Saurav Muralidharan; Michael Garland; Albert Sidelnik; Mary Hall",
    "corresponding_authors": "",
    "abstract": "This article describes Surge, a nested data-parallel programming system designed to simplify the porting and tuning of parallel applications to multiple target architectures. Surge decouples high-level specification of computations, expressed using a C++ programming interface, from low-level implementation details using two first-class constructs: schedules and policies. Schedules describe the valid ways in which data-parallel operators may be implemented, while policies encapsulate a set of parameters that govern platform-specific code generation. These two mechanisms are used to implement a code generation system that analyzes computations and automatically generates a search space of valid platform-specific implementations. An input and architecture-adaptive autotuning system then explores this search space to find optimized implementations. We express in Surge five real-world benchmarks from domains such as machine learning and sparse linear algebra and from the high-level specifications, Surge automatically generates CPU and GPU implementations that perform on par with or better than manually optimized versions.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2564103638",
    "type": "article"
  },
  {
    "title": "Impact of Intrinsic Profiling Limitations on Effectiveness of Adaptive Optimizations",
    "doi": "https://doi.org/10.1145/3008661",
    "publication_date": "2016-12-12",
    "publication_year": 2016,
    "authors": "Michael R. Jantz; Forrest J. Robinson; Prasad A. Kulkarni",
    "corresponding_authors": "",
    "abstract": "Many performance optimizations rely on or are enhanced by runtime profile information. However, both offline and online profiling techniques suffer from intrinsic and practical limitations that affect the quality of delivered profile data. The quality of profile data is its ability to accurately predict (relevant aspects of) future program behavior. While these limitations are known, their impact on the effectiveness of profile-guided optimizations, compared to the ideal performance, is not as well understood. We define ideal performance for adaptive optimizations as that achieved with a precise profile of future program behavior. In this work, we study and quantify the performance impact of fundamental profiling limitations by comparing the effectiveness of typical adaptive optimizations when using the best profiles generated by offline and online schemes against a baseline where the adaptive optimization is given access to profile information about the future execution of the program. We model and compare the behavior of three adaptive JVM optimizations—heap memory management using object usage profiles, code cache management using method usage profiles, and selective just-in-time compilation using method hotness profiles—for the Java DaCapo benchmarks. Our results provide insight into the advantages and drawbacks of current profiling strategies and shed light on directions for future profiling research.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2564747820",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2695583",
    "publication_date": "2015-01-09",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The shift toward parallel processor architectures has made programming and code generation increasingly challenging. To address this programmability challenge, this article presents a technique to fully automatically generate efficient and readable code ...",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4231100657",
    "type": "paratext"
  },
  {
    "title": "Preallocation instruction scheduling with register pressure minimization using a combinatorial optimization approach",
    "doi": "https://doi.org/10.1145/2509420.2512432",
    "publication_date": "2013-09-01",
    "publication_year": 2013,
    "authors": "Ghassan Shobaki; Maxim Shawabkeh; Najm Eldeen Abu Rmaileh",
    "corresponding_authors": "",
    "abstract": "Balancing Instruction-Level Parallelism (ILP) and register pressure during preallocation instruction scheduling is a fundamentally important problem in code generation and optimization. The problem is known to be NP-complete. Many heuristic techniques have been proposed to solve this problem. However, due to the inherently conflicting requirements of maximizing ILP and minimizing register pressure, heuristic techniques may produce poor schedules in many cases. If such cases occur in hot code, significant performance degradation may result. A few combinatorial optimization approaches have also been proposed, but none of them has been shown to solve large real-world instances within reasonable time. This article presents the first combinatorial algorithm that is efficient enough to optimally solve large instances of this problem (basic blocks with hundreds of instructions) within a few seconds per instance. The proposed algorithm uses branch-and-bound enumeration with a number of powerful pruning techniques to efficiently search the solution space. The search is based on a cost function that incorporates schedule length and register pressure. An implementation of the proposed scheduling algorithm has been integrated into the LLVM Compiler and evaluated using SPEC CPU 2006. On x86-64, with a time limit of 10ms per instruction, it optimally schedules 79% of the hot basic blocks in FP2006. Another 19% of the blocks are not optimally scheduled but are improved in cost relative to LLVM's heuristic. This improves the execution time of some benchmarks by up to 21%, with a geometric-mean improvement of 2.4% across the entire benchmark suite. With the use of precise latency information, the geometric-mean improvement is increased to 2.8%.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4238764801",
    "type": "article"
  },
  {
    "title": "JIT technology with C/C++",
    "doi": "https://doi.org/10.1145/2555289.2555315",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Dorit Nuzman; Revital Eres; Sergei Dyshel; Marcel Zalmanovici; José G. Castaños",
    "corresponding_authors": "",
    "abstract": "The growing gap between the advanced capabilities of static compilers as reflected in benchmarking results and the actual performance that users experience in real-life scenarios makes client-side dynamic optimization technologies imperative to the domain of static languages. Dynamic optimization of software distributed in the form of a platform-agnostic Intermediate-Representation (IR) has been very successful in the domain of managed languages, greatly improving upon interpreted code, especially when online profiling is used. However, can such feedback-directed IR-based dynamic code generation be viable in the domain of statically compiled, rather than interpreted, languages? We show that fat binaries, which combine the IR together with the statically compiled executable, can provide a practical solution for software vendors, allowing their software to be dynamically optimized without the limitation of binary-level approaches, which lack the high-level IR of the program, and without the warm-up costs associated with the IR-only software distribution approach. We describe and evaluate the fat-binary-based runtime compilation approach using SPECint2006, demonstrating that the overheads it incurs are low enough to be successfully surmounted by dynamic optimization. Building on Java JIT technologies, our results already improve upon common real-world usage scenarios, including very small workloads.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4242919131",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2400682",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4245179399",
    "type": "paratext"
  },
  {
    "title": "Using machine learning to partition streaming programs",
    "doi": "https://doi.org/10.1145/2509420.2512436",
    "publication_date": "2013-09-01",
    "publication_year": 2013,
    "authors": "Zheng Wang; Michael O’Boyle",
    "corresponding_authors": "",
    "abstract": "Stream-based parallel languages are a popular way to express parallelism in modern applications. The efficient mapping of streaming parallelism to today's multicore systems is, however, highly dependent on the program and underlying architecture. We address this by developing a portable and automatic compiler-based approach to partitioning streaming programs using machine learning. Our technique predicts the ideal partition structure for a given streaming application using prior knowledge learned offline. Using the predictor we rapidly search the program space (without executing any code) to generate and select a good partition. We applied this technique to standard StreamIt applications and compared against existing approaches. On a 4-core platform, our approach achieves 60% of the best performance found by iteratively compiling and executing over 3000 different partitions per program. We obtain, on average, a 1.90× speedup over the already tuned partitioning scheme of the StreamIt compiler. When compared against a state-of-the-art analytical, model-based approach, we achieve, on average, a 1.77× performance improvement. By porting our approach to an 8-core platform, we are able to obtain 1.8× improvement over the StreamIt default scheme, demonstrating the portability of our approach.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4248754567",
    "type": "article"
  },
  {
    "title": "Beyond reuse distance analysis",
    "doi": "https://doi.org/10.1145/2555289.2555309",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Naznin Fauzia; Venmugil Elango; M. Ravishankar; J. Ramanujam; Fabrice Rastello; Atanas Rountev; Louis-Noël Pouchet; P. Sadayappan",
    "corresponding_authors": "",
    "abstract": "Emerging computer architectures will feature drastically decreased flops/byte (ratio of peak processing rate to memory bandwidth) as highlighted by recent studies on Exascale architectural trends. Further, flops are getting cheaper, while the energy cost of data movement is increasingly dominant. The understanding and characterization of data locality properties of computations is critical in order to guide efforts to enhance data locality.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4254588531",
    "type": "article"
  },
  {
    "title": "Introduction to the special issue on high-performance and embedded architectures and compilers",
    "doi": "https://doi.org/10.1145/2086696.2086697",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Per Stenström; Koen De Bosschere",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2027996797",
    "type": "article"
  },
  {
    "title": "Token tenure and PATCH",
    "doi": "https://doi.org/10.1145/1839667.1839668",
    "publication_date": "2010-09-01",
    "publication_year": 2010,
    "authors": "Arun Raghavan; Colin Blundell; Milo M. K. Martin",
    "corresponding_authors": "",
    "abstract": "Traditional coherence protocols present a set of difficult trade-offs: the reliance of snoopy protocols on broadcast and ordered interconnects limits their scalability, while directory protocols incur a performance penalty on sharing misses due to indirection. This work introduces Patch (Predictive/Adaptive Token-Counting Hybrid), a coherence protocol that provides the scalability of directory protocols while opportunistically sending direct requests to reduce sharing latency. Patch extends a standard directory protocol to track tokens and use token-counting rules for enforcing coherence permissions. Token counting allows Patch to support direct requests on an unordered interconnect, while a mechanism called token tenure provides broadcast-free forward progress using the directory protocol's per-block point of ordering at the home along with either timeouts at requesters or explicit race notification messages. Patch makes three main contributions. First, Patch introduces token tenure, which provides broadcast-free forward progress for token-counting protocols. Second, Patch deprioritizes best-effort direct requests to match or exceed the performance of directory protocols without restricting scalability. Finally, Patch provides greater scalability than directory protocols when using inexact encodings of sharers because only processors holding tokens need to acknowledge requests. Overall, Patch is a “one-size-fits-all” coherence protocol that dynamically adapts to work well for small systems, large systems, and anywhere in between.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2161276540",
    "type": "article"
  },
  {
    "title": "A case for a complexity-effective, width-partitioned microarchitecture",
    "doi": "https://doi.org/10.1145/1162690.1162693",
    "publication_date": "2006-09-01",
    "publication_year": 2006,
    "authors": "Olivier Rochecouste; Gilles Pokam; André Seznec",
    "corresponding_authors": "",
    "abstract": "The analysis of program executions reveals that most integer and multimedia applications make heavy use of narrow-width operations, i.e., instructions exclusively using narrow-width operands and producing a narrow-width result. Moreover, this usage is relatively well distributed over the application. We observed this program property on the MediaBench and SPEC2000 benchmarks with about 40% of the instructions being narrow-width operations. Current superscalar processors use 64-bit datapaths to execute all the instructions of the applications. In this paper, we suggest the use of a width-partitioned microarchitecture (WPM) to master the hardware complexity of a superscalar processor. For a four-way issue machine, we split the processor in two two-way clusters: the main cluster executing 64-bit operations, load/store, and complex operations and a narrow cluster executing the 16-bit operations. We resort to partitioning to decouple the treatment of the narrow-width operations from that of the other program instructions. This provides the benefit of greatly simplifying the design of the critical processor components in each cluster (e.g., the register file and the bypass network). The dynamic interleaving of the two instruction types allows maintaining the workload balanced among clusters. WPM also helps to reduce the complexity of the interconnection fabric and of the issue logic. In fact, since the 16-bit cluster can only communicate narrow-width data, the datapath-width of the interconnect fabric can be significantly reduced, yielding a corresponding saving of the interconnect power and area. We explore different possible configurations of WPM, discussing the various implementation tradeoffs. We also examine a speculative steering heuristic to distribute the narrow-width operations among clusters. A detailed analysis of the complexity factors shows using WPM instead of a classical 64-bit two-cluster microarchitecture can save power and silicon area with a minimal impact on the overall performance.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W1999593226",
    "type": "article"
  },
  {
    "title": "Dynamic memory interval test vs. interprocedural pointer analysis in multimedia applications",
    "doi": "https://doi.org/10.1145/1071604.1071608",
    "publication_date": "2005-06-01",
    "publication_year": 2005,
    "authors": "Esther Salamí; Mateo Valero",
    "corresponding_authors": "",
    "abstract": "Techniques to detect aliasing between access patterns of array elements are quite effective for many numeric applications. However, although multimedia codes usually follow very regular memory access patterns, current commercial compilers remain unsuccessful in disambiguating them due mainly to complex pointer references. The Dynamic Memory Interval Test is a runtime memory disambiguation technique that takes advantage of the specific behavior of multimedia memory access patterns. It evaluates whether or not the full loop is disambiguated by analyzing the region domain of each load or store before each invocation of the loop.This paper provides a detailed evaluation of the approach, compares it against an advanced interprocedural pointer analysis framework, and analyzes the possibility of using both techniques at the same time. Both techniques achieve similar speedups separately (1.25X in average for a 8-issue width architecture). Furthermore, they can be used together to improve performance (reaching an average speed-up of 1.32X). Results also confirm that memory disambiguation is a key optimization to exploit the available parallelism in multimedia codes, especially for wide-issue architectures (1.50X average speed-up when scaling from 4- to 12-issue width in contrast to a low 1.10X for the baseline compiler).",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2048742489",
    "type": "article"
  },
  {
    "title": "Block-aware instruction set architecture",
    "doi": "https://doi.org/10.1145/1162690.1162694",
    "publication_date": "2006-09-01",
    "publication_year": 2006,
    "authors": "Ahmad Zmily; Christos Kozyrakis",
    "corresponding_authors": "",
    "abstract": "Instruction delivery is a critical component for wide-issue, high-frequency processors since its bandwidth and accuracy place an upper limit on performance. The processor front-end accuracy and bandwidth are limited by instruction-cache misses, multicycle instruction-cache accesses, and target or direction mispredictions for control-flow operations. This paper presents a block-aware instruction set (BLISS) that allows software to assist with front-end challenges. BLISS defines basic block descriptors that are stored separately from the actual instructions in a program. We show that BLISS allows for a decoupled front-end that tolerates instruction-cache latency, facilitates instruction prefetching, and leads to higher prediction accuracy.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2171528331",
    "type": "article"
  },
  {
    "title": "Systems-on-Chip with Strong Ordering",
    "doi": "https://doi.org/10.1145/3428153",
    "publication_date": "2021-01-20",
    "publication_year": 2021,
    "authors": "Sooraj Puthoor; Mikko H. Lipasti",
    "corresponding_authors": "",
    "abstract": "Sequential consistency (SC) is the most intuitive memory consistency model and the easiest for programmers and hardware designers to reason about. However, the strict memory ordering restrictions imposed by SC make it less attractive from a performance standpoint. Additionally, prior high-performance SC implementations required complex hardware structures to support speculation and recovery. In this article, we introduce the lockstep SC consistency model (LSC), a new memory model based on SC but carefully defined to accommodate the data parallel lockstep execution paradigm of GPUs. We also describe an efficient LSC implementation for an APU system-on-chip (SoC) and show that our implementation performs close to the baseline relaxed model. Evaluation of our implementation shows that the geometric mean performance cost for lockstep SC is just 0.76% for GPU execution and 6.11% for the entire APU SoC compared to a baseline with a weaker memory consistency model. Adoption of LSC in future APU and SoC designs will reduce the burden on programmers trying to write correct parallel programs, while also simplifying the implementation and verification of systems with heterogeneous processing elements and complex memory hierarchies. 1",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3124482667",
    "type": "article"
  },
  {
    "title": "On Predictable Reconfigurable System Design",
    "doi": "https://doi.org/10.1145/3436995",
    "publication_date": "2021-02-09",
    "publication_year": 2021,
    "authors": "Nils Voss; Bastiaan Kwaadgras; Oskar Mencer; Wayne Luk; Georgi Gaydadjiev",
    "corresponding_authors": "",
    "abstract": "We propose a design methodology to facilitate rigorous development of complex applications targeting reconfigurable hardware. Our methodology relies on analytical estimation of system performance and area utilisation for a given specific application and a particular system instance consisting of a controlflow machine working in conjunction with one or more reconfigurable dataflow accelerators. The targeted application is carefully analyzed, and the parts identified for hardware acceleration are reimplemented as a set of representative software models. Next, with the results of the application analysis, a suitable system architecture is devised and its performance is evaluated to determine bottlenecks, allowing predictable design. The architecture is iteratively refined, until the final version satisfying the specification requirements in terms of performance and required hardware area is obtained. We validate the presented methodology using a widely accepted convolutional neural network (VGG-16) and an important HPC application (BQCD). In both cases, our methodology relieved and alleviated all system bottlenecks before the hardware implementation was started. As a result the architectures were implemented first time right, achieving state-of-the-art performance within 15% of our modelling estimations.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3127745999",
    "type": "article"
  },
  {
    "title": "Cryptographic Software IP Protection without Compromising Performance or Timing Side-channel Leakage",
    "doi": "https://doi.org/10.1145/3443707",
    "publication_date": "2021-02-09",
    "publication_year": 2021,
    "authors": "Arnab Kumar Biswas",
    "corresponding_authors": "Arnab Kumar Biswas",
    "abstract": "Program obfuscation is a widely used cryptographic software intellectual property (IP) protection technique against reverse engineering attacks in embedded systems. However, very few works have studied the impact of combining various obfuscation techniques on the obscurity (difficulty of reverse engineering) and performance (execution time) of obfuscated programs. In this article, we propose a Genetic Algorithm (GA)-based framework that not only optimizes obscurity and performance of obfuscated cryptographic programs, but it also ensures very low timing side-channel leakage. Our proposed T iming S ide C hannel S ensitive P rogram O bfuscation O ptimization F ramework (TSC-SPOOF) determines the combination of obfuscation transformation functions that produce optimized obfuscated programs with preferred optimization parameters. In particular, TSC-SPOOF employs normalized compression distance (NCD) and channel capacity to measure obscurity and timing side-channel leakage, respectively. We also use RISC-V rocket core running on a Xilinx Zynq FPGA device as part of our framework to obtain realistic results. The experimental results clearly show that our proposed solution leads to cryptographic programs with lower execution time, higher obscurity, and lower timing side-channel leakage than unguided obfuscation.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3128977233",
    "type": "article"
  },
  {
    "title": "Automatic Sublining for Efficient Sparse Memory Accesses",
    "doi": "https://doi.org/10.1145/3452141",
    "publication_date": "2021-05-10",
    "publication_year": 2021,
    "authors": "Wim Heirman; Stijn Eyerman; Kristof Du Bois; Ibrahim Hur",
    "corresponding_authors": "",
    "abstract": "Sparse memory accesses, which are scattered accesses to single elements of a large data structure, are a challenge for current processor architectures. Their lack of spatial and temporal locality and their irregularity makes caches and traditional stream prefetchers useless. Furthermore, performing standard caching and prefetching on sparse accesses wastes precious memory bandwidth and thrashes caches, deteriorating performance for regular accesses. Bypassing prefetchers and caches for sparse accesses, and fetching only a single element (e.g., 8 B) from main memory (subline access), can solve these issues. Deciding which accesses to handle as sparse accesses and which as regular cached accesses, is a challenging task, with a large potential impact on performance. Not only is performance reduced by treating sparse accesses as regular accesses, not caching accesses that do have locality also negatively impacts performance by significantly increasing their latency and bandwidth consumption. Furthermore, this decision depends on the dynamic environment, such as input set characteristics and system load, making a static decision by the programmer or compiler suboptimal. We propose the Instruction Spatial Locality Estimator ( ISLE ), a hardware detector that finds instructions that access isolated words in a sea of unused data. These sparse accesses are dynamically converted into uncached subline accesses, while keeping regular accesses cached. ISLE does not require modifying source code or binaries, and adapts automatically to a changing environment (input data, available bandwidth, etc.). We apply ISLE to a graph analytics processor running sparse graph workloads, and show that ISLE outperforms the performance of no subline accesses, manual sublining, and prior work on detecting sparse accesses.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3163331881",
    "type": "article"
  },
  {
    "title": "Early Address Prediction",
    "doi": "https://doi.org/10.1145/3458883",
    "publication_date": "2021-06-08",
    "publication_year": 2021,
    "authors": "Ricardo N. Alves; Stefanos Kaxiras; David Black-Schaffer",
    "corresponding_authors": "",
    "abstract": "Achieving low load-to-use latency with low energy and storage overheads is critical for performance. Existing techniques either prefetch into the pipeline (via address prediction and validation) or provide data reuse in the pipeline (via register sharing or L0 caches). These techniques provide a range of tradeoffs between latency, reuse, and overhead. In this work, we present a pipeline prefetching technique that achieves state-of-the-art performance and data reuse without additional data storage, data movement, or validation overheads by adding address tags to the register file. Our addition of register file tags allows us to forward (reuse) load data from the register file with no additional data movement, keep the data alive in the register file beyond the instruction’s lifetime to increase temporal reuse, and coalesce prefetch requests to achieve spatial reuse. Further, we show that we can use the existing memory order violation detection hardware to validate prefetches and data forwards without additional overhead. Our design achieves the performance of existing pipeline prefetching while also forwarding 32% of the loads from the register file (compared to 15% in state-of-the-art register sharing), delivering a 16% reduction in L1 dynamic energy (1.6% total processor energy), with an area overhead of less than 0.5%.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3173074201",
    "type": "article"
  },
  {
    "title": "Scenario-Aware Program Specialization for Timing Predictability",
    "doi": "https://doi.org/10.1145/3473333",
    "publication_date": "2021-09-03",
    "publication_year": 2021,
    "authors": "Joscha Benz; Oliver Bringmann",
    "corresponding_authors": "",
    "abstract": "The successful application of static program analysis strongly depends on flow facts of a program such as loop bounds, control-flow constraints, and operating modes. This problem heavily affects the design of real-time systems, since static program analyses are a prerequisite to determine the timing behavior of a program. For example, this becomes obvious in worst-case execution time (WCET) analysis, which is often infeasible without user-annotated flow facts. Moreover, many timing simulation approaches use statically derived timings of partial program paths to reduce simulation overhead. Annotating flow facts on binary or source level is either error-prone and tedious, or requires specialized compilers that can transform source-level annotations along with the program during optimization. To overcome these obstacles, so-called scenarios can be used. Scenarios are a design-time methodology that describe a set of possible system parameters, such as image resolutions, operating modes, or application-dependent flow facts. The information described by a scenario is unknown in general but known and constant for a specific system. In this article, 1 we present a methodology for scenario-aware program specialization to improve timing predictability. Moreover, we provide an implementation of this methodology for embedded software written in C/C++. We show the effectiveness of our approach by evaluating its impact on WCET analysis using almost all of TACLeBench–achieving an average reduction of WCET of 31%. In addition, we provide a thorough qualitative and evaluation-based comparison to closely related work, as well as two case studies.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3197996019",
    "type": "article"
  },
  {
    "title": "TLB-pilot: Mitigating TLB Contention Attack on GPUs with Microarchitecture-Aware Scheduling",
    "doi": "https://doi.org/10.1145/3491218",
    "publication_date": "2021-12-06",
    "publication_year": 2021,
    "authors": "Bang Di; Daokun Hu; Zhen Xie; Jianhua Sun; Hao Chen; Jinkui Ren; Dong Li",
    "corresponding_authors": "",
    "abstract": "Co-running GPU kernels on a single GPU can provide high system throughput and improve hardware utilization, but this raises concerns on application security. We reveal that translation lookaside buffer (TLB) attack, one of the common attacks on CPU, can happen on GPU when multiple GPU kernels co-run. We investigate conditions or principles under which a TLB attack can take effect, including the awareness of GPU TLB microarchitecture, being lightweight, and bypassing existing software and hardware mechanisms. This TLB-based attack can be leveraged to conduct Denial-of-Service (or Degradation-of-Service) attacks. Furthermore, we propose a solution to mitigate TLB attacks. In particular, based on the microarchitecture properties of GPU, we introduce a software-based system, TLB-pilot, that binds thread blocks of different kernels to different groups of streaming multiprocessors by considering hardware isolation of last-level TLBs and the application’s resource requirement. TLB-pilot employs lightweight online profiling to collect kernel information before kernel launches. By coordinating software- and hardware-based scheduling and employing a kernel splitting scheme to reduce load imbalance, TLB-pilot effectively mitigates TLB attacks. The result shows that when under TLB attack, TLB-pilot mitigates the attack and provides on average 56.2% and 60.6% improvement in average normalized turnaround times and overall system throughput, respectively, compared to the traditional Multi-Process Service based co-running solution. When under TLB attack, TLB-pilot also provides up to 47.3% and 64.3% improvement (41% and 42.9% on average) in average normalized turnaround times and overall system throughput, respectively, compared to a state-of-the-art co-running solution for efficiently scheduling of thread blocks.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4200165187",
    "type": "article"
  },
  {
    "title": "Design and optimization of the store vectors memory dependence predictor",
    "doi": "https://doi.org/10.1145/1596510.1596514",
    "publication_date": "2009-10-01",
    "publication_year": 2009,
    "authors": "Samantika Subramaniam; Gabriel H. Loh",
    "corresponding_authors": "",
    "abstract": "Allowing loads that do not violate memory ordering to issue out of order with respect to earlier unresolved store addresses is very important for extracting parallelism in large-window superscalar processors. Previous research has proposed memory dependence prediction algorithms to prevent only loads with true memory dependencies from issuing in the presence of unresolved stores. Techniques such as load-store pair identification and store sets have been very successful in achieving performance levels close to that attained by an oracle-dependence predictor, but have relatively complex or power-hungry designs. In this article, we use the idea of dependency vectors from matrix schedulers for nonmemory instructions and adapt them to implement a new dependence prediction algorithm. We show that for conservatively sized processors, a simple PC-indexed table that tracks misordered loads is sufficient to provide most of the performance benefits achieved by more sophisticated predictors. On more aggressive processor configurations, however, our “Store Vector” algorithm provides better performance than the state-of-the-art store sets predictor while maintaining a simpler and more scalable design.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W1991871877",
    "type": "article"
  },
  {
    "title": "Formulating and implementing profiling over adaptive ranges",
    "doi": "https://doi.org/10.1145/1369396.1369398",
    "publication_date": "2008-05-01",
    "publication_year": 2008,
    "authors": "Shashidhar Mysore; Banit Agrawal; Rodolfo Neuber; Timothy Sherwood; Nisheeth Shrivastava; Subhash Suri",
    "corresponding_authors": "",
    "abstract": "Modern computer systems are called on to deal with billions of events every second, whether they are executed instructions, accessed memory locations, or forwarded packets. This presents a serious challenge to those who seek to quantify, analyze, or optimize such systems, because important trends and behaviors may easily be lost in a sea of data. We present range-adaptive profiling (RAP) as a new and general-purpose profiling method capable of hierarchically efficiently classifying streams of data in hardware. Through the use of RAP, events in an input stream are dynamically classified into increasingly precise categories, based on the frequency with which they occur. The more important a class, or range of events, the more precisely it is quantified. Despite the dynamic nature of our technique, we build upon tight theoretic bounds covering both worst-case error, as well as the required memory. In the limit, it is known that error and the memory bounds can be independent of the stream size and grow only linearly with the level of precision desired. Significantly, we expose the critical constants in these algorithms and through careful engineering, algorithm redesign, and use of heuristics, we show how a high-performance profile system can be implemented for range-adaptive profiling. RAP can be used on various profiles, such as PCs, load values, and memory addresses, and has a broad range of uses, from hot-region profiling to quantifying cache miss value locality. We propose two methods of implementation of RAP, one in software and the other with specialized hardware, for which we also describe our prototype FPGA implementation. We show that with just 8KB of memory, range profiles can be gathered with an average accuracy of 98%.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2030436480",
    "type": "article"
  },
  {
    "title": "Scratchpad Sharing in GPUs",
    "doi": "https://doi.org/10.1145/3075619",
    "publication_date": "2017-05-26",
    "publication_year": 2017,
    "authors": "Vishwesh Jatala; Jayvant Anantpur; Amey Karkare",
    "corresponding_authors": "",
    "abstract": "General-Purpose Graphics Processing Unit (GPGPU) applications exploit on-chip scratchpad memory available in the Graphics Processing Units (GPUs) to improve performance. The amount of thread level parallelism (TLP) present in the GPU is limited by the number of resident threads, which in turn depends on the availability of scratchpad memory in its streaming multiprocessor (SM). Since the scratchpad memory is allocated at thread block granularity, part of the memory may remain unutilized. In this article, we propose architectural and compiler optimizations to improve the scratchpad memory utilization. Our approach, called Scratchpad Sharing , addresses scratchpad under-utilization by launching additional thread blocks in each SM. These thread blocks use unutilized scratchpad memory and also share scratchpad memory with other resident blocks. To improve the performance of scratchpad sharing, we propose Owner Warp First (OWF) scheduling that schedules warps from the additional thread blocks effectively. The performance of this approach, however, is limited by the availability of the part of scratchpad memory that is shared among thread blocks. We propose compiler optimizations to improve the availability of shared scratchpad memory. We describe an allocation scheme that helps in allocating scratchpad variables such that shared scratchpad is accessed for short duration. We introduce a new hardware instruction, relssp , that when executed releases the shared scratchpad memory. Finally, we describe an analysis for optimal placement of relssp instructions, such that shared scratchpad memory is released as early as possible, but only after its last use, along every execution path. We implemented the hardware changes required for scratchpad sharing and the relssp instruction using the GPGPU-Sim simulator and implemented the compiler optimizations in Ocelot framework. We evaluated the effectiveness of our approach on 19 kernels from 3 benchmarks suites: CUDA-SDK, GPGPU-Sim, and Rodinia. The kernels that under-utilize scratchpad memory show an average improvement of 19% and maximum improvement of 92.17% in terms of the number of instruction executed per cycle when compared to the baseline approach, without affecting the performance of the kernels that are not limited by scratchpad memory.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2460667093",
    "type": "article"
  },
  {
    "title": "Dirty-Block Tracking in a Direct-Mapped DRAM Cache with Self-Balancing Dispatch",
    "doi": "https://doi.org/10.1145/3068460",
    "publication_date": "2017-05-10",
    "publication_year": 2017,
    "authors": "Dongwoo Lee; Sang‐Heon Lee; Soojung Ryu; Ki‐Young Choi",
    "corresponding_authors": "",
    "abstract": "Recently, processors have begun integrating 3D stacked DRAMs with the cores on the same package, and there have been several approaches to effectively utilizing the on-package DRAMs as caches. This article presents an approach that combines the previous approaches in a synergistic way by devising a module called the dirty-block tracker to maintain the dirtiness of each block in a dirty region. The approach avoids unnecessary tag checking for a write operation if the corresponding block in the cache is not dirty. Our simulation results show that the proposed technique achieves a 10.3% performance improvement on average over the state-of-the-art DRAM cache technique.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2613345006",
    "type": "article"
  },
  {
    "title": "Improving the Efficiency of GPGPU Work-Queue Through Data Awareness",
    "doi": "https://doi.org/10.1145/3151035",
    "publication_date": "2017-12-05",
    "publication_year": 2017,
    "authors": "Libo Huang; Yashuai Lü; Li Shen; Zhiying Wang",
    "corresponding_authors": "",
    "abstract": "The architecture and programming model of current GPGPUs are best suited for applications that are dominated by structured control and data flows across large regular datasets. Parallel workloads with irregular control and data structures cannot easily harness the processing power of the GPGPU. One approach for mapping these irregular-parallel workloads to GPGPUs is using work-queues. The work-queue approach improves the utilization of SIMD units by only processing useful works that are dynamically generated during execution. As current GPGPUs lack necessary supports for work-queues, a software-based work-queue implementation often suffers from memory contention and load balancing issues. In this article, we present a novel hardware work-queue design named DaQueue , which incorporates three data-aware features to improve the efficiency of work-queues on GPGPUs. We evaluate our proposal on the irregular-parallel workloads and carry out a case study on a path tracing pipeline with a cycle-level simulator. Experimental results show that for the tested workloads, DaQueue improves performance by 1.53× on average and up to 1.91×. Compared to a hardware worklist approach that is the state-of-the-art prior work, DaQueue can achieve an average of 33.92% extra speedup with less hardware area cost.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2774112409",
    "type": "article"
  },
  {
    "title": "Extreme-Scale High-Order WENO Simulations of 3-D Detonation Wave with 10 Million Cores",
    "doi": "https://doi.org/10.1145/3209208",
    "publication_date": "2018-06-12",
    "publication_year": 2018,
    "authors": "Ying Cai; Yulong Ao; Chao Yang; Wenjing Ma; Haitao Zhao",
    "corresponding_authors": "",
    "abstract": "High-order stencil computations, frequently found in many applications, pose severe challenges to emerging many-core platforms due to the complexities of hardware architectures as well as the sophisticated computing and data movement patterns. In this article, we tackle the challenges of high-order WENO computations in extreme-scale simulations of 3D gaseous waves on Sunway TaihuLight. We design efficient parallelization algorithms and present effective optimization techniques to fully exploit various parallelisms with reduced memory footprints, enhanced data reuse, and balanced computation load. Test results show the optimized code can scale to 9.98 million cores, solving 12.74 trillion unknowns with 23.12 Pflops double-precision performance.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2808318845",
    "type": "article"
  },
  {
    "title": "Exposing Memory Access Patterns to Improve Instruction and Memory Efficiency in GPUs",
    "doi": "https://doi.org/10.1145/3280851",
    "publication_date": "2018-10-29",
    "publication_year": 2018,
    "authors": "Neal Crago; Mark W. Stephenson; Stephen W. Keckler",
    "corresponding_authors": "",
    "abstract": "Modern computing workloads often have high memory intensity, requiring high bandwidth access to memory. The memory request patterns of these workloads vary and include regular strided accesses and indirect (pointer-based) accesses. Such applications require a large number of address generation instructions and a high degree of memory-level parallelism. This article proposes new memory instructions that exploit strided and indirect memory request patterns and improve efficiency in GPU architectures. The new instructions reduce address calculation instructions by offloading addressing to dedicated hardware, and reduce destructive memory request interference by grouping related requests together. Our results show that we can eliminate 33% of dynamic instructions across 16 GPU benchmarks. These improvements result in an overall runtime improvement of 26%, an energy reduction of 18%, and a reduction in energy-delay product of 32%.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2899396210",
    "type": "article"
  },
  {
    "title": "Processor-Tracing Guided Region Formation in Dynamic Binary Translation",
    "doi": "https://doi.org/10.1145/3281664",
    "publication_date": "2018-11-16",
    "publication_year": 2018,
    "authors": "Ding‐Yong Hong; Jan‐Jan Wu; Yuping Liu; Sheng‐Yu Fu; Wei‐Chung Hsu",
    "corresponding_authors": "",
    "abstract": "Region formation is an important step in dynamic binary translation to select hot code regions for translation and optimization. The quality of the formed regions determines the extent of optimizations and thus determines the final execution performance. Moreover, the overall performance is very sensitive to the formation overhead, because region formation can have a non-trivial cost. For addressing the dual issues of region quality and region formation overhead, this article presents a lightweight region formation method guided by processor tracing, e.g., Intel PT. We leverage the branch history information stored in the processor to reconstruct the program execution profile and effectively form high-quality regions with low cost. Furthermore, we present the designs of lightweight hardware performance monitoring sampling and the branch instruction decode cache to minimize region formation overhead. Using ARM64 to x86-64 translations, the experiment results show that our method achieves a performance speedup of up to 1.53× (1.16× on average) for SPEC CPU2006 benchmarks with reference inputs, compared to the well-known software-based trace formation method, Next Executing Tail (NET). The performance results of x86-64 to ARM64 translations also show a speedup of up to 1.25× over NET for CINT2006 benchmarks with reference inputs. The comparison with a relaxed NETPlus region formation method further demonstrates that our method achieves the best performance and lowest compilation overhead.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2901146625",
    "type": "article"
  },
  {
    "title": "Code reordering on limited branch offset",
    "doi": "https://doi.org/10.1145/1250727.1250730",
    "publication_date": "2007-06-01",
    "publication_year": 2007,
    "authors": "Yu Chen; Fuxin Zhang",
    "corresponding_authors": "",
    "abstract": "Since the 1980's code reordering has gained popularity as an important way to improve the spatial locality of programs. While the effect of the processor's microarchitecture and memory hierarchy on this optimization technique has been investigated, little research has focused on the impact of the instruction set. In this paper, we analyze the effect of limited branch offset of the MIPS-like instruction set [Hwu et al. 2004, 2005] on code reordering, explore two simple methods to handle the exceeded branches, and propose the bidirectional code layout (BCL) algorithm to reduce the number of branches exceeding the offset limit. The BCL algorithm sorts the chains according to the position of related chains, avoids cache conflict misses deliberately and lays out the code bidirectionally. It strikes a balance among the distance of related blocks, the instruction cache miss rate, the memory size required, and the control flow transfer. Experimental results show that BCL can effectively reduce exceeded branches by 50.1%, on average, with up to 100% for some programs. Except for some programs with little spatial locality, the BCL algorithm can achieve the performance, as the case with no branch offset limitation.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W1975426082",
    "type": "article"
  },
  {
    "title": "Morphable DRAM Cache Design for Hybrid Memory Systems",
    "doi": "https://doi.org/10.1145/3338505",
    "publication_date": "2019-07-18",
    "publication_year": 2019,
    "authors": "Sang-Hoon Cha; Bo-Kyeong Kim; Chang Hyun Park; Jaehyuk Huh",
    "corresponding_authors": "",
    "abstract": "DRAM caches have emerged as an efficient new layer in the memory hierarchy to address the increasing diversity of memory components. When a small amount of fast memory is combined with slow but large memory, the cache-based organization of the fast memory can provide a SW-transparent solution for the hybrid memory systems. In such DRAM cache designs, their effectiveness is affected by the bandwidth and latency of both fast and slow memory. To quantitatively assess the effect of memory configurations and application patterns on the DRAM cache designs, this article first investigates how three prior approaches perform with six hybrid memory scenarios. From the investigation, we observe no single DRAM cache organization always outperforms the other organizations across the diverse hybrid memory configurations and memory access patterns. Based on this observation, this article proposes a reconfigurable DRAM cache design that can adapt to different hybrid memory combinations and workload patterns. Unlike the fixed tag and data arrays of conventional on-chip SRAM caches, this study advocates to exploit the flexibility of DRAM caches, which can store tags and data to DRAM in any arbitrary way. Using a sample-based mechanism, the proposed DRAM cache controller dynamically finds the best organization from three candidates and applies the best one by reconfiguring the tags and data layout in the DRAM cache. Our evaluation shows that the proposed morphable DRAM cache can outperform the fixed DRAM configurations across six hybrid memory configurations.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2964213529",
    "type": "article"
  },
  {
    "title": "A First Step Toward Using Quantum Computing for Low-level WCETs Estimations",
    "doi": "https://doi.org/10.1145/3335549",
    "publication_date": "2019-07-18",
    "publication_year": 2019,
    "authors": "Stéphane Louise",
    "corresponding_authors": "Stéphane Louise",
    "abstract": "Low-Level analysis of Worst Case Execution Time (WCET) is an important field for real-time system validation. It stands between computer architecture and mathematics, as it relies strongly on variants of abstract interpretation. One of the features that causes the largest uncertainty regarding WCET evaluation for low-level analysis of sequential execution on a single processor is taking Cache Memory-related Delays (CMRD) and Cache-related Preemption Delays (CRPD) correctly into account. Research work from the 1990s provides a good basic framework for this problem as long as a task runs without preemption. But when preemption of tasks is allowed, although several formalisms exist, their predictive power is lower and the usual approach relies on analyses of NP-hard problems. In this article, we want to show some potential advantages of using a formalism inspired by Quantum Computing (QC) to evaluate CMRDs with preemptions while avoiding the NP-hard problem underneath. The experimental results, with a classic (non-quantum) numerical approach, on a selection of Malardalen benchmark programs display very good accuracy, while the complexity of the evaluation is a low-order polynomial of the number of memory accesses. While it is not yet a fully parallel quantum algorithm, we provide a first roadmap on how to reach such an objective.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2964339436",
    "type": "article"
  },
  {
    "title": "Chunking for Dynamic Linear Pipelines",
    "doi": "https://doi.org/10.1145/3363815",
    "publication_date": "2019-11-18",
    "publication_year": 2019,
    "authors": "Aristeidis Mastoras; Thomas R. Gross",
    "corresponding_authors": "",
    "abstract": "Dynamic scheduling and dynamic creation of the pipeline structure are crucial for efficient execution of pipelined programs. Nevertheless, dynamic systems imply higher overhead than static systems. Therefore, chunking is the key to decrease the synchronization and scheduling overhead by grouping activities. We present a chunking algorithm for dynamic systems that handles dynamic linear pipelines, which allow the number and duration of stages to be determined at run-time. The evaluation on 44 cores shows that chunking brings the overhead of dynamic scheduling down to that of a static scheduler, and it enables efficient and scalable execution of fine-grained dynamic linear pipelines.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2987548385",
    "type": "article"
  },
  {
    "title": "Improving Memory Efficiency in Heterogeneous MPSoCs through Row-Buffer Locality-aware Forwarding",
    "doi": "https://doi.org/10.1145/3377149",
    "publication_date": "2020-03-04",
    "publication_year": 2020,
    "authors": "Yang Song; Bill Lin",
    "corresponding_authors": "",
    "abstract": "In heterogeneous multicore systems, the memory subsystem plays a critical role, since most core-to-core communications are conducted through the main memory. Memory efficiency has a substantial impact on system performance. Although memory traffic from multimedia cores generally manifests high row-buffer locality, which is beneficial to memory efficiency, the locality is often lost as memory streams are forwarded through networks-on-chip (NoC). Previous studies have discussed the techniques that improve memory visibility to reveal scattered row-buffer hit opportunities to the memory scheduler. However, extending local memory visibility introduces little benefit after the locality has been severely diluted. As the alternative approach, preserving row-buffer locality in the NoC has not been well explored. What is worse, it remains to be studied how to perform network traffic scheduling with the awareness of both memory efficiency and quality-of-service (QoS). In this article, we propose a router design with embedded row-index caches to enable locality-aware packet forwarding. The proposed design requires minor modifications to existing router microarchitecture and can be easily implemented with priority arbiters to integrate QoS support. Extensive evaluations show that the proposed design achieves higher memory efficiency than prior memory-aware routers, in addition to providing QoS support. On basis of extant QoS-aware routers, locality-aware forwarding helps to increase row-buffer hits by 58.32% and reduce memory latency by 14.45% on average. It also introduces a net reduction in DRAM and NoC energy cost by 27.82%.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3009450728",
    "type": "article"
  },
  {
    "title": "Network Interface Architecture for Remote Indirect Memory Access (RIMA) in Datacenters",
    "doi": "https://doi.org/10.1145/3374215",
    "publication_date": "2020-05-29",
    "publication_year": 2020,
    "authors": "Jiachen Xue; T. N. Vijaykumar; Mithuna Thottethodi",
    "corresponding_authors": "",
    "abstract": "Remote Direct Memory Access (RDMA) fabrics such as InfiniBand and Converged Ethernet report latency shorter by a factor of 50 than TCP. As such, RDMA is a potential replacement for TCP in datacenters (DCs) running low-latency applications, such as Web search and memcached. InfiniBand’s Shared Receive Queues (SRQs), which use two-sided send/recv verbs (i.e., channel semantics ), reduce the amount of pre-allocated, pinned memory (despite optimizations such as InfiniBand’s on-demand paging (ODP)) for message buffers. However, SRQs are limited fundamentally to a single message size per queue, which incurs either memory wastage or significant programmer burden for typical DC traffic of an arbitrary number (level of burstiness) of messages of arbitrary size. We propose remote indirect memory access (RIMA) , which avoids these pitfalls by providing (1) network interface card (NIC) microarchitecture support for novel queue semantics and (2) a new “verb” called append . To append a sender’s message to a shared queue, the receiver NIC atomically increments the queue’s tail pointer by the incoming message’s size and places the message in the newly created space. As in traditional RDMA, the NIC is responsible for pointer lookup, address translation, and enforcing virtual memory protections. This indirection of specifying a queue (and not its tail pointer, which remains hidden from senders) handles the typical DC traffic of an arbitrary sender sending an arbitrary number of messages of arbitrary size. Because RIMA’s simple hardware adds only 1--2 ns to the multi-\\mu s message latency, RIMA achieves the same message latency and throughput as InfiniBand SRQ with unlimited buffering. Running memcached traffic on a 30-node InfiniBand cluster, we show that at similar, low programmer effort, RIMA achieves significantly smaller memory footprint than SRQ. However, while SRQ can be crafted to minimize memory footprint by expending significant programming effort, RIMA provides those benefits with little programmer effort. For memcached traffic, a high-performance key-value cache ( FastKV ) using RIMA achieves either 3× lower 96 th-percentile latency or significantly better throughput or memory footprint than FastKV using RDMA.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3030696077",
    "type": "article"
  },
  {
    "title": "Reliability Analysis for Unreliable FSM Computations",
    "doi": "https://doi.org/10.1145/3377456",
    "publication_date": "2020-05-29",
    "publication_year": 2020,
    "authors": "Amir Hossein Nodehi Sabet; Junqiao Qiu; Zhijia Zhao; Sriram Krishnamoorthy",
    "corresponding_authors": "",
    "abstract": "Finite State Machines (FSMs) are fundamental in both hardware design and software development. However, the reliability of FSM computations remains poorly understood. Existing reliability analyses are mainly designed for generic computations and are unaware of the special error tolerance characteristics in FSM computations. This work introduces RelyFSM -- a state-level reliability analysis framework for FSM computations. By modeling the behaviors of unreliable FSM executions and qualitatively reasoning about the transition structures, RelyFSM can precisely capture the inherent error tolerance in FSM computations. Our evaluation with real-world FSM benchmarks confirms both the accuracy and efficiency of RelyFSM.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3044186242",
    "type": "article"
  },
  {
    "title": "NNBench-X",
    "doi": "https://doi.org/10.1145/3417709",
    "publication_date": "2020-11-10",
    "publication_year": 2020,
    "authors": "Xinfeng Xie; Xing Hu; Peng Gu; Shuangchen Li; Yu Ji; Yuan Xie",
    "corresponding_authors": "",
    "abstract": "The tremendous impact of deep learning algorithms over a wide range of application domains has encouraged a surge of neural network (NN) accelerator research. Facilitating the NN accelerator design calls for guidance from an evolving benchmark suite that incorporates emerging NN models. Nevertheless, existing NN benchmarks are not suitable for guiding NN accelerator designs. These benchmarks are either selected for general-purpose processors without considering unique characteristics of NN accelerators or lack quantitative analysis to guarantee their completeness during the benchmark construction, update, and customization. In light of the shortcomings of prior benchmarks, we propose a novel benchmarking methodology for NN accelerators with a quantitative analysis of application performance features and a comprehensive awareness of software-hardware co-design. Specifically, we decouple the benchmarking process into three stages: First, we characterize the NN workloads with quantitative metrics and select the representative applications for the benchmark suite to ensure diversity and completeness. Second, we refine the selected applications according to the customized model compression techniques provided by specific software-hardware co-design. Finally, we evaluate a variety of accelerator designs on the generated benchmark suite. To demonstrate the effectiveness of our benchmarking methodology, we conduct a case study of composing an NN benchmark from the TensorFlow Model Zoo and compress these selected models with various model compression techniques. Finally, we evaluate compressed models on various architectures, including GPU, Neurocube, DianNao, and Cambricon-X.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3098560359",
    "type": "article"
  },
  {
    "title": "SG <sup>XL</sup>",
    "doi": "https://doi.org/10.1145/3433983",
    "publication_date": "2020-12-30",
    "publication_year": 2020,
    "authors": "Sujay Yadalam; Vinod Ganapathy; Arkaprava Basu",
    "corresponding_authors": "",
    "abstract": "Intel’s SGX architecture offers clients of public cloud computing platforms the ability to create hardware-protected enclaves whose contents are protected from privileged system software. However, SGX relies on system software for enclave memory management. In a sequence of recent papers, researchers have demonstrated that this reliance allows a malicious OS/hypervisor to snoop on the page addresses being accessed from within an enclave via various channels. This page address stream can then be used to infer secrets if the enclave’s page access pattern depends upon the secret and this constitutes an important class of side-channels. We propose SG XL , a hardware-software co-designed system that significantly increases the difficulty of any page address-based side-channels through the use of large pages. A large page maps address ranges at a much larger granularity than the default page size (at least 512× larger). SG XL thus significantly lowers resolution of the leaked page address stream and could practically throttle all flavors of page-address based side-channels. We detail the modifications needed to SGX’s software stack and the (minor) hardware enhancements required for SG XL to guarantee the use of large pages in the presence of adversarial system software. We empirically show that SG XL could be one of those rare systems that enhances security with the potential of improving performance as well.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3117113715",
    "type": "article"
  },
  {
    "title": "Low-power Near-data Instruction Execution Leveraging Opcode-based Timing Analysis",
    "doi": "https://doi.org/10.1145/3504005",
    "publication_date": "2022-01-31",
    "publication_year": 2022,
    "authors": "Athanasios Tziouvaras; Georgios Dimitriou; Stamoulis Georgios",
    "corresponding_authors": "",
    "abstract": "Traditional processor architectures utilize an external DRAM for data storage, while they also operate under worst-case timing constraints. Such designs are heavily constrained by the delay costs of the data transfer between the core pipeline and the DRAM, and they are incapable of exploiting the timing variations of their pipeline stages. In this work, we focus on a near-data processing methodology combined with a novel timing analysis technique that enables the adaptive frequency scaling of the core clock and boosts the performance of low-power designs. We propose a near-data processing and better-than-worst-case co-design methodology to efficiently move the instruction execution to the DRAM side and, at the same time, to allow the pipeline to operate at higher clock frequencies compared to the worst-case approach. To this end, we develop a timing analysis technique, which evaluates the timing requirements of individual instructions and we dynamically scale the clock frequency, according to the instructions types that currently occupy the pipeline. We evaluate the proposed methodology on six different RISC-V post-layout implementations using an HMC DRAM to enable the processing-in-memory (PIM) process. Results indicate an average speedup factor of 1.96× with a 1.6× reduction in energy consumption compared to a standard RISC-V PIM baseline implementation.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4210379803",
    "type": "article"
  },
  {
    "title": "Accelerating Video Captioning on Heterogeneous System Architectures",
    "doi": "https://doi.org/10.1145/3527609",
    "publication_date": "2022-04-01",
    "publication_year": 2022,
    "authors": "Horng-Ruey Huang; Ding‐Yong Hong; Jan‐Jan Wu; Kung-Fu Chen; Pangfeng Liu; Wei‐Chung Hsu",
    "corresponding_authors": "",
    "abstract": "Video captioning is a core technology to many important applications, such as AI-assisted medical diagnosis, video question answering, storytelling through videos, and lip-reading. Video captioning employs a hybrid CNN + RNN model. Accelerating such a hybrid model on a heterogeneous system is challenging for two reasons. First, CNN and RNN exhibit very different computing behaviors, making the mapping between computation and heterogeneous devices difficult. Second, data dependency exists between the CNN and RNN within a video frame and between adjacent RNNs across video frames. These data dependencies prohibit the full parallelization of the hybrid model. The issues also include the utilization of accelerator resources, which is critical to maximizing the performance. In this work, we propose a fine-grained scheduling scheme for mapping computation and devices within a video frame, and a pipeline scheduling scheme for exploiting maximum parallelism between the execution of the video frames. In addition, we propose two capacity-guided scheduling methods. On the server, the concurrent kernel execution mechanism is exploited for improving GPU utilization. On the edge platform, we rearrange CNN computation among the CPU and EdgeTPUs guided by the EdgeTPU’s SRAM capacity so that balanced computation is achieved and off-chip memory overhead is minimized. Experimental results show that our scheduling scheme improves video captioning performance by up to 3.24 \\( \\times \\) with CPU + GPU collaboration over the GPU-only execution. On an edge platform with an ARM CPU and two EdgeTPUs, our CPU + EdgeTPU scheduling exhibits outstanding performance, which achieves up to 54.9 \\( \\times \\) speedup compared to using ARM CPU only and can perform video captioning of 59 frames per second.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4220956204",
    "type": "article"
  },
  {
    "title": "MAPPER: Managing Application Performance via Parallel Efficiency Regulation",
    "doi": "https://doi.org/10.1145/3501767",
    "publication_date": "2022-03-24",
    "publication_year": 2022,
    "authors": "Sharanyan Srikanthan; Sayak Chakraborti; Princeton Ferro; Sandhya Dwarkadas",
    "corresponding_authors": "",
    "abstract": "State-of-the-art systems, whether in servers or desktops, provide ample computational and storage resources to allow multiple simultaneously executing potentially parallel applications. However, performance tends to be unpredictable, being a function of algorithmic design, resource allocation choices, and hardware resource limitations. In this article, we introduce MAPPER, a manager of application performance via parallel efficiency regulation. MAPPER uses a privileged daemon to monitor (using hardware performance counters) and coordinate all participating applications by making two coupled decisions: the degree of parallelism to allow each application to improve system efficiency while guaranteeing quality of service (QoS), and which specific CPU cores to schedule applications on. The QoS metric may be chosen by the application and could be in terms of execution time, throughput, or tail latency, relative to the maximum performance achievable on the machine. We demonstrate that using a normalized parallel efficiency metric allows comparison across and cooperation among applications to guarantee their required QoS. While MAPPER may be used without application or runtime modification, use of a simple interface to communicate application-level knowledge improves MAPPER’s efficacy. Using a QoS guarantee of 85% of the IPC achieved with a fair share of resources on the machine, MAPPER achieves up to 3.3 \\( \\times \\) speedup relative to unmodified Linux and runtime systems, with an average improvement of 17% in our test cases. At the same time, MAPPER violates QoS for only 2% of the applications (compared to 23% for Linux), while placing much tighter bounds on the worst case. MAPPER relieves hardware bottlenecks via task-to-CPU placement and allocates more CPU contexts to applications that exhibit higher parallel efficiency while guaranteeing QoS, resulting in both individual application performance predictability and overall system efficiency.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4221080558",
    "type": "article"
  },
  {
    "title": "An Application-oblivious Memory Scheduling System for DNN Accelerators",
    "doi": "https://doi.org/10.1145/3535355",
    "publication_date": "2022-05-09",
    "publication_year": 2022,
    "authors": "Jiansong Li; Xueying Wang; Xiaobing Chen; Guangli Li; Xiao Dong; Peng Zhao; Xianzhi Yu; Yongxin Yang; Wei Cao; Lei Liu; Xiaobing Feng",
    "corresponding_authors": "",
    "abstract": "Deep Neural Networks (DNNs) tend to go deeper and wider, which poses a significant challenge to the training of DNNs, due to the limited memory capacity of DNN accelerators. Existing solutions for memory-efficient DNN training are densely coupled with the application features of DNN workloads, e.g., layer structures or computational graphs of DNNs are necessary for these solutions. This would result in weak versatility for DNNs with sophisticated layer structures or complicated computation graphs. These schemes usually need to be re-implemented or re-adapted due to the new layer structures or the unusual operators in the computational graphs introduced by these DNNs. In this article, we review the memory pressure issues of DNN training from the perspective of runtime systems and model the memory access behaviors of DNN workloads. We identify the iterative, regularity , and extremalization properties of memory access patterns for DNN workloads. Based on these observations, we propose AppObMem, an application-oblivious memory scheduling system. AppObMem automatically traces the memory behaviors of DNN workloads and schedules the memory swapping to reduce the memory pressure of the device accelerators without the perception of high-level information of layer structures or computation graphs. Evaluations on a variety of DNN models show that, AppObMem obtains 40–60% memory savings with acceptable performance loss. AppObMem is also competitive with other open sourced SOTA schemes.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4229374303",
    "type": "article"
  },
  {
    "title": "TokenSmart: Distributed, Scalable Power Management in the Many-core Era",
    "doi": "https://doi.org/10.1145/3559762",
    "publication_date": "2022-08-27",
    "publication_year": 2022,
    "authors": "Parth Shah; Gautham Shenoy R.; Vaidyanathan Srinivasan; Pradip Bose; Alper Buyuktosunoglu",
    "corresponding_authors": "",
    "abstract": "Centralized power management control systems are hitting a scalability limit. In particular, enforcing a power cap in a many-core system in a performance-friendly manner is quite challenging. Today’s on-chip controller reduces the clock speed of compute domains in response to local or global power limit alerts. However, this is opaque to the operating system (OS), which continues to request higher clock frequency based on the workload characteristics acting against the centralized on-chip controller. To address these issues, we introduce TokenSmart, which implements a set of scalable distributed frequency control heuristics within the OS, using a novel token-based mechanism. The number of system-allocated power tokens represents the maximum allowable power consumption; and the OS governor orchestrates a token-passing (or sharing) algorithm between the compute engines. Token allocation count increase (decrease) corresponds to a increase (decrease) of clock frequency. The compute units are connected in a ring-topology allowing minimal meta-data to be passed along with the token value for regulating power budget. We explore different heuristics to assign tokens smartly across the units. This results in efficient power regulation and sustenance of turbo frequencies over a longer duration. Our proposed methodology can be implemented in hardware with multiple on-chip controllers, or in software where each set of cores acts as a compute unit. The methodology is currently implemented within the Linux kernel of a real IBM POWER9 many-core system and experimentally verified on different real world workloads such as Redis, Cassandra, PostgreSQL along with a micro-benchmark such as rt-app. Our experiments indicate the increase in throughput for all the workloads along with the benefit of power savings. For instance, results show a considerable boost of about 4% in throughput of both the PostgreSQL and Redis benchmark with a substantial savings in power consumption (18% and 37%, respectively). If the approach is implemented in hardware, then our experimental analysis speculates the throughput to increase up to 14% in PostgreSQL benchmark.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4293367489",
    "type": "article"
  },
  {
    "title": "FlexHM: A Practical System for Heterogeneous Memory with Flexible and Efficient Performance Optimizations",
    "doi": "https://doi.org/10.1145/3565885",
    "publication_date": "2022-10-12",
    "publication_year": 2022,
    "authors": "Bo Peng; Yaozu Dong; Jianguo Yao; Fengguang Wu; Haibing Guan",
    "corresponding_authors": "",
    "abstract": "With the rapid development of cloud computing, numerous cloud services, containers, and virtual machines have been bringing tremendous demands on high-performance memory resources to modern data centers. Heterogeneous memory, especially the newly released Optane memory, offer appropriate alternatives against DRAM in clouds with the advantages of larger capacity, lower purchase cost, and promising performance. However, cloud services suffer serious implementation inconvenience and performance degradation when using hybrid DRAM and Optane memory. This article proposes FlexHM, a practical system to manage transparent heterogeneous memory resources and flexibly optimize memory access performance for all VMs, containers, and native applications. We present an open-source prototype of FlexHM in Linux with several main contributions. First, FlexHM raises a novel two-level NUMA design to manage DRAM and Optane memory as transparent main memory resources. Second, FlexHM provides flexible and efficient memory management, helping optimize memory access performance or save purchase costs of memory resources for differential cloud services with customized management strategies. Finally, the evaluations show that cloud workloads using 50% Optane slow memory on FlexHM can achieve up to 93% of the performance when using all-DRAM, and FlexHM provides up to 5.8× improvement over the previous heterogeneous memory system solution when workloads use the same ratio of DRAM and Optane memory.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4304693763",
    "type": "article"
  },
  {
    "title": "Fence Placement for Legacy Data-Race-Free Programs via Synchronization Read Detection",
    "doi": "https://doi.org/10.1145/2835179",
    "publication_date": "2015-12-08",
    "publication_year": 2015,
    "authors": "Andrew J. McPherson; Vijay Nagarajan; Susmit Sarkar; Marcelo Cintra",
    "corresponding_authors": "",
    "abstract": "Shared-memory programmers traditionally assumed Sequential Consistency (SC), but modern systems have relaxed memory consistency. Here, the trend in languages is toward Data-Race-Free (DRF) models, where, assuming annotated synchronizations and the program being well-synchronized by those synchronizations, the hardware and compiler guarantee SC. However, legacy programs lack annotations, so even well-synchronized (legacy DRF) programs aren’t recognized. For legacy DRF programs, we can significantly prune the set of memory orderings determined by automated fence placement by automatically identifying synchronization reads. We prove our rules for identifying them conservatively, implement them within LLVM, and observe a 30% average performance improvement over previous techniques.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W1894589249",
    "type": "article"
  },
  {
    "title": "Exploiting Existing Comparators for Fine-Grained Low-Cost Error Detection",
    "doi": "https://doi.org/10.1145/2656341",
    "publication_date": "2014-10-27",
    "publication_year": 2014,
    "authors": "Gülay Yalçın; Oğuz Ergin; Emrah Islek; Osman Ünsal; Adrián Cristal",
    "corresponding_authors": "",
    "abstract": "Fault tolerance has become a fundamental concern in computer design, in addition to performance and power. Although several error detection schemes have been proposed to discover a faulty core in the system, these proposals could waste the whole core, including many error-free structures in it after error detection. Moreover, many fault-tolerant designs require additional hardware for data replication or for comparing the replicated data. In this study, we provide a low-cost, fine-grained error detection scheme by exploiting already existing comparators and data replications in the several pipeline stages such as issue queue, rename logic, and translation lookaside buffer. We reduce the vulnerability of the source register tags in IQ by 60%, the vulnerability of instruction TLB by 64%, the vulnerability of data TLB by 45%, and the vulnerability of the register tags of rename logic by 20%.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2013033442",
    "type": "article"
  },
  {
    "title": "Leveraging Transactional Execution for Memory Consistency Model Emulation",
    "doi": "https://doi.org/10.1145/2786980",
    "publication_date": "2015-08-31",
    "publication_year": 2015,
    "authors": "Ragavendra Natarajan; Antonia Zhai",
    "corresponding_authors": "",
    "abstract": "System emulation is widely used in today’s computer systems. This technology opens new opportunities for resource sharing as well as enhancing system security and reliability. System emulation across different instruction set architectures (ISA) can enable further opportunities. For example, cross-ISA emulation can enable workload consolidation over a wide range of microprocessors and potentially facilitate the seamless deployment of new processor architectures. As multicore and manycore processors become pervasive, it is important to address the challenges toward supporting system emulation on these platforms. A key challenge in cross-ISA emulation on multicore systems is ensuring the correctness of emulation when the guest and the host memory consistency models differ. Many existing cross-ISA system emulators are sequential, thus they are able to avoid this problem at the cost of significant performance degradation. Recently proposed parallel emulators are able to address the performance limitation; however, they provide limited support for memory consistency model emulation. When the host system has a weaker memory consistency model compared to the guest system, the emulator can insert memory fences at appropriate locations in the translated code to enforce the guest memory ordering constraints. These memory fences can significantly degrade the performance of the translated code. Transactional execution support available on certain recent microprocessors provides an alternative approach. Transactional execution of the translated code enforces sequential consistency (SC) at the coarse-grained transaction level, which in turn ensures that all memory accesses made on the host machine conform to SC. Enforcing SC on the host machine guarantees that the emulated execution will be correct for any guest memory model. In this article, we compare and evaluate the overheads associated with using transactions and fences for memory consistency model emulation on the Intel Haswell processor. Our experience of implementing these two approaches on a state-of-the-art parallel emulator, COREMU, demonstrates that memory consistency model emulation using transactions performs better when the transaction sizes are large enough to amortize the transaction overhead and the transaction conflict rate is low, whereas inserting memory fences is better for applications in which the transaction overhead is high. A hybrid implementation that dynamically determines which approach to invoke can outperform both approaches. Our results, based on the SPLASH-2 and the PARSEC benchmark suites, demonstrate that the proposed hybrid approach is able to outperform the fence insertion mechanism by 4.9% and the transactional execution approach by 24.9% for two-thread applications, and outperform them by 4.5% and 44.7%, respectively, for four-threaded execution.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2088634313",
    "type": "article"
  },
  {
    "title": "Leveraging GPUs using cooperative loop speculation",
    "doi": "https://doi.org/10.1145/2579617",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "Mehrzad Samadi; Amir Hormati; Janghaeng Lee; Scott Mahlke",
    "corresponding_authors": "",
    "abstract": "Graphics processing units, or GPUs, provide TFLOPs of additional performance potential in commodity computer systems that frequently go unused by most applications. Even with the emergence of languages such as CUDA and OpenCL, programming GPUs remains a difficult challenge for a variety of reasons, including the inherent algorithmic characteristics and data structure choices used by applications as well as the tedious performance optimization cycle that is necessary to achieve high performance. The goal of this work is to increase the applicability of GPUs beyond CUDA/OpenCL to implicitly data-parallel applications written in C/C++ using speculative parallelization. To achieve this goal, we propose Paragon : a static/dynamic compiler platform to speculatively run possibly data-parallel portions of sequential applications on the GPU while cooperating with the system CPU. For such loops, Paragon utilizes the GPU in an opportunistic way while orchestrating a cooperative relation between the CPU and GPU to reduce the overhead of miss-speculations. Paragon monitors the dependencies for the loops running speculatively on the GPU and nonspeculatively on the CPU using a lightweight distributed conflict detection designed specifically for GPUs, and transfers the execution to the CPU in case a conflict is detected. Paragon resumes the execution on the GPU after the CPU resolves the dependency. Our experiments show that Paragon achieves 4x on average and up to 30x speedup compared to unsafe CPU execution with four threads and 7x on average and up to 64x speedup versus sequential execution across a set of sequential but implicitly data-parallel applications.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2088754683",
    "type": "article"
  },
  {
    "title": "Revisiting LP-NUCA Energy Consumption",
    "doi": "https://doi.org/10.1145/2632217",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "Darío Suárez Gracia; Alexandra Ferrerón; Luis Montesano Del Campo; Teresa Monreal; Víctor Viñals",
    "corresponding_authors": "",
    "abstract": "Cache working-set adaptation is key as embedded systems move to multiprocessor and Simultaneous Multithreaded Architectures (SMT) because interthread pollution harms system performance and battery life. Light-Power NUCA (LP-NUCA) is a working-set adaptive cache that depends on temporal-locality to save energy. This work identifies the sources of energy waste in LP-NUCAs: parallel access to the tag and data arrays of the tiles and low locality phases with useless block migration. To counteract both issues, we prove that switching to serial access reduces energy without harming performance and propose a machine learning Adaptive Drop Rate (ADR) controller that minimizes the amount of replacement and migration when locality is low. This work demonstrates that these techniques efficiently adapt the cache drop and access policies to save energy. They reduce LP-NUCA consumption 22.7% for 1SMT. With interthread cache contention in 2SMT, the savings rise to 29%. Versus a conventional organization, energy--delay improves 20.8% and 25% for 1- and 2SMT benchmarks, and, in 65% of the 2SMT mixes, gains are larger than 20%.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2098817810",
    "type": "article"
  },
  {
    "title": "Integrated Mapping and Synthesis Techniques for Network-on-Chip Topologies with Express Channels",
    "doi": "https://doi.org/10.1145/2831233",
    "publication_date": "2015-11-16",
    "publication_year": 2015,
    "authors": "Sandeep D'souza; J. Soumya; Santanu Chattopadhyay",
    "corresponding_authors": "",
    "abstract": "The addition of express channels to a traditional mesh network-on-chip (NoC) has emerged as a viable solution to solve the problem of high latency. In this article, we address the problem of integrated mapping and synthesis for express channel--based mesh NoC topologies. An integer linear programming--based formulation has been presented for the mapping problem followed by a constructive heuristic for simultaneous application mapping and synthesis for an express channel--based NoC. The static and dynamic simulation results indicate that the obtained mappings lead to significant reduction in both average packet delay and network energy consumption. The obtained synthesized topologies were also found to be much more power efficient compared to conventional express channel topologies.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2259941569",
    "type": "article"
  },
  {
    "title": "A Filtering Mechanism to Reduce Network Bandwidth Utilization of Transaction Execution",
    "doi": "https://doi.org/10.1145/2837028",
    "publication_date": "2016-01-04",
    "publication_year": 2016,
    "authors": "Lihang Zhao; Lizhong Chen; Woojin Choi; Jeffrey Draper",
    "corresponding_authors": "",
    "abstract": "Hardware Transactional Memory (HTM) relies heavily on the on-chip network for intertransaction communication. However, the network bandwidth utilization of transactions has been largely neglected in HTM designs. In this work, we propose a cost model to analyze network bandwidth in transaction execution. The cost model identifies a set of key factors that can be optimized through system design to reduce the communication cost of HTM. Based on the model and network traffic characterization of a representative HTM design, we identify a huge source of superfluous traffic due to failed requests in transaction conflicts. As observed in a spectrum of workloads, 39% of the transactional requests fail due to conflicts, which renders 58% of the transactional network traffic futile. To combat this pathology, a novel in-network filtering mechanism is proposed. The on-chip router is augmented to predict conflicts among transactions and proactively filter out those requests that have a high probability to fail. Experimental results show the proposed mechanism reduces total network traffic by 24% on average for a set of high-contention TM applications, thereby reducing energy consumption by an average of 24%. Meanwhile, the contention in the coherence directory is reduced by 68%, on average. These improvements are achieved with only 5% area added to a conventional on-chip router design.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2270918761",
    "type": "article"
  },
  {
    "title": "User-Assisted Store Recycling for Dynamic Task Graph Schedulers",
    "doi": "https://doi.org/10.1145/3018111",
    "publication_date": "2016-12-28",
    "publication_year": 2016,
    "authors": "Mehmet Can Kurt; Sriram Krishnamoorthy; Gagan Agrawal; Bin Ren",
    "corresponding_authors": "",
    "abstract": "The emergence of the multi-core era has led to increased interest in designing effective yet practical parallel programming models. Models based on task graphs that operate on single-assignment data are attractive in several ways. Notably, they can support dynamic applications and precisely represent the available concurrency. However, for efficient execution, they also require nuanced algorithms for scheduling and memory management. In this article, we consider memory-efficient dynamic scheduling of task graphs. Specifically, we present a novel approach for dynamically recycling the memory locations assigned to data items as they are produced by tasks. We develop algorithms to identify memory-efficient store recycling functions by systematically evaluating the validity of a set of user-provided or automatically generated alternatives. Because recycling functions can be input data-dependent, we have also developed support for continued correct execution of a task graph in the presence of a potentially incorrect store recycling function. Experimental evaluation demonstrates that this approach to automatic store recycling incurs little to no overheads, achieves memory usage comparable to the best manually derived solutions, often produces recycling functions valid across problem sizes and input parameters, and efficiently recovers from an incorrect choice of store recycling functions.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2562005191",
    "type": "article"
  },
  {
    "title": "Accuracy Bugs",
    "doi": "https://doi.org/10.1145/3017991",
    "publication_date": "2016-12-12",
    "publication_year": 2016,
    "authors": "İsmail Aktürk; Riad Akram; Mohammad Majharul Islam; Abdullah Muzahid; Ulya R. Karpuzcu",
    "corresponding_authors": "",
    "abstract": "Parallel programming introduces notoriously difficult bugs, usually referred to as concurrency bugs. This article investigates the potential for deviating from the conventional wisdom of writing concurrency bug--free, parallel programs. It explores the benefit of accepting buggy but approximately correct parallel programs by leveraging the inherent tolerance of emerging parallel applications to inaccuracy in computations. Under algorithmic noise tolerance, a new class of concurrency bugs, accuracy bugs, degrade the accuracy of computation (often at acceptable levels) rather than causing catastrophic termination. This study demonstrates how embracing accuracy bugs affects the application output quality and performance and analyzes the impact on execution semantics.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2562573417",
    "type": "article"
  },
  {
    "title": "The Impact of Page Size and Microarchitecture on Instruction Address Translation Overhead",
    "doi": "https://doi.org/10.1145/3600089",
    "publication_date": "2023-05-27",
    "publication_year": 2023,
    "authors": "Yufeng Zhou; Alan L. Cox; Sandhya Dwarkadas; Xiaowan Dong",
    "corresponding_authors": "",
    "abstract": "As the volume of data processed by applications has increased, considerable attention has been paid to data address translation overheads, leading to the widespread use of larger page sizes (“superpages”) and multi-level translation lookaside buffers (TLBs). However, far less attention has been paid to instruction address translation and its relation to TLB and pipeline structure. In prior work, we quantified the impact of using code superpages on a variety of widely used applications, ranging from compilers to web user-interface frameworks, and the impact of sharing page table pages for executables and shared libraries. Within this article, we augment those results by first uncovering the effects that microarchitectural differences between Intel Skylake and AMD Zen+, particularly their different TLB organizations, have on instruction address translation overhead. This analysis provides some key insights into the microarchitectural design decisions that impact the cost of instruction address translation. First, a lower-level (level 2) TLB that has both instruction and data mappings competing for space within the same structure allows better overall performance and utilization when using code superpages. Code superpages not only reduce instruction address translation overhead but also indirectly reduce data address translation overhead. In fact, for a few applications, the use of just a few code superpages has a larger impact on overall performance than the use of a much larger number of data superpages. Second, a level 1 (L1) TLB with separate structures for different page sizes may require careful tuning of the superpage promotion policy for code, and a correspondingly suboptimal utilization of the level 2 TLB. In particular, increasing the number of superpages when the size of the L1 superpage structure is small may result in more L1 TLB misses for some applications. Moreover, on some microarchitectures, the cost of these misses can be highly variable, because replacement is delayed until all of the in-flight instructions mapped by the victim entry are retired. Hence, more superpage promotions can result in a performance regression. Finally, our findings also make a case for first-class OS support for superpages on ordinary files containing executables and shared libraries, as well as a more aggressive superpage policy for code.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4378710497",
    "type": "article"
  },
  {
    "title": "MPU: Memory-centric SIMT Processor via In-DRAM Near-bank Computing",
    "doi": "https://doi.org/10.1145/3603113",
    "publication_date": "2023-05-29",
    "publication_year": 2023,
    "authors": "Xinfeng Xie; Peng Gu; Yufei Ding; Dimin Niu; Hongzhong Zheng; Yuan Xie",
    "corresponding_authors": "",
    "abstract": "With the growing number of data-intensive workloads, GPU, which is the state-of-the-art single-instruction-multiple-thread (SIMT) processor, is hindered by the memory bandwidth wall. To alleviate this bottleneck, previously proposed 3D-stacking near-bank computing accelerators benefit from abundant bank-internal bandwidth by bringing computations closer to the DRAM banks. However, these accelerators are specialized for certain application domains with simple architecture data paths and customized software mapping schemes. For general-purpose scenarios, lightweight hardware designs for diverse data paths, architectural supports for the SIMT programming model, and end-to-end software optimizations remain challenging. To address these issues, we propose Memory-centric Processing Unit (MPU), the first SIMT processor based on 3D-stacking near-bank computing architecture. First, to realize diverse data paths with small overheads, MPU adopts a hybrid pipeline with the capability of offloading instructions to near-bank compute-logic. Second, we explore two architectural supports for the SIMT programming model, including a near-bank shared memory design and a multiple activated row-buffers enhancement. Third, we present an end-to-end compilation flow for MPU to support CUDA programs. To fully utilize MPU’s hybrid pipeline, we develop a backend optimization for the instruction offloading decision. The evaluation results of MPU demonstrate 3.46× speedup and 2.57× energy reduction compared with an NVIDIA Tesla V100 GPU on a set of representative data-intensive workloads.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4378715336",
    "type": "article"
  },
  {
    "title": "rNdN: Fast Query Compilation for NVIDIA GPUs",
    "doi": "https://doi.org/10.1145/3603503",
    "publication_date": "2023-06-09",
    "publication_year": 2023,
    "authors": "Alexander Krolik; Clark Verbrugge; Laurie Hendren",
    "corresponding_authors": "",
    "abstract": "GPU database systems are an effective solution to query optimization, particularly with compilation and data caching. They fall short, however, in end-to-end workloads, as existing compiler toolchains are too expensive for use with short-running queries. In this work, we define and evaluate a runtime-suitable query compilation pipeline for NVIDIA GPUs that extracts high performance with only minimal optimization. In particular, our balanced approach successfully trades minor slowdowns in execution for major speedups in compilation, even as data sizes increase. We demonstrate performance benefits compared to both CPU and GPU database systems using interpreters and compilers, extending query compilation for GPUs beyond cached use cases.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4380087783",
    "type": "article"
  },
  {
    "title": "Turn-based Spatiotemporal Coherence for GPUs",
    "doi": "https://doi.org/10.1145/3593054",
    "publication_date": "2023-05-10",
    "publication_year": 2023,
    "authors": "Sooraj Puthoor; Mikko H. Lipasti",
    "corresponding_authors": "",
    "abstract": "This article introduces turn-based spatiotemporal coherence. Spatiotemporal coherence is a novel coherence implementation that assigns write permission to epochs (or turns) as opposed to a processor core. This paradigm shift in the assignment of write permissions satisfies all conditions of a coherence protocol with virtually no coherence overhead. We discuss the implementation of this coherence mechanism on a baseline GPU. The evaluation shows that spatiotemporal coherence achieves a speedup of 7.13% for workloads with read data reuse across kernels compared to the baseline software-managed GPU coherence implementation while also providing write atomicity and avoiding the need for software inserted acquire-release operations. 1",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4382239998",
    "type": "article"
  },
  {
    "title": "Autovesk: Automatic Vectorized Code Generation from Unstructured Static Kernels Using Graph Transformations",
    "doi": "https://doi.org/10.1145/3631709",
    "publication_date": "2023-11-09",
    "publication_year": 2023,
    "authors": "Hayfa Tayeb; Ludovic Paillat; Bérenger Bramas",
    "corresponding_authors": "",
    "abstract": "Leveraging the SIMD capability of modern CPU architectures is mandatory to take full advantage of their increased performance. To exploit this capability, binary executables must be vectorized, either manually by developers or automatically by a tool. For this reason, the compilation research community has developed several strategies for transforming scalar code into a vectorized implementation. However, most existing automatic vectorization techniques in modern compilers are designed for regular codes, leaving irregular applications with non-contiguous data access patterns at a disadvantage. In this article, we present a new tool, Autovesk, that automatically generates vectorized code from scalar code, specifically targeting irregular data access patterns. We describe how our method transforms a graph of scalar instructions into a vectorized one, using different heuristics to reduce the number or cost of instructions. Finally, we demonstrate the effectiveness of our approach on various computational kernels using Intel AVX-512 and ARM SVE. We compare the speedups of Autovesk vectorized code over GCC, Clang LLVM, and Intel automatic vectorization optimizations. We achieve competitive results on linear kernels and up to 11× speedups on irregular kernels.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4385175635",
    "type": "article"
  },
  {
    "title": "Characterizing Multi-Chip GPU Data Sharing",
    "doi": "https://doi.org/10.1145/3629521",
    "publication_date": "2023-10-20",
    "publication_year": 2023,
    "authors": "Shiqing Zhang; Mahmood Naderan-Tahan; Magnus Jahre; Lieven Eeckhout",
    "corresponding_authors": "",
    "abstract": "Multi-chip Graphics Processing Unit (GPU) systems are critical to scale performance beyond a single GPU chip for a wide variety of important emerging applications. A key challenge for multi-chip GPUs, though, is how to overcome the bandwidth gap between inter-chip and intra-chip communication. Accesses to shared data, i.e., data accessed by multiple chips, pose a major performance challenge as they incur remote memory accesses possibly congesting the inter-chip links and degrading overall system performance. This article characterizes the shared dataset in multi-chip GPUs in terms of (1) truly versus falsely shared data, (2) how the shared dataset scales with input size, (3) along which dimensions the shared dataset scales, and (4) how sensitive the shared dataset is with respect to the input’s characteristics, i.e., node degree and connectivity in graph workloads. We observe significant variety in scaling behavior across workloads: some workloads feature a shared dataset that scales linearly with input size, whereas others feature sublinear scaling (following a \\(\\sqrt {2}\\) or \\(\\sqrt [3]{2}\\) relationship). We further demonstrate how the shared dataset affects the optimum last-level cache organization (memory-side versus SM-side) in multi-chip GPUs, as well as optimum memory page allocation and thread scheduling policy. Sensitivity analyses demonstrate the insights across the broad design space.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4387823818",
    "type": "article"
  },
  {
    "title": "Mapi-Pro: An Energy Efficient Memory Mapping Technique for Intermittent Computing",
    "doi": "https://doi.org/10.1145/3629524",
    "publication_date": "2023-10-20",
    "publication_year": 2023,
    "authors": "Satya Jaswanth Badri; Mukesh Saini; Neeraj Goel",
    "corresponding_authors": "",
    "abstract": "Battery-less technology evolved to replace battery usage in space, deep mines, and other environments to reduce cost and pollution. Non-volatile memory (NVM) based processors were explored for saving the system state during a power failure. Such devices have a small SRAM and large non-volatile memory. To make the system energy efficient, we need to use SRAM efficiently. So we must select some portions of the application and map them to either SRAM or FRAM. This paper proposes an ILP-based memory mapping technique for intermittently powered IoT devices. Our proposed technique gives an optimal mapping choice that reduces the system’s Energy-Delay Product (EDP). We validated our system using TI-based MSP430FR6989 and MSP430F5529 development boards. Our proposed memory configuration consumes 38.10% less EDP than the baseline configuration and 9.30% less EDP than the existing work under stable power. Our proposed configuration achieves 20.15% less EDP than the baseline configuration and 26.87% less EDP than the existing work under unstable power. This work supports intermittent computing and works efficiently during frequent power failures.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4387836954",
    "type": "article"
  },
  {
    "title": "Efficient Cross-platform Multiplexing of Hardware Performance Counters via Adaptive Grouping",
    "doi": "https://doi.org/10.1145/3629525",
    "publication_date": "2023-10-21",
    "publication_year": 2023,
    "authors": "Tongyu Liu; Jianmei Guo; Bo Huang",
    "corresponding_authors": "",
    "abstract": "Collecting sufficient microarchitecture performance data is essential for performance evaluation and workload characterization. There are many events to be monitored in a modern processor while only a few hardware performance monitoring counters (PMCs) can be used, so multiplexing is commonly adopted. However, inefficiency commonly exists in state-of-the-art profiling tools when grouping events for multiplexing PMCs. It has the risk of inaccurate measurement and misleading analysis. Commercial tools can leverage PMCs, but they are closed source and only support their specified platforms. To this end, we propose an approach for efficient cross-platform microarchitecture performance measurement via adaptive grouping, aiming to improve the metrics’ sampling ratios. The approach generates event groups based on the number of available PMCs detected on arbitrary machines while avoiding the scheduling pitfall of Linux perf_event subsystem. We evaluate our approach with SPEC CPU 2017 on four mainstream x86-64 and AArch64 processors and conduct comparative analyses of efficiency with two other state-of-the-art tools, LIKWID and ARM Top-down Tool. The experimental results indicate that our approach gains around 50% improvement in the average sampling ratio of metrics without compromising the correctness and reliability.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4387843053",
    "type": "article"
  },
  {
    "title": "ISP Agent: A Generalized In-storage-processing Workload Offloading Framework by Providing Multiple Optimization Opportunities",
    "doi": "https://doi.org/10.1145/3632951",
    "publication_date": "2023-11-14",
    "publication_year": 2023,
    "authors": "Seokwon Kang; Jongbin Kim; Gyeongyong Lee; Jeongmyung Lee; Jiwon Seo; Hyungsoo Jung; Yong Ho Song; Yongjun Park",
    "corresponding_authors": "",
    "abstract": "As solid-state drives (SSDs) with sufficient computing power have recently become the dominant devices in modern computer systems, in-storage processing (ISP), which processes data within the storage without transferring it to the host memory, is being utilized in various emerging applications. The main challenge of ISP is to deliver storage data to the offloaded workload. This is difficult because of the information gap between the host and storage, the data consistency problem between the host and offloaded workloads, and SSD-specific hardware limitations. Moreover, because the offloaded workloads use internal SSD resources, host I/O performance might be degraded due to resource conflicts. Although several ISP frameworks have been proposed, existing ISP approaches that do not deeply consider the internal SSD behavior are often insufficient to support efficient ISP workload offloading with high programmability. In this article, we propose an ISP agent, a lightweight ISP workload offloading framework for SSD devices. The ISP agent provides I/O and memory interfaces that allow users to run existing function codes on SSDs without major code modifications, and separates the resources for the offloaded workloads from the existing SSD firmware to minimize interference with host I/O processing. The ISP agent also provides further optimization opportunities for the offloaded workload by considering SSD architectures. We have implemented the ISP agent on the OpenSSD Cosmos+ board and evaluated its performance using synthetic benchmarks and a real-world ISP-assisted database checkpointing application. The experimental results demonstrate that the ISP agent enhances host application performance while increasing ISP programmability, and that the optimization opportunities provided by the ISP agent can significantly improve ISP-side performance without compromising host I/O processing.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4388657523",
    "type": "article"
  },
  {
    "title": "Fast Convolution Meets Low Precision: Exploring Efficient Quantized Winograd Convolution on Modern CPUs",
    "doi": "https://doi.org/10.1145/3632956",
    "publication_date": "2023-11-17",
    "publication_year": 2023,
    "authors": "Xueying Wang; Guangli Li; Zhen Jia; Xiaobing Feng; Yida Wang",
    "corresponding_authors": "",
    "abstract": "Low-precision computation has emerged as one of the most effective techniques for accelerating convolutional neural networks and has garnered widespread support on modern hardware. Despite its effectiveness in accelerating convolutional neural networks, low-precision computation has not been commonly applied to fast convolutions, such as the Winograd algorithm, due to numerical issues. In this article, we propose an effective quantized Winograd convolution, named LoWino, which employs an in-side quantization method in the Winograd domain to reduce the precision loss caused by transformations. Meanwhile, we present an efficient implementation that integrates well-designed optimization techniques, allowing us to fully exploit the capabilities of low-precision computation on modern CPUs. We evaluate LoWino on two Intel Xeon Scalable Processor platforms with representative convolutional layers and neural network models. The experimental results demonstrate that our approach can achieve an average of 1.84× and 1.91× operator speedups over state-of-the-art implementations in the vendor library while preserving accuracy loss at a reasonable level.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4388763336",
    "type": "article"
  },
  {
    "title": "Coherence Attacks and Countermeasures in Interposer-Based Chiplet Systems",
    "doi": "https://doi.org/10.1145/3633461",
    "publication_date": "2023-11-20",
    "publication_year": 2023,
    "authors": "Gino Chacon; Charles Williams; Johann Knechtel; Ozgur Sinanoglu; Paul V. Gratz; Vassos Soteriou",
    "corresponding_authors": "",
    "abstract": "Industry is moving towards large-scale hardware systems that bundle processor cores, memories, accelerators, and so on. via 2.5D integration. These components are fabricated separately as chiplets and then integrated using an interposer as an interconnect carrier. This new design style is beneficial in terms of yield and economies of scale, as chiplets may come from various vendors and are relatively easy to integrate into one larger sophisticated system. However, the benefits of this approach come at the cost of new security challenges, especially when integrating chiplets that come from untrusted or not fully trusted, third- party vendors. In this work, we explore these challenges for modern interposer-based systems of cache-coherent, multi-core chiplets. First, we present basic coherence-oriented hardware Trojan attacks that pose a significant threat to chiplet-based designs and demonstrate how these basic attacks can be orchestrated to pose a significant threat to interposer-based systems. Second, we propose a novel scheme using an active interposer as a generic, secure-by-construction platform that forms a physical root of trust for modern 2.5D systems. The implementation of our scheme is confined to the interposer, resulting in little cost and leaving the chiplets and coherence system untouched. We show that our scheme prevents a range of coherence attacks with low overheads on system performance, ∼4%. Further, we demonstrate that our scheme scales efficiently as system size and memory capacities increase, resulting in reduced performance overheads.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4388833455",
    "type": "article"
  },
  {
    "title": "Highly Efficient Self-checking Matrix Multiplication on Tiled AMX Accelerators",
    "doi": "https://doi.org/10.1145/3633332",
    "publication_date": "2023-11-22",
    "publication_year": 2023,
    "authors": "Chandra Sekhar Mummidi; Victor C. Ferreira; Sudarshan Srinivasan; Sandip Kundu",
    "corresponding_authors": "",
    "abstract": "General Matrix Multiplication (GEMM) is a computationally expensive operation that is used in many applications such as machine learning. Hardware accelerators are increasingly popular for speeding up GEMM computation, with Tiled Matrix Multiplication (TMUL) in recent Intel processors being an example. Unfortunately, the TMUL hardware is susceptible to errors, necessitating online error detection. The Algorithm-based Error Detection (ABED) technique is a powerful technique to detect errors in matrix multiplications. In this article, we consider implementation of an ABED technique that integrates seamlessly with the TMUL hardware to minimize performance overhead. Unfortunately, rounding errors introduced by floating-point operations do not allow a straightforward implementation of ABED in TMUL. Previously an error bound was considered for addressing rounding errors in ABED. If the error detection threshold is set too low, it will a trigger false alarm, while a loose bound will allow errors to escape detection. In this article, we propose an adaptive error threshold that takes into account the TMUL input values to address the problem of false triggers and error escapes and provide a taxonomy of various error classes. This threshold is obtained from theoretical error analysis but is not easy to implement in hardware. Consequently, we relax the threshold such that it can be easily computed in hardware. While ABED ensures error-free computation, it does not guarantee full coverage of all hardware faults. To address this problem, we propose an algorithmic pattern generation technique to ensure full coverage for all hardware faults. To evaluate the benefits of our proposed solution, we conducted fault injection experiments and show that our approach does not produce any false alarms or detection escapes for observable errors. We conducted additional fault injection experiments on a Deep Neural Network (DNN) model and find that if a fault is not detected, it does not cause any misclassification.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4388904208",
    "type": "article"
  },
  {
    "title": "Efficient address remapping in distributed shared-memory systems",
    "doi": "https://doi.org/10.1145/1138035.1138039",
    "publication_date": "2006-06-01",
    "publication_year": 2006,
    "authors": "Lixin Zhang; Mike Parker; John Carter",
    "corresponding_authors": "",
    "abstract": "As processor performance continues to improve at a rate much higher than DRAM and network performance, we are approaching a time when large-scale distributed shared memory systems will have remote memory latencies measured in tens of thousands of processor cycles. The Impulse memory system architecture adds an optional level of address indirection at the memory controller. Applications can use this level of indirection to control how data is accessed and cached and thereby improve cache and bus utilization and reduce the number of memory accesses required. Previous Impulse work focuses on uniprocessor systems and relies on software to flush processor caches when necessary to ensure data coherence. In this paper, we investigate an extension of Impulse to multiprocessor systems that extends the coherence protocol to maintain data coherence without requiring software-directed cache flushing. Specifically, the multiprocessor Impulse controller can gather/scatter data across the network while its coherence protocol guarantees that each gather request gets coherent data and each scatter request updates every coherent replica in the system. Our simulation results demonstrate that the proposed system can significantly outperform conventional systems, achieving an average speedup of 9X on four memory-bound benchmarks on a 32-processor system.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W1972098875",
    "type": "article"
  },
  {
    "title": "Recovery code generation for general speculative optimizations",
    "doi": "https://doi.org/10.1145/1132462.1132466",
    "publication_date": "2006-03-01",
    "publication_year": 2006,
    "authors": "Jin Lin; Wei‐Chung Hsu; Pen-Chung Yew; Roy Dz-Ching Ju; Tin‐Fook Ngai",
    "corresponding_authors": "",
    "abstract": "A general framework that integrates both control and data speculation using alias profiling and/or compiler heuristic rules has shown to improve CPU2000 performance on Itanium systems. However, speculative optimizations require check instructions and recovery code to ensure correct execution when speculation fails at runtime. How to generate check instructions and their associated recovery code efficiently and effectively is an issue yet to be well studied. It is also, very important that the recovery code generated in the earlier phases integrate gracefully in the later optimization phases. At the very least, it should not hinder later optimizations, thus, ensuring overall performance improvement. This paper proposes a framework that uses an if-block structure to facilitate check instructions and recovery code generation for general speculative optimizations. It allows speculative instructions and their recovery code generated in the early compiler optimization phases to be integrated effectively with the subsequent optimization phases. It also allows multilevel speculation for multilevel pointers and multilevel expression trees to be handled with no additional complexity. The proposed recovery code generation framework has been implemented and evaluated in the Open Research Compiler (ORC).",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2011232670",
    "type": "article"
  },
  {
    "title": "Virtually split cache",
    "doi": "https://doi.org/10.1145/2541228.2541234",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Dyer Rolán; Basilio B. Fraguela; Ramón Doallo",
    "corresponding_authors": "",
    "abstract": "First-level caches are usually split for both instructions and data instead of unifying them in a single cache. Although that approach eases the pipeline design and provides a simple way to independently treat data and instructions, its global hit rate is usually smaller than that of a unified cache. Furthermore, unified lower-level caches usually behave and process memory requests disregarding whether they are data or instruction requests. In this article, we propose a new technique aimed to balance the amount of space devoted to instructions and data for optimizing set-associative caches: the Virtually Split Cache or VSC. Our technique combines the sharing of resources from unified approaches with the bandwidth and parallelism that split configurations provide, thus reducing power consumption while not degrading performance. Our design dynamically adjusts cache resources devoted to instructions and data depending on their particular demand. Two VSC designs are proposed in order to track the instructions and data requirements. The Shadow Tag VSC (ST-VSC) is based on shadow tags that store the last evicted line related to data and instructions in order to determine how well the cache would work with one more way per set devoted to each kind. The Global Selector VSC (GS-VSC) uses a saturation counter that is updated every time a cache miss occurs either under an instruction or data request applying a duel-like mechanism. Experiments with a variable and a fixed latency VSC show that ST-VSC and GS-VSC reduce on average the cache hierarchy power consumption by 29% and 24%, respectively, with respect to a standard baseline. As for performance, while the fixed latency designs virtually match the split baseline in a single-core system, a variable latency ST-VSC and GS-VSC increase the average IPC by 2.5% and 2%, respectively. In multicore systems, even the slower fixed latency ST-VSC and GS-VSC designs improve the baseline IPC by 3.1% and 2.5%, respectively, in a four-core system thanks to the reduction in the bandwidth demanded from the lower cache levels. This is in contrast with many techniques that trade performance degradation for power consumption reduction. VSC particularly benefits embedded processors with a single level of cache, where up to an average 9.2% IPC improvement is achieved. Interestingly, we also find that partitioning the LLC for instructions and data can improve performance around 2%.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W1978696436",
    "type": "article"
  },
  {
    "title": "Revisiting memory management on virtualized environments",
    "doi": "https://doi.org/10.1145/2541228.2555304",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Xiaolin Wang; Lingmei Weng; Zhenlin Wang; Yingwei Luo",
    "corresponding_authors": "",
    "abstract": "With the evolvement of hardware, 64-bit Central Processing Units (CPUs) and 64-bit Operating Systems (OSs) have dominated the market. This article investigates the performance of virtual memory management of Virtual Machines (VMs) with a large virtual address space in 64-bit OSs, which imposes different pressure on memory virtualization than 32-bit systems. Each of the two conventional memory virtualization approaches, Shadowing Paging (SP) and Hardware-Assisted Paging (HAP), causes different overhead for different applications. Our experiments show that 64-bit applications prefer to run in a VM using SP, while 32-bit applications do not have a uniform preference between SP and HAP. In this article, we trace this inconsistency between 32-bit applications and 64-bit applications to its root cause through a systematic empirical study in Linux systems and discover that the major overhead of SP results from memory management in the 32-bit GNU C library ( glibc ). We propose enhancements to the existing memory management algorithms, which substantially reduce the overhead of SP. Based on the evaluations using SPEC CPU2006, Parsec 2.1, and cloud benchmarks, our results show that SP, with the improved memory allocators, can compete with HAP in almost all cases, in both 64-bit and 32-bit systems. We conclude that without a significant breakthrough in HAP, researchers should pay more attention to SP, which is more flexible and cost effective.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W1989096443",
    "type": "article"
  },
  {
    "title": "An integrated pseudo-associativity and relaxed-order approach to hardware transactional memory",
    "doi": "https://doi.org/10.1145/2400682.2400701",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Zhichao Yan; Hong Jiang; Yujuan Tan; Dan Feng",
    "corresponding_authors": "",
    "abstract": "Our experimental study and analysis reveal that the bottlenecks of existing hardware transactional memory systems are largely rooted in the extra data movements in version management and in the inefficient scheduling of conflicting transactions in conflict management, particularly in the presence of high-contention and coarse-grained applications. In order to address this problem, we propose an integrated Pseudo-Associativity and Relaxed-Order approach to hardware Transactional Memory, called PARO-TM. It exploits the extra pseudo-associative space in the data cache to hold the new value of each transactional modification, and maintains the mappings between the old and new versions via an implicit pseudo-associative hash algorithm (i.e., by inverting the specific bit of the SET index). PARO-TM can branch out the speculative version from the old version upon each transactional modification on demand without a dedicated hardware component to hold the uncommitted data. This means that it is able to automatically access the proper version upon the transaction's commit or abort. Moreover, PARO-TM augments multi-version support in a chained directory to schedule conflicting transactions in a relaxed-order manner to further reduce their overheads. We compare PARO-TM with the state-of-the-art LogTM-SE, TCC, DynTM, and SUV-TM systems and find that PARO-TM consistently outperforms these four representative HTMs. This performance advantage of PARO-TM is far more pronounced under the high-contention and coarse-grained applications in the STAMP benchmark suite, for which PARO-TM is motivated and designed.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2030628323",
    "type": "article"
  },
  {
    "title": "Efficient liveness computation using merge sets and DJ-graphs",
    "doi": "https://doi.org/10.1145/2086696.2086706",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Dibyendu Das; Benoît Dupont de Dinechin; Ramakrishna Upadrasta",
    "corresponding_authors": "",
    "abstract": "In this work we devise an efficient algorithm that computes the liveness information of program variables. The algorithm employs SSA form and DJ-graphs as representation to build Merge sets. The Merge set of node n , M ( n ) is based on the structure of the Control Flow Graph (CFG) and consists of all nodes where a ϕ-function needs to be placed, if a definition of a variable appears in n . The merge sets of a CFG can be computed using DJ-graphs without prior knowledge of how the variables are used and defined. Later, we can answer the liveness query (as a part of other optimization or analysis phase) by utilizing the knowledge of the use/def of variables, the dominator tree and the pre-computed merge sets. On average, merge sets have been shown to be of size comparable to the Dominance Frontier(DF) set of a CFG and can be computed efficiently for all kinds of applications consisting of both reducible and irreducible loops. This is an advantage over existing algorithms which require additional complexities while handling applications using irreducible loops. For cases where the merge sets have already been created during the SSA construction step, the cost of our algorithm reduces even further when we use these merge sets for liveness computation. We have compared our new algorithm with a recent algorithm for computing liveness based on SSA form, and show how it performs better in practice, though being simpler to understand and implement.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2038979313",
    "type": "article"
  },
  {
    "title": "Static analysis of the worst-case memory performance for irregular codes with indirections",
    "doi": "https://doi.org/10.1145/2355585.2355593",
    "publication_date": "2012-09-01",
    "publication_year": 2012,
    "authors": "Diego Andrade; Basilio B. Fraguela; Ramón Doallo",
    "corresponding_authors": "",
    "abstract": "Real-time systems are subject to timing constraints, whose upper bound is given by the Worst-Case Execution Time (WCET). Cache memory behavior is difficult to predict analytically and estimating a safe and precise worst-case value is even more challenging. The worst-case memory performance (WCMP) component of the WCET can only be estimated with the precise knowledge of the stream of data addresses accessed by the code, which is determined by the access patterns and the base addresses of the data structures accessed. The regularity of strided access patterns simplifies their analysis, as they are characterized by relatively few parameters, which are often available at compile time. Unfortunately codes may exhibit irregular access patterns, which are much more difficult to statically analyze. As for the base addresses of the data structures, they are not always available at compile-time for many reasons: stack variables, dynamically allocated memory, modules compiled separately, etc. This article addresses these problems by presenting a model that predicts an %safe and upper bound of the data cache performance for codes both with regular and irregular access patterns, which is valid for any possible base addresses of the data structures. The model analyzes irregular access patterns due to the presence of indirections in the code and it can provide two kinds of predictions: a safe hard boundary that is suitable for hard real-time systems and a soft boundary whose safeness is not guaranteed but which is valid most of the times. In fact, in all our experiments the number of misses was below the soft boundary predicted by the model. This turns this soft boundary prediction into a valuable tool, particularly for non and soft real-time systems, which tolerate a percentage of the runs exceeding their deadlines.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2071972373",
    "type": "article"
  },
  {
    "title": "Near-Optimal Microprocessor and Accelerators Codesign with Latency and Throughput Constraints",
    "doi": "https://doi.org/10.1145/2459316.2459317",
    "publication_date": "2013-05-01",
    "publication_year": 2013,
    "authors": "Angeliki Kritikakou; Francky Catthoor; George S. Athanasiou; Vasilios Kelefouras; Costas E. Goutis",
    "corresponding_authors": "",
    "abstract": "A systematic methodology for near-optimal software/hardware codesign mapping onto an FPGA platform with microprocessor and HW accelerators is proposed. The mapping steps deal with the inter-organization, the foreground memory management, and the datapath mapping. A step is described by parameters and equations combined in a scalable template. Mapping decisions are propagated as design constraints to prune suboptimal options in next steps. Several performance-area Pareto points are produced by instantiating the parameters. To evaluate our methodology we map a real-time bio-imaging application and loop-dominated benchmarks.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2096989239",
    "type": "article"
  },
  {
    "title": "Maintaining performance on power gating of microprocessor functional units by using a predictive pre-wakeup strategy",
    "doi": "https://doi.org/10.1145/2019608.2019615",
    "publication_date": "2011-10-01",
    "publication_year": 2011,
    "authors": "C. Yeh; Kuei‐Chung Chang; Tien-Fu Chen; Chingwei Yeh",
    "corresponding_authors": "",
    "abstract": "Power gating is an effective technique for reducing leakage power in deep submicron CMOS technology. Microarchitectural techniques for power gating of functional units have been developed by detecting suitable idle regions and turning them off to reduce leakage energy consumption; however, wakeup of functional units is needed when instructions are ready for execution such that wakeup overhead is naturally incurred. This study presents time-based power gating with reference pre-wakeup (PGRP), a novel predictive strategy that detects suitable idle periods for power gating and then enables pre-wakeup of needed functional units for avoiding wakeup overhead. The key insight is that most wakeups are repeated due to program locality. Thus, the pre-wakeup predictor learns the wakeup events and selects which prior branch instruction can provide early wakeup (wakeup patterns are visible); these information are then used to adequately prepare available functional units for instruction execution. Simulation results with benchmarks from SPEC2000 applications show that substantial leakage energy reduction with negligible performance degradation (0.38% on average) is worthwhile.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2122442934",
    "type": "article"
  },
  {
    "title": "Automatic parallelization of fine-grained metafunctions on a chip multiprocessor",
    "doi": "https://doi.org/10.1145/2541228.2541237",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Sang-Hoon Lee; James Tuck",
    "corresponding_authors": "",
    "abstract": "Due to the importance of reliability and security, prior studies have proposed inlining metafunctions into applications for detecting bugs and security vulnerabilities. However, because these software techniques add frequent, fine-grained instrumentation to programs, they often incur large runtime overheads. In this work, we consider an automatic thread extraction technique for removing these fine-grained checks from a main application and scheduling them on helper threads. In this way, we can leverage the resources available on a CMP to reduce the latency and overhead of fine-grained checking codes. Our parallelization strategy extracts metafunctions from a single threaded application and executes them in customized helper threads—threads constructed to mirror relevant fragments of the main program’s behavior in order to keep communication and overhead low. To get good performance, we consider optimizations that reduce communication and balance work among many threads. We evaluate our parallelization strategy on Mudflap, a pointer-use checking tool in GCC. To show the benefits of our technique, we compare it to a manually parallelized version of Mudflap. We run our experiments on an architectural simulator with support for fast queueing operations. On a subset of SPECint 2000, our automatically parallelized code using static load balance is only 19% slower, on average, than the manually parallelized version on a simulated eight-core system. In addition, our automatically parallelized code using dynamic load balance is competitive, on average, to the manually parallelized version on a simulated eight-core system. Furthermore, all the applications except parser achieve better speedups with our automatic algorithms than with the manual approach. Also, our approach introduces very little overhead in the main program—it is kept under 100%, which is more than a 5.3× reduction compared to serial Mudflap.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2621588992",
    "type": "article"
  },
  {
    "title": "ReSense",
    "doi": "https://doi.org/10.1145/2555289.2555298",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Tanima Dey; Wei Wang; Jack W. Davidson; Mary Lou Soffa",
    "corresponding_authors": "",
    "abstract": "To utilize the full potential of modern chip multiprocessors and obtain scalable performance improvements, it is critical to mitigate resource contention created by multithreaded workloads. In this article, we describe ReSense, the first runtime system that uses application characteristics to dynamically map multithreaded applications from dynamic workloads—workloads where multithreaded applications arrive, execute, and terminate continuously in unpredictable ways. ReSense mitigates contention for the shared resources in the memory hierarchy by applying a novel thread-mapping algorithm that dynamically adjusts the mapping of threads from dynamic workloads using a precalculated sensitivity score. The sensitivity score quantifies an application's sensitivity to sharing a particular memory resource and is calculated by an efficient characterization process that involves running the multithreaded application by itself on the target platform. To measure ReSense's effectiveness, sensitivity scores were determined for 21 benchmarks from PARSEC-2.1 and NPB-OMP-3.3 for the shared resources in the memory hierarchy on four different platforms. Using three different-sized dynamic workloads composed of randomly selected two, four, and eight corunning benchmarks with randomly selected start times, ReSense was able to improve the average response time of the three workloads by up to 27.03%, 20.89%, and 29.34% and throughput by up to 19.97%, 46.56%, and 29.86%, respectively, over the native OS on real hardware. By estimating and comparing ReSense's effectiveness with the optimal thread mapping for two different workloads, we found that the maximum average difference with the experimentally determined optimal performance was 1.49% for average response time and 2.08% for throughput.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4230815132",
    "type": "article"
  },
  {
    "title": "Reducing DRAM row activations with eager read/write clustering",
    "doi": "https://doi.org/10.1145/2555289.2555300",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Myeongjae Jeon; Conglong Li; Alan L. Cox; Scott Rixner",
    "corresponding_authors": "",
    "abstract": "This article describes and evaluates a new approach to optimizing DRAM performance and energy consumption that is based on eagerly writing dirty cache lines to DRAM. Under this approach, many dirty cache lines are written to DRAM before they are evicted. In particular, dirty cache lines that have not been recently accessed are eagerly written to DRAM when the corresponding row has been activated by an ordinary, noneager access, such as a read. This approach enables clustering of reads and writes that target the same row, resulting in a significant reduction in row activations. Specifically, for a variety of applications, it reduces the number of DRAM row activations by an average of 42% and a maximum of 82%. Moreover, the results from a full-system simulator show compelling performance improvements and energy consumption reductions. Out of 23 applications, 6 have overall performance improvements between 10% and 20%, and 3 have improvements in excess of 20%. Furthermore, 12 consume between 10% and 20% less DRAM energy, and 7 have energy consumption reductions in excess of 20%.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4234632414",
    "type": "article"
  },
  {
    "title": "Techniques to improve performance in requester-wins hardware transactional memory",
    "doi": "https://doi.org/10.1145/2555289.2555299",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Adrià Armejach; Rubén Titos-Gil; Anurag Negi; Osman Ünsal; Adrián Cristal",
    "corresponding_authors": "",
    "abstract": "The simplicity of requester-wins Hardware Transactional Memory (HTM) makes it easy to incorporate in existing chip multiprocessors. Hence, such systems are expected to be widely available in the near future. Unfortunately, these implementations are prone to suffer severe performance degradation due to transient and persistent livelock conditions. This article shows that existing techniques are unable to mitigate this degradation effectively. It then proposes and evaluates four novel techniques—two software-based that employ information provided by the hardware and two that require simple core-local hardware additions—which have the potential to boost the performance of requester-wins HTM designs substantially.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4236755874",
    "type": "article"
  },
  {
    "title": "Temporal-based multilevel correlating inclusive cache replacement",
    "doi": "https://doi.org/10.1145/2555289.2555290",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Yingying Tian; Samira Khan; Daniel A. Jiménez",
    "corresponding_authors": "",
    "abstract": "Inclusive caches have been widely used in Chip Multiprocessors (CMPs) to simplify cache coherence. However, they have poor performance compared with noninclusive caches not only because of the limited capacity of the entire cache hierarchy but also due to ignorance of temporal locality of the Last-Level Cache (LLC). Blocks that are highly referenced (referred to as hot blocks) are always hit in higher-level caches (e.g., L1 cache) and are rarely referenced in the LLC. Therefore, they become replacement victims in the LLC. Due to the inclusion property, blocks evicted from the LLC have to also be invalidated from higher-level caches. Invalidation of hot blocks from the entire cache hierarchy introduces costly off-chip misses that makes the inclusive cache perform poorly.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4239970547",
    "type": "article"
  },
  {
    "title": "WADE",
    "doi": "https://doi.org/10.1145/2555289.2555307",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Zhe Wang; Shuchang Shan; Ting Cao; Junli Gu; Yi Xu; Shuai Mu; Yuan Xie; Daniel A. Jiménez",
    "corresponding_authors": "",
    "abstract": "Emerging Non-Volatile Memory (NVM) technologies are explored as potential alternatives to traditional SRAM/DRAM-based memory architecture in future microprocessor design. One of the major disadvantages for NVM is the latency and energy overhead associated with write operations. Mitigation techniques to minimize the write overhead for NVM-based main memory architecture have been studied extensively. However, most prior work focuses on optimization techniques for NVM-based main memory itself, with little attention paid to cache management policies for the Last-Level Cache (LLC).",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4240406975",
    "type": "article"
  },
  {
    "title": "Idiom recognition framework using topological embedding",
    "doi": "https://doi.org/10.1145/2509420.2512431",
    "publication_date": "2013-09-01",
    "publication_year": 2013,
    "authors": "Motohiro Kawahito; Hideaki Komatsu; Takao Moriyama; Hiroshi Inoue; Toshio Nakatani",
    "corresponding_authors": "",
    "abstract": "Modern processors support hardware-assist instructions (such as TRT and TROT instructions on the IBM System z) to accelerate certain functions such as delimiter search and character conversion. Such special instructions are often used in high-performance libraries, but their exploitation in optimizing compilers has been limited. We devised a new idiom recognition technique based on a topological embedding algorithm to detect idiom patterns in the input programs more aggressively than in previous approaches using exact pattern matching. Our approach can detect a pattern even if the code segment does not exactly match the idiom. For example, we can detect a code segment that includes additional code within the idiom pattern. We also propose an instruction simplification for the idiom recognition. This optimization analyzes all of the usages of the output of the optimized code for a specific idiom. If we find that we do not need an actual value for the output but only a value in a subrange, then we can assign a value in that subrange as the output. The code generation can generate faster code with this optimization. We implemented our new idiom recognition approach based on the Java Just-In-Time (JIT) compiler that is part of the J9 Java Virtual Machine, and we supported several important idioms for the special hardware-assist instructions on the IBM System z and on some models of the IBM System p. To demonstrate the effectiveness of our technique, we performed two experiments. The first experiment was to see how many more patterns we can detect compared to the previous approach. The second experiment measured the performance improvements over the previous approaches. For the first experiment, we used the Java Compatibility Kit (JCK) API tests. For the second experiment we used the IBM XML parser, SPECjvm98, and SPCjbb2000. In summary, relative to a baseline implementation using exact pattern matching, our algorithm converted 76% more loops in JCK tests. On a z9, we also observed significant average performance improvement of the XML parser by 54%, of SPECjvm98 by 1.9%, and of SPECjbb2000 by 4.4%. Finally, we observed that the JIT compilation time increased by only 0.32% to 0.44%.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4240467781",
    "type": "article"
  },
  {
    "title": "Adaptive communication mechanism for accelerating MPI functions in NoC-based multicore processors",
    "doi": "https://doi.org/10.1145/2509420.2512434",
    "publication_date": "2013-09-01",
    "publication_year": 2013,
    "authors": "Libo Huang; Zhiying Wang; Nong Xiao; Yongwen Wang; Qiang Dou",
    "corresponding_authors": "",
    "abstract": "Multicore designs have emerged as the dominant organization for future high-performance microprocessors. Communication in such designs is often enabled by Networks-on-Chip (NoCs). A new trend in such architectures is to fit a Message Passing Interface (MPI) programming model on NoCs to achieve optimal parallel application performance. A key issue in designing MPI over NoCs is communication protocol, which has not been explored in previous research.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4241421486",
    "type": "article"
  },
  {
    "title": "Time- and space-efficient flow-sensitive points-to analysis",
    "doi": "https://doi.org/10.1145/2555289.2555296",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Rupesh Nasre",
    "corresponding_authors": "Rupesh Nasre",
    "abstract": "Compilation of real-world programs often requires hours. The term nightly build known to industrial researchers is an artifact of long compilation times. Our goal is to reduce the absolute analysis times for large C codes (of the order of millions of lines). Pointer analysis is one of the key analyses performed during compilation. Its scalability is paramount to achieve the efficiency of the overall compilation process and its precision directly affects that of the client analyses. In this work, we design a time- and space-efficient flow-sensitive pointer analysis and parallelize it on graphics processing units. Our analysis proposes to use an extended bloom filter, called multibloom, to store points-to information in an approximate manner and develops an analysis in terms of the operations over the multibloom. Since bloom filter is a probabilistic data structure, we develop ways to gain back the analysis precision. We achieve effective parallelization by achieving memory coalescing, reducing thread divergence, and improving load balance across GPU warps. Compared to a state-of-the-art sequential solution, our parallel version achieves a 7.8 × speedup with less than 5% precision loss on a suite of six large programs. Using two client transformations, we show that this loss in precision only minimally affects a client's precision.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4244329873",
    "type": "article"
  },
  {
    "title": "Fast pattern-specific routing for fat tree networks",
    "doi": "https://doi.org/10.1145/2555289.2555293",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Bogdan Prisacari; Germán Rodríguez; Cyriel Minkenberg; Torsten Hoefler",
    "corresponding_authors": "",
    "abstract": "In the context of eXtended Generalized Fat Tree (XGFT) topologies, widely used in HPC and datacenter network designs, we propose a generic method, based on Integer Linear Programming (ILP), to efficiently determine optimal routes for arbitrary workloads. We propose a novel approach that combines ILP with dynamic programming, effectively reducing the time to solution. Specifically, we divide the network into smaller subdomains optimized using a custom ILP formulation that ensures global optimality of local solutions. Local solutions are then combined into an optimal global solution using dynamic programming. Finally, we demonstrate through a series of extensive benchmarks that our approach scales in practice to networks interconnecting several thousands of nodes, using a single-threaded, freely available linear programming solver on commodity hardware, with the potential for higher scalability by means of commercial, parallel solvers.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4245412393",
    "type": "article"
  },
  {
    "title": "Fast modulo scheduler utilizing patternized routes for coarse-grained reconfigurable architectures",
    "doi": "https://doi.org/10.1145/2555289.2555314",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Wonsub Kim; Yoonseo Choi; Hae-woo Park",
    "corresponding_authors": "",
    "abstract": "Coarse-Grained Reconfigurable Architectures (CGRAs) present a potential of high compute throughput with energy efficiency. A CGRA consists of an array of Functional Units (FUs), which communicate with each other through an interconnect network containing transmission nodes and register files. To achieve high performance from the software solutions mapped onto CGRAs, modulo scheduling of loops is generally employed. One of the key challenges in modulo scheduling for CGRAs is to explicitly handle routings of operands from a source to a destination operations through various routing resources. Existing modulo schedulers for CGRAs are slow because finding a valid routing is generally a searching problem over a large space, even with the guidance of well-defined cost metrics. Applications in traditional embedded multimedia domains are regarded as relatively tolerant to a slow compile time in exchange for a high-quality solution. However, many rapidly growing domains of applications, such as 3D graphics, require a fast compilation. Entrances of CGRAs to these domains have been blocked mainly due to their long compile time. We attack this problem by utilizing patternized routes, for which resources and time slots for a success can be estimated in advance when a source operation is placed. By conservatively reserving predefined resources at predefined time slots, future routings originating from the source operation are guaranteed. Experiments on a real-world 3D graphics benchmark suite show that our scheduler improves the compile time up to 6,000 times while achieving an average 70% throughputs of the state-of-the-art CGRA modulo scheduler, the Edge-centric Modulo Scheduler (EMS).",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4249031323",
    "type": "article"
  },
  {
    "title": "Selecting representative benchmark inputs for exploring microprocessor design spaces",
    "doi": "https://doi.org/10.1145/2555289.2555294",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Maximilien Breughe; Lieven Eeckhout",
    "corresponding_authors": "",
    "abstract": "The design process of a microprocessor requires representative workloads to steer the search process toward an optimum design point for the target application domain. However, considering a broad set of workloads to cover the large space of potential workloads is infeasible given how time-consuming design space exploration typically is. Hence, it is crucial to select a small yet representative set of workloads, which leads to a shorter design cycle while yielding a (near) optimal design.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4250160151",
    "type": "article"
  },
  {
    "title": "Designing on-chip networks for throughput accelerators",
    "doi": "https://doi.org/10.1145/2509420.2512429",
    "publication_date": "2013-09-01",
    "publication_year": 2013,
    "authors": "Ali Bakhoda; John Kim; Tor M. Aamodt",
    "corresponding_authors": "",
    "abstract": "As the number of cores and threads in throughput accelerators such as Graphics Processing Units (GPU) increases, so does the importance of on-chip interconnection network design. This article explores throughput-effective Network-on-Chips (NoC) for future compute accelerators that employ Bulk-Synchronous Parallel (BSP) programming models such as CUDA and OpenCL. A hardware optimization is \"throughput effective\" if it improves parallel application-level performance per unit chip area. We evaluate performance of future looking workloads using detailed closed-loop simulations modeling compute nodes, NoC, and the DRAM memory system. We start from a mesh design with bisection bandwidth balanced to off-chip demand. Accelerator workloads tend to demand high off-chip memory bandwidth which results in a many-to-few traffic pattern when coupled with expected technology constraints of slow growth in pins-per-chip. Leveraging these observations we reduce NoC area by proposing a \"checkerboard\" NoC which alternates between conventional full routers and half routers with limited connectivity. Next, we show that increasing network terminal bandwidth at the nodes connected to DRAM controllers alleviates a significant fraction of the remaining imbalance resulting from the many-to-few traffic pattern. Furthermore, we propose a \"double checkerboard inverted\" NoC organization which takes advantage of channel slicing to reduce area while maintaining the performance improvements of the aforementioned techniques. This organization also has a simpler routing mechanism and improves average application throughput per unit area by 24.3%.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4251690065",
    "type": "article"
  },
  {
    "title": "Evaluator-executor transformation for efficient pipelining of loops with conditionals",
    "doi": "https://doi.org/10.1145/2555289.2555317",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Yeong-Hun Jeong; Seongseok Seo; Jongeun Lee",
    "corresponding_authors": "",
    "abstract": "Control divergence poses many problems in parallelizing loops. While predicated execution is commonly used to convert control dependence into data dependence, it often incurs high overhead because it allocates resources equally for both branches of a conditional statement regardless of their execution frequencies. For those loops with unbalanced conditionals, we propose a software transformation that divides a loop into two or three smaller loops so that the condition is evaluated only in the first loop, while the less frequent branch is executed in the second loop in a way that is much more efficient than in the original loop. To reduce the overhead of extra data transfer caused by the loop fission, we also present a hardware extension for a class of Coarse-Grained Reconfigurable Architectures (CGRAs). Our experiments using MiBench and computer vision benchmarks on a CGRA demonstrate that our techniques can improve the performance of loops over predicated execution by up to 65% (37.5%, on average), when the hardware extension is enabled. Without any hardware modification, our software-only version can improve performance by up to 64% (33%, on average), while simultaneously reducing the energy consumption of the entire CGRA including configuration and data memory by 22%, on average.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4254087851",
    "type": "article"
  },
  {
    "title": "Revisiting memory management on virtualized environments",
    "doi": "https://doi.org/10.1145/2555289.2555304",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Xiaolin Wang; Lingmei Weng; Zhenlin Wang; Yingwei Luo",
    "corresponding_authors": "",
    "abstract": "With the evolvement of hardware, 64-bit Central Processing Units (CPUs) and 64-bit Operating Systems (OSs) have dominated the market. This article investigates the performance of virtual memory management of Virtual Machines (VMs) with a large virtual address space in 64-bit OSs, which imposes different pressure on memory virtualization than 32-bit systems. Each of the two conventional memory virtualization approaches, Shadowing Paging (SP) and Hardware-Assisted Paging (HAP), causes different overhead for different applications. Our experiments show that 64-bit applications prefer to run in a VM using SP, while 32-bit applications do not have a uniform preference between SP and HAP. In this article, we trace this inconsistency between 32-bit applications and 64-bit applications to its root cause through a systematic empirical study in Linux systems and discover that the major overhead of SP results from memory management in the 32-bit GNU C library (glibc). We propose enhancements to the existing memory management algorithms, which substantially reduce the overhead of SP. Based on the evaluations using SPEC CPU2006, Parsec 2.1, and cloud benchmarks, our results show that SP, with the improved memory allocators, can compete with HAP in almost all cases, in both 64-bit and 32-bit systems. We conclude that without a significant breakthrough in HAP, researchers should pay more attention to SP, which is more flexible and cost effective.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4254393583",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2445572",
    "publication_date": "2013-04-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Debugging parallel programs is a well-known difficult problem. A promising method to facilitate debugging parallel programs is using hardware support to achieve deterministic replay on a Chip Multi-Processor (CMP). As a Design-For-Debug (DFD) feature, a ...",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4254941634",
    "type": "paratext"
  },
  {
    "title": "Boosting timestamp-based transactional memory by exploiting hardware cycle counters",
    "doi": "https://doi.org/10.1145/2555289.2555297",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Wenjia Ruan; Yujie Liu; Michael Spear",
    "corresponding_authors": "",
    "abstract": "Time-based transactional memories typically rely on a shared memory counter to ensure consistency. Unfortunately, such a counter can become a bottleneck. In this article, we identify properties of hardware cycle counters that allow their use in place of a shared memory counter. We then devise algorithms that exploit the x86 cycle counter to enable bottleneck-free transactional memory runtime systems. We also consider the impact of privatization safety and hardware ordering constraints on the correctness, performance, and generality of our algorithms.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4255961383",
    "type": "article"
  },
  {
    "title": "Acceleration of Parallel-Blocked QR Decomposition of Tall-and-Skinny Matrices on FPGAs",
    "doi": "https://doi.org/10.1145/3447775",
    "publication_date": "2021-05-10",
    "publication_year": 2021,
    "authors": "José M. Rodrı́guez-Borbón; Junjie Huang; Bryan M. Wong; Walid Najjar",
    "corresponding_authors": "",
    "abstract": "QR decomposition is one of the most useful factorization kernels in modern numerical linear algebra algorithms. In particular, the decomposition of tall-and-skinny matrices (TSMs) has major applications in areas including scientific computing, machine learning, image processing, wireless networks, and numerical methods. Traditionally, CPUs and GPUs have achieved better throughput on these applications by using large cache hierarchies and compute cores running at a high frequency, leading to high power consumption. With the advent of heterogeneous platforms, however, FPGAs are emerging as a promising viable alternative. In this work, we propose a high-throughput FPGA-based engine that has a very high computational efficiency (ratio of achieved to peak throughput) compared to similar QR solvers running on FPGAs. Although comparable QR solvers achieve an efficiency of 36%, our design exhibits an efficiency of 54%. For TSMs, our experimental results show that our design can outperform highly optimized QR solvers running on CPUs and GPUs. For TSMs with more than 50K rows, our design outperforms the Intel MKL solver running on an Intel quad-core processor by a factor of 1.5×. For TSMs containing 256 columns or less, our design outperforms the NVIDIA CUBLAS solver running on a K40 GPU by a factor of 3.0×. In addition to being fast, our design is energy efficient—competing platforms execute up to 0.6 GFLOPS/Joule, whereas our design executes more than 1.0 GFLOPS/Joule.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3161672423",
    "type": "article"
  },
  {
    "title": "Tetris-XL",
    "doi": "https://doi.org/10.1145/1582710.1582713",
    "publication_date": "2009-09-01",
    "publication_year": 2009,
    "authors": "Weifeng Xu; Russell Tessier",
    "corresponding_authors": "",
    "abstract": "As technology has advanced, the application space of Very Long Instruction Word (VLIW) processors has grown to include a variety of embedded platforms. Due to cost and power consumption constraints, many embedded VLIW processors contain limited resources, including registers. As a result, a VLIW compiler that maximizes instruction level parallelism (ILP) without considering register constraints may generate excessive register spills, leading to reduced overall system performance. To address this issue, this article presents a new spill reduction technique that improves VLIW runtime performance by reordering operations prior to register allocation and instruction scheduling. Unlike earlier algorithms, our approach explicitly considers both register reduction and data dependency in performing operation reordering. Data dependency control limits unexpected schedule length increases during subsequent instruction scheduling. Our technique has been evaluated using Trimaran, an academic VLIW compiler, and evaluated using a set of embedded systems benchmarks. Experimental results show that, on average, this technique improves VLIW performance by 10% for VLIW processors with 32 registers and 8 functional units compared with previous spill reduction techniques. Limited improvement is seen versus prior approaches for VLIW processors with 64 registers and 8 functional units.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2094571272",
    "type": "article"
  },
  {
    "title": "Optimizing Affine Control With Semantic Factorizations",
    "doi": "https://doi.org/10.1145/3162017",
    "publication_date": "2017-12-13",
    "publication_year": 2017,
    "authors": "Christophe Alias; Alexandru Plesco",
    "corresponding_authors": "",
    "abstract": "Hardware accelerators generated by polyhedral synthesis techniques make extensive use of affine expressions (affine functions and convex polyhedra) in control and steering logic. Since the control is pipelined, these affine objects must be evaluated at the same time for different values, which forbids aggressive reuse of operators. In this article, we propose a method to factorize a collection of affine expressions without preventing pipelining. Our key contributions are (i) to use semantic factorizations exploiting arithmetic properties of addition and multiplication and (ii) to rely on a cost function whose minimization ensures correct usage of FPGA resources. Our algorithm is totally parameterized by the cost function, which can be customized to fit a target FPGA. Experimental results on a large pool of linear algebra kernels show a significant improvement compared to traditional low-level RTL optimizations. In particular, we show how our method reduces resource consumption by revealing hidden strength reductions.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2739413883",
    "type": "article"
  },
  {
    "title": "Efficient Generation of Compact Execution Traces for Multicore Architectural Simulations",
    "doi": "https://doi.org/10.1145/3106342",
    "publication_date": "2017-08-30",
    "publication_year": 2017,
    "authors": "Ayman Hroub; Muhammad E. S. Elrabaa; Muhamed F. Mudawar; Ahmad Khayyat",
    "corresponding_authors": "",
    "abstract": "Requiring no functional simulation, trace-driven simulation has the potential of achieving faster simulation speeds than execution-driven simulation of multicore architectures. An efficient, on-the-fly, high-fidelity trace generation method for multithreaded applications is reported. The generated trace is encoded in an instruction-like binary format that can be directly “interpreted” by a timing simulator to simulate a general load/store or x8-like architecture. A complete tool suite that has been developed and used for evaluation of the proposed method showed that it produces smaller traces over existing trace compression methods while retaining good fidelity including all threading- and synchronization-related events.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2752214316",
    "type": "article"
  },
  {
    "title": "CG-OoO",
    "doi": "https://doi.org/10.1145/3151034",
    "publication_date": "2017-12-05",
    "publication_year": 2017,
    "authors": "Milad Mohammadi; Tor M. Aamodt; William J. Dally",
    "corresponding_authors": "",
    "abstract": "We introduce the Coarse-Grain Out-of-Order (CG-OoO) general-purpose processor designed to achieve close to In-Order (InO) processor energy while maintaining Out-of-Order (OoO) performance. CG-OoO is an energy-performance-proportional architecture. Block-level code processing is at the heart of this architecture; CG-OoO speculates, fetches, schedules, and commits code at block-level granularity. It eliminates unnecessary accesses to energy-consuming tables and turns large tables into smaller, distributed tables that are cheaper to access. CG-OoO leverages compiler-level code optimizations to deliver efficient static code and exploits dynamic block-level and instruction-level parallelism. CG-OoO introduces Skipahead, a complexity effective, limited out-of-order instruction scheduling model. Through the energy efficiency techniques applied to the compiler and processor pipeline stages, CG-OoO closes 62% of the average energy gap between the InO and OoO baseline processors at the same area and nearly the same performance as the OoO. This makes CG-OoO 1.8× more efficient than the OoO on the energy-delay product inverse metric. CG-OoO meets the OoO nominal performance while trading off the peak scheduling performance for superior energy efficiency.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2775727119",
    "type": "article"
  },
  {
    "title": "PETRA",
    "doi": "https://doi.org/10.1145/3446391",
    "publication_date": "2021-03-08",
    "publication_year": 2021,
    "authors": "Ramin Izadpanah; Christina Peterson; Yan Solihin; Damian Dechev",
    "corresponding_authors": "",
    "abstract": "Emerging byte-addressable Non-Volatile Memories (NVMs) enable persistent memory where process state can be recovered after crashes. To enable applications to rely on persistent data, durable data structures with failure-atomic operations have been proposed. However, they lack the ability to allow users to execute a sequence of operations as transactions. Meanwhile, persistent transactional memory (PTM) has been proposed by adding durability to Software Transactional Memory (STM). However, PTM suffers from high performance overheads and low scalability due to false aborts, logging, and ordering constraints on persistence. In this article, we propose PETRA, a new approach for constructing persistent transactional linked data structures. PETRA natively supports transactions, but unlike PTM, relies on the high-level information from the data structure semantics. This gives PETRA unique advantages in the form of high performance and high scalability. Our experimental results using various benchmarks demonstrate the scalability of PETRA in all workloads and transaction sizes. PETRA outperforms the state-of-the-art PTMs by an order of magnitude in transactions of size greater than one, and demonstrates superior performance in transactions of size one.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3134062784",
    "type": "article"
  },
  {
    "title": "PICO",
    "doi": "https://doi.org/10.1145/3460434",
    "publication_date": "2021-07-17",
    "publication_year": 2021,
    "authors": "Tina Jung; Fabian Ritter; Sebastian Hack",
    "corresponding_authors": "",
    "abstract": "Memory safety violations such as buffer overflows are a threat to security to this day. A common solution to ensure memory safety for C is code instrumentation. However, this often causes high execution-time overhead and is therefore rarely used in production. Static analyses can reduce this overhead by proving some memory accesses in bounds at compile time. In practice, however, static analyses may fail to verify in-bounds accesses due to over-approximation. Therefore, it is important to additionally optimize the checks that reside in the program. In this article, we present PICO, an approach to eliminate and replace in-bounds checks. PICO exactly captures the spatial memory safety of accesses using Presburger formulas to either verify them statically or substitute existing checks with more efficient ones. Thereby, PICO can generate checks of which each covers multiple accesses and place them at infrequently executed locations. We evaluate our LLVM-based PICO prototype with the well-known SoftBound instrumentation on SPEC benchmarks commonly used in related work. PICO reduces the execution-time overhead introduced by SoftBound by 36% on average (and the code-size overhead by 24%). Our evaluation shows that the impact of substituting checks dominates that of removing provably redundant checks.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3185662757",
    "type": "article"
  },
  {
    "title": "CIB-HIER",
    "doi": "https://doi.org/10.1145/3468062",
    "publication_date": "2021-07-17",
    "publication_year": 2021,
    "authors": "Cunlu Li; Dezun Dong; Shazhou Yang; Xiangke Liao; Guangyu Sun; Yongheng Liu",
    "corresponding_authors": "",
    "abstract": "Hierarchical organization is widely used in high-radix routers to enable efficient scaling to higher switch port count. A general-purpose hierarchical router must be symmetrically designed with the same input buffer depth, resulting in a large amount of unused input buffers due to the different link lengths. Sharing input buffers between different input ports can improve buffer utilization, but the implementation overhead also increases with the number of shared ports. Previous work allowed input buffers to be shared among all router ports, which maximizes the buffer utilization but also introduces higher implementation complexity. Moreover, such design can impair performance when faced with long packets, due to the head-of-line blocking in intermediate buffers. In this work, we explain that sharing unused buffers between a subset of router ports is a more efficient design. Based on this observation, we propose Centralized Input Buffer Design in Hierarchical High-radix Routers (CIB-HIER), a novel centralized input buffer design for hierarchical high-radix routers. CIB-HIER integrates multiple input ports onto a single tile and organizes all unused input buffers in the tile as a centralized input buffer. CIB-HIER only allows the centralized input buffer to be shared between ports on the same tile, without introducing additional intermediate virtual channels or global scheduling circuits. Going beyond the basic design of CIB-HIER, the centralized input buffer can be used to relieve the head-of-line blocking caused by shallow intermediate buffers, by stashing long packets in the centralized input buffer. Experimental results show that CIB-HIER is highly effective and can significantly increase the throughput of high-radix routers.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3186815770",
    "type": "article"
  },
  {
    "title": "Reducing complexity of multiobjective design space exploration in VLIW-based embedded systems",
    "doi": "https://doi.org/10.1145/1400112.1400116",
    "publication_date": "2008-08-01",
    "publication_year": 2008,
    "authors": "Vincenzo Catania; Maurizio Palesi; Davide Patti",
    "corresponding_authors": "",
    "abstract": "Architectures based on very-long instruction word (VLIW) have found fertile ground in multimedia electronic appliances thanks to their ability to exploit high degrees of instruction level parallelism (ILP) with a reasonable trade-off in complexity and silicon cost. Specialization of such architectures involves the configuration of both hardware-related aspects (e.g., register files, functional units, memory subsystem) and software-related issues (e.g., the compilation strategy). The complex interactions between the components of such systems will force a human designer to rely on judgment and experience in designing them, possibly eliminating interesting configurations, and making tuning of the system, for either power, energy, or performance, difficult. In this paper we propose tools and methodologies to efficiently cope with this complexity from a multiobjective perspective. We first analyze the impact of ILP-oriented code transformations using two alternative compilation profiles to quantitatively show the effect of such transformations on typical design objectives like performance, power dissipation, and energy consumption. Next, by means of statistical analysis, we collect useful data to predict the effectiveness of a given compilation profiles for a specific application. Information gathered from such analysis can be exploited to drastically reduce the computational effort needed to perform the design space exploration.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2069578061",
    "type": "article"
  },
  {
    "title": "SelSMaP",
    "doi": "https://doi.org/10.1145/3274650",
    "publication_date": "2018-10-10",
    "publication_year": 2018,
    "authors": "Jiajun Wang; Reena Panda; Lizy K. John",
    "corresponding_authors": "",
    "abstract": "Data prefetching, which intelligently loads data closer to the processor before demands, is a popular cache performance optimization technique to address the increasing processor-memory performance gap. Although prefetching concepts have been proposed for decades, sophisticated system architecture and emerging applications introduce new challenges. Large instruction windows coupled with out-of-order execution makes the program data access sequence distorted from a cache perspective. Furthermore, big data applications stress memory subsystems heavily with their large working set sizes and complex data access patterns. To address such challenges, this work proposes a high-performance hardware prefetching scheme, SelSMaP. SelSMaP is able to detect both regular and nonuniform stride patterns by taking the minimum observed address offset (called a reference stride) as a heuristic. A stride masking is generated according to the reference stride and is to filter out history accesses whose pattern can be rephrased as uniform stride accesses. Prefetching decision and prefetch degree are determined based on the masking outcome. As SelSMaP prediction logic does not rely on the chronological order of data accesses or program counter information, it is able to unveil the effect of out-of-order execution and compiler optimization. We evaluated SelSMaP with CloudSuite workloads and SPEC CPU2006 benchmarks. SelSMaP achieves an average CloudSuite performance improvement of 30% over nonprefetching systems. With one to two orders of magnitude less storage and much less functional logic, SelSMaP outperforms the highest-performing prefetcher by 8.6% in CloudSuite workloads.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2898679695",
    "type": "article"
  },
  {
    "title": "GenMatcher",
    "doi": "https://doi.org/10.1145/3281663",
    "publication_date": "2018-11-16",
    "publication_year": 2018,
    "authors": "Ping Wang; Luke McHale; Paul V. Gratz; Alex Sprintson",
    "corresponding_authors": "",
    "abstract": "Packet classification methods rely upon packet content/header matching against rules. Thus, throughput of matching operations is critical in many networking applications. Further, with the advent of Software Defined Networking (SDN), efficient implementation of software approaches to matching are critical for the overall system performance. This article presents 1 GenMatcher, a generic, software-only, arbitrary matching framework for fast, efficient searches. The key idea of our approach is to represent arbitrary rules with efficient prefix-based tries. To support arbitrary wildcards, we rearrange bits within the rules such that wildcards accumulate to one side of the bitstring. Since many non-contiguous wildcards often remain, we use multiple prefix-based tries. The main challenge in this context is to generate efficient trie groupings and expansions to support all arbitrary rules. Finding an optimal mix of grouping and expansion is an NP-complete problem. Our contribution includes a novel, clustering-based grouping algorithm to group rules based upon their bit-level similarities. Our algorithm generates near-optimal trie groupings with low configuration times and provides significantly higher match throughput compared to prior techniques. Experiments with synthetic traffic show that our method can achieve a 58.9X speedup compared to the baseline on a single core processor under a given memory constraint.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2901083288",
    "type": "article"
  },
  {
    "title": "RAGuard",
    "doi": "https://doi.org/10.1145/3280852",
    "publication_date": "2018-11-16",
    "publication_year": 2018,
    "authors": "Jun Zhang; Rui Hou; Wei Song; Sally A. McKee; Zhen Jia; Zheng Chen; Mingyu Chen; Lixin Zhang; Dan Meng",
    "corresponding_authors": "",
    "abstract": "Control-flow integrity (CFI) is a general method for preventing code-reuse attacks, which utilize benign code sequences to achieve arbitrary code execution. CFI ensures that the execution of a program follows the edges of its predefined static Control-Flow Graph: any deviation that constitutes a CFI violation terminates the application. Despite decades of research effort, there are still several implementation challenges in efficiently protecting the control flow of function returns (Return-Oriented Programming attacks). The set of valid return addresses of frequently called functions can be large and thus an attacker could bend the backward-edge CFI by modifying an indirect branch target to another within the valid return set. This article proposes RAGuard, an efficient and user-transparent hardware-based approach to prevent Return-Oreiented Programming attacks. RAGuard binds a message authentication code (MAC) to each return address to protect its integrity. To guarantee the security of the MAC and reduce runtime overhead: RAGuard (1) computes the MAC by encrypting the signature of a return address with AES-128, (2) develops a key management module based on a Physical Unclonable Function (PUF) and a True Random Number Generator (TRNG), and (3) uses a dedicated register to reduce MACs’ load and store operations of leaf functions. We have evaluated our mechanism based on the open-source LEON3 processor and the results show that RAGuard incurs acceptable performance overhead and occupies reasonable area.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2901207456",
    "type": "article"
  },
  {
    "title": "Memory-access-aware Safety and Profitability Analysis for Transformation of Accelerator-bound OpenMP Loops",
    "doi": "https://doi.org/10.1145/3333060",
    "publication_date": "2019-07-18",
    "publication_year": 2019,
    "authors": "Artem Chikin; Taylor Lloyd; José Nelson Amaral; Ettore Tiotto; Muhammad Usman",
    "corresponding_authors": "",
    "abstract": "Iteration Point Difference Analysis is a new static analysis framework that can be used to determine the memory coalescing characteristics of parallel loops that target GPU offloading and to ascertain safety and profitability of loop transformations with the goal of improving their memory access characteristics. This analysis can propagate definitions through control flow, works for non-affine expressions, and is capable of analyzing expressions that reference conditionally defined values. This analysis framework enables safe and profitable loop transformations. Experimental results demonstrate potential for dramatic performance improvements. GPU kernel execution time across the Polybench suite is improved by up to 25.5× on an Nvidia P100 with benchmark overall improvement of up to 3.2×. An opportunity detected in a SPEC ACCEL benchmark yields kernel speedup of 86.5× with a benchmark improvement of 3.3×. This work also demonstrates how architecture-aware compilers improve code portability and reduce programmer effort.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2962989815",
    "type": "article"
  },
  {
    "title": "FailAmp",
    "doi": "https://doi.org/10.1145/3369381",
    "publication_date": "2019-12-18",
    "publication_year": 2019,
    "authors": "Ian Briggs; Arnab Das; Mark Baranowski; Vishal Sharma; Sriram Krishnamoorthy; Zvonimir Rakamarić; Ganesh Gopalakrishnan",
    "corresponding_authors": "",
    "abstract": "We present FailAmp, a novel LLVM program transformation algorithm that makes programs employing structured index calculations more robust against soft errors. Without FailAmp, an offset error can go undetected; with FailAmp, all subsequent offsets are relativized, building on the faulty one. FailAmp can exploit ISAs such as ARM to further reduce overheads. We verify correctness properties of FailAMP using an SMT solver, and present a thorough evaluation using many high-performance computing benchmarks under a fault injection campaign. FailAmp provides full soft-error detection for address calculation while incurring an average overhead of around 5%.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2996635007",
    "type": "article"
  },
  {
    "title": "Camouflage: Utility-Aware Obfuscation for Accurate Simulation of Sensitive Program Traces",
    "doi": "https://doi.org/10.1145/3650110",
    "publication_date": "2024-02-29",
    "publication_year": 2024,
    "authors": "Asmita Pal; Keerthana Desai; Rahul Chatterjee; Joshua San Miguel",
    "corresponding_authors": "",
    "abstract": "Trace-based simulation is a widely used methodology for system design exploration. It relies on realistic traces that represent a range of behaviors necessary to be evaluated, containing a lot of information about the application, its inputs and the underlying system on which it was generated. Consequently, generating traces from real-world executions risks leakage of sensitive information. To prevent this, traces can be obfuscated before release. However, this can undermine their ideal utility, i.e., how realistically a program behavior was captured. To address this, we propose Camouflage, a novel obfuscation framework, designed with awareness of the necessary architectural properties required to preserve trace utility , while ensuring secrecy of the inputs used to generate the trace. Focusing on memory access traces, our extensive evaluation on various benchmarks shows that camouflaged traces preserve the performance measurements of the original execution, with an average τ correlation of 0.66. We model input secrecy as an input indistinguishability problem and show that the average security loss is 7.8%, which is better than traces generated from the state-of-the-art.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4392285534",
    "type": "article"
  },
  {
    "title": "ReSA: Reconfigurable Systolic Array for Multiple Tiny DNN Tensors",
    "doi": "https://doi.org/10.1145/3653363",
    "publication_date": "2024-03-21",
    "publication_year": 2024,
    "authors": "Ching-Jui Lee; Tsung Tai Yeh",
    "corresponding_authors": "",
    "abstract": "Systolic array architecture has significantly accelerated deep neural networks (DNNs). A systolic array comprises multiple processing elements (PEs) that can perform multiply-accumulate (MAC). Traditionally, the systolic array can execute a certain amount of tensor data that matches the size of the systolic array simultaneously at each cycle. However, hyper-parameters of DNN models differ across each layer and result in various tensor sizes in each layer. Mapping these irregular tensors to the systolic array while fully utilizing the entire PEs in a systolic array is challenging. Furthermore, modern DNN systolic accelerators typically employ a single dataflow. However, such a dataflow is not optimal for every DNN model. This work proposes ReSA, a reconfigurable dataflow architecture that aims to minimize the execution time of a DNN model by mapping tiny tensors on the spatially partitioned systolic array. Unlike conventional systolic array architectures, the ReSA data path controller enables the execution of the input, weight, and output-stationary dataflow on PEs. ReSA also decomposes the coarse-grain systolic array into multiple small ones to reduce the fragmentation issue on the tensor mapping. Each small systolic sub-array unit relies on our data arbiter to dispatch tensors to each other through the simple interconnected network. Furthermore, ReSA reorders the memory access to overlap the memory load and execution stages to hide the memory latency when tackling tiny tensors. Finally, ReSA splits tensors of each layer into multiple small ones and searches for the best dataflow for each tensor on the host side. Then, ReSA encodes the predefined dataflow in our proposed instruction to notify the systolic array to switch the dataflow correctly. As a result, our optimization on the systolic array architecture achieves a geometric mean speedup of 1.87× over the weight-stationary systolic array architecture across nine different DNN models.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4393042652",
    "type": "article"
  },
  {
    "title": "An Example of Parallel Merkle Tree Traversal: Post-Quantum Leighton-Micali Signature on the GPU",
    "doi": "https://doi.org/10.1145/3659209",
    "publication_date": "2024-04-16",
    "publication_year": 2024,
    "authors": "Ziheng Wang; Xiaoshe Dong; Yan Kang; Chen Heng; Qiang Wang",
    "corresponding_authors": "",
    "abstract": "The hash-based signature (HBS) is the most conservative and time-consuming among many post-quantum cryptography (PQC) algorithms. Two HBSs, LMS and XMSS, are the only PQC algorithms standardised by the National Institute of Standards and Technology (NIST) now. Existing HBSs are designed based on serial Merkle tree traversal, which is not conducive to taking full advantage of the computing power of parallel architectures such as CPUs and GPUs. We propose a parallel Merkle tree traversal (PMTT), which is tested by implementing LMS on the GPU. This is the first work accelerating LMS on the GPU, which performs well even with over 10,000 cores. Considering different scenarios of algorithmic parallelism and data parallelism, we implement corresponding variants for PMTT. The design of PMTT for algorithmic parallelism mainly considers the execution efficiency of a single task, while that for data parallelism starts with the full utilisation of GPU performance. In addition, we are the first to design a CPU-GPU collaborative processing solution for traversal algorithms to reduce the communication overhead between CPU and GPU. For algorithmic parallelism, our implementation is still 4.48× faster than the ideal time of the state-of-the-art traversal algorithm. For data parallelism, when the number of cores increases from 1 to 8,192, the parallel efficiency is 78.39%. In comparison, our LMS implementation outperforms most existing LMS and XMSS implementations.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4394844951",
    "type": "article"
  },
  {
    "title": "Intermediate Address Space: virtual memory optimization of heterogeneous architectures for cache-resident workloads",
    "doi": "https://doi.org/10.1145/3659207",
    "publication_date": "2024-04-20",
    "publication_year": 2024,
    "authors": "Qunyou Liu; Darong Huang; Luis Costero; Marina Zapater; David Atienza",
    "corresponding_authors": "",
    "abstract": "The increasing demand for computing power and the emergence of heterogeneous computing architectures have driven the exploration of innovative techniques to address current limitations in both the compute and memory subsystems. One such solution is the use of Accelerated Processing Units (APUs), processors that incorporate both a central processing unit (CPU) and an integrated graphics processing unit (iGPU). However, the performance of both APU and CPU systems can be significantly hampered by address translation overhead, leading to a decline in overall performance, especially for cache-resident workloads. To address this issue, we propose the introduction of a new intermediate address space (IAS) in both APU and CPU systems. IAS serves as a bridge between virtual address (VA) spaces and physical address (PA) spaces, optimizing the address translation process. In the case of APU systems, our research indicates that the iGPU suffers from significant translation look-aside buffer (TLB) misses in certain workload situations. Using an IAS, we can divide the initial address translation into front- and back-end phases, effectively shifting the bottleneck in address translation from the cache side to the memory controller side, a technique that proves to be effective for cache-resident workloads. Our simulations demonstrate that implementing IAS in the CPU system can boost performance by up to 40% compared to conventional CPU systems. Furthermore, we evaluate the effectiveness of APU systems, comparing the performance of IAS-based systems with traditional systems, showing up to a 185% improvement in APU system performance with our proposed IAS implementation. Furthermore, our analysis indicates that over 90% of TLB misses can be filtered by the cache, and employing a larger cache within the system could potentially result in even greater improvements. The proposed IAS offers a promising and practical solution to enhance the performance of both APU and CPU systems, contributing to state-of-the-art research in the field of computer architecture.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4394975824",
    "type": "article"
  },
  {
    "title": "CoolDC: A Cost-Effective Immersion-Cooled Datacenter with Workload-Aware Temperature Scaling",
    "doi": "https://doi.org/10.1145/3664925",
    "publication_date": "2024-05-14",
    "publication_year": 2024,
    "authors": "Dongmoon Min; Ilkwon Byun; Gyu-hyeon Lee; Jangwoo Kim",
    "corresponding_authors": "",
    "abstract": "For datacenter architects, it is the most important goal to minimize the datacenter’s total cost of ownership for the target performance (i.e., TCO/performance). As the major component of a datacenter is a server farm, the most effective way of reducing TCO/performance is to improve the server’s performance and power efficiency. To achieve the goal, we claim that it is highly promising to reduce each server’s temperature to its most cost-effective point (or temperature scaling). In this article, we propose CoolDC , a novel and immediately applicable low-temperature cooling method to minimize the datacenter’s TCO. The key idea is to find and apply the most cost-effective sub-freezing temperature to target servers and workloads. For that purpose, we first apply the immersion cooling method to the entire servers to maintain a stable low temperature with little extra cooling and maintenance costs. Second, we define the TCO-optimal temperature for datacenter operation (e.g., 248K~273K (-25℃~0℃)) by carefully estimating all the costs and benefits at low temperatures. Finally, we propose CoolDC, our immersion-cooling datacenter architecture to run every workload at its own TCO-optimal temperature. By incorporating our low-temperature workload-aware temperature scaling, CoolDC achieves 12.7% and 13.4% lower TCO/performance than the conventional air-cooled and immersion-cooled datacenters, respectively, without any modification to existing computers.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4396893445",
    "type": "article"
  },
  {
    "title": "Achieving Tunable Erasure Coding with Cluster-Aware Redundancy Transitioning",
    "doi": "https://doi.org/10.1145/3672077",
    "publication_date": "2024-06-10",
    "publication_year": 2024,
    "authors": "F.-Z. Zhang; Fulin Nan; Binbin Xu; Zhirong Shen; J.Q. Zhai; Dmitrii Kalplun; Jiwu Shu",
    "corresponding_authors": "",
    "abstract": "Erasure coding has been demonstrated as a storage-efficient means against failures, yet its tunability remains a challenging issue in data centers, which is prone to induce substantial cross-cluster traffic. In this paper, we present ClusterRT , a cluster-aware redundancy transitioning approach that can dynamically tailor the redundancy degree of erasure coding in data centers. ClusterRT formulates the data relocation as the maximum flow problem to reduce cross-cluster data transfers. It then designs a parity-coordinated update algorithm, which gathers the parity chunks within the same cluster and leverage encoding dependency to further decrease the cross-cluster update traffic. ClusterRT finally rotates the parity chunks to balance the cross-cluster transitioning traffic across the data center. Large-scale simulation and Alibaba Cloud ECS experiments show that ClusterRT reduces 94.0-96.2% of transitioning traffic and reduces 70.4-88.4% of transitioning time.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4399498881",
    "type": "article"
  },
  {
    "title": "SAL: Optimizing the Dataflow of Spin-based Architectures for Lightweight Neural Networks",
    "doi": "https://doi.org/10.1145/3673654",
    "publication_date": "2024-06-14",
    "publication_year": 2024,
    "authors": "Yunping Zhao; Sheng Ma; Hengzhu Liu; Dongsheng Li",
    "corresponding_authors": "",
    "abstract": "As the Convolutional Neural Network (CNN) goes deeper and more complex, the network becomes memory-intensive and computation-intensive. To address this issue, the lightweight neural network reduces parameters and Multiplication-and-Accumulation (MAC) operations by using the Depthwise Separable Convolution (DSC) to improve speed and efficiency. Nonetheless, the energy efficiency of classical Von Neumann architectures for CNNs is limited due to the memory wall challenge. Spin-based architectures have the potential to address this challenge thanks to the integration of memory and computing with ultra-high energy efficiency. However, deploying the DSC on spin-based architectures with the traditional dataflow leads to huge activation movements and low hardware utilization. Moreover, the inter-layer data dependency of neural networks increases latency. These factors become the bottleneck of improving energy efficiency and performance. Inspired by these challenges, we propose a novel dataflow on Spin-based Architectures for Lightweight neural networks (SAL). The novel dataflow replaces convolution unrolling by selecting activations in the crossbar according to the convolution window and also realizes the inter-layer data reuse. Moreover, the novel dataflow also reduces the latency due to the data dependency between layers, realizing higher performance. To the best of our knowledge, this is the first design to use hybrid dataflow for the PIM architecture. We also optimize the structure of the spin-based crossbar and the pipeline based on the dataflow to achieve better data reuse and computational parallelism. For deploying the MobileNet V1, the novel dataflow improves the hardware utilization by 23×∼ 105× and reduces the data traffic by 1.09×∼ 18.6×. Compared with the NEBULA, a spin-based non-Von Neumann architecture, the SAL reduces the energy consumption by 4× and improves the performance by 7.3×, which are 0.32 mJ and 10.43 GOPs -1 , respectively. Moreover, the SAL improves power efficiency over 29 times more than the NEBULA. Compared with the Eyeriss, the SAL improves the energy efficiency by four orders of magnitude.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4399677075",
    "type": "article"
  },
  {
    "title": "Environmental Condition Aware Super-Resolution Acceleration Framework in Server-Client Hierarchies",
    "doi": "https://doi.org/10.1145/3678008",
    "publication_date": "2024-07-12",
    "publication_year": 2024,
    "authors": "Zhuoran Song; Zhongkai Yu; Xinkai Song; Yifan Hao; Li Jiang; Naifeng Jing; Xiaoyao Liang",
    "corresponding_authors": "",
    "abstract": "In the current landscape, high-resolution (HR) videos have gained immense popularity, promising an elevated viewing experience. Recent research has demonstrated that the video super-resolution (SR) algorithm, empowered by deep neural networks (DNNs), can substantially enhance the quality of HR videos by processing low-resolution (LR) frames. However, the existing DNN models demand significant computational resources, posing challenges for the deployment of SR algorithms on client devices. While numerous accelerators have proposed solutions, their primary focus remains on client-side optimization. In contrast, our research recognizes that the HR video is originally stored in the cloud server and presents an untapped opportunity for achieving both high accuracy and performance improvements. Building on this insight, this article introduces an end-to-end video CODEC-assisted super-resolution (E 2 SR+) algorithm, which tightly integrates the cloud server with the client device to deliver a seamless and real-time video viewing experience. We propose the motion vector search algorithm executed in a cloud server, which can search the motion vectors and residuals for a part of the HR video frames and then pack them as add-ons. We also design an auto-encoder algorithm to down-sample the residuals to save the bitstream cost while guaranteeing the quality of the residuals. Lastly, we propose a reconstruction algorithm performed in the client to quickly reconstruct the corresponding HR frames using the add-ons to skip part of the DNN computations. To implement the E 2 SR+ algorithm, we design corresponding E 2 SR+ architecture in the client, which achieves significant speedup with minimal hardware overhead. Given that the environmental condition varies in the server–client hierarchies, we believe that simply applying E 2 SR+ to all frames is irrational. Accordingly, we offer an environmental condition–aware system to chase the best performance while adapting to the diverse environment. In the system, we design a linear programming (LP) model to simulate the environment and allocate frames to three existing mechanisms. Our experimental results demonstrate that the E 2 SR+ algorithm enhances the peak signal-to-noise ratio by 1.2, 2.5, and 2.3 compared with the state-of-the-art (SOTA) methods EDVR, BasicVSR, and BasicVSR++, respectively. In terms of performance, the E 2 SR+ architecture offers significant improvements over existing SOTA methods. For instance, while BasicVSR++ requires 98 ms on an NVIDIA V100 graphics processing unit (GPU) to generate a 1,280 × 720 HR frame, the E 2 SR+ architecture reduces the execution time to just 39 ms, highlighting the efficiency and effectiveness of our proposed method. Overall, the E 2 SR+ architecture respectively achieves 1.4×, 2.2×, 4.6×, and 442.0× performance improvement compared with ADAS, ISRAcc, the NVIDIA V100 GPU, and a central processing unit. Lastly, the proposed system showcases its superiority and surpasses all the existing mechanisms in terms of execution time when varying environmental conditions.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4400587597",
    "type": "article"
  },
  {
    "title": "A NUMA-Aware Version of an Adaptive Self-Scheduling Loop Scheduler",
    "doi": "https://doi.org/10.1145/3680549",
    "publication_date": "2024-07-26",
    "publication_year": 2024,
    "authors": "Joshua Dennis Booth; Phillip Lane",
    "corresponding_authors": "",
    "abstract": "Parallelizing code in a shared-memory environment is commonly done utilizing loop scheduling (LS) in a fork-join manner as in OpenMP. This manner of parallelization is popular due to its ease to code, but the choice of the LS method is important when the workload per iteration is highly variable. Currently, the shared-memory environment is evolving in high-performance computing as larger chiplet-based processors with high core counts and segmented L3 cache are introduced. These processors have a stronger non-uniform memory access (NUMA) effect than the previous generation of x86-64 processors. This work attempts to modify the adaptive self-scheduling loop scheduler known as iCh ( i rregular Ch unk) for these NUMA environments while analyzing the impact of these systems on default OpenMP LS methods. In particular, iCh is as a default LS method for irregular applications (i.e., applications where the workload per iteration is highly variable) that guarantees “good” performance without tuning. The modified version, named NiCh , is demonstrated over multiple irregular applications to show the variation in performance. The work demonstrates that NiCh is able to better handle architectures with stronger NUMA effects, and particularly is better than iCh when the number of threads is greater than the number of cores. However, NiCh also comes with being less universally “good” than iCh and a set of parameters that are hardware dependent.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4401030239",
    "type": "article"
  },
  {
    "title": "CoNST: Code Generator for Sparse Tensor Networks",
    "doi": "https://doi.org/10.1145/3689342",
    "publication_date": "2024-08-17",
    "publication_year": 2024,
    "authors": "Saurabh Raje; Yufan Xu; Atanas Rountev; Edward F. Valeev; P. Sadayappan",
    "corresponding_authors": "",
    "abstract": "Sparse tensor networks represent contractions over multiple sparse tensors. Tensor contractions are higher-order analogs of matrix multiplication. Tensor networks arise commonly in many domains of scientific computing and data science. Such networks are typically computed using a tree of binary contractions. Several critical inter-dependent aspects must be considered in the generation of efficient code for a contraction tree, including sparse tensor layout mode order, loop fusion to reduce intermediate tensors, and the mutual dependence of loop order, mode order, and contraction order. We propose CoNST, a novel approach that considers these factors in an integrated manner using a single formulation. Our approach creates a constraint system that encodes these decisions and their interdependence, while aiming to produce reduced-order intermediate tensors via fusion. The constraint system is solved by the Z3 SMT solver and the result is used to create the desired fused loop structure and tensor mode layouts for the entire contraction tree. This structure is lowered to the IR of the TACO compiler, which is then used to generate executable code. Our experimental evaluation demonstrates significant performance improvements over current state-of-the-art sparse tensor compiler/library alternatives.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4401657015",
    "type": "article"
  },
  {
    "title": "A <sup>2</sup> : Towards Accelerator Level Parallelism for Autonomous Micromobility Systems",
    "doi": "https://doi.org/10.1145/3688611",
    "publication_date": "2024-08-21",
    "publication_year": 2024,
    "authors": "Lingyu Sun; Xiaofeng Hou; Chao Li; Jiacheng Liu; Xinkai Wang; Quan Chen; Minyi Guo",
    "corresponding_authors": "",
    "abstract": "Autonomous micromobility systems (AMS) such as low-speed minicabs and robots are thriving. In AMS, multiple Deep Neural Networks execute in parallel on heterogeneous AI accelerators. An emerging paradigm called Accelerator Level Parallelism (ALP) suggests managing accelerators holistically. However, there lacks a specialized and practical solution populating ALP for an AMS, where the varying real-time requirements under different working scenarios bring an opportunity to dynamically tradeoff between latency and efficiency. Furthermore, accelerator heterogeneity introduces enormous configuration space, and the shared-memory architecture results in dynamic bandwidth interference. In this article, we propose A 2 , a novel AMS resource manager optimizing energy and memory space efficiency under variable latency constraints. We gain insight from prior Learn&amp;Control scheme to design an Analyze&amp;Adapt scheme specialized for heterogeneous AI accelerators under shared-memory architecture. It features analyzing the system thoroughly offline to support two-step adaptation online. We build a prototype of A 2 and evaluate it on a commercial edge platform. We show that A 2 achieves 32.8% improvements in power and 13.8% in memory compared with control-based methods. As for timeliness enhancement, A 2 reduces the deadline violation rate by 9.2 percentage points (12.8% → 3.6%) on average compared to directly porting Learn&amp;Control methods.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4401725695",
    "type": "article"
  },
  {
    "title": "A Data-Loader Tunable Knob to Shorten GPU Idleness for Distributed Deep Learning",
    "doi": "https://doi.org/10.1145/3680546",
    "publication_date": "2024-08-22",
    "publication_year": 2024,
    "authors": "Danlin Jia; Geng Yuan; Yiming Xie; Xue Lin; Ningfang Mi",
    "corresponding_authors": "",
    "abstract": "Deep Neural Networks (DNNs) have been applied as an effective machine learning algorithm to tackle problems in different domains. However, the endeavor to train sophisticated DNN models can stretch from days into weeks, presenting substantial obstacles in the realm of research focused on large-scale DNN architectures. Distributed Deep Learning (DDL) contributes to accelerating DNN training by distributing training workloads across multiple computation accelerators, for example, graphics processing units (GPUs). Despite the considerable amount of research directed toward enhancing DDL training, the influence of data loading on GPU utilization and overall training efficacy remains relatively overlooked. It is non-trivial to optimize data-loading in DDL applications that need intensive central processing unit (CPU) and input/output (I/O) resources to process enormous training data. When multiple DDL applications are deployed on a system (e.g., Cloud and High-Performance Computing (HPC) system), the lack of a practical and efficient technique for data-loader allocation incurs GPU idleness and degrades the training throughput. Therefore, our work first focuses on investigating the impact of data-loading on the global training throughput. We then propose a throughput prediction model to predict the maximum throughput for an individual DDL training application. By leveraging the predicted results, A-Dloader is designed to dynamically allocate CPU and I/O resources to concurrently running DDL applications and use the data-loader allocation as a knob to reduce GPU idle intervals and thus improve the overall training throughput. We implement and evaluate A-Dloader in a DDL framework for a series of DDL applications arriving and completing across the runtime. Our experimental results show that A-Dloader can achieve a 28.9% throughput improvement and a 10% makespan improvement compared with allocating resources evenly across applications.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4401765190",
    "type": "article"
  },
  {
    "title": "Mentor: A Memory-Efficient Sparse-dense Matrix Multiplication Accelerator Based on Column-Wise Product",
    "doi": "https://doi.org/10.1145/3688612",
    "publication_date": "2024-08-26",
    "publication_year": 2024,
    "authors": "Xiaobo Lu; Jianbin Fang; Peng Lin; Chun Huang; Zidong Du; Yongwei Zhao; Zheng Wang",
    "corresponding_authors": "",
    "abstract": "Sparse-dense matrix multiplication (SpMM) is the performance bottleneck of many high-performance and deep-learning applications, making it attractive to design specialized SpMM hardware accelerators. Unfortunately, existing hardware solutions do not take full advantage of data reuse opportunities of the input and output matrices or suffer from irregular memory access patterns. Their strategies increase the off-chip memory traffic and bandwidth pressure, leaving much room for improvement. We present Mentor , a new approach to designing SpMM accelerators. Our key insight is that column-wise dataflow, while rarely exploited in prior works, can address these issues in SpMM computations. Mentor is a software-hardware co-design approach for leveraging column-wise dataflow to improve data reuse and regular memory accesses of SpMM. On the software level, Mentor incorporates a novel streaming construction scheme to preprocess the input matrix for enabling a streaming access pattern. On the hardware level, it employs a fully pipelined design to unlock the potential of column-wise dataflow further. The design of Mentor is underpinned by a carefully designed analytical model to find the tradeoff between performance and hardware resources. We have implemented an FPGA prototype of Mentor . Experimental results show that Mentor achieves speedup by geomean 2.05× (up to 3.98×), reduces the memory traffic by geomean 2.92× (up to 4.93×), and improves bandwidth utilization by geomean 1.38× (up to 2.89×), compared with the state-of-the-art hardware solutions.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4401895034",
    "type": "article"
  },
  {
    "title": "DELTA: Memory-Efficient Training via Dynamic Fine-Grained Recomputation and Swapping",
    "doi": "https://doi.org/10.1145/3689338",
    "publication_date": "2024-08-20",
    "publication_year": 2024,
    "authors": "Yu Tang; Qiao Li; Lujia Yin; Dongsheng Li; Yiming Zhang; C.X. Wang; Xingcheng Zhang; Linbo Qiao; Zhaoning Zhang; Kai Lü",
    "corresponding_authors": "",
    "abstract": "To accommodate the increasingly large-scale models within limited-capacity GPU memory, various coarse-grained techniques, such as recomputation and swapping, have been proposed to optimize memory usage. However, these methods have encountered limitations, either in terms of inefficient memory reduction or diminished training performance. In response to this, our article introduces dynamic tensor offloading and recomputation (DELTA), an innovative approach for memory-efficient large-scale model training that combines fine-grained memory optimization and prefetching technology to reduce memory usage while maintaining high training throughput concurrently. Initially, we formulate the problem of memory-throughput joint optimization as an easy-solving 0/1 Knapsack problem. Leveraging this formalization, we use an improving polynomial complexity heuristic algorithm to address the problem effectively. Furthermore, we introduce, to the best of our knowledge, a novel bidirectional prefetching technology into dynamic memory management that significantly accelerates the model training when compared to relying solely on recomputation or swapping. Finally, DELTA offers users an automated training execution library, eliminating the need for manual configuration or specialized expertise. Experimental results demonstrate the effectiveness of DELTA in reducing GPU memory consumption. Compared to state-of-the-art methods, DELTA achieves substantial memory savings ranging from 40% to 72%, while maintaining comparable convergence performance for various models, including ResNet-50, ResNet-101, and BERT-Large. Notably, DELTA enables the training of GPT2-Large and GPT2-XL with batch sizes increased by 5.5× and 6×, respectively, showcasing its versatility and practicality in enabling large-scale model training on GPU hardware.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4401977362",
    "type": "article"
  },
  {
    "title": "Optimizing Garbage Collection for ZNS SSDs via In-storage Data Migration and Address Remapping",
    "doi": "https://doi.org/10.1145/3689336",
    "publication_date": "2024-08-20",
    "publication_year": 2024,
    "authors": "Zhenhua Tan; Linbo Long; Jingcheng Shen; Renping Liu; Congming Gao; Kan Zhong; Yi Jiang",
    "corresponding_authors": "",
    "abstract": "The NVMe Zoned Namespace (ZNS) is a high-performance interface for flash-based solid-state drives (SSDs), which divides the logical address space into fixed-size and sequential-write zones. Meanwhile, ZNS SSDs eliminate in-device garbage collection (GC) by shifting the responsibility of GC to the host. However, the host-side GC of ZNS SSDs is not efficient. On the one hand, data migration during GC first moves data to the host buffer and then writes back the transferred data to the new location in the SSD, resulting in an unnecessary end-to-end transfer overhead. On the other hand, due to the pre-configured mapping between zones and blocks, GC incurs a large block-to-block rewrite overhead, i.e., even if most of the data in a block of the victim zone is valid, the valid data will still be rewritten to another block in the target zone. To address these issues, this article proposes a novel ZNS SSD design that features dynamic zone mapping, termed Brick-ZNS . Brick-ZNS implements two key functionalities: in-storage data migration and address remapping. New ZNS commands are first designed to realize in-storage data migration to avoid the end-to-end transfer overhead of GC while ensuring performance predictability. Then, a remapping strategy exploiting parallel physical blocks is proposed to reduce the large block-to-block rewrite overhead while ensuring zone-level access parallelism. The basic idea of the strategy is to directly remap the parallel physical blocks with a sufficient amount of valid data in the victim zone to the target zone, hence avoiding the large block-to-block rewrite overhead. Based on a full-stack SSD emulator, the evaluation results show that Brick-ZNS improves write throughput by 25% and SSD lifetime by 1.41×.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4401978254",
    "type": "article"
  },
  {
    "title": "DLAS: A Conceptual Model for Across-Stack Deep Learning Acceleration",
    "doi": "https://doi.org/10.1145/3688609",
    "publication_date": "2024-09-02",
    "publication_year": 2024,
    "authors": "Perry Gibson; José Cano; Elliot J. Crowley; Amos Storkey; Michael O’Boyle",
    "corresponding_authors": "",
    "abstract": "Deep Neural Networks (DNNs) are very computationally demanding, which presents a significant barrier to their deployment, especially on resource-constrained devices. Significant work from both the machine learning and computing systems communities has attempted to accelerate DNNs. However, the number of techniques available and the required domain knowledge for their exploration continues to grow, making design space exploration (DSE) increasingly difficult. To unify the perspectives from these two communities, this paper introduces the Deep Learning Acceleration Stack (DLAS), a conceptual model for DNN deployment and acceleration. We adopt a six-layer representation that organizes and illustrates the key areas for DNN acceleration, from machine learning to software and computer architecture. We argue that the DLAS model balances simplicity and expressiveness, assisting practitioners from various domains in tackling co-design acceleration challenges. We demonstrate the interdependence of the DLAS layers, and thus the need for co-design, through an across-stack perturbation study, using a modified tensor compiler to generate experiments for combinations of a few parameters across the DLAS layers. Our perturbation study assesses the impact on inference time and accuracy when varying DLAS parameters across two datasets, seven popular DNN architectures, four compression techniques, three algorithmic primitives (with sparse and dense variants), untuned and auto-scheduled code generation, and four hardware platforms. The study observes significant changes in the relative performance of design choices with the introduction of new DLAS parameters (e.g., the fastest algorithmic primitive varies with the level of quantization). Given the strong evidence for the need for co-design, and the high costs of DSE, DLAS offers a valuable conceptual model for better exploring advanced co-designed accelerated deep learning solutions.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4402134764",
    "type": "article"
  },
  {
    "title": "PIMSAB: A <u>P</u> rocessing- <u>I</u> n- <u>M</u> emory System with <u>S</u> patially- <u>A</u> ware Communication and <u>B</u> it-Serial-Aware Computation",
    "doi": "https://doi.org/10.1145/3690824",
    "publication_date": "2024-09-05",
    "publication_year": 2024,
    "authors": "Siyuan Ma; Kaustubh Mhatre; Jian Weng; Bagus Hanindhito; Zhengrong Wang; Tony Nowatzki; Lizy K. John; Aman Arora",
    "corresponding_authors": "",
    "abstract": "Bit-serial Processing-In-Memory (PIM) is an attractive paradigm for accelerator architectures, for parallel workloads such as Deep Learning (DL), because of its capability to achieve massive data parallelism at a low area overhead and provide orders-of-magnitude data movement savings by moving computational resources closer to the data. While many PIM architectures have been proposed, improvements are needed in communicating intermediate results to consumer kernels, for communication between tiles at scale, for reduction operations, and for efficiently performing bit-serial operations with constants. We present PIMSAB, a scalable architecture that provides a spatially aware communication network for efficient intra-tile and inter-tile data movement and provides efficient computation support for generally inefficient bit-serial compute patterns. Our architecture consists of a massive hierarchical array of compute-enabled SRAMs (CRAMs), which is codesigned with a compiler to achieve high utilization. The key novelties of our architecture are (1) in providing efficient support for spatially-aware communication by providing local H-tree network for reductions, by adding explicit hardware for shuffling operands, and by deploying systolic broadcasting, as well as (2) by taking advantage of the divisible nature of bit-serial computations through adaptive precision and efficient handling of constant operations. These innovations are integrated into a tensor expressions-based programming framework (including a compiler for easy programmability) that enables simple programmer control of optimizations for mapping programs into massively parallel binaries for millions of PIM processing elements. When compared against a similarly provisioned modern Tensor Core GPU (NVIDIA A100), across common DL kernels and end-to-end DL networks (Resnet18 and BERT), PIMSAB outperforms the GPU by 4.80 ×, and reduces energy by 3.76 ×. We compare PIMSAB with similarly provisioned state-of-the-art SRAM PIM (Duality Cache) and DRAM PIM (SIMDRAM), and observe a speedup of 3.7 × and 3.88 × respectively.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4402269472",
    "type": "article"
  },
  {
    "title": "SuccinctKV: a CPU-efficient LSM-tree Based KV Store with Scan-based Compaction",
    "doi": "https://doi.org/10.1145/3695873",
    "publication_date": "2024-09-13",
    "publication_year": 2024,
    "authors": "Yinan Zhang; Shun Yang; Huiqi Hu; Chengcheng Yang; Peng Cai; Xuan Zhou",
    "corresponding_authors": "",
    "abstract": "The CPU overhead of the LSM-tree becomes increasingly significant when high-speed storage devices are utilized. In this paper, we propose SuccinctKV , a key-value store based on LSM-tree that is optimized to improve CPU efficiency in mixed workload scenarios. To achieve this, SuccinctKV reduces the CPU overhead of compaction by writing scan-sorted data directly to the storage device. SuccinctKV also redesigns the merge-sort operation of the LSM-tree, enhancing CPU locality and reducing the unnecessary CPU overhead of cache accesses and I/O system calls. Additionally, SuccinctKV introduces a scheduler to resolve potential bursty I/O contention by autonomously initiating I/O requests at appropriate times and quickly relieving I/O pressure by terminating background I/O requests. We implement SuccinctKV on RocksDB and conduct extensive experiments to evaluate our proposed methods. The experimental results demonstrate that, compared to RocksDB, SuccinctKV achieves a maximum improvement of 2.6x in scan performance and reduces CPU overhead of compaction by up to 89% under mixed workloads.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4402525558",
    "type": "article"
  },
  {
    "title": "A Stable Idle Time Detection Platform for Real I/O Workloads",
    "doi": "https://doi.org/10.1145/3695871",
    "publication_date": "2024-09-12",
    "publication_year": 2024,
    "authors": "Yen-Yu Lu; Chin-Hsien Wu; Shih-Jen Li; Cheng-Tze Lee; Cheng-Yen Wu",
    "corresponding_authors": "",
    "abstract": "It is important to utilize the idle time of a workload to improve the system performance. In the paper, we will explore multiple idle time detection methods to predict the idle time of the real I/O workloads. The objective is to build a stable idle time detection platform by investigating the impact of multiple representative methods to pursue a more stable prediction accuracy. The experimental results show that the prediction accuracy of the proposed platform can be stable between 60% and 80%.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4402530368",
    "type": "article"
  },
  {
    "title": "Towards High Performance QNNs via Distribution-Based CNOT Gate Reduction",
    "doi": "https://doi.org/10.1145/3695872",
    "publication_date": "2024-09-14",
    "publication_year": 2024,
    "authors": "Manojna Sistla; Yiding Liu; Xin Fu",
    "corresponding_authors": "",
    "abstract": "Quantum Neural Networks (QNNs) are one of the most promising applications that can be implemented on NISQ-era quantum computers. In this study, we observe that QNNs often suffer from gate redundancy, which hugely declines the performance and accuracy of the network. Even state-of-the-art architecture search techniques like QuantumNAS do not completely alleviate this problem. Especially, We find that CNOT gates are major contributors to the execution delay and noise in quantum circuits, and there are many redundant CNOT gates in the QNN post-training. This motivates us to propose a novel distribution-based greedy-search circuit optimization technique, that can be employed after the completion of the training process. Our technique significantly reduces the number of CNOT gates in QNNs without affecting the accuracy of the network. With this technique, we have achieved an average of 3 × improvement in execution time while reaching a maximum of 12.4 × improvement.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4402543609",
    "type": "article"
  },
  {
    "title": "An Intelligent Scheduling Approach on Mobile OS for Optimizing UI Smoothness and Power",
    "doi": "https://doi.org/10.1145/3674910",
    "publication_date": "2024-10-26",
    "publication_year": 2024,
    "authors": "Xinglei Dou; Lei Liu; Limin Xiao",
    "corresponding_authors": "",
    "abstract": "Mobile devices need to respond quickly to diverse user inputs. The existing approaches often heuristically raise the CPU/GPU frequency according to the empirical rules when facing burst inputs and various changes. Although doing so can be effective sometimes, the existing approaches still need improvements. For instance, raising processors’ frequency can lead to high power consumption when the frequency is over-provisioned or fails to meet user demands when the frequency is under-provisioned. To this end, we propose MobiRL, a reinforcement learning-based scheduler for intelligent adjusting CPU/GPU frequency to satisfy user demands accurately on mobile systems. MobiRL monitors the mobile system status and autonomously learns to optimize UI smoothness and power consumption by conducting CPU/GPU frequency-adjusting actions. The experimental results on the latest delivered smartphones show that MobiRL outperforms the widely used commercial scheduler on real devices – reducing the frame drop rate by 4.1% and reducing power consumption by 42.8%, respectively. Moreover, compared with a study using Q-Learning for CPU frequency scheduling, MobiRL achieves up to 2.5% lower frame drop rate and reduces power consumption by 32.6%, respectively. Our approach has been deployed in mobile phone products.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403790509",
    "type": "article"
  },
  {
    "title": "DeepZoning: Re-accelerate CNN Inference with Zoning Graph for Heterogeneous Edge Cluster",
    "doi": "https://doi.org/10.1145/3701995",
    "publication_date": "2024-10-28",
    "publication_year": 2024,
    "authors": "Jingyu Wang; Ruilong Ma; Xiang Yang; Qi Qi; Zirui Zhuang; Jing Wang; Jianxin Liao; Song Guo",
    "corresponding_authors": "",
    "abstract": "Parallelizing CNN inference on heterogeneous edge clusters with data parallelism has gained popularity as a way to meet real-time requirements without sacrificing model accuracy. However, existing algorithms struggle to find optimal parallel granularity for complex CNNS, the structure of which is a directed acyclic graph (DAG) rather than a chain, and the parallel dimension is inflexible. To distribute the workload of modern CNNs on heterogeneous devices is also proven as NP-hard problem. In this paper, we introduce DeepZoning , a versatile and cooperative inference framework that combines both model and data parallelism to accelerate CNN inference. DeepZoning employs two algorithms at different levels: (1) a low-level Adaptive Workload Partition algorithm that uses linear programming and takes spatial and channel dimensions into optimization during the search for feature map distribution on heterogeneous devices, and (2) a high-level Model Partition algorithm that finds the optimal model granularity and organizes complex CNNs into sequential zones to balance communication and computation during execution. Our experimental evaluations show that DeepZoning is effective, achieving up to a 3.02 × speed improvement on our experimental prototype compared to state-of-the-art algorithms.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403842273",
    "type": "article"
  },
  {
    "title": "Multiple Function Merging for Code Size Reduction",
    "doi": "https://doi.org/10.1145/3702000",
    "publication_date": "2024-10-28",
    "publication_year": 2024,
    "authors": "Yuta Saito; Kazunori Sakamoto; Hironori Washizaki; Yoshiaki Fukazawa",
    "corresponding_authors": "",
    "abstract": "Resource-constrained environments, such as embedded devices, have limited amounts of memory and storage. Practical programming languages such as C++ and Rust tend to output multiple similar functions by monomorphizing polymorphic functions. An optimization technique called Function Merging, which merges similar functions into a single function, has been studied. However, in the state-of-the-art approach, the number of functions that can be merged at once is limited to two; thus, efficiently merging three or more functions, which are often generated from polymorphic functions, has been impossible. In this study, we propose Multiple Function Merging optimization, which targets merging three or more similar functions into a single function using a multiple sequence alignment algorithm. With multiple aligned information, Multiple Function Merging can increase merge opportunities and reduce extra branching overheads at the code generation stage. We evaluated it using the SPEC CPU benchmark suite and some large-scale C/C++ programs, and the results show that it reduces code size by as much as 7.61% compared with the state-of-the-art approach.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403842430",
    "type": "article"
  },
  {
    "title": "Constructing a Supplementary Benchmark Suite to Represent Android Applications with User Interactions by using Performance Counters",
    "doi": "https://doi.org/10.1145/3701999",
    "publication_date": "2024-10-29",
    "publication_year": 2024,
    "authors": "Chenghao Ouyang; Jinhan Xin; Siqi Zeng; Guohui Li; Jianjun Li; Zhibin Yu",
    "corresponding_authors": "",
    "abstract": "We find existing benchmark suites for smartphone CPU micro-architecture design such as Geekbench 5.0 fail to authentically represent the micro-architecture level performance behavior of widely used real Android applications with interactive operations such as screen sliding. It is therefore crucial to systematically construct a benchmark suite as a supplementary to Geekbench to represent the user interaction behavior of Android applications for CPU micro-architecture design. The key is to identify a small number of representative programs from a large number of real applications. To this end, a set of features used to represent a program need to be constructed, and these features should be fair for different micro-architectures and can be collected efficiently. However, this is extremely difficult for Android applications. For example, the feature collection tools for Android applications are unavailable for benchmark selection. In this paper, we propose a novel benchmark suite construction approach dubbed BEMAP to efficiently build a supplementary benchmark suite from real-world Android applications to represent their user interaction behavior. 1 BEMAP innovates four techniques. The first technique, called two-stage RFC (representative feature construction), constructs program features from performance counters (events) to represent a program for selecting benchmarks from a large number of real Android applications in two stages. The first stage identifies a set of important performance events in terms of IPC (instructions per cycle) by employing a machine learning algorithm named SGBRT (Stochastic Gradient Boosted Regression Tree). The second stage constructs representative features based on the important performance events by using ICA (independent component analysis). The second technique, named SPC-MMA (source performance counters from multiple micro-architectures), collects the performance events from multiple mobile CPUs with different micro-architectures and mixes them as the source of RFC. The goal of these two innovations is to make the program features fair to different mobile CPU micro-architectures. The third technique, called ES (Elbow-Silhouette) approach, artfully leverages the synergy between the elbow method and the silhouette method to determine an optimal K when we use K-Means to group Android applications. The fourth technique is that we design a new tool named AutoProfiler to automatically profile the micro-architecture events (e.g., IPC, L1 Icache misses) of Android applications with interactive operations. Using the proposed BEMAP methodology 2 , we constructed SPBench, a novel benchmark suite supplementary to traditional mobile benchmark suites like Geekbench, for mobile CPU micro-architecture design. It consists of fifteen benchmarks selected from one hundred real Android applications with three common user interaction operations, which is the fifth innovation of this paper. The experimental results on four significantly different micro-architectures show that SPBench can represent the micro-architecture performance behaviors of the one hundred real-world applications with three common user interactive operations on each micro-architecture with significantly higher accuracy than benchmark suites produced by the state-of-the-art approaches.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403875474",
    "type": "article"
  },
  {
    "title": "PARADISE: Criticality-Aware Instruction Reordering for Power Attack Resistance",
    "doi": "https://doi.org/10.1145/3701991",
    "publication_date": "2024-10-29",
    "publication_year": 2024,
    "authors": "Yun Chen; Ali Hajiabadi; Romain Poussier; Yaswanth Tavva; Andreas Diavastos; Shivam Bhasin; Trevor E. Carlson",
    "corresponding_authors": "",
    "abstract": "Power side-channel attacks exploit the correlation of power consumption with the instructions and data being processed to extract secrets from a device (e.g., cryptographic keys). Prior work primarily focused on protecting small embedded micro-controllers and in-order processors rather than high-performance, out-of-order desktop and server CPUs. In this paper, we present Paradise , a general-purpose out-of-order processor with always-on protection, that implements a novel dynamic instruction scheduler to provide obfuscated execution and mitigate power analysis attacks. To achieve this, we exploit the time between operand availability of critical instructions ( slack ) and create high-performance random schedules. Further, we highlight the dangers of using incorrect adversarial assumptions, which can often lead to a false sense of security. Therefore, we perform an extended security analysis on AES-128 using different levels of adversaries, from basic to advanced, including a CNN-based attack. Our advanced security evaluation assumes a strong adversary with full knowledge of the countermeasure and demonstrates a significant security improvement of 556 × when combined with Boolean Masking over a baseline only protected by masking, and 62, 500 × over an unprotected baseline. The resulting overhead in performance, power and area of Paradise is \\(3.2\\% \\) , \\(1.2\\% \\) and \\(0.8\\% \\) respectively.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403875476",
    "type": "article"
  },
  {
    "title": "Iterating Pointers: Enabling Static Analysis for Loop-based Pointers",
    "doi": "https://doi.org/10.1145/3701993",
    "publication_date": "2024-10-29",
    "publication_year": 2024,
    "authors": "Andrea Lepori; Alexandru Calotoiu; Torsten Hoefler",
    "corresponding_authors": "",
    "abstract": "Pointers are an integral part of C and other programming languages. They enable substantial flexibility from the programmer’s standpoint, allowing the user fine, unmediated control over data access patterns. However, accesses done through pointers are often hard to track, and challenging to understand for optimizers, compilers, and sometimes, even for the developers themselves because of the direct memory access they provide. We alleviate this problem by exposing additional information to analyzers and compilers. By separating the concept of a pointer into a data container and an offset, we can optimize C programs beyond what other state-of-the-art approaches are capable of, in some cases even enabling auto-parallelization. Using this process, we are able to successfully analyze and optimize code from OpenSSL, the Mantevo benchmark suite, and the Lempel–Ziv–Oberhumer compression algorithm. We provide the only automatic approach able to find all parallelization opportunities in the HPCCG benchmark from the Mantevo suite the developers identified and even outperform the reference implementation by up to 18%, as well as speed up the PBKDF2 algorithm implementation from OpenSSL by up to 11x.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403875510",
    "type": "article"
  },
  {
    "title": "Conflict Management in Vector Register Files",
    "doi": "https://doi.org/10.1145/3702002",
    "publication_date": "2024-10-29",
    "publication_year": 2024,
    "authors": "Viktor Razilov; Ipek Gecin; Emil Matúš; Gerhard Fettweis",
    "corresponding_authors": "",
    "abstract": "The instruction set architecture (ISA) of vector processors operates on vectors stored in the vector register file (VRF) which needs to handle several concurrent accesses by functional units (FUs) with multiple ports. When the vector processor is running with high utilization, access conflicts become a major source of performance degradation. With a software model of a vector processor, we take a deep dive into the runtime impact of conflicts, their characteristics, and on ways to manage them, i.e., avoidance, resolution, and mitigation. For conflict avoidance, we study the existing approaches of banking with different static bank layouts and propose a dynamic bank layout to overcome their shortcomings. Our approach assigns newly written registers a temporarily unique starting bank. For conflict resolution, we compare different arbitration algorithms and optimize round-robin arbitration for mixed-width arithmetics by prioritizing wide operands. For conflict mitigation, operand queues (OPQs) of varying depths are studied. Our inventions are likely to increase the area efficiency of vector processors. Either, because they allow to use shallower operand queues (OPQs) while keeping the same performance, or reduce the area even further by using less banks, albeit at a performance impairment of 10 % or less. The insights of the study can further be applied to other shared-memory systems.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403875613",
    "type": "article"
  },
  {
    "title": "Leveraging the Hardware Resources to Accelerate cryo-EM Reconstruction of RELION on the New Sunway Supercomputer",
    "doi": "https://doi.org/10.1145/3701990",
    "publication_date": "2024-10-30",
    "publication_year": 2024,
    "authors": "Jingle Xu; Jiayu Fu; Lin Gan; Yaojian Chen; Zhaoqi Sun; Zhenchun Huang; Guangwen Yang",
    "corresponding_authors": "",
    "abstract": "The fast development of biomolecular structure determination has enabled the fine-grained study of objects in the micro-world, such as proteins and RNAs. The world is benefited. However, as the computational algorithms are constantly developed, the enrichment of features increases the algorithmic complexity and brings more computationally unfriendly modules. It calls for efficient solutions to leverage the rich and various hardware resources from the world’s most state-of-the-art supercomputing systems, and to fully accelerate the performance of the applications. In this paper, we present our efforts on porting and optimizing the 3D reconstruction of RELION, one of the most popular cryo-EM software for biomolecular structure determinations, by leveraging different resources of the latest generation of Sunway heterogeneous supercomputer. Several novel approaches are proposed to resolve different challenges faced by the complex algorithm, including a multi-level parallel scheme and operator optimizations to smartly map and scale RELION, efficient strategies to largely address the memory bottlenecks and improve data locality, lock-free writing solutions to minimize write-write conflicts, and pipelining approaches to obtain excellent computation and communication overlap. Combining all proposed optimizations, the computation time is greatly reduced to under 2 hours, achieving 11.9 × and 8.9 × speedups on two different datasets. The overall design scales to 131,072 cores, increasing parallel efficiency from 33% to 61% and from 46% to 70%, respectively. To the best of our knowledge, this is the first work that fully optimized and scaled the 3D reconstruction of RELION using the latest Sunway system.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403899655",
    "type": "article"
  },
  {
    "title": "A High Scalability Memory NoC with Shared-Inside Hierarchical-Groupings for Triplet-Based Many-Core Architecture",
    "doi": "https://doi.org/10.1145/3688610",
    "publication_date": "2024-11-02",
    "publication_year": 2024,
    "authors": "Chunfeng Li; Feng Shi; Fei Yin; Karim Soliman; Jin Wei",
    "corresponding_authors": "",
    "abstract": "Innovative processor architecture designs are shifting towards Many-Core Architectures (MCAs) to meet the future demands of high-performance computing as the limits of Moore’s Law have almost been reached. Many-core processors utilize shared memory hierarchies to achieve high-speed memory systems, improving memory access efficiency. However, as the number of cores multiplies, the scalability of this system is significantly constrained by the increased proportion of long-distance and Non-Uniform Memory Access (NUMA). Improving the scalability of MCAs is crucial for achieving large/super-scale general-purpose many-core processors. This work proposes a high scalability memory Network-on-Chip (NoC) for Triplet-Based Many-Core Architecture (TriBA), named TriBA-mNoC. TriBA-mNoC maintains a consistent core-to-core spacing as the network scale increases, effectively preventing increased long-distance memory access latency. Moreover, it leverages an inherent advantage of shared-inside hierarchical-groupings, alleviating common NUMA issues in the NoC design. Evaluations of static network characteristics show that TriBA-mNoC outperforms most classical NoCs in network diameter, average distance, and cost. TriBA-mNoC can be integrated with TriBA in the same silicon die with a tile-like floorplan, forming a novel NoC called TriBA-NoC, which can combine the strengths of both networks to maximize the architecture performance. We evaluated the memory access performance and scalability of TriBA-NoC using the mathematical evaluation models and actual simulations with real traffic (PARSEC 3.0 and SPLASH-2) at different network scales. The mathematical evaluation results indicate that TriBA-NoC achieves an aggregate speedup of approximately 3x compared with 2D-Mesh for a similar number of cores. Furthermore, TriBA-NoC’s single-core speedup efficiency remains stable as the number of cores increases under the same cache hit ratio, while 2D-Mesh experiences a rapid decline, highlighting TriBA-NoC’s exceptional scalability. Finally, the actual traffic simulation results show that TriBA-NoC achieves an average memory access latency and time reduction of 25.90% − 40.50% and 5.61% − 31.69% respectively, compared with 2D-Mesh.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404006803",
    "type": "article"
  },
  {
    "title": "Characterizing and Understanding HGNN Training on GPUs",
    "doi": "https://doi.org/10.1145/3703356",
    "publication_date": "2024-11-04",
    "publication_year": 2024,
    "authors": "Dengke Han; Mingyu Yan; Xiaochun Ye; Dongrui Fan",
    "corresponding_authors": "",
    "abstract": "Owing to their remarkable representation capabilities for heterogeneous graph data, Heterogeneous Graph Neural Networks (HGNNs) have been widely adopted in many critical real-world domains such as recommendation systems and medical analysis. Prior to their practical application, identifying the optimal HGNN model parameters tailored to specific tasks through extensive training is a time-consuming and costly process. To enhance the efficiency of HGNN training, it is essential to characterize and analyze the execution semantics and patterns within the training process to identify performance bottlenecks. In this study, we conduct a comprehensive quantification and in-depth analysis of two mainstream HGNN training scenarios, including single-GPU and multi-GPU distributed training. Based on the characterization results, we reveal the performance bottlenecks and their underlying causes in different HGNN training scenarios and propose optimization guidelines from both software and hardware perspectives.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404047373",
    "type": "article"
  },
  {
    "title": "SPIRIT: Scalable and Persistent In-Memory Indices for Real-Time Search",
    "doi": "https://doi.org/10.1145/3703351",
    "publication_date": "2024-11-04",
    "publication_year": 2024,
    "authors": "Shoaib Akram; Adnan Hasnat",
    "corresponding_authors": "",
    "abstract": "Today, real-time search over big microblogging data requires low indexing and query latency. Online services, therefore, prefer to host inverted indices in memory. Unfortunately, as datasets grow, indices grow proportionally, and with limited DRAM scaling, the main memory faces high pressure. Also, indices must be persisted on disks as building them is computationally intensive. Consequently, it becomes necessary to frequently move on-heap index segments to storage, slowing down indexing. Reading storage-resident index segments requires filesystem calls and disk accesses during query evaluation, leading to high and unpredictable tail latency. This work exploits hybrid DRAM and scalable non-volatile memory (NVM) to offer dynamically growing and instantly searchable (i.e., real-time) persistent indices in on-heap memory. We implement SPIRIT, a real-time text inversion engine over hybrid memory. SPIRIT exploits the byte-addressability of hybrid memory to enable direct access to the index on a pre-allocated heap, eliminating expensive block storage accesses and filesystem calls during live operation. It uses an in-memory segment descriptor table to offer: ① instant segment availability to query evaluators upon fresh ingestion, ② low-overhead segment movement across memory tiers transparent to query evaluators, and ③ decoupled segment movement into NVM from their visibility to query evaluators, enabling different policies for mitigating NVM latency. SPIRIT accelerates compaction with zero-copy merging. It supports volatile, graceful shutdown, and crash-consistent indexing modes. The latter two modes offer instant recovery using persistent pointers. SPIRIT with hybrid memory and strong crash consistency guarantees exhibits many orders of magnitude better tail response times and query throughout than the state-of-the-art Lucene search engine. Compared against a highly optimized non-real-time evaluation of Lucene with liberal DRAM size, on average, across six query workloads, SPIRIT still delivers 2.5 × better (real-time) query throughput. Our work applies to other services that will benefit from direct on-heap access to large persistent indices.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404047395",
    "type": "article"
  },
  {
    "title": "Tiaozhuan: A General and Efficient Indirect Branch Optimization for Binary Translation",
    "doi": "https://doi.org/10.1145/3703355",
    "publication_date": "2024-11-04",
    "publication_year": 2024,
    "authors": "Xinyu Li; Guangwu Guo; Yanzhi Lan; Feng Xue; Chenji Han; Gen Niu; Fuxin Zhang",
    "corresponding_authors": "",
    "abstract": "Binary translation enables transparent execution, analysis, and modification of the binary program, serving as a core technology that facilitates instruction set emulation, cross-platform compatibility of software, and program instrumentation. Handling indirect branch instructions is widely recognized as a significant performance bottleneck in binary translation. While the target of direct branch can be determined during the translation phase, indirect branch requires a run-time lookup from the guest program counter to the host program counter, significantly influencing the performance of translator. Although several methods have been proposed to accelerate this process, each guest indirect branch instruction still translates into approximately ten host instructions, resulting in considerable overhead. This paper introduces Tiaozhuan, which addresses this issue by employing two optimization schemes. Firstly, Full Address Mapping uses a larger address space to store address mappings from guest to host, effectively reducing the number of instructions required to lookup the target of an indirect branch. Secondly, Exception Assisted Branch Elimination further eliminates branch instructions that check target correctness of targets in the lookup process. These two approaches enable indirect branches target lookup to be completed within 1-2 instructions, noticeably decreasing the overhead of indirect branches. Compared to state-of-the-art mechanisms, the SPEC CPU2006 benchmark suite showed a reduction in the number of instructions by an average of 4.2%, with the highest observed performance improvement reaching 19.4% and an average increase of 3.9%.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404047495",
    "type": "article"
  },
  {
    "title": "RaNAS: Resource-Aware Neural Architecture Search for Edge Computing",
    "doi": "https://doi.org/10.1145/3703353",
    "publication_date": "2024-11-05",
    "publication_year": 2024,
    "authors": "Jianhua Gao; Zeming Liu; Yizhuo Wang; Weixing Ji",
    "corresponding_authors": "",
    "abstract": "Neural architecture search (NAS) for edge devices is often time-consuming because of long-latency deploying and testing on edge devices. The ability to accurately predict the computation cost and memory requirement for convolutional neural networks (CNNs) in advance holds substantial value. Existing work primarily relies on analytical models, which can result in high prediction errors. This paper proposes a resource-aware NAS (RaNAS) model based on various features. Additionally, a new graph neural network is introduced to predict inference latency and maximum memory requirements for CNNs on edge devices. Experimental results show that, within the error bound of ±1%, RaNAS achieves an accuracy improvement of approximately 8% for inference latency prediction and about 25% for maximum memory occupancy prediction over the state-of-the-art approaches.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404077674",
    "type": "article"
  },
  {
    "title": "ApSpGEMM: Accelerating Large-scale SpGEMM with Heterogeneous Collaboration and Adaptive Panel",
    "doi": "https://doi.org/10.1145/3703352",
    "publication_date": "2024-11-06",
    "publication_year": 2024,
    "authors": "Dezhong Yao; Sifan Zhao; Tongtong Liu; Gang Wu; Hai Jin",
    "corresponding_authors": "",
    "abstract": "The Sparse General Matrix-Matrix multiplication (SpGEMM) is a fundamental component for many applications, such as algebraic multigrid methods (AMG), graphic processing, and deep learning. However, the unbearable latency of computing high-dimensional, large-scale sparse matrix multiplication on GPUs hinders the development of these applications. An effective approach is heterogeneous cores collaborative computing, but this method must address three aspects: (1) irregular non-zero elements lead to load imbalance and irregular memory access, (2) different core computing latency differences reduce computational parallelism, and (3) temporary data transfer between different cores introduces additional latency overhead. In this work, we propose an innovative framework for collaborative large-scale sparse matrix multiplication on CPU-GPU heterogeneous cores, named ApSpGEMM. ApSpGEMM is based on sparsity rules and proposes reordering and splitting algorithms to eliminate the impact of non-zero element distribution features on load and memory access. Then adaptive panels allocation with affinity constraints among cores improves computational parallelism. Finally, carefully arranged asynchronous data transmission and computation balance communication overhead. Compared with state-of-the-art SpGEMM methods, our approach provides excellent absolute performance on matrices with different sparse structures. On heterogeneous cores, the GFlops of large-scale sparse matrix multiplication is improved by 2.25 to 7.21 times.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404112662",
    "type": "article"
  },
  {
    "title": "exZNS: Extending Zoned Namespace to Support Byte-loggable Zones",
    "doi": "https://doi.org/10.1145/3705318",
    "publication_date": "2024-11-20",
    "publication_year": 2024,
    "authors": "Wenjie Qi; Zhipeng Tan; Ziyue Zhang; Ying Yuan; Dan Feng",
    "corresponding_authors": "",
    "abstract": "Emerging Zoned Namespace (ZNS) provides hosts with fine-grained, performance-predictable storage management. ZNS organizes the address space into zones composed of fixed-size, sequentially written, non-overwritable blocks, making it suitable for log-structured file systems. However, our experimental analysis reveals that ZNS’s write restrictions introduce notable persistence overhead. Firstly, out-of-place updates of data blocks require frequent small modifications to file metadata blocks, which are typically much smaller than a block, to record the latest logical block address. Secondly, some files, such as databases’ Write-Ahead Logging files, frequently execute synchronous small writes, with I/O sizes typically smaller than a logical block. The persistence of these file metadata and file data requires writing back the entire block even if it is only partially updated. This significantly increases the I/O latency and potentially reduces device lifespan. This paper proposes exZNS , an innovative extension of ZNS, designed to provide both regular zones and byte-loggable zones . By exposing the persistent write buffer of the opened zones on the device to the application, the byte-loggable zone allows for appending at byte granularity through a new set of APIs. To reduce the persistence overhead described above, we built exBlzFS , a novel high-performance file system for exZNS. exBlzFS selectively records the partial updates of metadata blocks to the byte-loggable zone to ensure metadata persistence, and persists file data to the byte-loggable zone at byte granularity to absorb the frequent small writes. Evaluations show that exBlzFS increases the IOPS of RocksDB by 42.7% and 76.3%, and reduces the device’s write traffic by 86% and 94%, compared with BlzFS and F2FS, respectively.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404542354",
    "type": "article"
  },
  {
    "title": "GCNTrain+: A Versatile and Efficient Accelerator for Graph Convolutional Neural Network Training",
    "doi": "https://doi.org/10.1145/3705317",
    "publication_date": "2024-11-23",
    "publication_year": 2024,
    "authors": "Zhuoran Song; Jingwen Long; Li Jiang; Naifeng Jing; Xiaoyao Liang",
    "corresponding_authors": "",
    "abstract": "Recently, graph convolutional networks (GCNs) have gained wide attention due to their ability to capture node relationships in graphs. One problem appears when full-batch GCN is trained on large graph datasets, where the computational and memory requirements are unacceptable. To address this issue, mini-batch GCN training is introduced to improve the scalability of GCN training for large datasets by sampling and training only a subset of the graph in each batch. Although several acceleration techniques have been designed for boosting the efficiency of full-batch GCN, they lack attention to mini-batch GCN, which differs from full-batch GCN in terms of the sampled dynamic graph structures. Based on our previous work GCNTrain [28], which was originally excogitated for accelerating full-batch GCN training, we devise GCNTrain+—a universal accelerator to tackle the performance bottlenecks associated with both full-batch and mini-batch GCN training. GCNTrain+ is equipped with two engines to optimize computation and memory access in GCN training, respectively. To reduce the computation overhead, we propose to dynamically reconfigure the computation order based on the varying data dimensions involved in each training batch. Moreover, we build a unified computation engine to perform the sparse-dense matrix multiplications (SpDM) and sparse-sparse matrix multiplications (SpSpM) discovered in GCN training uniformly. To alleviate the memory burden, we devise a two-phased dynamic clustering mechanism to capture data locality as well as customized hardware to reduce the clustering overhead. We evaluate GCNTrain+ on seven datasets, and the result shows that GCNTrain+ achieves 136.0 ×, 52.6 ×, 2.2 ×, and 1.5 × speedup over CPU, GPU, GCNAX, and GCNTrain in full-batch GCN training. Additionally, GCNTrain+ outperforms them with speedups of 131.6 ×, 67.1 ×, 4.4 ×, and 1.5 × in mini-batch GCN training.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404650788",
    "type": "article"
  },
  {
    "title": "AIS: An Active Idleness I/O Scheduler to Reduce Buffer-Exhausted Degradation of Solid-State Drives",
    "doi": "https://doi.org/10.1145/3708538",
    "publication_date": "2024-12-16",
    "publication_year": 2024,
    "authors": "Yekang Zhan; Xiangrui Yang; Hefeng Hu; Qiang Cao; Yifan Zhang; Jie Yao",
    "corresponding_authors": "",
    "abstract": "Modern solid-state drives (SSDs) continue to boost storage density and I/O bandwidth at the cost of flash-access I/O latency, especially for write, hence prevalently deploy a build-in buffer to absorb incoming writes. However, when the buffer is used up, the applications suffer from a sudden and long performance declines, i.e., buffer-exhausted degradation (BED). To holistically understand BED and recovery, we design an automated testing toolset (SSDTest) to measure six commodity NVMe SSDs and find: 1) the occurrence of the BED strictly relies on the written-data amount, 2) BED dramatically increases I/O latency of SSDs, especially write and read-after-write, 3) BED can be conditionally reduced and recovered only after a period of idle time, and 4) a read without preceding writes is largely immune to BED, but prolongs the required idle time to recover the available buffer. Furthermore, we build a black-box SSD buffer-recovery model to quantitatively characterize the idleness-recovery behaviors and design an SSD BED predictor to make BED occurrence and buffer recovery predictable. Leveraging this model, we further design an Active Idleness I/O Scheduler (AIS) with small-sized auxiliary storage to actively regulate the I/O idle-intervals to maximize the internal buffer recovery of SSD. AIS adaptively steers incoming data to the auxiliary storage to 1) strategically keep SSD idle to reduce the occurrence of BED and 2) mitigate the tail latency of SSDs caused by read-after-writes during BED. We perform extensive evaluations under a variety of workloads. The results show that AIS improves average, 99th, 99.9th and 99.99th-percentile latencies of SSDs by up to 29.3%, 37.3%, 78.7% and 67.2% respectively, with up to 512MB auxiliary storage.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4405453092",
    "type": "article"
  },
  {
    "title": "Bubble-Swap Flow Control",
    "doi": "https://doi.org/10.1145/3705316",
    "publication_date": "2024-12-17",
    "publication_year": 2024,
    "authors": "Yi Dai; Kai Lü; Sheng Ma; Jinshu Su; Dongsheng Li",
    "corresponding_authors": "",
    "abstract": "Deadlock-free adaptive routing is extensively adopted in both on-chip and off-chip interconnection networks to improve communication bandwidth and reduce latency. Introducing virtual channels (VCs), also known as virtual lanes (VLs), is the mainstream technique to handle deadlocks incurred by adaptive routing, and provides VC preemption for higher priority traffic. However, existing deadlock-free flow control schemes either underutilize memory resources due to inefficient buffer management to simplify hardware implementation, or rely on complicated global coordination and synchronization with very high hardware complexity. Most hardware-friendly schemes use more VCs and memory resources to enable ease of implementation of deadlock-free flow control. In contrast, sophisticated schemes achieve deadlock freedom with minimum VC cost, even eliminating additional buffer requirement through the complicated control mechanisms. In this work, we rethink the root cause of the deadlock problem from a different perspective by considering it as a lack of credit, which makes us find an efficient solution to the deadlock problem. With minor modification of credit accumulation and return, our proposed bubble-swap flow control (BSFC) ensures atomic buffer swap between two adjacent routers only based on local credit status while making full use of the buffer space. BSFC achieves a better tradeoff between implementation complexity and memory overhead and can be easily integrated in the industrial router with no modification on buffer allocation or port arbitration. The simulation results demonstrate BSFC outperforms existing bubble-based deadlock-free methods by average 64% higher throughput. We further propose a credit reservation strategy to eliminate the escape virtual channel (VC) cost for fully adaptive routing implementation. The synthesizing results demonstrate that BSFC along with credit reservation (BSFC-CR) can reduce the area and power consumption by respectively 29% and 26% in contrast to the traditional critical bubble scheme (CBS).",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4405504945",
    "type": "article"
  },
  {
    "title": "Consequence-based Clustered Architecture",
    "doi": "https://doi.org/10.1145/3708539",
    "publication_date": "2024-12-17",
    "publication_year": 2024,
    "authors": "K Shruthi; Rajshekar Kalayappan; Sandeep Chandran",
    "corresponding_authors": "",
    "abstract": "We recognize that the execution of many dynamic instructions have no consequence on the overall execution of the program. For example, the execution of a correctly predicted conditional branch instruction, as well as all the instructions leading up to it, are inconsequential. We propose a clustered architecture that steers consequential instructions to the primary cluster, and inconsequential ones to the secondary one called the I-Pipe that is less capable and thereby, more area and power efficient. The proposed architecture also entails minimal inter-cluster communication, thereby greatly reducing the complexities of inter-cluster result buses. Such a steering policy also helps increase the performance as the consequential instructions do not face any interference from the inconsequential ones. We demonstrate a \\(42\\% \\) area reduction as compared to a baseline single cluster (Tigerlake-based) architecture, a \\(18.5\\% \\) power reduction in the SPEC CPU2017 suite ( \\(13.7\\% \\) power reduction in GAPBS), and a \\(5.15\\% \\) performance uplift in the SPEC CPU2017 suite ( \\(10.22\\% \\) in the GAPBS suite).",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4405504982",
    "type": "article"
  },
  {
    "title": "Steered Bubble: An Interposer-based Deadlock Recovery Algorithm for Multi-chiplet Systems",
    "doi": "https://doi.org/10.1145/3708543",
    "publication_date": "2024-12-17",
    "publication_year": 2024,
    "authors": "Zhiqiang Chen; Yongwen Wang; Hongwei Zhou; Jian Zhang",
    "corresponding_authors": "",
    "abstract": "Dividing a single System-on-Chip (SoC) into multiple chiplets and integrating them via an interposer can achieve an optimal balance between continuous transistor integration and monetary cost. However, potential deadlock may arise between the chiplets and the interposer. This deadlock can be avoided by applying turn restriction or injection control on the boundary routers, at the cost of additional latency and sub-optimal performance. Compared to deadlock avoidance, deadlock recovery exerts less impact on network performance. Nevertheless, accurate and timely deadlock detection, along with efficient deadlock recovery, continues to pose significant challenges. Additionally, modularity is a specific concern, which involves integrating chiplets of various functions, sizes, manufacturing processes, and so on. Minimizing the negative impact of deadlock resolution while maximizing modularity is crucial for achieving the benefit of chiplets. This paper proposes a modular deadlock detection strategy, Up-Down, which monitors both the upward and downward directions of vertical channels, facilitating information exchange through the congestion-sense network. When a pair of blocked upward and downward vertical channels is detected simultaneously, it is considered that an inter-chiplet deadlock has occurred. This significantly enhances the accuracy of deadlock detection by two orders of magnitude compared to time-out deadlock detection. Furthermore, this paper introduces Steered Bubble, a low-cost deadlock recovery algorithm. It does so by injecting bubbles into potential deadlock cycles identified by Up-Down. These bubbles follow preset paths, ensuring efficient deadlock recovery. Experimental results indicate that the Steered Bubble results in an average performance enhancement of \\(1\\% \\sim 10\\% \\) during full-system simulations, with an area overhead of less than 2%.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4405513857",
    "type": "article"
  },
  {
    "title": "Flexible and Effective Object Tiering for Heterogeneous Memory Systems",
    "doi": "https://doi.org/10.1145/3708540",
    "publication_date": "2024-12-17",
    "publication_year": 2024,
    "authors": "Brandon Kammerdiener; Jessica McMichael; Michael R. Jantz; Kshitij Doshi; Terry Jones",
    "corresponding_authors": "",
    "abstract": "Computing platforms that package multiple types of memory, each with their own performance characteristics, are quickly becoming mainstream. To operate efficiently, heterogeneous memory architectures require new data management solutions that are able to match the needs of each application with an appropriate type of memory. As the primary generators of memory usage, applications create a great deal of information that can be useful for guiding memory management, but the community still lacks tools to collect, organize, and leverage this information effectively. To address this gap, this work introduces a novel software framework that collects and analyzes object-level information to guide memory tiering. The framework includes tools to monitor the capacity and usage of individual data objects, routines that aggregate and convert this information into tier recommendations for the host platform, and mechanisms to enforce these recommendations according to user-selected policies. Moreover, the developed tools and techniques are fully automatic, work on standard Linux systems, and do not require modification or recompilation of existing software. Using this framework, this study evaluates and compares the impact of a variety of design choices for memory tiering, including different policies for prioritizes objects for fast memory tier as well as the frequency and timing of migration events. The results, collected on a modern Intel ® platform with conventional DDR4 SDRAM as well as Intel Optane NVRAM, show that guiding data tiering with object-level information can enable significant performance and efficiency benefits compared to standard hardware- and software-directed data tiering strategies for a diverse set of memory-intensive workloads.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4405513947",
    "type": "article"
  },
  {
    "title": "Implicit array bounds checking on 64-bit architectures",
    "doi": "https://doi.org/10.1145/1187976.1187982",
    "publication_date": "2006-12-01",
    "publication_year": 2006,
    "authors": "Chris Bentley; Scott A. Watterson; David K. Lowenthal; Barry Rountree",
    "corresponding_authors": "",
    "abstract": "Several programming languages guarantee that array subscripts are checked to ensure they are within the bounds of the array. While this guarantee improves the correctness and security of array-based code, it adds overhead to array references. This has been an obstacle to using higher-level languages, such as Java, for high-performance parallel computing, where the language specification requires that all array accesses must be checked to ensure they are within bounds. This is because, in practice, array-bounds checking in scientific applications may increase execution time by more than a factor of 2. Previous research has explored optimizations to statically eliminate bounds checks, but the dynamic nature of many scientific codes makes this difficult or impossible. Our approach is, instead, to create a compiler and operating system infrastructure that does not generate explicit bounds checks. It instead places arrays inside of Index Confinement Regions (ICRs), which are large, isolated, mostly unmapped virtual memory regions. Any array reference outside of its bounds will cause a protection violation; this provides implicit bounds checking. Our results show that when applying this infrastructure to high-performance computing programs written in Java, the overhead of bounds checking relative to a program with no bounds checks is reduced from an average of 63% to an average of 9%.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2089632661",
    "type": "article"
  },
  {
    "title": "SHASTA",
    "doi": "https://doi.org/10.1145/3412375",
    "publication_date": "2020-09-30",
    "publication_year": 2020,
    "authors": "Gokul Subramanian Ravi; Joshua San Miguel; Mikko H. Lipasti",
    "corresponding_authors": "",
    "abstract": "A key requirement for efficient general purpose approximate computing is an amalgamation of flexible hardware design and intelligent application tuning, which together can leverage the appropriate amount of approximation that the applications engender and reap the best efficiency gains from them. To achieve this, we have identified three important features to build better general-purpose cross-layer approximation systems: ① individual per-operation (“spatio-temporally fine-grained”) approximation, ② hardware-cognizant application tuning for approximation, ③ systemwide approximation-synergy. We build an efficient general purpose approximation system called SHASTA: Synergic HW-SW Architecture for Spatio-Temporal Approximation, to achieve these goals. 1 First, in terms of hardware, SHASTA approximates both compute and memory—SHASTA proposes (a) a form of timing approximation called Slack-control Approximation, which controls the computation timing of each approximation operation and (b) a Dynamic Pre-L1 Load Approximation mechanism to approximate loads prior to cache access. These hardware mechanisms are designed to achieve fine-grained spatio-temporally diverse approximation. Next, SHASTA proposes a Hardware-cognizant Approximation Tuning mechanism to tune an application’s approximation to achieve the optimum execution efficiency under the prescribed error tolerance. The tuning mechanism is implemented atop a gradient descent algorithm and, thus, the application’s approximation is tuned along the steepest error vs. execution efficiency gradient. Finally, SHASTA is designed with a full-system perspective, which achieves Synergic benefits across its optimizations, building a closer-to-ideal general purpose approximation system. SHASTA is implemented on top of an OOO core and achieves mean speedups/energy savings of 20%–40% over a non-approximate baseline for greater than 90% accuracy—these benefits are substantial for applications executing on a traditional general purpose processing system. SHASTA can be tuned to specific accuracy constraints and execution metrics and is quantitatively shown to achieve 2–15× higher benefits, in terms of performance and energy, compared to prior work.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3091355678",
    "type": "article"
  },
  {
    "title": "Performance-Energy Trade-off in Modern CMPs",
    "doi": "https://doi.org/10.1145/3427092",
    "publication_date": "2020-12-30",
    "publication_year": 2020,
    "authors": "Solomon Abera; S. Balakrishnan; Anshul Kumar",
    "corresponding_authors": "",
    "abstract": "Chip multiprocessors (CMPs) are ubiquitous in all computing systems ranging from high-end servers to mobile devices. In these systems, energy consumption is a critical design constraint as it constitutes the most significant operating cost for computing clouds. Analogous to this, longer battery life continues to be an essential user concern in mobile devices. To optimize on power consumption, modern processors are designed with Dynamic Voltage and Frequency Scaling (DVFS) support at the individual core as well as the uncore level. This allows fine-grained control of performance and energy. For an n core processor with m core and uncore frequency choices, the total DVFS configuration space is now m (n+1) (with the uncore accounting for the + 1). In addition to that, in CMPs, the performance-energy trade-off due to core/uncore frequency scaling concerning a single application cannot be determined independently as cores share critical resources like the last level cache (LLC) and the memory. Thus, unlike the uni-processor environment, the energy consumption of an application running on a CMP depends not only on its characteristics but also on those of its co-runners (applications running on other cores). The key objective of our work is to select a suitable core and uncore frequency that minimizes power consumption while limiting application performance degradation within certain pre-defined limits (can be termed as QoS requirements). The key contribution of our work is a learning-based model that is able to capture the interference due to shared cache, bus bandwidth, and memory bandwidth between applications running on multiple cores and predict near-optimal frequencies for core and uncore.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3116090179",
    "type": "article"
  },
  {
    "title": "HyGain: High-performance, Energy-efficient Hybrid Gain Cell-based Cache Hierarchy",
    "doi": "https://doi.org/10.1145/3572839",
    "publication_date": "2022-11-30",
    "publication_year": 2022,
    "authors": "Sarabjeet Singh; Neelam Surana; Kailash Prasad; Pranjali Jain; Joycee Mekie; Manu Awasthi",
    "corresponding_authors": "",
    "abstract": "In this article, we propose a “full-stack” solution to designing high-apacity and low-latency on-chip cache hierarchies by starting at the circuit level of the hardware design stack. We propose a novel half V DD precharge 2T Gain Cell (GC) design for the cache hierarchy. The GC has several desirable characteristics, including ~50% higher storage density and ~50% lower dynamic energy as compared to the traditional 6T SRAM, even after accounting for peripheral circuit overheads. We also demonstrate data retention time of 350 us (~17.5× of eDRAM) at 28 nm technology with V DD = 0.9V and temperature = 27°C that, combined with optimizations like staggered refresh, makes it an ideal candidate to architect all levels of on-chip caches. We show that compared to 6T SRAM, for a given area budget, GC-based caches, on average, provide 30% and 36% increase in IPC for single- and multi-programmed workloads, respectively, on contemporary workloads, including SPEC CPU 2017. We also observe dynamic energy savings of 42% and 34% for single- and multi-programmed workloads, respectively. Finally, in a quest to utilize the best of all worlds, we combine GC with STT-RAM to create hybrid hierarchies. We show that a hybrid hierarchy with GC caches at L1 and L2 and an LLC split between GC and STT-RAM is able to provide a 46% benefit in energy-delay product (EDP) as compared to an all-SRAM design, and 13% as compared to an all-GC cache hierarchy, averaged across multi-programmed workloads.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3202202452",
    "type": "article"
  },
  {
    "title": "Delay-on-Squash: Stopping Microarchitectural Replay Attacks in Their Tracks",
    "doi": "https://doi.org/10.1145/3563695",
    "publication_date": "2022-09-19",
    "publication_year": 2022,
    "authors": "Christos Sakalis; Stefanos Kaxiras; Magnus Själander",
    "corresponding_authors": "",
    "abstract": "MicroScope and other similar microarchitectural replay attacks take advantage of the characteristics of speculative execution to trap the execution of the victim application in a loop, enabling the attacker to amplify a side-channel attack by executing it indefinitely. Due to the nature of the replay, it can be used to effectively attack software that are shielded against replay, even under conditions where a side-channel attack would not be possible (e.g., in secure enclaves). At the same time, unlike speculative side-channel attacks, microarchitectural replay attacks can be used to amplify the correct path of execution, rendering many existing speculative side-channel defenses ineffective. In this work, we generalize microarchitectural replay attacks beyond MicroScope and present an efficient defense against them. We make the observation that such attacks rely on repeated squashes of so-called “replay handles” and that the instructions causing the side-channel must reside in the same reorder buffer window as the handles. We propose Delay-on-Squash, a hardware-only technique for tracking squashed instructions and preventing them from being replayed by speculative replay handles. Our evaluation shows that it is possible to achieve full security against microarchitectural replay attacks with very modest hardware requirements while still maintaining 97% of the insecure baseline performance.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3214253003",
    "type": "article"
  },
  {
    "title": "Cooperative Slack Management: Saving Energy of Multicore Processors by Trading Performance Slack Between QoS-Constrained Applications",
    "doi": "https://doi.org/10.1145/3505559",
    "publication_date": "2022-01-31",
    "publication_year": 2022,
    "authors": "Mehrzad Nejat; Madhavan Manivannan; Miquel Pericàs; Per Stenström",
    "corresponding_authors": "",
    "abstract": "Processor resources can be adapted at runtime according to the dynamic behavior of applications to reduce the energy consumption of multicore processors without affecting the Quality-of-Service (QoS). To achieve this, an online resource management scheme is needed to control processor configurations such as cache partitioning, dynamic voltage-frequency scaling, and dynamic adaptation of core resources. Prior State-of-the-art has shown the potential for reducing energy without any performance degradation by coordinating the control of different resources. However, in this article, we show that by allowing short-term variations in processing speed (e.g., instructions per second rate), in a controlled fashion, we can enable substantial improvements in energy savings while maintaining QoS. We keep track of such variations in the form of performance slack. Slack can be generated, at some energy cost, by processing faster than the performance target. On the other hand, it can be utilized to save energy by allowing a temporary relaxation in the performance target. Based on this insight, we present Cooperative Slack Management (CSM). During runtime, CSM finds opportunities to generate slack at low energy cost by estimating the performance and energy for different resource configurations using analytical models. This slack is used later when it enables larger energy savings. CSM performs such trade-offs across multiple applications, which means that the slack collected for one application can be used to reduce the energy consumption of another. This cooperative approach significantly increases the opportunities to reduce system energy compared with independent slack management for each application. For example, we show that CSM can potentially save up to 41% of system energy (on average, 25%) in a scenario in which both prior art and an extended version with local slack management for each core are ineffective.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4210490954",
    "type": "article"
  },
  {
    "title": "Weaving Synchronous Reactions into the Fabric of SSA-form Compilers",
    "doi": "https://doi.org/10.1145/3506706",
    "publication_date": "2022-03-08",
    "publication_year": 2022,
    "authors": "Hugo Pompougnac; Ulysse Beaugnon; Albert Cohen; Dumitru Potop Butucaru",
    "corresponding_authors": "",
    "abstract": "We investigate the programming of reactive systems combining closed-loop control with performance-intensive components such as Machine Learning (ML). Reactive control systems are often safety-critical and associated with real-time execution requirements, a domain of predilection for synchronous programming languages. Extending the high levels of assurance found in reactive control systems to computationally intensive code remains an open issue. We tackle it by unifying concepts and algorithms from synchronous languages with abstractions commonly found in general-purpose and ML compilers. This unification across embedded and high-performance computing enables a high degree of reuse of compiler abstractions and code. We first recall commonalities between dataflow synchronous languages and the static single assignment (SSA) form of general-purpose/ML compilers. We highlight the key mechanisms of synchronous languages that SSA does not cover—denotational concepts such as synchronizing computations with an external time base, cyclic and reactive I/O, as well as the operational notions of relaxing control flow dominance and the modeling of absent values. We discover that initialization-related static analyses and code generation aspects can be fully decoupled from other aspects of synchronous semantics such as memory management and causality analysis, the latter being covered by existing dominance-based algorithms of SSA-form compilers. We show how the SSA form can be seamlessly extended to enable all SSA-based transformations and optimizations on reactive programs with synchronous concurrency. We derive a compilation flow suitable for both high-performance and reactive aspects of a control application, by embedding the Lustre dataflow synchronous language into the SSA-based MLIR/LLVM compiler infrastructure. This allows the modeling of signal processing and deep neural network inference in the (closed) loop of feedback-directed control systems. With only minor efforts leveraging the MLIR infrastructure, the generated code matches or outperforms state-of-the-art synchronous language compilers on computationally intensive ML applications.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4220686429",
    "type": "article"
  },
  {
    "title": "EXPERTISE: An Effective Software-level Redundant Multithreading Scheme against Hardware Faults",
    "doi": "https://doi.org/10.1145/3546073",
    "publication_date": "2022-07-04",
    "publication_year": 2022,
    "authors": "Hwisoo So; Moslem Didehban; Yohan Ko; Aviral Shrivastava; Kyoungwoo Lee",
    "corresponding_authors": "",
    "abstract": "Error resilience is the primary design concern for safety- and mission-critical applications. Redundant MultiThreading (RMT) is one of the most promising soft and hard error resilience strategies because it does not require additional hardware modification. While the state-of-the-art software RMT scheme can achieve a high degree of error protection, our detailed investigation revealed that it suffers from performance overhead and insufficient fault coverage. This paper proposes EXPERTISE, a compiler-level RMT scheme that can detect the manifestation of hardware faults in all processor components. EXPERTISE transformation generates a checker-thread for the main execution thread. These redundant threads are executed simultaneously on two physically different cores of a multicore processor and perform almost the same computations. After each memory write operation is committed by the main-thread, the checker-thread loads back the written data from the memory and checks it against its own locally computed values. If they match, the execution continues. Otherwise, the error flag is raised. In order to evaluate the effectiveness of the proposed solution, we performed soft and hard error injection experiments on all the different hardware components of an ARM Cortex53-like μ-architecturally simulated microprocessor. Based on statistical fault injection campaigns, we have found that EXPERTISE provides 188× better fault coverage with 27% faster performance as compared to the state-of-the-art scheme.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4283810012",
    "type": "article"
  },
  {
    "title": "Adaptive Contention Management for Fine-Grained Synchronization on Commodity GPUs",
    "doi": "https://doi.org/10.1145/3547301",
    "publication_date": "2022-07-11",
    "publication_year": 2022,
    "authors": "Lan Gao; Jing Wang; Weigong Zhang",
    "corresponding_authors": "",
    "abstract": "As more emerging applications are moving to GPUs, fine-grained synchronization has become imperative. However, their performance can be severely impaired in case of frequent synchronization failures caused by high data contention. Differently from CPUs, GPUs own thousands of hardware threads and adopt single instruction multiple threads paradigm, making it impractical to deploy the CPU contention management mechanisms directly on GPUs. In this article, we design a Software Warp Controlling Framework (SWCF), which employs producer-consumer execution model and leverages GPU hardware barriers to dynamically control the execution of warps at runtime. On the basis of SWCF, we propose a contention management strategy to decrease frequent synchronization failures while avoiding the over-reducing of parallelism. We evaluate SWCF and the proposed strategy on commodity GPUs using a set of applications with fine-grained synchronization. The results show that on V100 GPU our contention management achieves a 4.7X speedup and outperforms the conventional GPU software backoff solution by 42% on average.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4285021184",
    "type": "article"
  },
  {
    "title": "DynamAP: Architectural Support for Dynamic Graph Traversal on the Automata Processor",
    "doi": "https://doi.org/10.1145/3556976",
    "publication_date": "2022-08-13",
    "publication_year": 2022,
    "authors": "Yiding Liu; Xingyao Zhang; Donglin Zhuang; Xin Fu; Shuaiwen Leon Song",
    "corresponding_authors": "",
    "abstract": "Dynamic graph traversals (DGTs) currently are widely used in many important application domains, especially in this big-data era that urgently demands high-performance graph processing and analysis. Unlike static graph traversals, DGTs in real-world application scenarios require not only fast traversal acceleration itself but also, more importantly, a runtime strategy that can effectively accommodate the ever-evolving nature of the graph structure updates followed by a diverse range of graph traversal algorithms . Because of these special features, state-of-the-art designs on conventional compute-centric architectures (e.g., CPU and GPU) struggle to provide sufficient acceleration for DGT processing due to the dominating irregular memory access patterns in graph traversal algorithms and inefficient platform-specific update mechanisms. In this article, we explore the algorithmic features and runtime requirements of real-world DGTs and identify their unique opportunities of acceleration on the recent Micron Automata Processor (AP), an in-situ memory-centric pattern-matching architecture. These features include the natural mapping between traversal algorithms’ path exploration pattern to classic non-deterministic finite automata processing, AP’s architectural and compilation support for DGTs’ evolving traversal operations, and its inherent hardware fitness. However, despite these benefits, enabling highly efficient DGT execution on AP is non-trivial and faces several major challenges. To tackle them, we propose DynamAP , the first AP framework design that enables fast processing for general DGTs. DynamAP is oblivious to periodical traversal algorithm changes and can address the significant overhead caused by frequent graph updates and AP recompilation through our novel hybrid macro designs and associated efficient updating strategies. We evaluate DynamAP against the current DGT designs on a CPU, GPU, and AP with a range of widely adopted DGT algorithms and real-world graphs. For a single update request , our DynamAP achieves an average speedup of 21.3x (up to 39.2x ) over the state-of-the-art implementation on host-AP architecture; an average speedup of 9.2x (up to 14.7x ) and 1.7x (up to 2.8x ) over two highly optimized DGT design frameworks on a 64-GB Intel(R) Xeon CPU and a 32-GB NVIDIA Tesla V100 GPU. DynamAP also maintains high performance and resource utilization for high graph update ratios, and can significantly benefit natural graphs that present a high average vertex degree.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4291202557",
    "type": "article"
  },
  {
    "title": "<scp>BullsEye</scp> : Scalable and Accurate Approximation Framework for Cache Miss Calculation",
    "doi": "https://doi.org/10.1145/3558003",
    "publication_date": "2022-08-23",
    "publication_year": 2022,
    "authors": "Nilesh Rajendra Shah; Ashitabh Misra; Antoine Miné; Rakesh Venkat; Ramakrishna Upadrasta",
    "corresponding_authors": "",
    "abstract": "For Affine Control Programs or Static Control Programs (SCoP), symbolic counting of reuse distances could induce polynomials for each reuse pair. These polynomials along with cache capacity constraints lead to non-affine (semi-algebraic) sets; and counting these sets is considered to be a hard problem. The state-of-the-art methods use various exact enumeration techniques relying on existing cardinality algorithms that can efficiently count affine sets. We propose BullsEye , a novel, scalable, accurate, and problem-size independent approximation framework. It is an analytical cache model for fully associative caches with LRU replacement policy focusing on sampling and linearization of non-affine stack distance polynomials. First, we propose a simple domain sampling method that can improve the scalability of exact enumeration. Second, we propose linearization techniques relying on Handelman’s theorem and Bernstein’s representation . To improve the scalability of the Handelman’s theorem linearization technique, we propose template (Interval or Octagon) sub-polyhedral approximations. Our methods obtain significant compile-time improvements with high-accuracy when compared to HayStack on important polyhedral compilation kernels such as nussinov , cholesky , and adi from PolyBench , and harris , gaussianblur from LLVM -TestSuite. Overall, on PolyBench kernels, our methods show up to 3.31× (geomean) speedup with errors below ≈ 0.08% (geomean) for the octagon sub-polyhedral approximation.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4292738089",
    "type": "article"
  },
  {
    "title": "Quantifying Resource Contention of Co-located Workloads with the System-level Entropy",
    "doi": "https://doi.org/10.1145/3563696",
    "publication_date": "2022-09-22",
    "publication_year": 2022,
    "authors": "Yi Liang; Shaokang Zeng; Lei Wang",
    "corresponding_authors": "",
    "abstract": "The workload co-location, such as deploying offline analysis workloads with online service workloads on the same node, has become common for modern data centers. Workload co-location deployment improves data center resource utilization significantly. Still, it also introduces resource contention, resulting in the online service’s quality of service fluctuation, which we call performance interference. As the online service is a tail-latency-sensitive workload, the tail-latency metric can reflect the performance interference degree of the co-location workloads at the application level. However, to guide system design and evaluation, quantitatively evaluating the resource contention of the co-located workloads at the system level is also essential. This article proposes a novel metric called System-Level Entropy (SLE). As a system-level metric, SLE can measure quantitatively resource contention of the co-location systems and perform the apples-to-apples comparison between systems. The experimental results show that SLE can accurately reflect the performance interference of workloads and then evaluate the system resource contention. We also demonstrate two case studies of the SLE. We quantify the affinity of different co-location combinations, including three online services and five offline workloads. Furthermore, we evaluate the effects of state-of-the-art isolated mechanisms (the container and the CPU–affinity binding) with these co-location combinations.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4296642740",
    "type": "article"
  },
  {
    "title": "As-Is Approximate Computing",
    "doi": "https://doi.org/10.1145/3559761",
    "publication_date": "2022-10-08",
    "publication_year": 2022,
    "authors": "Mitali Soni; Asmita Pal; Joshua San Miguel",
    "corresponding_authors": "",
    "abstract": "Although approximate computing promises better performance for applications allowing marginal errors, dearth of hardware support and lack of run-time accuracy guarantees makes it difficult to adopt. We present As-Is, an Anytime Speculative Interruptible System that takes an approximate program and executes it with time-proportional approximations. That is, an approximate version of the program output is generated early and is gradually refined over time, thus providing the run-time guarantee of eventually reaching 100% accuracy. The novelty of our As-Is architecture is in its ability to conceptually marry approximate computing and speculative computing. We show how existing innovations in speculative architectures can be repurposed for anytime, best-effort approximation, facilitating the design efforts and overheads needed for approximate hardware support. As-Is provides a platform for real-time constraints and interactive users to interrupt programs early and accept their current approximate results as is. 100% accuracy is always guaranteed if more time can be spared. Our evaluations demonstrate favorable performance-accuracy tradeoffs for a range of approximate applications.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4303647439",
    "type": "article"
  },
  {
    "title": "An Optimized Framework for Matrix Factorization on the New Sunway Many-core Platform",
    "doi": "https://doi.org/10.1145/3571856",
    "publication_date": "2022-11-19",
    "publication_year": 2022,
    "authors": "Wenjing Ma; Fangfang Liu; Daokun Chen; Qinglin Lu; Yi Hu; Hongsen Wang; Xinhui Yuan",
    "corresponding_authors": "",
    "abstract": "Matrix factorization functions are used in many areas and often play an important role in the overall performance of the applications. In the LAPACK library, matrix factorization functions are implemented with blocked factorization algorithm, shifting most of the workload to the high-performance Level-3 BLAS functions. But the non-blocked part, the panel factorization, becomes the performance bottleneck, especially for small- and medium-size matrices that are the common cases in many real applications. On the new Sunway many-core platform, the performance bottleneck of panel factorization can be alleviated by keeping the panel in the LDM for the panel factorization. Therefore, we propose a new framework for implementing matrix factorization functions on the new Sunway many-core platform, facilitating the in-LDM panel factorization. The framework provides a template class with wrapper functions, which integrates inter-CPE communication for the Level-1 and Level-2 BLAS functions with flexible interfaces and can accommodate different partitioning schemes. With the framework, writing panel factorization code with data residing in the LDM space can be done with much higher productivity. We implemented three functions ( dgetrf , dgeqrf , and dpotrf ) based on the framework and compared our work with a CPE_BLAS version, which uses the original LAPACK implementation linked with optimized BLAS library that runs on the CPE mesh. Using the most favorable partitioning, the panel factorization part achieves speedup of up to 26.3, 19.1, and 18.2 for the three matrix factorization functions. For the whole function, our implementation is based on a carefully tuned recursion framework, and we added specific optimization to some subroutines used in the factorization functions. Overall, we obtained average speedup of 9.76 on dgetrf , 10.12 on dgeqrf , and 4.16 on dpotrf , compared to the CPE_BLAS version. Based on the current template class, our work can be extended to support more categories of linear algebra functions.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4309561838",
    "type": "article"
  },
  {
    "title": "Lavender: An Efficient Resource Partitioning Framework for Large-Scale Job Colocation",
    "doi": "https://doi.org/10.1145/3674736",
    "publication_date": "2024-06-24",
    "publication_year": 2024,
    "authors": "W. Peng; Yusen Li; Xiaoguang Liu; Gang Wang",
    "corresponding_authors": "",
    "abstract": "Workload consolidation is a widely used approach to enhance resource utilization in modern data centers. However, the concurrent execution of multiple jobs on a shared server introduces contention for essential shared resources such as CPU cores, Last Level Cache, and memory bandwidth. This contention negatively impacts job performance, leading to significant degradation in throughput. To mitigate resource contention, effective resource isolation techniques at the software or hardware level can be employed to partition the shared resources among colocated jobs. However, existing solutions for resource partitioning often assume a limited number of jobs that can be colocated, making them unsuitable for scenarios with a large-scale job colocation due to several critical challenges. In this study, we propose Lavender, a framework specifically designed for addressing large-scale resource partitioning problems. Lavender incorporates several key techniques to tackle the challenges associated with large-scale resource partitioning, ensuring efficiency, adaptivity, and optimality. We conducted comprehensive evaluations of Lavender to validate its performance and analyze the reasons for its advantages. The experimental results demonstrate that Lavender significantly outperforms state-of-the-art baselines. Lavender is publicly available at https://github.com/yanxiaoqi932/OpenSourceLavender.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4399984557",
    "type": "article"
  },
  {
    "title": "AG-SpTRSV: An Automatic Framework to Optimize Sparse Triangular Solve on GPUs",
    "doi": "https://doi.org/10.1145/3674911",
    "publication_date": "2024-06-25",
    "publication_year": 2024,
    "authors": "Zhengding Hu; Jingwei Sun; Zhongyang Li; Guangzhong Sun",
    "corresponding_authors": "",
    "abstract": "Sparse Triangular Solve (SpTRSV) has long been an essential kernel in the field of scientific computing. Due to its low computational intensity and internal data dependencies, SpTRSV is hard to implement and optimize on graphics processing units (GPUs). Based on our experimental observations, existing implementations on GPUs fail to achieve the optimal performance due to their suboptimal parallelism setups and code implementations plus lack of consideration of the irregular data distribution. Moreover, their algorithm design lacks the adaptability to different input matrices, which may involve substantial manual efforts of algorithm redesigning and parameter tuning for performance consistency. In this work, we propose AG-SpTRSV, an automatic framework to optimize SpTRSV on GPUs, which provides high performance on various matrices while eliminating the costs of manual design. AG-SpTRSV abstracts the procedures of optimizing an SpTRSV kernel as a scheme and constructs a comprehensive optimization space based on it. By defining a unified code template and preparing code variants, AG-SpTRSV enables fine-grained dynamic parallelism and adaptive code optimizations to handle various tasks. Through computation graph transformation and multi-hierarchy heuristic scheduling, AG-SpTRSV generates schemes for task partitioning and mapping, which effectively address the issues of irregular data distribution and internal data dependencies. AG-SpTRSV searches for the best scheme to optimize the target kernel for the specific matrix. A learned lightweight performance model is also introduced to reduce search costs and provide an efficient end-to-end solution. Experimental results with SuiteSparse Matrix Collection on NVIDIA Tesla A100 and RTX 3080 Ti show that AG-SpTRSV outperforms state-of-the-art implementations with geometric average speedups of 2.12x ∼ 3.99x. With the performance model enabled, AG-SpTRSV can provide an efficient end-to-end solution, with preprocessing times ranging from 3.4 to 245 times of the execution time.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4400017463",
    "type": "article"
  },
  {
    "title": "Hyperion: A Highly Effective Page and PC Based Delta Prefetcher",
    "doi": "https://doi.org/10.1145/3675398",
    "publication_date": "2024-07-01",
    "publication_year": 2024,
    "authors": "Yujie Cui; W Chen; Xu Cheng; Jiangfang Yi",
    "corresponding_authors": "",
    "abstract": "Hardware prefetching plays an important role in modern processors for hiding memory access latency. Delta prefetchers show great potential at the L1D cache level, as they can impose small storage overhead by recording deltas. Furthermore, local delta prefetchers, such as Berti, have been shown to achieve high L1D accuracy. However, there is still room for improving the L1D coverage of existing delta prefetchers. Our goal is to develop a delta prefetcher capable of achieving both high L1D coverage and accuracy. We explore delta prefetchers trained on various types of contextual information, ranging from coarse-grained to fine-grained, and analyze their L1D coverage and accuracy. Our findings indicate that training deltas based on the access histories of both PCs and memory pages for individual PCs and memory pages can lead to increased L1D coverage alongside high accuracy. Therefore, we introduce Hyperion, a highly efficient Page and PC-based delta prefetcher. In terms of the vital component of recording access histories, we implement three different structures and engage in a detailed discussion about them. Furthermore, Hyperion utilizes micro-architecture information (e.g., L1D hits or misses, PQ occupancy) and real-time L1D accuracy to dynamically adjust its issuing mechanism, further enhancing performance and L1D accuracy. Our results show that Hyperion achieves an L1D accuracy of 92.4% and an L1D coverage of 51.9%, along with an L2C coverage of 63.0% and an LLC coverage of 67.5% across a diverse range of applications, including SPEC CPU2006, SPEC CPU2017, GAP, and PARSEC, with a baseline of no prefetching. Regarding performance, Hyperion achieves a 50.1% performance gain, outperforming the state-of-the-art delta prefetcher Berti by 5.0% over baseline across all memory-intensive traces from the four benchmark suites.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4400205501",
    "type": "article"
  },
  {
    "title": "LO-SpMM: Low-cost Search for High-performance SpMM Kernels on GPUs",
    "doi": "https://doi.org/10.1145/3685277",
    "publication_date": "2024-07-29",
    "publication_year": 2024,
    "authors": "J. Lin; Jingwei Sun; Xiaolong Shi; Honghe Zhang; Xianzhi Yu; Xinzhi Wang; Jun Yao; Guangzhong Sun",
    "corresponding_authors": "",
    "abstract": "As deep neural networks (DNNs) become increasingly large and complicated, pruning techniques are proposed for lower memory footprint and more efficient inference. The most critical kernel to execute pruned sparse DNNs on GPUs is Sparse-dense Matrix Multiplication (SpMM). To maximize the performance of SpMM, despite the high-performance implementation generated from advanced tensor compilers, they often take a long time to iteratively search tuning configurations. Such a long time slows down the cycle of exploring better DNN architectures or pruning algorithms. In this paper, we propose LO-SpMM to efficiently generate high-performance SpMM implementations for sparse DNN inference. Based on the analysis of nonzero elements’ layout, the characterization of the GPU architecture, and a rank-based cost model, LO-SpMM can effectively reduce the search space and eliminate possibly low-performance candidates. Besides, rather than generating complete SpMM implementations for evaluation, LO-SpMM constructs simplified proxies to quickly estimate performance, thereby substantially reducing compilation and execution costs. Experimental results show that LO-SpMM can reduce the search time by 281 × at most, while the performance of generated SpMM implementations is comparable to or better than the state-of-the-art sparse tensor compiling solutions.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4401090539",
    "type": "article"
  },
  {
    "title": "TPRepair: Tree-Based Pipelined Repair in Clustered Storage Systems",
    "doi": "https://doi.org/10.1145/3705895",
    "publication_date": "2024-12-06",
    "publication_year": 2024,
    "authors": "Jiahui Yang; Fulin Nan; Zhirong Shen; Jiwu Shu; Zhisheng Chen; Xiaoli Wang; Quanqing Xu; Chuanhui Yang; Dmitrii Kaplun; Yuhui Cai",
    "corresponding_authors": "",
    "abstract": "Erasure coding is an effective technique for guaranteeing data reliability for storage systems, yet it incurs a high repair penalty with amplified repair traffic. The repair becomes more intricate in clustered storage systems with the bandwidth diversity property. We present TPRepair , a T ree-based P ipelined Repair approach, aiming to expedite the overall repair process with the tailored pipelined repair procedure. TPRepair first prioritizes selecting racks with the current minimum load to participate in the repair process. It subsequently formulates tree-based links, tailored to align seamlessly with the pipelined repair procedure. TPRepair further designs an optimization algorithm to reduce the bottleneck load when repairing multiple chunks. Large-scale simulations demonstrate that TPRepair can increase 13.8%-41.3% of the balance ratio without amplifying cross-rack traffic. Meanwhile, Alibaba Cloud ECS experiments indicate that TPRepair can increase repair throughput by 11.3% to 72.9%.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4405114236",
    "type": "article"
  },
  {
    "title": "MasterPlan: A Reinforcement Learning Based Scheduler for Archive Storage",
    "doi": "https://doi.org/10.1145/3708542",
    "publication_date": "2024-12-17",
    "publication_year": 2024,
    "authors": "X.-Y. Chen; Erci Xu; Dengyao Mo; Ruiming Lu; Hongbing Wu; Dian Ding; Guangtao Xue",
    "corresponding_authors": "",
    "abstract": "With the sheer volume of data in today’s world, archive storage systems play a significant role in persisting the cold data. Due to stringent cost concerns, one popular design is to organize disks into groups and periodically switch them to be powered on for serving user requests. Scheduling thus becomes critical for both CapEx and performance. Unfortunately, field results indicate that existing schedulers can be often suboptimal. Our further analysis suggests that the main reason is the mismatch between the ever-changing workloads and the fixed set of coarsely-configured parameters in current heuristic-based schedulers. In this paper, we propose MasterPlan , a reinforcement learning (RL) based scheduler for archive storage systems. By identifying the unique characteristics of archive storage service, we design a state space and reward function for the RL agent. MasterPlan includes a continuous action encoding approach to guarantee efficient exploration, and a meta adaptation module to extract features of workload series. Experiments show that MasterPlan can achieve 1.25 × throughput, 2.16 × 99 th latency and 1.47 × power draw improvement compared to existing solutions.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4405471691",
    "type": "article"
  },
  {
    "title": "Virtual Ways: Low-Cost Coherence for Instruction Set Extensions with Architecturally Visible Storage",
    "doi": "https://doi.org/10.1145/2576877",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "Theo Kluter; Samuel Burri; Philip Brisk; Edoardo Charbon; Paolo Ienne",
    "corresponding_authors": "",
    "abstract": "Instruction set extensions (ISEs) improve the performance and energy consumption of application-specific processors. ISEs can use architecturally visible storage (AVS), localized compiler-controlled memories, to provide higher I/O bandwidth than reading data from the processor pipeline. AVS creates coherence and consistence problems with the data cache. Although a hardware coherence protocol could solve the problem, this approach is costly for a single-processor system. As a low-cost alternative, we introduce Virtual Ways, which ensures coherence through a reduced form of inclusion between the data cache and AVS. Virtual Ways achieve higher performance and lower energy consumption than using a hardware coherence protocol.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2066498039",
    "type": "article"
  },
  {
    "title": "Tuning the continual flow pipeline architecture with virtual register renaming",
    "doi": "https://doi.org/10.1145/2579675",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "Komal Jothi; Haitham Akkary",
    "corresponding_authors": "",
    "abstract": "Continual Flow Pipelines (CFPs) allow a processor core to process hundreds of in-flight instructions without increasing cycle-critical pipeline resources. When a load misses the data cache, CFP checkpoints the processor register state and then moves all miss-dependent instructions into a low-complexity WB to unblock the pipeline. Meanwhile, miss-independent instructions execute normally and update the processor state. When the miss data return, CFP replays the miss-dependent instructions from the WB and then merges the miss-dependent and miss-independent execution results. CFP was initially proposed for cache misses to DRAM. Later work focused on reducing the execution overhead of CFP by avoiding the pipeline flush before replaying miss-dependent instructions and executing dependent and independent instructions concurrently. The goal of these improvements was to gain performance by applying CFP to L1 data cache misses that hit the last level on chip cache. However, many applications or execution phases of applications incur excessive amount of replay and/or rollbacks to the checkpoint. This frequently cancels benefits from CFP and reduces performance. In this article, we improve the CFP architecture by using a novel virtual register renaming substrate and by tuning the replay policies to mitigate excessive replays and rollbacks to the checkpoint. We describe these new design optimizations and show, using Spec 2006 benchmarks and microarchitecture performance and power models of our design, that our Tuned-CFP architecture improves performance and energy consumption over previous CFP architectures by ∼10% and ∼8%, respectively. We also demonstrate that our proposed architecture gives better performance return on energy per instruction compared to a conventional superscalar as well as previous CFP architectures.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2079746183",
    "type": "article"
  },
  {
    "title": "Power Efficient Hardware Transactional Memory",
    "doi": "https://doi.org/10.1145/2875425",
    "publication_date": "2016-03-28",
    "publication_year": 2016,
    "authors": "Sang Wook Stephen; Michel Dubois",
    "corresponding_authors": "",
    "abstract": "Transactional Memory (TM) is no longer just an academic interest as industry has started to adopt the idea in its commercial products. In this paper, we propose Dynamic Transaction Issue (DTI), a new scheme that can be easily implemented on top of existing Hardware TM (HTM) systems, provided additional messages. Instead of wasting power and energy in transaction aborts, Dynamic Transaction Issue puts a processor core into a low-power state when there is a reasonable suspicion that the current transaction running on it will be aborted soon in the future. We have implemented Dynamic Transaction Issue on a cycle-accurate simulator of a multicore processor system with out-of-order superscalar cores, augmented with a power package and a TM package which add accurate dynamic power estimates and a TM framework to the simulator. Our simulation results show that Dynamic Transaction Issue can achieve energy savings up to 37% from the energy consumption of a base machine with no mechanism to suppress useless aborts. We also compare Dynamic Transaction Issue with various alternative hardware TM mechanisms.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2318357715",
    "type": "article"
  },
  {
    "title": "List of Distinguished Reviewers ACM TACO 2014",
    "doi": "https://doi.org/10.1145/2989990",
    "publication_date": "2016-09-17",
    "publication_year": 2016,
    "authors": "Manuel E. Acacio",
    "corresponding_authors": "Manuel E. Acacio",
    "abstract": "editorial Free Access Share on List of Distinguished Reviewers ACM TACO 2014 Author: Manuel Acacio University of Murcia, Spain University of Murcia, SpainView Profile Authors Info & Claims ACM Transactions on Architecture and Code OptimizationVolume 13Issue 3September 2016 Article No.: 31pp 1–3https://doi.org/10.1145/2989990Published:17 September 2016Publication History 0citation181DownloadsMetricsTotal Citations0Total Downloads181Last 12 Months20Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2522550661",
    "type": "article"
  },
  {
    "title": "Accelerating Intercommunication in Highly Parallel Systems",
    "doi": "https://doi.org/10.1145/3005717",
    "publication_date": "2016-12-02",
    "publication_year": 2016,
    "authors": "Nikolaos Tampouratzis; Pavlos M. Mattheakis; Ioannis Papaefstathiou",
    "corresponding_authors": "",
    "abstract": "Every HPC system consists of numerous processing nodes interconnect using a number of different inter-process communication protocols such as Messaging Passing Interface (MPI) and Global Arrays (GA). Traditionally, research has focused on optimizing these protocols and identifying the most suitable ones for each system and/or application. Recently, there has been a proposal to unify the primitive operations of the different inter-processor communication protocols through the Portals library. Portals offer a set of low-level communication routines which can be composed in order to implement the functionality of different intercommunication protocols. However, Portals modularity comes at a performance cost, since it adds one more layer in the actual protocol implementation. This work aims at closing the performance gap between a generic and reusable intercommunication layer, such as Portals, and the several monolithic and highly optimized intercommunication protocols. This is achieved through the development of a novel hardware offload engine efficiently implementing the basic Portals’ modules. Our innovative system is up to two2 orders of magnitude faster than the conventional software implementation of Portals’ while the speedup achieved over the conventional monolithic software implementations of MPI and GAs is more than an order of magnitude. The power consumption of our hardware system is less than 1/100th of what a low-power CPU consumes when executing the Portal's software while its silicon cost is less than 1/10th of that of a very simple RISC CPU. Moreover, our design process is also innovative since we have first modeled the hardware within an untimed virtual prototype which allowed for rapid design space exploration; then we applied a novel methodology to transform the untimed description into an efficient timed hardware description, which was then transformed into a hardware netlist through a High-Level Synthesis (HLS) tool.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2558865419",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2988523",
    "publication_date": "2016-09-17",
    "publication_year": 2016,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "In the wake of the current trend of increasing the number of cores on a chip, compiler optimizations for improving the memory performance have assumed increased importance. Loop fusion is one such key optimization that can alleviate memory and bandwidth ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4229910036",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2658949",
    "publication_date": "2014-10-27",
    "publication_year": 2014,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Many new nonvolatile memory (NVM) technologies have been heavily studied to replace the power-hungry SRAM/DRAM-based memory hierarchy in today's computers. Among various emerging NVM technologies, Spin-Transfer Torque RAM (STT-RAM) has many benefits, ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4232341524",
    "type": "paratext"
  },
  {
    "title": "List of Distinguished Reviewers ACM TACO 2014",
    "doi": "https://doi.org/10.1145/2714082",
    "publication_date": "2015-01-09",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "editorial Free Access Share on List of Distinguished Reviewers ACM TACO 2014ACM Transactions on Architecture and Code OptimizationVolume 11Issue 4January 2015 Article No.: 68pp 1–2https://doi.org/10.1145/2714082Published:09 January 2015Publication History 0citation200DownloadsMetricsTotal Citations0Total Downloads200Last 12 Months17Last 6 weeks4 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4234090947",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2775085",
    "publication_date": "2015-07-08",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The Register File (RF) is a particularly vulnerable component within processor core and at the same time a hotspot with high power density. To reduce RF vulnerability, conventional HW-only approaches such as Error Correction Codes (ECCs) or modular ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4234448210",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3012405",
    "publication_date": "2016-12-28",
    "publication_year": 2016,
    "authors": "Keval Vora; Rajiv Gupta; G. Xu",
    "corresponding_authors": "Rajiv Gupta",
    "abstract": "Evolving graph processing involves repeating analyses, which are often iterative, over multiple snapshots of the graph corresponding to different points in time. Since the snapshots of an evolving graph share a great number of vertices and edges, ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4234795571",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2836331",
    "publication_date": "2016-01-07",
    "publication_year": 2016,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "This article proposes Probabilistic Replacement Policy (PRP), a novel replacement policy that evicts the line with minimum estimated hit probability under optimal replacement instead of the line with maximum expected reuse distance. The latter is ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4239191163",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2591460",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We propose a reduced-port Register File (RF) architecture for reducing RF energy in a VLIW processor. With port reduction, RF ports need to be shared among Function Units (FUs), which may lead to access conflicts, and thus, reduced performance. Our ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4240911165",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2744295",
    "publication_date": "2015-04-16",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The number of cores of contemporary processors is constantly increasing and thus continues to deliver ever higher peak performance (following Moore’s transistor law). Yet high core counts present a challenge to hardware and software alike. Following ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4241376581",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2952301",
    "publication_date": "2016-06-27",
    "publication_year": 2016,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Compilers for Course-Grained Reconfigurable Array (CGRA) architectures suffer from long compilation times and code quality levels far below the theoretical upper bounds. This article presents a new scheduler, called the Bimodal Modulo Scheduler (BMS), ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4244567693",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2818748",
    "publication_date": "2015-10-06",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Classification of data into private and shared has proven to be a catalyst for techniques to reduce coherence cost, since private data can be taken out of coherence and resources can be concentrated on providing coherence for shared data. In this ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4250525058",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2639036",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Instruction set extensions (ISEs) improve the performance and energy consumption of application-specific processors. ISEs can use architecturally visible storage (AVS), localized compiler-controlled memories, to provide higher I/O bandwidth than reading ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4252090117",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2899032",
    "publication_date": "2016-04-05",
    "publication_year": 2016,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "This work proposes a novel scheme to facilitate heterogeneous systems with unified virtual memory. Research proposals implement coherence protocols for sequential consistency (SC) between central processing unit (CPU) cores and between devices. Such ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4253164041",
    "type": "paratext"
  },
  {
    "title": "A Non-Intrusive Tool Chain to Optimize MPSoC End-to-End Systems",
    "doi": "https://doi.org/10.1145/3445030",
    "publication_date": "2021-02-09",
    "publication_year": 2021,
    "authors": "Maxime France-Pillois; Jérôme Martin; Frédéric Rousseau",
    "corresponding_authors": "",
    "abstract": "Multi-core systems are now found in many electronic devices. But does current software design fully leverage their capabilities? The complexity of the hardware and software stacks in these platforms requires software optimization with end-to-end knowledge of the system. To optimize software performance, we must have accurate information about system behavior and time losses. Standard monitoring engines impose tradeoffs on profiling tools, making it impossible to reconcile all the expected requirements: accurate hardware views, fine-grain measurements, speed, and so on. Subsequently, new approaches have to be examined. In this article, we propose a non-intrusive, accurate tool chain, which can reveal and quantify slowdowns in low-level software mechanisms. Based on emulation, this tool chain extracts behavioral information (time, contention) through hardware side channels, without distorting the software execution flow. This tool consists of two parts. (1) An online acquisition part that dumps hardware platform signals. (2) An offline processing part that consolidates meaningful behavioral information from the dumped data. Using our tool chain, we studied and propose optimizations to MultiProcessor System on Chip (MPSoC) support in the Linux kernel, saving about 60% of the time required for the release phase of the GNU OpenMP synchronization barrier when running on a 64-core MPSoC.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3128786894",
    "type": "article"
  },
  {
    "title": "Decreasing the Miss Rate and Eliminating the Performance Penalty of a Data Filter Cache",
    "doi": "https://doi.org/10.1145/3449043",
    "publication_date": "2021-05-10",
    "publication_year": 2021,
    "authors": "Michael Stokes; David Whalley; Soner Önder",
    "corresponding_authors": "",
    "abstract": "While data filter caches (DFCs) have been shown to be effective at reducing data access energy, they have not been adopted in processors due to the associated performance penalty caused by high DFC miss rates. In this article, we present a design that both decreases the DFC miss rate and completely eliminates the DFC performance penalty even for a level-one data cache (L1 DC) with a single cycle access time. First, we show that a DFC that lazily fills each word in a DFC line from an L1 DC only when the word is referenced is more energy-efficient than eagerly filling the entire DFC line. For a 512B DFC, we are able to eliminate loads of words into the DFC that are never referenced before being evicted, which occurred for about 75% of the words in 32B lines. Second, we demonstrate that a lazily word filled DFC line can effectively share and pack data words from multiple L1 DC lines to lower the DFC miss rate. For a 512B DFC, we completely avoid accessing the L1 DC for loads about 23% of the time and avoid a fully associative L1 DC access for loads 50% of the time, where the DFC only requires about 2.5% of the size of the L1 DC. Finally, we present a method that completely eliminates the DFC performance penalty by speculatively performing DFC tag checks early and only accessing DFC data when a hit is guaranteed. For a 512B DFC, we improve data access energy usage for the DTLB and L1 DC by 33% with no performance degradation.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3160871404",
    "type": "article"
  },
  {
    "title": "Flynn’s Reconciliation",
    "doi": "https://doi.org/10.1145/3458357",
    "publication_date": "2021-06-08",
    "publication_year": 2021,
    "authors": "Daniel Thuerck; Nicolas Weber; Roberto Bifulco",
    "corresponding_authors": "",
    "abstract": "A large portion of the recent performance increase in the High Performance Computing (HPC) and Machine Learning (ML) domains is fueled by accelerator cards. Many popular ML frameworks support accelerators by organizing computations as a computational graph over a set of highly optimized, batched general-purpose kernels. While this approach simplifies the kernels’ implementation for each individual accelerator, the increasing heterogeneity among accelerator architectures for HPC complicates the creation of portable and extensible libraries of such kernels. Therefore, using a generalization of the CUDA community’s warp register cache programming idiom, we propose a new programming idiom (CoRe) and a virtual architecture model (PIRCH), abstracting over SIMD and SIMT paradigms. We define and automate the mapping process from a single source to PIRCH’s intermediate representation and develop backends that issue code for three different architectures: Intel AVX512, NVIDIA GPUs, and NEC SX-Aurora. Code generated by our source-to-source compiler for batched kernels, borG, competes favorably with vendor-tuned libraries and is up to 2× faster than hand-tuned kernels across architectures.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3166489582",
    "type": "article"
  },
  {
    "title": "Fast Key-Value Lookups with Node Tracker",
    "doi": "https://doi.org/10.1145/3452099",
    "publication_date": "2021-06-08",
    "publication_year": 2021,
    "authors": "Mustafa Çavus; Mohammed Shatnawi; Resit Sendag; Augustus K. Uht",
    "corresponding_authors": "",
    "abstract": "Lookup operations for in-memory databases are heavily memory bound, because they often rely on pointer-chasing linked data structure traversals. They also have many branches that are hard-to-predict due to random key lookups. In this study, we show that although cache misses are the primary bottleneck for these applications, without a method for eliminating the branch mispredictions only a small fraction of the performance benefit is achieved through prefetching alone. We propose the Node Tracker (NT), a novel programmable prefetcher/pre-execution unit that is highly effective in exploiting inter key-lookup parallelism to improve single-thread performance. We extend NT with branch outcome streaming (BOS) to reduce branch mispredictions and show that this achieves an extra 3× speedup. Finally, we evaluate the NT as a pre-execution unit and demonstrate that we can further improve the performance in both single- and multi-threaded execution modes. Our results show that, on average, NT improves single-thread performance by 4.1× when used as a prefetcher; 11.9× as a prefetcher with BOS; 14.9× as a pre-execution unit and 18.8× as a pre-execution unit with BOS. Finally, with 24 cores of the latter version, we achieve a speedup of 203× and 11× over the single-core and 24-core baselines, respectively.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3169543748",
    "type": "article"
  },
  {
    "title": "PRISM",
    "doi": "https://doi.org/10.1145/3450523",
    "publication_date": "2021-06-08",
    "publication_year": 2021,
    "authors": "Hamza Omar; Omer Khan",
    "corresponding_authors": "",
    "abstract": "Multicores increasingly deploy safety-critical parallel applications that demand resiliency against soft-errors to satisfy the safety standards. However, protection against these errors is challenging due to complex communication and data access protocols that aggressively share on-chip hardware resources. Research has explored various temporal and spatial redundancy-based resiliency schemes that provide multicores with high soft-error coverage. However, redundant execution incurs performance overheads due to interference effects induced by aggressive resource sharing. Moreover, these schemes require intrusive hardware modifications and fall short in providing efficient system availability guarantees. This article proposes PRISM, a resilient multicore architecture that incorporates strong hardware isolation to form redundant clusters of cores, ensuring a non-interference-based redundant execution environment. A soft error in one cluster does not effect the execution of the other cluster, resulting in high system availability. Implementing strong isolation for shared hardware resources, such as queues, caches, and networks requires logic for partitioning. However, it is less intrusive as complex hardware modifications to protocols, such as hardware cache coherence, are avoided. The PRISM approach is prototyped on a real Tilera Tile-Gx72 processor that enables primitives to implement the proposed cluster-level hardware resource isolation. The evaluation shows performance benefits from avoiding destructive hardware interference effects with redundant execution, while delivering superior system availability.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3171256430",
    "type": "article"
  },
  {
    "title": "CacheInspector",
    "doi": "https://doi.org/10.1145/3457373",
    "publication_date": "2021-06-08",
    "publication_year": 2021,
    "authors": "Weijia Song; Christina Delimitrou; Zhiming Shen; Robbert van Renesse; Hakim Weatherspoon; Lotfi Benmohamed; Frederic de Vaulx; Charif Mahmoudi",
    "corresponding_authors": "",
    "abstract": "Infrastructure-as-a-Service cloud providers sell virtual machines that are only specified in terms of number of CPU cores, amount of memory, and I/O throughput. Performance-critical aspects such as cache sizes and memory latency are missing or reported in ways that make them hard to compare across cloud providers. It is difficult for users to adapt their application’s behavior to the available resources. In this work, we aim to increase the visibility that cloud users have into shared resources on public clouds. Specifically, we present CacheInspector , a lightweight runtime that determines the performance and allocated capacity of shared caches on multi-tenant public clouds. We validate CacheInspector ’s accuracy in a controlled environment, and use it to study the characteristics and variability of cache resources in the cloud, across time, instances, availability regions, and cloud providers. We show that CacheInspector ’s output allows cloud users to tailor their application’s behavior, including their output quality, to avoid suboptimal performance when resources are scarce.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3171289365",
    "type": "article"
  },
  {
    "title": "Towards Enhanced System Efficiency while Mitigating Row Hammer",
    "doi": "https://doi.org/10.1145/3458749",
    "publication_date": "2021-07-17",
    "publication_year": 2021,
    "authors": "Kaustav Goswami; Dip Sankar Banerjee; Shirshendu Das",
    "corresponding_authors": "",
    "abstract": "In recent years, DRAM-based main memories have become susceptible to the Row Hammer (RH) problem, which causes bits to flip in a row without accessing them directly. Frequent activation of a row, called an aggressor row , causes its adjacent rows’ ( victim ) bits to flip. The state-of-the-art solution is to refresh the victim rows explicitly to prevent bit flipping. There have been several proposals made to detect RH attacks. These include both probabilistic as well as deterministic counter-based methods. The technique of handling RH attacks, however, remains the same. In this work, we propose an efficient technique for handling the RH problem. We show that the mechanism is agnostic of the detection mechanism. Our RH handling technique omits the necessity of refreshing the victim rows. Instead, we use a small non-volatile Spin-Transfer Torque Magnetic Random Access Memory (STTRAM) that ensures no unnecessary refreshes of the victim rows on the DRAM device and thus allowing more time for normal applications in the same DRAM device. Our model relies on the migration of the aggressor rows. This accounts for removing blocking of the DRAM operations due to the refreshing of victim rows incurred in the previous solution. After extensive evaluation, we found that, compared to the conventional RH mitigation techniques, our model minimizes the blocking time of the memory that is imposed due to explicit refreshing by an average of 80.72% in the worst-case scenario and provides energy savings of about 15.82% on average, across different types of RH-based workloads. A lookup table is necessary to pinpoint the location of a particular row, which, when combined with the STTMRAM, limits the storage overhead to 0.39% of a 2 GB DRAM. Our proposed model prevents repeated refreshing of the same victim rows in different refreshing windows on the DRAM device and leads to an efficient RH handling technique.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3186069602",
    "type": "article"
  },
  {
    "title": "Efficient and effective misaligned data access handling in a dynamic binary translation system",
    "doi": "https://doi.org/10.1145/1970386.1970388",
    "publication_date": "2011-06-21",
    "publication_year": 2011,
    "authors": "Jianjun Li; Chenggang Wu; Wei‐Chung Hsu",
    "corresponding_authors": "",
    "abstract": "Binary Translation (BT) has been commonly used to migrate application software across Instruction Set Architectures (ISAs). Some architectures, such as X86, allow Misaligned Data Accesses (MDAs), while most modern architectures require natural data alignments. In a binary translation system, where the source ISA allows MDA and the target ISA does not, memory operations must be carefully translated. Naive translation may cause frequent misaligned data access traps to occur at runtime on the target machine and severely slow down the migrated application. This article evaluates different approaches in handling MDA in a binary translation system including how to identify MDA candidates and how to translate such memory instructions. This article also proposes some new mechanisms to more effectively deal with MDAs. Extensive measurements based on SPEC CPU2000 and CPU2006 benchmarks show that the proposed approaches are more effective than existing methods and getting close to the performance upper bound of MDA handling.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2040531796",
    "type": "article"
  },
  {
    "title": "SCIN-cache",
    "doi": "https://doi.org/10.1145/2400682.2400717",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Anurag Negi; Rubén Titos-Gil",
    "corresponding_authors": "",
    "abstract": "This article describes cache designs for efficiently supporting speculative techniques like transactional memory on chip multiprocessors with multithreaded cores. On-demand allocation and prompt freeing of speculative cache space in the design reduces the burden on nonspeculative execution. Quick access to both clean and speculative versions of data for multiple contexts provides flexibility and greater design freedom to HTM architects. Performance analysis shows the designs stand up well against other HTM design proposals, with potential performance gains in high contention applications with small transactions.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2098633867",
    "type": "article"
  },
  {
    "title": "Scalable high-radix router microarchitecture using a network switch organization",
    "doi": "https://doi.org/10.1145/2509420.2512433",
    "publication_date": "2013-09-01",
    "publication_year": 2013,
    "authors": "Jung Ho Ahn; Young Hoon Son; John Kim",
    "corresponding_authors": "",
    "abstract": "As the system size of supercomputers and datacenters increases, cost-efficient networks become critical in achieving good scalability on those systems. High-radix routers reduce network cost by lowering the network diameter while providing a high bisection bandwidth and path diversity. The building blocks of these large-scale networks are the routers or the switches and they need to scale accordingly to the increasing port count and increasing pin bandwidth. However, as the port count increases, the high-radix router microarchitecture itself needs to scale efficiently. Hierarchical crossbar switch organization has been proposed where a single large crossbar used for a router switch is partitioned into many small crossbars and overcomes the limitations of conventional router microarchitecture. Although the organization provides high performance, it has limited scalability due to excessive power and area overheads by the wires and intermediate buffers.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4231654921",
    "type": "article"
  },
  {
    "title": "Reducing instruction fetch energy in multi-issue processors",
    "doi": "https://doi.org/10.1145/2555289.2555318",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Peter Gavin; David Whalley; Magnus Själander",
    "corresponding_authors": "",
    "abstract": "The need to minimize power while maximizing performance has led to recent developments of powerful superscalar designs targeted at embedded and portable use. Instruction fetch is responsible for a significant fraction of microprocessor power and energy, and is therefore an attractive target for architectural power optimization. We present novel techniques that take advantage of guarantees so that the instruction translation lookaside buffer, branch target buffer, and branch prediction buffer can frequently be disabled, reducing their energy usage, while simultaneously reducing branch predictor contention. These techniques require no changes to the instruction set and can easily be integrated into most single- and multiple-issue processors.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4231881390",
    "type": "article"
  },
  {
    "title": "GPU code generation for ODE-based applications with phased shared-data access patterns",
    "doi": "https://doi.org/10.1145/2555289.2555311",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Andrei Hagiescu; Bing Liu; Ravishankar Ramanathan; Sucheendra K. Palaniappan; Zheng Cui; Bipasa Chattopadhyay; P. S. Thiagarajan; Weng‐Fai Wong",
    "corresponding_authors": "",
    "abstract": "We present a novel code generation scheme for GPUs. Its key feature is the platform-aware generation of a heterogeneous pool of threads. This exposes more data-sharing opportunities among the concurrent threads and reduces the memory requirements that would otherwise exceed the capacity of the on-chip memory. Instead of the conventional strategy of focusing on exposing as much parallelism as possible, our scheme leverages on the phased nature of memory access patterns found in many applications that exhibit massive parallelism. We demonstrate the effectiveness of our code generation strategy on a computational systems biology application. This application consists of computing a Dynamic Bayesian Network (DBN) approximation of the dynamics of signalling pathways described as a system of Ordinary Differential Equations (ODEs). The approximation algorithm involves (i) sampling many (of the order of a few million) times from the set of initial states, (ii) generating trajectories through numerical integration, and (iii) storing the statistical properties of this set of trajectories in Conditional Probability Tables (CPTs) of a DBN via a prespecified discretization of the time and value domains. The trajectories can be computed in parallel. However, the intermediate data needed for computing them, as well as the entries for the CPTs, are too large to be stored locally. Our experiments show that the proposed code generation scheme scales well, achieving significant performance improvements on three realistic signalling pathways models. These results suggest how our scheme could be extended to deal with other applications involving systems of ODEs.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4233913793",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2459316",
    "publication_date": "2013-05-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "A systematic methodology for near-optimal software/hardware codesign mapping onto an FPGA platform with microprocessor and HW accelerators is proposed. The mapping steps deal with the inter-organization, the foreground memory management, and the ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4235407021",
    "type": "paratext"
  },
  {
    "title": "Profile-guided transaction coalescing—lowering transactional overheads by merging transactions",
    "doi": "https://doi.org/10.1145/2555289.2555306",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Srđan Stipić; Vesna Smiljković; Osman Ünsal; Adrián Cristal; Mateo Valero",
    "corresponding_authors": "",
    "abstract": "Previous studies in software transactional memory mostly focused on reducing the overhead of transactional read and write operations. In this article, we introduce transaction coalescing, a profile-guided compiler optimization technique that attempts to reduce the overheads of starting and committing a transaction by merging two or more small transactions into one large transaction. We develop a profiling tool and a transaction coalescing heuristic to identify candidate transactions suitable for coalescing. We implement a compiler extension to automatically merge the candidate transactions at the compile time. We evaluate the effectiveness of our technique using the hash table micro-benchmark and the STAMP benchmark suite. Transaction coalescing improves the performance of the hash table significantly and the performance of Vacation and SSCA2 benchmarks by 19.4% and 36.4%, respectively, when running with 12 threads.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4235993954",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2019608",
    "publication_date": "2011-10-01",
    "publication_year": 2011,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "This article proposes techniques to predict the performance impact of pending cache hits, hardware prefetching, and miss status holding register resources on superscalar microprocessors using hybrid analytical models. The proposed models focus on ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4240010312",
    "type": "paratext"
  },
  {
    "title": "HPar",
    "doi": "https://doi.org/10.1145/2555289.2555301",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Zhijia Zhao; Michael Bebenita; Dave Herman; Jianhua Sun; Xipeng Shen",
    "corresponding_authors": "",
    "abstract": "Parallelizing HTML parsing is challenging due to the complexities of HTML documents and the inherent dependencies in its parsing algorithm. As a result, despite numerous studies in parallel parsing, HTML parsing remains sequential today. It forms one of the final barriers for fully parallelizing browser operations to minimize the browser’s response time—an important variable for user experiences, especially on portable devices. This article provides a comprehensive analysis on the special complexities of parallel HTML parsing and presents a systematic exploration in overcoming those difficulties through specially designed speculative parallelizations. This work develops, to the best of our knowledge, the first pipelining and data-level parallel HTML parsers. The data-level parallel parser, named HPar, achieves up to 2.4× speedup on quadcore devices. This work demonstrates the feasibility of efficient, parallel HTML parsing for the first time and offers a set of novel insights for parallel HTML parsing",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4240070498",
    "type": "article"
  },
  {
    "title": "Low-latency adaptive mode transitions and hierarchical power management in asymmetric clustered cores",
    "doi": "https://doi.org/10.1145/2509420.2499901",
    "publication_date": "2013-09-01",
    "publication_year": 2013,
    "authors": "Eran Shifer; Shlomo Weiss",
    "corresponding_authors": "",
    "abstract": "Recently, engineering solutions that include asymmetric multicores have been fabricated for low form-factor computing devices, indicating a potential direction for future evolution of processors. In this article we propose an asymmetric clustered core architecture, exhibiting low-latency switching between modes relative to asymmetric multicores, and having similarities with the same asymmetric multicore architecture in the context of a wider dynamic range of the processor power-performance characteristic. Asymmetric clustered cores incur additional microarchitectural complexity and area cost inside a core but exhibit better chip-level integration characteristics compared to asymmetric multicores. Focusing on power efficiency of asymmetric clustered cores, we describe: (1) a hierarchical power management partitioning between the operating system and on-die firmware for coarse-grain switch policies, and (2) core-internal tracking hardware for fine-grain switching. The mode switch policies of the core's tracking hardware are dependent on higher-level directives and hints from the operating system, on-die firmware, and compiler or profiling software. We further explore the potential power management benefits of asymmetric clustered cores relative to asymmetric multicores, demonstrating that the ability of asymmetric clustered cores to use tight training periods for adaptive behavior, with low overhead switching between modes, results in a more efficient utilization of power management directives.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4240319387",
    "type": "article"
  },
  {
    "title": "Orchestrating stream graphs using model checking",
    "doi": "https://doi.org/10.1145/2509420.2512435",
    "publication_date": "2013-09-01",
    "publication_year": 2013,
    "authors": "Avinash Malik; David Gregg",
    "corresponding_authors": "",
    "abstract": "In this article we use model checking to statically distribute and schedule Synchronous DataFlow (SDF) graphs on heterogeneous execution architectures. We show that model checking is capable of providing an optimal solution and it arrives at these solutions faster (in terms of algorithm runtime) than equivalent ILP formulations. Furthermore, we also show how different types of optimizations such as task parallelism, data parallelism, and state sharing can be included within our framework. Finally, comparison of our approach with the current state-of-the-art heuristic techniques show the pitfalls of these techniques and gives a glimpse of how these heuristic techniques can be improved.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4242343298",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2541228",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Dynamic or Just-in-Time (JIT) compilation is essential to achieve high-performance emulation for programs written in managed languages, such as Java and C#. It has been observed that a conservative JIT compilation policy is most effective to obtain good ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4242872465",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2509420",
    "publication_date": "2013-09-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Recently, engineering solutions that include asymmetric multicores have been fabricated for low form-factor computing devices, indicating a potential direction for future evolution of processors. In this article we propose an asymmetric clustered core ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4244454092",
    "type": "paratext"
  },
  {
    "title": "TornadoNoC",
    "doi": "https://doi.org/10.1145/2555289.2555312",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Jung­hee Lee; Chrysostomos Nicopoulos; Hyung Gyu Lee; Jong Man Kim",
    "corresponding_authors": "",
    "abstract": "The rapid emergence of Chip Multi-Processors (CMP) as the de facto microprocessor archetype has highlighted the importance of scalable and efficient on-chip networks. Packet-based Networks-on-Chip (NoC) are gradually cementing themselves as the medium of choice for the multi-/many-core systems of the near future, due to their innate scalability. However, the prominence of the debilitating power wall requires the NoC to also be as energy efficient as possible. To achieve these two antipodal requirements—scalability and energy efficiency—we propose TornadoNoC, an interconnect architecture that employs a novel flow control mechanism. To prevent livelocks and deadlocks, a sequence numbering scheme and a dynamic ring inflation technique are proposed, and their correctness formally proven. The primary objective of TornadoNoC is to achieve substantial gains in (a) scalability to many-core systems and (b) the area/power footprint, as compared to current state-of-the-art router implementations. The new router is demonstrated to provide better scalability to hundreds of cores than an ideal single-cycle wormhole implementation and other scalability-enhanced low-cost routers. Extensive simulations using both synthetic traffic patterns and real applications running in a full-system simulator corroborate the efficacy of the proposed design. Finally, hardware synthesis analysis using commercial 65nm standard-cell libraries indicates that the area and power budgets of the new router are reduced by up to 53% and 58%, respectively, as compared to existing state-of-the-art low-cost routers.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4248276454",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2555289",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4248436975",
    "type": "paratext"
  },
  {
    "title": "List of distinguished reviewers ACM TACO",
    "doi": "https://doi.org/10.1145/2560216",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "review-article Free Access Share on List of distinguished reviewers ACM TACOACM Transactions on Architecture and Code OptimizationVolume 10Issue 4December 2013 Article No.: 65pp 1–3https://doi.org/10.1145/2560216Online:01 December 2013Publication History 0citation146DownloadsMetricsTotal Citations0Total Downloads146Last 12 Months20Last 6 weeks3 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4248993008",
    "type": "article"
  },
  {
    "title": "PCantorSim",
    "doi": "https://doi.org/10.1145/2555289.2555305",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Chuntao Jiang; Zhibin Yu; Hai Jin; Chengzhong Xu; Lieven Eeckhout; Wim Heirman; Trevor E. Carlson; Xiaofei Liao",
    "corresponding_authors": "",
    "abstract": "Computer architects rely heavily on microarchitecture simulation to evaluate design alternatives. Unfortunately, cycle-accurate simulation is extremely slow, being at least 4 to 6 orders of magnitude slower than real hardware. This longstanding problem is further exacerbated in the multi-/many-core era, because single-threaded simulation performance has not improved much, while the design space has expanded substantially. Parallel simulation is a promising approach, yet does not completely solve the simulation challenge. Furthermore, existing sampling techniques, which are widely used for single-threaded applications, do not readily apply to multithreaded applications as thread interaction and synchronization must now be taken into account. This work presents PCantorSim, a novel Cantor set (a classic fractal)--based sampling scheme to accelerate parallel simulation of multithreaded applications. Through the use of the proposed methodology, only less than 5% of an application's execution time is simulated in detail. We have implemented our approach in Sniper (a parallel multicore simulator) and evaluated it by running the PARSEC benchmarks on a simulated 8-core system. The results show that PCantorSim increases simulation speed over detailed parallel simulation by a factor of 20×, on average, with an average absolute execution time prediction error of 5.3%.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4249712578",
    "type": "article"
  },
  {
    "title": "VLIW coprocessor for IEEE-754 quadruple-precision elementary functions",
    "doi": "https://doi.org/10.1145/2509420.2512430",
    "publication_date": "2013-09-01",
    "publication_year": 2013,
    "authors": "Yuanwu Lei; Yong Dou; Lei Guo; Jinbo Xu; Jie Zhou; Yazhuo Dong; Hongjian Li",
    "corresponding_authors": "",
    "abstract": "In this article, a unified VLIW coprocessor, based on a common group of atomic operation units, for Quad arithmetic and elementary functions (QP_VELP) is presented. The explicitly parallel scheme of VLIW instruction and Estrin's evaluation scheme for polynomials are used to improve the performance. A two-level VLIW instruction RAM scheme is introduced to achieve high scalability and customizability, even for more complex key program kernels. Finally, the Quad arithmetic accelerator (QAA) with the QP_VELP array is implemented on ASIC. Compared with hyper-thread software implementation on an Intel Xeon E5620, QAA with 8 QP_VELP units achieves improvement by a factor of 18X.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4250335454",
    "type": "article"
  },
  {
    "title": "Accelerating an application domain with specialized functional units",
    "doi": "https://doi.org/10.1145/2555289.2555303",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Cecilia González-Álvarez; Jennifer B. Sartor; Carlos Álvarez; Daniel Jiménez-González; Lieven Eeckhout",
    "corresponding_authors": "",
    "abstract": "Hardware specialization has received renewed interest recently as chips are hitting power limits. Chip designers of traditional processor architectures have primarily focused on general-purpose computing, partially due to time-to-market pressure and simpler design processes. But new power limits require some chip specialization. Although hardware configured for a specific application yields large speedups for low-power dissipation, its design is more complex and less reusable. We instead explore domain-based specialization, a scalable approach that balances hardware's reusability and performance efficiency. We focus on specialization using customized compute units that accelerate particular operations. In this article, we develop automatic techniques to identify code sequences from different applications within a domain that can be targeted to a new custom instruction that will be run inside a configurable specialized functional unit (SFU). We demonstrate that using a canonical representation of computations finds more common code sequences among applications that can be mapped to the same custom instruction, leading to larger speedups while specializing a smaller core area than previous pattern-matching techniques. We also propose new heuristics to narrow the search space of domain-specific custom instructions, finding those that achieve the best performance across applications. We estimate the overall performance achieved with our automatic techniques using hardware models on a set of nine media benchmarks, showing that when limiting the core area devoted to specialization, the SFU customization with the largest speedups includes both application- and domain-specific custom instructions. We demonstrate that exploring domain-specific hardware acceleration is key to continued computing system performance improvements.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4250375276",
    "type": "article"
  },
  {
    "title": "TACO Reviewers 2012",
    "doi": "https://doi.org/10.1145/2509420.2509421",
    "publication_date": "2013-09-16",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "review-article Free Access Share on TACO Reviewers 2012ACM Transactions on Architecture and Code OptimizationVolume 10Issue 3September 2013 Article No.: 9pp 1–2https://doi.org/10.1145/2509420.2509421Online:16 September 2013Publication History 0citation192DownloadsMetricsTotal Citations0Total Downloads192Last 12 Months4Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4251603783",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1970386",
    "publication_date": "2011-06-21",
    "publication_year": 2011,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4251617803",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1952998",
    "publication_date": "2011-04-01",
    "publication_year": 2011,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Limit studies on Dynamic Voltage and Frequency Scaling (DVFS) provide apparently contradictory conclusions. On the one hand early limit studies report that DVFS is effective at large timescales (on the order of million(s) of cycles) with large scaling ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4252430021",
    "type": "paratext"
  },
  {
    "title": "C1C",
    "doi": "https://doi.org/10.1145/2555289.2555308",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Yong Li; Yaojun Zhang; Hai Li; Yiran Chen; Alex K. Jones",
    "corresponding_authors": "",
    "abstract": "Spin-Transfer Torque RAM (STT-RAM), a promising alternative to SRAM for reducing leakage power consumption, has been widely studied to mitigate the impact of its asymmetrically long write latency. Recently, STT-RAM has been proposed for L1 caches by relaxing the data retention time to improve write performance and dynamic energy. However, as the technology scales down from 65nm to 22nm, the performance of the read operation scales poorly due to reduced sense margins and sense amplifier delays. In this article, we leverage a dual-mode STT memory cell to design a configurable L1 cache architecture termed C1C to mitigate read performance barriers with technology scaling. Guided by application access characteristics discovered through novel compiler analyses, the proposed cache adaptively switches between a high performance and a low-power access mode. Our evaluation demonstrates that the proposed cache with compiler guidance outperforms a state-of-the-art STT-RAM cache design by 9% with high dynamic energy efficiency, leading to significant performance/watt improvements over several competing approaches.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4252782727",
    "type": "article"
  },
  {
    "title": "Easy, fast, and energy-efficient object detection on heterogeneous on-chip architectures",
    "doi": "https://doi.org/10.1145/2555289.2555302",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Ehsan Totoni; Mert Dikmen; María Jesús Garzarán",
    "corresponding_authors": "",
    "abstract": "We optimize a visual object detection application (that uses Vision Video Library kernels) and show that OpenCL is a unified programming paradigm that can provide high performance when running on the Ivy Bridge heterogeneous on-chip architecture. We evaluate different mapping techniques and show that running each kernel where it fits the best and using software pipelining can provide 1.91 times higher performance and 42% better energy efficiency. We also show how to trade accuracy for energy at runtime. Overall, our application can perform accurate object detection at 40 frames per second (fps) in an energy-efficient manner.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4254212429",
    "type": "article"
  },
  {
    "title": "Analysis of dependence tracking algorithms for task dataflow execution",
    "doi": "https://doi.org/10.1145/2555289.2555316",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Hans Vandierendonck; George Tzenakis; Dimitrios S. Nikolopoulos",
    "corresponding_authors": "",
    "abstract": "Processor architectures has taken a turn toward many-core processors, which integrate multiple processing cores on a single chip to increase overall performance, and there are no signs that this trend will stop in the near future. Many-core processors are harder to program than multicore and single-core processors due to the need for writing parallel or concurrent programs with high degrees of parallelism. Moreover, many-cores have to operate in a mode of strong scaling because of memory bandwidth constraints. In strong scaling, increasingly finer-grain parallelism must be extracted in order to keep all processing cores busy.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4256022513",
    "type": "article"
  },
  {
    "title": "The single-referent collector",
    "doi": "https://doi.org/10.1145/1596510.1596513",
    "publication_date": "2009-10-01",
    "publication_year": 2009,
    "authors": "Michał Węgiel; Chandra Krintz",
    "corresponding_authors": "",
    "abstract": "Compactors that move or copy objects need to adjust pointers. In extant compactors, pointer adjustment involves inspecting every pointer in the heap and computing the target address for each pointer. At the same time, in modern Managed Runtime Environments (MREs), only a fraction of pointers in the heap changes during compaction. This is because state-of-the-art MREs do not compact the prefix of the heap that contains few dead objects, allowing gaps between live objects and tolerating small space overhead. We describe the design and implementation of the Single-Referent Collector (SRC), a new compactor that reduces the cost of pointer manipulation by avoiding inspection and adjustment of pointers that do not change. SRC exploits the fact that in modern applications, most live objects have only one incoming pointer. For such objects, SRC stores the address of the referent in the object header. Only objects that move have their referent inspected and adjusted. The remaining pointers in the heap are not processed. SRC uses an overflow table to handle objects with multiple incoming pointers. We investigate a number of standard benchmarks and open-source applications to substantiate key statistical observations that underlie the design of SRC. We implement SRC in the HotSpot JVM as part of a generational collection system and compare it empirically with the Lisp2 compactor. We find that, by decreasing the cost of pointer processing, SRC enables significant reduction in pause times and improves application throughput.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2007928819",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1596510",
    "publication_date": "2009-10-01",
    "publication_year": 2009,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The register file is a critical component in a modern superscalar processor. It must be large enough to accommodate the results of all in-flight instructions. It must also have enough ports to allow simultaneous issue and writeback of many values each ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4229916427",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1582710",
    "publication_date": "2009-09-01",
    "publication_year": 2009,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Scratchpad memory (SPM), a fast on-chip SRAM managed by software, is widely used in embedded systems. This article introduces a general-purpose compiler approach, called memory coloring, to assign static data aggregates, such as arrays and structs, in a ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4231008474",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2086696",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Hardware data prefetch is a very well known technique for hiding memory latencies. However, in a multicore system fitted with a shared Last-Level Cache (LLC), prefetch induced by a core consumes common resources such as shared cache space and main ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4236162133",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2133382",
    "publication_date": "2012-03-01",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Value prediction is a technique to increase parallelism by attempting to overcome serialization constraints caused by true data dependences. By predicting the outcome of an instruction before it executes, value prediction allows data dependent ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4237082874",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1509864",
    "publication_date": "2009-03-01",
    "publication_year": 2009,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4237370375",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2207222",
    "publication_date": "2012-06-01",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Symbiotic job scheduling improves simultaneous multithreading (SMT) processor performance by coscheduling jobs that have “compatible” demands on the processor's shared resources. Existing approaches however require a sampling phase, evaluate a limited ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4238438799",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1880037",
    "publication_date": "2010-12-01",
    "publication_year": 2010,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Profiling and online analysis are important tasks in program understanding and feedback-directed optimization. However, fine-grained profiling and online analysis tend to seriously slow down the application. To cope with the slowdown, one may have to ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4239132834",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2355585",
    "publication_date": "2012-09-01",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Efficiently utilizing multicore processors to improve their performance potentials demands extracting thread-level parallelism from the applications. Various novel and sophisticated execution models have been proposed to extract thread-level parallelism ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4239176718",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1736065",
    "publication_date": "2010-04-01",
    "publication_year": 2010,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4241626563",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1543753",
    "publication_date": "2009-06-01",
    "publication_year": 2009,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Memory bugs are a broad class of bugs that is becoming increasingly common with increasing software complexity, and many of these bugs are also security vulnerabilities. Existing software and hardware approaches for finding and identifying memory bugs ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4251620121",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1498690",
    "publication_date": "2009-03-01",
    "publication_year": 2009,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "In today's digital world, computer security issues have become increasingly important. In particular, researchers have proposed designs for secure processors that utilize hardware-based memory encryption and integrity verification to protect the privacy ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4253214327",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1839667",
    "publication_date": "2010-09-01",
    "publication_year": 2010,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Traditional coherence protocols present a set of difficult trade-offs: the reliance of snoopy protocols on broadcast and ordered interconnects limits their scalability, while directory protocols incur a performance penalty on sharing misses due to ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4254566908",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1880043",
    "publication_date": "2010-12-01",
    "publication_year": 2010,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Reconfigurable computers, where one or more FPGAs are attached to a conventional microprocessor, are promising platforms for code acceleration. Despite their advantages, programmability concerns and the lack of efficient design tools/compilers for FPGAs ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4255411517",
    "type": "paratext"
  },
  {
    "title": "<i>Exceptionization</i>",
    "doi": "https://doi.org/10.1145/3046681",
    "publication_date": "2017-03-31",
    "publication_year": 2017,
    "authors": "Byungsun Yang; Jae-Yun Kim; Soo‐Mook Moon",
    "corresponding_authors": "",
    "abstract": "Java virtual machine (JVM) has recently evolved into a general-purpose language runtime environment to execute popular programming languages such as JavaScript, Ruby, Python, and Scala. These languages have complex non-Java features, including dynamic typing and first-class function, so additional language runtimes (engines) are provided on top of the JVM to support them with bytecode extensions. Although there are high-performance JVMs with powerful just-in-time (JIT) compilers, running these languages efficiently on the JVM is still a challenge. This article introduces a simple and novel technique for the JVM JIT compiler called exceptionization to improve the performance of JVM-based language runtimes. We observed that the JVM executing some non-Java languages encounters at least 2 times more branch bytecodes than Java, most of which are highly biased to take only one target. Exceptionization treats such a highly biased branch as some implicit exception-throwing instruction. This allows the JVM JIT compiler to prune the infrequent target of the branch from the frequent control flow, thus compiling the frequent control flow more aggressively with better optimization. If a pruned path were taken, then it would run like a Java exception handler, that is, a catch block. We also devised de-exceptionization , a mechanism to cope with the case when a pruned path is executed more often than expected. Since exceptionization is a generic JVM optimization, independent of any specific language runtime, it would be generally applicable to other language runtimes on the JVM. Our experimental result shows that exceptionization accelerates the performance of several non-Java languages. For example, JavaScript-on-JVM runs faster by as much as 60% and by 6% on average, when experimented with the Octane benchmark suite on Oracle’s latest Nashorn JavaScript engine and HotSpot 1.9 JVM. Furthermore, the performance of Ruby-on-JVM shows an improvement by as much as 60% and by 6% on average, while Python-on-JVM improves by as much as 6% and by 2% on average. We found that exceptionization is more effective to apply to the branch bytecode of the language runtime itself than the bytecode corresponding to the application code or the bytecode of the Java class libraries. This implies that the performance benefit of exceptionization comes from better JIT compilation of the language runtime of non-Java languages.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2607079380",
    "type": "article"
  },
  {
    "title": "ReDirect",
    "doi": "https://doi.org/10.1145/3162015",
    "publication_date": "2017-12-20",
    "publication_year": 2017,
    "authors": "George Patsilaras; James Tuck",
    "corresponding_authors": "",
    "abstract": "As we enter the dark silicon era, architects should not envision designs in which every transistor remains turned on permanently but rather ones in which portions of the chip are judiciously turned on/off depending on the characteristics of a workload. At the same time, due to the increasing cost per transistor, architects should also consider new ways to re-purpose transistors to increase their architectural value. In this work, we consider the design of directory-based cache coherence in light of the dark silicon era and the need to re-purpose transistors. We point out that directories are not needed all of the time, and we argue that directories (and coherence) should be off unless it is actually needed for correctness. In our design, directories will be disabled and powered off for workloads with no sharing. Then only when parallel workloads need cache coherence will directories be enabled in proportion to the sharing that is present. At the same time, we exploit the structural similarities of directories and cache. If a directory is idle, then we reconfigure it to be used as extra capacity in the last-level cache. Since our novel approach can keep most directories off, we are free to select sparse overprovisioned directory designs that are reconfigurable to large amounts of cache that can significantly boost performance when the directory is idle. We call these combined features Reconfigured Dark Directories , since directories are usually dark (off) and can be reconfigured. Our results for Reconfigurable Dark Directories running SPEC 2006 applications show a performance benefit, on average, of 17% for an 8× overprovisioned fully mapped directory on a 64-tile system under low system concurrency (10% under heavy concurrency), or a 29% average speedup for a 2× overprovisioned directory on 256-tile system (10% under heavy concurrency) to systems with a conventional sparse directory design using the same overprovisioning factor.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2777541610",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3058793",
    "publication_date": "2017-04-14",
    "publication_year": 2017,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Energy efficiency is becoming increasingly important, yet few developers understand how source code changes affect the energy and power consumption of their programs. To enable them to achieve energy savings, we must associate energy consumption with ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4230367700",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1275937",
    "publication_date": "2007-09-01",
    "publication_year": 2007,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Researchers have proposed the use of adaptation to reduce the energy consumption of different hardware components, such as the processor, memory, disk, and display for general-purpose applications. Previous algorithms to control these adaptations, ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4236215526",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1328195",
    "publication_date": "2008-01-01",
    "publication_year": 2008,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4239377988",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3132652",
    "publication_date": "2017-09-06",
    "publication_year": 2017,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Specialized Digital Signal Processors (DSPs), which can be found in a wide range of modern devices, play an important role in power-efficient, high-performance image processing. Applications including camera sensor post-processing and computer vision ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4240793957",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3154814",
    "publication_date": "2017-12-20",
    "publication_year": 2017,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Modern multi-core systems provide huge computational capabilities, which can be used to run multiple processes concurrently. To achieve the best possible performance within limited power budgets, the various system resources need to be allocated ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4242623378",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1455650",
    "publication_date": "2008-11-01",
    "publication_year": 2008,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "There are two competing models for the on-chip memory in Chip Multiprocessor (CMP) systems: hardware-managed coherent caches and software-managed streaming memory. This paper performs a direct comparison of the two models under the same set of ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4243149263",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1250727",
    "publication_date": "2007-06-01",
    "publication_year": 2007,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We develop a microprocessor design that tolerates hard faults, including fabrication defects and in-field faults, by leveraging existing microprocessor redundancy. To do this, we must: detect and correct errors, diagnose hard faults at the field ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4243554655",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1369396",
    "publication_date": "2008-05-01",
    "publication_year": 2008,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Modern computer systems are called on to deal with billions of events every second, whether they are executed instructions, accessed memory locations, or forwarded packets. This presents a serious challenge to those who seek to quantify, analyze, or ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4244474540",
    "type": "paratext"
  },
  {
    "title": "Introduction",
    "doi": "https://doi.org/10.1145/1216544.1229348",
    "publication_date": "2007-03-01",
    "publication_year": 2007,
    "authors": "Brad Calder; Dean M. Tullsen",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4245962210",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1400112",
    "publication_date": "2008-08-01",
    "publication_year": 2008,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Any successful solution to using multicore processors to scale general-purpose program performance will have to contend with rising intercore communication costs while exposing coarse-grained parallelism. Recently proposed pipelined multithreading (PMT) ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4247526709",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1216544",
    "publication_date": "2007-03-01",
    "publication_year": 2007,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "As silicon technologies move into the nanometer regime, transistor reliability is expected to wane as devices become subject to extreme process variation, particle-induced transient errors, and transistor wear-out. Unless these challenges are addressed, ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4248992524",
    "type": "paratext"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/1369396.1369397",
    "publication_date": "2008-05-01",
    "publication_year": 2008,
    "authors": "Brad Calder; Dean M. Tullsen",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4249022373",
    "type": "editorial"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3086564",
    "publication_date": "2017-07-21",
    "publication_year": 2017,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Recently, processors have begun integrating 3D stacked DRAMs with the cores on the same package, and there have been several approaches to effectively utilizing the on-package DRAMs as caches. This article presents an approach that combines the previous ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4253452576",
    "type": "paratext"
  },
  {
    "title": "ASM: An Adaptive Secure Multicore for Co-located Mutually Distrusting Processes",
    "doi": "https://doi.org/10.1145/3587480",
    "publication_date": "2023-03-17",
    "publication_year": 2023,
    "authors": "Abdul Rasheed Sahni; Hamza Omar; Usman Ali; Omer Khan",
    "corresponding_authors": "",
    "abstract": "With the ever-increasing virtualization of software and hardware, the privacy of user-sensitive data is a fundamental concern in computation outsourcing. Secure processors enable a trusted execution environment to guarantee security properties based on the principles of isolation, sealing, and integrity. However, the shared hardware resources within the microarchitecture are increasingly being used by co-located adversarial software to create timing-based side-channel attacks. State-of-the-art secure processors implement the strong isolation primitive to enable non-interference for shared hardware but suffer from frequent state purging and resource utilization overheads, leading to degraded performance. This article proposes ASM , an adaptive secure multicore architecture that enables a reconfigurable, yet strongly isolated execution environment. For outsourced security-critical processes, the proposed security kernel and hardware extensions allow either a given process to execute using all available cores or co-execute multiple processes on strongly isolated clusters of cores. This spatio-temporal execution environment is configured based on resource demands of processes, such that the secure processor mitigates state purging overheads and maximizes hardware resource utilization.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4327727834",
    "type": "article"
  },
  {
    "title": "Jointly Optimizing Job Assignment and Resource Partitioning for Improving System Throughput in Cloud Datacenters",
    "doi": "https://doi.org/10.1145/3593055",
    "publication_date": "2023-04-18",
    "publication_year": 2023,
    "authors": "Ruobing Chen; Haosen Shi; Jinping Wu; Yusen Li; Xiaoguang Liu; Gang Wang",
    "corresponding_authors": "",
    "abstract": "Colocating multiple jobs on the same server has been widely applied for improving resource utilization in cloud datacenters. However, the colocated jobs would contend for the shared resources, which could lead to significant performance degradation. An efficient approach for eliminating performance interference is to partition the shared resources among the colocated jobs. However, this makes the resource management in datacenters very challenging. In this paper, we propose JointOPT, the first resource management framework that optimizes job assignment and resource partitioning jointly for improving the throughput of cloud datacenters. JointOPT uses a local search based algorithm to find the near optimal job assignment configuration, and uses a deep reinforcement learning (DRL) based approach to dynamically partition the shared resources among the colocated jobs. In order to reduce the interaction overhead with real systems, it leverages deep learning to estimate job performance without running them on real servers. We conduct extensive experiments to evaluate JointOPT and the results show that JointOPT significantly outperforms the state-of-the-art baselines, with an advantage from 13.3% to 47.7%.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4366275135",
    "type": "article"
  },
  {
    "title": "<scp>MicroProf</scp> : Code-level Attribution of Unnecessary Data Transfer in Microservice Applications",
    "doi": "https://doi.org/10.1145/3622787",
    "publication_date": "2023-09-08",
    "publication_year": 2023,
    "authors": "Syed Salauddin Mohammad Tariq; Lance Menard; Pengfei Su; Probir Roy",
    "corresponding_authors": "",
    "abstract": "The microservice architecture style has gained popularity due to its ability to fault isolation, ease of scaling applications, and developer’s agility. However, writing applications in the microservice design style has its challenges. Due to the loosely coupled nature, services communicate with others through standard communication APIs. This incurs significant overhead in the application due to communication protocol and data transformation. An inefficient service communication at the microservice application logic can further overwhelm the application. We perform a grey literature review showing that unnecessary data transfer is a real challenge in the industry. To the best of our knowledge, no effective tool is currently available to accurately identify the origins of unnecessary microservice communications that lead to significant performance overhead and provide guidance for optimization. To bridge the knowledge gap, we propose MicroProf , a dynamic program analysis tool to detect unnecessary data transfer in Java-based microservice applications. At the implementation level, MicroProf proposes novel techniques such as remote object sampling and hardware debug registers to monitor remote object usage. MicroProf reports the unnecessary data transfer at the application source code level. Furthermore, MicroProf pinpoints the opportunities for communication API optimization. MicroProf is evaluated on four well-known applications involving two real-world applications and two benchmarks, identifying five inefficient remote invocations. Guided by MicroProf , API optimization achieves an 87.5% reduction in the number of fields within REST API responses. The empirical evaluation further reveals that the optimized services experience a speedup of up to 4.59×.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4386541703",
    "type": "article"
  },
  {
    "title": "Multiply-and-Fire: An Event-Driven Sparse Neural Network Accelerator",
    "doi": "https://doi.org/10.1145/3630255",
    "publication_date": "2023-10-27",
    "publication_year": 2023,
    "authors": "Miao Yu; Tingting Xiang; Venkata Pavan Kumar Miriyala; Trevor E. Carlson",
    "corresponding_authors": "",
    "abstract": "Deep neural network inference has become a vital workload for many systems from edge-based computing to data centers. To reduce the performance and power requirements for deep neural networks (DNNs) running on these systems, pruning is commonly used as a way to maintain most of the accuracy of the system while significantly reducing the workload requirements. Unfortunately, accelerators designed for unstructured pruning typically employ expensive methods to either determine non-zero activation-weight pairings or reorder computation. These methods require additional storage and memory accesses compared to the more regular data access patterns seen in structurally pruned models. However, even existing works that focus on the more regular access patterns seen in structured pruning continue to suffer from inefficient designs, which either ignore or expensively handle activation sparsity leading to low performance. To address these inefficiencies, we leverage structured pruning and propose the multiply-and-fire (MnF) technique, which aims to solve these problems in three ways: (a) the use of a novel event-driven dataflow that naturally exploits activation sparsity without complex, high-overhead logic; (b) an optimized dataflow takes an activation-centric approach, which aims to maximize the reuse of activation data in computation and ensures the data are only fetched once from off-chip global and on-chip local memory; and (c) based on the proposed event-driven dataflow, we develop an energy-efficient, high-performance sparsity-aware DNN accelerator. Our results show that our MnF accelerator achieves a significant improvement across a number of modern benchmarks and presents a new direction to enable highly efficient AI inference for both CNN and MLP workloads. Overall, this work achieves a geometric mean of 11.2× higher energy efficiency and 1.41× speedup compared to a state-of-the-art sparsity-aware accelerator.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4388192801",
    "type": "article"
  },
  {
    "title": "JiuJITsu: Removing Gadgets with Safe Register Allocation for JIT Code Generation",
    "doi": "https://doi.org/10.1145/3631526",
    "publication_date": "2023-11-03",
    "publication_year": 2023,
    "authors": "Zhang Jiang; Ying Chen; Xiaoli Gong; Jin Zhang; Wenwen Wang; Pen-Chung Yew",
    "corresponding_authors": "",
    "abstract": "Code-reuse attacks have the capability to craft malicious instructions from small code fragments, commonly referred to as “gadgets.” These gadgets are generated by JIT (Just-In-Time) engines as integral components of native instructions, with the flexibility to be embedded in various fields, including Displacement . In this article, we introduce a novel approach for potential gadget insertion, achieved through the manipulation of ModR/M and SIB bytes via JavaScript code. This manipulation influences a JIT engine’s register allocation and code generation algorithms. These newly generated gadgets do not rely on constants and thus evade existing constant blinding schemes. Furthermore, they can be combined with 1-byte constants, a combination that proves to be challenging to defend against using conventional constant blinding techniques. To showcase the feasibility of our approach, we provide proof-of-concept (POC) code for three distinct types of gadgets. Our research underscores the potential for attackers to exploit ModR/M and SIB bytes within JIT-generated native instructions. In response, we propose a practical defense mechanism to mitigate such attacks. We introduce JiuJITsu , a security-enhanced register allocation scheme designed to prevent harmful register assignments during the JIT code generation phase, thereby thwarting the generation of these malicious gadgets. We conduct a comprehensive analysis of JiuJITsu ’s effectiveness in defending against code-reuse attacks. Our findings demonstrate that it incurs a runtime overhead of under 1% when evaluated using JetStream2 benchmarks and real-world websites.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4388281765",
    "type": "article"
  },
  {
    "title": "Critical Data Backup with Hybrid Flash-Based Consumer Devices",
    "doi": "https://doi.org/10.1145/3631529",
    "publication_date": "2023-11-06",
    "publication_year": 2023,
    "authors": "Longfei Luo; Dingcui Yu; Yina Lv; Liang Shi",
    "corresponding_authors": "",
    "abstract": "Hybrid flash-based storage constructed with high-density and low-cost flash memory has become increasingly popular in consumer devices in the last decade due to its low cost. However, its poor reliability is one of the major concerns. To protect critical data for guaranteeing user experience, some methods are proposed to improve the reliability of consumer devices with non-hybrid flash storage. However, with the widespread use of hybrid storage, these methods will result in severe problems, including significant performance and endurance degradation. This is caused by the fact that the different characteristics of flash memory in hybrid storage are not considered, e.g., performance, endurance, and access granularity. To address these problems, a critical data backup (CDB) design is proposed to ensure critical data reliability at a low cost. The basic idea is to accumulate two copies of critical data in the fast memory first to make full use of its performance and endurance. Then, one copy will be migrated to the slow memory in the stripe to avoid the write amplification caused by different access granularity between them. By respecting the different characteristics of flash memory in hybrid storage, CDB can achieve encouraging performance and endurance improvement compared with the state-of-the-art. Furthermore, to avoid performance and lifetime degradation caused by the backup data occupying too much space of fast memory, CDB Pro is designed. Two advanced schemes are integrated. One is making use of the pseudo-single-level-cell (pSLC) technique to make a part of slow memory become high-performance. By supplying some high-performance space, data will be fully updated before being evicted to slow memory. More invalid data are generated which reduces eviction costs. Another is to categorize data into three types according to their different life cycles. By putting the same type of data in a block, the eviction efficiency is improved. Therefore, both can improve device performance and lifetime based on CDB. Experiments are conducted to prove the efficiency of CDB and CDB Pro. Experimental results show that compared with the state-of-the-arts, CDB can ensure critical data reliability with lower device performance and lifetime loss whereas CDB Pro can diminish the loss further.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4388412502",
    "type": "article"
  },
  {
    "title": "Transversely anisotropic curved optical fibres: Variational analysis of a nonstandard Eigenproblem",
    "doi": null,
    "publication_date": "1987-07-01",
    "publication_year": 1987,
    "authors": "M.I. Oksanen; Ismo V. Lindell",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3021507034",
    "type": "article"
  },
  {
    "title": "P <scp>oker</scp>",
    "doi": "https://doi.org/10.1145/3280850",
    "publication_date": "2018-11-16",
    "publication_year": 2018,
    "authors": "Feng Zhang; Jingling Xue",
    "corresponding_authors": "",
    "abstract": "We introduce P oker , a permutation-based approach for vectorizing multiple queries over B + -trees. Our key insight is to combine vector loads and path-encoding-based permutations to alleviate memory latency while keeping the number of key comparisons needed for a query to a minimum. Implemented as a C++ template library, P oker represents a general-purpose solution for vectorizing the queries over indexing trees on multi-core processors equipped with SIMD units. For a set of five representative benchmarks evaluated with 24 configurations each, P oker outperforms the state of the art by 2.11x with one single thread and 2.28x with eight threads on an Intel Broadwell processor that supports 256-bit AVX2, on average. In addition, strip-mining queries will further improve P oker ’s performance by 1.21x (with one single thread) and 1.31x (with eight threads), on average.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2901007022",
    "type": "article"
  },
  {
    "title": "List of 2018 Distinguished Reviewers ACM TACO",
    "doi": "https://doi.org/10.1145/3293444",
    "publication_date": "2018-12-31",
    "publication_year": 2018,
    "authors": "Angelos Bilas",
    "corresponding_authors": "Angelos Bilas",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2909995108",
    "type": "article"
  },
  {
    "title": "Combining Source-adaptive and Oblivious Routing with Congestion Control in High-performance Interconnects using Hybrid and Direct Topologies",
    "doi": "https://doi.org/10.1145/3319805",
    "publication_date": "2019-04-18",
    "publication_year": 2019,
    "authors": "Pedro Yébenes; José Rocher-González; Jesús Escudero‐Sahuquillo; Pedro J. García; Francisco J. Alfaro; Francisco J. Quiles; C. Gómez; J. Duato",
    "corresponding_authors": "",
    "abstract": "Hybrid and direct topologies are cost-efficient and scalable options to interconnect thousands of end nodes in high-performance computing (HPC) systems. They offer a rich path diversity, high bisection bandwidth, and a reduced diameter guaranteeing low latency. In these topologies, efficient deterministic routing algorithms can be used to balance smartly the traffic flows among the available routes. Unfortunately, congestion leads these networks to saturation, where the HoL blocking effect degrades their performance dramatically. Among the proposed solutions to deal with HoL blocking, the routing algorithms selecting alternative routes, such as adaptive and oblivious, can mitigate the congestion effects. Other techniques use queues to separate congested flows from non-congested ones, thus reducing the HoL blocking. In this article, we propose a new approach that reduces HoL blocking in hybrid and direct topologies using source-adaptive and oblivious routing. This approach also guarantees deadlock-freedom as it uses virtual networks to break potential cycles generated by the routing policy in the topology. Specifically, we propose two techniques, called Source-Adaptive Solution for Head-of-Line Blocking Avoidance (SASHA) and Oblivious Solution for Head-of-Line Blocking Avoidance (OSHA). Experiment results, carried out through simulations under different traffic scenarios, show that SASHA and OSHA can significantly reduce the HoL blocking.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2936821322",
    "type": "article"
  },
  {
    "title": "Optimizing Remote Communication in X10",
    "doi": "https://doi.org/10.1145/3345558",
    "publication_date": "2019-10-11",
    "publication_year": 2019,
    "authors": "Arun Thangamani; V. Krishna Nandivada",
    "corresponding_authors": "",
    "abstract": "X10 is a partitioned global address space programming language that supports the notion of places ; a place consists of some data and some lightweight tasks called activities. Each activity runs at a place and may invoke a place-change operation (using the at-construct) to synchronously perform some computation at another place. These place-change operations can be very expensive, as they need to copy all the required data from the current place to the remote place. However, identifying the necessary number of place-change operations and the required data during each place-change operation are non-trivial tasks, especially in the context of irregular applications (like graph applications) that contain complex code with large amounts of cross-referencing objects—not all of those objects may be actually required, at the remote place. In this article, we present AT-Com, a scheme to optimize X10 code with place-change operations. AT-Com consists of two inter-related new optimizations: (i) AT-Opt, which minimizes the amount of data serialized and communicated during place-change operations, and (ii) AT-Pruning, which identifies/elides redundant place-change operations and does parallel execution of place-change operations. AT-Opt uses a novel abstraction, called abstract-place-tree , to capture place-change operations in the program. For each place-change operation, AT-Opt uses a novel inter-procedural analysis to precisely identify the data required at the remote place in terms of the variables in the current scope. AT-Opt then emits the appropriate code to copy the identified data-items to the remote place. AT-Pruning introduces a set of program transformation techniques to emit optimized code such that it avoids the redundant place-change operations. We have implemented AT-Com in the x10v2.6.0 compiler and tested it over the IMSuite benchmark kernels. Compared to the current X10 compiler, the AT-Com optimized code achieved a geometric mean speedup of 18.72× and 17.83× on a four-node (32 cores per node) Intel and two-node (16 cores per node) AMD system, respectively.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2979877239",
    "type": "article"
  },
  {
    "title": "GPU上のRSAのサイドチャネルタイミング攻撃【JST・京大機械翻訳】",
    "doi": null,
    "publication_date": "2019-01-01",
    "publication_year": 2019,
    "authors": "Luo Chao; Fei Yunsi; Kaeli David",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3183449542",
    "type": "article"
  },
  {
    "title": "既製予測可能ハードウェア上でのハード実時間アビオニクス応用の正しい構築並列化【JST・京大機械翻訳】",
    "doi": null,
    "publication_date": "2019-01-01",
    "publication_year": 2019,
    "authors": "Didier Keryan; Potop-Butucaru Dumitru; Iooss Guillaume; Albert Cohen; Souyris Jean; Baufreton Philippe; Graillat Amaury",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3187721525",
    "type": "article"
  },
  {
    "title": "能力強化コプロセッサによるメモリ側保護【JST・京大機械翻訳】",
    "doi": null,
    "publication_date": "2019-01-01",
    "publication_year": 2019,
    "authors": "Azriel Leonid; Humbel Lukas; Reto Achermann; Richardson Alex; Hoffmann Moritz; Avi Mendelson; R Church Timothy; N M Watson Robert; Paolo Faraboschi; Milojicic Dejan",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3187988724",
    "type": "article"
  },
  {
    "title": "重み共有による畳込みニューラルネットワークのための低複雑性多重累積ユニット【JST・京大機械翻訳】",
    "doi": null,
    "publication_date": "2018-01-01",
    "publication_year": 2018,
    "authors": "Garland James; Gregg David",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3188436124",
    "type": "article"
  },
  {
    "title": "遅延マスキングハードウェアスレッドを用いたメモリ内データベース選択の加速【JST・京大機械翻訳】",
    "doi": null,
    "publication_date": "2019-01-01",
    "publication_year": 2019,
    "authors": "Budhkar Prerna; Absalyamov Ildar; Zois Vasileios; Windh Skyler; A Najjar Walid; J Tsotras Vassilis",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3193059279",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3341169",
    "publication_date": "2019-08-20",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "GPUs provide high-bandwidth/low-latency on-chip shared memory and L1 cache to efficiently service a large number of concurrent memory requests. Specifically, concurrent memory requests accessing contiguous memory space are coalesced into warp-wide ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4231582365",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1113841",
    "publication_date": "2005-12-01",
    "publication_year": 2005,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Applications in embedded systems often need to meet specified timing constraints. It is advantageous to not only calculate the worst-case execution time (WCET) of an application, but to also perform transformation, which reduce the WCET, since an ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4233240701",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1138035",
    "publication_date": "2006-06-01",
    "publication_year": 2006,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "A lifetime optimal algorithm, called MC-PRE, is presented for the first time that performs speculative PRE based on edge profiles. In addition to being computationally optimal in the sense that the total number of dynamic computations for an expression ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4236344248",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1162690",
    "publication_date": "2006-09-01",
    "publication_year": 2006,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Although optimizations have been applied for a number of years to improve the performance of software, problems with respect to the application of optimizations have not been adequately addressed. For example, in certain circumstances, optimizations may ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4236403696",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1187976",
    "publication_date": "2006-12-01",
    "publication_year": 2006,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We present Minos, a microarchitecture that implements Biba's low water-mark integrity policy on individual words of data. Minos stops attacks that corrupt control data to hijack program control flow, but is orthogonal to the memory model. Control data ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4236510593",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3274266",
    "publication_date": "2018-10-08",
    "publication_year": 2018,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Modern data centers consolidate workloads to increase server utilization and reduce total cost of ownership, and cope with scaling limitations. However, server resource sharing introduces performance interference across applications and, consequently, ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4237709653",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3212710",
    "publication_date": "2018-06-22",
    "publication_year": 2018,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The paradigm shift from planar (two dimensional (2D)) to vertical (three-dimensional (3D)) models has placed the NAND flash technology on the verge of a design evolution that can handle the demands of next-generation storage applications. However, it ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4240206070",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1132462",
    "publication_date": "2006-03-01",
    "publication_year": 2006,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4243570394",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3199680",
    "publication_date": "2018-04-02",
    "publication_year": 2018,
    "authors": "Hochan Lee; Mansureh S. Moghaddam; Dongkwan Suh; Bernhard Egger",
    "corresponding_authors": "",
    "abstract": "Modulo-scheduled course-grain reconfigurable array (CGRA) processors excel at exploiting loop-level parallelism at a high performance per watt ratio. The frequent reconfiguration of the array, however, causes between 25% and 45% of the consumed chip ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4248626572",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3325131",
    "publication_date": "2019-06-01",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "With the fast development of deep learning (DL), the communication is increasingly a bottleneck for distributed workloads, and a series of optimization works have been done to scale out successfully. Nevertheless, the network behavior has not been ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4249201491",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3366460",
    "publication_date": "2019-12-26",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4249334520",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3284745",
    "publication_date": "2019-01-08",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Performance optimization of stencil codes requires data locality improvements. The polyhedron model for loop transformation is well suited for such optimizations with established techniques, such as the PLuTo algorithm and diamond tiling. However, in ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4249608927",
    "type": "paratext"
  },
  {
    "title": "Introduction",
    "doi": "https://doi.org/10.1145/1061267.1061268",
    "publication_date": "2005-03-01",
    "publication_year": 2005,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4250387594",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3313806",
    "publication_date": "2019-03-08",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Multiple combinatorial algorithms have been proposed for doing pre-allocation instruction scheduling with the objective of minimizing register pressure or balancing register pressure and instruction-level parallelism. The cost function that is minimized ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4253499760",
    "type": "paratext"
  },
  {
    "title": "Introduction",
    "doi": "https://doi.org/10.1145/1132462.1132463",
    "publication_date": "2006-03-01",
    "publication_year": 2006,
    "authors": "Brad Calder; Dean M. Tullsen",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4254353952",
    "type": "article"
  },
  {
    "title": "Introduction",
    "doi": "https://doi.org/10.1145/980152.980153",
    "publication_date": "2004-03-01",
    "publication_year": 2004,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4255851186",
    "type": "article"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/3409369",
    "publication_date": "2020-08-03",
    "publication_year": 2020,
    "authors": "Dave Kaeli",
    "corresponding_authors": "Dave Kaeli",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3081863379",
    "type": "editorial"
  },
  {
    "title": "深層学習フレームワークによる並列性機会の利用【JST・京大機械翻訳】",
    "doi": null,
    "publication_date": "2020-01-01",
    "publication_year": 2020,
    "authors": "Wang Yu Emma; Wu Carole-Jean; Wang Xiaodong; Hazelwood Kim; Brooks David",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3178069828",
    "type": "article"
  },
  {
    "title": "The Forward Slice Core: A High-Performance, Yet Low-Complexity Microarchitecture",
    "doi": "https://doi.org/10.1145/3499424",
    "publication_date": "2022-01-31",
    "publication_year": 2022,
    "authors": "Kartik Lakshminarasimhan; Ajeya Naithani; Josué Feliu; Lieven Eeckhout",
    "corresponding_authors": "",
    "abstract": "Superscalar out-of-order cores deliver high performance at the cost of increased complexity and power budget. In-order cores, in contrast, are less complex and have a smaller power budget, but offer low performance. A processor architecture should ideally provide high performance in a power- and cost-efficient manner. Recently proposed slice-out-of-order (sOoO) cores identify backward slices of memory operations which they execute out-of-order with respect to the rest of the dynamic instruction stream for increased instruction-level and memory-hierarchy parallelism. Unfortunately, constructing backward slices is imprecise and hardware-inefficient, leaving performance on the table. In this article, we propose Forward Slice Core (FSC ), a novel core microarchitecture that builds on a stall-on-use in-order core and extracts more instruction-level and memory-hierarchy parallelism than slice-out-of-order cores. FSC does so by identifying and steering forward slices (rather than backward slices) to dedicated in-order FIFO queues. Moreover, FSC puts load-consumers that depend on L1 D-cache misses on the side to enable younger independent load-consumers to execute faster. Finally, FSC eliminates the need for dynamic memory disambiguation by replicating store-address instructions across queues. Considering 3-wide pipeline configurations, we find that FSC improves performance by 27.1%, 21.1%, and 14.6% on average compared to Freeway, the state-of-the-art sOoO core, across SPEC CPU2017, GAP, and DaCapo, respectively, while at the same time incurring reduced hardware complexity. Compared to an OoO core, FSC reduces power consumption by 61.3% and chip area by 47%, providing a microarchitecture with high performance at low complexity.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4210245450",
    "type": "article"
  },
  {
    "title": "MUA-Router: Maximizing the Utility-of-Allocation for On-chip Pipelining Routers",
    "doi": "https://doi.org/10.1145/3519027",
    "publication_date": "2022-04-01",
    "publication_year": 2022,
    "authors": "Cunlu Li; Dezun Dong; Xiangke Liao",
    "corresponding_authors": "",
    "abstract": "As an important pipeline stage in the router of Network-on-Chips, switch allocation assigns output ports to input ports and allows flits to transit through the switch without conflicts. Previous work designed efficient switch allocation strategies by maximizing the matching efficiency in time series. However, those works neglected the interaction between different router pipeline stages. In this article, we propose the concept of Utility-of-Allocation (UoA) to indicate the quality of allocation to be practically used in on-chip routers. We demonstrate that router pipelines can interact with each other, and the UoA can be maximized if the interaction between router pipelines is taken into consideration. Based on these observations, a novel class of routers, MUA-Router, is proposed to maximize the UoA through the collaborative design (co-design) between router pipelines. MUA-Router achieves this goal in two ways and accordingly implements two novel instance router architectures. In the first, MUA-Router improves the UoA by mitigating the impact of endpoint congestion in the switch allocation, and thus Eca-Router is proposed. Eca-Router achieves an endpoint-congestion-aware switch allocation through the co-design between routing computation and switch allocation. Based on Eca-Router, CoD-Router is proposed to feed back switch allocation information to routing computation stage to provide switch allocator with more conflict-free requests. Through the co-design between pipelines, MUA-Router significantly improves the efficiency of switch allocation and the performance of the entire network. Evaluation results show that our design can achieve significant performance improvement with moderate overheads.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4220711129",
    "type": "article"
  },
  {
    "title": "Dependence-aware Slice Execution to Boost MLP in Slice-out-of-order Cores",
    "doi": "https://doi.org/10.1145/3506704",
    "publication_date": "2022-03-07",
    "publication_year": 2022,
    "authors": "Rakesh Kumar; Mehdi Alipour; David Black-Schaffer",
    "corresponding_authors": "",
    "abstract": "Exploiting memory-level parallelism (MLP) is crucial to hide long memory and last-level cache access latencies. While out-of-order (OoO) cores, and techniques building on them, are effective at exploiting MLP, they deliver poor energy efficiency due to their complex and energy-hungry hardware. This work revisits slice-out-of-order (sOoO) cores as an energy-efficient alternative for MLP exploitation. sOoO cores achieve energy efficiency by constructing and executing slices of MLP-generating instructions out-of-order only with respect to the rest of instructions; the slices and the remaining instructions, by themselves, execute in-order. However, we observe that existing sOoO cores miss significant MLP opportunities due to their dependence-oblivious in-order slice execution, which causes dependent slices to frequently block MLP generation. To boost MLP generation, we introduce Freeway, a sOoO core based on a new dependence-aware slice execution policy that tracks dependent slices and keeps them from blocking subsequent independent slices and MLP extraction. The proposed core incurs minimal area and power overheads, yet approaches the MLP benefits of fully OoO cores. Our evaluation shows that Freeway delivers 12% better performance than the state-of-the-art sOoO core and is within 7% of the MLP limits of full OoO execution.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4220742774",
    "type": "article"
  },
  {
    "title": "Object Intersection Captures on Interactive Apps to Drive a Crowd-sourced Replay-based Compiler Optimization",
    "doi": "https://doi.org/10.1145/3517338",
    "publication_date": "2022-03-29",
    "publication_year": 2022,
    "authors": "Paschalis Mpeis; Pavlos Petoumenos; Kim Hazelwood; Hugh Leather",
    "corresponding_authors": "",
    "abstract": "Traditional offline optimization frameworks rely on representative hardware, software, and inputs to compare different optimizations on. With application-specific optimization for mobile systems though, the idea of a representative testbench is unrealistic while creating offline inputs is non-trivial. Online approaches partially overcome these problems but they might expose users to suboptimal or even erroneous code. Therefore, our mobile code is poorly optimized, resulting in wasted performance and energy and user frustration. In this article, we introduce a novel compiler optimization approach designed for mobile applications. It requires no developer effort, it tunes applications for the user’s device and usage patterns, and it has no negative impact on the user experience. It is based on a lightweight capture and replay mechanism. Our previous work [ 46 ] captures the state accessed by any targeted code region during its online stage. By repurposing existing OS capabilities, it keeps the overhead low. In its offline stage, it replays the code region but under different optimization decisions to enable sound comparisons of different optimizations under realistic conditions. In this article, we propose a technique that further decreases the storage sizes without any additional overhead. It captures only the intersection of reachable objects and accessed heap pages. We compare this with another new approach that has minimal runtime overheads at the cost of higher capture sizes. Coupled with a search heuristic for the compiler optimization space, our capture and replay mechanism allows us to discover optimization decisions that improve performance without testing these decisions directly on the user. Finally, with crowd-sourcing we split this offline evaluation effort between several users, allowing us to discover better code in less time. We implemented a prototype system in Android based on LLVM combined with a genetic search engine and a crowd-sourcing architecture. We evaluated it on both benchmarks and real Android applications. Online captures are infrequent and introduce ~5 ms or 15 ms on average, depending on the approach used. For this negligible effect on user experience, we achieve speedups of 44% on average over the Android compiler and 35% over LLVM -O3. Our collaborative search is just 5% short of that speedup, which is impressive given the acceleration gains. The user with the highest workload concluded the search 7 \\( \\times \\) faster.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4220821662",
    "type": "article"
  },
  {
    "title": "Using Barrier Elision to Improve Transactional Code Generation",
    "doi": "https://doi.org/10.1145/3533318",
    "publication_date": "2022-05-04",
    "publication_year": 2022,
    "authors": "Bruno Chinelato Honorio; João P. L. de Carvalho; Catalina Muñoz Morales; Alexandro Baldassin; Guido Araújo",
    "corresponding_authors": "",
    "abstract": "With chip manufacturers such as Intel, IBM, and ARM offering native support for transactional memory in their instruction set architectures, memory transactions are on the verge of being considered a genuine application tool rather than just an interesting research topic. Despite this recent increase in popularity on the hardware side of transactional memory (HTM) , software support for transactional memory (STM) is still scarce and the only compiler with transactional support currently available, the GNU Compiler Collection (GCC) , does not generate code that achieves desirable performance. For hybrid solutions of TM (HyTM) , which are frameworks that leverage the best aspects of HTM and STM, the subpar performance of the software side, caused by inefficient compiler generated code, might forbid HyTM to offer optimal results. This article extends previous work focused exclusively on STM implementations by presenting a detailed analysis of transactional code generated by GCC in the context of HybridTM implementations. In particular, it builds on previous research of transactional memory support in the Clang/LLVM compiler framework, which is decoupled from any TM runtime, and presents the following novel contributions: (a) it shows that STM’s performance overhead, due to an excessive amount of read and write barriers added by the compiler, also impacts the performance of HyTM systems; and (b) it reveals the importance of the previously proposed annotation mechanism to reduce the performance gap between HTM and STM in phased runtime systems. Furthermore, it shows that, by correctly using the annotations on just a few lines of code, it is possible to reduce the total number of instrumented barriers by 95% and to achieve speed-ups of up to 7× when compared to the original code generated by GCC and the Clang compiler. 1",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4225381858",
    "type": "article"
  },
  {
    "title": "HAIR: Halving the Area of the Integer Register File with Odd/Even Banking",
    "doi": "https://doi.org/10.1145/3544838",
    "publication_date": "2022-06-21",
    "publication_year": 2022,
    "authors": "Pierre Michaud; Anis Peysieux",
    "corresponding_authors": "",
    "abstract": "This article proposes a new microarchitectural scheme for reducing the hardware complexity of the integer register file of a superscalar processor. The register file is split into two banks holding even-numbered and odd-numbered physical registers, respectively. Each bank provides one read port to each two-input integer execution unit. This way, each bank has half the total number of read ports, and the register file area is roughly halved, which reduces the energy dissipated per register access and the register access time. However, a bank conflict occurs when both inputs of a two-input micro-operation lie in the same bank. Bank conflicts hurt performance, and we propose a simple solution to remove most bank conflicts, thus recovering most of the lost performance.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4283204994",
    "type": "article"
  },
  {
    "title": "Symbolic Analysis for Data Plane Programs Specialization",
    "doi": "https://doi.org/10.1145/3557727",
    "publication_date": "2022-08-16",
    "publication_year": 2022,
    "authors": "Thomas Luinaud; J. M. Pierre Langlois; Yvon Savaria",
    "corresponding_authors": "",
    "abstract": "Programmable network data planes have extended the capabilities of packet processing in network devices by allowing custom processing pipelines and agnostic packet processing. While a variety of applications can be implemented on current programmable data planes, there are significant constraints due to hardware limitations. One way to meet these constraints is by optimizing data plane programs. Program optimization can be achieved by specializing code that leverages architectural specificity or by compilation passes. In the case of programmable data planes, to respond to the varying requirements of a large set of applications, data plane programs can target different architectures. This leads to difficulties when developers want to reuse the code. One solution to that is to use compiler optimization techniques. We propose performing data plane program specialization to reduce the generated program size. To this end, we propose to specialize in programs written in P4, a Domain Specific Language (DSL) designed for specifying network data planes. The proposed method takes advantage of key aspects of the P4 language to perform a symbolic analysis on a P4 program and then partially evaluate the program to specialize it. The approach we propose is independent of the target architecture. We evaluate the specialization technique by implementing a packet deparser on an FPGA. The results demonstrate that program specialization can reduce the resource usage by a factor of 2 for various packet deparsers.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4292179105",
    "type": "article"
  },
  {
    "title": "SSD-SGD: Communication Sparsification for Distributed Deep Learning Training",
    "doi": "https://doi.org/10.1145/3563038",
    "publication_date": "2022-09-14",
    "publication_year": 2022,
    "authors": "Yemao Xu; Dezun Dong; D. Wang; Xu Shi; Enda Yu; Weixia Xu; Xiangke Liao",
    "corresponding_authors": "",
    "abstract": "Intensive communication and synchronization cost for gradients and parameters is the well-known bottleneck of distributed deep learning training. Based on the observations that Synchronous SGD (SSGD) obtains good convergence accuracy while asynchronous SGD (ASGD) delivers a faster raw training speed, we propose Several Steps Delay SGD (SSD-SGD) to combine their merits, aiming at tackling the communication bottleneck via communication sparsification. SSD-SGD explores both global synchronous updates in the parameter servers and asynchronous local updates in the workers in each periodic iteration. The periodic and flexible synchronization makes SSD-SGD achieve good convergence accuracy and fast training speed. To the best of our knowledge, we strike the new balance between synchronization quality and communication sparsification, and improve the tradeoff between accuracy and training speed. Specifically, the core components of SSD-SGD include proper warm-up stage, steps delay stage, and the novel algorithm of global gradient for local update (GLU). GLU is critical for local update operations by using global gradient information to effectively compensate for the delayed local weights. Furthermore, we implement SSD-SGD on MXNet framework and comprehensively evaluate its performance with CIFAR-10 and ImageNet datasets. Experimental results show that SSD-SGD can accelerate distributed training speed under different experimental configurations, by up to 110% (or 2.1× of the original speed), while achieving good convergence accuracy.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4295678827",
    "type": "article"
  },
  {
    "title": "Emergency workshop on Energy Conservation in Buildings. National Conference of states on building codes and standards and National Bureau of Standards joint emergency workshop on Energy Conservation in Buildings",
    "doi": null,
    "publication_date": "1975-07-01",
    "publication_year": 1975,
    "authors": "Sandra A Berry",
    "corresponding_authors": "Sandra A Berry",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3011696044",
    "type": "article"
  },
  {
    "title": "Proceedings of the 1973 Workshop on Naval Applications of Superconductivity",
    "doi": null,
    "publication_date": "1974-10-01",
    "publication_year": 1974,
    "authors": "M. Nisenoff",
    "corresponding_authors": "M. Nisenoff",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3021595704",
    "type": "article"
  },
  {
    "title": "Semiconductor measurement technology: ARPA/NBS Workshop 4: Surface Analysis for Silicon Devices",
    "doi": null,
    "publication_date": "1976-03-01",
    "publication_year": 1976,
    "authors": "A. George Lieberman",
    "corresponding_authors": "A. George Lieberman",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3012979478",
    "type": "article"
  },
  {
    "title": "Spiking Neural Networks in Spintronic Computational RAM",
    "doi": null,
    "publication_date": "2021-12-31",
    "publication_year": 2021,
    "authors": "Hüsrev Cılasun; Salonik Resch; Zamshed I. Chowdhury; Erin Olson; Masoud Zabihi; Zhengyang Zhao; T. Peterson; Keshab K. Parhi; Jian‐Ping Wang; Sachin S. Sapatnekar; Ulya R. Karpuzcu",
    "corresponding_authors": "",
    "abstract": "Spiking Neural Networks (SNNs) represent a biologically inspired computation model capable of emulating neural computation in human brain and brain-like structures. The main promise is very low energy consumption. Classic Von Neumann architecture based SNN accelerators in hardware, however, often fall short of addressing demanding computation and data transfer requirements efficiently at scale. In this article, we propose a promising alternative to overcome scalability limitations, based on a network of in-memory SNN accelerators, which can reduce the energy consumption by up to 150.25= when compared to a representative ASIC solution. The significant reduction in energy comes from two key aspects of the hardware design to minimize data communication overheads: (1) each node represents an in-memory SNN accelerator based on a spintronic Computational RAM array, and (2) a novel, De Bruijn graph based architecture establishes the SNN array connectivity.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3033075043",
    "type": "article"
  },
  {
    "title": "All-gather Algorithms Resilient to Imbalanced Process Arrival Patterns",
    "doi": "https://doi.org/10.1145/3460122",
    "publication_date": "2021-07-17",
    "publication_year": 2021,
    "authors": "Jerzy Proficz",
    "corresponding_authors": "Jerzy Proficz",
    "abstract": "Two novel algorithms for the all-gather operation resilient to imbalanced process arrival patterns (PATs) are presented. The first one, Background Disseminated Ring (BDR), is based on the regular parallel ring algorithm often supplied in MPI implementations and exploits an auxiliary background thread for early data exchange from faster processes to accelerate the performed all-gather operation. The other algorithm, Background Sorted Linear synchronized tree with Broadcast (BSLB), is built upon the already existing PAP-aware gather algorithm, that is, Background Sorted Linear Synchronized tree (BSLS), followed by a regular broadcast distributing gathered data to all participating processes. The background of the imbalanced PAP subject is described, along with the PAP monitoring and evaluation topics. An experimental evaluation of the algorithms based on a proposed mini-benchmark is presented. The mini-benchmark was performed over 2,000 times in a typical HPC cluster architecture with homogeneous compute nodes. The obtained results are analyzed according to different PATs, data sizes, and process numbers, showing that the proposed optimization works well for various configurations, is scalable, and can significantly reduce the all-gather elapsed times, in our case, up to factor 1.9 or 47% in comparison with the best state-of-the-art solution.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3184165693",
    "type": "article"
  },
  {
    "title": "Device Hopping",
    "doi": "https://doi.org/10.1145/3471909",
    "publication_date": "2021-09-29",
    "publication_year": 2021,
    "authors": "Paul Metzger; Volker Seeker; Christian Fensch; Murray Cole",
    "corresponding_authors": "",
    "abstract": "Existing OS techniques for homogeneous many-core systems make it simple for single and multithreaded applications to migrate between cores. Heterogeneous systems do not benefit so fully from this flexibility, and applications that cannot migrate in mid-execution may lose potential performance. The situation is particularly challenging when a switch of language runtime would be desirable in conjunction with a migration. We present a case study in making heterogeneous CPU + GPU systems more flexible in this respect. Our technique for fine-grained application migration, allows switches between OpenMP, OpenCL, and CUDA execution, in conjunction with migrations from GPU to CPU, and CPU to GPU. To achieve this, we subdivide iteration spaces into slices, and consider migration on a slice-by-slice basis. We show that slice sizes can be learned offline by machine learning models. To further improve performance, memory transfers are made migration-aware. The complexity of the migration capability is hidden from programmers behind a high-level programming model. We present a detailed evaluation of our mid-kernel migration mechanism with the First Come, First Served scheduling policy. We compare our technique in a focused evaluation scenario against idealized kernel-by-kernel scheduling, which is typical for current systems, and makes perfect kernel to device scheduling decisions, but cannot migrate kernels mid-execution. Models show that up to 1.33× speedup can be achieved over these systems by adding fine-grained migration. Our experimental results with all nine applicable SHOC and Rodinia benchmarks achieve speedups of up to 1.30× (1.08× on average) over an implementation of a perfect but kernel-migration incapable scheduler when migrated to a faster device. Our mechanism and slice size choices introduce an average slowdown of only 2.44% if kernels never migrate. Lastly, our programming model reduces the code size by at least 88% if compared to manual implementations of migratable kernels.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3195412309",
    "type": "article"
  },
  {
    "title": "System-level Early-stage Modeling and Evaluation of IVR-assisted Processor Power Delivery System",
    "doi": "https://doi.org/10.1145/3468145",
    "publication_date": "2021-09-03",
    "publication_year": 2021,
    "authors": "An Zou; Huifeng Zhu; Jingwen Leng; Xin He; Vijay Janapa Reddi; Christopher Gill; Xuan Zhang",
    "corresponding_authors": "",
    "abstract": "Despite being employed in numerous efforts to improve power delivery efficiency, the integrated voltage regulator (IVR) approach has yet to be evaluated rigorously and quantitatively in a full power delivery system (PDS) setting. To fulfill this need, we present a system-level modeling and design space exploration framework called Ivory for IVR-assisted power delivery systems. Using a novel modeling methodology, it can accurately estimate power delivery efficiency, static performance characteristics, and dynamic transient responses under different load variations and external voltage/frequency scaling conditions. We validate the model over a wide range of IVR topologies with silicon measurement and SPICE simulation. Finally, we present two case studies using architecture-level performance and power simulators. The first case study focuses on optimal PDS design for multi-core systems, which achieves 8.6% power efficiency improvement over conventional off-chip voltage regulator module– (VRM) based PDS. The second case study explores the design tradeoffs for IVR-assisted PDSs in CPU and GPU systems with fast per-core dynamic voltage and frequency scaling (DVFS). We find 2 μs to be the optimal DVFS timescale, which not only reaps energy benefits (12.5% improvement in CPU and 50.0% improvement in GPU) but also avoids costly IVR overheads.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3197339217",
    "type": "article"
  },
  {
    "title": "Byte-Select Compression",
    "doi": "https://doi.org/10.1145/3462209",
    "publication_date": "2021-09-03",
    "publication_year": 2021,
    "authors": "Matthew Tomei; Shomit Das; Seyed Mohammad Seyedzadeh; Philip Bedoukian; Bradford M. Beckmann; Rakesh Kumar; David A. Wood",
    "corresponding_authors": "",
    "abstract": "Cache-block compression is a highly effective technique for both reducing accesses to lower levels in the memory hierarchy (cache compression) and minimizing data transfers (link compression). While many effective cache-block compression algorithms have been proposed, the design of these algorithms is largely ad hoc and manual and relies on human recognition of patterns. In this article, we take an entirely different approach. We introduce a class of “byte-select” compression algorithms, as well as an automated methodology for generating compression algorithms in this class. We argue that, based on upper bounds within the class, the study of this class of byte-select algorithms has potential to yield algorithms with better performance than existing cache-block compression algorithms. The upper bound we establish on the compression ratio is 2X that of any existing algorithm. We then offer a generalized representation of a subset of byte-select compression algorithms and search through the resulting space guided by a set of training data traces. Using this automated process, we find efficient and effective algorithms for various hardware applications. We find that the resulting algorithms exploit novel patterns that can inform future algorithm designs. The generated byte-select algorithms are evaluated against a separate set of traces and evaluations show that Byte-Select has a 23% higher compression ratio on average. While no previous algorithm performs best for all our data sets which include CPU and GPU applications, our generated algorithms do. Using an automated hardware generator for these algorithms, we show that their decompression and compression latency is one and two cycles respectively, much lower than any existing algorithm with a competitive compression ratio.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3198221083",
    "type": "article"
  },
  {
    "title": "Joint Program and Layout Transformations to Enable Convolutional Operators on Specialized Hardware Based on Constraint Programming",
    "doi": "https://doi.org/10.1145/3487922",
    "publication_date": "2021-12-06",
    "publication_year": 2021,
    "authors": "Dennis Rieber; Axel Acosta; Holger Fröning",
    "corresponding_authors": "",
    "abstract": "The success of Deep Artificial Neural Networks (DNNs) in many domains created a rich body of research concerned with hardware accelerators for compute-intensive DNN operators. However, implementing such operators efficiently with complex hardware intrinsics such as matrix multiply is a task not yet automated gracefully. Solving this task often requires joint program and data layout transformations. First solutions to this problem have been proposed, such as TVM, UNIT, or ISAMIR, which work on a loop-level representation of operators and specify data layout and possible program transformations before the embedding into the operator is performed. This top-down approach creates a tension between exploration range and search space complexity, especially when also exploring data layout transformations such as im2col, channel packing, or padding. In this work, we propose a new approach to this problem. We created a bottom-up method that allows the joint transformation of both computation and data layout based on the found embedding. By formulating the embedding as a constraint satisfaction problem over the scalar dataflow, every possible embedding solution is contained in the search space. Adding additional constraints and optimization targets to the solver generates the subset of preferable solutions. An evaluation using the VTA hardware accelerator with the Baidu DeepBench inference benchmark shows that our approach can automatically generate code competitive to reference implementations. Further, we show that dynamically determining the data layout based on intrinsic and workload is beneficial for hardware utilization and performance. In cases where the reference implementation has low hardware utilization due to its fixed deployment strategy, we achieve a geomean speedup of up to × 2.813, while individual operators can improve as much as × 170.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4200613974",
    "type": "article"
  },
  {
    "title": "Acoustic Fluctuation Workshop: Technical review, editorial summary, synopsis and papers, volume 1",
    "doi": null,
    "publication_date": "1978-07-01",
    "publication_year": 1978,
    "authors": "S. Hanish; C. R. Rollins; J. Cybulski",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3014902654",
    "type": "article"
  },
  {
    "title": "Workshop on Diffuse Discharge Opening Switches",
    "doi": null,
    "publication_date": "1982-04-01",
    "publication_year": 1982,
    "authors": "M. Kristiansen; Karl H. Schoenbach",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3024372316",
    "type": "article"
  }
]