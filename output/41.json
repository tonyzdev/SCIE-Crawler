[
  {
    "title": "The MovieLens Datasets",
    "doi": "https://doi.org/10.1145/2827872",
    "publication_date": "2015-12-22",
    "publication_year": 2015,
    "authors": "F. Maxwell Harper; Joseph A. Konstan",
    "corresponding_authors": "",
    "abstract": "The MovieLens datasets are widely used in education, research, and industry. They are downloaded hundreds of thousands of times each year, reflecting their use in popular press programming books, traditional and online courses, and software. These datasets are a product of member activity in the MovieLens movie recommendation system, an active research platform that has hosted many experiments since its launch in 1997. This article documents the history of MovieLens and the MovieLens datasets. We include a discussion of lessons learned from running a long-standing, live research platform from the perspective of a research organization. We document best practices and limitations of using the MovieLens datasets in new research.",
    "cited_by_count": 3418,
    "openalex_id": "https://openalex.org/W2219888463",
    "type": "article"
  },
  {
    "title": "Bridging the Gap Between Ethics and Practice",
    "doi": "https://doi.org/10.1145/3419764",
    "publication_date": "2020-10-16",
    "publication_year": 2020,
    "authors": "Ben Shneiderman",
    "corresponding_authors": "Ben Shneiderman",
    "abstract": "This article attempts to bridge the gap between widely discussed ethical principles of Human-centered AI (HCAI) and practical steps for effective governance. Since HCAI systems are developed and implemented in multiple organizational structures, I propose 15 recommendations at three levels of governance: team, organization, and industry. The recommendations are intended to increase the reliability, safety, and trustworthiness of HCAI systems: (1) reliable systems based on sound software engineering practices, (2) safety culture through business management strategies, and (3) trustworthy certification by independent oversight. Software engineering practices within teams include audit trails to enable analysis of failures, software engineering workflows, verification and validation testing, bias testing to enhance fairness, and explainable user interfaces. The safety culture within organizations comes from management strategies that include leadership commitment to safety, hiring and training oriented to safety, extensive reporting of failures and near misses, internal review boards for problems and future plans, and alignment with industry standard practices. The trustworthiness certification comes from industry-wide efforts that include government interventions and regulation, accounting firms conducting external audits, insurance companies compensating for failures, non-governmental and civil society organizations advancing design principles, and professional organizations and research institutes developing standards, policies, and novel ideas. The larger goal of effective governance is to limit the dangers and increase the benefits of HCAI to individuals, organizations, and society.",
    "cited_by_count": 490,
    "openalex_id": "https://openalex.org/W3094233077",
    "type": "article"
  },
  {
    "title": "A Multidisciplinary Survey and Framework for Design and Evaluation of Explainable AI Systems",
    "doi": "https://doi.org/10.1145/3387166",
    "publication_date": "2021-09-03",
    "publication_year": 2021,
    "authors": "Sina Mohseni; Niloofar Zarei; Eric D. Ragan",
    "corresponding_authors": "",
    "abstract": "The need for interpretable and accountable intelligent systems grows along with the prevalence of artificial intelligence ( AI ) applications used in everyday life. Explainable AI ( XAI ) systems are intended to self-explain the reasoning behind system decisions and predictions. Researchers from different disciplines work together to define, design, and evaluate explainable systems. However, scholars from different disciplines focus on different objectives and fairly independent topics of XAI research, which poses challenges for identifying appropriate design and evaluation methodology and consolidating knowledge across efforts. To this end, this article presents a survey and framework intended to share knowledge and experiences of XAI design and evaluation methods across multiple disciplines. Aiming to support diverse design goals and evaluation methods in XAI research, after a thorough review of XAI related papers in the fields of machine learning, visualization, and human-computer interaction, we present a categorization of XAI design goals and evaluation methods. Our categorization presents the mapping between design goals for different XAI user groups and their evaluation methods. From our findings, we develop a framework with step-by-step design guidelines paired with evaluation methods to close the iterative design and evaluation cycles in multidisciplinary XAI teams. Further, we provide summarized ready-to-use tables of evaluation methods and recommendations for different goals in XAI research.",
    "cited_by_count": 486,
    "openalex_id": "https://openalex.org/W3197347140",
    "type": "article"
  },
  {
    "title": "Common Sense Reasoning for Detection, Prevention, and Mitigation of Cyberbullying",
    "doi": "https://doi.org/10.1145/2362394.2362400",
    "publication_date": "2012-09-01",
    "publication_year": 2012,
    "authors": "Karthik Dinakar; Birago Jones; Catherine Havasi; Henry Lieberman; Rosalind W. Picard",
    "corresponding_authors": "",
    "abstract": "Cyberbullying (harassment on social networks) is widely recognized as a serious social problem, especially for adolescents. It is as much a threat to the viability of online social networks for youth today as spam once was to email in the early days of the Internet. Current work to tackle this problem has involved social and psychological studies on its prevalence as well as its negative effects on adolescents. While true solutions rest on teaching youth to have healthy personal relationships, few have considered innovative design of social network software as a tool for mitigating this problem. Mitigating cyberbullying involves two key components: robust techniques for effective detection and reflective user interfaces that encourage users to reflect upon their behavior and their choices. Spam filters have been successful by applying statistical approaches like Bayesian networks and hidden Markov models. They can, like Google’s GMail, aggregate human spam judgments because spam is sent nearly identically to many people. Bullying is more personalized, varied, and contextual. In this work, we present an approach for bullying detection based on state-of-the-art natural language processing and a common sense knowledge base, which permits recognition over a broad spectrum of topics in everyday life. We analyze a more narrow range of particular subject matter associated with bullying (e.g. appearance, intelligence, racial and ethnic slurs, social acceptance, and rejection), and construct BullySpace , a common sense knowledge base that encodes particular knowledge about bullying situations. We then perform joint reasoning with common sense knowledge about a wide range of everyday life topics. We analyze messages using our novel AnalogySpace common sense reasoning technique. We also take into account social network analysis and other factors. We evaluate the model on real-world instances that have been reported by users on Formspring, a social networking website that is popular with teenagers. On the intervention side, we explore a set of reflective user-interaction paradigms with the goal of promoting empathy among social network participants. We propose an “air traffic control”-like dashboard, which alerts moderators to large-scale outbreaks that appear to be escalating or spreading and helps them prioritize the current deluge of user complaints. For potential victims, we provide educational material that informs them about how to cope with the situation, and connects them with emotional support from others. A user evaluation shows that in-context, targeted, and dynamic help during cyberbullying situations fosters end-user reflection that promotes better coping strategies.",
    "cited_by_count": 414,
    "openalex_id": "https://openalex.org/W2044173330",
    "type": "article"
  },
  {
    "title": "Diversity, Serendipity, Novelty, and Coverage",
    "doi": "https://doi.org/10.1145/2926720",
    "publication_date": "2016-12-19",
    "publication_year": 2016,
    "authors": "Marius Kaminskas; Derek Bridge",
    "corresponding_authors": "",
    "abstract": "What makes a good recommendation or good list of recommendations? Research into recommender systems has traditionally focused on accuracy, in particular how closely the recommender’s predicted ratings are to the users’ true ratings. However, it has been recognized that other recommendation qualities—such as whether the list of recommendations is diverse and whether it contains novel items—may have a significant impact on the overall quality of a recommender system. Consequently, in recent years, the focus of recommender systems research has shifted to include a wider range of “beyond accuracy” objectives. In this article, we present a survey of the most discussed beyond-accuracy objectives in recommender systems research: diversity, serendipity, novelty, and coverage. We review the definitions of these objectives and corresponding metrics found in the literature. We also review works that propose optimization strategies for these beyond-accuracy objectives. Since the majority of works focus on one specific objective, we find that it is not clear how the different objectives relate to each other. Hence, we conduct a set of offline experiments aimed at comparing the performance of different optimization approaches with a view to seeing how they affect objectives other than the ones they are optimizing. We use a set of state-of-the-art recommendation algorithms optimized for recall along with a number of reranking strategies for optimizing the diversity, novelty, and serendipity of the generated recommendations. For each reranking strategy, we measure the effects on the other beyond-accuracy objectives and demonstrate important insights into the correlations between the discussed objectives. For instance, we find that rating-based diversity is positively correlated with novelty, and we demonstrate the positive influence of novelty on recommendation coverage.",
    "cited_by_count": 390,
    "openalex_id": "https://openalex.org/W2562236173",
    "type": "article"
  },
  {
    "title": "AutoTutor and affective autotutor",
    "doi": "https://doi.org/10.1145/2395123.2395128",
    "publication_date": "2012-12-01",
    "publication_year": 2012,
    "authors": "Sidney K. D’Mello; Art Graesser",
    "corresponding_authors": "",
    "abstract": "We present AutoTutor and Affective AutoTutor as examples of innovative 21 st century interactive intelligent systems that promote learning and engagement. AutoTutor is an intelligent tutoring system that helps students compose explanations of difficult concepts in Newtonian physics and enhances computer literacy and critical thinking by interacting with them in natural language with adaptive dialog moves similar to those of human tutors. AutoTutor constructs a cognitive model of students' knowledge levels by analyzing the text of their typed or spoken responses to its questions. The model is used to dynamically tailor the interaction toward individual students' zones of proximal development. Affective AutoTutor takes the individualized instruction and human-like interactivity to a new level by automatically detecting and responding to students' emotional states in addition to their cognitive states. Over 20 controlled experiments comparing AutoTutor with ecological and experimental controls such reading a textbook have consistently yielded learning improvements of approximately one letter grade after brief 30--60-minute interactions. Furthermore, Affective AutoTutor shows even more dramatic improvements in learning than the original AutoTutor system, particularly for struggling students with low domain knowledge. In addition to providing a detailed description of the implementation and evaluation of AutoTutor and Affective AutoTutor, we also discuss new and exciting technologies motivated by AutoTutor such as AutoTutor-Lite, Operation ARIES, GuruTutor, DeepTutor, MetaTutor, and AutoMentor. We conclude this article with our vision for future work on interactive and engaging intelligent tutoring systems.",
    "cited_by_count": 312,
    "openalex_id": "https://openalex.org/W1974219072",
    "type": "article"
  },
  {
    "title": "A Review of User Interface Design for Interactive Machine Learning",
    "doi": "https://doi.org/10.1145/3185517",
    "publication_date": "2018-06-13",
    "publication_year": 2018,
    "authors": "John J. Dudley; Per Ola Kristensson",
    "corresponding_authors": "",
    "abstract": "Interactive Machine Learning (IML) seeks to complement human perception and intelligence by tightly integrating these strengths with the computational power and speed of computers. The interactive process is designed to involve input from the user but does not require the background knowledge or experience that might be necessary to work with more traditional machine learning techniques. Under the IML process, non-experts can apply their domain knowledge and insight over otherwise unwieldy datasets to find patterns of interest or develop complex data-driven applications. This process is co-adaptive in nature and relies on careful management of the interaction between human and machine. User interface design is fundamental to the success of this approach, yet there is a lack of consolidated principles on how such an interface should be implemented. This article presents a detailed review and characterisation of Interactive Machine Learning from an interactive systems perspective. We propose and describe a structural and behavioural model of a generalised IML system and identify solution principles for building effective interfaces for IML. Where possible, these emergent solution principles are contextualised by reference to the broader human-computer interaction literature. Finally, we identify strands of user interface research key to unlocking more efficient and productive non-expert interactive machine learning applications.",
    "cited_by_count": 297,
    "openalex_id": "https://openalex.org/W2794079986",
    "type": "review"
  },
  {
    "title": "Empathy in Virtual Agents and Robots",
    "doi": "https://doi.org/10.1145/2912150",
    "publication_date": "2017-09-19",
    "publication_year": 2017,
    "authors": "Ana Paiva; Iolanda Leite; Hana Boukricha; Ipke Wachsmuth",
    "corresponding_authors": "",
    "abstract": "This article surveys the area of computational empathy, analysing different ways by which artificial agents can simulate and trigger empathy in their interactions with humans. Empathic agents can be seen as agents that have the capacity to place themselves into the position of a user’s or another agent’s emotional situation and respond appropriately. We also survey artificial agents that, by their design and behaviour, can lead users to respond emotionally as if they were experiencing the agent’s situation. In the course of this survey, we present the research conducted to date on empathic agents in light of the principles and mechanisms of empathy found in humans. We end by discussing some of the main challenges that this exciting area will be facing in the future.",
    "cited_by_count": 278,
    "openalex_id": "https://openalex.org/W2754425142",
    "type": "article"
  },
  {
    "title": "Adaptive Persuasive Systems",
    "doi": "https://doi.org/10.1145/2209310.2209313",
    "publication_date": "2012-06-01",
    "publication_year": 2012,
    "authors": "Maurits Kaptein; Boris de Ruyter; Panos Markopoulos; Emile Aarts",
    "corresponding_authors": "",
    "abstract": "This article describes the use of personalized short text messages (SMS) to reduce snacking. First, we describe the development and validation ( N = 215) of a questionnaire to measure individual susceptibility to different social influence strategies. To evaluate the external validity of this Susceptibility to Persuasion Scale (STPS) we set up a two week text-messaging intervention that used text messages implementing social influence strategies as prompts to reduce snacking behavior. In this experiment ( N = 73) we show that messages that are personalized (tailored) to the individual based on their scores on the STPS, lead to a higher decrease in snacking consumption than randomized messages or messages that are not tailored (contra-tailored) to the individual. We discuss the importance of this finding for the design of persuasive systems and detail how designers can use tailoring at the level of social influence strategies to increase the effects of their persuasive technologies.",
    "cited_by_count": 246,
    "openalex_id": "https://openalex.org/W1999082074",
    "type": "article"
  },
  {
    "title": "Co-design of Human-centered, Explainable AI for Clinical Decision Support",
    "doi": "https://doi.org/10.1145/3587271",
    "publication_date": "2023-03-14",
    "publication_year": 2023,
    "authors": "Cecilia Panigutti; Andrea Beretta; Daniele Fadda; Fosca Giannotti; Dino Pedreschi; Alan Perotti; Salvatore Rinzivillo",
    "corresponding_authors": "",
    "abstract": "eXplainable AI (XAI) involves two intertwined but separate challenges: the development of techniques to extract explanations from black-box AI models and the way such explanations are presented to users, i.e., the explanation user interface. Despite its importance, the second aspect has received limited attention so far in the literature. Effective AI explanation interfaces are fundamental for allowing human decision-makers to take advantage and oversee high-risk AI systems effectively. Following an iterative design approach, we present the first cycle of prototyping-testing-redesigning of an explainable AI technique and its explanation user interface for clinical Decision Support Systems (DSS). We first present an XAI technique that meets the technical requirements of the healthcare domain: sequential, ontology-linked patient data, and multi-label classification tasks. We demonstrate its applicability to explain a clinical DSS, and we design a first prototype of an explanation user interface. Next, we test such a prototype with healthcare providers and collect their feedback with a two-fold outcome: First, we obtain evidence that explanations increase users’ trust in the XAI system, and second, we obtain useful insights on the perceived deficiencies of their interaction with the system, so we can re-design a better, more human-centered explanation interface.",
    "cited_by_count": 57,
    "openalex_id": "https://openalex.org/W4324155120",
    "type": "article"
  },
  {
    "title": "ID.8: Co-Creating Visual Stories with Generative AI",
    "doi": "https://doi.org/10.1145/3672277",
    "publication_date": "2024-06-15",
    "publication_year": 2024,
    "authors": "Victor Nikhil Antony; Chien-Ming Huang",
    "corresponding_authors": "",
    "abstract": "Storytelling is an integral part of human culture and significantly impacts cognitive and socio-emotional development and connection. Despite the importance of interactive visual storytelling, the process of creating such content requires specialized skills and is labor-intensive. This article introduces ID.8, an open-source system designed for the co-creation of visual stories with generative AI. We focus on enabling an inclusive storytelling experience by simplifying the content creation process and allowing for customization. Our user evaluation confirms a generally positive user experience in domains such as enjoyment and exploration while highlighting areas for improvement, particularly in immersiveness, alignment, and partnership between the user and the AI system. Overall, our findings indicate promising possibilities for empowering people to create visual stories with generative AI. This work contributes a novel content authoring system, ID.8, and insights into the challenges and potential of using generative AI for multimedia content creation.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W4399703381",
    "type": "article"
  },
  {
    "title": "Conversational gaze mechanisms for humanlike robots",
    "doi": "https://doi.org/10.1145/2070719.2070725",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Bilge Mutlu; Takayuki Kanda; Jodi Forlizzi; Jessica K. Hodgins; Hiroshi Ishiguro",
    "corresponding_authors": "",
    "abstract": "During conversations, speakers employ a number of verbal and nonverbal mechanisms to establish who participates in the conversation, when, and in what capacity. Gaze cues and mechanisms are particularly instrumental in establishing the participant roles of interlocutors, managing speaker turns, and signaling discourse structure. If humanlike robots are to have fluent conversations with people, they will need to use these gaze mechanisms effectively. The current work investigates people's use of key conversational gaze mechanisms, how they might be designed for and implemented in humanlike robots, and whether these signals effectively shape human-robot conversations. We focus particularly on whether humanlike gaze mechanisms might help robots signal different participant roles, manage turn-exchanges, and shape how interlocutors perceive the robot and the conversation. The evaluation of these mechanisms involved 36 trials of three-party human-robot conversations. In these trials, the robot used gaze mechanisms to signal to its conversational partners their roles either of two addressees, an addressee and a bystander, or an addressee and a nonparticipant. Results showed that participants conformed to these intended roles 97% of the time. Their conversational roles affected their rapport with the robot, feelings of groupness with their conversational partners, and attention to the task.",
    "cited_by_count": 185,
    "openalex_id": "https://openalex.org/W2063688338",
    "type": "article"
  },
  {
    "title": "Interacting with Recommenders—Overview and Research Directions",
    "doi": "https://doi.org/10.1145/3001837",
    "publication_date": "2017-09-19",
    "publication_year": 2017,
    "authors": "Michael Jugovac; Dietmar Jannach",
    "corresponding_authors": "",
    "abstract": "Automated recommendations have become a ubiquitous part of today’s online user experience. These systems point us to additional items to purchase in online shops, they make suggestions to us on movies to watch, or recommend us people to connect with on social websites. In many of today’s applications, however, the only way for users to interact with the system is to inspect the recommended items. Often, no mechanisms are implemented for users to give the system feedback on the recommendations or to explicitly specify preferences, which can limit the potential overall value of the system for its users. Academic research in recommender systems is largely focused on algorithmic approaches for item selection and ranking. Nonetheless, over the years a variety of proposals were made on how to design more interactive recommenders. This work provides a comprehensive overview on the existing literature on user interaction aspects in recommender systems. We cover existing approaches for preference elicitation and result presentation, as well as proposals that consider recommendation as an interactive process. Throughout the work, we furthermore discuss examples of real-world systems and outline possible directions for future works.",
    "cited_by_count": 166,
    "openalex_id": "https://openalex.org/W2754576213",
    "type": "article"
  },
  {
    "title": "Continuous body and hand gesture recognition for natural human-computer interaction",
    "doi": "https://doi.org/10.1145/2133366.2133371",
    "publication_date": "2012-03-01",
    "publication_year": 2012,
    "authors": "Yale Song; David Demirdjian; Randall Davis",
    "corresponding_authors": "",
    "abstract": "Intelligent gesture recognition systems open a new era of natural human-computer interaction: Gesturing is instinctive and a skill we all have, so it requires little or no thought, leaving the focus on the task itself, as it should be, not on the interaction modality. We present a new approach to gesture recognition that attends to both body and hands, and interprets gestures continuously from an unsegmented and unbounded input stream. This article describes the whole procedure of continuous body and hand gesture recognition, from the signal acquisition to processing, to the interpretation of the processed signals. Our system takes a vision-based approach, tracking body and hands using a single stereo camera. Body postures are reconstructed in 3D space using a generative model-based approach with a particle filter, combining both static and dynamic attributes of motion as the input feature to make tracking robust to self-occlusion. The reconstructed body postures guide searching for hands. Hand shapes are classified into one of several canonical hand shapes using an appearance-based approach with a multiclass support vector machine. Finally, the extracted body and hand features are combined and used as the input feature for gesture recognition. We consider our task as an online sequence labeling and segmentation problem. A latent-dynamic conditional random field is used with a temporal sliding window to perform the task continuously. We augment this with a novel technique called multilayered filtering, which performs filtering both on the input layer and the prediction layer. Filtering on the input layer allows capturing long-range temporal dependencies and reducing input signal noise; filtering on the prediction layer allows taking weighted votes of multiple overlapping prediction results as well as reducing estimation noise. We tested our system in a scenario of real-world gestural interaction using the NATOPS dataset, an official vocabulary of aircraft handling gestures. Our experimental results show that: (1) the use of both static and dynamic attributes of motion in body tracking allows statistically significant improvement of the recognition performance over using static attributes of motion alone; and (2) the multilayered filtering statistically significantly improves recognition performance over the nonfiltering method. We also show that, on a set of twenty-four NATOPS gestures, our system achieves a recognition accuracy of 75.37%.",
    "cited_by_count": 148,
    "openalex_id": "https://openalex.org/W2089513492",
    "type": "article"
  },
  {
    "title": "Modeling User Preferences in Recommender Systems",
    "doi": "https://doi.org/10.1145/2512208",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "Gawesh Jawaheer; Peter Weller; Patty Kostkova",
    "corresponding_authors": "",
    "abstract": "Recommender systems are firmly established as a standard technology for assisting users with their choices; however, little attention has been paid to the application of the user model in recommender systems, particularly the variability and noise that are an intrinsic part of human behavior and activity. To enable recommender systems to suggest items that are useful to a particular user, it can be essential to understand the user and his or her interactions with the system. These interactions typically manifest themselves as explicit and implicit user feedback that provides the key indicators for modeling users’ preferences for items and essential information for personalizing recommendations. In this article, we propose a classification framework for the use of explicit and implicit user feedback in recommender systems based on a set of distinct properties that include Cognitive Effort, User Model, Scale of Measurement, and Domain Relevance. We develop a set of comparison criteria for explicit and implicit user feedback to emphasize the key properties. Using our framework, we provide a classification of recommender systems that have addressed questions about user feedback, and we review state-of-the-art techniques to improve such user feedback and thereby improve the performance of the recommender system. Finally, we formulate challenges for future research on improvement of user feedback.",
    "cited_by_count": 143,
    "openalex_id": "https://openalex.org/W2053473945",
    "type": "article"
  },
  {
    "title": "The Tag Genome",
    "doi": "https://doi.org/10.1145/2362394.2362395",
    "publication_date": "2012-09-01",
    "publication_year": 2012,
    "authors": "Jesse Vig; Shilad Sen; John Riedl",
    "corresponding_authors": "",
    "abstract": "This article introduces the tag genome, a data structure that extends the traditional tagging model to provide enhanced forms of user interaction. Just as a biological genome encodes an organism based on a sequence of genes, the tag genome encodes an item in an information space based on its relationship to a common set of tags. We present a machine learning approach for computing the tag genome, and we evaluate several learning models on a ground truth dataset provided by users. We describe an application of the tag genome called Movie Tuner which enables users to navigate from one item to nearby items along dimensions represented by tags. We present the results of a 7-week field trial of 2,531 users of Movie Tuner and a survey evaluating users’ subjective experience. Finally, we outline the broader space of applications of the tag genome.",
    "cited_by_count": 135,
    "openalex_id": "https://openalex.org/W2108790711",
    "type": "article"
  },
  {
    "title": "Making Decisions about Privacy",
    "doi": "https://doi.org/10.1145/2499670",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Bart P. Knijnenburg; Alfred Kobsa",
    "corresponding_authors": "",
    "abstract": "Recommender systems increasingly use contextual and demographical data as a basis for recommendations. Users, however, often feel uncomfortable providing such information. In a privacy-minded design of recommenders, users are free to decide for themselves what data they want to disclose about themselves. But this decision is often complex and burdensome, because the consequences of disclosing personal information are uncertain or even unknown. Although a number of researchers have tried to analyze and facilitate such information disclosure decisions, their research results are fragmented, and they often do not hold up well across studies. This article describes a unified approach to privacy decision research that describes the cognitive processes involved in users’ “privacy calculus” in terms of system-related perceptions and experiences that act as mediating factors to information disclosure. The approach is applied in an online experiment with 493 participants using a mock-up of a context-aware recommender system. Analyzing the results with a structural linear model, we demonstrate that personal privacy concerns and disclosure justification messages affect the perception of and experience with a system, which in turn drive information disclosure decisions. Overall, disclosure justification messages do not increase disclosure. Although they are perceived to be valuable, they decrease users’ trust and satisfaction. Another result is that manipulating the order of the requests increases the disclosure of items requested early but decreases the disclosure of items requested later.",
    "cited_by_count": 131,
    "openalex_id": "https://openalex.org/W2091649703",
    "type": "article"
  },
  {
    "title": "Using Machine Learning to Support Qualitative Coding in Social Science",
    "doi": "https://doi.org/10.1145/3185515",
    "publication_date": "2018-06-21",
    "publication_year": 2018,
    "authors": "Nan-Chen Chen; Margaret Drouhard; Rafał Kocielnik; Jina Suh; C. Aragon",
    "corresponding_authors": "",
    "abstract": "Machine learning (ML) has become increasingly influential to human society, yet the primary advancements and applications of ML are driven by research in only a few computational disciplines. Even applications that affect or analyze human behaviors and social structures are often developed with limited input from experts outside of computational fields. Social scientists—experts trained to examine and explain the complexity of human behavior and interactions in the world—have considerable expertise to contribute to the development of ML applications for human-generated data, and their analytic practices could benefit from more human-centered ML methods. Although a few researchers have highlighted some gaps between ML and social sciences [51, 57, 70], most discussions only focus on quantitative methods. Yet many social science disciplines rely heavily on qualitative methods to distill patterns that are challenging to discover through quantitative data. One common analysis method for qualitative data is qualitative coding . In this article, we highlight three challenges of applying ML to qualitative coding. Additionally, we utilize our experience of designing a visual analytics tool for collaborative qualitative coding to demonstrate the potential in using ML to support qualitative coding by shifting the focus to identifying ambiguity. We illustrate dimensions of ambiguity and discuss the relationship between disagreement and ambiguity. Finally, we propose three research directions to ground ML applications for social science as part of the progression toward human-centered machine learning.",
    "cited_by_count": 130,
    "openalex_id": "https://openalex.org/W2809100814",
    "type": "article"
  },
  {
    "title": "A Review and Taxonomy of Interactive Optimization Methods in Operations Research",
    "doi": "https://doi.org/10.1145/2808234",
    "publication_date": "2015-09-23",
    "publication_year": 2015,
    "authors": "David Meignan; Sigrid Knust; Jean‐Marc Frayret; Gilles Pesant; Nicolas Gaud",
    "corresponding_authors": "",
    "abstract": "This article presents a review and a classification of interactive optimization methods. These interactive methods are used for solving optimization problems. The interaction with an end user or decision maker aims at improving the efficiency of the optimization procedure, enriching the optimization model, or informing the user regarding the solutions proposed by the optimization system. First, we present the challenges of using optimization methods as a tool for supporting decision making, and we justify the integration of the user in the optimization process. This integration is generally achieved via a dynamic interaction between the user and the system. Next, the different classes of interactive optimization approaches are presented. This detailed review includes trial and error, interactive reoptimization, interactive multiobjective optimization, interactive evolutionary algorithms, human-guided search, and other approaches that are less well covered in the research literature. On the basis of this review, we propose a classification that aims to better describe and compare interaction mechanisms. This classification offers two complementary views on interactive optimization methods. The first perspective focuses on the user’s contribution to the optimization process, and the second concerns the components of interactive optimization systems. Finally, on the basis of this review and classification, we identify some open issues and potential perspectives for interactive optimization methods.",
    "cited_by_count": 124,
    "openalex_id": "https://openalex.org/W2080927183",
    "type": "review"
  },
  {
    "title": "Toward an Understanding of Trust Repair in Human-Robot Interaction",
    "doi": "https://doi.org/10.1145/3181671",
    "publication_date": "2018-11-16",
    "publication_year": 2018,
    "authors": "Anthony L. Baker; Elizabeth Phillips; Daniel Ullman; Joseph R. Keebler",
    "corresponding_authors": "",
    "abstract": "Gone are the days of robots solely operating in isolation, without direct interaction with people. Rather, robots are increasingly being deployed in environments and roles that require complex social interaction with humans. The implementation of human-robot teams continues to increase as technology develops in tandem with the state of human-robot interaction (HRI) research. Trust, a major component of human interaction, is an important facet of HRI. However, the ideas of trust repair and trust violations are understudied in the HRI literature. Trust repair is the activity of rebuilding trust after one party breaks the trust of another. These trust breaks are referred to as trust violations . Just as with humans, trust violations with robots are inevitable; as a result, a clear understanding of the process of HRI trust repair must be developed in order to ensure that a human-robot team can continue to perform well after a trust violation. Previous research on human-automation trust and human-human trust can serve as starting places for exploring trust repair in HRI. Although existing models of human-automation and human-human trust are helpful, they do not account for some of the complexities of building and maintaining trust in unique relationships between humans and robots. The purpose of this article is to provide a foundation for exploring human-robot trust repair by drawing upon prior work in the human-robot, human-automation, and human-human trust literature, concluding with recommendations for advancing this body of work.",
    "cited_by_count": 122,
    "openalex_id": "https://openalex.org/W2901151989",
    "type": "article"
  },
  {
    "title": "Trusting Virtual Agents",
    "doi": "https://doi.org/10.1145/3232077",
    "publication_date": "2019-03-18",
    "publication_year": 2019,
    "authors": "Michelle X. Zhou; Gloria Mark; Jingyi Jessica Li; Huahai Yang",
    "corresponding_authors": "",
    "abstract": "We present artificial intelligent (AI) agents that act as interviewers to engage with a user in a text-based conversation and automatically infer the user's personality traits. We investigate how the personality of an AI interviewer and the inferred personality of a user influences the user's trust in the AI interviewer from two perspectives: the user's willingness to confide in and listen to an AI interviewer. We have developed two AI interviewers with distinct personalities and deployed them in a series of real-world events. We present findings from four such deployments involving 1,280 users, including 606 actual job applicants. Notably, users are more willing to confide in and listen to an AI interviewer with a serious, assertive personality in a high-stakes job interview. Moreover, users’ personality traits, inferred from their chat text, along with interview context, influence their perception of and their willingness to confide in and listen to an AI interviewer. Finally, we discuss the design implications of our work on building hyper-personalized, intelligent agents.",
    "cited_by_count": 122,
    "openalex_id": "https://openalex.org/W2924196385",
    "type": "article"
  },
  {
    "title": "Multimodal behavior and interaction as indicators of cognitive load",
    "doi": "https://doi.org/10.1145/2395123.2395127",
    "publication_date": "2012-12-01",
    "publication_year": 2012,
    "authors": "Fang Chen; Natalie Ruiz; Eric H. C. Choi; Julien Epps; M. Asif Khawaja; Ronnie Taib; Bo Yin; Yang Wang",
    "corresponding_authors": "",
    "abstract": "High cognitive load arises from complex time and safety-critical tasks, for example, mapping out flight paths, monitoring traffic, or even managing nuclear reactors, causing stress, errors, and lowered performance. Over the last five years, our research has focused on using the multimodal interaction paradigm to detect fluctuations in cognitive load in user behavior during system interaction. Cognitive load variations have been found to impact interactive behavior: by monitoring variations in specific modal input features executed in tasks of varying complexity, we gain an understanding of the communicative changes that occur when cognitive load is high. So far, we have identified specific changes in: speech, namely acoustic, prosodic, and linguistic changes; interactive gesture; and digital pen input, both interactive and freeform. As ground-truth measurements, galvanic skin response, subjective, and performance ratings have been used to verify task complexity. The data suggest that it is feasible to use features extracted from behavioral changes in multiple modal inputs as indices of cognitive load. The speech-based indicators of load, based on data collected from user studies in a variety of domains, have shown considerable promise. Scenarios include single-user and team-based tasks; think-aloud and interactive speech; and single-word, reading, and conversational speech, among others. Pen-based cognitive load indices have also been tested with some success, specifically with pen-gesture, handwriting, and freeform pen input, including diagraming. After examining some of the properties of these measurements, we present a multimodal fusion model, which is illustrated with quantitative examples from a case study. The feasibility of employing user input and behavior patterns as indices of cognitive load is supported by experimental evidence. Moreover, symptomatic cues of cognitive load derived from user behavior such as acoustic speech signals, transcribed text, digital pen trajectories of handwriting, and shapes pen, can be supported by well-established theoretical frameworks, including O'Donnell and Eggemeier's workload measurement [1986] Sweller's Cognitive Load Theory [Chandler and Sweller 1991], and Baddeley's model of modal working memory [1992] as well as McKinstry et al.'s [2008] and Rosenbaum's [2005] action dynamics work. The benefit of using this approach to determine the user's cognitive load in real time is that the data can be collected implicitly that is, during day-to-day use of intelligent interactive systems, thus overcomes problems of intrusiveness and increases applicability in real-world environments, while adapting information selection and presentation in a dynamic computer interface with reference to load.",
    "cited_by_count": 120,
    "openalex_id": "https://openalex.org/W1969783622",
    "type": "article"
  },
  {
    "title": "Gaze and turn-taking behavior in casual conversational interactions",
    "doi": "https://doi.org/10.1145/2499474.2499481",
    "publication_date": "2013-07-01",
    "publication_year": 2013,
    "authors": "Kristiina Jokinen; Hirohisa Furukawa; Masafumi Nishida; Seiichi Yamamoto",
    "corresponding_authors": "",
    "abstract": "Eye gaze is an important means for controlling interaction and coordinating the participants' turns smoothly. We have studied how eye gaze correlates with spoken interaction and especially focused on the combined effect of the speech signal and gazing to predict turn taking possibilities. It is well known that mutual gaze is important in the coordination of turn taking in two-party dialogs, and in this article, we investigate whether this fact also holds for three-party conversations. In group interactions, it may be that different features are used for managing turn taking than in two-party dialogs. We collected casual conversational data and used an eye tracker to systematically observe a participant's gaze in the interactions. By studying the combined effect of speech and gaze on turn taking, we aimed to answer our main questions: How well can eye gaze help in predicting turn taking? What is the role of eye gaze when the speaker holds the turn? Is the role of eye gaze as important in three-party dialogs as in two-party dialogue? We used Support Vector Machines (SVMs) to classify turn taking events with respect to speech and gaze features, so as to estimate how well the features signal a change of the speaker or a continuation of the same speaker. The results confirm the earlier hypothesis that eye gaze significantly helps in predicting the partner's turn taking activity, and we also get supporting evidence for our hypothesis that the speaker is a prominent coordinator of the interaction space. Such a turn taking model could be used in interactive applications to improve the system's conversational performance.",
    "cited_by_count": 114,
    "openalex_id": "https://openalex.org/W2141666434",
    "type": "article"
  },
  {
    "title": "Influencing Individually",
    "doi": "https://doi.org/10.1145/2209310.2209312",
    "publication_date": "2012-06-01",
    "publication_year": 2012,
    "authors": "Shlomo Berkovsky; Jill Freyne; Harri Oinas‐Kukkonen",
    "corresponding_authors": "",
    "abstract": "Personalized technologies aim to enhance user experience by taking into account users’ interests, preferences, and other relevant information. Persuasive technologies aim to modify user attitudes, intentions, or behavior through computer-human dialogue and social influence. While both personalized and persuasive technologies influence user interaction and behavior, we posit that this influence could be significantly increased if the two technologies were combined to create personalized and persuasive systems. For example, the persuasive power of a one-size-fits-all persuasive intervention could be enhanced by considering the users being influenced and their susceptibility to the persuasion being offered. Likewise, personalized technologies could cash in on increased success, in terms of user satisfaction, revenue, and user experience, if their services used persuasive techniques. Hence, the coupling of personalization and persuasion has the potential to enhance the impact of both technologies. This new, developing area clearly offers mutual benefits to both research areas, as we illustrate in this special issue.",
    "cited_by_count": 111,
    "openalex_id": "https://openalex.org/W1985438984",
    "type": "article"
  },
  {
    "title": "Investigating the Persuasion Potential of Recommender Systems from a Quality Perspective",
    "doi": "https://doi.org/10.1145/2209310.2209314",
    "publication_date": "2012-06-01",
    "publication_year": 2012,
    "authors": "Paolo Cremonesi; Franca Garzotto; Roberto Turrin",
    "corresponding_authors": "",
    "abstract": "Recommender Systems (RSs) help users search large amounts of digital contents and services by allowing them to identify the items that are likely to be more attractive or useful. RSs play an important persuasion role, as they can potentially augment the users’ trust towards in an application and orient their decisions or actions towards specific directions. This article explores the persuasiveness of RSs, presenting two vast empirical studies that address a number of research questions. First, we investigate if a design property of RSs, defined by the statistically measured quality of algorithms, is a reliable predictor of their potential for persuasion. This factor is measured in terms of perceived quality, defined by the overall satisfaction, as well as by how users judge the accuracy and novelty of recommendations. For our purposes, we designed an empirical study involving 210 subjects and implemented seven full-sized versions of a commercial RS, each one using the same interface and dataset (a subset of Netflix), but each with a different recommender algorithm. In each experimental configuration we computed the statistical quality (recall and F-measures) and collected data regarding the quality perceived by 30 users. The results show us that algorithmic attributes are less crucial than we might expect in determining the user’s perception of an RS’s quality, and suggest that the user’s judgment and attitude towards a recommender are likely to be more affected by factors related to the user experience. Second, we explore the persuasiveness of RSs in the context of large interactive TV services. We report a study aimed at assessing whether measurable persuasion effects (e.g., changes of shopping behavior) can be achieved through the introduction of a recommender. Our data, collected for more than one year, allow us to conclude that, (1) the adoption of an RS can affect both the lift factor and the conversion rate, determining an increased volume of sales and influencing the user’s decision to actually buy one of the recommended products, (2) the introduction of an RS tends to diversify purchases and orient users towards less obvious choices (the long tail), and (3) the perceived novelty of recommendations is likely to be more influential than their perceived accuracy. Overall, the results of these studies improve our understanding of the persuasion phenomena induced by RSs, and have implications that can be of interest to academic scholars, designers, and adopters of this class of systems.",
    "cited_by_count": 110,
    "openalex_id": "https://openalex.org/W2018371603",
    "type": "article"
  },
  {
    "title": "Attentive documents",
    "doi": "https://doi.org/10.1145/2070719.2070722",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Georg Buscher; Andreas Dengel; Ralf Biedert; Ludger van Elst",
    "corresponding_authors": "",
    "abstract": "Reading is one of the most frequent activities of knowledge workers. Eye tracking can provide information on what document parts users read, and how they were read. This article aims at generating implicit relevance feedback from eye movements that can be used for information retrieval personalization and further applications. We report the findings from two studies which examine the relation between several eye movement measures and user-perceived relevance of read text passages. The results show that the measures are generally noisy, but after personalizing them we find clear relations between the measures and relevance. In addition, the second study demonstrates the effect of using reading behavior as implicit relevance feedback for personalizing search. The results indicate that gaze-based feedback is very useful and can greatly improve the quality of Web search. The article concludes with an outlook introducing attentive documents keeping track of how users consume them. Based on eye movement feedback, we describe a number of possible applications to make working with documents more effective.",
    "cited_by_count": 110,
    "openalex_id": "https://openalex.org/W2028113824",
    "type": "article"
  },
  {
    "title": "Human Decision Making and Recommender Systems",
    "doi": "https://doi.org/10.1145/2533670.2533675",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Li Chen; Marco de Gemmis; Alexander Felfernig; Pasquale Lops; Francesco Ricci⋆; Giovanni Semeraro",
    "corresponding_authors": "",
    "abstract": "Recommender systems have already proved to be valuable for coping with the information overload problem in several application domains. They provide people with suggestions for items which are likely to be of interest for them; hence, a primary function of recommender systems is to help people make good choices and decisions. However, most previous research has focused on recommendation techniques and algorithms, and less attention has been devoted to the decision making processes adopted by the users and possibly supported by the system. There is still a gap between the importance that the community gives to the assessment of recommendation algorithms and the current range of ongoing research activities concerning human decision making. Different decision-psychological phenomena can influence the decision making of users of recommender systems, and research along these lines is becoming increasingly important and popular. This special issue highlights how the coupling of recommendation algorithms with the understanding of human choice and decision making theory has the potential to benefit research and practice on recommender systems and to enable users to achieve a good balance between decision accuracy and decision effort.",
    "cited_by_count": 108,
    "openalex_id": "https://openalex.org/W2002317872",
    "type": "article"
  },
  {
    "title": "Using Video to Automatically Detect Learner Affect in Computer-Enabled Classrooms",
    "doi": "https://doi.org/10.1145/2946837",
    "publication_date": "2016-07-20",
    "publication_year": 2016,
    "authors": "Nigel Bosch; Sidney K. D’Mello; Jaclyn Ocumpaugh; Ryan S. Baker; Valerie J. Shute",
    "corresponding_authors": "",
    "abstract": "Affect detection is a key component in intelligent educational interfaces that respond to students’ affective states. We use computer vision and machine-learning techniques to detect students’ affect from facial expressions (primary channel) and gross body movements (secondary channel) during interactions with an educational physics game. We collected data in the real-world environment of a school computer lab with up to 30 students simultaneously playing the game while moving around, gesturing, and talking to each other. The results were cross-validated at the student level to ensure generalization to new students. Classification accuracies, quantified as area under the receiver operating characteristic curve (AUC), were above chance (AUC of 0.5) for all the affective states observed, namely, boredom (AUC = .610), confusion (AUC = .649), delight (AUC = .867), engagement (AUC = .679), frustration (AUC = .631), and for off-task behavior (AUC = .816). Furthermore, the detectors showed temporal generalizability in that there was less than a 2% decrease in accuracy when tested on data collected from different times of the day and from different days. There was also some evidence of generalizability across ethnicity (as perceived by human coders) and gender, although with a higher degree of variability attributable to differences in affect base rates across subpopulations. In summary, our results demonstrate the feasibility of generalizable video-based detectors of naturalistic affect in a real-world setting, suggesting that the time is ripe for affect-sensitive interventions in educational games and other intelligent interfaces.",
    "cited_by_count": 107,
    "openalex_id": "https://openalex.org/W2485075409",
    "type": "article"
  },
  {
    "title": "Adaptive Body Gesture Representation for Automatic Emotion Recognition",
    "doi": "https://doi.org/10.1145/2818740",
    "publication_date": "2016-03-09",
    "publication_year": 2016,
    "authors": "Stefano Piana; Alessandra Staglianò; Francesca Odone; Antonio Camurri",
    "corresponding_authors": "",
    "abstract": "We present a computational model and a system for the automated recognition of emotions starting from full-body movement. Three-dimensional motion data of full-body movements are obtained either from professional optical motion-capture systems (Qualisys) or from low-cost RGB-D sensors (Kinect and Kinect2). A number of features are then automatically extracted at different levels, from kinematics of a single joint to more global expressive features inspired by psychology and humanistic theories (e.g., contraction index, fluidity, and impulsiveness). An abstraction layer based on dictionary learning further processes these movement features to increase the model generality and to deal with intraclass variability, noise, and incomplete information characterizing emotion expression in human movement. The resulting feature vector is the input for a classifier performing real-time automatic emotion recognition based on linear support vector machines. The recognition performance of the proposed model is presented and discussed, including the tradeoff between precision of the tracking measures (we compare the Kinect RGB-D sensor and the Qualisys motion-capture system) versus dimension of the training dataset. The resulting model and system have been successfully applied in the development of serious games for helping autistic children learn to recognize and express emotions by means of their full-body movement.",
    "cited_by_count": 104,
    "openalex_id": "https://openalex.org/W2333212123",
    "type": "article"
  },
  {
    "title": "Chatbots to Support Young Adults’ Mental Health: An Exploratory Study of Acceptability",
    "doi": "https://doi.org/10.1145/3485874",
    "publication_date": "2022-05-24",
    "publication_year": 2022,
    "authors": "Theodora Koulouri; Robert D. Macredie; David Olakitan",
    "corresponding_authors": "",
    "abstract": "Despite the prevalence of mental health conditions, stigma, lack of awareness, and limited resources impede access to care, creating a need to improve mental health support. The recent surge in scientific and commercial interest in conversational agents and their potential to improve diagnosis and treatment seems a potentially fruitful area in this respect, particularly for young adults who widely use such systems in other contexts. Yet, there is little research that considers the acceptability of conversational agents in mental health. This study, therefore, presents three research activities that explore whether conversational agents and, in particular, chatbots can be an acceptable solution in mental healthcare for young adults. First, a survey of young adults (in a university setting) provides an understanding of the landscape of mental health in this age group and of their views around mental health technology, including chatbots. Second, a literature review synthesises current evidence relating to the acceptability of mental health conversational agents and points to future research priorities. Third, interviews with counsellors who work with young adults, supported by a chatbot prototype and user-centred design techniques, reveal the perceived benefits and potential roles of mental health chatbots from the perspective of mental health professionals, while suggesting preconditions for the acceptability of the technology. Taken together, these research activities: provide evidence that chatbots are an acceptable solution to offering mental health support for young adults; identify specific challenges relating to both the technology and environment; and argue for the application of user-centred approaches during development of mental health chatbots and more systematic and rigorous evaluations of the resulting solutions.",
    "cited_by_count": 78,
    "openalex_id": "https://openalex.org/W3200630942",
    "type": "article"
  },
  {
    "title": "An Empirical Study of Older Adult’s Voice Assistant Use for Health Information Seeking",
    "doi": "https://doi.org/10.1145/3484507",
    "publication_date": "2022-05-20",
    "publication_year": 2022,
    "authors": "Robin Brewer; Casey Pierce; Pooja Upadhyay; Leeseul Park",
    "corresponding_authors": "",
    "abstract": "Although voice assistants are increasingly being adopted by older adults, we lack empirical research on how they interact with these devices for health information seeking. Also, prior work shows how voice assistant responses can provide misleading or inaccurate information and be harmful particularly in health contexts. Because of increased health needs while aging, this paper studies older adult’s (ages 65+) health-related voice assistant interactions. Motivated by a lack of empirical evidence for how older adults approach information seeking with emerging technologies, we first conducted a survey of n = 201 older adults to understand how they engage voice assistants compared to a range of offline and digital sources for health information seeking. Findings show how voice assistants were used for confirmatory health queries, with users showing signs of distrust. As much prior work focuses on perceptions of voice assistant use, we conducted scenario-based interviews with n = 35 older adults to study health-related voice assistant behavior. In interviews, participants engaged with different health topics (flu, migraine, high blood pressure) and scenario types (symptom-driven, behavior-driven) using a voice assistant. Findings show how conversational and human-like expectations with voice assistants lead to information breakdowns between the older adult and voice assistant. This paper contributes a nuanced query-level analysis of older adults’ voice-based health information seeking behaviors. Further, data provide evidence for how query reformulation happens with complex topics in voice-based information seeking. We use our findings to discuss how voice interfaces can better support older adults’ health information seeking behaviors and expectations.",
    "cited_by_count": 62,
    "openalex_id": "https://openalex.org/W4286273849",
    "type": "article"
  },
  {
    "title": "Toward Involving End-users in Interactive Human-in-the-loop AI Fairness",
    "doi": "https://doi.org/10.1145/3514258",
    "publication_date": "2022-04-28",
    "publication_year": 2022,
    "authors": "Yuri Nakao; Simone Stumpf; Subeida Ahmed; Aisha Naseer; Lorenzo Strappelli",
    "corresponding_authors": "",
    "abstract": "Ensuring fairness in artificial intelligence (AI) is important to counteract bias and discrimination in far-reaching applications. Recent work has started to investigate how humans judge fairness and how to support machine learning experts in making their AI models fairer. Drawing inspiration from an Explainable AI approach called explanatory debugging used in interactive machine learning, our work explores designing interpretable and interactive human-in-the-loop interfaces that allow ordinary end-users without any technical or domain background to identify potential fairness issues and possibly fix them in the context of loan decisions. Through workshops with end-users, we co-designed and implemented a prototype system that allowed end-users to see why predictions were made, and then to change weights on features to “debug” fairness issues. We evaluated the use of this prototype system through an online study. To investigate the implications of diverse human values about fairness around the globe, we also explored how cultural dimensions might play a role in using this prototype. Our results contribute to the design of interfaces to allow end-users to be involved in judging and addressing AI fairness through a human-in-the-loop approach.",
    "cited_by_count": 48,
    "openalex_id": "https://openalex.org/W4288032566",
    "type": "article"
  },
  {
    "title": "An Agile New Research Framework for Hybrid Human-AI Teaming: Trust, Transparency, and Transferability",
    "doi": "https://doi.org/10.1145/3514257",
    "publication_date": "2022-04-29",
    "publication_year": 2022,
    "authors": "Sabrina Caldwell; Penny Sweetser; Nicholas O’Donnell; Matthew J. Knight; Matthew Aitchison; Tom Gedeon; Daniel Johnson; Margot Brereton; Marcus Gallagher; David Conroy",
    "corresponding_authors": "",
    "abstract": "We propose a new research framework by which the nascent discipline of human-AI teaming can be explored within experimental environments in preparation for transferal to real-world contexts. We examine the existing literature and unanswered research questions through the lens of an Agile approach to construct our proposed framework. Our framework aims to provide a structure for understanding the macro features of this research landscape, supporting holistic research into the acceptability of human-AI teaming to human team members and the affordances of AI team members. The framework has the potential to enhance decision-making and performance of hybrid human-AI teams. Further, our framework proposes the application of Agile methodology for research management and knowledge discovery. We propose a transferability pathway for hybrid teaming to be initially tested in a safe environment, such as a real-time strategy video game, with elements of lessons learned that can be transferred to real-world situations.",
    "cited_by_count": 47,
    "openalex_id": "https://openalex.org/W4225151977",
    "type": "article"
  },
  {
    "title": "Visualization and Visual Analytics Approaches for Image and Video Datasets: A Survey",
    "doi": "https://doi.org/10.1145/3576935",
    "publication_date": "2023-01-02",
    "publication_year": 2023,
    "authors": "Shehzad Afzal; Sohaib Ghani; Mohamad Mazen Hittawe; Sheikh Faisal Rashid; Omar Knio; Markus Hadwiger; Ibrahim Hoteit",
    "corresponding_authors": "",
    "abstract": "Image and video data analysis has become an increasingly important research area with applications in different domains such as security surveillance, healthcare, augmented and virtual reality, video and image editing, activity analysis and recognition, synthetic content generation, distance education, telepresence, remote sensing, sports analytics, art, non-photorealistic rendering, search engines, and social media. Recent advances in Artificial Intelligence (AI) and particularly deep learning have sparked new research challenges and led to significant advancements, especially in image and video analysis. These advancements have also resulted in significant research and development in other areas such as visualization and visual analytics, and have created new opportunities for future lines of research. In this survey article, we present the current state of the art at the intersection of visualization and visual analytics, and image and video data analysis. We categorize the visualization articles included in our survey based on different taxonomies used in visualization and visual analytics research. We review these articles in terms of task requirements, tools, datasets, and application areas. We also discuss insights based on our survey results, trends and patterns, the current focus of visualization research, and opportunities for future research.",
    "cited_by_count": 46,
    "openalex_id": "https://openalex.org/W4313476570",
    "type": "article"
  },
  {
    "title": "Explainable Activity Recognition for Smart Home Systems",
    "doi": "https://doi.org/10.1145/3561533",
    "publication_date": "2023-03-22",
    "publication_year": 2023,
    "authors": "Devleena Das; Yasutaka Nishimura; Rajan P. Vivek; Naoto Takeda; Sean T. Fish; Thomas Plötz; Sonia Chernova",
    "corresponding_authors": "",
    "abstract": "Smart home environments are designed to provide services that help improve the quality of life for the occupant via a variety of sensors and actuators installed throughout the space. Many automated actions taken by a smart home are governed by the output of an underlying activity recognition system. However, activity recognition systems may not be perfectly accurate and therefore inconsistencies in smart home operations can lead users reliant on smart home predictions to wonder \"why did the smart home do that?\" In this work, we build on insights from Explainable Artificial Intelligence (XAI) techniques and introduce an explainable activity recognition framework in which we leverage leading XAI methods to generate natural language explanations that explain what about an activity led to the given classification. Within the context of remote caregiver monitoring, we perform a two-step evaluation: (a) utilize ML experts to assess the sensibility of explanations, and (b) recruit non-experts in two user remote caregiver monitoring scenarios, synchronous and asynchronous, to assess the effectiveness of explanations generated via our framework. Our results show that the XAI approach, SHAP, has a 92% success rate in generating sensible explanations. Moreover, in 83% of sampled scenarios users preferred natural language explanations over a simple activity label, underscoring the need for explainable activity recognition systems. Finally, we show that explanations generated by some XAI methods can lead users to lose confidence in the accuracy of the underlying activity recognition model. We make a recommendation regarding which existing XAI method leads to the best performance in the domain of smart home automation, and discuss a range of topics for future work to further improve explainable activity recognition.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W3162602085",
    "type": "article"
  },
  {
    "title": "The Role of Explainable AI in the Research Field of AI Ethics",
    "doi": "https://doi.org/10.1145/3599974",
    "publication_date": "2023-06-01",
    "publication_year": 2023,
    "authors": "Heidi Vainio-Pekka; Mamia Agbese; Marianna Jantunen; Ville Vakkuri; Tommi Mikkonen; Rebekah Rousi; Pekka Abrahamsson",
    "corresponding_authors": "",
    "abstract": "Ethics of Artificial Intelligence (AI) is a growing research field that has emerged in response to the challenges related to AI. Transparency poses a key challenge for implementing AI ethics in practice. One solution to transparency issues is AI systems that can explain their decisions. Explainable AI (XAI) refers to AI systems that are interpretable or understandable to humans. The research fields of AI ethics and XAI lack a common framework and conceptualization. There is no clarity of the field’s depth and versatility. A systematic approach to understanding the corpus is needed. A systematic review offers an opportunity to detect research gaps and focus points. This article presents the results of a systematic mapping study (SMS) of the research field of the Ethics of AI. The focus is on understanding the role of XAI and how the topic has been studied empirically. An SMS is a tool for performing a repeatable and continuable literature search. This article contributes to the research field with a Systematic Map that visualizes what, how, when, and why XAI has been studied empirically in the field of AI ethics. The mapping reveals research gaps in the area. Empirical contributions are drawn from the analysis. The contributions are reflected on in regards to theoretical and practical implications. As the scope of the SMS is a broader research area of AI ethics, the collected dataset opens possibilities to continue the mapping process in other directions.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W4378981991",
    "type": "article"
  },
  {
    "title": "Visualization for Recommendation Explainability: A Survey and New Perspectives",
    "doi": "https://doi.org/10.1145/3672276",
    "publication_date": "2024-06-11",
    "publication_year": 2024,
    "authors": "Mohamed Amine Chatti; Mouadh Guesmi; Arham Muslim",
    "corresponding_authors": "",
    "abstract": "Providing system-generated explanations for recommendations represents an important step toward transparent and trustworthy recommender systems. Explainable recommender systems provide a human-understandable rationale for their outputs. Over the past two decades, explainable recommendation has attracted much attention in the recommender systems research community. This paper aims to provide a comprehensive review of research efforts on visual explanation in recommender systems. More concretely, we systematically review the literature on explanations in recommender systems based on four dimensions, namely explanation aim, explanation scope, explanation method, and explanation format. Recognizing the importance of visualization, we approach the recommender system literature from the angle of explanatory visualizations, that is using visualizations as a display style of explanation. As a result, we derive a set of guidelines that might be constructive for designing explanatory visualizations in recommender systems and identify perspectives for future work in this field. The aim of this review is to help recommendation researchers and practitioners better understand the potential of visually explainable recommendation research and to support them in the systematic design of visual explanations in current and future recommender systems.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W4399528491",
    "type": "article"
  },
  {
    "title": "A Reasoning and Value Alignment Test to Assess Advanced GPT Reasoning",
    "doi": "https://doi.org/10.1145/3670691",
    "publication_date": "2024-06-03",
    "publication_year": 2024,
    "authors": "Timothy R. McIntosh; Tong Liu; Teo Sušnjak; Paul Watters; Malka N. Halgamuge",
    "corresponding_authors": "",
    "abstract": "In response to diverse perspectives on artificial general intelligence (AGI), ranging from potential safety and ethical concerns to more extreme views about the threats it poses to humanity, this research presents a generic method to gauge the reasoning capabilities of artificial intelligence (AI) models as a foundational step in evaluating safety measures. Recognizing that AI reasoning measures cannot be wholly automated, due to factors such as cultural complexity, we conducted an extensive examination of five commercial generative pre-trained transformers (GPTs), focusing on their comprehension and interpretation of culturally intricate contexts. Utilizing our novel “Reasoning and Value Alignment Test,” we assessed the GPT models’ ability to reason in complex situations and grasp local cultural subtleties. Our findings have indicated that, although the models have exhibited high levels of human-like reasoning, significant limitations remained, especially concerning the interpretation of cultural contexts. This article also explored potential applications and use-cases of our Test, underlining its significance in AI training, ethics compliance, sensitivity auditing, and AI-driven cultural consultation. We concluded by emphasizing its broader implications in the AGI domain, highlighting the necessity for interdisciplinary approaches, wider accessibility to various GPT models, and a profound understanding of the interplay between GPT reasoning and cultural sensitivity.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4399285823",
    "type": "article"
  },
  {
    "title": "Phrase detectives",
    "doi": "https://doi.org/10.1145/2448116.2448119",
    "publication_date": "2013-04-01",
    "publication_year": 2013,
    "authors": "Massimo Poesio; Jon Chamberlain; Udo Kruschwitz; Livio Robaldo; Luca Ducceschi",
    "corresponding_authors": "",
    "abstract": "We are witnessing a paradigm shift in Human Language Technology (HLT) that may well have an impact on the field comparable to the statistical revolution: acquiring large-scale resources by exploiting collective intelligence. An illustration of this new approach is Phrase Detectives , an interactive online game with a purpose for creating anaphorically annotated resources that makes use of a highly distributed population of contributors with different levels of expertise. The purpose of this article is to first of all give an overview of all aspects of Phrase Detectives, from the design of the game and the HLT methods we used to the results we have obtained so far. It furthermore summarizes the lessons that we have learned in developing this game which should help other researchers to design and implement similar games.",
    "cited_by_count": 95,
    "openalex_id": "https://openalex.org/W1971218560",
    "type": "article"
  },
  {
    "title": "Say Anything",
    "doi": "https://doi.org/10.1145/2362394.2362398",
    "publication_date": "2012-09-01",
    "publication_year": 2012,
    "authors": "Reid Swanson; Andrew S. Gordon",
    "corresponding_authors": "",
    "abstract": "We describe Say Anything, a new interactive storytelling system that collaboratively writes textual narratives with human users. Unlike previous attempts, this interactive storytelling system places no restrictions on the content or direction of the user’s contribution to the emerging storyline. In response to these contributions, the computer continues the storyline with narration that is both coherent and entertaining. This capacity for open-domain interactive storytelling is enabled by an extremely large repository of nonfiction personal stories, which is used as a knowledge base in a case-based reasoning architecture. In this article, we describe the three main components of our case-based reasoning approach: a million-item corpus of personal stories mined from internet weblogs, a case retrieval strategy that is optimized for narrative coherence, and an adaptation strategy that ensures that repurposed sentences from the case base are appropriate for the user’s emerging fiction. We describe a series of evaluations of the system’s ability to produce coherent and entertaining stories, and we compare these narratives with single-author stories posted to internet weblogs.",
    "cited_by_count": 88,
    "openalex_id": "https://openalex.org/W2128116572",
    "type": "article"
  },
  {
    "title": "Creating New Technologies for Companionable Agents to Support Isolated Older Adults",
    "doi": "https://doi.org/10.1145/3213050",
    "publication_date": "2018-07-24",
    "publication_year": 2018,
    "authors": "Candace L. Sidner; Timothy Bickmore; Bahador Nooraie; Charles Rich; Lazlo Ring; Mahni Shayganfar; Laura Vardoulakis",
    "corresponding_authors": "",
    "abstract": "This article reports on the development of capabilities for (on-screen) virtual agents and robots to support isolated older adults in their homes. A real-time architecture was developed to use a virtual agent or a robot interchangeably to interact via dialog and gesture with a human user. Users could interact with either agent on 12 different activities, some of which included on-screen games, and forms to complete. The article reports on a pre-study that guided the choice of interaction activities. A month-long study with 44 adults between the ages of 55 and 91 assessed differences in the use of the robot and virtual agent.",
    "cited_by_count": 82,
    "openalex_id": "https://openalex.org/W2884126467",
    "type": "article"
  },
  {
    "title": "Adaptive Gesture Recognition with Variation Estimation for Interactive Systems",
    "doi": "https://doi.org/10.1145/2643204",
    "publication_date": "2014-12-19",
    "publication_year": 2014,
    "authors": "Baptiste Caramiaux; Nicola Montecchio; Atau Tanaka; Frédéric Bevilacqua",
    "corresponding_authors": "",
    "abstract": "This article presents a gesture recognition/adaptation system for human--computer interaction applications that goes beyond activity classification and that, as a complement to gesture labeling, characterizes the movement execution. We describe a template-based recognition method that simultaneously aligns the input gesture to the templates using a Sequential Monte Carlo inference technique. Contrary to standard template-based methods based on dynamic programming, such as Dynamic Time Warping, the algorithm has an adaptation process that tracks gesture variation in real time. The method continuously updates, during execution of the gesture, the estimated parameters and recognition results, which offers key advantages for continuous human--machine interaction. The technique is evaluated in several different ways: Recognition and early recognition are evaluated on 2D onscreen pen gestures; adaptation is assessed on synthetic data; and both early recognition and adaptation are evaluated in a user study involving 3D free-space gestures. The method is robust to noise, and successfully adapts to parameter variation. Moreover, it performs recognition as well as or better than nonadapting offline template-based methods.",
    "cited_by_count": 80,
    "openalex_id": "https://openalex.org/W2142639790",
    "type": "article"
  },
  {
    "title": "Detecting Users’ Cognitive Load by Galvanic Skin Response with Affective Interference",
    "doi": "https://doi.org/10.1145/2960413",
    "publication_date": "2017-09-19",
    "publication_year": 2017,
    "authors": "Nargess Nourbakhsh; Fang Chen; Yang Wang; Rafael A. Calvo",
    "corresponding_authors": "",
    "abstract": "Experiencing high cognitive load during complex and demanding tasks results in performance reduction, stress, and errors. However, these could be prevented by a system capable of constantly monitoring users’ cognitive load fluctuations and adjusting its interactions accordingly. Physiological data and behaviors have been found to be suitable measures of cognitive load and are now available in many consumer devices. An advantage of these measures over subjective and performance-based methods is that they are captured in real time and implicitly while the user interacts with the system, which makes them suitable for real-world applications. On the other hand, emotion interference can change physiological responses and make accurate cognitive load measurement more challenging. In this work, we have studied six galvanic skin response (GSR) features in detection of four cognitive load levels with the interference of emotions. The data was derived from two arithmetic experiments and emotions were induced by displaying pleasant and unpleasant pictures in the background. Two types of classifiers were applied to detect cognitive load levels. Results from both studies indicate that the features explored can detect four and two cognitive load levels with high accuracy even under emotional changes. More specifically, rise duration and accumulative GSR are the common best features in all situations, having the highest accuracy especially in the presence of emotions.",
    "cited_by_count": 79,
    "openalex_id": "https://openalex.org/W2754889145",
    "type": "article"
  },
  {
    "title": "Inferring Visualization Task Properties, User Performance, and User Cognitive Abilities from Eye Gaze Data",
    "doi": "https://doi.org/10.1145/2633043",
    "publication_date": "2014-07-01",
    "publication_year": 2014,
    "authors": "Ben Steichen; Cristina Conati; Giuseppe Carenini",
    "corresponding_authors": "",
    "abstract": "Information visualization systems have traditionally followed a one-size-fits-all model, typically ignoring an individual user's needs, abilities, and preferences. However, recent research has indicated that visualization performance could be improved by adapting aspects of the visualization to the individual user. To this end, this article presents research aimed at supporting the design of novel user-adaptive visualization systems. In particular, we discuss results on using information on user eye gaze patterns while interacting with a given visualization to predict properties of the user's visualization task; the user's performance (in terms of predicted task completion time); and the user's individual cognitive abilities, such as perceptual speed, visual working memory, and verbal working memory. We provide a detailed analysis of different eye gaze feature sets, as well as over-time accuracies. We show that these predictions are significantly better than a baseline classifier even during the early stages of visualization usage. These findings are then discussed with a view to designing visualization systems that can adapt to the individual user in real time.",
    "cited_by_count": 78,
    "openalex_id": "https://openalex.org/W2013729144",
    "type": "article"
  },
  {
    "title": "Updatable, Accurate, Diverse, and Scalable Recommendations for Interactive Applications",
    "doi": "https://doi.org/10.1145/2955101",
    "publication_date": "2016-12-19",
    "publication_year": 2016,
    "authors": "Bibek Paudel; Fabian Christoffel; Chris Newell; Abraham Bernstein",
    "corresponding_authors": "",
    "abstract": "Recommender systems form the backbone of many interactive systems. They incorporate user feedback to personalize the user experience typically via personalized recommendation lists. As users interact with a system, an increasing amount of data about a user’s preferences becomes available, which can be leveraged for improving the systems’ performance. Incorporating these new data into the underlying recommendation model is, however, not always straightforward. Many models used by recommender systems are computationally expensive and, therefore, have to perform offline computations to compile the recommendation lists. For interactive applications, it is desirable to be able to update the computed values as soon as new user interaction data is available: updating recommendations in interactive time using new feedback data leads to better accuracy and increases the attraction of the system to the users. Additionally, there is a growing consensus that accuracy alone is not enough and user satisfaction is also dependent on diverse recommendations. In this work, we tackle this problem of updating personalized recommendation lists for interactive applications in order to provide both accurate and diverse recommendations. To that end, we explore algorithms that exploit random walks as a sampling technique to obtain diverse recommendations without compromising on efficiency and accuracy. Specifically, we present a novel graph vertex ranking recommendation algorithm called RP 3 β that reranks items based on three-hop random walk transition probabilities. We show empirically that RP 3 β provides accurate recommendations with high long-tail item frequency at the top of the recommendation list. We also present approximate versions of RP 3 β and the two most accurate previously published vertex ranking algorithms based on random walk transition probabilities and show that these approximations converge with an increasing number of samples. To obtain interactively updatable recommendations, we additionally show how our algorithm can be extended for online updates at interactive speeds. The underlying random walk sampling technique makes it possible to perform the updates without having to recompute the values for the entire dataset. In an empirical evaluation with three real-world datasets, we show that RP 3 β provides highly accurate and diverse recommendations that can easily be updated with newly gathered information at interactive speeds (≪ 100 ms ).",
    "cited_by_count": 76,
    "openalex_id": "https://openalex.org/W1587374567",
    "type": "article"
  },
  {
    "title": "A Gaze-Contingent Adaptive Virtual Reality Driving Environment for Intervention in Individuals with Autism Spectrum Disorders",
    "doi": "https://doi.org/10.1145/2892636",
    "publication_date": "2016-03-17",
    "publication_year": 2016,
    "authors": "Joshua Wade; Lian Zhang; Dayi Bian; Jing Fan; Amy Swanson; Amy Weitlauf; Medha Sarkar; Zachary Warren; Nilanjan Sarkar",
    "corresponding_authors": "",
    "abstract": "In addition to social and behavioral deficits, individuals with Autism Spectrum Disorder (ASD) often struggle to develop the adaptive skills necessary to achieve independence. Driving intervention in individuals with ASD is a growing area of study, but it is still widely under-researched. We present the development and preliminary assessment of a gaze-contingent adaptive virtual reality driving simulator that uses real-time gaze information to adapt the driving environment with the aim of providing a more individualized method of driving intervention. We conducted a small pilot study of 20 adolescents with ASD using our system: 10 with the adaptive gaze-contingent version of the system and 10 in a purely performance-based version. Preliminary results suggest that the novel intervention system may be beneficial in teaching driving skills to individuals with ASD.",
    "cited_by_count": 71,
    "openalex_id": "https://openalex.org/W2310698163",
    "type": "article"
  },
  {
    "title": "Artificial Intelligence for Modeling Complex Systems: Taming the Complexity of Expert Models to Improve Decision Making",
    "doi": "https://doi.org/10.1145/3453172",
    "publication_date": "2021-06-30",
    "publication_year": 2021,
    "authors": "Yolanda Gil; Daniel Garijo; Deborah Khider; Craig A. Knoblock; Varun Ratnakar; Maximiliano Osorio; Hernán Vargas; Tam Minh Pham; Jay Pujara; Basel Shbita; Bình Dương Vũ; Yao‐Yi Chiang; Dan Feldman; Yijun Lin; Hae Jin Song; Vipin Kumar; Ankush Khandelwal; Michael Steinbach; Kshitij Tayal; Shaoming Xu; Suzanne A. Pierce; Lissa Pearson; Daniel Hardesty-Lewis; Ewa Deelman; Rafael Ferreira da Silva; Rajiv Mayani; Armen R. Kemanian; Yuning Shi; Lorne Leonard; S. D. Peckham; Maria Stoica; Kelly M. Cobourn; Zeya Zhang; Christopher Duffy; Lele Shu",
    "corresponding_authors": "",
    "abstract": "Major societal and environmental challenges involve complex systems that have diverse multi-scale interacting processes. Consider, for example, how droughts and water reserves affect crop production and how agriculture and industrial needs affect water quality and availability. Preventive measures, such as delaying planting dates and adopting new agricultural practices in response to changing weather patterns, can reduce the damage caused by natural processes. Understanding how these natural and human processes affect one another allows forecasting the effects of undesirable situations and study interventions to take preventive measures. For many of these processes, there are expert models that incorporate state-of-the-art theories and knowledge to quantify a system's response to a diversity of conditions. A major challenge for efficient modeling is the diversity of modeling approaches across disciplines and the wide variety of data sources available only in formats that require complex conversions. Using expert models for particular problems requires integration of models with third-party data as well as integration of models across disciplines. Modelers face significant heterogeneity that requires resolving semantic, spatiotemporal, and execution mismatches, which are largely done by hand today and may take more than 2 years of effort. We are developing a modeling framework that uses artificial intelligence (AI) techniques to reduce modeling effort while ensuring utility for decision making. Our work to date makes several innovative contributions: (1) an intelligent user interface that guides analysts to frame their modeling problem and assists them by suggesting relevant choices and automating steps along the way; (2) semantic metadata for models, including their modeling variables and constraints, that ensures model relevance and proper use for a given decision-making problem; and (3) semantic representations of datasets in terms of modeling variables that enable automated data selection and data transformations. This framework is implemented in the MINT (Model INTegration) framework, and currently includes data and models to analyze the interactions between natural and human systems involving climate, water availability, agricultural production, and markets. Our work to date demonstrates the utility of AI techniques to accelerate modeling to support decision-making and uncovers several challenging directions for future work.",
    "cited_by_count": 58,
    "openalex_id": "https://openalex.org/W3183787761",
    "type": "article"
  },
  {
    "title": "Mental Models of Mere Mortals with Explanations of Reinforcement Learning",
    "doi": "https://doi.org/10.1145/3366485",
    "publication_date": "2020-05-30",
    "publication_year": 2020,
    "authors": "A. W. Anderson; Jonathan Dodge; Amrita Sadarangani; Zoe Juozapaitis; Evan Newman; Jed Irvine; Souti Chattopadhyay; Matthew Olson; Alan Fern; Margaret Burnett",
    "corresponding_authors": "",
    "abstract": "How should reinforcement learning (RL) agents explain themselves to humans not trained in AI? To gain insights into this question, we conducted a 124-participant, four-treatment experiment to compare participants’ mental models of an RL agent in the context of a simple Real-Time Strategy (RTS) game. The four treatments isolated two types of explanations vs. neither vs. both together. The two types of explanations were as follows: (1) saliency maps (an “Input Intelligibility Type” that explains the AI’s focus of attention) and (2) reward-decomposition bars (an “Output Intelligibility Type” that explains the AI’s predictions of future types of rewards). Our results show that a combined explanation that included saliency and reward bars was needed to achieve a statistically significant difference in participants’ mental model scores over the no-explanation treatment. However, this combined explanation was far from a panacea: It exacted disproportionately high cognitive loads from the participants who received the combined explanation. Further, in some situations, participants who saw both explanations predicted the agent’s next action worse than all other treatments’ participants.",
    "cited_by_count": 56,
    "openalex_id": "https://openalex.org/W2990540492",
    "type": "article"
  },
  {
    "title": "Effects of Explanations in AI-Assisted Decision Making: Principles and Comparisons",
    "doi": "https://doi.org/10.1145/3519266",
    "publication_date": "2022-04-29",
    "publication_year": 2022,
    "authors": "Xinru Wang; Ming Yin",
    "corresponding_authors": "",
    "abstract": "Recent years have witnessed the growing literature in empirical evaluation of explainable AI (XAI) methods. This study contributes to this ongoing conversation by presenting a comparison on the effects of a set of established XAI methods in AI-assisted decision making. Based on our review of previous literature, we highlight three desirable properties that ideal AI explanations should satisfy — improve people’s understanding of the AI model, help people recognize the model uncertainty, and support people’s calibrated trust in the model. Through three randomized controlled experiments, we evaluate whether four types of common model-agnostic explainable AI methods satisfy these properties on two types of AI models of varying levels of complexity, and in two kinds of decision making contexts where people perceive themselves as having different levels of domain expertise. Our results demonstrate that many AI explanations do not satisfy any of the desirable properties when used on decision making tasks that people have little domain expertise in. On decision making tasks that people are more knowledgeable, the feature contribution explanation is shown to satisfy more desiderata of AI explanations, even when the AI model is inherently complex. We conclude by discussing the implications of our study for improving the design of XAI methods to better support human decision making, and for advancing more rigorous empirical evaluation of XAI methods.",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W4225117847",
    "type": "article"
  },
  {
    "title": "Directive Explanations for Actionable Explainability in Machine Learning Applications",
    "doi": "https://doi.org/10.1145/3579363",
    "publication_date": "2023-01-12",
    "publication_year": 2023,
    "authors": "Ronal Singh; Tim Miller; Henrietta Lyons; Liz Sonenberg; Eduardo Velloso; Frank Vetere; Piers D. L. Howe; Paul Dourish",
    "corresponding_authors": "",
    "abstract": "In this article, we show that explanations of decisions made by machine learning systems can be improved by not only explaining why a decision was made but also explaining how an individual could obtain their desired outcome. We formally define the concept of directive explanations (those that offer specific actions an individual could take to achieve their desired outcome), introduce two forms of directive explanations (directive-specific and directive-generic), and describe how these can be generated computationally. We investigate people’s preference for and perception toward directive explanations through two online studies, one quantitative and the other qualitative, each covering two domains (the credit scoring domain and the employee satisfaction domain). We find a significant preference for both forms of directive explanations compared to non-directive counterfactual explanations. However, we also find that preferences are affected by many aspects, including individual preferences and social factors. We conclude that deciding what type of explanation to provide requires information about the recipients and other contextual information. This reinforces the need for a human-centered and context-specific approach to explainable AI.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W3128490559",
    "type": "article"
  },
  {
    "title": "“It would work for me too”: How Online Communities Shape Software Developers’ Trust in AI-Powered Code Generation Tools",
    "doi": "https://doi.org/10.1145/3651990",
    "publication_date": "2024-03-09",
    "publication_year": 2024,
    "authors": "Ruijia Cheng; Ruotong Wang; Thomas Zimmermann; Denae Ford",
    "corresponding_authors": "",
    "abstract": "While revolutionary AI-powered code generation tools have been rising rapidly, we know little about how and how to help software developers form appropriate trust in those AI tools. Through a two-phase formative study, we investigate how online communities shape developers’ trust in AI tools and how we can leverage community features to facilitate appropriate user trust. Through interviewing 17 developers, we find that developers collectively make sense of AI tools using the experiences shared by community members and leverage community signals to evaluate AI suggestions. We then surface design opportunities and conduct 11 design probe sessions to explore the design space of using community features to support user trust in AI code generation systems. We synthesize our findings and extend an existing model of user trust in AI technologies with sociotechnical factors. We map out the design considerations for integrating user community into the AI code generation experience.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W4392623181",
    "type": "article"
  },
  {
    "title": "Emotional body language displayed by artificial agents",
    "doi": "https://doi.org/10.1145/2133366.2133368",
    "publication_date": "2012-03-01",
    "publication_year": 2012,
    "authors": "Aryel Beck; Brett Stevens; Kim A. Bard; Lola Cañamero",
    "corresponding_authors": "",
    "abstract": "Complex and natural social interaction between artificial agents (computer-generated or robotic) and humans necessitates the display of rich emotions in order to be believable, socially relevant, and accepted, and to generate the natural emotional responses that humans show in the context of social interaction, such as engagement or empathy. Whereas some robots use faces to display (simplified) emotional expressions, for other robots such as Nao, body language is the best medium available given their inability to convey facial expressions. Displaying emotional body language that can be interpreted whilst interacting with the robot should significantly improve naturalness. This research investigates the creation of an affect space for the generation of emotional body language to be displayed by humanoid robots. To do so, three experiments investigating how emotional body language displayed by agents is interpreted were conducted. The first experiment compared the interpretation of emotional body language displayed by humans and agents. The results showed that emotional body language displayed by an agent or a human is interpreted in a similar way in terms of recognition. Following these results, emotional key poses were extracted from an actor's performances and implemented in a Nao robot. The interpretation of these key poses was validated in a second study where it was found that participants were better than chance at interpreting the key poses displayed. Finally, an affect space was generated by blending key poses and validated in a third study. Overall, these experiments confirmed that body language is an appropriate medium for robots to display emotions and suggest that an affect space for body expressions can be used to improve the expressiveness of humanoid robots.",
    "cited_by_count": 75,
    "openalex_id": "https://openalex.org/W1977948674",
    "type": "article"
  },
  {
    "title": "Taming Mona Lisa",
    "doi": "https://doi.org/10.1145/2070719.2070724",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Samer Al Moubayed; Jens Edlund; Jonas Beskow",
    "corresponding_authors": "",
    "abstract": "The perception of gaze plays a crucial role in human-human interaction. Gaze has been shown to matter for a number of aspects of communication and dialogue, especially for managing the flow of the dialogue and participant attention, for deictic referencing, and for the communication of attitude. When developing embodied conversational agents (ECAs) and talking heads, modeling and delivering accurate gaze targets is crucial. Traditionally, systems communicating through talking heads have been displayed to the human conversant using 2D displays, such as flat monitors. This approach introduces severe limitations for an accurate communication of gaze since 2D displays are associated with several powerful effects and illusions, most importantly the Mona Lisa gaze effect, where the gaze of the projected head appears to follow the observer regardless of viewing angle. We describe the Mona Lisa gaze effect and its consequences in the interaction loop, and propose a new approach for displaying talking heads using a 3D projection surface (a physical model of a human head) as an alternative to the traditional flat surface projection. We investigate and compare the accuracy of the perception of gaze direction and the Mona Lisa gaze effect in 2D and 3D projection surfaces in a five subject gaze perception experiment. The experiment confirms that a 3D projection surface completely eliminates the Mona Lisa gaze effect and delivers very accurate gaze direction that is independent of the observer's viewing angle. Based on the data collected in this experiment, we rephrase the formulation of the Mona Lisa gaze effect. The data, when reinterpreted, confirms the predictions of the new model for both 2D and 3D projection surfaces. Finally, we discuss the requirements on different spatially interactive systems in terms of gaze direction, and propose new applications and experiments for interaction in a human-ECA and a human-robot settings made possible by this technology.",
    "cited_by_count": 74,
    "openalex_id": "https://openalex.org/W2093364769",
    "type": "article"
  },
  {
    "title": "Why-oriented end-user debugging of naive Bayes text classification",
    "doi": "https://doi.org/10.1145/2030365.2030367",
    "publication_date": "2011-10-01",
    "publication_year": 2011,
    "authors": "Todd Kulesza; Simone Stumpf; Weng‐Keen Wong; Margaret Burnett; Stephen Perona; Amy J. Ko; Ian Oberst",
    "corresponding_authors": "",
    "abstract": "Machine learning techniques are increasingly used in intelligent assistants , that is, software targeted at and continuously adapting to assist end users with email, shopping, and other tasks. Examples include desktop SPAM filters, recommender systems, and handwriting recognition. Fixing such intelligent assistants when they learn incorrect behavior, however, has received only limited attention. To directly support end-user “debugging” of assistant behaviors learned via statistical machine learning, we present a Why-oriented approach which allows users to ask questions about how the assistant made its predictions, provides answers to these “why” questions, and allows users to interactively change these answers to debug the assistant's current and future predictions. To understand the strengths and weaknesses of this approach, we then conducted an exploratory study to investigate barriers that participants could encounter when debugging an intelligent assistant using our approach, and the information those participants requested to overcome these barriers. To help ensure the inclusiveness of our approach, we also explored how gender differences played a role in understanding barriers and information needs. We then used these results to consider opportunities for Why-oriented approaches to address user barriers and information needs.",
    "cited_by_count": 73,
    "openalex_id": "https://openalex.org/W2112378171",
    "type": "article"
  },
  {
    "title": "Creating personalized systems that people can scrutinize and control",
    "doi": "https://doi.org/10.1145/2395123.2395129",
    "publication_date": "2012-12-01",
    "publication_year": 2012,
    "authors": "Judy Kay; Bob Kummerfeld",
    "corresponding_authors": "",
    "abstract": "Widespread personalized computing systems play an already important and fast-growing role in diverse contexts, such as location-based services, recommenders, commercial Web-based services, and teaching systems. The personalization in these systems is driven by information about the user, a user model . Moreover, as computers become both ubiquitous and pervasive, personalization operates across the many devices and information stores that constitute the user's personal digital ecosystem. This enables personalization, and the user models driving it, to play an increasing role in people's everyday lives. This makes it critical to establish ways to address key problems of personalization related to privacy , invisibility of personalization, errors in user models , wasted user models , and the broad issue of enabling people to control their user models and associated personalization. We offer scrutable user models as a foundation for tackling these problems. This article argues the importance of scrutable user modeling and personalization, illustrating key elements in case studies from our work. We then identify the broad roles for scrutable user models. The article describes how to tackle the technical and interface challenges of designing and building scrutable user modeling systems, presenting design principles and showing how they were established over our twenty years of work on the Personis software framework. Our contributions are the set of principles for scrutable personalization linked to our experience from creating and evaluating frameworks and associated applications built upon them. These constitute a general approach to tackling problems of personalization by enabling users to scrutinize their user models as a basis for understanding and controlling personalization.",
    "cited_by_count": 66,
    "openalex_id": "https://openalex.org/W1992446872",
    "type": "article"
  },
  {
    "title": "The <i>SignCom</i> system for data-driven animation of interactive virtual signers",
    "doi": "https://doi.org/10.1145/2030365.2030371",
    "publication_date": "2011-10-01",
    "publication_year": 2011,
    "authors": "Sylvie Gibet; Nicolas Courty; Kyle Duarte; Thibaut Le Naour",
    "corresponding_authors": "",
    "abstract": "In this article we present a multichannel animation system for producing utterances signed in French Sign Language (LSF) by a virtual character. The main challenges of such a system are simultaneously capturing data for the entire body, including the movements of the torso, hands, and face, and developing a data-driven animation engine that takes into account the expressive characteristics of signed languages. Our approach consists of decomposing motion along different channels, representing the body parts that correspond to the linguistic components of signed languages. We show the ability of this animation system to create novel utterances in LSF, and present an evaluation by target users which highlights the importance of the respective body parts in the production of signs. We validate our framework by testing the believability and intelligibility of our virtual signer.",
    "cited_by_count": 63,
    "openalex_id": "https://openalex.org/W2077367280",
    "type": "article"
  },
  {
    "title": "Inferring Capabilities of Intelligent Agents from Their External Traits",
    "doi": "https://doi.org/10.1145/2963106",
    "publication_date": "2016-11-19",
    "publication_year": 2016,
    "authors": "Bart P. Knijnenburg; Martijn C. Willemsen",
    "corresponding_authors": "",
    "abstract": "We investigate the usability of humanlike agent-based interfaces for interactive advice-giving systems. In an experiment with a travel advisory system, we manipulate the “humanlikeness” of the agent interface. We demonstrate that users of the more humanlike agents try to exploit capabilities that were not signaled by the system. This severely reduces the usability of systems that look human but lack humanlikehumanlike capabilities (overestimation effect). We explain this effect by showing that users of humanlike agents form anthropomorphic beliefs (a user's “mental model”) about the system: They act humanlike towards the system and try to exploit typical humanlike capabilities they believe the system possesses. Furthermore, we demonstrate that the mental model users form of an agent-based system is inherently integrated (as opposed to the compositional mental model they form of conventional interfaces): Cues provided by the system do not instill user responses in a one-to-one matter but are instead integrated into a single mental model.",
    "cited_by_count": 61,
    "openalex_id": "https://openalex.org/W2551095554",
    "type": "article"
  },
  {
    "title": "USMART",
    "doi": "https://doi.org/10.1145/2662870",
    "publication_date": "2014-11-13",
    "publication_year": 2014,
    "authors": "Juan Ye; Graeme Stevenson; Simon Dobson",
    "corresponding_authors": "",
    "abstract": "Recognising high-level human activities from low-level sensor data is a crucial driver for pervasive systems that wish to provide seamless and distraction-free support for users engaged in normal activities. Research in this area has grown alongside advances in sensing and communications, and experiments have yielded sensor traces coupled with ground truth annotations about the underlying environmental conditions and user actions. Traditional machine learning has had some success in recognising human activities; but the need for large volumes of annotated data and the danger of overfitting to specific conditions represent challenges in connection with the building of models applicable to a wide range of users, activities, and environments. We present USMART, a novel unsupervised technique that combines data- and knowledge-driven techniques. USMART uses a general ontology model to represent domain knowledge that can be reused across different environments and users, and we augment a range of learning techniques with ontological semantics to facilitate the unsupervised discovery of patterns in how each user performs daily activities. We evaluate our approach against four real-world third-party datasets featuring different user populations and sensor configurations, and we find that USMART achieves up to 97.5% accuracy in recognising daily activities.",
    "cited_by_count": 60,
    "openalex_id": "https://openalex.org/W2021794253",
    "type": "article"
  },
  {
    "title": "Gaze and Attention Management for Embodied Conversational Agents",
    "doi": "https://doi.org/10.1145/2724731",
    "publication_date": "2015-03-25",
    "publication_year": 2015,
    "authors": "Tomislav Pejša; Sean Andrist; Michael Gleicher; Bilge Mutlu",
    "corresponding_authors": "",
    "abstract": "To facilitate natural interactions between humans and embodied conversational agents (ECAs), we need to endow the latter with the same nonverbal cues that humans use to communicate. Gaze cues in particular are integral in mechanisms for communication and management of attention in social interactions, which can trigger important social and cognitive processes, such as establishment of affiliation between people or learning new information. The fundamental building blocks of gaze behaviors are gaze shifts : coordinated movements of the eyes, head, and body toward objects and information in the environment. In this article, we present a novel computational model for gaze shift synthesis for ECAs that supports parametric control over coordinated eye, head, and upper body movements. We employed the model in three studies with human participants. In the first study, we validated the model by showing that participants are able to interpret the agent’s gaze direction accurately. In the second and third studies, we showed that by adjusting the participation of the head and upper body in gaze shifts, we can control the strength of the attention signals conveyed, thereby strengthening or weakening their social and cognitive effects. The second study shows that manipulation of eye--head coordination in gaze enables an agent to convey more information or establish stronger affiliation with participants in a teaching task, while the third study demonstrates how manipulation of upper body coordination enables the agent to communicate increased interest in objects in the environment.",
    "cited_by_count": 58,
    "openalex_id": "https://openalex.org/W2012804445",
    "type": "article"
  },
  {
    "title": "Providing Arguments in Discussions on the Basis of the Prediction of Human Argumentative Behavior",
    "doi": "https://doi.org/10.1145/2983925",
    "publication_date": "2016-12-09",
    "publication_year": 2016,
    "authors": "Ariel Rosenfeld; Sarit Kraus",
    "corresponding_authors": "",
    "abstract": "Argumentative discussion is a highly demanding task. In order to help people in such discussions, this article provides an innovative methodology for developing agents that can support people in argumentative discussions by proposing possible arguments. By gathering and analyzing human argumentative behavior from more than 1000 human study participants, we show that the prediction of human argumentative behavior using Machine Learning (ML) is possible and useful in designing argument provision agents. This paper first demonstrates that ML techniques can achieve up to 76% accuracy when predicting people’s top three argument choices given a partial discussion. We further show that well-established Argumentation Theory is not a good predictor of people’s choice of arguments. Then, we present 9 argument provision agents, which we empirically evaluate using hundreds of human study participants. We show that the Predictive and Relevance-Based Heuristic agent (PRH), which uses ML prediction with a heuristic that estimates the relevance of possible arguments to the current state of the discussion, results in significantly higher levels of satisfaction among study participants compared with the other evaluated agents. These other agents propose arguments based on Argumentation Theory; propose predicted arguments without the heuristics or with only the heuristics; or use Transfer Learning methods. Our findings also show that people use the PRH agents proposed arguments significantly more often than those proposed by the other agents.",
    "cited_by_count": 56,
    "openalex_id": "https://openalex.org/W2559772828",
    "type": "article"
  },
  {
    "title": "A Wearable Assistant for Gait Training for Parkinson’s Disease with Freezing of Gait in Out-of-the-Lab Environments",
    "doi": "https://doi.org/10.1145/2701431",
    "publication_date": "2015-03-09",
    "publication_year": 2015,
    "authors": "Sînziana Mazilu; Ulf Blanke; Moran Dorfman; Eran Gazit; Anat Mirelman; Jeffrey M. Hausdorff; Gerhard Tröster",
    "corresponding_authors": "",
    "abstract": "People with Parkinson’s disease (PD) suffer from declining mobility capabilities, which cause a prevalent risk of falling. Commonly, short periods of motor blocks occur during walking, known as freezing of gait (FoG). To slow the progressive decline of motor abilities, people with PD usually undertake stationary motor-training exercises in the clinics or supervised by physiotherapists. We present a wearable system for the support of people with PD and FoG. The system is designed for independent use. It enables motor training and gait assistance at home and other unsupervised environments. The system consists of three components. First, FoG episodes are detected in real time using wearable inertial sensors and a smartphone as the processing unit. Second, a feedback mechanism triggers a rhythmic auditory signal to the user to alleviate freeze episodes in an assistive mode. Third, the smartphone-based application features support for training exercises. Moreover, the system allows unobtrusive and long-term monitoring of the user’s clinical condition by transmitting sensing data and statistics to a telemedicine service. We investigate the at-home acceptance of the wearable system in a study with nine PD subjects. Participants deployed and used the system on their own, without any clinical support, at their homes during three protocol sessions in 1 week. Users’ feedback suggests an overall positive attitude toward adopting and using the system in their daily life, indicating that the system supports them in improving their gait. Further, in a data-driven analysis with sensing data from five participants, we study whether there is an observable effect on the gait during use of the system. In three out of five subjects, we observed a decrease in FoG duration distributions over the protocol days during gait-training exercises. Moreover, sensing data-driven analysis shows a decrease in FoG duration and FoG number in four out of five participants when they use the system as a gait-assistive tool during normal daily life activities at home.",
    "cited_by_count": 55,
    "openalex_id": "https://openalex.org/W1996518303",
    "type": "article"
  },
  {
    "title": "Emotional States Associated with Music",
    "doi": "https://doi.org/10.1145/2723575",
    "publication_date": "2015-03-25",
    "publication_year": 2015,
    "authors": "James J. Deng; Clement Leung; Alfredo Milani; Li Chen",
    "corresponding_authors": "",
    "abstract": "We present several interrelated technical and empirical contributions to the problem of emotion-based music recommendation and show how they can be applied in a possible usage scenario. The contributions are (1) a new three-dimensional resonance-arousal-valence model for the representation of emotion expressed in music, together with methods for automatically classifying a piece of music in terms of this model, using robust regression methods applied to musical/acoustic features; (2) methods for predicting a listener’s emotional state on the assumption that the emotional state has been determined entirely by a sequence of pieces of music recently listened to, using conditional random fields and taking into account the decay of emotion intensity over time; and (3) a method for selecting a ranked list of pieces of music that match a particular emotional state, using a minimization iteration method. A series of experiments yield information about the validity of our operationalizations of these contributions. Throughout the article, we refer to an illustrative usage scenario in which all of these contributions can be exploited, where it is assumed that (1) a listener’s emotional state is being determined entirely by the music that he or she has been listening to and (2) the listener wants to hear additional music that matches his or her current emotional state. The contributions are intended to be useful in a variety of other scenarios as well.",
    "cited_by_count": 55,
    "openalex_id": "https://openalex.org/W2005152349",
    "type": "article"
  },
  {
    "title": "Modeling the Human-Robot Trust Phenomenon",
    "doi": "https://doi.org/10.1145/3152890",
    "publication_date": "2018-11-16",
    "publication_year": 2018,
    "authors": "Alan R. Wagner; Paul Robinette; Ayanna Howard",
    "corresponding_authors": "",
    "abstract": "This article presents a conceptual framework for human-robot trust which uses computational representations inspired by game theory to represent a definition of trust, derived from social psychology. This conceptual framework generates several testable hypotheses related to human-robot trust. This article examines these hypotheses and a series of experiments we have conducted which both provide support for and also conflict with our framework for trust. We also discuss the methodological challenges associated with investigating trust. The article concludes with a description of the important areas for future research on the topic of human-robot trust.",
    "cited_by_count": 50,
    "openalex_id": "https://openalex.org/W2901499449",
    "type": "article"
  },
  {
    "title": "Progressive Disclosure",
    "doi": "https://doi.org/10.1145/3374218",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "Aaron Springer; Steve Whittaker",
    "corresponding_authors": "",
    "abstract": "It is essential that users understand how algorithmic decisions are made, as we increasingly delegate important decisions to intelligent systems. Prior work has often taken a techno-centric approach, focusing on new computational techniques to support transparency. In contrast, this article employs empirical methods to better understand user reactions to transparent systems to motivate user-centric designs for transparent systems. We assess user reactions to transparency feedback in four studies of an emotional analytics system. In Study 1, users anticipated that a transparent system would perform better but unexpectedly retracted this evaluation after experience with the system. Study 2 offers an explanation for this paradox by showing that the benefits of transparency are context dependent. On the one hand, transparency can help users form a model of the underlying algorithm's operation. On the other hand, positive accuracy perceptions may be undermined when transparency reveals algorithmic errors. Study 3 explored real-time reactions to transparency. Results confirmed Study 2, in showing that users are both more likely to consult transparency information and to experience greater system insights when formulating a model of system operation. Study 4 used qualitative methods to explore real-time user reactions to motivate transparency design principles. Results again suggest that users may benefit from initially simplified feedback that hides potential system errors and assists users in building working heuristics about system operation. We use these findings to motivate new progressive disclosure principles for transparency in intelligent systems and discuss theoretical implications.",
    "cited_by_count": 48,
    "openalex_id": "https://openalex.org/W3042124235",
    "type": "article"
  },
  {
    "title": "Integrity-based Explanations for Fostering Appropriate Trust in AI Agents",
    "doi": "https://doi.org/10.1145/3610578",
    "publication_date": "2023-07-24",
    "publication_year": 2023,
    "authors": "Siddharth Mehrotra; Carolina Centeio Jorge; Catholijn M. Jonker; Myrthe L. Tielman",
    "corresponding_authors": "",
    "abstract": "Appropriate trust is an important component of the interaction between people and AI systems, in that “inappropriate” trust can cause disuse, misuse, or abuse of AI. To foster appropriate trust in AI, we need to understand how AI systems can elicit appropriate levels of trust from their users. Out of the aspects that influence trust, this article focuses on the effect of showing integrity. In particular, this article presents a study of how different integrity-based explanations made by an AI agent affect the appropriateness of trust of a human in that agent. To explore this, (1) we provide a formal definition to measure appropriate trust, (2) present a between-subject user study with 160 participants who collaborated with an AI agent in such a task. In the study, the AI agent assisted its human partner in estimating calories on a food plate by expressing its integrity through explanations focusing on either honesty, transparency, or fairness. Our results show that (a) an agent who displays its integrity by being explicit about potential biases in data or algorithms achieved appropriate trust more often compared to being honest about capability or transparent about the decision-making process, and (b) subjective trust builds up and recovers better with honesty-like integrity explanations. Our results contribute to the design of agent-based AI systems that guide humans to appropriately trust them, a formal method to measure appropriate trust, and how to support humans in calibrating their trust in AI.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W4385216343",
    "type": "article"
  },
  {
    "title": "Explaining Recommendations through Conversations: Dialog Model and the Effects of Interface Type and Degree of Interactivity",
    "doi": "https://doi.org/10.1145/3579541",
    "publication_date": "2023-01-21",
    "publication_year": 2023,
    "authors": "Diana C. Hernandez-Bocanegra; Jürgen Ziegler",
    "corresponding_authors": "",
    "abstract": "Explaining system-generated recommendations based on user reviews can foster users’ understanding and assessment of the recommended items and the recommender system (RS) as a whole. While up to now explanations have mostly been static, shown in a single presentation unit, some interactive explanatory approaches have emerged in explainable artificial intelligence (XAI), making it easier for users to examine system decisions and to explore arguments according to their information needs. However, little is known about how interactive interfaces should be conceptualized and designed to meet the explanatory aims of transparency, effectiveness, and trust in RS. Thus, we investigate the potential of interactive, conversational explanations in review-based RS and propose an explanation approach inspired by dialog models and formal argument structures. In particular, we investigate users’ perception of two different interface types for presenting explanations, a graphical user interface (GUI)-based dialog consisting of a sequence of explanatory steps, and a chatbot-like natural-language interface. Since providing explanations by means of natural language conversation is a novel approach, there is a lack of understanding how users would formulate their questions with a corresponding lack of datasets. We thus propose an intent model for explanatory queries and describe the development of ConvEx-DS, a dataset containing intent annotations of 1,806 user questions in the domain of hotels, that can be used to to train intent detection methods as part of the development of conversational agents for explainable RS. We validate the model by measuring user-perceived helpfulness of answers given based on the implemented intent detection. Finally, we report on a user study investigating users’ evaluation of the two types of interactive explanations proposed (GUI and chatbot), and to test the effect of varying degrees of interactivity that result in greater or lesser access to explanatory information. By using Structural Equation Modeling, we reveal details on the relationships between the perceived quality of an explanation and the explanatory objectives of transparency, trust, and effectiveness. Our results show that providing interactive options for scrutinizing explanatory arguments has a significant positive influence on the evaluation by users (compared to low interactive alternatives). Results also suggest that user characteristics such as decision-making style may have a significant influence on the evaluation of different types of interactive explanation interfaces.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W4317659197",
    "type": "article"
  },
  {
    "title": "Effects of AI and Logic-Style Explanations on Users’ Decisions Under Different Levels of Uncertainty",
    "doi": "https://doi.org/10.1145/3588320",
    "publication_date": "2023-03-16",
    "publication_year": 2023,
    "authors": "Federico Maria Cau; Hanna Hauptmann; Lucio Davide Spano; Nava Tintarev",
    "corresponding_authors": "",
    "abstract": "Existing eXplainable Artificial Intelligence (XAI) techniques support people in interpreting AI advice. However, although previous work evaluates the users’ understanding of explanations, factors influencing the decision support are largely overlooked in the literature. This article addresses this gap by studying the impact of user uncertainty , AI correctness , and the interaction between AI uncertainty and explanation logic-styles for classification tasks. We conducted two separate studies: one requesting participants to recognize handwritten digits and one to classify the sentiment of reviews. To assess the decision making, we analyzed the task performance, agreement with the AI suggestion, and the user’s reliance on the XAI interface elements. Participants make their decision relying on three pieces of information in the XAI interface (image or text instance, AI prediction, and explanation). Participants were shown one explanation style (between-participants design) according to three styles of logical reasoning (inductive, deductive, and abductive). This allowed us to study how different levels of AI uncertainty influence the effectiveness of different explanation styles. The results show that user uncertainty and AI correctness on predictions significantly affected users’ classification decisions considering the analyzed metrics. In both domains (images and text), users relied mainly on the instance to decide. Users were usually overconfident about their choices, and this evidence was more pronounced for text. Furthermore, the inductive style explanations led to overreliance on the AI advice in both domains—it was the most persuasive, even when the AI was incorrect. The abductive and deductive styles have complex effects depending on the domain and the AI uncertainty levels.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W4327591912",
    "type": "article"
  },
  {
    "title": "Understanding Trust and Reliance Development in AI Advice: Assessing Model Accuracy, Model Explanations, and Experiences from Previous Interactions.",
    "doi": "https://doi.org/10.1145/3686164",
    "publication_date": "2024-08-02",
    "publication_year": 2024,
    "authors": "Patricia Kahr; Gerrit Rooks; Martijn C. Willemsen; Chris Snijders",
    "corresponding_authors": "",
    "abstract": "People are increasingly interacting with AI systems, but successful interactions depend on people trusting these systems only when appropriate. Since neither gaining trust in AI advice nor restoring lost trust after AI mistakes is warranted, we seek to better understand the development of trust and reliance in sequential human-AI interaction scenarios. In a 2x2 between-subject simulated AI experiment, we tested how model accuracy (high vs. low) and explanation type (human-like vs. abstract) affect trust and reliance on AI advice for repeated interactions. In the experiment, participants estimated jail times for 20 criminal law cases, first without and then with AI advice. Our results show that trust and reliance are significantly higher for high model accuracy. In addition, reliance does not decline over the trial sequence, and trust increases significantly with high accuracy. Human-like (vs. abstract) explanations only increased reliance on the high-accuracy condition. We furthermore tested the extent to which trust and reliance in a trial round can be explained by trust and reliance experiences from prior rounds. We find that trust assessments in prior trials correlate with trust in subsequent ones. We also find that the cumulative trust experience of a person in all earlier trial rounds correlates with trust in subsequent ones. Furthermore, we find that the two trust measures, trust and reliance, impact each other: prior trust beliefs not only influence subsequent trust beliefs but likewise influence subsequent reliance behavior, and vice versa. Executing a replication study yielded comparable results to our original study, thereby enhancing the validity of our findings.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4401248771",
    "type": "article"
  },
  {
    "title": "People, sensors, decisions",
    "doi": "https://doi.org/10.1145/2395123.2395125",
    "publication_date": "2012-12-01",
    "publication_year": 2012,
    "authors": "Jesse Hoey; Craig Boutilier; Pascal Poupart; Patrick Olivier; Andrew Monk; Alex Mihailidis",
    "corresponding_authors": "",
    "abstract": "The ratio of healthcare professionals to care recipients is dropping at an alarming rate, particularly for the older population. It is estimated that the number of persons with Alzheimer's disease, for example, will top 100 million worldwide by the year 2050 [Alzheimer's Disease International 2009]. It will become harder and harder to provide needed health services to this population of older adults. Further, patients are becoming more aware and involved in their own healthcare decisions. This is creating a void in which technology has an increasingly important role to play as a tool to connect providers with recipients. Examples of interactive technologies range from telecare for remote regions to computer games promoting fitness in the home. Currently, such technologies are developed for specific applications and are difficult to modify to suit individual user needs. The future potential economic and social impact of technology in the healthcare field therefore lies in our ability to make intelligent devices that are customizable by healthcare professionals and their clients, that are adaptive to users over time, and that generalize across tasks and environments. A wide application area for technology in healthcare is for assistance and monitoring in the home. As the population ages, it becomes increasingly dependent on chronic healthcare, such as assistance for tasks of everyday life (washing, cooking, dressing), medication taking, nutrition, and fitness. This article will present a summary of work over the past decade on the development of intelligent systems that provide assistance to persons with cognitive disabilities. These systems are unique in that they are all built using a common framework, a decision-theoretic model for general-purpose assistance in the home. In this article, we will show how this type of general model can be applied to a range of assistance tasks, including prompting for activities of daily living, assistance for art therapists, and stroke rehabilitation. This model is a Partially Observable Markov Decision Process (POMDP) that can be customized by end-users, that can integrate complex sensor information, and that can adapt over time. These three characteristics of the POMDP model will allow for increasing uptake and long-term efficiency and robustness of technology for assistance.",
    "cited_by_count": 58,
    "openalex_id": "https://openalex.org/W2075457858",
    "type": "article"
  },
  {
    "title": "Adaptive eye gaze patterns in interactions with human and artificial agents",
    "doi": "https://doi.org/10.1145/2070719.2070726",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Chen Yu; Paul Schermerhorn; Matthias Scheutz",
    "corresponding_authors": "",
    "abstract": "Efficient collaborations between interacting agents, be they humans, virtual or embodied agents, require mutual recognition of the goal, appropriate sequencing and coordination of each agent's behavior with others, and making predictions from and about the likely behavior of others. Moment-by-moment eye gaze plays an important role in such interaction and collaboration. In light of this, we used a novel experimental paradigm to systematically investigate gaze patterns in both human-human and human-agent interactions. Participants in the study were asked to interact with either another human or an embodied agent in a joint attention task. Fine-grained multimodal behavioral data were recorded including eye movement data, speech, first-person view video, which were then analyzed to discover various behavioral patterns. Those patterns show that human participants are highly sensitive to momentary multimodal behaviors generated by the social partner (either another human or an artificial agent) and they rapidly adapt their gaze behaviors accordingly. Our results from this data-driven approach provide new findings for understanding micro-behaviors in human-human communication which will be critical for the design of artificial agents that can generate human-like gaze behaviors and engage in multimodal interactions with humans.",
    "cited_by_count": 57,
    "openalex_id": "https://openalex.org/W2039954205",
    "type": "article"
  },
  {
    "title": "A multitask approach to continuous five-dimensional affect sensing in natural speech",
    "doi": "https://doi.org/10.1145/2133366.2133372",
    "publication_date": "2012-03-01",
    "publication_year": 2012,
    "authors": "Florian Eyben; Martin Wöllmer; Björn W. Schuller",
    "corresponding_authors": "",
    "abstract": "Automatic affect recognition is important for the ability of future technical systems to interact with us socially in an intelligent way by understanding our current affective state. In recent years there has been a shift in the field of affect recognition from “in the lab” experiments with acted data to “in the wild” experiments with spontaneous and naturalistic data. Two major issues thereby are the proper segmentation of the input and adequate description and modeling of affective states. The first issue is crucial for responsive, real-time systems such as virtual agents and robots, where the latency of the analysis must be as small as possible. To address this issue we introduce a novel method of incremental segmentation to be used in combination with supra-segmental modeling. For modeling of continuous affective states we use Long Short-Term Memory Recurrent Neural Networks, with which we can show an improvement in performance over standard recurrent neural networks and feed-forward neural networks as well as Support Vector Regression. For experiments we use the SEMAINE database, which contains recordings of spontaneous and natural human to Wizard-of-Oz conversations. The recordings are annotated continuously in time and magnitude with FeelTrace for five affective dimensions, namely activation, expectation, intensity, power/dominance, and valence. To exploit dependencies between the five affective dimensions we investigate multitask learning of all five dimensions augmented with inter-rater standard deviation. We can show improvements for multitask over single-task modeling. Correlation coefficients of up to 0.81 are obtained for the activation dimension and up to 0.58 for the valence dimension. The performance for the remaining dimensions were found to be in between that for activation and valence.",
    "cited_by_count": 53,
    "openalex_id": "https://openalex.org/W2024289965",
    "type": "article"
  },
  {
    "title": "Automatic Detection of Social Behavior of Museum Visitor Pairs",
    "doi": "https://doi.org/10.1145/2662869",
    "publication_date": "2014-11-13",
    "publication_year": 2014,
    "authors": "Eyal Dim; Tsvi Kuflik",
    "corresponding_authors": "",
    "abstract": "In many cases, visitors come to a museum in small groups. In such cases, the visitors’ social context has an impact on their museum visit experience. Knowing the social context may allow a system to provide socially aware services to the visitors. Evidence of the social context can be gained from observing/monitoring the visitors’ social behavior. However, automatic identification of a social context requires, on the one hand, identifying typical social behavior patterns and, on the other, using relevant sensors that measure various signals and reason about them to detect the visitors’ social behavior. We present such typical social behavior patterns of visitor pairs, identified by observations, and then the instrumentation, detection process, reasoning, and analysis of measured signals that enable us to detect the visitors’ social behavior. Simple sensors’ data, such as proximity to other visitors, proximity to museum points of interest, and visitor orientation are used to detect social synchronization, attention to the social companion, and interest in museum exhibits. The presented approach may allow future research to offer adaptive services to museum visitors based on their social context to support their group visit experience better.",
    "cited_by_count": 52,
    "openalex_id": "https://openalex.org/W2029044896",
    "type": "article"
  },
  {
    "title": "VizRec",
    "doi": "https://doi.org/10.1145/2983923",
    "publication_date": "2016-11-07",
    "publication_year": 2016,
    "authors": "Belgin Mutlu; Eduardo Veas; Christoph Trattner",
    "corresponding_authors": "",
    "abstract": "Visualizations have a distinctive advantage when dealing with the information overload problem: Because they are grounded in basic visual cognition, many people understand them. However, creating proper visualizations requires specific expertise of the domain and underlying data. Our quest in this article is to study methods to suggest appropriate visualizations autonomously. To be appropriate, a visualization has to follow known guidelines to find and distinguish patterns visually and encode data therein. A visualization tells a story of the underlying data; yet, to be appropriate, it has to clearly represent those aspects of the data the viewer is interested in. Which aspects of a visualization are important to the viewer? Can we capture and use those aspects to recommend visualizations? This article investigates strategies to recommend visualizations considering different aspects of user preferences. A multi-dimensional scale is used to estimate aspects of quality for visualizations for collaborative filtering. Alternatively, tag vectors describing visualizations are used to recommend potentially interesting visualizations based on content. Finally, a hybrid approach combines information on what a visualization is about (tags) and how good it is (ratings). We present the design principles behind VizRec , our visual recommender. We describe its architecture, the data acquisition approach with a crowd sourced study, and the analysis of strategies for visualization recommendation.",
    "cited_by_count": 48,
    "openalex_id": "https://openalex.org/W2555066320",
    "type": "article"
  },
  {
    "title": "Prediction of Who Will Be the Next Speaker and When Using Gaze Behavior in Multiparty Meetings",
    "doi": "https://doi.org/10.1145/2757284",
    "publication_date": "2016-05-05",
    "publication_year": 2016,
    "authors": "Ryo Ishii; Kazuhiro Otsuka; Shiro Kumano; Junji Yamato",
    "corresponding_authors": "",
    "abstract": "In multiparty meetings, participants need to predict the end of the speaker’s utterance and who will start speaking next, as well as consider a strategy for good timing to speak next. Gaze behavior plays an important role in smooth turn-changing. This article proposes a prediction model that features three processing steps to predict (I) whether turn-changing or turn-keeping will occur, (II) who will be the next speaker in turn-changing, and (III) the timing of the start of the next speaker’s utterance. For the feature values of the model, we focused on gaze transition patterns and the timing structure of eye contact between a speaker and a listener near the end of the speaker’s utterance. Gaze transition patterns provide information about the order in which gaze behavior changes. The timing structure of eye contact is defined as who looks at whom and who looks away first, the speaker or listener, when eye contact between the speaker and a listener occurs. We collected corpus data of multiparty meetings, using the data to demonstrate relationships between gaze transition patterns and timing structure and situations (I), (II), and (III). The results of our analyses indicate that the gaze transition pattern of the speaker and listener and the timing structure of eye contact have a strong association with turn-changing, the next speaker in turn-changing, and the start time of the next utterance. On the basis of the results, we constructed prediction models using the gaze transition patterns and timing structure. The gaze transition patterns were found to be useful in predicting turn-changing, the next speaker in turn-changing, and the start time of the next utterance. Contrary to expectations, we did not find that the timing structure is useful for predicting the next speaker and the start time. This study opens up new possibilities for predicting the next speaker and the timing of the next utterance using gaze transition patterns in multiparty meetings.",
    "cited_by_count": 46,
    "openalex_id": "https://openalex.org/W2399858921",
    "type": "article"
  },
  {
    "title": "A User Perception--Based Approach to Create Smiling Embodied Conversational Agents",
    "doi": "https://doi.org/10.1145/2925993",
    "publication_date": "2017-01-02",
    "publication_year": 2017,
    "authors": "Magalie Ochs; Catherine Pélachaud; Gary McKeown",
    "corresponding_authors": "",
    "abstract": "In order to improve the social capabilities of embodied conversational agents, we propose a computational model to enable agents to automatically select and display appropriate smiling behavior during human--machine interaction. A smile may convey different communicative intentions depending on subtle characteristics of the facial expression and contextual cues. To construct such a model, as a first step, we explore the morphological and dynamic characteristics of different types of smiles (polite, amused, and embarrassed smiles) that an embodied conversational agent may display. The resulting lexicon of smiles is based on a corpus of virtual agents’ smiles directly created by users and analyzed through a machine-learning technique. Moreover, during an interaction, a smiling expression impacts on the observer’s perception of the interpersonal stance of the speaker. As a second step, we propose a probabilistic model to automatically compute the user’s potential perception of the embodied conversational agent’s social stance depending on its smiling behavior and on its physical appearance. This model, based on a corpus of users’ perceptions of smiling and nonsmiling virtual agents, enables a virtual agent to determine the appropriate smiling behavior to adopt given the interpersonal stance it wants to express. An experiment using real human--virtual agent interaction provided some validation of the proposed model.",
    "cited_by_count": 46,
    "openalex_id": "https://openalex.org/W2567635461",
    "type": "article"
  },
  {
    "title": "Affective Analysis of Professional and Amateur Abstract Paintings Using Statistical Analysis and Art Theory",
    "doi": "https://doi.org/10.1145/2768209",
    "publication_date": "2015-06-30",
    "publication_year": 2015,
    "authors": "Andreza Sartori; Victoria Yanulevskaya; Alkım Almıla Akdağ Salah; Jasper Uijlings; Elia Bruni; Nicu Sebe",
    "corresponding_authors": "",
    "abstract": "When artists express their feelings through the artworks they create, it is believed that the resulting works transform into objects with “emotions” capable of conveying the artists' mood to the audience. There is little to no dispute about this belief: Regardless of the artwork, genre, time, and origin of creation, people from different backgrounds are able to read the emotional messages. This holds true even for the most abstract paintings. Could this idea be applied to machines as well? Can machines learn what makes a work of art “emotional”? In this work, we employ a state-of-the-art recognition system to learn which statistical patterns are associated with positive and negative emotions on two different datasets that comprise professional and amateur abstract artworks. Moreover, we analyze and compare two different annotation methods in order to establish the ground truth of positive and negative emotions in abstract art. Additionally, we use computer vision techniques to quantify which parts of a painting evoke positive and negative emotions. We also demonstrate how the quantification of evidence for positive and negative emotions can be used to predict which parts of a painting people prefer to focus on. This method opens new opportunities of research on why a specific painting is perceived as emotional at global and local scales.",
    "cited_by_count": 45,
    "openalex_id": "https://openalex.org/W2149203771",
    "type": "article"
  },
  {
    "title": "Integrated online localization and navigation for people with visual impairments using smart phones",
    "doi": "https://doi.org/10.1145/2499669",
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "Ilias Apostolopoulos; Navid Fallah; Eelke Folmer; Kostas E. Bekris",
    "corresponding_authors": "",
    "abstract": "Indoor localization and navigation systems for individuals with Visual Impairments (VIs) typically rely upon extensive augmentation of the physical space, significant computational resources, or heavy and expensive sensors; thus, few systems have been implemented on a large scale. This work describes a system able to guide people with VIs through indoor environments using inexpensive sensors, such as accelerometers and compasses, which are available in portable devices like smart phones. The method takes advantage of feedback from the human user, who confirms the presence of landmarks, something that users with VIs already do when navigating in a building. The system calculates the user's location in real time and uses it to provide audio instructions on how to reach the desired destination. Initial early experiments suggested that the accuracy of the localization depends on the type of directions and the availability of an appropriate transition model for the user. A critical parameter for the transition model is the user's step length. Consequently, this work also investigates different schemes for automatically computing the user's step length and reducing the dependence of the approach on the definition of an accurate transition model. In this way, the direction provision method is able to use the localization estimate and adapt to failed executions of paths by the users. Experiments are presented that evaluate the accuracy of the overall integrated system, which is executed online on a smart phone. Both people with VIs and blindfolded sighted people participated in the experiments, which included paths along multiple floors that required the use of stairs and elevators.",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W2174913512",
    "type": "article"
  },
  {
    "title": "Crowdsourcing Ground Truth for Medical Relation Extraction",
    "doi": "https://doi.org/10.1145/3152889",
    "publication_date": "2018-06-13",
    "publication_year": 2018,
    "authors": "Anca Dumitrache; Lora Aroyo; Chris Welty",
    "corresponding_authors": "",
    "abstract": "Cognitive computing systems require human labeled data for evaluation, and often for training. The standard practice used in gathering this data minimizes disagreement between annotators, and we have found this results in data that fails to account for the ambiguity inherent in language. We have proposed the CrowdTruth method for collecting ground truth through crowdsourcing, that reconsiders the role of people in machine learning based on the observation that disagreement between annotators provides a useful signal for phenomena such as ambiguity in the text. We report on using this method to build an annotated data set for medical relation extraction for the $cause$ and $treat$ relations, and how this data performed in a supervised training experiment. We demonstrate that by modeling ambiguity, labeled data gathered from crowd workers can (1) reach the level of quality of domain experts for this task while reducing the cost, and (2) provide better training data at scale than distant supervision. We further propose and validate new weighted measures for precision, recall, and F-measure, that account for ambiguity in both human and machine performance on this task.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W2574781439",
    "type": "article"
  },
  {
    "title": "A Human-in-the-Loop System for Sound Event Detection and Annotation",
    "doi": "https://doi.org/10.1145/3214366",
    "publication_date": "2018-06-21",
    "publication_year": 2018,
    "authors": "Bongjun Kim; Bryan Pardo",
    "corresponding_authors": "",
    "abstract": "Labeling of audio events is essential for many tasks. However, finding sound events and labeling them within a long audio file is tedious and time-consuming. In cases where there is very little labeled data (e.g., a single labeled example), it is often not feasible to train an automatic labeler because many techniques (e.g., deep learning) require a large number of human-labeled training examples. Also, fully automated labeling may not show sufficient agreement with human labeling for many uses. To solve this issue, we present a human-in-the-loop sound labeling system that helps a user quickly label target sound events in a long audio. It lets a user reduce the time required to label a long audio file (e.g., 20 hours) containing target sounds that are sparsely distributed throughout the recording (10% or less of the audio contains the target) when there are too few labeled examples (e.g., one) to train a state-of-the-art machine audio labeling system. To evaluate the effectiveness of our tool, we performed a human-subject study. The results show that it helped participants label target sound events twice as fast as labeling them manually. In addition to measuring the overall performance of the proposed system, we also measure interaction overhead and machine accuracy, which are two key factors that determine the overall performance. The analysis shows that an ideal interface that does not have interaction overhead at all could speed labeling by as much as a factor of four.",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W2809183397",
    "type": "article"
  },
  {
    "title": "The Effect of Culture on Trust in Automation",
    "doi": "https://doi.org/10.1145/3230736",
    "publication_date": "2018-11-16",
    "publication_year": 2018,
    "authors": "Shih‐Yi Chien; Michael Lewis; Katia Sycara; Jyi-Shane Liu; Asiye Kumru",
    "corresponding_authors": "",
    "abstract": "Trust in automation has become a topic of intensive study since the late 1990s and is of increasing importance with the advent of intelligent interacting systems. While the earliest trust experiments involved human interventions to correct failures/errors in automated control systems, a majority of subsequent studies have investigated information acquisition and analysis decision aiding tasks such as target detection for which automation reliability is more easily manipulated. Despite the high level of international dependence on automation in industry, almost all current studies have employed Western samples primarily from the U.S. The present study addresses these gaps by running a large sample experiment in three (U.S., Taiwan, and Turkey) diverse cultures using a “trust sensitive task” consisting of both automated control and target detection subtasks. This article presents results for the target detection subtask for which reliability and task load were manipulated. The current experiments allow us to determine whether reported effects are universal or specific to Western culture, vary in baseline or magnitude, or differ across cultures. Results generally confirm consistent effects of manipulations across the three cultures as well as cultural differences in initial trust and variation in effects of manipulations consistent with 10 cultural hypotheses based on Hofstede's Cultural Dimensions and Leung and Cohen's theory of Cultural Syndromes. These results provide critical implications and insights for correct trust calibration and to enhance human trust in intelligent automation systems across cultures. Additionally, our results would be useful in designing intelligent systems for users of different cultures. Our article presents the following contributions: First, to the best of our knowledge, this is the first set of studies that deal with cultural factors across all the cultural syndromes identified in the literature by comparing trust in the Honor, Face, Dignity cultures. Second, this is the first set of studies that uses a validated cross-cultural trust measure for measuring trust in automation. Third, our experiments are the first to study the dynamics of trust across cultures.",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W2900390255",
    "type": "article"
  },
  {
    "title": "Toward Responsible AI: An Overview of Federated Learning for User-centered Privacy-preserving Computing",
    "doi": "https://doi.org/10.1145/3485875",
    "publication_date": "2021-10-25",
    "publication_year": 2021,
    "authors": "Qiang Yang",
    "corresponding_authors": "Qiang Yang",
    "abstract": "With the rapid advances of Artificial Intelligence (AI) technologies and applications, an increasing concern is on the development and application of responsible AI technologies. Building AI technologies or machine-learning models often requires massive amounts of data, which may include sensitive, user private information to be collected from different sites or countries. Privacy, security, and data governance constraints rule out a brute force process in the acquisition and integration of these data. It is thus a serious challenge to protect user privacy while achieving high-performance models. This article reviews recent progress of federated learning in addressing this challenge in the context of privacy-preserving computing. Federated learning allows global AI models to be trained and used among multiple decentralized data sources with high security and privacy guarantees, as well as sound incentive mechanisms. This article presents the background, motivations, definitions, architectures, and applications of federated learning as a new paradigm for building privacy-preserving, responsible AI ecosystems.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W3208334254",
    "type": "article"
  },
  {
    "title": "Understanding, Discovering, and Mitigating Habitual Smartphone Use in Young Adults",
    "doi": "https://doi.org/10.1145/3447991",
    "publication_date": "2021-06-30",
    "publication_year": 2021,
    "authors": "Alberto Monge Roffarello; Luigi De Russis",
    "corresponding_authors": "",
    "abstract": "People, especially young adults, often use their smartphones out of habit: They compulsively browse social networks, check emails, and play video-games with little or no awareness at all. While previous studies analyzed this phenomena qualitatively , e.g., by showing that users perceive it as meaningless and addictive, yet our understanding of how to discover smartphone habits and mitigate their disruptive effects is limited. Being able to automatically assess habitual smartphone use, in particular, might have different applications, e.g., to design better “digital wellbeing” solutions for mitigating meaningless habitual use. To close this gap, we first define a data analytic methodology based on clustering and association rules mining to automatically discover complex smartphone habits from mobile usage data. We assess the methodology over more than 130,000 phone usage sessions collected from users aged between 16 and 33, and we show evidence that smartphone habits of young adults can be characterized by various types of links between contextual situations and usage sessions, which are highly diversified and differently perceived across users. We then apply the proposed methodology in Socialize, a digital wellbeing app that (i) monitors habitual smartphone behaviors in real time and (ii) uses proactive notifications and just-in-time reminders to encourage users to avoid any identified smartphone habits they consider as meaningless. An in-the-wild study with 20 users (ages 19–31) demonstrates that Socialize can assist young adults in better controlling their smartphone usage with a significant reduction of their unwanted smartphone habits.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W3185428400",
    "type": "article"
  },
  {
    "title": "EDAssistant: Supporting Exploratory Data Analysis in Computational Notebooks with In Situ Code Search and Recommendation",
    "doi": "https://doi.org/10.1145/3545995",
    "publication_date": "2022-06-29",
    "publication_year": 2022,
    "authors": "Xingjun Li; Y. Zhang; J. Leung; C. P. Sun; Jian Zhao",
    "corresponding_authors": "",
    "abstract": "Using computational notebooks (e.g., Jupyter Notebook), data scientists rationalize their exploratory data analysis (EDA) based on their prior experience and external knowledge, such as online examples. For novices or data scientists who lack specific knowledge about the dataset or problem to investigate, effectively obtaining and understanding the external information is critical to carrying out EDA. This article presents EDAssistant, a JupyterLab extension that supports EDA with in situ search of example notebooks and recommendation of useful APIs, powered by novel interactive visualization of search results. The code search and recommendation are enabled by advanced machine learning models, trained on a large corpus of EDA notebooks collected online. A user study is conducted to investigate both EDAssistant and data scientists’ current practice (i.e., using external search engines). The results demonstrate the effectiveness and usefulness of EDAssistant, and participants appreciated its smooth and in-context support of EDA. We also report several design implications regarding code recommendation tools.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W4226018502",
    "type": "article"
  },
  {
    "title": "Discourse Behavior of Older Adults Interacting with a Dialogue Agent Competent in Multiple Topics",
    "doi": "https://doi.org/10.1145/3484510",
    "publication_date": "2022-05-25",
    "publication_year": 2022,
    "authors": "Seyedeh Zahra Razavi; Lenhart K. Schubert; Kimberly A. Van Orden; Mohammad Rafayet Ali; Benjamin Kane; Ehsan Hoque",
    "corresponding_authors": "",
    "abstract": "We present a conversational agent designed to provide realistic conversational practice to older adults at risk of isolation or social anxiety, and show the results of a content analysis on a corpus of data collected from experiments with elderly patients interacting with our system. The conversational agent, represented by a virtual avatar, is designed to hold multiple sessions of casual conversation with older adults. Throughout each interaction, the system analyzes the prosodic and nonverbal behavior of users and provides feedback to the user in the form of periodic comments and suggestions on how to improve. Our avatar is unique in its ability to hold natural dialogues on a wide range of everyday topics—27 topics in three groups, developed in collaboration with a team of gerontologists. The three groups vary in “degrees of intimacy,” and as such in degrees of cognitive difficulty for the user. After collecting data from nine participants who interacted with the avatar for seven to nine sessions over a period of 3 to 4 weeks, we present results concerning dialogue behavior and inferred sentiment of the users. Analysis of the dialogues reveals correlations such as greater elaborateness for more difficult topics, increasing elaborateness with successive sessions, stronger sentiments in topics concerned with life goals rather than routine activities, and stronger self-disclosure for more intimate topics. In addition to their intrinsic interest, these results also reflect positively on the sophistication and practical applicability of our dialogue system.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2960124774",
    "type": "article"
  },
  {
    "title": "How Do Users Experience Traceability of AI Systems? Examining Subjective Information Processing Awareness in Automated Insulin Delivery (AID) Systems",
    "doi": "https://doi.org/10.1145/3588594",
    "publication_date": "2023-03-24",
    "publication_year": 2023,
    "authors": "Tim Schrills; Thomas Franke",
    "corresponding_authors": "",
    "abstract": "When interacting with artificial intelligence (AI) in the medical domain, users frequently face automated information processing, which can remain opaque to them. For example, users with diabetes may interact daily with automated insulin delivery (AID). However, effective AID therapy requires traceability of automated decisions for diverse users. Grounded in research on human-automation interaction, we study Subjective Information Processing Awareness (SIPA) as a key construct to research users’ experience of explainable AI. The objective of the present research was to examine how users experience differing levels of traceability of an AI algorithm. We developed a basic AID simulation to create realistic scenarios for an experiment with N = 80, where we examined the effect of three levels of information disclosure on SIPA and performance. Attributes serving as the basis for insulin needs calculation were shown to users, who predicted the AID system’s calculation after over 60 observations. Results showed a difference in SIPA after repeated observations, associated with a general decline of SIPA ratings over time. Supporting scale validity, SIPA was strongly correlated with trust and satisfaction with explanations. The present research indicates that the effect of different levels of information disclosure may need several repetitions before it manifests. Additionally, high levels of information disclosure may lead to a miscalibration between SIPA and performance in predicting the system’s results. The results indicate that for a responsible design of XAI, system designers could utilize prediction tasks in order to calibrate experienced traceability.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4360851815",
    "type": "article"
  },
  {
    "title": "“I Want It That Way”: Enabling Interactive Decision Support Using Large Language Models and Constraint Programming",
    "doi": "https://doi.org/10.1145/3685053",
    "publication_date": "2024-08-01",
    "publication_year": 2024,
    "authors": "Connor Lawless; Jakob Schoeffer; Lindy Le; Kael Rowan; Shilad Sen; Cristina St. Hill; Jina Suh; Bahareh Sarrafzadeh",
    "corresponding_authors": "",
    "abstract": "A critical factor in the success of many decision support systems is the accurate modeling of user preferences. Psychology research has demonstrated that users often develop their preferences during the elicitation process, highlighting the pivotal role of system-user interaction in developing personalized systems. This paper introduces a novel approach, combining Large Language Models (LLMs) with Constraint Programming to facilitate interactive decision support. We study this hybrid framework through the lens of meeting scheduling, a time-consuming daily activity faced by a multitude of information workers. We conduct three studies to evaluate the novel framework, including a diary study to characterize contextual scheduling preferences, a quantitative evaluation of the system’s performance, and a user study to elicit insights with a technology probe that encapsulates our framework. Our work highlights the potential for a hybrid LLM and optimization approach for iterative preference elicitation, and suggests design considerations for building systems that support human-system collaborative decision-making processes.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4401218824",
    "type": "article"
  },
  {
    "title": "Interactions for Socially Shared Regulation in Collaborative Learning: An Interdisciplinary Multimodal Dataset",
    "doi": "https://doi.org/10.1145/3658376",
    "publication_date": "2024-04-22",
    "publication_year": 2024,
    "authors": "Yante Li; Yang Liu; Andy Nguyen; Henglin Shi; Eija Vuorenmaa; Sanna Järvelä; Guoying Zhao",
    "corresponding_authors": "",
    "abstract": "Socially shared regulation plays a pivotal role in the success of collaborative learning. However, evaluating socially shared regulation of learning (SSRL) proves challenging due to the dynamic and infrequent cognitive and socio-emotional interactions, which constitute the focal point of SSRL. To address this challenge, this article gathers interdisciplinary researchers to establish a multimodal dataset with cognitive and socio-emotional interactions for SSRL study. Firstly, to induce cognitive and socio-emotional interactions, learning science researchers designed a special collaborative learning task with regulatory trigger events among triadic people for the SSRL study. Secondly, this dataset includes various modalities like video, Kinect data, audio, and physiological data (accelerometer, EDA, heart rate) from 81 high school students in 28 groups, offering a comprehensive view of the SSRL process. Thirdly, three-level verbal interaction annotations and nonverbal interactions including facial expression, eye gaze, gesture, and posture are provided, which could further contribute to interdisciplinary fields such as computer science, sociology, and education. In addition, comprehensive analysis verifies the dataset’s effectiveness. As far as we know, this is the first multimodal dataset for studying SSRL among triadic group members.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4395013064",
    "type": "article"
  },
  {
    "title": "Recognizing sketched multistroke primitives",
    "doi": "https://doi.org/10.1145/2030365.2030369",
    "publication_date": "2011-10-01",
    "publication_year": 2011,
    "authors": "Tracy Hammond; Brandon Paulson",
    "corresponding_authors": "",
    "abstract": "Sketch recognition attempts to interpret the hand-sketched markings made by users on an electronic medium. Through recognition, sketches and diagrams can be interpreted and sent to simulators or other meaningful analyzers. Primitives are the basic building block shapes used by high-level visual grammars to describe the symbols of a given sketch domain. However, one limitation of these primitive recognizers is that they often only support basic shapes drawn with a single stroke. Furthermore, recognizers that do support multistroke primitives place additional constraints on users, such as temporal timeouts or modal button presses to signal shape completion. The goal of this research is twofold. First, we wanted to determine the drawing habits of most users. Our studies found multistroke primitives to be more prevalent than multiple primitives drawn in a single stroke. Additionally, our studies confirmed that threading is less frequent when there are more sides to a figure. Next, we developed an algorithm that is capable of recognizing multistroke primitives without requiring special drawing constraints. The algorithm uses a graph-building and search technique that takes advantage of Tarjan's linear search algorithm, along with principles to determine the goodness of a fit. Our novel, constraint-free recognizer achieves accuracy rates of 96% on freely-drawn primitives.",
    "cited_by_count": 47,
    "openalex_id": "https://openalex.org/W2066119762",
    "type": "article"
  },
  {
    "title": "Spotting laughter in natural multiparty conversations",
    "doi": "https://doi.org/10.1145/2133366.2133370",
    "publication_date": "2012-03-01",
    "publication_year": 2012,
    "authors": "Stefan Scherer; Michael Glodek; Friedhelm Schwenker; Nick Campbell; Günther Palm",
    "corresponding_authors": "",
    "abstract": "It is essential for the advancement of human-centered multimodal interfaces to be able to infer the current user's state or communication state. In order to enable a system to do that, the recognition and interpretation of multimodal social signals (i.e., paralinguistic and nonverbal behavior) in real-time applications is required. Since we believe that laughs are one of the most important and widely understood social nonverbal signals indicating affect and discourse quality, we focus in this work on the detection of laughter in natural multiparty discourses. The conversations are recorded in a natural environment without any specific constraint on the discourses using unobtrusive recording devices. This setup ensures natural and unbiased behavior, which is one of the main foci of this work. To compare results of methods, namely Gaussian Mixture Model (GMM) supervectors as input to a Support Vector Machine (SVM), so-called Echo State Networks (ESN), and a Hidden Markov Model (HMM) approach, are utilized in online and offline detection experiments. The SVM approach proves very accurate in the offline classification task, but is outperformed by the ESN and HMM approach in the online detection (F 1 scores: GMM SVM 0.45, ESN 0.63, HMM 0.72). Further, we were able to utilize the proposed HMM approach in a cross-corpus experiment without any retraining with respectable generalization capability (F 1 score: 0.49). The results and possible reasons for these outcomes are shown and discussed in the article. The proposed methods may be directly utilized in practical tasks such as the labeling or the online detection of laughter in conversational data and affect-aware applications.",
    "cited_by_count": 46,
    "openalex_id": "https://openalex.org/W1968399311",
    "type": "article"
  },
  {
    "title": "Affect recognition based on physiological changes during the watching of music videos",
    "doi": "https://doi.org/10.1145/2133366.2133373",
    "publication_date": "2012-03-01",
    "publication_year": 2012,
    "authors": "Ashkan Yazdani; Jong‐Seok Lee; Jean-Marc Vésin; Touradj Ebrahimi",
    "corresponding_authors": "",
    "abstract": "Assessing emotional states of users evoked during their multimedia consumption has received a great deal of attention with recent advances in multimedia content distribution technologies and increasing interest in personalized content delivery. Physiological signals such as the electroencephalogram (EEG) and peripheral physiological signals have been less considered for emotion recognition in comparison to other modalities such as facial expression and speech, although they have a potential interest as alternative or supplementary channels. This article presents our work on: (1) constructing a dataset containing EEG and peripheral physiological signals acquired during presentation of music video clips, which is made publicly available, and (2) conducting binary classification of induced positive/negative valence, high/low arousal, and like/dislike by using the aforementioned signals. The procedure for the dataset acquisition, including stimuli selection, signal acquisition, self-assessment, and signal processing is described in detail. Especially, we propose a novel asymmetry index based on relative wavelet entropy for measuring the asymmetry in the energy distribution of EEG signals, which is used for EEG feature extraction. Then, the classification systems based on EEG and peripheral physiological signals are presented. Single-trial and single-run classification results indicate that, on average, the performance of the EEG-based classification outperforms that of the peripheral physiological signals. However, the peripheral physiological signals can be considered as a good alternative to EEG signals in the case of assessing a user's preference for a given music video clip (like/dislike) since they have a comparable performance to EEG signals while being more easily measured.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W2064188028",
    "type": "article"
  },
  {
    "title": "Plan Recognition and Visualization in Exploratory Learning Environments",
    "doi": "https://doi.org/10.1145/2533670.2533674",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Ofra Amir; Kobi Gal",
    "corresponding_authors": "",
    "abstract": "Modern pedagogical software is open-ended and flexible, allowing students to solve problems through exploration and trial-and-error. Such exploratory settings provide for a rich educational environment for students, but they challenge teachers to keep track of students’ progress and to assess their performance. This article presents techniques for recognizing students’ activities in such pedagogical software and visualizing these activities to teachers. It describes a new plan recognition algorithm that uses a recursive grammar that takes into account repetition and interleaving of activities. This algorithm was evaluated empirically using an exploratory environment for teaching chemistry used by thousands of students in several countries. It was always able to correctly infer students’ plans when the appropriate grammar was available. We designed two methods for visualizing students’ activities for teachers: one that visualizes students’ inferred plans, and one that visualizes students’ interactions over a timeline. Both of these visualization methods were preferred to and found more helpful than a baseline method which showed a movie of students’ interactions. These results demonstrate the benefit of combining novel AI techniques and visualization methods for the purpose of designing collaborative systems that support students in their problem solving and teachers in their understanding of students’ performance.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W2077767741",
    "type": "article"
  },
  {
    "title": "Gaze awareness in conversational agents",
    "doi": "https://doi.org/10.1145/2499474.2499480",
    "publication_date": "2013-07-01",
    "publication_year": 2013,
    "authors": "Ryo Ishii; Yukiko Nakano; Toyoaki Nishida",
    "corresponding_authors": "",
    "abstract": "In face-to-face conversations, speakers are continuously checking whether the listener is engaged in the conversation, and they change their conversational strategy if the listener is not fully engaged. With the goal of building a conversational agent that can adaptively control conversations, in this study we analyze listener gaze behaviors and develop a method for estimating whether a listener is engaged in the conversation on the basis of these behaviors. First, we conduct a Wizard-of-Oz study to collect information on a user's gaze behaviors. We then investigate how conversational disengagement, as annotated by human judges, correlates with gaze transition, mutual gaze (eye contact) occurrence, gaze duration, and eye movement distance. On the basis of the results of these analyses, we identify useful information for estimating a user's disengagement and establish an engagement estimation method using a decision tree technique. The results of these analyses show that a model using the features of gaze transition, mutual gaze occurrence, gaze duration, and eye movement distance provides the best performance and can estimate the user's conversational engagement accurately. The estimation model is then implemented as a real-time disengagement judgment mechanism and incorporated into a multimodal dialog manager in an animated conversational agent. This agent is designed to estimate the user's conversational engagement and generate probing questions when the user is distracted from the conversation. Finally, we evaluate the engagement-sensitive agent and find that asking probing questions at the proper times has the expected effects on the user's verbal/nonverbal behaviors during communication with the agent. We also find that our agent system improves the user's impression of the agent in terms of its engagement awareness, behavior appropriateness, conversation smoothness, favorability, and intelligence.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W2099936971",
    "type": "article"
  },
  {
    "title": "Context-Aware Automated Analysis and Annotation of Social Human--Agent Interactions",
    "doi": "https://doi.org/10.1145/2764921",
    "publication_date": "2015-06-30",
    "publication_year": 2015,
    "authors": "Tobias Baur; Gregor Mehlmann; Ionuț Damian; Florian Lingenfelser; Johannes Wagner; Birgit Lugrin; Elisabeth André; Patrick Gebhard",
    "corresponding_authors": "",
    "abstract": "The outcome of interpersonal interactions depends not only on the contents that we communicate verbally, but also on nonverbal social signals. Because a lack of social skills is a common problem for a significant number of people, serious games and other training environments have recently become the focus of research. In this work, we present NovA ( No n v erbal behavior A nalyzer), a system that analyzes and facilitates the interpretation of social signals automatically in a bidirectional interaction with a conversational agent. It records data of interactions, detects relevant social cues, and creates descriptive statistics for the recorded data with respect to the agent's behavior and the context of the situation. This enhances the possibilities for researchers to automatically label corpora of human--agent interactions and to give users feedback on strengths and weaknesses of their social behavior.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W1021544130",
    "type": "article"
  },
  {
    "title": "See You See Me",
    "doi": "https://doi.org/10.1145/2882970",
    "publication_date": "2016-05-05",
    "publication_year": 2016,
    "authors": "Tian Xu; Hui Zhang; Chen Yu",
    "corresponding_authors": "",
    "abstract": "We focus on a fundamental looking behavior in human-robot interactions—gazing at each other's face. Eye contact and mutual gaze between two social partners are critical in smooth human-human interactions. Therefore, investigating at what moments and in what ways a robot should look at a human user's face as a response to the human's gaze behavior is an important topic. Toward this goal, we developed a gaze-contingent human-robot interaction system, which relied on momentary gaze behaviors from a human user to control an interacting robot in real time. Using this system, we conducted an experiment in which human participants interacted with the robot in a joint-attention task. In the experiment, we systematically manipulated the robot's gaze toward the human partner's face in real time and then analyzed the human's gaze behavior as a response to the robot's gaze behavior. We found that more face looks from the robot led to more look-backs (to the robot's face) from human participants, and consequently, created more mutual gaze and eye contact between the two. Moreover, participants demonstrated more coordinated and synchronized multimodal behaviors between speech and gaze when more eye contact was successfully established and maintained.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W2406567157",
    "type": "article"
  },
  {
    "title": "The Effects of Interpersonal Attitude of a Group of Agents on User’s Presence and Proxemics Behavior",
    "doi": "https://doi.org/10.1145/2914796",
    "publication_date": "2016-07-20",
    "publication_year": 2016,
    "authors": "Angelo Cafaro; Brian Ravenet; Magalie Ochs; Hannes Högni Vilhjálmsson; Catherine Pélachaud",
    "corresponding_authors": "",
    "abstract": "In the everyday world people form small conversing groups where social interaction takes place, and much of the social behavior takes place through managing interpersonal space (i.e., proxemics) and group formation, signaling their attentio to others (i.e., through gaze behavior), and expressing certain attitudes, for example, friendliness, by smiling, getting close through increased engagement and intimacy, and welcoming newcomers. Many real-time interactive systems feature virtual anthropomorphic characters in order to simulate conversing groups and add plausibility and believability to the simulated environments. However, only a few have dealt with autonomous behavior generation, and in those cases, the agents’ exhibited behavior should be evaluated by users in terms of appropriateness, believability, and conveyed meaning (e.g., attitudes). In this article we present an integrated intelligent interactive system for generating believable nonverbal behavior exhibited by virtual agents in small simulated group conversations. The produced behavior supports group formation management and the expression of interpersonal attitudes (friendly vs. unfriendly) both among the agents in the group (i.e., in-group attitude) and towards an approaching user in an avatar-based interaction (out-group attitude). A user study investigating the effects of these attitudes on users’ social presence evaluation and proxemics behavior (with their avatar) in a three-dimensional virtual city environment is presented. We divided the study into two trials according to the task assigned to users, that is, joining a conversing group and reaching a target destination behind the group. Results showed that the out-group attitude had a major impact on social presence evaluations in both trials, whereby friendly groups were perceived as more socially rich. The user’s proxemics behavior depended on both out-group and in-group attitudes expressed by the agents. Implications of these results for the design and implementation of similar intelligent interactive systems for the autonomous generation of agents’ multimodal behavior are briefly discussed.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W2490014606",
    "type": "article"
  },
  {
    "title": "Adaptive Real-Time Emotion Recognition from Body Movements",
    "doi": "https://doi.org/10.1145/2738221",
    "publication_date": "2015-12-22",
    "publication_year": 2015,
    "authors": "Weiyi Wang; Valentin Enescu; Hichem Sahli",
    "corresponding_authors": "",
    "abstract": "We propose a real-time system that continuously recognizes emotions from body movements. The combined low-level 3D postural features and high-level kinematic and geometrical features are fed to a Random Forests classifier through summarization (statistical values) or aggregation (bag of features). In order to improve the generalization capability and the robustness of the system, a novel semisupervised adaptive algorithm is built on top of the conventional Random Forests classifier. The MoCap UCLIC affective gesture database (labeled with four emotions) was used to train the Random Forests classifier, which led to an overall recognition rate of 78% using a 10-fold cross-validation. Subsequently, the trained classifier was used in a stream-based semisupervised Adaptive Random Forests method for continuous unlabeled Kinect data classification. The very low update cost of our adaptive classifier makes it highly suitable for data stream applications. Tests performed on the publicly available emotion datasets (body gestures and facial expressions) indicate that our new classifier outperforms existing algorithms for data streams in terms of accuracy and computational costs.",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W2201722321",
    "type": "article"
  },
  {
    "title": "A Classification Model for Sensing Human Trust in Machines Using EEG and GSR",
    "doi": "https://doi.org/10.1145/3132743",
    "publication_date": "2018-11-16",
    "publication_year": 2018,
    "authors": "Kumar Akash; Wan-Lin Hu; Neera Jain; Tahira Reid",
    "corresponding_authors": "",
    "abstract": "Today, intelligent machines \\emph{interact and collaborate} with humans in a way that demands a greater level of trust between human and machine. A first step towards building intelligent machines that are capable of building and maintaining trust with humans is the design of a sensor that will enable machines to estimate human trust level in real-time. In this paper, two approaches for developing classifier-based empirical trust sensor models are presented that specifically use electroencephalography (EEG) and galvanic skin response (GSR) measurements. Human subject data collected from 45 participants is used for feature extraction, feature selection, classifier training, and model validation. The first approach considers a general set of psychophysiological features across all participants as the input variables and trains a classifier-based model for each participant, resulting in a trust sensor model based on the general feature set (i.e., a \"general trust sensor model\"). The second approach considers a customized feature set for each individual and trains a classifier-based model using that feature set, resulting in improved mean accuracy but at the expense of an increase in training time. This work represents the first use of real-time psychophysiological measurements for the development of a human trust sensor. Implications of the work, in the context of trust management algorithm design for intelligent machines, are also discussed.",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W2794595230",
    "type": "article"
  },
  {
    "title": "Machine Learning for Social Multiparty Human--Robot Interaction",
    "doi": "https://doi.org/10.1145/2600021",
    "publication_date": "2014-10-14",
    "publication_year": 2014,
    "authors": "Simon Keizer; Mary Ellen Foster; Zhuoran Wang; Oliver Lemon",
    "corresponding_authors": "",
    "abstract": "We describe a variety of machine-learning techniques that are being applied to social multiuser human--robot interaction using a robot bartender in our scenario. We first present a data-driven approach to social state recognition based on supervised learning . We then describe an approach to social skills execution—that is, action selection for generating socially appropriate robot behavior—which is based on reinforcement learning , using a data-driven simulation of multiple users to train execution policies for social skills. Next, we describe how these components for social state recognition and skills execution have been integrated into an end-to-end robot bartender system, and we discuss the results of a user evaluation. Finally, we present an alternative unsupervised learning framework that combines social state recognition and social skills execution based on hierarchical Dirichlet processes and an infinite POMDP interaction manager. The models make use of data from both human--human interactions collected in a number of German bars and human--robot interactions recorded in the evaluation of an initial version of the system.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W1987867889",
    "type": "article"
  },
  {
    "title": "Interactive Visuals as Metaphors for Dance Movement Qualities",
    "doi": "https://doi.org/10.1145/2738219",
    "publication_date": "2015-09-08",
    "publication_year": 2015,
    "authors": "Sarah Fdili Alaoui; Frédéric Bevilacqua; Christian Jacquemin",
    "corresponding_authors": "",
    "abstract": "The notion of “movement qualities” is central in contemporary dance; it describes the manner in which a movement is executed. Movement qualities convey information revealing movement expressiveness; their use has strong potential for movement-based interaction with applications in arts, entertainment, education, or rehabilitation. The purpose of our research is to design and evaluate interactive reflexive visuals for movement qualities. The theoretical basis for this research is drawn from a collaboration with the members of the international dance company Emio Greco|PC to study their formalization of movement qualities. We designed a pedagogical interactive installation called Double Skin/Double Mind (DS/DM) for the analysis and visualization of movement qualities through physical model-based interactive renderings. In this article, we first evaluate dancers’ perception of the visuals as metaphors for movement qualities. This evaluation shows that, depending on the physical model parameterization, the visuals are capable of generating dynamic behaviors that the dancers associate with DS/DM movement qualities. Moreover, we evaluate dance students’ and professionals’ experience of the interactive visuals in the context of a dance pedagogical workshop and a professional dance training. The results of these evaluations show that the dancers consider the interactive visuals to be a reflexive system that encourages them to perform, improves their experience, and contributes to a better understanding of movement qualities. Our findings support research on interactive systems for real-time analysis and visualization of movement qualities, which open new perspectives in movement-based interaction design.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W2095427297",
    "type": "article"
  },
  {
    "title": "High-Volume Hypothesis Testing",
    "doi": "https://doi.org/10.1145/2890478",
    "publication_date": "2016-03-17",
    "publication_year": 2016,
    "authors": "Sana Malik; Ben Shneiderman; Fan Du; Catherine Plaisant; Margrét Bjarnadóttir",
    "corresponding_authors": "",
    "abstract": "Cohort comparison studies have traditionally been hypothesis driven and conducted in carefully controlled environments (such as clinical trials). Given two groups of event sequence data, researchers test a single hypothesis (e.g., does the group taking Medication A exhibit more deaths than the group taking Medication B?). Recently, however, researchers have been moving toward more exploratory methods of retrospective analysis with existing data. In this article, we begin by showing that the task of cohort comparison is specific enough to support automatic computation against a bounded set of potential questions and objectives, a method that we refer to as High-Volume Hypothesis Testing (HVHT). From this starting point, we demonstrate that the diversity of these objectives, both across and within different domains, as well as the inherent complexities of real-world datasets, still requires human involvement to determine meaningful insights. We explore how visualization and interaction better support the task of exploratory data analysis and the understanding of HVHT results (how significant they are, why they are meaningful, and whether the entire dataset has been exhaustively explored). Through interviews and case studies with domain experts, we iteratively design and implement visualization and interaction techniques in a visual analytics tool, CoCo. As a result of our evaluation, we propose six design guidelines for enabling users to explore large result sets of HVHT systematically and flexibly in order to glean meaningful insights more quickly. Finally, we illustrate the utility of this method with three case studies in the medical domain.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W2335772274",
    "type": "article"
  },
  {
    "title": "User Evaluations on Sentiment-based Recommendation Explanations",
    "doi": "https://doi.org/10.1145/3282878",
    "publication_date": "2019-08-09",
    "publication_year": 2019,
    "authors": "Li Chen; Dongning Yan; Feng Wang",
    "corresponding_authors": "",
    "abstract": "The explanation interface has been recognized as important in recommender systems because it can allow users to better judge the relevance of recommendations to their preferences and, hence, make more informed decisions. In different product domains, the specific purpose of explanation can be different. For high-investment products (e.g., digital cameras, laptops), how to educate the typical type of new buyers about product knowledge and, consequently, improve their preference certainty and decision quality is essentially crucial. With this objective, we have developed a novel tradeoff-oriented explanation interface that particularly takes into account sentiment features as extracted from product reviews to generate recommendations and explanations in a category structure. In this manuscript, we first reported the results of an earlier user study (in both before-after and counter-balancing setups) that compared our prototype system with the traditional one that purely considers static specifications for explanations. This experiment revealed that adding sentiment-based explanations can significantly increase users’ product knowledge, preference certainty, perceived information usefulness, perceived recommendation transparency and quality, and purchase intention. In order to further identify the reason behind users’ perception improvements on the sentiment-based explanation interface, we performed a follow-up lab controlled eye-tracking experiment that investigated how users viewed information and compared products on the interface. This study shows that incorporating sentiment features into the tradeoff-oriented explanations can significantly affect users’ eye-gaze pattern. They were stimulated to not only notice bottom categories of products, but also, more frequently, to compare products across categories. The results also disclose users’ inherent information needs for sentiment-based explanations, as they allow users to better understand the recommended products and gain more knowledge about static specifications.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W2967299966",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on Human-Centered Machine Learning",
    "doi": "https://doi.org/10.1145/3205942",
    "publication_date": "2018-06-15",
    "publication_year": 2018,
    "authors": "Rebecca Fiebrink; Marco Gillies",
    "corresponding_authors": "",
    "abstract": "Machine learning is one of the most important and successful techniques in contemporary computer science. Although it can be applied to myriad problems of human interest, research in machine learning is often framed in an impersonal way, as merely algorithms being applied to model data. However, this viewpoint hides considerable human work of tuning the algorithms, gathering the data, deciding what should be modeled in the first place, and using the outcomes of machine learning in the real world. Examining machine learning from a human-centered perspective includes explicitly recognizing human work, as well as reframing machine learning workflows based on situated human working practices, and exploring the co-adaptation of humans and intelligent systems. A human-centered understanding of machine learning in human contexts can lead not only to more usable machine learning tools, but to new ways of understanding what machine learning is good for and how to make it more useful. This special issue brings together nine articles that present different ways to frame machine learning in a human context. They represent very different application areas (from medicine to audio) and methodologies (including machine learning methods, human-computer interaction methods, and hybrids), but they all explore the human contexts in which machine learning is used. This introduction summarizes the articles in this issue and draws out some common themes.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W2807746307",
    "type": "article"
  },
  {
    "title": "Comparing and Combining Interaction Data and Eye-tracking Data for the Real-time Prediction of User Cognitive Abilities in Visualization Tasks",
    "doi": "https://doi.org/10.1145/3301400",
    "publication_date": "2020-05-30",
    "publication_year": 2020,
    "authors": "Cristina Conati; Sébastien Lallé; Md Abed Rahman; Dereck Toker",
    "corresponding_authors": "",
    "abstract": "Previous work has shown that some user cognitive abilities relevant for processing information visualizations can be predicted from eye-tracking data. Performing this type of user modeling is important for devising visualizations that can detect a user's abilities and adapt accordingly during the interaction. In this article, we extend previous user modeling work by investigating for the first time interaction data as an alternative source to predict cognitive abilities during visualization processing when it is not feasible to collect eye-tracking data. We present an extensive comparison of user models based solely on eye-tracking data, on interaction data, as well as on a combination of the two. Although we found that eye-tracking data generate the most accurate predictions, results show that interaction data can still outperform a majority-class baseline, meaning that adaptation for interactive visualizations could be enabled even when it is not feasible to perform eye tracking, using solely interaction data. Furthermore, we found that interaction data can predict several cognitive abilities with better accuracy at the very beginning of the task than eye-tracking data, which are valuable for delivering adaptation early in the task. We also extend previous work by examining the value of multimodal classifiers combining interaction data and eye-tracking data, with promising results for some of our target user cognitive abilities. Next, we contribute to previous work by extending the type of visualizations considered and the set of cognitive abilities that can be predicted from either eye-tracking data and interaction data. Finally, we evaluate how noise in gaze data impacts prediction accuracy and find that retaining rather noisy gaze datapoints can yield equal or even better predictions than discarding them, a novel and important contribution for devising adaptive visualizations in real settings where eye-tracking data are typically noisier than in laboratory settings.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W3033300303",
    "type": "article"
  },
  {
    "title": "Generating and Understanding Personalized Explanations in Hybrid Recommender Systems",
    "doi": "https://doi.org/10.1145/3365843",
    "publication_date": "2020-11-08",
    "publication_year": 2020,
    "authors": "Pigi Kouki; James Schaffer; Jay Pujara; John O’Donovan; Lise Getoor",
    "corresponding_authors": "",
    "abstract": "Recommender systems are ubiquitous and shape the way users access information and make decisions. As these systems become more complex, there is a growing need for transparency and interpretability. In this article, we study the problem of generating and visualizing personalized explanations for recommender systems that incorporate signals from many different data sources. We use a flexible, extendable probabilistic programming approach and show how we can generate real-time personalized recommendations. We then turn these personalized recommendations into explanations. We perform an extensive user study to evaluate the benefits of explanations for hybrid recommender systems. We conduct a crowd-sourced user study where our system generates personalized recommendations and explanations for real users of the last.fm music platform. First, we evaluate the performance of the recommendations in terms of perceived accuracy and novelty. Next, we experiment with (1) different explanation styles (e.g., user-based, item-based), (2) manipulating the number of explanation styles presented, and (3) manipulating the presentation format (e.g., textual vs. visual). We also apply a mixed-model statistical analysis to consider user personality traits as a control variable and demonstrate the usefulness of our approach in creating personalized hybrid explanations with different style, number, and format. Finally, we perform a post analysis that shows different preferences for explanation styles between experienced and novice last.fm users.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W3103536874",
    "type": "article"
  },
  {
    "title": "Predicting Users’ Movie Preference and Rating Behavior from Personality and Values",
    "doi": "https://doi.org/10.1145/3338244",
    "publication_date": "2020-09-30",
    "publication_year": 2020,
    "authors": "Euna Mehnaz Khan; Md. Saddam Hossain Mukta; Mohammed Eunus Ali; Jalal Mahmud",
    "corresponding_authors": "",
    "abstract": "In this article, we propose novel techniques to predict a user’s movie genre preference and rating behavior from her psycholinguistic attributes obtained from the social media interactions. The motivation of this work comes from various psychological studies that demonstrate that psychological attributes such as personality and values can influence one’s decision or choice in real life. In this work, we integrate user interactions in Twitter and IMDb to derive interesting relations between human psychological attributes and their movie preferences. In particular, we first predict a user’s movie genre preferences from the personality and value scores of the user derived from her tweets. Second, we also develop models to predict user movie rating behavior from her tweets in Twitter and movie genre and storyline preferences from IMDb. We further strengthen the movie rating model by incorporating the user reviews. In the above models, we investigate the role of personality and values independently and combinedly while predicting movie genre preferences and movie rating behaviors. We find that our combined models significantly improve the accuracy than that of a single model that is built by using personality or values independently. We also compare our technique with the traditional movie genre and rating prediction techniques. The experimental results show that our models are effective in recommending movies to users.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W3093656879",
    "type": "article"
  },
  {
    "title": "Humanized Recommender Systems: State-of-the-art and Research Issues",
    "doi": "https://doi.org/10.1145/3446906",
    "publication_date": "2021-06-30",
    "publication_year": 2021,
    "authors": "Thi Ngoc Trang Tran; Alexander Felfernig; Nava Tintarev",
    "corresponding_authors": "",
    "abstract": "Psychological factors such as personality, emotions, social connections , and decision biases can significantly affect the outcome of a decision process. These factors are also prevalent in the existing literature related to the inclusion of psychological aspects in recommender system development. Personality and emotions of users have strong connections with their interests and decision-making behavior. Hence, integrating these factors into recommender systems can help to better predict users’ item preferences and increase the satisfaction with recommended items. In scenarios where decisions are made by groups (e.g., selecting a tourism destination to visit with friends), group composition and social connections among group members can affect the outcome of a group decision. Decision biases often occur in a recommendation process, since users usually apply heuristics when making a decision. These biases can result in low-quality decisions. In this article, we provide a rigorous review of existing research on the influence of the mentioned psychological factors on recommender systems. These factors are not only considered in single-user recommendation scenarios but, importantly, also in group recommendation ones, where groups of users are involved in a decision-making process. We include working examples to provide a deeper understanding of how to take into account these factors in recommendation processes. The provided examples go beyond single-user recommendation scenarios by also considering specific aspects of group recommendation settings.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W3183235320",
    "type": "article"
  },
  {
    "title": "Experiences of a Speech-enabled Conversational Agent for the Self-report of Well-being among People Living with Affective Disorders: An In-the-Wild Study",
    "doi": "https://doi.org/10.1145/3484508",
    "publication_date": "2022-05-20",
    "publication_year": 2022,
    "authors": "Raju Maharjan; Kevin Doherty; Darius A. Rohani; Per Bækgaard; Jakob E. Bardram",
    "corresponding_authors": "",
    "abstract": "The growing commercial success of smart speaker devices following recent advancements in speech recognition technology has surfaced new opportunities for collecting self-reported health and well-being data. Speech-enabled conversational agents (CAs) in particular, deployed in home environments using just such systems, may offer increasingly intuitive and engaging means of self-report. To date, however, few real-world studies have examined users’ experiences of engaging in the self-report of mental health using such devices or the challenges of deploying these systems in the home context. With these aims in mind, this article recounts findings from a 4-week “in-the-wild” study during which 20 individuals with depression or bipolar disorder used a speech-enabled CA named “Sofia” to maintain a daily diary log, responding also to the World Health Organization–Five Well-Being Index WHO-5 scale every 2 weeks. Thematic analysis of post-study interviews highlights actions taken by participants to overcome CAs’ limitations, diverse personifications of a speech-enabled agent, and unique forms of valuing of this system among users’ personal and social circles. These findings serve as initial evidence for the potential of CAs to support the self-report of mental health and well-being, while highlighting the need to address outstanding technical limitations in addition to design challenges of conversational pattern matching, filling unmet interpersonal gaps, and the use of self-report CAs in the at-home social context. Based on these insights, we discuss implications for the future design of CAs to support the self-report of mental health and well-being.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W4286273951",
    "type": "article"
  },
  {
    "title": "I Know This Looks Bad, But I Can Explain: Understanding When AI Should Explain Actions In Human-AI Teams",
    "doi": "https://doi.org/10.1145/3635474",
    "publication_date": "2023-12-02",
    "publication_year": 2023,
    "authors": "Rui Zhang; Christopher Flathmann; Geoff Musick; Beau G. Schelble; Nathan J. McNeese; Bart P. Knijnenburg; Wen Duan",
    "corresponding_authors": "",
    "abstract": "Explanation of artificial intelligence (AI) decision-making has become an important research area in human–computer interaction (HCI) and computer-supported teamwork research. While plenty of research has investigated AI explanations with an intent to improve AI transparency and human trust in AI, how AI explanations function in teaming environments remains unclear. Given that a major benefit of AI giving explanations is to increase human trust understanding how AI explanations impact human trust is crucial to effective human-AI teamwork. An online experiment was conducted with 156 participants to explore this question by examining how a teammate’s explanations impact the perceived trust of the teammate and the effectiveness of the team and how these impacts vary based on whether the teammate is a human or an AI. This study shows that explanations facilitate trust in AI teammates when explaining why AI disobeyed humans’ orders but hindered trust when explaining why an AI lied to humans. In addition, participants’ personal characteristics (e.g., their gender and the individual’s ethical framework) impacted their perceptions of AI teammates both directly and indirectly in different scenarios. Our study contributes to interactive intelligent systems and HCI by shedding light on how an AI teammate’s actions and corresponding explanations are perceived by humans while identifying factors that impact trust and perceived effectiveness. This work provides an initial understanding of AI explanations in human-AI teams, which can be used for future research to build upon in exploring AI explanation implementation in collaborative environments.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4389269333",
    "type": "article"
  },
  {
    "title": "XAutoML: A Visual Analytics Tool for Understanding and Validating Automated Machine Learning",
    "doi": "https://doi.org/10.1145/3625240",
    "publication_date": "2023-09-28",
    "publication_year": 2023,
    "authors": "Marc-André Zöller; Waldemar Titov; Thomas Schlegel; Marco F. Huber",
    "corresponding_authors": "",
    "abstract": "In the last 10 years, various automated machine learning (AutoML) systems have been proposed to build end-to-end machine learning (ML) pipelines with minimal human interaction. Even though such automatically synthesized ML pipelines are able to achieve competitive performance, recent studies have shown that users do not trust models constructed by AutoML due to missing transparency of AutoML systems and missing explanations for the constructed ML pipelines. In a requirements analysis study with 36 domain experts, data scientists, and AutoML researchers from different professions with vastly different expertise in ML, we collect detailed informational needs for AutoML. We propose XAutoML , an interactive visual analytics tool for explaining arbitrary AutoML optimization procedures and ML pipelines constructed by AutoML. XAutoML combines interactive visualizations with established techniques from explainable artificial intelligence (XAI) to make the complete AutoML procedure transparent and explainable. By integrating XAutoML with JupyterLab , experienced users can extend the visual analytics with ad-hoc visualizations based on information extracted from XAutoML . We validate our approach in a user study with the same diverse user group from the requirements analysis. All participants were able to extract useful information from XAutoML , leading to a significantly increased understanding of ML pipelines produced by AutoML and the AutoML optimization itself.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4387130605",
    "type": "article"
  },
  {
    "title": "Cooperative Multi-Objective Bayesian Design Optimization",
    "doi": "https://doi.org/10.1145/3657643",
    "publication_date": "2024-04-17",
    "publication_year": 2024,
    "authors": "George Mo; John J. Dudley; Liwei Chan; Yi-Chi Liao; Antti Oulasvirta; Per Ola Kristensson",
    "corresponding_authors": "",
    "abstract": "Computational methods can potentially facilitate user interface design by complementing designer intuition, prior experience, and personal preference. Framing a user interface design task as a multi-objective optimization problem can help with operationalizing and structuring this process at the expense of designer agency and experience. While offering a systematic means of exploring the design space, the optimization process cannot typically leverage the designer’s expertise in quickly identifying that a given “bad” design is not worth evaluating. We here examine a cooperative approach where both the designer and optimization process share a common goal and work in partnership by establishing a shared understanding of the design space. We tackle the research question: How can we foster cooperation between the designer and a systematic optimization process in order to best leverage their combined strength? We introduce and present an evaluation of a cooperative approach that allows the user to express their design insight and work in concert with a multi-objective design process. We find that the cooperative approach successfully encourages designers to explore more widely in the design space than when they are working without assistance from an optimization process. The cooperative approach also delivers design outcomes that are comparable to an optimization process run without any direct designer input but achieves this with greater efficiency and substantially higher designer engagement levels.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4394877027",
    "type": "article"
  },
  {
    "title": "Reassuring, Misleading, Debunking: Comparing Effects of XAI Methods on Human Decisions",
    "doi": "https://doi.org/10.1145/3665647",
    "publication_date": "2024-05-22",
    "publication_year": 2024,
    "authors": "Christina Humer; Andreas Hinterreiter; Benedikt Leichtmann; Martina Mara; Marc Streit",
    "corresponding_authors": "",
    "abstract": "Trust calibration is essential in AI-assisted decision-making. If human users understand the rationale on which an AI model has made a prediction, they can decide whether they consider this prediction reasonable. Especially in high-risk tasks such as mushroom hunting (where a wrong decision may be fatal), it is important that users make correct choices to trust or overrule the AI. Various explainable AI (XAI) methods are currently being discussed as potentially useful for facilitating understanding and subsequently calibrating user trust. So far, however, it remains unclear which approaches are most effective. In this article, the effects of XAI methods on human AI-assisted decision-making in the high-risk task of mushroom picking were tested. For that endeavor, the effects of (i) Grad-CAM attributions, (ii) nearest-neighbor examples, and (iii) network-dissection concepts were compared in a between-subjects experiment with \\(N=501\\) participants representing end-users of the system. In general, nearest-neighbor examples improved decision correctness the most. However, varying effects for different task items became apparent. All explanations seemed to be particularly effective when they revealed reasons to (i) doubt a specific AI classification when the AI was wrong and (ii) trust a specific AI classification when the AI was correct. Our results suggest that well-established methods, such as Grad-CAM attribution maps, might not be as beneficial to end users as expected and that XAI techniques for use in real-world scenarios must be chosen carefully.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4398200348",
    "type": "article"
  },
  {
    "title": "Unpacking Human-AI interactions: From Interaction Primitives to a Design Space",
    "doi": "https://doi.org/10.1145/3664522",
    "publication_date": "2024-06-08",
    "publication_year": 2024,
    "authors": "Konstantinos Tsiakas; Dave Murray-Rust",
    "corresponding_authors": "",
    "abstract": "This article aims to develop a semi-formal representation for Human-AI (HAI) interactions, by building a set of interaction primitives which can specify the information exchanges between users and AI systems during their interaction. We show how these primitives can be combined into a set of interaction patterns which can capture common interactions between humans and AI/ML models. The motivation behind this is twofold: firstly, to provide a compact generalization of existing practices for the design and implementation of HAI interactions; and secondly, to support the creation of new interactions by extending the design space of HAI interactions. Taking into consideration frameworks, guidelines, and taxonomies related to human-centered design and implementation of AI systems, we define a vocabulary for describing information exchanges based on the model’s characteristics and interactional capabilities. Based on this vocabulary, a message passing model for interactions between humans and models is presented, which we demonstrate can account for existing HAI interaction systems and approaches. Finally, we build this into design patterns which can describe common interactions between users and models, and we discuss how this approach can be used toward a design space for HAI interactions that creates new possibilities for designs as well as keeping track of implementation issues and concerns.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4399460475",
    "type": "article"
  },
  {
    "title": "Eliciting caregiving behavior in dyadic human-robot attachment-like interactions",
    "doi": "https://doi.org/10.1145/2133366.2133369",
    "publication_date": "2012-03-01",
    "publication_year": 2012,
    "authors": "Antoine Hiolle; Lola Cañamero; Marina Davila‐Ross; Kim A. Bard",
    "corresponding_authors": "",
    "abstract": "We present here the design and applications of an arousal-based model controlling the behavior of a Sony AIBO robot during the exploration of a novel environment: a children's play mat. When the robot experiences too many new perceptions, the increase of arousal triggers calls for attention towards its human caregiver. The caregiver can choose to either calm the robot down by providing it with comfort, or to leave the robot coping with the situation on its own. When the arousal of the robot has decreased, the robot moves on to further explore the play mat. We gathered results from two experiments using this arousal-driven control architecture. In the first setting, we show that such a robotic architecture allows the human caregiver to influence greatly the learning outcomes of the exploration episode, with some similarities to a primary caregiver during early childhood. In a second experiment, we tested how human adults behaved in a similar setup with two different robots: one “needy”, often demanding attention, and one more independent, requesting far less care or assistance. Our results show that human adults recognise each profile of the robot for what they have been designed, and behave accordingly to what would be expected, caring more for the needy robot than for the other. Additionally, the subjects exhibited a preference and more positive affect whilst interacting and rating the robot we designed as needy. This experiment leads us to the conclusion that our architecture and setup succeeded in eliciting positive and caregiving behavior from adults of different age groups and technological background. Finally, the consistency and reactivity of the robot during this dyadic interaction appeared crucial for the enjoyment and engagement of the human partner.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W2000465267",
    "type": "article"
  },
  {
    "title": "Active Learning and Visual Analytics for Stance Classification with ALVA",
    "doi": "https://doi.org/10.1145/3132169",
    "publication_date": "2017-09-30",
    "publication_year": 2017,
    "authors": "Kostiantyn Kucher; Carita Paradis; Magnus Sahlgren; Andreas Kerren",
    "corresponding_authors": "",
    "abstract": "The automatic detection and classification of stance (e.g., certainty or agreement) in text data using natural language processing and machine-learning methods creates an opportunity to gain insight into the speakers’ attitudes toward their own and other people’s utterances. However, identifying stance in text presents many challenges related to training data collection and classifier training. To facilitate the entire process of training a stance classifier, we propose a visual analytics approach, called ALVA, for text data annotation and visualization. ALVA’s interplay with the stance classifier follows an active learning strategy to select suitable candidate utterances for manual annotaion. Our approach supports annotation process management and provides the annotators with a clean user interface for labeling utterances with multiple stance categories. ALVA also contains a visualization method to help analysts of the annotation and training process gain a better understanding of the categories used by the annotators. The visualization uses a novel visual representation, called CatCombos, which groups individual annotation items by the combination of stance categories. Additionally, our system makes a visualization of a vector space model available that is itself based on utterances. ALVA is already being used by our domain experts in linguistics and computational linguistics to improve the understanding of stance phenomena and to build a stance classifier for applications such as social media monitoring.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2763367528",
    "type": "article"
  },
  {
    "title": "Observation-Level and Parametric Interaction for High-Dimensional Data Analysis",
    "doi": "https://doi.org/10.1145/3158230",
    "publication_date": "2018-06-13",
    "publication_year": 2018,
    "authors": "Jessica Zeitz Self; Michelle Dowling; John Wenskovitch; Ian Crandell; Ming Wang; Leanna House; Scotland Leman; Chris North",
    "corresponding_authors": "",
    "abstract": "Exploring high-dimensional data is challenging. Dimension reduction algorithms, such as weighted multidimensional scaling, support data exploration by projecting datasets to two dimensions for visualization. These projections can be explored through parametric interaction, tweaking underlying parameterizations, and observation-level interaction, directly interacting with the points within the projection. In this article, we present the results of a controlled usability study determining the differences, advantages, and drawbacks among parametric interaction, observation-level interaction, and their combination. The study assesses both interaction technique effects on domain-specific high-dimensional data analyses performed by non-experts of statistical algorithms. This study is performed using Andromeda, a tool that enables both parametric and observation-level interaction to provide in-depth data exploration. The results indicate that the two forms of interaction serve different, but complementary, purposes in gaining insight through steerable dimension reduction algorithms.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2808343117",
    "type": "article"
  },
  {
    "title": "Context-Sensitive Affect Recognition for a Robotic Game Companion",
    "doi": "https://doi.org/10.1145/2622615",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "Ginevra Castellano; Iolanda Leite; André Pereira; Carlos Martinho; Ana Paiva; Peter W. McOwan",
    "corresponding_authors": "",
    "abstract": "Social perception abilities are among the most important skills necessary for robots to engage humans in natural forms of interaction. Affect-sensitive robots are more likely to be able to establish and maintain believable interactions over extended periods of time. Nevertheless, the integration of affect recognition frameworks in real-time human-robot interaction scenarios is still underexplored. In this article, we propose and evaluate a context-sensitive affect recognition framework for a robotic game companion for children. The robot can automatically detect affective states experienced by children in an interactive chess game scenario. The affect recognition framework is based on the automatic extraction of task features and social interaction-based features. Vision-based indicators of the children’s nonverbal behaviour are merged with contextual features related to the game and the interaction and given as input to support vector machines to create a context-sensitive multimodal system for affect recognition. The affect recognition framework is fully integrated in an architecture for adaptive human-robot interaction. Experimental evaluation showed that children’s affect can be successfully predicted using a combination of behavioural and contextual data related to the game and the interaction with the robot. It was found that contextual data alone can be used to successfully predict a subset of affective dimensions, such as interest toward the robot. Experiments also showed that engagement with the robot can be predicted using information about the user’s valence, interest and anticipatory behaviour. These results provide evidence that social engagement can be modelled as a state consisting of affect and attention components in the context of the interaction.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W2033735801",
    "type": "article"
  },
  {
    "title": "Trust-Based Multi-Robot Symbolic Motion Planning with a Human-in-the-Loop",
    "doi": "https://doi.org/10.1145/3213013",
    "publication_date": "2018-11-16",
    "publication_year": 2018,
    "authors": "Yue Wang; Laura Humphrey; Zhanrui Liao; Huanfei Zheng",
    "corresponding_authors": "",
    "abstract": "Symbolic motion planning for robots is the process of specifying and planning robot tasks in a discrete space, then carrying them out in a continuous space in a manner that preserves the discrete-level task specifications. Despite progress in symbolic motion planning, many challenges remain, including addressing scalability for multi-robot systems and improving solutions by incorporating human intelligence. In this article, distributed symbolic motion planning for multi-robot systems is developed to address scalability. More specifically, compositional reasoning approaches are developed to decompose the global planning problem, and atomic propositions for observation, communication, and control are proposed to address inter-robot collision avoidance. To improve solution quality and adaptability, a hypothetical dynamic, quantitative, and probabilistic human-to-robot trust model is developed to aid this decomposition. Furthermore, a trust-based real-time switching framework is proposed to switch between autonomous and manual motion planning for tradeoffs between task safety and efficiency. Deadlock- and livelock-free algorithms are designed to guarantee reachability of goals with a human-in-the-loop. A set of nontrivial multi-robot simulations with direct human inputs and trust evaluation is provided, demonstrating the successful implementation of the trust-based multi-robot symbolic motion planning methods.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W3121815699",
    "type": "article"
  },
  {
    "title": "Teaching Social Communication Skills Through Human-Agent Interaction",
    "doi": "https://doi.org/10.1145/2937757",
    "publication_date": "2016-08-03",
    "publication_year": 2016,
    "authors": "Hiroki Tanaka; Sakriani Sakti; Graham Neubig; Tomoki Toda; Hideki Negoro; Hidemi Iwasaka; Satoshi Nakamura",
    "corresponding_authors": "",
    "abstract": "There are a large number of computer-based systems that aim to train and improve social skills. However, most of these do not resemble the training regimens used by human instructors. In this article, we propose a computer-based training system that follows the procedure of social skills training (SST), a well-established method to decrease human anxiety and discomfort in social interaction, and acquire social skills. We attempt to automate the process of SST by developing a dialogue system named the automated social skills trainer , which teaches social communication skills through human-agent interaction. The system includes a virtual avatar that recognizes user speech and language information and gives feedback to users. Its design is based on conventional SST performed by human participants, including defining target skills, modeling, role-play, feedback, reinforcement, and homework. We performed a series of three experiments investigating (1) the advantages of using computer-based training systems compared to human-human interaction (HHI) by subjectively evaluating nervousness, ease of talking, and ability to talk well; (2) the relationship between speech language features and human social skills; and (3) the effect of computer-based training using our proposed system. Results of our first experiment show that interaction with an avatar decreases nervousness and increases the user's subjective impression of his or her ability to talk well compared to interaction with an unfamiliar person. The experimental evaluation measuring the relationship between social skill and speech and language features shows that these features have a relationship with social skills. Finally, experiments measuring the effect of performing SST with the proposed application show that participants significantly improve their skill, as assessed by separate evaluators, by using the system for 50 minutes. A user survey also shows that the users thought our system is useful and easy to use, and that interaction with the avatar felt similar to HHI.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2500714156",
    "type": "article"
  },
  {
    "title": "Using Respiration to Predict Who Will Speak Next and When in Multiparty Meetings",
    "doi": "https://doi.org/10.1145/2946838",
    "publication_date": "2016-08-03",
    "publication_year": 2016,
    "authors": "Ryo Ishii; Kazuhiro Otsuka; Shiro Kumano; Junji Yamato",
    "corresponding_authors": "",
    "abstract": "Techniques that use nonverbal behaviors to predict turn-changing situations—such as, in multiparty meetings, who the next speaker will be and when the next utterance will occur—have been receiving a lot of attention in recent research. To build a model for predicting these behaviors we conducted a research study to determine whether respiration could be effectively used as a basis for the prediction. Results of analyses of utterance and respiration data collected from participants in multiparty meetings reveal that the speaker takes a breath more quickly and deeply after the end of an utterance in turn-keeping than in turn-changing. They also indicate that the listener who will be the next speaker takes a bigger breath more quickly and deeply in turn-changing than the other listeners. On the basis of these results, we constructed and evaluated models for predicting the next speaker and the time of the next utterance in multiparty meetings. The results of the evaluation suggest that the characteristics of the speaker's inhalation right after an utterance unit—the points in time at which the inhalation starts and ends after the end of the utterance unit and the amplitude, slope, and duration of the inhalation phase—are effective for predicting the next speaker in multiparty meetings. They further suggest that the characteristics of listeners' inhalation—the points in time at which the inhalation starts and ends after the end of the utterance unit and the minimum and maximum inspiration, amplitude, and slope of the inhalation phase—are effective for predicting the next speaker. The start time and end time of the next speaker's inhalation are also useful for predicting the time of the next utterance in turn-changing.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2486828091",
    "type": "article"
  },
  {
    "title": "Motion-Sound Mapping through Interaction",
    "doi": "https://doi.org/10.1145/3211826",
    "publication_date": "2018-06-13",
    "publication_year": 2018,
    "authors": "Jules Françoise; Frédéric Bevilacqua",
    "corresponding_authors": "",
    "abstract": "Technologies for sensing movement are expanding toward everyday use in virtual reality, gaming, and artistic practices. In this context, there is a need for methodologies to help designers and users create meaningful movement experiences. This article discusses a user-centered approach for the design of interactive auditory feedback using interactive machine learning. We discuss Mapping through Interaction, a method for crafting sonic interactions from corporeal demonstrations of embodied associations between motion and sound. It uses an interactive machine learning approach to build the mapping from user demonstrations, emphasizing an iterative design process that integrates acted and interactive experiences of the relationships between movement and sound. We examine Gaussian Mixture Regression and Hidden Markov Regression for continuous movement recognition and real-time sound parameter generation. We illustrate and evaluate this approach through an application in which novice users can create interactive sound feedback based on coproduced gestures and vocalizations. Results indicate that Gaussian Mixture Regression and Hidden Markov Regression can efficiently learn complex motion-sound mappings from few examples.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2808506583",
    "type": "article"
  },
  {
    "title": "Effects of the Advisor and Environment on Requesting and Complying With Automated Advice",
    "doi": "https://doi.org/10.1145/2905370",
    "publication_date": "2016-11-07",
    "publication_year": 2016,
    "authors": "Steven C. Sutherland; Casper Harteveld; Michael E. Young",
    "corresponding_authors": "",
    "abstract": "Given the rapid technological advances in our society and the increase in artificial and automated advisors with whom we interact on a daily basis, it is becoming increasingly necessary to understand how users interact with and why they choose to request and follow advice from these types of advisors. More specifically, it is necessary to understand errors in advice utilization. In the present study, we propose a methodological framework for studying interactions between users and automated or other artificial advisors. Specifically, we propose the use of virtual environments and the tarp technique for stimulus sampling, ensuring sufficient sampling of important extreme values and the stimulus space between those extremes. We use this proposed framework to identify the impact of several factors on when and how advice is used. Additionally, because these interactions take place in different environments, we explore the impact of where the interaction takes place on the decision to interact. We varied the cost of advice, the reliability of the advisor, and the predictability of the environment to better understand the impact of these factors on the overutilization of suboptimal advisors and underutilization of optimal advisors. We found that less predictable environments, more reliable advisors, and lower costs for advice led to overutilization, whereas more predictable environments and less reliable advisors led to underutilization. Moreover, once advice was received, users took longer to make a final decision, suggesting less confidence and trust in the advisor when the reliability of the advisor was lower, the environment was less predictable, and the advice was not consistent with the environmental cues. These results contribute to a more complete understanding of advice utilization and trust in advisors.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2554071492",
    "type": "article"
  },
  {
    "title": "Analysis of Movement Quality in Full-Body Physical Activities",
    "doi": "https://doi.org/10.1145/3132369",
    "publication_date": "2019-02-11",
    "publication_year": 2019,
    "authors": "Radosław Niewiadomski; Ksenia Kolykhalova; Stefano Piana; Paolo Alborno; Gualtiero Volpe; Antonio Camurri",
    "corresponding_authors": "",
    "abstract": "Full-body human movement is characterized by fine-grain expressive qualities that humans are easily capable of exhibiting and recognizing in others’ movement. In sports (e.g., martial arts) and performing arts (e.g., dance), the same sequence of movements can be performed in a wide range of ways characterized by different qualities, often in terms of subtle (spatial and temporal) perturbations of the movement. Even a non-expert observer can distinguish between a top-level and average performance by a dancer or martial artist. The difference is not in the performed movements--the same in both cases--but in the “quality” of their performance. In this article, we present a computational framework aimed at an automated approximate measure of movement quality in full-body physical activities. Starting from motion capture data, the framework computes low-level (e.g., a limb velocity) and high-level (e.g., synchronization between different limbs) movement features. Then, this vector of features is integrated to compute a value aimed at providing a quantitative assessment of movement quality approximating the evaluation that an external expert observer would give of the same sequence of movements. Next, a system representing a concrete implementation of the framework is proposed. Karate is adopted as a testbed. We selected two different katas (i.e., detailed choreographies of movements in karate) characterized by different overall attitudes and expressions (aggressiveness, meditation), and we asked seven athletes, having various levels of experience and age, to perform them. Motion capture data were collected from the performances and were analyzed with the system. The results of the automated analysis were compared with the scores given by 14 karate experts who rated the same performances. Results show that the movement-quality scores computed by the system and the ratings given by the human observers are highly correlated (Pearson’s correlations r = 0.84, p = 0.001 and r = 0.75, p = 0.005).",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2914533830",
    "type": "article"
  },
  {
    "title": "An Autonomous Cognitive Empathy Model Responsive to Users’ Facial Emotion Expressions",
    "doi": "https://doi.org/10.1145/3341198",
    "publication_date": "2020-09-30",
    "publication_year": 2020,
    "authors": "Elahe Bagheri; Pablo Gómez Esteban; Hoang-Long Cao; Albert De Beir; Dirk Lefeber; Bram Vanderborght",
    "corresponding_authors": "",
    "abstract": "Successful social robot services depend on how robots can interact with users. The effective service can be obtained through smooth, engaged, and humanoid interactions in which robots react properly to a user’s affective state. This article proposes a novel Automatic Cognitive Empathy Model, ACEM, for humanoid robots to achieve longer and more engaged human-robot interactions (HRI) by considering humans’ emotions and replying to them appropriately. The proposed model continuously detects the affective states of a user based on facial expressions and generates desired, either parallel or reactive, empathic behaviors that are already adapted to the user’s personality. Users’ affective states are detected using a stacked autoencoder network that is trained and tested on the RAVDESS dataset. The overall proposed empathic model is verified throughout an experiment, where different emotions are triggered in participants and then empathic behaviors are applied based on proposed hypothesis. The results confirm the effectiveness of the proposed model in terms of related social and friendship concepts that participants perceived during interaction with the robot.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W3100362702",
    "type": "article"
  },
  {
    "title": "Gaze guidance reduces the number of collisions with pedestrians in a driving simulator",
    "doi": "https://doi.org/10.1145/2070719.2070721",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Laura Pomarjanschi; Michael Dörr; Erhardt Barth",
    "corresponding_authors": "",
    "abstract": "Our study explores the potential of gaze guidance in driving and analyzes eye movements and driving behavior in safety-critical situations. We collected eye movements from subjects instructed to drive predetermined routes in a driving simulator. While driving, the subjects performed various cognitive tasks designed to divert their attention away from the road. The 30 subjects were equally divided in two groups, a control and a gaze guidance group. For the latter, potentially dangerous events, such as a pedestrian suddenly crossing the street, were highlighted with temporally transient gaze-contingent cues, which were triggered if the subject did not look at the pedestrian. For the group that drove with gaze guidance, eye movements have a reduced variability after the gaze-capturing event and shorter reaction times to it. More importantly, gaze guidance leads to a safer driving behavior and a significantly reduced number of collisions.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2135943815",
    "type": "article"
  },
  {
    "title": "Gliding and saccadic gaze gesture recognition in real time",
    "doi": "https://doi.org/10.1145/2070719.2070723",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "David Rozado; Javier San Agustin; Francisco B. Rodrı́guez; Pablo Varona",
    "corresponding_authors": "",
    "abstract": "Eye movements can be consciously controlled by humans to the extent of performing sequences of predefined movement patterns, or ’gaze gestures’. Gaze gestures can be tracked non-invasively employing a video- based eye tracking system. Gaze gestures hold the potential to become an emerging input paradigm in the context of human-machine interaction as low-cost gaze trackers become more ubiquitous. The viability of gaze gestures as an innovative way to control a computer rests on how easily they can be assimilated by potential users and also on the ability of machine learning algorithms to discriminate intentional gaze gestures from typical gaze activity performed during standard interaction with electronic devices. In this work, through a set of experiments and user studies, we evaluate the performance of two different gaze gestures modalities, gliding gaze gestures and saccadic gaze gestures, and their corresponding real-time recognition algorithms, Hierarchical Temporal Memory networks and the Needleman-Wunsch algorithm for sequence alignment. Our results show how a specific combination of gaze gesture modality, namely saccadic gaze gestures, and recognition algorithm, Needleman-Wunsch, allows for reliable usage of intentional gaze gestures to interact with a computer with accuracy rates of up to 98% and acceptable completion speed. Furthermore, the gesture recognition engine does not interfere with otherwise standard human-machine gaze interaction generating therefore, very low false positive rates. These positive results open a new human- machine interaction paradigm for the fields of accessibility and interaction with smartphones, projected displays and traditional desktop computers.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2006567158",
    "type": "article"
  },
  {
    "title": "Discovering User Behavioral Features to Enhance Information Search on Big Data",
    "doi": "https://doi.org/10.1145/2856059",
    "publication_date": "2017-06-30",
    "publication_year": 2017,
    "authors": "Nunziato Cassavia; Elio Masciari; Chiara Pulice; Domenico Saccà",
    "corresponding_authors": "",
    "abstract": "Due to the emerging Big Data paradigm, driven by the increasing availability of intelligent services easily accessible by a large number of users (e.g., social networks), traditional data management techniques are inadequate in many real-life scenarios. In particular, the availability of huge amounts of data pertaining to user social interactions, user preferences, and opinions calls for advanced analysis strategies to understand potentially interesting social dynamics. Furthermore, heterogeneity and high speed of user-generated data require suitable data storage and management tools to be designed from scratch. This article presents a framework tailored for analyzing user interactions with intelligent systems while seeking some domain-specific information (e.g., choosing a good restaurant in a visited area). The framework enhances a user's quest for information by exploiting previous knowledge about their social environment, the extent of influence the users are potentially subject to, and the influence they may exert on other users. User influence spread across the network is dynamically computed as well to improve user search strategy by providing specific suggestions, represented as tailored faceted features. Such features are the result of data exchange activity (called data posting) that enriches information sources with additional background information and knowledge derived from experiences and behavioral properties of domain experts and users. The approach is tested in an important application scenario such as tourist recommendation, but it can be profitably exploited in several other contexts, for example, viral marketing and food education.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2741751553",
    "type": "article"
  },
  {
    "title": "Experiments with Mobile Drama in an Instrumented Museum for Inducing Conversation in Small Groups",
    "doi": "https://doi.org/10.1145/2584250",
    "publication_date": "2014-04-01",
    "publication_year": 2014,
    "authors": "Charles Callaway; Oliviero Stock; Elyon DeKoven",
    "corresponding_authors": "",
    "abstract": "Small groups can have a better museum visit when that visit is both a social and an educational occasion. The unmediated discussion that often ensues during a shared cultural experience, especially when it is with a small group whose members already know each other, has been shown by ethnographers to be important for a more enriching experience. We present DRAMATRIC, a mobile presentation system that delivers hour-long dramas to small groups of museum visitors. DRAMATRIC continuously receives sensor data from the museum environment during a museum visit and analyzes group behavior from that data. On the basis of that analysis, DRAMATRIC delivers a series of dynamically coordinated dramatic scenes about exhibits that the group walks near, each designed to stimulate group discussion. Each drama presentation contains small, complementary differences in the narrative content heard by the different members of the group, leveraging the tension/release cycle of narrative to naturally lead visitors to fill in missing pieces in their own drama by interacting with their fellow group members. Using four specific techniques to produce these coordinated narrative variations, we describe two experiments: one in a neutral, nonmobile environment, and the other a controlled experiment with a full-scale drama in an actual museum. The first experiment tests the hypothesis that narrative differences will lead to increased conversation compared to hearing identical narratives, whereas the second experiment tests whether switching from presenting a drama using one technique to using another technique for the subsequent drama will result in increased conversation. The first experiment shows that hearing coordinated narrative variations can in fact lead to significantly increased conversation. The second experiment also serves as a framework for future studies that evaluate strategies for similar adaptive systems.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2003851785",
    "type": "article"
  },
  {
    "title": "Dynamic Handwriting Signal Features Predict Domain Expertise",
    "doi": "https://doi.org/10.1145/3213309",
    "publication_date": "2018-07-24",
    "publication_year": 2018,
    "authors": "Sharon Oviatt; Kevin Hang; Jianlong Zhou; Kun Yu; Fang Chen",
    "corresponding_authors": "",
    "abstract": "As commercial pen-centric systems proliferate, they create a parallel need for analytic techniques based on dynamic writing. Within educational applications, recent empirical research has shown that signal-level features of students’ writing, such as stroke distance, pressure and duration, are adapted to conserve total energy expenditure as they consolidate expertise in a domain. The present research examined how accurately three different machine-learning algorithms could automatically classify users’ domain expertise based on signal features of their writing, without any content analysis. Compared with an unguided machine-learning classification accuracy of 71%, hybrid methods using empirical-statistical guidance correctly classified 79–92% of students by their domain expertise level. In addition to improved accuracy, the hybrid approach contributed a causal understanding of prediction success and generalization to new data. These novel findings open up opportunities to design new automated learning analytic systems and student-adaptive educational technologies for the rapidly expanding sector of commercial pen systems.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2884596617",
    "type": "article"
  },
  {
    "title": "Miscommunication Detection and Recovery in Situated Human–Robot Dialogue",
    "doi": "https://doi.org/10.1145/3237189",
    "publication_date": "2019-02-17",
    "publication_year": 2019,
    "authors": "Matthew Marge; Alexander I. Rudnicky",
    "corresponding_authors": "",
    "abstract": "Even without speech recognition errors, robots may face difficulties interpreting natural-language instructions. We present a method for robustly handling miscommunication between people and robots in task-oriented spoken dialogue. This capability is implemented in TeamTalk, a conversational interface to robots that supports detection and recovery from the situated grounding problems of referential ambiguity and impossible actions. We introduce a representation that detects these problems and a nearest-neighbor learning algorithm that selects recovery strategies for a virtual robot. When the robot encounters a grounding problem, it looks back on its interaction history to consider how it resolved similar situations. The learning method is trained initially on crowdsourced data but is then supplemented by interactions from a longitudinal user study in which six participants performed navigation tasks with the robot. We compare results collected using a general model to user-specific models and find that user-specific models perform best on measures of dialogue efficiency, while the general model yields the highest agreement with human judges. Our overall contribution is a novel approach to detecting and recovering from miscommunication in dialogue by including situated context, namely, information from a robot’s path planner and surroundings.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2915579687",
    "type": "article"
  },
  {
    "title": "A Comparison of Techniques for Sign Language Alphabet Recognition Using Armband Wearables",
    "doi": "https://doi.org/10.1145/3150974",
    "publication_date": "2019-03-27",
    "publication_year": 2019,
    "authors": "Prajwal Paudyal; Junghyo Lee; Ayan Banerjee; Sandeep K. S. Gupta",
    "corresponding_authors": "",
    "abstract": "Recent research has shown that reliable recognition of sign language words and phrases using user-friendly and noninvasive armbands is feasible and desirable. This work provides an analysis and implementation of including fingerspelling recognition (FR) in such systems, which is a much harder problem due to lack of distinctive hand movements. A novel algorithm called DyFAV (Dynamic Feature Selection and Voting) is proposed for this purpose that exploits the fact that fingerspelling has a finite corpus (26 alphabets for the American Sign Language (ASL)). Detailed analysis of the algorithm used as well as comparisons with other traditional machine-learning algorithms is provided. The system uses an independent multiple-agent voting approach to identify letters with high accuracy. The independent voting of the agents ensures that the algorithm is highly parallelizable and thus recognition times can be kept low to suit real-time mobile applications. A thorough explanation and analysis is presented on results obtained on the ASL alphabet corpus for nine people with limited training. An average recognition accuracy of 95.36% is reported and compared with recognition results from other machine-learning techniques. This result is extended by including six additional validation users with data collected under similar settings as the previous dataset. Furthermore, a feature selection schema using a subset of the sensors is proposed and the results are evaluated. The mobile, noninvasive, and real-time nature of the technology is demonstrated by evaluating performance on various types of Android phones and remote server configurations. A brief discussion of the user interface is provided along with guidelines for best practices.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2927544136",
    "type": "article"
  },
  {
    "title": "Content-based tag propagation and tensor factorization for personalized item recommendation based on social tagging",
    "doi": "https://doi.org/10.1145/2487164",
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "Dimitrios Rafailidis; Απόστολος Αξενόπουλος; Jonas Etzold; Stavroula Manolopoulou; Petros Daras",
    "corresponding_authors": "",
    "abstract": "In this article, a novel method for personalized item recommendation based on social tagging is presented. The proposed approach comprises a content-based tag propagation method to address the sparsity and “cold start” problems, which often occur in social tagging systems and decrease the quality of recommendations. The proposed method exploits (a) the content of items and (b) users' tag assignments through a relevance feedback mechanism in order to automatically identify the optimal number of content-based and conceptually similar items. The relevance degrees between users, tags, and conceptually similar items are calculated in order to ensure accurate tag propagation and consequently to address the issue of “learning tag relevance.” Moreover, the ternary relation among users, tags, and items is preserved by performing tag propagation in the form of triplets based on users' personal preferences and “cold start” degree. The latent associations among users, tags, and items are revealed based on a tensor factorization model in order to build personalized item recommendations. In our experiments with real-world social data, we show the superiority of the proposed approach over other state-of-the-art methods, since several problems in social tagging systems are successfully tackled. Finally, we present the recommendation methodology in the multimodal engine of I-SEARCH, where users' interaction capabilities are demonstrated.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2067621994",
    "type": "article"
  },
  {
    "title": "Developing a Hand Gesture Recognition System for Mapping Symbolic Hand Gestures to Analogous Emojis in Computer-Mediated Communication",
    "doi": "https://doi.org/10.1145/3297277",
    "publication_date": "2019-03-01",
    "publication_year": 2019,
    "authors": "Jung In Koh; Josh Cherian; Paul Taele; Tracy Hammond",
    "corresponding_authors": "",
    "abstract": "Recent trends in computer-mediated communication (CMC) have not only led to expanded instant messaging through the use of images and videos but have also expanded traditional text messaging with richer content in the form of visual communication markers (VCMs) such as emoticons, emojis, and stickers. VCMs could prevent a potential loss of subtle emotional conversation in CMC, which is delivered by nonverbal cues that convey affective and emotional information. However, as the number of VCMs grows in the selection set, the problem of VCM entry needs to be addressed. Furthermore, conventional means of accessing VCMs continue to rely on input entry methods that are not directly and intimately tied to expressive nonverbal cues. In this work, we aim to address this issue by facilitating the use of an alternative form of VCM entry: hand gestures. To that end, we propose a user-defined hand gesture set that is highly representative of a number of VCMs and a two-stage hand gesture recognition system (trajectory-based, shape-based) that can identify these user-defined hand gestures with an accuracy of 82%. By developing such a system, we aim to allow people using low-bandwidth forms of CMCs to still enjoy their convenient and discreet properties while also allowing them to experience more of the intimacy and expressiveness of higher-bandwidth online communication.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2920079024",
    "type": "article"
  },
  {
    "title": "Multimodal Analysis and Prediction of Persuasiveness in Online Social Multimedia",
    "doi": "https://doi.org/10.1145/2897739",
    "publication_date": "2016-10-17",
    "publication_year": 2016,
    "authors": "Sunghyun Park; Han Suk Shim; Moitreya Chatterjee; Kenji Sagae; Louis‐Philippe Morency",
    "corresponding_authors": "",
    "abstract": "Our lives are heavily influenced by persuasive communication, and it is essential in almost any type of social interaction from business negotiation to conversation with our friends and family. With the rapid growth of social multimedia websites, it is becoming ever more important and useful to understand persuasiveness in the context of social multimedia content online. In this article, we introduce a newly created multimedia corpus of 1,000 movie review videos with subjective annotations of persuasiveness and related high-level characteristics or attributes (e.g., confidence). This dataset will be made freely available to the research community. We designed our experiments around the following five main research hypotheses. First, we study if computational descriptors derived from verbal and nonverbal behavior can be predictive of persuasiveness. We further explore combining descriptors from multiple communication modalities (acoustic, verbal, para-verbal, and visual) for predicting persuasiveness and compare with using a single modality alone. Second, we investigate how certain high-level attributes, such as credibility or expertise, are related to persuasiveness and how the information can be used in modeling and predicting persuasiveness. Third, we investigate differences when speakers are expressing a positive or negative opinion and if the opinion polarity has any influence in the persuasiveness prediction. Fourth, we further study if gender has any influence in the prediction performance. Last, we test if it is possible to make comparable predictions of persuasiveness by only looking at thin slices (i.e., shorter time windows) of a speaker's behavior.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2533128854",
    "type": "article"
  },
  {
    "title": "Smell Pittsburgh",
    "doi": "https://doi.org/10.1145/3369397",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "Yen-Chia Hsu; Jennifer Cross; Paul Dille; Michael Tasota; Beatrice Dias; Randy Sargent; Ting-Hao Huang; Illah Nourbakhsh",
    "corresponding_authors": "",
    "abstract": "Urban air pollution has been linked to various human health concerns, including cardiopulmonary diseases. Communities who suffer from poor air quality often rely on experts to identify pollution sources due to the lack of accessible tools. Taking this into account, we developed Smell Pittsburgh , a system that enables community members to report odors and track where these odors are frequently concentrated. All smell report data are publicly accessible online. These reports are also sent to the local health department and visualized on a map along with air quality data from monitoring stations. This visualization provides a comprehensive overview of the local pollution landscape. Additionally, with these reports and air quality data, we developed a model to predict upcoming smell events and send push notifications to inform communities. We also applied regression analysis to identify statistically significant effects of push notifications on user engagement. Our evaluation of this system demonstrates that engaging residents in documenting their experiences with pollution odors can help identify local air pollution patterns and can empower communities to advocate for better air quality. All citizen-contributed smell data are publicly accessible and can be downloaded from https://smellpgh.org .",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W3041270835",
    "type": "article"
  },
  {
    "title": "Automatic Detection of Usability Problem Encounters in Think-aloud Sessions",
    "doi": "https://doi.org/10.1145/3385732",
    "publication_date": "2020-05-30",
    "publication_year": 2020,
    "authors": "Mingming Fan; Yue Li; Khai N. Truong",
    "corresponding_authors": "",
    "abstract": "Think-aloud protocols are a highly valued usability testing method for identifying usability problems. Despite the value of conducting think-aloud usability test sessions, analyzing think-aloud sessions is often time-consuming and labor-intensive. Consequently, previous research has urged the community to develop techniques to support fast-paced analysis. In this work, we took the first step to design and evaluate machine learning (ML) models to automatically detect usability problem encounters based on users’ verbalization and speech features in think-aloud sessions. Inspired by recent research that shows subtle patterns in users’ verbalizations and speech features tend to occur when they encounter problems, we examined whether these patterns can be utilized to improve the automatic detection of usability problems. We first conducted and recorded think-aloud sessions and then examined the effect of different input features, ML models, test products, and users on usability problem encounters detection. Our work uncovers several technical and user interface design challenges and sets a baseline for automating usability problem detection and integrating such automation into UX practitioners’ workflow.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W3048030566",
    "type": "article"
  },
  {
    "title": "Promoting Energy-Efficient Behavior by Depicting Social Norms in a Recommender Interface",
    "doi": "https://doi.org/10.1145/3460005",
    "publication_date": "2021-09-03",
    "publication_year": 2021,
    "authors": "Alain D. Starke; Martijn C. Willemsen; Chris J. Snijders",
    "corresponding_authors": "",
    "abstract": "How can recommender interfaces help users to adopt new behaviors? In the behavioral change literature, social norms and other nudges are studied to understand how people can be convinced to take action (e.g., towel re-use is boosted when stating that “75% of hotel guests” do so), but most of these nudges are not personalized. In contrast, recommender systems know what to recommend in a personalized way, but not much human-computer interaction ( HCI ) research has considered how personalized advice should be presented to help users to change their current habits. We examine the value of depicting normative messages (e.g., “75% of users do X”), based on actual user data, in a personalized energy recommender interface called “Saving Aid.” In a study among 207 smart thermostat owners, we compared three different normative explanations (“Global.” “Similar,” and “Experienced” norm rates) to a non-social baseline (“kWh savings”). Although none of the norms increased the total number of chosen measures directly, we show that depicting high peer adoption rates alongside energy-saving measures increased the likelihood that they would be chosen from a list of recommendations. In addition, we show that depicting social norms positively affects a user’s evaluation of a recommender interface.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W3197914022",
    "type": "article"
  },
  {
    "title": "FAtiMA Toolkit: Toward an Accessible Tool for the Development of Socio-emotional Agents",
    "doi": "https://doi.org/10.1145/3510822",
    "publication_date": "2022-02-04",
    "publication_year": 2022,
    "authors": "Samuel Mascarenhas; Manuel Guimarães; Rui Prada; Pedro A. Santos; João Dias; Ana Paiva",
    "corresponding_authors": "",
    "abstract": "More than a decade has passed since the development of FearNot!, an application designed to help children deal with bullying through role-playing with virtual characters. It was also the application that led to the creation of FAtiMA, an affective agent architecture for creating autonomous characters that can evoke empathic responses. In this article, we describe the FAtiMA Toolkit, a collection of open-source tools that is designed to help researchers, game developers, and roboticists incorporate a computational model of emotion and decision-making in their work. The toolkit was developed with the goal of making FAtiMA more accessible, easier to incorporate into different projects, and more flexible in its capabilities for human-agent interaction, based upon the experience gathered over the years across different virtual environments and human-robot interaction scenarios. As a result, this work makes several different contributions to the field of Agent-Based Architectures. More precisely, the FAtiMA Toolkit’s library-based design allows developers to easily integrate it with other frameworks, its meta-cognitive model affords different internal reasoners and affective components, and its explicit dialogue structure gives control to the author even within highly complex scenarios. To demonstrate the use of the FAtiMA Toolkit, several different use cases where the toolkit was successfully applied are described and discussed.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W4210812838",
    "type": "article"
  },
  {
    "title": "<tt>Auto-Icon+</tt> : An Automated End-to-End Code Generation Tool for Icon Designs in UI Development",
    "doi": "https://doi.org/10.1145/3531065",
    "publication_date": "2022-04-28",
    "publication_year": 2022,
    "authors": "Sidong Feng; M. H. Jiang; Tingting Zhou; Yankun Zhen; Chunyang Chen",
    "corresponding_authors": "",
    "abstract": "Approximately 50% of development resources are devoted to UI development tasks [9]. Occupying a large proportion of development resources, developing icons can be a time-consuming task, because developers need to consider not only effective implementation methods but also easy-to-understand descriptions. In this paper, we present Auto-Icon+, an approach for automatically generating readable and efficient code for icons from design artifacts. According to our interviews to understand the gap between designers (icons are assembled from multiple components) and developers (icons as single images), we apply a heuristic clustering algorithm to compose the components into an icon image. We then propose an approach based on a deep learning model and computer vision methods to convert the composed icon image to fonts with descriptive labels, thereby reducing the laborious manual effort for developers and facilitating UI development. We quantitatively evaluate the quality of our method in the real world UI development environment and demonstrate that our method offers developers accurate, efficient, readable, and usable code for icon designs, in terms of saving 65.2% implementing time.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W4224882009",
    "type": "article"
  },
  {
    "title": "On the Importance of User Backgrounds and Impressions: Lessons Learned from Interactive AI Applications",
    "doi": "https://doi.org/10.1145/3531066",
    "publication_date": "2022-04-29",
    "publication_year": 2022,
    "authors": "Mahsan Nourani; Chiradeep Roy; Jeremy E. Block; Donald R. Honeycutt; Tahrima Rahman; Eric D. Ragan; Vibhav Gogate",
    "corresponding_authors": "",
    "abstract": "While EXplainable Artificial Intelligence (XAI) approaches aim to improve human-AI collaborative decision-making by improving model transparency and mental model formations, experiential factors associated with human users can cause challenges in ways system designers do not anticipate. In this article, we first showcase a user study on how anchoring bias can potentially affect mental model formations when users initially interact with an intelligent system and the role of explanations in addressing this bias. Using a video activity recognition tool in cooking domain, we asked participants to verify whether a set of kitchen policies are being followed, with each policy focusing on a weakness or a strength. We controlled the order of the policies and the presence of explanations to test our hypotheses. Our main finding shows that those who observed system strengths early on were more prone to automation bias and made significantly more errors due to positive first impressions of the system, while they built a more accurate mental model of the system competencies. However, those who encountered weaknesses earlier made significantly fewer errors, since they tended to rely more on themselves, while they also underestimated model competencies due to having a more negative first impression of the model. Motivated by these findings and similar existing work, we formalize and present a conceptual model of user’s past experiences that examine the relations between user’s backgrounds, experiences, and human factors in XAI systems based on usage time. Our work presents strong findings and implications, aiming to raise the awareness of AI designers toward biases associated with user impressions and backgrounds.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W4225139622",
    "type": "article"
  },
  {
    "title": "Synthesizing Game Levels for Collaborative Gameplay in a Shared Virtual Environment",
    "doi": "https://doi.org/10.1145/3558773",
    "publication_date": "2022-08-23",
    "publication_year": 2022,
    "authors": "Huimin Liu; Minsoo Choi; Dominic Kao; Christos Mousas",
    "corresponding_authors": "",
    "abstract": "We developed a method to synthesize game levels that accounts for the degree of collaboration required by two players to finish a given game level. We first asked a game level designer to create playable game level chunks. Then, two artificial intelligence (AI) virtual agents driven by behavior trees played each game level chunk. We recorded the degree of collaboration required to accomplish each game level chunk by the AI virtual agents and used it to characterize each game level chunk. To synthesize a game level, we assigned to the total cost function cost terms that encode both the degree of collaboration and game level design decisions. Then, we used a Markov-chain Monte Carlo optimization method, called simulated annealing, to solve the total cost function and proposed a design for a game level. We synthesized three game levels (low, medium, and high degrees of collaboration game levels) to evaluate our implementation. We then recruited groups of participants to play the game levels to explore whether they would experience a certain degree of collaboration and validate whether the AI virtual agents provided sufficient data that described the collaborative behavior of players in each game level chunk. By collecting both in-game objective measurements and self-reported subjective ratings, we found that the three game levels indeed impacted the collaboration gameplay behavior of our participants. Moreover, by analyzing our collected data, we found moderate and strong correlations between the participants and the AI virtual agents. These results show that game developers can consider AI virtual agents as an alternative method for evaluating the degree of collaboration required to finish a game level.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W4292738064",
    "type": "article"
  },
  {
    "title": "Adaptive Cognitive Training with Reinforcement Learning",
    "doi": "https://doi.org/10.1145/3476777",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Floriano Zini; Fabio Le Piane; Mauro Gáspari",
    "corresponding_authors": "",
    "abstract": "Computer-assisted cognitive training can help patients affected by several illnesses alleviate their cognitive deficits or healthy people improve their mental performance. In most computer-based systems, training sessions consist of graded exercises, which should ideally be able to gradually improve the trainee’s cognitive functions. Indeed, adapting the difficulty of the exercises to how individuals perform in their execution is crucial to improve the effectiveness of cognitive training activities. In this article, we propose the use of reinforcement learning (RL) to learn how to automatically adapt the difficulty of computerized exercises for cognitive training. In our approach, trainees’ performance in performed exercises is used as a reward to learn a policy that changes over time the values of the parameters that determine exercise difficulty. We illustrate a method to be initially used to learn difficulty-variation policies tailored for specific categories of trainees, and then to refine these policies for single individuals. We present the results of two user studies that provide evidence for the effectiveness of our method: a first study, in which a student category policy obtained via RL was found to have better effects on the cognitive function than a standard baseline training that adopts a mechanism to vary the difficulty proposed by neuropsychologists, and a second study, demonstrating that adding an RL-based individual customization further improves the training process.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4214812567",
    "type": "article"
  },
  {
    "title": "PEACE: A Model of Key Social and Emotional Qualities of Conversational Chatbots",
    "doi": "https://doi.org/10.1145/3531064",
    "publication_date": "2022-04-29",
    "publication_year": 2022,
    "authors": "Ekaterina Svikhnushina; Pearl Pu",
    "corresponding_authors": "",
    "abstract": "Open-domain chatbots engage with users in natural conversations to socialize and establish bonds. However, designing and developing an effective open-domain chatbot is challenging. It is unclear what qualities of a chatbot most correspond to users’ expectations and preferences. Even though existing work has considered a wide range of aspects, some key components are still missing. For example, the role of chatbots’ ability to communicate with humans at the emotional level remains an open subject of study. Furthermore, these trait qualities are likely to cover several dimensions. It is crucial to understand how the different qualities relate and interact with each other and what the core aspects would be. For this purpose, we first designed an exploratory user study aimed at gaining a basic understanding of the desired qualities of chatbots with a special focus on their emotional intelligence. Using the findings from the first study, we constructed a model of the desired traits by carefully selecting a set of features. With the help of a large-scale survey and structural equation modeling, we further validated the model using data collected from the survey. The final outcome is called the PEACE model (Politeness, Entertainment, Attentive Curiosity, and Empathy) . By analyzing the dependencies between the different PEACE constructs, we shed light on the importance of and interplay between the chatbots’ qualities and the effect of users’ attitudes and concerns on their expectations of the technology. Not only PEACE defines the key ingredients of the social qualities of a chatbot, it also helped us derive a set of design implications useful for the development of socially adequate and emotionally aware open-domain chatbots.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4225166612",
    "type": "article"
  },
  {
    "title": "Generating User-Centred Explanations via Illocutionary Question Answering: From Philosophy to Interfaces",
    "doi": "https://doi.org/10.1145/3519265",
    "publication_date": "2022-04-28",
    "publication_year": 2022,
    "authors": "Francesco Sovrano; Fabio Vitali",
    "corresponding_authors": "",
    "abstract": "We propose a new method for generating explanations with Artificial Intelligence (AI) and a tool to test its expressive power within a user interface. In order to bridge the gap between philosophy and human-computer interfaces, we show a new approach for the generation of interactive explanations based on a sophisticated pipeline of AI algorithms for structuring natural language documents into knowledge graphs, answering questions effectively and satisfactorily. With this work, we aim to prove that the philosophical theory of explanations presented by Achinstein can be actually adapted for being implemented into a concrete software application, as an interactive and illocutionary process of answering questions. Specifically, our contribution is an approach to frame illocution in a computer-friendly way, to achieve user-centrality with statistical question answering. Indeed, we frame the illocution of an explanatory process as that mechanism responsible for anticipating the needs of the explainee in the form of unposed, implicit, archetypal questions, hence improving the user-centrality of the underlying explanatory process. Therefore, we hypothesise that if an explanatory process is an illocutionary act of providing content-giving answers to questions, and illocution is as we defined it, the more explicit and implicit questions can be answered by an explanatory tool, the more usable (as per ISO 9241-210) its explanations. We tested our hypothesis with a user-study involving more than 60 participants, on two XAI-based systems, one for credit approval (finance) and one for heart disease prediction (healthcare). The results showed that increasing the illocutionary power of an explanatory tool can produce statistically significant improvements (hence with a P value lower than .05) on effectiveness. This, combined with a visible alignment between the increments in effectiveness and satisfaction, suggests that our understanding of illocution can be correct, giving evidence in favour of our theory.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4286912227",
    "type": "article"
  },
  {
    "title": "Meaningful Explanation Effect on User’s Trust in an AI Medical System: Designing Explanations for Non-Expert Users",
    "doi": "https://doi.org/10.1145/3631614",
    "publication_date": "2023-11-08",
    "publication_year": 2023,
    "authors": "Retno Larasati; Anna De Liddo; Enrico Motta",
    "corresponding_authors": "",
    "abstract": "Whereas most research in AI system explanation for healthcare applications looks at developing algorithmic explanations targeted at AI experts or medical professionals, the question we raise is: How do we build meaningful explanations for laypeople? And how does a meaningful explanation affect user’s trust perceptions? Our research investigates how the key factors affecting human-AI trust change in the light of human expertise, and how to design explanations specifically targeted at non-experts. By means of a stage-based design method, we map the ways laypeople understand AI explanations in a User Explanation Model. We also map both medical professionals and AI experts’ practice in an Expert Explanation Model. A Target Explanation Model is then proposed, which represents how experts’ practice and layperson’s understanding can be combined to design meaningful explanations. Design guidelines for meaningful AI explanations are proposed, and a prototype of AI system explanation for non-expert users in a breast cancer scenario is presented and assessed on how it affect users’ trust perceptions.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4388499385",
    "type": "article"
  },
  {
    "title": "Talk2Data: A Natural Language Interface for Exploratory Visual Analysis via Question Decomposition",
    "doi": "https://doi.org/10.1145/3643894",
    "publication_date": "2024-02-07",
    "publication_year": 2024,
    "authors": "Yi Guo; Danqing Shi; Mingjuan Guo; Yanqiu Wu; Nan Cao; Qing Chen",
    "corresponding_authors": "",
    "abstract": "Through a natural language interface (NLI) for exploratory visual analysis, users can directly “ask” analytical questions about the given tabular data. This process greatly improves user experience and lowers the technical barriers of data analysis. Existing techniques focus on generating a visualization from a concrete question. However, complex questions, requiring multiple data queries and visualizations to answer, are frequently asked in data exploration and analysis, which cannot be easily solved with the existing techniques. To address this issue, in this article, we introduce Talk2Data, a natural language interface for exploratory visual analysis that supports answering complex questions. It leverages an advanced deep-learning model to resolve complex questions into a series of simple questions that could gradually elaborate on the users’ requirements. To present answers, we design a set of annotated and captioned visualizations to represent the answers in a form that supports interpretation and narration. We conducted an ablation study and a controlled user study to evaluate the Talk2Data’s effectiveness and usefulness.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4391614548",
    "type": "article"
  },
  {
    "title": "System Personality and Persuasion in Human-Computer Dialogue",
    "doi": "https://doi.org/10.1145/2209310.2209315",
    "publication_date": "2012-06-01",
    "publication_year": 2012,
    "authors": "Pierre Andrews",
    "corresponding_authors": "Pierre Andrews",
    "abstract": "The human-computer dialogue research field has been studying interaction with computers since the early stage of Artificial Intelligence, however, research has often focused on very practical tasks to be completed with the dialogues. A new trend in the field tries to implement persuasive techniques with automated interactive agents; unlike booking a train ticket, for example, such dialogues require the system to show more anthropomorphic qualities. The influences of such qualities in the effectiveness of persuasive dialogue is only starting to be studied. In this article we focus on one important perceived trait of the system: personality, and explore how it influences the persuasiveness of a dialogue system. We introduce a new persuasive dialogue system and combine it with a state of the art personality utterance generator. By doing so, we can control the system’s extraversion personality trait and observe its influence on the user’s perception of the dialogue and its output. In particular, we observe that the user’s extraversion influences their perception of the dialogue and its persuasiveness, and that the perceived personality of the system can affect its trustworthiness and persuasiveness. We believe that theses observations will help to set up guidelines to tailor dialogue systems to the user’s interaction expectations and improve the persuasive interventions.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2040250580",
    "type": "article"
  },
  {
    "title": "Multimodal approach to affective human-robot interaction design with children",
    "doi": "https://doi.org/10.1145/2030365.2030370",
    "publication_date": "2011-10-01",
    "publication_year": 2011,
    "authors": "Sandra Y. Okita; Victor Ng‐Thow‐Hing; Ravi Kiran Sarvadevabhatla",
    "corresponding_authors": "",
    "abstract": "Two studies examined the different features of humanoid robots and the influence on children's affective behavior. The first study looked at interaction styles and general features of robots. The second study looked at how the robot's attention influences children's behavior and engagement. Through activities familiar to young children (e.g., table setting, story telling), the first study found that cooperative interaction style elicited more oculesic behavior and social engagement. The second study found that quality of attention, type of attention, and length of interaction influences affective behavior and engagement. In the quality of attention, Wizard-of-Oz (woz) elicited the most affective behavior, but automatic attention worked as well as woz when the interaction was short. The type of attention going from nonverbal to verbal attention increased children's oculesic behavior, utterance, and physiological response. Affective interactions did not seem to depend on a single mechanism, but a well-chosen confluence of technical features.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2077442262",
    "type": "article"
  },
  {
    "title": "Fluid gesture interaction design",
    "doi": "https://doi.org/10.1145/2543921",
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "Bruno Zamborlin; Frédéric Bevilacqua; Marco Gillies; Mark d’Inverno",
    "corresponding_authors": "",
    "abstract": "This article presents Gesture Interaction DEsigner (GIDE), an innovative application for gesture recognition. Instead of recognizing gestures only after they have been entirely completed, as happens in classic gesture recognition systems, GIDE exploits the full potential of gestural interaction by tracking gestures continuously and synchronously, allowing users to both control the target application moment to moment and also receive immediate and synchronous feedback about system recognition states. By this means, they quickly learn how to interact with the system in order to develop better performances. Furthermore, rather than learning the predefined gestures of others, GIDE allows users to design their own gestures, making interaction more natural and also allowing the applications to be tailored by users' specific needs. We describe our system that demonstrates these new qualities—that combine to provide fluid gesture interaction design—through evaluations with a range of performers and artists.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2074778119",
    "type": "article"
  },
  {
    "title": "Triggering effective social support for online groups",
    "doi": "https://doi.org/10.1145/2499672",
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "Rohit Kumar; Carolyn Penstein Rosé",
    "corresponding_authors": "",
    "abstract": "Conversational agent technology is an emerging paradigm for creating a social environment in online groups that is conducive to effective teamwork. Prior work has demonstrated advantages in terms of learning gains and satisfaction scores when groups learning together online have been supported by conversational agents that employ Balesian social strategies. This prior work raises two important questions that are addressed in this article. The first question is one of generality. Specifically, are the positive effects of the designed support specific to learning contexts? Or are they in evidence in other collaborative task domains as well? We present a study conducted within a collaborative decision-making task where we see that the positive effects of the Balesian social strategies extend to this new context. The second question is whether it is possible to increase the effectiveness of the Balesian social strategies by increasing the context sensitivity with which the social strategies are triggered. To this end, we present technical work that increases the sensitivity of the triggering. Next, we present a user study that demonstrates an improvement in performance of the support agent with the new, more sensitive triggering policy over the baseline approach from prior work. The technical contribution of this article is that we extend prior work where such support agents were modeled using a composition of conversational behaviors integrated within an event-driven framework. Within the present approach, conversation is orchestrated through context-sensitive triggering of the composed behaviors. The core effort involved in applying this approach involves building a set of triggering policies that achieve this orchestration in a time-sensitive and coherent manner. In line with recent developments in data-driven approaches for building dialog systems, we present a novel technique for learning behavior-specific triggering policies, deploying it as part of our efforts to improve a socially capable conversational tutor agent that supports collaborative learning.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2128602042",
    "type": "article"
  },
  {
    "title": "Exploring the Benefits of Context in 3D Gesture Recognition for Game-Based Virtual Environments",
    "doi": "https://doi.org/10.1145/2656345",
    "publication_date": "2015-03-09",
    "publication_year": 2015,
    "authors": "Eugene M. Taranta; Thaddeus K. Simons; Rahul Sukthankar; Joseph J. LaViola",
    "corresponding_authors": "",
    "abstract": "We present a systematic exploration of how to utilize video game context (e.g., player and environmental state) to modify and augment existing 3D gesture recognizers to improve accuracy for large gesture sets. Specifically, our work develops and evaluates three strategies for incorporating context into 3D gesture recognizers. These strategies include modifying the well-known Rubine linear classifier to handle unsegmented input streams and per-frame retraining using contextual information (CA-Linear); a GPU implementation of dynamic time warping (DTW) that reduces the overhead of traditional DTW by utilizing context to evaluate only relevant time sequences inside of a multithreaded kernel (CA-DTW); and a multiclass SVM with per-class probability estimation that is combined with a contextually based prior probability distribution (CA-SVM). We evaluate each strategy using a Kinect-based third-person perspective VE game prototype that combines parkour-style navigation with hand-to-hand combat. Using a simple gesture collection application to collect a set of 57 gestures and the game prototype that implements 37 of these gestures, we conduct three experiments. In the first experiment, we evaluate the effectiveness of several established classifiers on our gesture set and demonstrate state-of-the-art results using our proposed method. In our second experiment, we generate 500 random scenarios having between 5 and 19 of the 57 gestures in context. We show that the contextually aware classifiers CA-Linear, CA-DTW, and CA-SVM significantly outperform their non--contextually aware counterparts by 37.74%, 36.04%, and 20.81%, respectively. On the basis of the results of the second experiment, we derive upper-bound expectations for in-game performance for the three CA classifiers: 96.61%, 86.79%, and 96.86%, respectively. Finally, our third experiment is an in-game evaluation of the three CA classifiers with and without context. Our results show that through the use of context, we are able to achieve an average in-game recognition accuracy of 89.67% with CA-Linear compared to 65.10% without context, 79.04% for CA-DTW compared to 58.1% without context, and 90.85% with CA-SVM compared to 75.2% without context.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2073098112",
    "type": "article"
  },
  {
    "title": "Interacting with social networks of intelligent things and people in the world of gastronomy",
    "doi": "https://doi.org/10.1145/2448116.2448120",
    "publication_date": "2013-04-01",
    "publication_year": 2013,
    "authors": "Luca Console; Fabrizio Antonelli; Giulia Biamino; Francesca Carmagnola; Federica Cena; Elisa Chiabrando; Vincenzo Cuciti; Matteo Demichelis; Franco Fassio; Fabrizio Franceschi; Roberto Furnari; Cristina Gena; Marina Geymonat; Piercarlo Grimaldi; Pierluige Grillo; Silvia Likavec; Ilaria Lombardi; Dario Mana; Alessandro Marcengo; Michele Mioli; Mario Mirabelli; Monica Perrero; Claudia Picardi; Federica Protti; Amon Rapp; Rossana Simeoni; Daniele Theseider Dupré; Ilaria Torre; Andrea Toso; Fabio Torta; Fabiana Vernero",
    "corresponding_authors": "",
    "abstract": "This article introduces a framework for creating rich augmented environments based on a social web of intelligent things and people. We target outdoor environments, aiming to transform a region into a smart environment that can share its cultural heritage with people, promoting itself and its special qualities. Using the applications developed in the framework, people can interact with things, listen to the stories that these things tell them, and make their own contributions. The things are intelligent in the sense that they aggregate information provided by users and behave in a socially active way. They can autonomously establish social relationships on the basis of their properties and their interaction with users. Hence when a user gets in touch with a thing, she is also introduced to its social network consisting of other things and of users; she can navigate this network to discover and explore the world around the thing itself. Thus the system supports serendipitous navigation in a network of things and people that evolves according to the behavior of users. An innovative interaction model was defined that allows users to interact with objects in a natural, playful way using smartphones without the need for a specially created infrastructure. The framework was instantiated into a suite of applications called WantEat, in which objects from the domain of tourism and gastronomy (such as cheese wheels or bottles of wine) are taken as testimonials of the cultural roots of a region. WantEat includes an application that allows the definition and registration of things, a mobile application that allows users to interact with things, and an application that supports stakeholders in getting feedback about the things that they have registered in the system. WantEat was developed and tested in a real-world context which involved a region and gastronomy-related items from it (such as products, shops, restaurants, and recipes), through an early evaluation with stakeholders and a final evaluation with hundreds of users.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2096618504",
    "type": "article"
  },
  {
    "title": "A Process for Systematic Development of Symbolic Models for Activity Recognition",
    "doi": "https://doi.org/10.1145/2806893",
    "publication_date": "2015-12-22",
    "publication_year": 2015,
    "authors": "Kristina Yordanova; Thomas Kirste",
    "corresponding_authors": "",
    "abstract": "Several emerging approaches to activity recognition (AR) combine symbolic representation of user actions with probabilistic elements for reasoning under uncertainty. These approaches provide promising results in terms of recognition performance, coping with the uncertainty of observations, and model size explosion when complex problems are modelled. But experience has shown that it is not always intuitive to model even seemingly simple problems. To date, there are no guidelines for developing such models. To address this problem, in this work we present a development process for building symbolic models that is based on experience acquired so far as well as on existing engineering and data analysis workflows. The proposed process is a first attempt at providing structured guidelines and practices for designing, modelling, and evaluating human behaviour in the form of symbolic models for AR. As an illustration of the process, a simple example from the office domain was developed. The process was evaluated in a comparative study of an intuitive process and the proposed process. The results showed a significant improvement over the intuitive process. Furthermore, the study participants reported greater ease of use and perceived effectiveness when following the proposed process. To evaluate the applicability of the process to more complex AR problems, it was applied to a problem from the kitchen domain. The results showed that following the proposed process yielded an average accuracy of 78%. The developed model outperformed state-of-the-art methods applied to the same dataset in previous work, and it performed comparably to a symbolic model developed by a model expert without following the proposed development process.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2210463108",
    "type": "article"
  },
  {
    "title": "Chronodes",
    "doi": "https://doi.org/10.1145/3152888",
    "publication_date": "2018-02-06",
    "publication_year": 2018,
    "authors": "Peter J. Polack; Shang-Tse Chen; Minsuk Kahng; Kaya de Barbaro; Rahul C. Basole; Moushumi Sharmin; Duen Horng Chau",
    "corresponding_authors": "",
    "abstract": "The advent of mobile health (mHealth) technologies challenges the capabilities of current visualizations, interactive tools, and algorithms. We present Chronodes, an interactive system that unifies data mining and human-centric visualization techniques to support explorative analysis of longitudinal mHealth data. Chronodes extracts and visualizes frequent event sequences that reveal chronological patterns across multiple participant timelines of mHealth data. It then combines novel interaction and visualization techniques to enable multifocus event sequence analysis, which allows health researchers to interactively define, explore, and compare groups of participant behaviors using event sequence combinations. Through summarizing insights gained from a pilot study with 20 behavioral and biomedical health experts, we discuss Chronodes’s efficacy and potential impact in the mHealth domain. Ultimately, we outline important open challenges in mHealth, and offer recommendations and design guidelines for future research.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2527561266",
    "type": "article"
  },
  {
    "title": "An internet-scale idea generation system",
    "doi": "https://doi.org/10.1145/2448116.2448118",
    "publication_date": "2013-04-01",
    "publication_year": 2013,
    "authors": "Lixiu Yu; Jeffrey V. Nickerson",
    "corresponding_authors": "",
    "abstract": "A method of organizing the crowd to generate ideas is described. It integrates crowds using evolutionary algorithms. The method increases the creativity of ideas across generations, and it works better than greenfield idea generation. Specifically, a design space of internet-scale idea generation systems is defined, and one instance is tested: a crowd idea generation system that uses combination to improve previous designs. The key process of the system is the following: A crowd generates designs, then another crowd combines the designs of the previous crowd. In an experiment with 540 participants, the combined designs were compared to the initial designs and to the designs produced by a greenfield idea generation system. The results show that the sequential combination system produced more creative ideas in the last generation and outperformed the greenfield idea generation system. The design space of crowdsourced idea generation developed here may be used to instantiate systems that can be applied to a wide range of design problems. The work has both pragmatic and theoretical implications: New forms of coordination are now possible, and, using the crowd, it is possible to test existing and emerging theories of coordination and participatory design. Moreover, it may be possible for human designers, organized as a crowd, to codesign with each other and with automated algorithms.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W3125027061",
    "type": "article"
  },
  {
    "title": "In the Mood for Vlog",
    "doi": "https://doi.org/10.1145/2641577",
    "publication_date": "2015-06-30",
    "publication_year": 2015,
    "authors": "Dairazalia Sánchez-Cortés; Shiro Kumano; Kazuhiro Otsuka; Daniel Gática-Pérez",
    "corresponding_authors": "",
    "abstract": "The prevalent “share what's on your mind” paradigm of social media can be examined from the perspective of mood: short-term affective states revealed by the shared data. This view takes on new relevance given the emergence of conversational social video as a popular genre among viewers looking for entertainment and among video contributors as a channel for debate, expertise sharing, and artistic expression. From the perspective of human behavior understanding, in conversational social video both verbal and nonverbal information is conveyed by speakers and decoded by viewers. We present a systematic study of classification and ranking of mood impressions in social video, using vlogs from YouTube. Our approach considers eleven natural mood categories labeled through crowdsourcing by external observers on a diverse set of conversational vlogs. We extract a comprehensive number of nonverbal and verbal behavioral cues from the audio and video channels to characterize the mood of vloggers. Then we implement and validate vlog classification and vlog ranking tasks using supervised learning methods. Following a reliability and correlation analysis of the mood impression data, our study demonstrates that, while the problem is challenging, several mood categories can be inferred with promising performance. Furthermore, multimodal features perform consistently better than single-channel features. Finally, we show that addressing mood as a ranking problem is a promising practical direction for several of the mood categories studied.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W1855315697",
    "type": "article"
  },
  {
    "title": "Incremental Learning of Daily Routines as Workflows in a Smart Home Environment",
    "doi": "https://doi.org/10.1145/2675063",
    "publication_date": "2015-01-28",
    "publication_year": 2015,
    "authors": "Berardina De Carolis; Stefano Ferilli; Domenico Redavid",
    "corresponding_authors": "",
    "abstract": "Smart home environments should proactively support users in their activities, anticipating their needs according to their preferences. Understanding what the user is doing in the environment is important for adapting the environment's behavior, as well as for identifying situations that could be problematic for the user. Enabling the environment to exploit models of the user's most common behaviors is an important step toward this objective. In particular, models of the daily routines of a user can be exploited not only for predicting his/her needs, but also for comparing the actual situation at a given moment with the expected one, in order to detect anomalies in his/her behavior. While manually setting up process models in business and factory environments may be cost-effective, building models of the processes involved in people's everyday life is infeasible. This fact fully justifies the interest of the Ambient Intelligence community in automatically learning such models from examples of actual behavior. Incremental adaptation of the models and the ability to express/learn complex conditions on the involved tasks are also desirable. This article describes how process mining can be used for learning users’ daily routines from a dataset of annotated sensor data. The solution that we propose relies on a First-Order Logic learning approach. Indeed, First-Order Logic provides a single, comprehensive and powerful framework for supporting all the previously mentioned features. Our experiments, performed both on a proprietary toy dataset and on publicly available real-world ones, indicate that this approach is efficient and effective for learning and modeling daily routines in Smart Home Environments.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2058899487",
    "type": "article"
  },
  {
    "title": "Supporting the Design of Machine Learning Workflows with a Recommendation System",
    "doi": "https://doi.org/10.1145/2852082",
    "publication_date": "2016-02-22",
    "publication_year": 2016,
    "authors": "Dietmar Jannach; Michael Jugovac; Lukas Lerche",
    "corresponding_authors": "",
    "abstract": "Machine learning and data analytics tasks in practice require several consecutive processing steps. RapidMiner is a widely used software tool for the development and execution of such analytics workflows. Unlike many other algorithm toolkits, it comprises a visual editor that allows the user to design processes on a conceptual level. This conceptual and visual approach helps the user to abstract from the technical details during the development phase and to retain a focus on the core modeling task. The large set of preimplemented data analysis and machine learning operations available in the tool, as well as their logical dependencies, can, however, be overwhelming in particular for novice users. In this work, we present an add-on to the RapidMiner framework that supports the user during the modeling phase by recommending additional operations to insert into the currently developed machine learning workflow. First, we propose different recommendation techniques and evaluate them in an offline setting using a pool of several thousand existing workflows. Second, we present the results of a laboratory study, which show that our tool helps users to significantly increase the efficiency of the modeling process. Finally, we report on analyses using data that were collected during the real-world deployment of the plug-in component and compare the results of the live deployment of the tool with the results obtained through an offline analysis and a replay simulation.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2288695776",
    "type": "article"
  },
  {
    "title": "Effects of Speed, Cyclicity, and Dimensionality on Distancing, Time, and Preference in Human-Aerial Vehicle Interactions",
    "doi": "https://doi.org/10.1145/2983927",
    "publication_date": "2017-09-19",
    "publication_year": 2017,
    "authors": "Brittany A. Duncan; Robin R. Murphy",
    "corresponding_authors": "",
    "abstract": "This article will present a simulation-based approach to testing multiple variables in the behavior of a small Unmanned Aerial Vehicle (sUAV), inspired by insect and animal motions, to understand how these variables impact time of interaction, preference for interaction, and distancing in Human-Robot Interaction (HRI). Previous work has focused on communicating directionality of flight, intentionality of the robot, and perception of motion in sUAVs, while interactions involving direct distancing from these vehicles have been limited to a single study (likely due to safety concerns). This study takes place in a Cave Automatic Virtual Environment (CAVE) to maintain a sense of scale and immersion with the users, while also allowing for safe interaction. Additionally, the two-alternative forced-choice method is employed as a unique methodology to the study of collocated HRI in order to both study the impact of these variables on preference and allow participants to choose whether or not to interact with a specific robot. This article will be of interest to end-users of sUAV technologies to encourage appropriate distancing based on their application, practitioners in HRI to understand the use of this new methodology, and human-aerial vehicle researchers to understand the perception of these vehicles by 64 naive users. Results suggest that low speed (by 0.27m, p &lt; 0.02) and high cyclicity (by 0.28m, p &lt; 0.01) expressions can be used to increase distancing; that low speed (by 4.4s, p &lt; 0.01) and three-dimensional (by 2.6s, p &lt; 0.01) expressions can be used to decrease time of interaction; and low speed (by 10.4%, p &lt; 0.01) expressions are less preferred for passability in human-aerial vehicle interactions.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2754277884",
    "type": "article"
  },
  {
    "title": "Exploring Social Recommendations with Visual Diversity-Promoting Interfaces",
    "doi": "https://doi.org/10.1145/3231465",
    "publication_date": "2019-08-09",
    "publication_year": 2019,
    "authors": "Chun-Hua Tsai; Peter Brusilovsky",
    "corresponding_authors": "",
    "abstract": "The beyond-relevance objectives of recommender systems have been drawing more and more attention. For example, a diversity-enhanced interface has been shown to associate positively with overall levels of user satisfaction. However, little is known about how users adopt diversity-enhanced interfaces to accomplish various real-world tasks. In this article, we present two attempts at creating a visual diversity-enhanced interface that presents recommendations beyond a simple ranked list. Our goal was to design a recommender system interface to help users explore the different relevance prospects of recommended items in parallel and to stress their diversity. Two within-subject user studies in the context of social recommendation at academic conferences were conducted to compare our visual interfaces. Results from our user study show that the visual interfaces significantly reduced the exploration efforts required for given tasks and helped users to perceive the recommendation diversity. We show that the users examined a diverse set of recommended items while experiencing an improvement in overall user satisfaction. Also, the users’ subjective evaluations show significant improvement in many user-centric metrics. Experiences are discussed that shed light on avenues for future interface designs.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2967581940",
    "type": "article"
  },
  {
    "title": "Interactive Topic Modeling for Exploring Asynchronous Online Conversations",
    "doi": "https://doi.org/10.1145/2854158",
    "publication_date": "2016-02-22",
    "publication_year": 2016,
    "authors": "Enamul Hoque; Giuseppe Carenini",
    "corresponding_authors": "",
    "abstract": "Since the mid-2000s, there has been exponential growth of asynchronous online conversations, thanks to the rise of social media. Analyzing and gaining insights from such conversations can be quite challenging for a user, especially when the discussion becomes very long. A promising solution to this problem is topic modeling, since it may help the user to understand quickly what was discussed in a long conversation and to explore the comments of interest. However, the results of topic modeling can be noisy, and they may not match the user’s current information needs. To address this problem, we propose a novel topic modeling system for asynchronous conversations that revises the model on the fly on the basis of users’ feedback. We then integrate this system with interactive visualization techniques to support the user in exploring long conversations, as well as in revising the topic model when the current results are not adequate to fulfill the user’s information needs. Finally, we report on an evaluation with real users that compared the resulting system with both a traditional interface and an interactive visual interface that does not support human-in-the-loop topic modeling. Both the quantitative results and the subjective feedback from the participants illustrate the potential benefits of our interactive topic modeling approach for exploring conversations, relative to its counterparts.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2292818183",
    "type": "article"
  },
  {
    "title": "AnchorViz",
    "doi": "https://doi.org/10.1145/3241379",
    "publication_date": "2019-08-09",
    "publication_year": 2019,
    "authors": "Jina Suh; Soroush Ghorashi; Gonzalo Ramos; Nan-Chen Chen; Steven M. Drucker; Johan Verwey; Patrice Simard",
    "corresponding_authors": "",
    "abstract": "When building a classifier in interactive machine learning (iML), human knowledge about the target class can be a powerful reference to make the classifier robust to unseen items. The main challenge lies in finding unlabeled items that can either help discover or refine concepts for which the current classifier has no corresponding features (i.e., it has feature blindness ). Yet it is unrealistic to ask humans to come up with an exhaustive list of items, especially for rare concepts that are hard to recall. This article presents AnchorViz , an interactive visualization that facilitates the discovery of prediction errors and previously unseen concepts through human-driven semantic data exploration. By creating example-based or dictionary-based anchors representing concepts, users create a topology that (a) spreads data based on their similarity to the concepts and (b) surfaces the prediction and label inconsistencies between data points that are semantically related. Once such inconsistencies and errors are discovered, users can encode the new information as labels or features and interact with the retrained classifier to validate their actions in an iterative loop. We evaluated AnchorViz through two user studies. Our results show that AnchorViz helps users discover more prediction errors than stratified random and uncertainty sampling methods. Furthermore, during the beginning stages of a training task, an iML tool with AnchorViz can help users build classifiers comparable to the ones built with the same tool with uncertainty sampling and keyword search, but with fewer labels and more generalizable features. We discuss exploration strategies observed during the two studies and how AnchorViz supports discovering, labeling, and refining of concepts through a sensemaking loop.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2967579878",
    "type": "article"
  },
  {
    "title": "Transfer Learning for Semisupervised Collaborative Recommendation",
    "doi": "https://doi.org/10.1145/2835497",
    "publication_date": "2016-07-20",
    "publication_year": 2016,
    "authors": "Weike Pan; Qiang Yang; Yuchao Duan; Zhong Ming",
    "corresponding_authors": "",
    "abstract": "Users’ online behaviors such as ratings and examination of items are recognized as one of the most valuable sources of information for learning users’ preferences in order to make personalized recommendations. But most previous works focus on modeling only one type of users’ behaviors such as numerical ratings or browsing records, which are referred to as explicit feedback and implicit feedback, respectively. In this article, we study a Semisupervised Collaborative Recommendation (SSCR) problem with labeled feedback (for explicit feedback) and unlabeled feedback (for implicit feedback), in analogy to the well-known Semisupervised Learning (SSL) setting with labeled instances and unlabeled instances. SSCR is associated with two fundamental challenges, that is, heterogeneity of two types of users’ feedback and uncertainty of the unlabeled feedback. As a response, we design a novel Self-Transfer Learning (sTL) algorithm to iteratively identify and integrate likely positive unlabeled feedback, which is inspired by the general forward/backward process in machine learning. The merit of sTL is its ability to learn users’ preferences from heterogeneous behaviors in a joint and selective manner. We conduct extensive empirical studies of sTL and several very competitive baselines on three large datasets. The experimental results show that our sTL is significantly better than the state-of-the-art methods.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2482189148",
    "type": "article"
  },
  {
    "title": "Toward Effective Robot--Child Tutoring",
    "doi": "https://doi.org/10.1145/3213768",
    "publication_date": "2019-02-11",
    "publication_year": 2019,
    "authors": "Aditi Ramachandran; Chien‐Ming Huang; Brian Scassellati",
    "corresponding_authors": "",
    "abstract": "Personalized learning environments have the potential to improve learning outcomes for children in a variety of educational domains, as they can tailor instruction based on the unique learning needs of individuals. Robot tutoring systems can further engage users by leveraging their potential for embodied social interaction and take into account crucial aspects of a learner, such as a student’s motivation in learning. In this article, we demonstrate that motivation in young learners corresponds to observable behaviors when interacting with a robot tutoring system, which, in turn, impact learning outcomes. We first detail a user study involving children interacting one on one with a robot tutoring system over multiple sessions. Based on empirical data, we show that academic motivation stemming from one’s own values or goals as assessed by the Academic Self-Regulation Questionnaire (SRQ-A) correlates to observed suboptimal help-seeking behavior during the initial tutoring session. We then show how an interactive robot that responds intelligently to these observed behaviors in subsequent tutoring sessions can positively impact both student behavior and learning outcomes over time. These results provide empirical evidence for the link between internal motivation, observable behavior, and learning outcomes in the context of robot--child tutoring. We also identified an additional suboptimal behavioral feature within our tutoring environment and demonstrated its relationship to internal factors of motivation, suggesting further opportunities to design robot intervention to enhance learning. We provide insights on the design of robot tutoring systems aimed to deliver effective behavioral intervention during learning interactions for children and present a discussion on the broader challenges currently faced by robot--child tutoring systems.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2911867998",
    "type": "article"
  },
  {
    "title": "Individualising Graphical Layouts with Predictive Visual Search Models",
    "doi": "https://doi.org/10.1145/3241381",
    "publication_date": "2019-08-30",
    "publication_year": 2019,
    "authors": "Kashyap Todi; Jussi Jokinen; Kris Luyten; Antti Oulasvirta",
    "corresponding_authors": "",
    "abstract": "In domains where users are exposed to large variations in visuo-spatial features among designs, they often spend excess time searching for common elements (features) on an interface. This article contributes individualised predictive models of visual search, and a computational approach to restructure graphical layouts for an individual user such that features on a new, unvisited interface can be found quicker. It explores four technical principles inspired by the human visual system (HVS) to predict expected positions of features and create individualised layout templates: (I) the interface with highest frequency is chosen as the template; (II) the interface with highest predicted recall probability (serial position curve) is chosen as the template; (III) the most probable locations for features across interfaces are chosen (visual statistical learning) to generate the template; (IV) based on a generative cognitive model, the most likely visual search locations for features are chosen (visual sampling modelling) to generate the template. Given a history of previously seen interfaces, we restructure the spatial layout of a new (unseen) interface with the goal of making its features more easily findable. The four HVS principles are implemented in Familiariser, a web browser that automatically restructures webpage layouts based on the visual history of the user. Evaluation of Familiariser (using visual statistical learning) with users provides first evidence that our approach reduces visual search time by over 10%, and number of eye-gaze fixations by over 20%, during web browsing tasks.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2972101714",
    "type": "article"
  },
  {
    "title": "Modeling Dyslexic Students’ Motivation for Enhanced Learning in E-learning Systems",
    "doi": "https://doi.org/10.1145/3341197",
    "publication_date": "2020-09-30",
    "publication_year": 2020,
    "authors": "Ruijie Wang; Liming Chen; Ivar Solheim",
    "corresponding_authors": "",
    "abstract": "E-Learning systems can support real-time monitoring of learners’ learning desires and effects, thus offering opportunities for enhanced personalized learning. Recognition of the determinants of dyslexic users’ motivation to use e-learning systems is important to help developers improve the design of e-learning systems and educators direct their efforts to relevant factors to enhance dyslexic students’ motivation. Existing research has rarely attempted to model dyslexic users’ motivation in e-learning context from a comprehensive perspective. The present work has conceived a hybrid approach, namely, combining the strengths of qualitative and quantitative analysis methods, to motivation modeling. It examines a variety of factors that affect dyslexic students’ motivation to engage in e-learning systems from psychological, behavioral, and technical perspectives, and establishes their interrelationships. Specifically, the study collects data from a multi-item Likert-style questionnaire to measure relevant factors for conceptual motivation modeling. It then applies both covariance-based (CB-SEM) and variance-based structural equation modeling (PLS-SEM) approaches to determine the quantitative mapping between dyslexic students’ continued use intention and motivational factors, followed by discussions about theoretical findings and design instructions according to our motivation model. Our research has led to a novel motivation model with new constructs of Learning Experience, Reading Experience, Perceived Control, and Perceived Privacy. From both the CB-SEM and PLS-SEM analyses, results on the total effects have indicated consistently that Visual Attractiveness, Reading Experience, and Feedback have the strongest effects on continued use intention.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W3103611717",
    "type": "article"
  },
  {
    "title": "Developing Conversational Agents for Use in Criminal Investigations",
    "doi": "https://doi.org/10.1145/3444369",
    "publication_date": "2021-09-03",
    "publication_year": 2021,
    "authors": "Sam Hepenstal; Leishi Zhang; Neesha Kodagoda; B. L. William Wong",
    "corresponding_authors": "",
    "abstract": "The adoption of artificial intelligence (AI) systems in environments that involve high risk and high consequence decision-making is severely hampered by critical design issues. These issues include system transparency and brittleness, where transparency relates to (i) the explainability of results and (ii) the ability of a user to inspect and verify system goals and constraints; and brittleness, (iii) the ability of a system to adapt to new user demands. Transparency is a particular concern for criminal intelligence analysis, where there are significant ethical and trust issues that arise when algorithmic and system processes are not adequately understood by a user. This prevents adoption of potentially useful technologies in policing environments. In this article, we present a novel approach to designing a conversational agent (CA) AI system for intelligence analysis that tackles these issues. We discuss the results and implications of three different studies; a Cognitive Task Analysis to understand analyst thinking when retrieving information in an investigation, Emergent Themes Analysis to understand the explanation needs of different system components, and an interactive experiment with a prototype conversational agent. Our prototype conversational agent, named Pan, demonstrates transparency provision and mitigates brittleness by evolving new CA intentions. We encode interactions with the CA with human factors principles for situation recognition and use interactive visual analytics to support analyst reasoning. Our approach enables complex AI systems, such as Pan, to be used in sensitive environments, and our research has broader application than the use case discussed.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W3196430655",
    "type": "article"
  },
  {
    "title": "Initial Responses to False Positives in AI-Supported Continuous Interactions: A Colonoscopy Case Study",
    "doi": "https://doi.org/10.1145/3480247",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Niels van Berkel; Jeremy Opie; Omer F. Ahmad; Laurence Lovat; Danail Stoyanov; Ann Blandford",
    "corresponding_authors": "",
    "abstract": "The use of artificial intelligence (AI) in clinical support systems is increasing. In this article, we focus on AI support for continuous interaction scenarios. A thorough understanding of end-user behaviour during these continuous human-AI interactions, in which user input is sustained over time and during which AI suggestions can appear at any time, is still missing. We present a controlled lab study involving 21 endoscopists and an AI colonoscopy support system. Using a custom-developed application and an off-the-shelf videogame controller, we record participants' navigation behaviour and clinical assessment across 14 endoscopic videos. Each video is manually annotated to mimic an AI recommendation, being either true positive or false positive in nature. We find that time between AI recommendation and clinical assessment is significantly longer for incorrect assessments. Further, the type of medical content displayed significantly affects decision time. Finally, we discover that the participant's clinical role plays a large part in the perception of clinical AI support systems. Our study presents a realistic assessment of the effects of imperfect and continuous AI support in a clinical scenario.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W4214917404",
    "type": "article"
  },
  {
    "title": "“I don’t know what you mean by `I am anxious'”: A New Method for Evaluating Conversational Agent Responses to Standardized Mental Health Inputs for Anxiety and Depression",
    "doi": "https://doi.org/10.1145/3488057",
    "publication_date": "2022-05-24",
    "publication_year": 2022,
    "authors": "Tessa Eagle; Conrad Blau; Sophie Bales; Noopur Desai; Victor C. Li; Steve Whittaker",
    "corresponding_authors": "",
    "abstract": "Conversational agents (CAs) are increasingly ubiquitous and are now commonly used to access medical information. However, we lack systematic data about the quality of advice such agents provide. This paper evaluates CA advice for mental health (MH) questions, a pressing issue given that we are undergoing a mental health crisis. Building on prior work, we define a new method to systematically evaluate mental health responses from CAs. We develop multi-utterance conversational probes derived from two widely used mental health diagnostic surveys, the PHQ-9 (Depression) and the GAD-7 (Anxiety). We evaluate the responses of two text-based chatbots and four voice assistants to determine whether CAs provide relevant responses and treatments. Evaluations were conducted both by clinicians and immersively by trained raters, yielding consistent results across all raters. Although advice and recommendations were generally low quality, they were better for Crisis probes and for probes concerning symptoms of Anxiety rather than Depression. Responses were slightly improved for text versus speech-based agents, and when CAs had access to extended dialogue context. Design implications include suggestions for improved responses through clarification sub-dialogues. Responses may also be improved by the incorporation of empathy although this needs to be combined with effective treatments or advice.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4286273835",
    "type": "article"
  },
  {
    "title": "RadarSense: Accurate Recognition of Mid-air Hand Gestures with Radar Sensing and Few Training Examples",
    "doi": "https://doi.org/10.1145/3589645",
    "publication_date": "2023-03-31",
    "publication_year": 2023,
    "authors": "Arthur Sluÿters; Sébastien Lambot; Jean Vanderdonckt; Radu-Daniel Vatavu",
    "corresponding_authors": "",
    "abstract": "Microwave radars bring many benefits to mid-air gesture sensing due to their large field of view and independence from environmental conditions, such as ambient light and occlusion. However, radar signals are highly dimensional and usually require complex deep learning approaches. To understand this landscape, we report results from a systematic literature review of ( N =118) scientific papers on radar sensing, unveiling a large variety of radar technology of different operating frequencies and bandwidths and antenna configurations but also various gesture recognition techniques. Although highly accurate, these techniques require a large amount of training data that depend on the type of radar. Therefore, the training results cannot be easily transferred to other radars. To address this aspect, we introduce a new gesture recognition pipeline that implements advanced full-wave electromagnetic modeling and inversion to retrieve physical characteristics of gestures that are radar independent, i.e., independent of the source, antennas, and radar-hand interactions. Inversion of radar signals further reduces the size of the dataset by several orders of magnitude, while preserving the essential information. This approach is compatible with conventional gesture recognizers, such as those based on template matching, which only need a few training examples to deliver high recognition accuracy rates. To evaluate our gesture recognition pipeline, we conducted user-dependent and user-independent evaluations on a dataset of 16 gesture types collected with the Walabot, a low-cost off-the-shelf array radar. We contrast these results with those obtained for the same gesture types collected with an ultra-wideband radar made of a vector network analyzer with a single horn antenna and with a computer vision sensor, respectively. Based on our findings, we suggest some design implications to support future development in radar-based gesture recognition.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4362467322",
    "type": "article"
  },
  {
    "title": "How Should an AI Trust its Human Teammates? Exploring Possible Cues of Artificial Trust",
    "doi": "https://doi.org/10.1145/3635475",
    "publication_date": "2023-12-06",
    "publication_year": 2023,
    "authors": "Carolina Centeio Jorge; Catholijn M. Jonker; Myrthe L. Tielman",
    "corresponding_authors": "",
    "abstract": "In teams composed of humans, we use trust in others to make decisions, such as what to do next, who to help and who to ask for help. When a team member is artificial, they should also be able to assess whether a human teammate is trustworthy for a certain task. We see trustworthiness as the combination of (1) whether someone will do a task and (2) whether they can do it. With building beliefs in trustworthiness as an ultimate goal, we explore which internal factors (krypta) of the human may play a role (e.g., ability, benevolence, and integrity) in determining trustworthiness, according to existing literature. Furthermore, we investigate which observable metrics (manifesta) an agent may take into account as cues for the human teammate’s krypta in an online 2D grid-world experiment ( n = 54). Results suggest that cues of ability, benevolence and integrity influence trustworthiness. However, we observed that trustworthiness is mainly influenced by human’s playing strategy and cost-benefit analysis, which deserves further investigation. This is a first step towards building informed beliefs of human trustworthiness in human-AI teamwork.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4389386859",
    "type": "article"
  },
  {
    "title": "Measuring User Experience Inclusivity in Human-AI Interaction via Five User Problem-Solving Styles",
    "doi": "https://doi.org/10.1145/3663740",
    "publication_date": "2024-05-08",
    "publication_year": 2024,
    "authors": "A. W. Anderson; Jimena Noa Guevara; Fatima Moussaoui; Tianyi Li; Mihaela Vorvoreanu; Margaret Burnett",
    "corresponding_authors": "",
    "abstract": "Motivations : Recent research has emerged on generally how to improve AI products’ human-AI interaction (HAI) user experience (UX), but relatively little is known about HAI-UX inclusivity. For example, what kinds of users are supported, and who are left out? What product changes would make it more inclusive? Objectives : To help fill this gap, we present an approach to measuring what kinds of diverse users an AI product leaves out and how to act upon that knowledge. To bring actionability to the results, the approach focuses on users’ problem-solving diversity. Thus, our specific objectives were (1) to show how the measure can reveal which participants with diverse problem-solving styles were left behind in a set of AI products and (2) to relate participants’ problem-solving diversity to their demographic diversity, specifically gender and age. Methods : We performed 18 experiments, discarding two that failed manipulation checks. Each experiment was a 2 \\(\\times\\) 2 factorial experiment with online participants, comparing two AI products: one deliberately violating 1 of 18 HAI guidelines and the other applying the same guideline. For our first objective, we used our measure to analyze how much each AI product gained/lost HAI-UX inclusivity compared to its counterpart, where inclusivity meant supportiveness to participants with particular problem-solving styles. For our second objective, we analyzed how participants’ problem-solving styles aligned with their gender identities and ages. Results and Implications : Participants’ diverse problem-solving styles revealed six types of inclusivity results: (1) the AI products that followed an HAI guideline were almost always more inclusive across diversity of problem-solving styles than the products that did not follow that guideline—but “who” got most of the inclusivity varied widely by guideline and by problem-solving style; (2) when an AI product had risk implications, four variables’ values varied in tandem: participants’ feelings of control, their (lack of) suspicion, their trust in the product, and their certainty while using the product; (3) the more control an AI product offered users, the more inclusive it was; (4) whether an AI product was learning from “my” data or other people’s affected how inclusive that product was; (5) participants’ problem-solving styles skewed differently by gender and age group; and (6) almost all of the results suggested actions that HAI practitioners could take to improve their products’ inclusivity further. Together, these results suggest that a key to improving the demographic inclusivity of an AI product (e.g., across a wide range of genders, ages) can often be obtained by improving the product’s support of diverse problem-solving styles.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4396733043",
    "type": "article"
  },
  {
    "title": "Affectionate Interaction with a Small Humanoid Robot Capable of Recognizing Social Touch Behavior",
    "doi": "https://doi.org/10.1145/2685395",
    "publication_date": "2014-12-19",
    "publication_year": 2014,
    "authors": "Martin Cooney; Shuichi Nishio; Hiroshi Ishiguro",
    "corresponding_authors": "",
    "abstract": "Activity recognition, involving a capability to recognize people's behavior and its underlying significance, will play a crucial role in facilitating the integration of interactive robotic artifacts into everyday human environments. In particular, social intelligence in recognizing affectionate behavior will offer value by allowing companion robots to bond meaningfully with interacting persons. The current article addresses the issue of designing an affectionate haptic interaction between a person and a companion robot by exploring how a small humanoid robot can behave to elicit affection while recognizing touches. We report on an experiment conducted to gain insight into how people perceive three fundamental interactive strategies in which a robot is either always highly affectionate, appropriately affectionate, or superficially unaffectionate (emphasizing positivity, contingency, and challenge, respectively). Results provide insight into the structure of affectionate interaction between humans and humanoid robots—underlining the importance of an interaction design expressing sincere liking, stability and variation—and suggest the usefulness of novel modalities such as warmth and cold.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2031833949",
    "type": "article"
  },
  {
    "title": "It’s Not Just about Accuracy",
    "doi": "https://doi.org/10.1145/3181673",
    "publication_date": "2018-07-13",
    "publication_year": 2018,
    "authors": "Tracy Hammond; Shalini Priya Ashok Kumar; Matthew Runyon; Josh Cherian; Blake Williford; Swarna Keshavabhotla; Stephanie Valentine; Wayne Li; Julie Linsey",
    "corresponding_authors": "",
    "abstract": "Design sketching is an important skill for designers, engineers, and creative professionals, as it allows them to express their ideas and concepts in a visual medium. Being a critical and versatile skill for many different disciplines, courses on design sketching are often taught in universities. Courses today predominately rely on pen and paper; however, this traditional pedagogy is limited by the availability of human instructors, who can provide personalized feedback. Using a stylus-based intelligent tutoring system called SketchTivity , we aim to eventually mimic the feedback given by an instructor and assess student-drawn sketches to give students insight into areas for improvement. To provide effective feedback to users, it is important to identify what aspects of their sketches they should work on to improve their sketching ability. After consulting with several domain experts in sketching, we came up with several classes of features that could potentially differentiate expert and novice sketches. Because improvement on one metric, such as speed, may result in a decrease in another metric, such as accuracy, the creation of a single score may not mean much to the user. We attempted to create a single internal score that represents overall drawing skill so that the system can track improvement over time and found that this score correlates highly with expert rankings. We gathered over 2,000 sketches from 20 novices and four experts for analysis. We identified key metrics for quality assessment that were shown to significantly correlate with the quality of expert sketches and provide insight into providing intelligent user feedback in the future.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2883852503",
    "type": "article"
  },
  {
    "title": "Characterizing and Predicting the Multifaceted Nature of Quality in Educational Web Resources",
    "doi": "https://doi.org/10.1145/2533670.2533673",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Philipp Wetzler; Steven Bethard; Heather Leary; Kirsten R. Butcher; Soheil Danesh Bahreini; Jin Zhao; James Martin; Tamara Sumner",
    "corresponding_authors": "",
    "abstract": "Efficient learning from Web resources can depend on accurately assessing the quality of each resource. We present a methodology for developing computational models of quality that can assist users in assessing Web resources. The methodology consists of four steps: 1) a meta-analysis of previous studies to decompose quality into high-level dimensions and low-level indicators, 2) an expert study to identify the key low-level indicators of quality in the target domain, 3) human annotation to provide a collection of example resources where the presence or absence of quality indicators has been tagged, and 4) training of a machine learning model to predict quality indicators based on content and link features of Web resources. We find that quality is a multifaceted construct, with different aspects that may be important to different users at different times. We show that machine learning models can predict this multifaceted nature of quality, both in the context of aiding curators as they evaluate resources submitted to digital libraries, and in the context of aiding teachers as they develop online educational resources. Finally, we demonstrate how computational models of quality can be provided as a service, and embedded into applications such as Web search.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2133162081",
    "type": "article"
  },
  {
    "title": "Generating Robot Gaze on the Basis of Participation Roles and Dominance Estimation in Multiparty Interaction",
    "doi": "https://doi.org/10.1145/2743028",
    "publication_date": "2015-12-22",
    "publication_year": 2015,
    "authors": "Yukiko Nakano; Takashi Yoshino; Misato Yatsushiro; Yutaka Takase",
    "corresponding_authors": "",
    "abstract": "Gaze is an important nonverbal feedback signal in multiparty face-to-face conversations. It is well known that gaze behaviors differ depending on participation role: speaker, addressee, or side participant. In this study, we focus on dominance as another factor that affects gaze. First, we conducted an empirical study and analyzed its results that showed how gaze behaviors are affected by both dominance and participation roles. Then, using speech and gaze information that was statistically significant for distinguishing the more dominant and less dominant person in an empirical study, we established a regression-based model for estimating conversational dominance. On the basis of the model, we implemented a dominance estimation mechanism that processes online speech and head direction data. Then we applied our findings to human-robot interaction. To design robot gaze behaviors, we analyzed gaze transitions with respect to participation roles and dominance and implemented gaze-transition models as robot gaze behavior generation rules. Finally, we evaluated a humanoid robot that has dominance estimation functionality and determines its gaze based on the gaze models, and we found that dominant participants had a better impression of less dominant robot gaze behaviors. This suggests that a robot using our gaze models was preferred to a robot that was simply looking at the speaker. We have demonstrated the importance of considering dominance in human-robot multiparty interaction.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2201256059",
    "type": "article"
  },
  {
    "title": "Proactive Information Retrieval by Capturing Search Intent from Primary Task Context",
    "doi": "https://doi.org/10.1145/3150975",
    "publication_date": "2018-07-05",
    "publication_year": 2018,
    "authors": "Markus Koskela; Petri Luukkonen; Tuukka Ruotsalo; Mats Sjöberg; Patrik Floréen",
    "corresponding_authors": "",
    "abstract": "A significant fraction of information searches are motivated by the user’s primary task . An ideal search engine would be able to use information captured from the primary task to proactively retrieve useful information. Previous work has shown that many information retrieval activities depend on the primary task in which the retrieved information is to be used, but fairly little research has been focusing on methods that automatically learn the informational intents from the primary task context. We study how the implicit primary task context can be used to model the user’s search intent and to proactively retrieve relevant and useful information. Data comprising of logs from a user study, in which users are writing an essay, demonstrate that users’ search intents can be captured from the task and relevant and useful information can be proactively retrieved. Data from simulations with several datasets of different complexity show that the proposed approach of using primary task context generalizes to a variety of data. Our findings have implications for the design of proactive search systems that can infer users’ search intent implicitly by monitoring users’ primary task activities.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2811593923",
    "type": "article"
  },
  {
    "title": "A Data-Driven Approach to Designing for Privacy in Household IoT",
    "doi": "https://doi.org/10.1145/3241378",
    "publication_date": "2019-09-26",
    "publication_year": 2019,
    "authors": "Yangyang He; Paritosh Bahirat; Bart P. Knijnenburg; Abhilash Menon",
    "corresponding_authors": "",
    "abstract": "In this article, we extend and improve upon a previously developed data-driven approach to design privacy-setting interfaces for users of household IoT devices. The essence of this approach is to gather users’ feedback on household IoT scenarios before developing the interface, which allows us to create a navigational structure that preemptively maximizes users’ efficiency in expressing their privacy preferences, and develop a series of ‘privacy profiles’ that allow users to express a complex set of privacy preferences with the single click of a button. We expand upon the existing approach by proposing a more sophisticated translation of statistical results into interface design, and by extensively discussing and analyzing the tradeoff between user-model parsimony and accuracy in developing privacy profiles and default settings.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2975111084",
    "type": "article"
  },
  {
    "title": "Agents Vs. Users",
    "doi": "https://doi.org/10.1145/2946794",
    "publication_date": "2016-07-20",
    "publication_year": 2016,
    "authors": "Katrien Verbert; Denis Parra; Peter Brusilovsky",
    "corresponding_authors": "",
    "abstract": "Several approaches have been researched to help people deal with abundance of information. An important feature pioneered by social tagging systems and later used in other kinds of social systems is the ability to explore different community relevance prospects by examining items bookmarked by a specific user or items associated by various users with a specific tag . A ranked list of recommended items offered by a specific recommender engine can be considered as another relevance prospect. The problem that we address is that existing personalized social systems do not allow their users to explore and combine multiple relevance prospects. Only one prospect can be explored at any given time—a list of recommended items, a list of items bookmarked by a specific user, or a list of items marked with a specific tag. In this article, we explore the notion of combining multiple relevance prospects as a way to increase effectiveness and trust. We used a visual approach to recommend articles at a conference by explicitly presenting multiple dimensions of relevance. Suggestions offered by different recommendation techniques were embodied as recommender agents to put them on the same ground as users and tags. The results of two user studies performed at academic conferences allowed us to obtain interesting insights to enhance user interfaces of personalized social systems. More specifically, effectiveness and probability of item selection increase when users are able to explore and interrelate prospects of items relevance—that is, items bookmarked by users, recommendations and tags. Nevertheless, a less-technical audience may require guidance to understand the rationale of such intersections.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2483041185",
    "type": "article"
  },
  {
    "title": "A Multimodal Approach to Assessing User Experiences with Agent Helpers",
    "doi": "https://doi.org/10.1145/2983926",
    "publication_date": "2016-11-19",
    "publication_year": 2016,
    "authors": "Leigh Clark; Abdulmalik Yusuf Ofemile; Svenja Adolphs; Tom Rodden",
    "corresponding_authors": "",
    "abstract": "The study of agent helpers using linguistic strategies such as vague language and politeness has often come across obstacles. One of these is the quality of the agent's voice and its lack of appropriate fit for using these strategies. The first approach of this article compares human vs. synthesised voices in agents using vague language. This approach analyses the 60,000-word text corpus of participant interviews to investigate the differences of user attitudes towards the agents, their voices and their use of vague language. It discovers that while the acceptance of vague language is still met with resistance in agent instructors, using a human voice yields more positive results than the synthesised alternatives. The second approach in this article discusses the development of a novel multimodal corpus of video and text data to create multiple analyses of human-agent interaction in agent-instructed assembly tasks. The second approach analyses user spontaneous facial actions and gestures during their interaction in the tasks. It found that agents are able to elicit these facial actions and gestures and posits that further analysis of this nonverbal feedback may help to create a more adaptive agent. Finally, the approaches used in this article suggest these can contribute to furthering the understanding of what it means to interact with software agents.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2551929891",
    "type": "article"
  },
  {
    "title": "Predicting Visual Search Task Success from Eye Gaze Data as a Basis for User-Adaptive Information Visualization Systems",
    "doi": "https://doi.org/10.1145/3446638",
    "publication_date": "2021-05-20",
    "publication_year": 2021,
    "authors": "Moritz Spiller; Ying‐Hsang Liu; Md Zakir Hossain; Tom Gedeon; Julia Geißler; Andreas Nürnberger",
    "corresponding_authors": "",
    "abstract": "Information visualizations are an efficient means to support the users in understanding large amounts of complex, interconnected data; user comprehension, however, depends on individual factors such as their cognitive abilities. The research literature provides evidence that user-adaptive information visualizations positively impact the users' performance in visualization tasks. This study attempts to contribute toward the development of a computational model to predict the users' success in visual search tasks from eye gaze data and thereby drive such user-adaptive systems. State-of-the-art deep learning models for time series classification have been trained on sequential eye gaze data obtained from 40 study participants' interaction with a circular and an organizational graph. The results suggest that such models yield higher accuracy than a baseline classifier and previously used models for this purpose. In particular, a Multivariate Long Short Term Memory Fully Convolutional Network shows encouraging performance for its use in online user-adaptive systems. Given this finding, such a computational model can infer the users' need for support during interaction with a graph and trigger appropriate interventions in user-adaptive information visualization systems. This facilitates the design of such systems since further interaction data like mouse clicks is not required.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W3165133123",
    "type": "article"
  },
  {
    "title": "Access to multimodal articles for individuals with sight impairments",
    "doi": "https://doi.org/10.1145/2395123.2395126",
    "publication_date": "2012-12-01",
    "publication_year": 2012,
    "authors": "Sandra Carberry; Stephanie Elzer Schwartz; Kathleen McCoy; Şeniz Demir; Peng Wu; Charles Greenbacker; Daniel Chester; Edward J. Schwartz; David M. Oliver; Priscilla Moraes",
    "corresponding_authors": "",
    "abstract": "Although intelligent interactive systems have been the focus of many research efforts, very few have addressed systems for individuals with disabilities. This article presents our methodology for an intelligent interactive system that provides individuals with sight impairments with access to the content of information graphics (such as bar charts and line graphs) in popular media. The article describes the methodology underlying the system's intelligent behavior, its interface for interacting with users, examples processed by the implemented system, and evaluation studies both of the methodology and the effectiveness of the overall system. This research advances universal access to electronic documents.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W1979339135",
    "type": "article"
  },
  {
    "title": "A Computational Framework for Media Bias Mitigation",
    "doi": "https://doi.org/10.1145/2209310.2209311",
    "publication_date": "2012-06-01",
    "publication_year": 2012,
    "authors": "Souneil Park; Seungwoo Kang; Sangyoung Chung; Junehwa Song",
    "corresponding_authors": "",
    "abstract": "Bias in the news media is an inherent flaw of the news production process. The bias often causes a sharp increase in political polarization and in the cost of conflict on social issues such as the Iraq war. This article presents NewsCube, a novel Internet news service which aims to mitigate the effect of media bias. NewsCube automatically creates and promptly provides readers with multiple classified views on a news event. As such, it helps readers understand the event from a plurality of views and to formulate their own, more balanced, viewpoints. The media bias problem has been studied extensively in mass communications and social science. This article reviews related mass communication and journalism studies and provides a structured view of the media bias problem and its solution. We propose media bias mitigation as a practical solution and demonstrate it through NewsCube. We evaluate and discuss the effectiveness of NewsCube through various performance studies.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W1984948218",
    "type": "article"
  },
  {
    "title": "VisForum",
    "doi": "https://doi.org/10.1145/3162075",
    "publication_date": "2018-02-20",
    "publication_year": 2018,
    "authors": "Siwei Fu; Yong Wang; Yi Yang; Qing-Qing Bi; Fangzhou Guo; Huamin Qu",
    "corresponding_authors": "",
    "abstract": "User grouping in asynchronous online forums is a common phenomenon nowadays. People with similar backgrounds or shared interests like to get together in group discussions. As tens of thousands of archived conversational posts accumulate, challenges emerge for forum administrators and analysts to effectively explore user groups in large-volume threads and gain meaningful insights into the hierarchical discussions. Identifying and comparing groups in discussion threads are nontrivial, since the number of users and posts increases with time and noises may hamper the detection of user groups. Researchers in data mining fields have proposed a large body of algorithms to explore user grouping. However, the mining result is not intuitive to understand and difficult for users to explore the details. To address these issues, we present VisForum, a visual analytic system allowing people to interactively explore user groups in a forum. We work closely with two educators who have released courses in Massive Open Online Courses (MOOC) platforms to compile a list of design goals to guide our design. Then, we design and implement a multi-coordinated interface as well as several novel glyphs, i.e., group glyph, user glyph, and set glyph, with different granularities. Accordingly, we propose the group Detecting 8 Sorting Algorithm to reduce noises in a collection of posts, and employ the concept of “forum-index” for users to identify high-impact forum members. Two case studies using real-world datasets demonstrate the usefulness of the system and the effectiveness of novel glyph designs. Furthermore, we conduct an in-lab user study to present the usability of VisForum.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2793981077",
    "type": "article"
  },
  {
    "title": "Evaluation of Facial Expression Recognition by a Smart Eyewear for Facial Direction Changes, Repeatability, and Positional Drift",
    "doi": "https://doi.org/10.1145/3012941",
    "publication_date": "2017-12-13",
    "publication_year": 2017,
    "authors": "Katsutoshi Masai; Kai Kunze; Yuta Sugiura; Masa Ogata; Masahiko İnami; Maki Sugimoto",
    "corresponding_authors": "",
    "abstract": "This article presents a novel smart eyewear that recognizes the wearer’s facial expressions in daily scenarios. Our device uses embedded photo-reflective sensors and machine learning to recognize the wearer’s facial expressions. Our approach focuses on skin deformations around the eyes that occur when the wearer changes his or her facial expressions. With small photo-reflective sensors, we measure the distances between the skin surface on the face and the 17 sensors embedded in the eyewear frame. A Support Vector Machine (SVM) algorithm is then applied to the information collected by the sensors. The sensors can cover various facial muscle movements. In addition, they are small and light enough to be integrated into daily-use glasses. Our evaluation of the device shows the robustness to the noises from the wearer’s facial direction changes and the slight changes in the glasses’ position, as well as the reliability of the device’s recognition capacity. The main contributions of our work are as follows: (1) We evaluated the recognition accuracy in daily scenes, showing 92.8% accuracy regardless of facial direction and removal/remount. Our device can recognize facial expressions with 78.1% accuracy for repeatability and 87.7% accuracy in case of its positional drift. (2) We designed and implemented the device by taking usability and social acceptability into account. The device looks like a conventional eyewear so that users can wear it anytime, anywhere. (3) Initial field trials in a daily life setting were undertaken to test the usability of the device. Our work is one of the first attempts to recognize and evaluate a variety of facial expressions with an unobtrusive wearable device.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2775793881",
    "type": "article"
  },
  {
    "title": "The Effect of Embodied Interaction in Visual-Spatial Navigation",
    "doi": "https://doi.org/10.1145/2953887",
    "publication_date": "2016-12-19",
    "publication_year": 2016,
    "authors": "Ting Zhang; Yuting Li; Juan Wachs",
    "corresponding_authors": "",
    "abstract": "This article aims to assess the effect of embodied interaction on attention during the process of solving spatio-visual navigation problems. It presents a method that links operator's physical interaction, feedback, and attention. Attention is inferred through networks called Bayesian Attentional Networks (BANs). BANs are structures that describe cause-effect relationship between attention and physical action. Then, a utility function is used to determine the best combination of interaction modalities and feedback. Experiments involving five physical interaction modalities (vision-based gesture interaction, glove-based gesture interaction, speech, feet, and body stance) and two feedback modalities (visual and sound) are described. The main findings are: (i) physical expressions have an effect in the quality of the solutions to spatial navigation problems; (ii) the combination of feet gestures with visual feedback provides the best task performance.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2567499856",
    "type": "article"
  },
  {
    "title": "Exploring a Design Space of Graphical Adaptive Menus",
    "doi": "https://doi.org/10.1145/3237190",
    "publication_date": "2019-07-29",
    "publication_year": 2019,
    "authors": "Jean Vanderdonckt; Sara Bouzit; Gaëlle Calvary; Denis Chêne",
    "corresponding_authors": "",
    "abstract": "Graphical Adaptive Menus are Graphical User Interface menus whose predicted items of immediate use can be automatically rendered in a prediction window. Rendering this prediction window is a key question for adaptivity to enable the end-user to efficiently differentiate predicted items from normal ones and to consequently select appropriate items. Adaptivity for graphical menus has been investigated more for normal screens, such as desktops, than for small screens, such as smartphones, where real estate imposes severe rendering constraints. To address this question, this article defines and explores a design space where graphical adaptive menus are structured based on Bertin’s eight visual variables (i.e., position, size, shape, value, color, orientation, texture, and motion) and their combination by comparing their rendering for small screens with respect to normal screens. Based on this design space, previously introduced graphical adaptive menus are revisited in terms of four stability properties (i.e., spatial, physical, format, and temporal), and new menu designs are introduced and discussed for both normal and small screens. The resulting set of graphical adaptive menu has been subject to a preference analysis from which a particular design emerged: the cloud menu, where predicted items are arranged in an adaptive tag cloud. We investigate empirically the effect of the cloud menu on the item selection time and the error rate with respect to a static menu and an adaptive linear menu. This article then suggests a set of usability guidelines for designers and practitioners to design graphical adaptive menus in general and cloud menus in particular.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2967625944",
    "type": "article"
  },
  {
    "title": "Automatic Analysis of Naturalistic Hand-Over-Face Gestures",
    "doi": "https://doi.org/10.1145/2946796",
    "publication_date": "2016-07-20",
    "publication_year": 2016,
    "authors": "Marwa Mahmoud; Tadas Baltrušaitis; Peter Robinson",
    "corresponding_authors": "",
    "abstract": "One of the main factors that limit the accuracy of facial analysis systems is hand occlusion. As the face becomes occluded, facial features are lost, corrupted, or erroneously detected. Hand-over-face occlusions are considered not only very common but also very challenging to handle. However, there is empirical evidence that some of these hand-over-face gestures serve as cues for recognition of cognitive mental states. In this article, we present an analysis of automatic detection and classification of hand-over-face gestures. We detect hand-over-face occlusions and classify hand-over-face gesture descriptors in videos of natural expressions using multi-modal fusion of different state-of-the-art spatial and spatio-temporal features. We show experimentally that we can successfully detect face occlusions with an accuracy of 83%. We also demonstrate that we can classify gesture descriptors ( hand shape , hand action , and facial region occluded ) significantly better than a naïve baseline. Our detailed quantitative analysis sheds some light on the challenges of automatic classification of hand-over-face gestures in natural expressions.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2482322722",
    "type": "article"
  },
  {
    "title": "Adaptive Driving Assistant Model (ADAM) for Advising Drivers of Autonomous Vehicles",
    "doi": "https://doi.org/10.1145/3545994",
    "publication_date": "2022-07-04",
    "publication_year": 2022,
    "authors": "Sheng‐Jen Hsieh; Andy R. Wang; Anna Madison; Chad C. Tossell; Ewart J. de Visser",
    "corresponding_authors": "",
    "abstract": "Fully autonomous driving is on the horizon; vehicles with advanced driver assistance systems (ADAS) such as Tesla's Autopilot are already available to consumers. However, all currently available ADAS applications require a human driver to be alert and ready to take control if needed. Partially automated driving introduces new complexities to human interactions with cars and can even increase collision risk. A better understanding of drivers’ trust in automation may help reduce these complexities. Much of the existing research on trust in ADAS has relied on use of surveys and physiological measures to assess trust and has been conducted using driving simulators. There have been relatively few studies that use telemetry data from real automated vehicles to assess trust in ADAS. In addition, although some ADAS technologies provide alerts when, for example, drivers’ hands are not on the steering wheel, these systems are not personalized to individual drivers. Needed are adaptive technologies that can help drivers of autonomous vehicles avoid crashes based on multiple real-time data streams. In this paper, we propose an architecture for adaptive autonomous driving assistance. Two layers of multiple sensory fusion models are developed to provide appropriate voice reminders to increase driving safety based on predicted driving status. Results suggest that human trust in automation can be quantified and predicted with 80% accuracy based on vehicle data, and that adaptive speech-based advice can be provided to drivers with 90 to 95% accuracy. With more data, these models can be used to evaluate trust in driving assistance tools, which can ultimately lead to safer and appropriate use of these features.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4283819297",
    "type": "article"
  },
  {
    "title": "How to Support Users in Understanding Intelligent Systems? An Analysis and Conceptual Framework of User Questions Considering User Mindsets, Involvement, and Knowledge Outcomes",
    "doi": "https://doi.org/10.1145/3519264",
    "publication_date": "2022-04-28",
    "publication_year": 2022,
    "authors": "Daniel Buschek; Malin Eiband; Heinrich Hußmann",
    "corresponding_authors": "",
    "abstract": "The opaque nature of many intelligent systems violates established usability principles and thus presents a challenge for human-computer interaction. Research in the field therefore highlights the need for transparency, scrutability, intelligibility, interpretability and explainability, among others. While all of these terms carry a vision of supporting users in understanding intelligent systems, the underlying notions and assumptions about users and their interaction with the system often remain unclear. We review the literature in HCI through the lens of implied user questions to synthesise a conceptual framework integrating user mindsets, user involvement, and knowledge outcomes to reveal, differentiate and classify current notions in prior work. This framework aims to resolve conceptual ambiguity in the field and enables researchers to clarify their assumptions and become aware of those made in prior work. We further discuss related aspects such as stakeholders and trust, and also provide material to apply our framework in practice (e.g., ideation/design sessions). We thus hope to advance and structure the dialogue on supporting users in understanding intelligent systems.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4293236901",
    "type": "article"
  },
  {
    "title": "GRAFS: Graphical Faceted Search System to Support Conceptual Understanding in Exploratory Search",
    "doi": "https://doi.org/10.1145/3588319",
    "publication_date": "2023-03-31",
    "publication_year": 2023,
    "authors": "Mengtian Guo; Zhilan Zhou; David Gotz; Yue Wang",
    "corresponding_authors": "",
    "abstract": "When people search for information about a new topic within large document collections, they implicitly construct a mental model of the unfamiliar information space to represent what they currently know and guide their exploration into the unknown. Building this mental model can be challenging as it requires not only finding relevant documents but also synthesizing important concepts and the relationships that connect those concepts both within and across documents. This article describes a novel interactive approach designed to help users construct a mental model of an unfamiliar information space during exploratory search. We propose a new semantic search system to organize and visualize important concepts and their relations for a set of search results. A user study ( n =20) was conducted to compare the proposed approach against a baseline faceted search system on exploratory literature search tasks. Experimental results show that the proposed approach is more effective in helping users recognize relationships between key concepts, leading to a more sophisticated understanding of the search topic while maintaining similar functionality and usability as a faceted search system.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4362470113",
    "type": "article"
  },
  {
    "title": "Ajna: A Wearable Shared Perception System for Extreme Sensemaking",
    "doi": "https://doi.org/10.1145/3690829",
    "publication_date": "2024-09-02",
    "publication_year": 2024,
    "authors": "Matthew Wilchek; Kurt Luther; Feras A. Batarseh",
    "corresponding_authors": "",
    "abstract": "This paper introduces the design and prototype of Ajna, a wearable shared perception system for supporting extreme sensemaking in emergency scenarios. Ajna addresses technical challenges in Augmented Reality (AR) devices, specifically the limitations of depth sensors and cameras. These limitations confine object detection to close proximity and hinder perception beyond immediate surroundings, through obstructions, or across different structural levels, impacting collaborative use. It harnesses the Inertial Measurement Unit (IMU) in AR devices to measure users’ relative distances from a set physical point, enabling object detection sharing among multiple users across obstacles like walls and over distances. We tested Ajna's effectiveness in a controlled study with 15 participants simulating emergency situations in a multi-story building. We found that Ajna improved object detection, location awareness, and situational awareness, and reduced search times by 15%. Ajna's performance in simulated environments highlights the potential of artificial intelligence (AI) to enhance sensemaking in critical situations, offering insights for law enforcement, search and rescue, and infrastructure management.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4402131251",
    "type": "article"
  },
  {
    "title": "Introduction to the Transactions on Interactive Intelligent Systems",
    "doi": "https://doi.org/10.1145/2030365.2030366",
    "publication_date": "2011-10-01",
    "publication_year": 2011,
    "authors": "Anthony Jameson; John Riedl",
    "corresponding_authors": "",
    "abstract": "This editorial introduction describes the aims and scope of the ACM Transactions on Interactive Intelligent Systems , explains how it aims to constitute a landmark addition to the publication landscape, and shows how the five articles in this inaugural issue fit into the journal's conception.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W1975815871",
    "type": "article"
  },
  {
    "title": "An English-Language Argumentation Interface for Explanation Generation with Markov Decision Processes in the Domain of Academic Advising",
    "doi": "https://doi.org/10.1145/2513564",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Thomas B. Dodson; Nicholas Mattei; Joshua T. Guerin; Judy Goldsmith",
    "corresponding_authors": "",
    "abstract": "A Markov Decision Process (MDP) policy presents, for each state, an action, which preferably maximizes the expected utility accrual over time. In this article, we present a novel explanation system for MDP policies. The system interactively generates conversational English-language explanations of the actions suggested by an optimal policy, and does so in real time. We rely on natural language explanations in order to build trust between the user and the explanation system, leveraging existing research in psychology in order to generate salient explanations. Our explanation system is designed for portability between domains and uses a combination of domain-specific and domain-independent techniques. The system automatically extracts implicit knowledge from an MDP model and accompanying policy. This MDP-based explanation system can be ported between applications without additional effort by knowledge engineers or model builders. Our system separates domain-specific data from the explanation logic, allowing for a robust system capable of incremental upgrades. Domain-specific explanations are generated through case-based explanation techniques specific to the domain and a knowledge base of concept mappings used to generate English-language explanations.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2125241857",
    "type": "article"
  },
  {
    "title": "Supporting Exploratory Search with a Visual User-Driven Approach",
    "doi": "https://doi.org/10.1145/3009976",
    "publication_date": "2017-12-13",
    "publication_year": 2017,
    "authors": "Cecilia di Sciascio; Vedran Sabol; Eduardo Veas",
    "corresponding_authors": "",
    "abstract": "Whenever users engage in gathering and organizing new information, searching and browsing activities emerge at the core of the exploration process. As the process unfolds and new knowledge is acquired, interest drifts occur inevitably and need to be accounted for. Despite the advances in retrieval and recommender algorithms, real-world interfaces have remained largely unchanged: results are delivered in a relevance-ranked list. However, it quickly becomes cumbersome to reorganize resources along new interests, as any new search brings new results. We introduce an interactive user-driven tool that aims at supporting users in understanding, refining, and reorganizing documents on the fly as information needs evolve. Decisions regarding visual and interactive design aspects are tightly grounded on a conceptual model for exploratory search. In other words, the different views in the user interface address stages of awareness, exploration, and explanation unfolding along the discovery process, supported by a set of text-mining methods. A formal evaluation showed that gathering items relevant to a particular topic of interest with our tool incurs in a lower cognitive load compared to a traditional ranked list. A second study reports on usage patterns and usability of the various interaction techniques within a free, unsupervised setting.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2773585006",
    "type": "article"
  },
  {
    "title": "A Comparison of Adaptive View Techniques for Exploratory 3D Drone Teleoperation",
    "doi": "https://doi.org/10.1145/3232232",
    "publication_date": "2019-03-18",
    "publication_year": 2019,
    "authors": "John Thomason; Photchara Ratsamee; Jason Orlosky; Kiyoshi Kiyokawa; Tomohiro Mashita; Yuki Uranishi; Haruo Takemura",
    "corresponding_authors": "",
    "abstract": "Drone navigation in complex environments poses many problems to teleoperators. Especially in three dimensional (3D) structures such as buildings or tunnels, viewpoints are often limited to the drone’s current camera view, nearby objects can be collision hazards, and frequent occlusion can hinder accurate manipulation. To address these issues, we have developed a novel interface for teleoperation that provides a user with environment-adaptive viewpoints that are automatically configured to improve safety and provide smooth operation. This real-time adaptive viewpoint system takes robot position, orientation, and 3D point-cloud information into account to modify the user’s viewpoint to maximize visibility. Our prototype uses simultaneous localization and mapping (SLAM) based reconstruction with an omnidirectional camera, and we use the resulting models as well as simulations in a series of preliminary experiments testing navigation of various structures. Results suggest that automatic viewpoint generation can outperform first- and third-person view interfaces for virtual teleoperators in terms of ease of control and accuracy of robot operation.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2924348398",
    "type": "article"
  },
  {
    "title": "Interactive Statistics with <i>Illmo</i>",
    "doi": "https://doi.org/10.1145/2509108",
    "publication_date": "2014-04-01",
    "publication_year": 2014,
    "authors": "Jean-Bernard Martens",
    "corresponding_authors": "Jean-Bernard Martens",
    "abstract": "Progress in empirical research relies on adequate statistical analysis and reporting. This article proposes an alternative approach to statistical modeling that is based on an old but mostly forgotten idea, namely Thurstone modeling. Traditional statistical methods assume that either the measured data, in the case of parametric statistics, or the rank-order transformed data, in the case of nonparametric statistics, are samples from a specific (usually Gaussian) distribution with unknown parameters. Consequently, such methods should not be applied when this assumption is not valid. Thurstone modeling similarly assumes the existence of an underlying process that obeys an a priori assumed distribution with unknown parameters, but combines this underlying process with a flexible response mechanism that can be either continuous or discrete and either linear or nonlinear. One important advantage of Thurstone modeling is that traditional statistical methods can still be applied on the underlying process, irrespective of the nature of the measured data itself. Another advantage is that Thurstone models can be graphically represented, which helps to communicate them to a broad audience. A new interactive statistical package, Interactive Log Likelihood MOdeling ( Illmo ), was specifically designed for estimating and rendering Thurstone models and is intended to bring Thurstone modeling within the reach of persons who are not experts in statistics. Illmo is unique in the sense that it provides not only extensive graphical renderings of the data analysis results but also an interface for navigating between different model options. In this way, users can interactively explore different models and decide on an adequate balance between model complexity and agreement with the experimental data. Hypothesis testing on model parameters is also made intuitive and is supported by both textual and graphical feedback. The flexibility and ease of use of Illmo means that it is also potentially useful as a didactic tool for teaching statistics.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2004986590",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on Human Interaction with Artificial Advice Givers",
    "doi": "https://doi.org/10.1145/3014432",
    "publication_date": "2016-12-26",
    "publication_year": 2016,
    "authors": "Nava Tintarev; John O’Donovan; Alexander Felfernig",
    "corresponding_authors": "",
    "abstract": "Many interactive systems in today’s world can be viewed as providing advice to their users. Commercial examples include recommender systems, satellite navigation systems, intelligent personal assistants on smartphones, and automated checkout systems in supermarkets. We will call these systems that support people in making choices and decisions artificial advice givers (AAGs) : They propose and evaluate options while involving their human users in the decision-making process. This special issue addresses the challenge of improving the interaction between artificial and human agents. It answers the question of how an agent of each type (human and artificial) can influence and understand the reasoning, working models, and conclusions of the other agent by means of novel forms of interaction. To address this challenge, the articles in the special issue are organized around three themes: (a) human factors to consider when designing interactions with AAGs (e.g., over- and under-reliance, overestimation of the system’s capabilities), (b) methods for supporting interaction with AAGs (e.g., natural language, visualization, and argumentation), and (c) considerations for evaluating AAGs (both criteria and methodology for applying them).",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2562027261",
    "type": "article"
  },
  {
    "title": "Bi-Level Thresholding",
    "doi": "https://doi.org/10.1145/3181672",
    "publication_date": "2019-04-02",
    "publication_year": 2019,
    "authors": "Keiko Katsuragawa; Ankit Kamal; Qi Feng Liu; Matei Negulescu; Edward Lank",
    "corresponding_authors": "",
    "abstract": "In gesture recognition, one challenge that researchers and developers face is the need for recognition strategies that mediate between false positives and false negatives. In this article, we examine bi-level thresholding, a recognition strategy that uses two thresholds: a tighter threshold limits false positives and recognition errors, and a looser threshold prevents repeated errors (false negatives) by analyzing movements in sequence. We first describe early observations that led to the development of the bi-level thresholding algorithm. Next, using a Wizard-of-Oz recognizer, we hold recognition rates constant and adjust for fixed versus bi-level thresholding; we show that systems using bi-level thresholding result in significantly lower workload scores on the NASA-TLX and significantly lower accelerometer variance when performing gesture input. Finally, we examine the effect that bi-level thresholding has on a real-world dataset of wrist and finger gestures, showing an ability to significantly improve measures of precision and recall. Overall, these results argue for the viability of bi-level thresholding as an effective technique for balancing between false positives, recognition errors, and false negatives.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2935126934",
    "type": "article"
  },
  {
    "title": "EventAction",
    "doi": "https://doi.org/10.1145/3301402",
    "publication_date": "2019-08-09",
    "publication_year": 2019,
    "authors": "Fan Du; Catherine Plaisant; Neil Spring; Kenyon Crowley; Ben Shneiderman",
    "corresponding_authors": "",
    "abstract": "People use recommender systems to improve their decisions; for example, item recommender systems help them find films to watch or books to buy. Despite the ubiquity of item recommender systems, they can be improved by giving users greater transparency and control. This article develops and assesses interactive strategies for transparency and control, as applied to event sequence recommender systems, which provide guidance in critical life choices such as medical treatments, careers decisions, and educational course selections. This article’s main contribution is the use of both record attributes and temporal event information as features to identify similar records and provide appropriate recommendations. While traditional item recommendations are based on choices by people with similar attributes, such as those who looked at this product or watched this movie, our event sequence recommendation approach allows users to select records that share similar attribute values and start with a similar event sequence. Then users see how different choices of actions and the orders and times between them might lead to users’ desired outcomes. This paper applies a visual analytics approach to present and explain recommendations of event sequences. It presents a workflow for event sequence recommendation that is implemented in EventAction and reports on three case studies in two domains to illustrate the use of generating event sequence recommendations based on personal histories. It also offers design guidelines for the construction of user interfaces for event sequence recommendation and discusses ethical issues in dealing with personal histories. A demo video of EventAction is available at https://hcil.umd.edu/eventaction.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2969093670",
    "type": "article"
  },
  {
    "title": "Visual Exploration of Air Quality Data with a Time-correlation-partitioning Tree Based on Information Theory",
    "doi": "https://doi.org/10.1145/3182187",
    "publication_date": "2019-02-11",
    "publication_year": 2019,
    "authors": "Fangzhou Guo; Tianlong Gu; Wei Chen; Feiran Wu; Qi Wang; Lei Shi; Huamin Qu",
    "corresponding_authors": "",
    "abstract": "&lt;?tight?&gt;Discovering the correlations among variables of air quality data is challenging, because the correlation time series are long-lasting, multi-faceted, and information-sparse. In this article, we propose a novel visual representation, called Time-correlation-partitioning (TCP) tree, that compactly characterizes correlations of multiple air quality variables and their evolutions. A TCP tree is generated by partitioning the information-theoretic correlation time series into pieces with respect to the variable hierarchy and temporal variations, and reorganizing these pieces into a hierarchically nested structure. The visual exploration of a TCP tree provides a sparse data traversal of the correlation variations and a situation-aware analysis of correlations among variables. This can help meteorologists understand the correlations among air quality variables better. We demonstrate the efficiency of our approach in a real-world air quality investigation scenario.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2997798788",
    "type": "article"
  },
  {
    "title": "Algorithmic and HCI Aspects for Explaining Recommendations of Artistic Images",
    "doi": "https://doi.org/10.1145/3369396",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "Vicente Domı́nguez; Ivania Donoso-Guzmán; Pablo Messina; Denis Parra",
    "corresponding_authors": "",
    "abstract": "Explaining suggestions made by recommendation systems is key to make users trust and accept these systems. This is specially critical in areas such as art image recommendation. Traditionally, artworks are sold in galleries where people can see them physically, and artists have the chance to persuade the people into buying them. On the other side, online art stores only offer the user the action of navigating through the catalog, but nobody plays the persuading role of the artist. Moreover, few works in recommendation systems provide a perspective of the many variables involved in the user perception of several aspects of the system such as domain knowledge, relevance, explainability, and trust. In this article, we aim to fill this gap by studying several aspects of the user experience with a recommender system of artistic images, from algorithmic and HCI perspectives. We conducted two user studies in Amazon Mechanical Turk to evaluate different levels of explainability, combined with different algorithms. While in study 1 we focus only on a desktop interface, in study 2 we attempt to understand the effect of explanations in mobile devices. In general, our experiments confirm that explanations of recommendations in the image domain are useful and increase user satisfaction, perception of explainability and relevance. In the first study, our results show that the observed effects are dependent on the underlying recommendation algorithm used. In the second study, our results show that these effects are also dependent of the device used in the study but with a smaller effect. Finally, using the framework by Knijnenburg et al., we provide a comprehensive model, for each study, which synthesizes the effects between different variables involved in the user experience with explainable visual recommender systems of artistic images.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W3040731011",
    "type": "article"
  },
  {
    "title": "Exploratory Visual Analysis and Interactive Pattern Extraction from Semi-Structured Data",
    "doi": "https://doi.org/10.1145/2812115",
    "publication_date": "2015-09-08",
    "publication_year": 2015,
    "authors": "Axel J. Soto; Ryan Kiros; Vlado Kešelj; Evangelos Milios",
    "corresponding_authors": "",
    "abstract": "Semi-structured documents are a common type of data containing free text in natural language (unstructured data) as well as additional information about the document, or meta-data, typically following a schema or controlled vocabulary (structured data). Simultaneous analysis of unstructured and structured data enables the discovery of hidden relationships that cannot be identified from either of these sources when analyzed independently of each other. In this work, we present a visual text analytics tool for semi-structured documents (ViTA-SSD), that aims to support the user in the exploration and finding of insightful patterns in a visual and interactive manner in a semi-structured collection of documents. It achieves this goal by presenting to the user a set of coordinated visualizations that allows the linking of the metadata with interactively generated clusters of documents in such a way that relevant patterns can be easily spotted. The system contains two novel approaches in its back end: a feature-learning method to learn a compact representation of the corpus and a fast-clustering approach that has been redesigned to allow user supervision. These novel contributions make it possible for the user to interact with a large and dynamic document collection and to perform several text analytical tasks more efficiently. Finally, we present two use cases that illustrate the suitability of the system for in-depth interactive exploration of semi-structured document collections, two user studies, and results of several evaluations of our text-mining components.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W1973050785",
    "type": "article"
  },
  {
    "title": "Quantitative Study of Music Listening Behavior in a Smartphone Context",
    "doi": "https://doi.org/10.1145/2738220",
    "publication_date": "2015-09-08",
    "publication_year": 2015,
    "authors": "Yi‐Hsuan Yang; Yuan-Ching Teng",
    "corresponding_authors": "",
    "abstract": "Context-based services have attracted increasing attention because of the prevalence of sensor-rich mobile devices such as smartphones. The idea is to recommend information that a user would be interested in according to the user’s surrounding context. Although remarkable progress has been made to contextualize music playback, relatively little research has been made using a large collection of real-life listening records collected in situ . In light of this fact, we present in this article a quantitative study of the personal, situational, and musical factors of musical preference in a smartphone context, using a new dataset comprising the listening records and self-report context annotation of 48 participants collected over 3wk via an Android app. Although the number of participants is limited and the population is biased towards students, the dataset is unique in that it is collected in a daily context, with sensor data and music listening profiles recorded at the same time. We investigate 3 core research questions evaluating the strength of a rich set of low-level and high-level audio features for music usage auto-tagging (i.e., music preference in different user activities), the strength of time-domain and frequency-domain sensor features for user activity classification, and how user factors such as personality traits are correlated with the predictability of music usage and user activity, using a closed set of 8 activity classes. We provide an in-depth discussion of the main findings of this study and their implications for the development of context-based music services for smartphones.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2026836846",
    "type": "article"
  },
  {
    "title": "Design and Exploration of Mid-Air Authentication Gestures",
    "doi": "https://doi.org/10.1145/2832919",
    "publication_date": "2016-09-14",
    "publication_year": 2016,
    "authors": "Ilhan Aslan; Andreas Uhl; Alexander Meschtscherjakov; Manfred Tscheligi",
    "corresponding_authors": "",
    "abstract": "Authentication based on touchless mid-air gestures would benefit a multitude of ubiquitous computing applications, especially those that are used in clean environments (e.g., medical environments or clean rooms). In order to explore the potential of mid-air gestures for novel authentication approaches, we performed a series of studies and design experiments. First, we collected data from more then 200 users during a 3-day science event organized within a shopping mall. These data were used to investigate capabilities of the Leap Motion sensor, observe interaction in the wild, and to formulate an initial design problem. The design problem, as well as the design of mid-air gestures for authentication purposes, were iterated in subsequent design activities. In a final study with 13 participants, we evaluated two mid-air gestures for authentication purposes in different situations, including different body positions. Our results highlight a need for different mid-air gestures for differing situations and carefully chosen constraints for mid-air gestures. We conclude by proposing an exemplary system, which aims to provide tool-support for designers and engineers, allowing them to explore authentication gestures in the original context of use and thus support them with the design of contextual mid-air authentication gestures.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2521024136",
    "type": "article"
  },
  {
    "title": "Personality Sensing",
    "doi": "https://doi.org/10.1145/3357459",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "Ronnie Taib; Shlomo Berkovsky; Irena Koprinska; Eileen Wang; Yucheng Zeng; Jingjie Li",
    "corresponding_authors": "",
    "abstract": "Personality detection is an important task in psychology, as different personality traits are linked to different behaviours and real-life outcomes. Traditionally it involves filling out lengthy questionnaires, which is time-consuming, and may also be unreliable if respondents do not fully understand the questions or are not willing to honestly answer them. In this article, we propose a framework for objective personality detection that leverages humans’ physiological responses to external stimuli. We exemplify and evaluate the framework in a case study, where we expose subjects to affective image and video stimuli, and capture their physiological responses using non-invasive commercial-grade eye-tracking and skin conductivity sensors. These responses are then processed and used to build a machine learning classifier capable of accurately predicting a wide range of personality traits. We investigate and discuss the performance of various machine learning methods, the most and least accurately predicted traits, and also assess the importance of the different stimuli, features, and physiological signals. Our work demonstrates that personality traits can be accurately detected, suggesting the applicability of the proposed framework for robust personality detection and use by psychology practitioners and researchers, as well as designers of personalised interactive systems.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W3040959123",
    "type": "article"
  },
  {
    "title": "A Taxonomy of Property Measures to Unify Active Learning and Human-centered Approaches to Data Labeling",
    "doi": "https://doi.org/10.1145/3439333",
    "publication_date": "2021-09-03",
    "publication_year": 2021,
    "authors": "Jürgen Bernard; Marco Hutter; Michael Sedlmair; Matthias Zeppelzauer; Tamara Munzner",
    "corresponding_authors": "",
    "abstract": "Strategies for selecting the next data instance to label, in service of generating labeled data for machine learning, have been considered separately in the machine learning literature on active learning and in the visual analytics literature on human-centered approaches. We propose a unified design space for instance selection strategies to support detailed and fine-grained analysis covering both of these perspectives. We identify a concise set of 15 properties, namely measureable characteristics of datasets or of machine learning models applied to them, that cover most of the strategies in these literatures. To quantify these properties, we introduce Property Measures (PM) as fine-grained building blocks that can be used to formalize instance selection strategies. In addition, we present a taxonomy of PMs to support the description, evaluation, and generation of PMs across four dimensions: machine learning (ML) Model Output , Instance Relations , Measure Functionality , and Measure Valence . We also create computational infrastructure to support qualitative visual data analysis: a visual analytics explainer for PMs built around an implementation of PMs using cascades of eight atomic functions. It supports eight analysis tasks, covering the analysis of datasets and ML models using visual comparison within and between PMs and groups of PMs, and over time during the interactive labeling process. We iteratively refined the PM taxonomy, the explainer, and the task abstraction in parallel with each other during a two-year formative process, and show evidence of their utility through a summative evaluation with the same infrastructure. This research builds a formal baseline for the better understanding of the commonalities and differences of instance selection strategies, which can serve as the stepping stone for the synthesis of novel strategies in future work.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W3196524514",
    "type": "article"
  },
  {
    "title": "After-Action Review for AI (AAR/AI)",
    "doi": "https://doi.org/10.1145/3453173",
    "publication_date": "2021-09-03",
    "publication_year": 2021,
    "authors": "Jonathan Dodge; Roli Khanna; Jed Irvine; Kin-Ho Lam; Theresa Mai; Zhengxian Lin; Nicholas George Kiddle; Evan Newman; A. W. Anderson; Sai Raja; Caleb Matthews; Christopher Perdriau; Margaret Burnett; Alan Fern",
    "corresponding_authors": "",
    "abstract": "Explainable AI is growing in importance as AI pervades modern society, but few have studied how explainable AI can directly support people trying to assess an AI agent. Without a rigorous process, people may approach assessment in ad hoc ways—leading to the possibility of wide variations in assessment of the same agent due only to variations in their processes. AAR, or After-Action Review, is a method some military organizations use to assess human agents, and it has been validated in many domains. Drawing upon this strategy, we derived an After-Action Review for AI (AAR/AI), to organize ways people assess reinforcement learning agents in a sequential decision-making environment. We then investigated what AAR/AI brought to human assessors in two qualitative studies. The first investigated AAR/AI to gather formative information, and the second built upon the results, and also varied the type of explanation (model-free vs. model-based) used in the AAR/AI process. Among the results were the following: (1) participants reporting that AAR/AI helped to organize their thoughts and think logically about the agent, (2) AAR/AI encouraged participants to reason about the agent from a wide range of perspectives , and (3) participants were able to leverage AAR/AI with the model-based explanations to falsify the agent’s predictions.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W3197450565",
    "type": "article"
  },
  {
    "title": "Finding AI’s Faults with AAR/AI: An Empirical Study",
    "doi": "https://doi.org/10.1145/3487065",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Roli Khanna; Jonathan Dodge; A. W. Anderson; Rupika Dikkala; Jed Irvine; Zeyad Shureih; Kin-Ho Lam; Caleb Matthews; Zhengxian Lin; Minsuk Kahng; Alan Fern; Margaret Burnett",
    "corresponding_authors": "",
    "abstract": "Would you allow an AI agent to make decisions on your behalf? If the answer is “not always,” the next question becomes “in what circumstances”? Answering this question requires human users to be able to assess an AI agent—and not just with overall pass/fail assessments or statistics. Here users need to be able to localize an agent’s bugs so that they can determine when they are willing to rely on the agent and when they are not. After-Action Review for AI (AAR/AI), a new AI assessment process for integration with Explainable AI systems, aims to support human users in this endeavor, and in this article we empirically investigate AAR/AI’s effectiveness with domain-knowledgeable users. Our results show that AAR/AI participants not only located significantly more bugs than non-AAR/AI participants did (i.e., showed greater recall) but also located them more precisely (i.e., with greater precision). In fact, AAR/AI participants outperformed non-AAR/AI participants on every bug and were, on average, almost six times as likely as non-AAR/AI participants to find any particular bug. Finally, evidence suggests that incorporating labeling into the AAR/AI process may encourage domain-knowledgeable users to abstract above individual instances of bugs; we hypothesize that doing so may have contributed further to AAR/AI participants’ effectiveness.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4214851743",
    "type": "article"
  },
  {
    "title": "Detection and Recognition of Driver Distraction Using Multimodal Signals",
    "doi": "https://doi.org/10.1145/3519267",
    "publication_date": "2022-04-28",
    "publication_year": 2022,
    "authors": "Kapotaksha Das; Michalis Papakostas; Kais Riani; Andrew Brian Gasiorowski; Mohamed Abouelenien; Mihai Burzo; Rada Mihalcea",
    "corresponding_authors": "",
    "abstract": "Distracted driving is a leading cause of accidents worldwide. The tasks of distraction detection and recognition have been traditionally addressed as computer vision problems. However, distracted behaviors are not always expressed in a visually observable way. In this work, we introduce a novel multimodal dataset of distracted driver behaviors, consisting of data collected using twelve information channels coming from visual, acoustic, near-infrared, thermal, physiological and linguistic modalities. The data were collected from 45 subjects while being exposed to four different distractions (three cognitive and one physical). For the purposes of this paper, we performed experiments with visual, physiological, and thermal information to explore potential of multimodal modeling for distraction recognition. In addition, we analyze the value of different modalities by identifying specific visual, physiological, and thermal groups of features that contribute the most to distraction characterization. Our results highlight the advantage of multimodal representations and reveal valuable insights for the role played by the three modalities on identifying different types of driving distractions.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4293236873",
    "type": "article"
  },
  {
    "title": "Developing an AI-based Explainable Expert Support System for Art Therapy",
    "doi": "https://doi.org/10.1145/3689649",
    "publication_date": "2024-08-30",
    "publication_year": 2024,
    "authors": "Jiwon Kim; Jiwon Kang; Migyeong Yang; Chaehee Park; Taeeun Kim; Hayeon Song; Jinyoung Han",
    "corresponding_authors": "",
    "abstract": "Sketch-based drawing assessments in art therapy are widely used to understand individuals’ cognitive and psychological states, such as cognitive impairments or mental disorders. Along with self-reported measures based on questionnaires, psychological drawing assessments can augment information regarding an individual’s psychological state. Interpreting drawing assessments demands significant time and effort, particularly for large groups such as schools or companies, and relies on the expertise of art therapists. To address this issue, we propose an artificial intelligence (AI)-based expert support system called AlphaDAPR to support art therapists and psychologists in conducting large-scale automatic drawing assessments. In Study 1, we first investigated user experience in AlphaDAPR . Through surveys involving 64 art therapists, we observed a substantial willingness (64.06% of participants) in using the proposed system. Structural equation modeling highlighted the pivotal role of explainable AI in the interface design, affecting perceived usefulness, trust, satisfaction, and intention to use. However, our interviews unveiled a nuanced perspective: while many art therapists showed a strong inclination to use the proposed system, they also voiced concerns about potential AI limitations and risks. Since most concerns arose from insufficient trust, which was the focal point of our attention, we conducted Study 2 with the aim of enhancing trust. Study 2 delved deeper into the necessity of clear communication regarding the division of roles between AI and users for elevating trust. Through experimentation with another 26 art therapists, we demonstrated that clear communication enhances users’ trust in our system. Our work not only highlights the potential of AlphaDAPR to streamline drawing assessments but also underscores broader implications for human-AI collaboration in psychological domains. By addressing concerns and optimizing communication, we pave the way for a symbiotic relationship between AI and human expertise, ultimately enhancing the efficacy and accessibility of psychological assessment tools.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4402042561",
    "type": "article"
  },
  {
    "title": "AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy",
    "doi": "https://doi.org/10.1145/3707649",
    "publication_date": "2024-12-13",
    "publication_year": 2024,
    "authors": "Philipp Schoenegger; Peter S. Park; Ezra Karger; Sean Trott; Philip E. Tetlock",
    "corresponding_authors": "",
    "abstract": "Large language models (LLMs) match and sometimes exceed human performance in many domains. This study explores the potential of LLMs to augment human judgment in a forecasting task. We evaluate the effect on human forecasters of two LLM assistants: one designed to provide high-quality (“superforecasting”) advice, and the other designed to be overconfident and base-rate neglecting, thus providing noisy forecasting advice. We compare participants using these assistants to a control group that received a less advanced model that did not provide numerical predictions or engage in explicit discussion of predictions. Participants ( N \\(=\\) 991) answered a set of six forecasting questions and had the option to consult their assigned LLM assistant throughout. Our preregistered analyses show that interacting with each of our frontier LLM assistants significantly enhances prediction accuracy by between 24% and 28% compared to the control group. Exploratory analyses showed a pronounced outlier effect in one forecasting item, without which we find that the superforecasting assistant increased accuracy by 41%, compared with 29% for the noisy assistant. We further examine whether LLM forecasting augmentation disproportionately benefits less skilled forecasters, degrades the wisdom-of-the-crowd by reducing prediction diversity, or varies in effectiveness with question difficulty. Our data do not consistently support these hypotheses. Our results suggest that access to a frontier LLM assistant, even a noisy one, can be a helpful decision aid in cognitively demanding tasks compared to a less powerful model that does not provide specific forecasting advice. However, the effects of outliers suggest that further research into the robustness of this pattern is needed.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4405372230",
    "type": "article"
  },
  {
    "title": "Making Machine-Learning Applications for Time-Series Sensor Data Graphical and Interactive",
    "doi": "https://doi.org/10.1145/2983924",
    "publication_date": "2017-06-30",
    "publication_year": 2017,
    "authors": "SeungJun Kim; Dan Tasse; Anind K. Dey",
    "corresponding_authors": "",
    "abstract": "The recent profusion of sensors has given consumers and researchers the ability to collect significant amounts of data. However, understanding sensor data can be a challenge, because it is voluminous, multi-sourced, and unintelligible. Nonetheless, intelligent systems, such as activity recognition, require pattern analysis of sensor data streams to produce compelling results; machine learning (ML) applications enable this type of analysis. However, the number of ML experts able to proficiently classify sensor data is limited, and there remains a lack of interactive, usable tools to help intermediate users perform this type of analysis. To learn which features these tools must support, we conducted interviews with intermediate users of ML and conducted two probe-based studies with a prototype ML and visual analytics system, Gimlets. Our system implements ML applications for sensor-based time-series data as a novel domain-specific prototype that integrates interactive visual analytic features into the ML pipeline. We identify future directions for usable ML systems based on sensor data that will enable intermediate users to build systems that have been prohibitively difficult.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2741249634",
    "type": "article"
  },
  {
    "title": "Predicting User Confidence During Visual Decision Making",
    "doi": "https://doi.org/10.1145/3185524",
    "publication_date": "2018-06-13",
    "publication_year": 2018,
    "authors": "James E. Smith; Phil Legg; Milos Matovic; Kristofer Kinsey",
    "corresponding_authors": "",
    "abstract": "People are not infallible consistent “oracles”: their confidence in decision-making may vary significantly between tasks and over time. We have previously reported the benefits of using an interface and algorithms that explicitly captured and exploited users’ confidence: error rates were reduced by up to 50% for an industrial multi-class learning problem; and the number of interactions required in a design-optimisation context was reduced by 33%. Having access to users’ confidence judgements could significantly benefit intelligent interactive systems in industry, in areas such as intelligent tutoring systems and in health care. There are many reasons for wanting to capture information about confidence implicitly . Some are ergonomic, but others are more “social”—such as wishing to understand (and possibly take account of) users’ cognitive state without interrupting them. We investigate the hypothesis that users’ confidence can be accurately predicted from measurements of their behaviour. Eye-tracking systems were used to capture users’ gaze patterns as they undertook a series of visual decision tasks, after each of which they reported their confidence on a 5-point Likert scale. Subsequently, predictive models were built using “conventional” machine learning approaches for numerical summary features derived from users’ behaviour. We also investigate the extent to which the deep learning paradigm can reduce the need to design features specific to each application by creating “gaze maps”—visual representations of the trajectories and durations of users’ gaze fixations—and then training deep convolutional networks on these images. Treating the prediction of user confidence as a two-class problem (confident/not confident), we attained classification accuracy of 88% for the scenario of new users on known tasks, and 87% for known users on new tasks. Considering the confidence as an ordinal variable, we produced regression models with a mean absolute error of ≈0.7 in both cases. Capturing just a simple subset of non-task-specific numerical features gave slightly worse, but still quite high accuracy (e.g., MAE ≈ 1.0). Results obtained with gaze maps and convolutional networks are competitive, despite not having access to longer-term information about users and tasks, which was vital for the “summary” feature sets. This suggests that the gaze-map-based approach forms a viable, transferable alternative to handcrafting features for each different application. These results provide significant evidence to confirm our hypothesis, and offer a way of substantially improving many interactive artificial intelligence applications via the addition of cheap non-intrusive hardware and computationally cheap prediction algorithms.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2787116251",
    "type": "article"
  },
  {
    "title": "A Visual Analytics Framework for Exploring Theme Park Dynamics",
    "doi": "https://doi.org/10.1145/3162076",
    "publication_date": "2018-02-20",
    "publication_year": 2018,
    "authors": "Michael Steptoe; Robert A. Kruger; Rolando Garcia; Xing Liang; Ross Maciejewski",
    "corresponding_authors": "",
    "abstract": "In 2015, the top 10 largest amusement park corporations saw a combined annual attendance of over 400 million visitors. Daily average attendance in some of the most popular theme parks in the world can average 44,000 visitors per day. These visitors ride attractions, shop for souvenirs, and dine at local establishments; however, a critical component of their visit is the overall park experience. This experience depends on the wait time for rides, the crowd flow in the park, and various other factors linked to the crowd dynamics and human behavior. As such, better insight into visitor behavior can help theme parks devise competitive strategies for improved customer experience. Research into the use of attractions, facilities, and exhibits can be studied, and as behavior profiles emerge, park operators can also identify anomalous behaviors of visitors which can improve safety and operations. In this article, we present a visual analytics framework for analyzing crowd dynamics in theme parks. Our proposed framework is designed to support behavioral analysis by summarizing patterns and detecting anomalies. We provide methodologies to link visitor movement data, communication data, and park infrastructure data. This combination of data sources enables a semantic analysis of who , what , when , and where , enabling analysts to explore visitor-visitor interactions and visitor-infrastructure interactions. Analysts can identify behaviors at the macro level through semantic trajectory clustering views for group behavior dynamics, as well as at the micro level using trajectory traces and a novel visitor network analysis view. We demonstrate the efficacy of our framework through two case studies of simulated theme park visitors.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2791619436",
    "type": "article"
  },
  {
    "title": "Visualizing Research Impact through Citation Data",
    "doi": "https://doi.org/10.1145/3132744",
    "publication_date": "2018-03-09",
    "publication_year": 2018,
    "authors": "Yong Wang; Conglei Shi; Liangyue Li; Hanghang Tong; Huamin Qu",
    "corresponding_authors": "",
    "abstract": "Research impact plays a critical role in evaluating the research quality and influence of a scholar, a journal, or a conference. Many researchers have attempted to quantify research impact by introducing different types of metrics based on citation data, such as h -index, citation count, and impact factor. These metrics are widely used in the academic community. However, quantitative metrics are highly aggregated in most cases and sometimes biased, which probably results in the loss of impact details that are important for comprehensively understanding research impact. For example, which research area does a researcher have great research impact on? How does the research impact change over time? How do the collaborators take effect on the research impact of an individual? Simple quantitative metrics can hardly help answer such kind of questions, since more detailed exploration of the citation data is needed. Previous work on visualizing citation data usually only shows limited aspects of research impact and may suffer from other problems including visual clutter and scalability issues. To fill this gap, we propose an interactive visualization tool, ImpactVis , for better exploration of research impact through citation data. Case studies and in-depth expert interviews are conducted to demonstrate the effectiveness of ImpactVis .",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2792393069",
    "type": "article"
  },
  {
    "title": "A Visual Approach for Interactive Keyterm-Based Clustering",
    "doi": "https://doi.org/10.1145/3181669",
    "publication_date": "2018-02-20",
    "publication_year": 2018,
    "authors": "Seyednaser Nourashrafeddin; Ehsan Sherkat; Rosane Minghim; Evangelos Milios",
    "corresponding_authors": "",
    "abstract": "The keyterm-based approach is arguably intuitive for users to direct text-clustering processes and adapt results to various applications in text analysis. Its way of markedly influencing the results, for instance, by expressing important terms in relevance order, requires little knowledge of the algorithm and has predictable effect, speeding up the task. This article first presents a text-clustering algorithm that can easily be extended into an interactive algorithm. We evaluate its performance against state-of-the-art clustering algorithms in unsupervised mode. Next, we propose three interactive versions of the algorithm based on keyterm labeling, document labeling, and hybrid labeling. We then demonstrate that keyterm labeling is more effective than document labeling in text clustering. Finally, we propose a visual approach to support the keyterm-based version of the algorithm. Visualizations are provided for the whole collection as well as for detailed views of document and cluster relationships. We show the effectiveness and flexibility of our framework, Vis-Kt , by presenting typical clustering cases on real text document collections. A user study is also reported that reveals overwhelmingly positive acceptance toward keyterm-based clustering.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2792584195",
    "type": "article"
  },
  {
    "title": "Toward a Unified Theory of Learned Trust in Interpersonal and Human-Machine Interactions",
    "doi": "https://doi.org/10.1145/3230735",
    "publication_date": "2019-10-10",
    "publication_year": 2019,
    "authors": "Ion Juvina; Michael G. Collins; Othalia Larue; William G. Kennedy; Ewart J. de Visser; Celso P. de Melo",
    "corresponding_authors": "",
    "abstract": "A proposal for a unified theory of learned trust implemented in a cognitive architecture is presented. The theory is instantiated as a computational cognitive model of learned trust that integrates several seemingly unrelated categories of findings from the literature on interpersonal and human-machine interactions and makes unintuitive predictions for future studies. The model relies on a combination of learning mechanisms to explain a variety of phenomena such as trust asymmetry, the higher impact of early trust breaches, the black-hat/white-hat effect, the correlation between trust and cognitive ability, and the higher resilience of interpersonal as compared to human-machine trust. In addition, the model predicts that trust decays in the absence of evidence of trustworthiness or untrustworthiness. The implications of the model for the advancement of the theory on trust are discussed. Specifically, this work suggests two more trust antecedents on the trustor's side: perceived trust necessity and cognitive ability to detect cues of trustworthiness.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W3004990836",
    "type": "article"
  },
  {
    "title": "Design and evaluation techniques for authoring interactive and stylistic behaviors",
    "doi": "https://doi.org/10.1145/2499671",
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "James E. Young; Takeo Igarashi; Ehud Sharlin; Daisuke Sakamoto; Jeffrey Allen",
    "corresponding_authors": "",
    "abstract": "We present a series of projects for end-user authoring of interactive robotic behaviors, with a particular focus on the style of those behaviors: we call this approach Style-by-Demonstration (SBD). We provide an overview introduction of three different SBD platforms: SBD for animated character interactive locomotion paths, SBD for interactive robot locomotion paths, and SBD for interactive robot dance. The primary contribution of this article is a detailed cross-project SBD analysis of the interaction designs and evaluation approaches employed, with the goal of providing general guidelines stemming from our experiences, for both developing and evaluating SBD systems. In addition, we provide the first full account of our Puppet Master SBD algorithm, with an explanation of how it evolved through the projects.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2082308368",
    "type": "article"
  },
  {
    "title": "Added Value of Gaze-Exploiting Semantic Representation to Allow Robots Inferring Human Behaviors",
    "doi": "https://doi.org/10.1145/2939381",
    "publication_date": "2017-03-23",
    "publication_year": 2017,
    "authors": "Karinne Ramirez-Amaro; Humera Noor Minhas; Michael Zehetleitner; Michael Beetz; Gordon Cheng",
    "corresponding_authors": "",
    "abstract": "Neuroscience studies have shown that incorporating gaze view with third view perspective has a great influence to correctly infer human behaviors. Given the importance of both first and third person observations for the recognition of human behaviors, we propose a method that incorporates these observations in a technical system to enhance the recognition of human behaviors, thus improving beyond third person observations in a more robust human activity recognition system. First, we present the extension of our proposed semantic reasoning method by including gaze data and external observations as inputs to segment and infer human behaviors in complex real-world scenarios. Then, from the obtained results we demonstrate that the combination of gaze and external input sources greatly enhance the recognition of human behaviors. Our findings have been applied to a humanoid robot to online segment and recognize the observed human activities with better accuracy when using both input sources; for example, the activity recognition increases from 77% to 82% in our proposed pancake-making dataset. To provide completeness of our system, we have evaluated our approach with another dataset with a similar setup as the one proposed in this work, that is, the CMU-MMAC dataset. In this case, we improved the recognition of the activities for the egg scrambling scenario from 54% to 86% by combining the external views with the gaze information, thus showing the benefit of incorporating gaze information to infer human behaviors across different datasets.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2602933517",
    "type": "article"
  },
  {
    "title": "A Visual Analytics Approach for Interactive Document Clustering",
    "doi": "https://doi.org/10.1145/3241380",
    "publication_date": "2019-08-09",
    "publication_year": 2019,
    "authors": "Ehsan Sherkat; Evangelos Milios; Rosane Minghim",
    "corresponding_authors": "",
    "abstract": "Document clustering is a necessary step in various analytical and automated activities. When guided by the user, algorithms are tailored to imprint a perspective on the clustering process that reflects the user’s understanding of the dataset. More than just allow for customized adjustment of the clusters, a visual analytics approach will provide tools for the user to draw new insights on the collection. While contributing his or her perspective, the user will also acquire a deeper understanding of the data set. To that effect, we propose a novel visual analytics system for interactive document clustering. We built our system on top of clustering algorithms that can adapt to user’s feedback. In the proposed system, initial clustering is created based on the user-defined number of clusters and the selected clustering algorithm. A set of coordinated visualizations allow the examination of the dataset and the results of the clustering. The visualization provides the user with the highlights of individual documents and understanding of the evolution of documents over the time period to which they relate. The users then interact with the process by means of changing key-terms that drive the process according to their knowledge of the documents domain. In key-term-based interaction, the user assigns a set of key-terms to each target cluster to guide the clustering algorithm. We have improved that process with a novel algorithm for choosing proper seeds for the clustering. Results demonstrate that not only the system has improved considerably its precision, but also its effectiveness in the document-based decision making. A set of quantitative experiments and a user study have been conducted to show the advantages of the approach for document analytics based on clustering. We performed and reported on the use of the framework in a real decision-making scenario that relates users discussion by email to decision making in improving patient care. Results show that the framework is useful even for more complex data sets such as email conversations.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W3000988915",
    "type": "article"
  },
  {
    "title": "Evaluation of Normal Model Visualization for Anomaly Detection in Maritime Traffic",
    "doi": "https://doi.org/10.1145/2591511",
    "publication_date": "2014-04-01",
    "publication_year": 2014,
    "authors": "Maria Riveiro",
    "corresponding_authors": "Maria Riveiro",
    "abstract": "Monitoring dynamic objects in surveillance applications is normally a demanding activity for operators, not only because of the complexity and high dimensionality of the data but also because of other factors like time constraints and uncertainty. Timely detection of anomalous objects or situations that need further investigation may reduce operators’ cognitive load. Surveillance applications may include anomaly detection capabilities, but their use is not widespread, as they usually generate a high number of false alarms, they do not provide appropriate cognitive support for operators, and their outcomes can be difficult to comprehend and trust. Visual analytics can bridge the gap between computational and human approaches to detecting anomalous behavior in traffic data, making this process more transparent. As a step toward this goal of transparency, this article presents an evaluation that assesses whether visualizations of normal behavioral models of vessel traffic support two of the main analytical tasks specified during our field work in maritime control centers. The evaluation combines quantitative and qualitative usability assessments. The quantitative evaluation, which was carried out with a proof-of-concept prototype, reveals that participants who used the visualization of normal behavioral models outperformed the group that did not do so. The qualitative assessment shows that domain experts have a positive attitude toward the provision of automatic support and the visualization of normal behavioral models, as these aids may reduce reaction time and increase trust in and comprehensibility of the system.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2066232412",
    "type": "article"
  },
  {
    "title": "Nonstrict Hierarchical Reinforcement Learning for Interactive Systems and Robots",
    "doi": "https://doi.org/10.1145/2659003",
    "publication_date": "2014-10-14",
    "publication_year": 2014,
    "authors": "Heriberto Cuayáhuitl; Ivana Kruijff‐Korbayová; Nina Dethlefs",
    "corresponding_authors": "",
    "abstract": "Conversational systems and robots that use reinforcement learning for policy optimization in large domains often face the problem of limited scalability. This problem has been addressed either by using function approximation techniques that estimate the approximate true value function of a policy or by using a hierarchical decomposition of a learning task into subtasks. We present a novel approach for dialogue policy optimization that combines the benefits of both hierarchical control and function approximation and that allows flexible transitions between dialogue subtasks to give human users more control over the dialogue. To this end, each reinforcement learning agent in the hierarchy is extended with a subtask transition function and a dynamic state space to allow flexible switching between subdialogues. In addition, the subtask policies are represented with linear function approximation in order to generalize the decision making to situations unseen in training. Our proposed approach is evaluated in an interactive conversational robot that learns to play quiz games. Experimental results, using simulation and real users, provide evidence that our proposed approach can lead to more flexible (natural) interactions than strict hierarchical control and that it is preferred by human users.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2138394088",
    "type": "article"
  },
  {
    "title": "QuestionComb: A Gamification Approach for the Visual Explanation of Linguistic Phenomena through Interactive Labeling",
    "doi": "https://doi.org/10.1145/3429448",
    "publication_date": "2021-09-03",
    "publication_year": 2021,
    "authors": "Rita Sevastjanova; Wolfgang Jentner; Fabian Sperrle; Rebecca Kehlbeck; Jürgen Bernard; Mennatallah El‐Assady",
    "corresponding_authors": "",
    "abstract": "Linguistic insight in the form of high-level relationships and rules in text builds the basis of our understanding of language. However, the data-driven generation of such structures often lacks labeled resources that can be used as training data for supervised machine learning. The creation of such ground-truth data is a time-consuming process that often requires domain expertise to resolve text ambiguities and characterize linguistic phenomena. Furthermore, the creation and refinement of machine learning models is often challenging for linguists as the models are often complex, in-transparent, and difficult to understand. To tackle these challenges, we present a visual analytics technique for interactive data labeling that applies concepts from gamification and explainable Artificial Intelligence (XAI) to support complex classification tasks. The visual-interactive labeling interface promotes the creation of effective training data. Visual explanations of learned rules unveil the decisions of the machine learning model and support iterative and interactive optimization. The gamification-inspired design guides the user through the labeling process and provides feedback on the model performance. As an instance of the proposed technique, we present QuestionComb , a workspace tailored to the task of question classification (i.e., in information-seeking vs. non-information-seeking questions). Our evaluation studies confirm that gamification concepts are beneficial to engage users through continuous feedback, offering an effective visual analytics technique when combined with active learning and XAI.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W3198380835",
    "type": "article"
  },
  {
    "title": "A Multilingual Neural Coaching Model with Enhanced Long-term Dialogue Structure",
    "doi": "https://doi.org/10.1145/3487066",
    "publication_date": "2022-06-14",
    "publication_year": 2022,
    "authors": "Asier López Zorrilla; M. Inés Torres",
    "corresponding_authors": "",
    "abstract": "In this work we develop a fully data driven conversational agent capable of carrying out motivational coaching sessions in Spanish, French, Norwegian, and English. Unlike the majority of coaching, and in general well-being related conversational agents that can be found in the literature, ours is not designed by hand-crafted rules. Instead, we directly model the coaching strategy of professionals with end users. To this end, we gather a set of virtual coaching sessions through a Wizard of Oz platform, and apply state of the art Natural Language Processing techniques. We employ a transfer learning approach, pretraining GPT2 neural language models and fine-tuning them on our corpus. However, since these only take as input a local dialogue history, a simple fine-tuning procedure is not capable of modeling the long-term dialogue strategies that appear in coaching sessions. To alleviate this issue, we first propose to learn dialogue phase and scenario embeddings in the fine-tuning stage. These indicate to the model at which part of the dialogue it is and which kind of coaching session it is carrying out. Second, we develop a global deep learning system which controls the long-term structure of the dialogue. We also show that this global module can be used to visualize and interpret the decisions taken by the the conversational agent, and that the learnt representations are comparable to dialogue acts. Automatic and human evaluation show that our proposals serve to improve the baseline models. Finally, interaction experiments with coaching experts indicate that the system is usable and gives rise to positive emotions in Spanish, French and English, while the results in Norwegian point out that there is still work to be done in fully data driven approaches with very low resource languages.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4282825269",
    "type": "article"
  },
  {
    "title": "SketchMaker: Sketch Extraction and Reuse for Interactive Scene Sketch Composition",
    "doi": "https://doi.org/10.1145/3543956",
    "publication_date": "2022-06-25",
    "publication_year": 2022,
    "authors": "Fang Liu; Xiaoming Deng; Jiancheng Song; Yu‐Kun Lai; Yong‐Jin Liu; Hao Wang; Cuixia Ma; Shengfeng Qin; Hongan Wang",
    "corresponding_authors": "",
    "abstract": "Sketching is an intuitive and simple way to depict sciences with various object form and appearance characteristics. In the past few years, widely available touchscreen devices have increasingly made sketch-based human-AI co-creation applications popular. One key issue of sketch-oriented interaction is to prepare input sketches efficiently by non-professionals because it is usually difficult and time-consuming to draw an ideal sketch with appropriate outlines and rich details, especially for novice users with no sketching skills. Thus, sketching brings great obstacles for sketch applications in daily life. On the other hand, hand-drawn sketches are scarce and hard to collect. Given the fact that there are several large-scale sketch datasets providing sketch data resources, but they usually have a limited number of objects and categories in sketch, and do not support users to collect new sketch materials according to their personal preferences. In addition, few sketch-related applications support the reuse of existing sketch elements. Thus, knowing how to extract sketches from existing drawings and effectively re-use them in interactive scene sketch composition will provide an elegant way for sketch-based image retrieval (SBIR) applications, which are widely used in various touch screen devices. In this study, we first conduct a study on current SBIR to better understand the main requirements and challenges in sketch-oriented applications. Then we develop the SketchMaker as an interactive sketch extraction and composition system to help users generate scene sketches via reusing object sketches in existing scene sketches with minimal manual intervention. Moreover, we demonstrate how SBIR improves from composited scene sketches to verify the performance of our interactive sketch processing system. We also include a sketch-based video localization task as an alternative application of our sketch composition scheme. Our pilot study shows that our system is effective and efficient, and provides a way to promote practical applications of sketches.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4283521239",
    "type": "article"
  },
  {
    "title": "The Author’s Journey - Understanding and improving the authoring process of theory-driven socially intelligent agents",
    "doi": "https://doi.org/10.1145/3711672",
    "publication_date": "2025-01-28",
    "publication_year": 2025,
    "authors": "Manuel Guimarães; Joana Campos; Pedro A. Santos; João Dias; Rui Prada",
    "corresponding_authors": "",
    "abstract": "State of the art agent-modelling tools support the creation of powerful Socially Intelligent Agents (SIAs) capable of engaging in social interactions with participants in various roles and environments. However, their deployment demands a laborious authoring task as it is necessary to manually define behaviour rules and create content for different interaction scenarios. While Socially Intelligent Agents (SIAs) research has centered on the user experience, we shift focus to the authors. To understand the challenges faced by authors who create these agents, we performed an innovative analysis of the authoring experience in modern agent modelling tools. One key finding is that, while SIA concepts are generally understandable, emotional-based concepts are not as easily comprehended or used by authors. We propose a hybrid solution approach that culminated in the development of Authoring Assisted FAtiMA-Toolkit. The augmented agent modeling tool incorporates a data-driven authoring assistant to boost author productivity while promoting transparency and authorial control. To evaluate the impact of this framework on the authoring experience, we conducted a user study. Results showed that authors using the Authoring-Assisted FAtiMA-Toolkit were on average able to create more SIA-related content in less time. Our findings suggest that data-augmented, theory-grounded agent modeling tools can support the development of affective social agents by reducing the authoring burden without sacrificing the framework’s clarity or the authors’ control over the content",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406921225",
    "type": "article"
  },
  {
    "title": "Behavioural Indicators of Usability in Visual Analytics Dashboards",
    "doi": "https://doi.org/10.1145/3715710",
    "publication_date": "2025-02-04",
    "publication_year": 2025,
    "authors": "Mohammed Alhamadi; Hatim Alsayahani; Sarah Clinch; Markel Vigo",
    "corresponding_authors": "",
    "abstract": "Information presentation problems on interactive dashboards are known to hinder decision-making. Since a traditional user-centred approach to designing usable dashboards cannot fully satisfy user demands, needs and skills, we isolate behavioural indicators of usability when users conduct typical information-seeking and comparison tasks. In a first study (N=50), we identified strategies derived from 486,435 interaction events logged in a controlled setting with synthetic dashboards. User models consisting of these user strategies and graph literacy produced strong signals indicating that usability was predictable. In a second study (N=65), we tested the initial insights on real-world dashboards. While most of our hypotheses were confirmed, graph literacy emerged as the best predictor of usability. Usability was better predicted in dashboards with problems, suggesting promising opportunities for automated usability evaluation and real-time support for users struggling with visual analytics dashboards.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407131747",
    "type": "article"
  },
  {
    "title": "Panda or not Panda? Understanding Adversarial Attacks with Interactive Visualization",
    "doi": "https://doi.org/10.1145/3725739",
    "publication_date": "2025-03-25",
    "publication_year": 2025,
    "authors": "Y. You; Jarvis Tse; Jian Zhao",
    "corresponding_authors": "",
    "abstract": "Adversarial machine learning (AML) studies attacks that can fool machine learning algorithms into generating incorrect outcomes as well as the defenses against worst-case attacks to strengthen model robustness. Specifically for image classification, it is challenging to understand adversarial attacks due to their use of subtle perturbations that are not human-interpretable, as well as the variability of attack impacts influenced by diverse methodologies, instance differences, and model architectures. Through a design study with AML learners and teachers, we introduce AdvEx , a multi-level interactive visualization system that comprehensively presents the properties and impacts of evasion attacks on different image classifiers for novice AML learners. We quantitatively and qualitatively assessed AdvEx in a two-part evaluation including user studies and expert interviews. Our results show that AdvEx is not only highly effective as a visualization tool for understanding AML mechanisms, but also provides an engaging and enjoyable learning experience, thus demonstrating its overall benefits for AML learners.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408834070",
    "type": "article"
  },
  {
    "title": "What are you looking forward to? Deliberate positivity as a promising strategy for conversational agents",
    "doi": "https://doi.org/10.1145/3725738",
    "publication_date": "2025-03-27",
    "publication_year": 2025,
    "authors": "Libby Ferland; Risako Owan; Zachary Kunkel; Hannah Qu; Maria Gini; Wilma Koutstaal",
    "corresponding_authors": "",
    "abstract": "Conversational agents (CAs) are one of the most promising technologies for helping older adults maintain independence longer by augmenting their support and social networks. Voice-based technology in particular is especially powerful in this regard due to its accessibility and ease of use. There is also a growing body of evidence supporting the potential use of such technology in mitigating common issues such as loneliness and isolation, particularly for independent older adults aging in place. One of the key challenges for smart technologies deployed in this context is the development and maintenance of long-term user engagement and adoption, which is often addressed by attempting to closely mimic human social interactions. However, the more human-like the system, the more glaring fault conditions become, and the more jarring they are for users. In this study we explore the effectiveness of an alternative conversational strategy meant to encourage users to engage in positive reflection and introspection. We detail the iterative design and implementation of a prototype CA developed to engage in social conversation with older adults on selected topics of interest. We then use this system as part of a multi-method approach to investigate the effect of deliberate positivity as a conversational strategy, including its impact on user impressions and willingness to continue using the CA. Our results from different approaches, including methods such as psycholinguistic analysis, user self-report, and researcher-based coding, paint a promising picture of this conversational design. We show that the deliberate encouragement by a CA of positive conversation and reflection in users has a measurable positive impact on both user enjoyment and desire to continue engaging with a system. We further demonstrate how some user characteristics may amplify this effect, and discuss the implications of these results for the design and testing of future conversational systems for older adults.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408906791",
    "type": "article"
  },
  {
    "title": "HAIFAI: <u>H</u> uman- <u>AI</u> Interaction for Mental <u>F</u> ace Reconstruct <u>i</u> on",
    "doi": "https://doi.org/10.1145/3725891",
    "publication_date": "2025-04-02",
    "publication_year": 2025,
    "authors": "Florian Strohm; Mihai Bâce; Andreas Bulling",
    "corresponding_authors": "",
    "abstract": "We present HAIFAI – a novel two-stage system where humans and AI interact to tackle the challenging task of reconstructing a visual representation of a face that exists only in a person’s mind. In the first stage, users iteratively rank images our reconstruction system presents based on their resemblance to a mental image. These rankings, in turn, allow the system to extract relevant image features, fuse them into a unified feature vector, and use a generative model to produce an initial reconstruction of the mental image. The second stage leverages an existing face editing method, allowing users to manually refine and further improve this reconstruction using an easy-to-use slider interface for face shape manipulation. To avoid the need for tedious human data collection for training the reconstruction system, we introduce a computational user model of human ranking behaviour. For this, we collected a small face ranking dataset through an online crowd-sourcing study containing data from 275 participants. We evaluate HAIFAI and an ablated version in a 12-participant user study and demonstrate that our approach outperforms the previous state of the art regarding reconstruction quality, usability, perceived workload, and reconstruction speed. We further validate the reconstructions in a subsequent face ranking study with 18 participants and show that HAIFAI achieves a new state-of-the-art identification rate of \\(60.6\\%\\) . These findings represent a significant advancement towards developing new interactive intelligent systems capable of reliably and effortlessly reconstructing a user’s mental image.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409094714",
    "type": "article"
  },
  {
    "title": "Practitioners and Bias in Machine Learning: A Study",
    "doi": "https://doi.org/10.1145/3733838",
    "publication_date": "2025-05-05",
    "publication_year": 2025,
    "authors": "Robert Cinca; Enrico Costanza; Mirco Musolesi",
    "corresponding_authors": "",
    "abstract": "The increasing adoption of Machine Learning (ML) raises ethical concerns, particularly regarding bias. This study explores how ML practitioners with limited experience in bias understand and apply bias definitions, detection measures, and mitigation methods. Through a take-home task, exercises, and interviews with twenty-two participants, we identified five key themes: sources of bias, selecting bias metrics, detecting bias, mitigating bias, and ethical considerations. Participants faced unresolved conflicts, such as applying fairness definitions in practice, selecting context-dependent bias metrics, addressing real-world biases, balancing model performance with bias mitigation, and relying on personal perspectives over data-driven metrics. While bias mitigation techniques helped identify biases in two datasets, participants could not fully eliminate bias, citing the oversimplification of complex processes into models with limited variables. We propose designing bias detection tools that encourage practitioners to focus on the underlying assumptions and integrating bias concepts into ML practices, such as using a harmonic mean-based approach, akin to the F1 score, to balance bias and accuracy.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410089679",
    "type": "article"
  },
  {
    "title": "Using Emotion Diversification Based on Movie Reviews to Improve the User Experience of Movie Recommender Systems",
    "doi": "https://doi.org/10.1145/3743147",
    "publication_date": "2025-06-10",
    "publication_year": 2025,
    "authors": "Lior Lansman; Osnat Mokryn; Lijie Guo; Mehtab Iqbal; Bart P. Knijnenburg",
    "corresponding_authors": "",
    "abstract": "Diversifying movie recommendations is an effective way to address choice overload, a phenomenon where recommenders generate lists with highly similar recommendations that are difficult to choose from. However, existing diversification algorithms often rely on latent features, which limits their interpretability and makes it less clear why a particular set of movies is recommended. Given that movies are designed to elicit emotional responses, researchers have suggested leveraging these responses to enhance recommender system performance. This study introduces a novel “emotion diversification” approach, which diversifies movie recommendations based on emotional signals extracted from audience reviews. We evaluate this method against latent and non-diversified baselines in a controlled user study (N = 115), finding that it significantly improves perceived taste coverage and system satisfaction without compromising recommendation quality. Going beyond the traditional rating- and/or interaction data used by traditional recommender systems, our work demonstrates the user experience benefits of extracting emotional data from rich, qualitative user feedback and using it to give users a more emotionally diverse set of recommendations.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411183534",
    "type": "article"
  },
  {
    "title": "Data has Entered the Chat: How Data Workers Conduct Exploratory Visual Analytic Conversations with GenAI Agents",
    "doi": "https://doi.org/10.1145/3744750",
    "publication_date": "2025-06-26",
    "publication_year": 2025,
    "authors": "Matt-Heun Hong; Anamaria Crisan",
    "corresponding_authors": "",
    "abstract": "We investigate the potential of leveraging the code-generating capabilities of Large Language Models (LLMs) to support exploratory visual analysis (EVA) via conversational user interfaces (CUIs). We developed a technology probe that was deployed through two studies with a total of 50 data workers to explore the structure and flow of visual analytic conversations during EVA. We analyzed conversations from both studies using thematic analysis and derived a state transition diagram summarizing the conversational flow between four states of participant utterances ( Analytic Tasks , Editing Operations , Elaborations and Enrichments , and Directive Commands ) and two states of Generative AI (GenAI) agent responses (visualization, text). We describe the capabilities and limitations of GenAI agents according to each state and transitions between states as three co-occurring loops: analysis elaboration, refinement, and explanation. We discuss our findings as future research trajectories to improve the experiences of data workers using GenAI. Code &amp; Data: https://osf.io/6wxpa",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411673851",
    "type": "article"
  },
  {
    "title": "2024 TiiS Best Paper announcement",
    "doi": "https://doi.org/10.1145/3749645",
    "publication_date": "2025-07-23",
    "publication_year": 2025,
    "authors": "Shlomo Berkovsky",
    "corresponding_authors": "Shlomo Berkovsky",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412603553",
    "type": "article"
  },
  {
    "title": "Imperfections of XAI: Phenomena Influencing AI-Assisted Decision-Making",
    "doi": "https://doi.org/10.1145/3750052",
    "publication_date": "2025-07-23",
    "publication_year": 2025,
    "authors": "Philipp Spitzer; Katelyn Morrison; Violet Turri; Michelle Feng; Adam Perer; Niklas Kühl",
    "corresponding_authors": "",
    "abstract": "With the increasing use of artificial intelligence (AI), recent research in human-computer interaction explores explainable AI (XAI) to make AI advice more interpretable. While research addresses the effects of incorrect AI advice on AI-assisted decision-making, the impact of incorrect explanations is neglected so far. Additionally, recent work shows that not only different explanation modalities impact decision-makers, but also human factors play a critical role. To analyze relevant phenomena influencing AI-assisted decision-making, this work explores the impacting factors by conceptualizing theories of appropriate reliance and taking the first steps toward empirical evidence. We show that humans’ reliance on AI and the human-AI team performance are impacted by imperfect XAI in a study with 136 participants. Additionally, we find that cognitive styles affect decision-making in different explanation modalities. Hence, we shed light on diverse factors that impact human-AI collaboration and provide guidelines for designers to tailor such human-AI collaboration systems to individuals’ needs.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412603555",
    "type": "article"
  },
  {
    "title": "AI-Enhanced Sensemaking: Exploring the Design of a Generative AI-Based Assistant to Support Genetic Professionals",
    "doi": "https://doi.org/10.1145/3756326",
    "publication_date": "2025-08-06",
    "publication_year": 2025,
    "authors": "Angela Mastrianni; Hope Twede; Aleksandra Sarcevic; Jeremiah Wander; Christina Austin‐Tse; Scott Saponas; Heidi L. Rehm; Ashley Mae Conard; Amanda K. Hall",
    "corresponding_authors": "",
    "abstract": "Generative AI has the potential to transform knowledge work, but further research is needed to understand how knowledge workers envision using and interacting with generative AI. We investigate the development of generative AI tools to support domain experts in knowledge work, examining task delegation and the design of human-AI interactions. Our research focused on designing a generative AI assistant to aid genetic professionals in analyzing whole genome sequences (WGS) and other clinical data for rare disease diagnosis. Through interviews with 17 genetics professionals, we identified current challenges in WGS analysis. We then conducted co-design sessions with six genetics professionals to determine tasks that could be supported by an AI assistant and considerations for designing interactions with the AI assistant. From our findings, we identified sensemaking as both a current challenge in WGS analysis and a process that could be supported by AI. We contribute an understanding of how domain experts envision interacting with generative AI in their knowledge work, a detailed empirical study of WGS analysis, and three design considerations for using generative AI to support domain experts in sensemaking during knowledge work.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413019883",
    "type": "article"
  },
  {
    "title": "MV-Crafter: An Intelligent System for Music-guided Video Generation",
    "doi": "https://doi.org/10.1145/3748515",
    "publication_date": "2025-07-17",
    "publication_year": 2025,
    "authors": "Chuer Chen; Shengqi Dang; Ye Liu; Nanxuan Zhao; Yang Shi; Nan Cao",
    "corresponding_authors": "",
    "abstract": "Music videos, as a prevalent form of multimedia entertainment, deliver engaging audio-visual experiences to audiences and have gained immense popularity among singers and fans. Creators can express their interpretations of music naturally through visual elements. However, the creation process of music video demands proficiency in script design, video shooting, and music-video synchronization, posing significant challenges for non-professionals. Previous work has designed automated music video generation frameworks. However, they suffer from complexity in input and poor output quality. In response, we present MV-Crafter, a system capable of producing high-quality music videos with synchronized music-video rhythm and style. Our approach involves three technical modules that simulate the human creation process: the script generation module, video generation module, and music-video synchronization module. MV-Crafter leverages a large language model to generate scripts considering the musical semantics. To address the challenge of synchronizing short video clips with music of varying lengths, we propose a dynamic beat-matching algorithm and visual envelope-induced warping method to ensure precise, monotonic music-video synchronization. Besides, we design a user-friendly interface to simplify the creation process with intuitive editing features. Extensive experiments have demonstrated that MV-Crafter provides an effective solution for improving the quality of generated music videos.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413078334",
    "type": "article"
  },
  {
    "title": "Exploring Trust, Acceptance, and Behavioral Differences When Humans Collaborate with Large Language Models as Tools and Teammates",
    "doi": "https://doi.org/10.1145/3764591",
    "publication_date": "2025-08-28",
    "publication_year": 2025,
    "authors": "Christopher Flathmann; Nathan J. McNeese; Subhasree Sengupta; Ethan Johnson",
    "corresponding_authors": "",
    "abstract": "With the emergence of new artificial intelligence (AI) technologies, research on the potential for AI to function as teammates alongside humans has expanded. The recent introduction of highly capable large language models (LLMs) is particularly noteworthy, showing strong potential in human-AI teaming where communication is crucial. However, this novel technology has yet to be validated in human-AI teaming or as a teammate, hindering its application in research and practice. This article presents an empirical online experiment (N = 778) where participants engaged in a real-time and interdependent interaction with a commercially available LLM, with the presentation of the LLM manipulated to be either a tool or a teammate. Results show that when compared to presenting an LLM as a teammate rather than a tool significantly increases trust and significantly impacts the sentiment humans have when talking with their AI, with LLM teammates seeing more positive sentiment. Perceptions of trust, acceptance, and performance were generally high for LLMs presented as teammates. Despite these impacts, participants’ prior experiences with AI technology were still shown to predict the perceptions they formed with their AI teammate. Based on these findings, this article presents an important empirical result, which is that presenting highly capable AI, such as LLMs, as teammates can improve perception and interaction compared to presenting an AI as a tool. In turn, a discussion is had on how future research can continue to identify when and how to introduce LLMs and other AI technologies as teammates.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413919475",
    "type": "article"
  },
  {
    "title": "Modeling Human Concepts with Subspaces in Deep Vision Models",
    "doi": "https://doi.org/10.1145/3768340",
    "publication_date": "2025-09-22",
    "publication_year": 2025,
    "authors": "Anna Bavaresco; Nhut Truong; Uri Hasson",
    "corresponding_authors": "",
    "abstract": "Improving the modeling of human representations of everyday semantic categories, such as animals or food, can lead to better alignment between AI systems and humans. Humans are thought to represent such categories using dimensions that capture relevant variance, in this way defining the relationship between category members. In AI systems, the representational space for a category is defined by the distances between its members. Importantly, in this context, the same features are used for distance computations across all categories. In two experiments, we show that pruning a model’s feature space to better align with human representations of a category selects for different model features and different subspaces for different categories. In addition, we provide a proof of concept demonstrating the relevance of these findings for evaluating the quality of images generated by AI systems.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414392140",
    "type": "article"
  },
  {
    "title": "An Exploration of Mental Models of AI in Human-AI Co-Creativity: A Framework and Insights",
    "doi": "https://doi.org/10.1145/3769072",
    "publication_date": "2025-09-24",
    "publication_year": 2025,
    "authors": "Jeba Rezwana; Mary Lou Maher",
    "corresponding_authors": "",
    "abstract": "As AI becomes increasingly prevalent in creative domains, it is imperative to understand users’ mental models of AI in human-AI co-creation as mental models shape user experiences. Additionally, gaining insights into users’ mental models is essential for the development of human-centered co-creative AI. This paper introduces a framework for exploring users’ mental models of co-creative AI. Using a large-scale study (n=155), we explore mental models of two existing AI systems, ChatGPT and Stable Diffusion, in co-creation contexts. Participants engaged in creative tasks with both AI and completed surveys, revealing insights into mental models and their associations with demographic factors and users’ ethical stances. The results highlight the major types and patterns of mental models of AI in co-creative contexts. Findings also reveal that individuals with expertise in AI typically have Partnership-oriented mental models of co-creative AI, while those lacking AI literacy tend to have more Tool-oriented mental models. Furthermore, individuals with Partnership-oriented mental models usually have a positive ethical perspective toward anthropomorphism in AI, data collection by AI, and AI's societal impact. Additionally, results highlight that conversational co-creative AI is generally perceived as a collaborator, whereas non-conversational AI is typically viewed as a tool.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414475875",
    "type": "article"
  },
  {
    "title": "Skeptik: A Hybrid Framework for Combating Potential Misinformation in Journalism",
    "doi": "https://doi.org/10.1145/3766891",
    "publication_date": "2025-09-24",
    "publication_year": 2025,
    "authors": "Arlen Fan; Lei Fan; Steven R. Corman; Ross Maciejewski",
    "corresponding_authors": "",
    "abstract": "The proliferation of misinformation in journalism, often stemming from flawed reasoning and logical fallacies, poses significant challenges to public understanding and trust in news media. Traditional fact-checking methods, while valuable, are insufficient for detecting the subtle logical inconsistencies that can mislead readers within seemingly factual content. To address this gap, we introduce Skeptik, a hybrid framework that integrates Large Language Models (LLMs) with heuristic approaches to analyze and annotate potential logical fallacies and reasoning errors in online news articles. Operating as a web browser extension, Skeptik automatically highlights sentences that may contain logical fallacies, provides detailed explanations, and offers multi-layered interventions to help readers critically assess the information presented. The system is designed to be extensible, accommodating a wide range of fallacy types and adapting to evolving misinformation tactics. Through comprehensive case studies, quantitative analyses, usability experiments, and expert evaluations, we demonstrate the effectiveness of Skeptik in enhancing readers’ critical examination of news content and promoting media literacy. Our contributions include the development of an expandable classification system for logical fallacies, the innovative integration of LLMs for real-time analysis and annotation, and the creation of an interactive user interface that fosters user engagement and close reading. By emphasizing the logical integrity of textual content rather than relying solely on factual accuracy, Skeptik offers a comprehensive solution to combat potential misinformation in journalism. Ultimately, our framework aims to improve critical reading and protect the public from deceptive information online and enhance the overall credibility of news media.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414476222",
    "type": "article"
  },
  {
    "title": "Spiders Based on Anxiety: How Reinforcement Learning Can Deliver Desired User Experience in Virtual Reality Personalized Arachnophobia Treatment",
    "doi": "https://doi.org/10.1145/3766062",
    "publication_date": "2025-10-03",
    "publication_year": 2025,
    "authors": "Athar Mahmoudi-Nejad; Matthew Guzdial; Pierre Boulanger",
    "corresponding_authors": "",
    "abstract": "The need to generate a spider to provoke a desired anxiety response arises in the context of personalized virtual reality exposure therapy (VRET), a treatment approach for arachnophobia. This treatment involves patients observing virtual spiders in order to become desensitized and decrease their phobia, which requires that the spiders elicit specific anxiety responses. However, VRET approaches tend to require therapists to hand-select the appropriate spider for each patient, which is a time-consuming process and takes significant technical knowledge and patient insight. While automated methods exist, they tend to employ rules-based approaches with minimal ability to adapt to specific users. To address these challenges, we present a framework for VRET utilizing procedural content generation (PCG) and reinforcement learning (RL), which automatically adapts a spider to elicit a desired anxiety response. We demonstrate the superior performance of this system compared to a more common rules-based VRET method.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414786978",
    "type": "article"
  },
  {
    "title": "Design Principles for Human-Centred Explainable AI: A Scoping Review",
    "doi": "https://doi.org/10.1145/3771720",
    "publication_date": "2025-10-14",
    "publication_year": 2025,
    "authors": "Nathan Hughes; Yan Jia; Mark Sujan; Tom Lawton; Ibrahim Habli; John McDermid",
    "corresponding_authors": "",
    "abstract": "The field of Human-Centred Explainable AI (HCXAI) has been rapidly expanding. In turn, there has been an increase in the number of papers suggesting design principles for HCXAI. However, it is unclear the extent to which design requirements overlap between papers, and in turn what the field overall considers to be HCXAI design requirements. To overcome this, this study analysed the state of the field via a scoping review of papers suggesting HCXAI design requirements, and a Content Analysis of the extracted principles. A total of 330 design principles were identified from 35 papers, which were subsequently categorised into 43 codes and grouped into four main areas of focus. Based on these findings, we propose a definition of HCXAI which identifies HCXAI as a design process rather than an XAI technique. Finally, an overview of the current state of HCXAI is presented, as well as areas where further research is required.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415157142",
    "type": "review"
  },
  {
    "title": "Emotion Recognition from Peripheral Physiological Signals: A Systematic Review of Trends, Challenges and Opportunities",
    "doi": "https://doi.org/10.1145/3771719",
    "publication_date": "2025-10-16",
    "publication_year": 2025,
    "authors": "Isabel Barradas; Zunera Khan; Angelika Peer",
    "corresponding_authors": "",
    "abstract": "The adaptability and intuitiveness of Human-Computer-Interaction systems are enhanced by emotion recognition capabilities, whose rapid advancement asks for updated and more complete surveys. In this comprehensive work, papers using at least one of three peripheral physiological signals (galvanic skin response, heart rate, and respiration signals) were identified, resulting into 386 papers and 448 studies that were reviewed according to the entire emotion recognition pipeline, and not just based on types of signals and recognition methods as done in related work. Accordingly, this review identifies trends, challenges and opportunities across different aspects of the emotion recognition literature. Our investigation showed that multimodal approaches, benefitting from complementary physiological information, dominate the literature. Emotion-inducing methods tend to be dynamic and to progress towards real-life applications. To facilitate such applications, building novel datasets should be considered. For instance, there is room for novel continuously annotated datasets to facilitate the development of dynamic emotion models – which is also crucial for reliable real-life applications. At the same time, to guarantee a reliable continuous annotation, the combination of stimuli and assessment/report method should not be too overwhelming for the studies’ participants. Our results showed that support vector machines remain prevalent among traditional machine learning methods, but the growth of deep learning methods used either for feature extraction or end-to-end recognition is evident – both in number of studies and advanced developed techniques. Although a balance between algorithms’ performance and interpretability is essential in emotion recognition, there is a noticeable gap in integrating emotion theory into algorithms, which would improve such balance. Besides bringing to light a broad panorama of the literature, this work offers a digital table with the analysis of all studies and a filter possibility, allowing researchers to take advantage of it to accelerate and/or get inspiration for their own work.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415260842",
    "type": "review"
  },
  {
    "title": "Urania: Visualizing Data Analysis Pipelines for Natural Language-Based Data Exploration",
    "doi": "https://doi.org/10.1145/3770071",
    "publication_date": "2025-10-16",
    "publication_year": 2025,
    "authors": "Yi Guo; Nan Cao; Xiaoyu Qi; Haoyang Li; Jing Zhang; Danqing Shi; Qing Chen; Daniel Weiskopf",
    "corresponding_authors": "",
    "abstract": "Exploratory Data Analysis (EDA) is an essential yet tedious process for examining a new dataset. To facilitate it, natural language interfaces (NLIs) can help people intuitively explore the dataset via data-oriented questions. However, existing NLIs primarily focus on providing accurate answers to questions, with few offering explanations or presentations of the data analysis pipeline used to uncover the answer. Such presentations are crucial for EDA as they enhance the interpretability and reliability of the answer, while also helping users understand the analysis process and derive insights. To fill this gap, we introduce Urania, a natural language interactive system that is able to visualize the data analysis pipelines used to resolve input questions. It integrates a natural language interface that allows users to explore data via questions, and a novel data-aware question decomposition algorithm that resolves each input question into a data analysis pipeline. This pipeline is visualized in the form of a datamation, with animated presentations of analysis operations and their corresponding data changes. Through two quantitative experiments and expert interviews, we demonstrated that our data-aware question decomposition algorithm shows competitive performance compared to existing techniques in terms of execution accuracy, and that Urania can help people explore datasets better. In the end, we discuss the observations from the studies and the potential future works.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415261024",
    "type": "article"
  },
  {
    "title": "Tricking into trusting? The influence of social cues of a generative AI on perceived trust",
    "doi": "https://doi.org/10.1145/3771844",
    "publication_date": "2025-10-16",
    "publication_year": 2025,
    "authors": "Nicole C. Krämer; Ivana Lamia; Hanne Siegert; Florian Wenda; Lovis Bero Suchmann",
    "corresponding_authors": "",
    "abstract": "Generative AI systems such as ChatGPT are increasingly used to assist with tasks or to receive information. Given that the systems are not perfectly reliable regarding the content they produce, human users need to carefully navigate to which degree they can place trust in the system (calibrated trust). However, based on media equation assumptions it can be hypothesized that social cues displayed by the system might instill more trust than is warranted. Against this background, the present study investigates in a 2x2 between subject design (N=617) whether the social cues “typing behavior” and “personalized address” used by ChatGPT increase perceived trust (benevolence, ability and integrity) in the system and whether this is mediated by perceived similarity and moderated by anthropomorphism inclination. The results show that the social cue “typing behavior” leads to a significant increase in trust in the dimension benevolence. Neither perceived similarity nor anthropomorphism inclination modulate this effect. However, as a side effect, perceived similarity was found to significantly predict trust in ChatGPT.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415261222",
    "type": "article"
  },
  {
    "title": "DiffGaze: A Diffusion Model for Modelling Fine-grained Human Gaze Behaviour on 360° Images",
    "doi": "https://doi.org/10.1145/3772075",
    "publication_date": "2025-10-18",
    "publication_year": 2025,
    "authors": "Chuhan Jiao; Yao Wang; Guanhua Zhang; Mihai Bâce; Zhiming Hu; Andreas Bulling",
    "corresponding_authors": "",
    "abstract": "Modelling human gaze behaviour on 360 \\({}^{\\circ}\\) images is important for various human-computer interaction applications. However, existing methods are limited to predicting discrete fixation sequences or aggregated saliency maps, thereby neglecting fine-grained gaze behaviour such as saccadic eye movements that can be captured by commercial eye-trackers. We introduce a more challenging task— fine-grained gaze sequence generation . This task aims to generate eye-tracker-like gaze data for given stimuli. We propose DiffGaze , a diffusion-based method for generating realistic and diverse fine-grained human gaze sequences conditioned on 360 \\({}^{\\circ}\\) images. We evaluate DiffGaze on two 360 \\({}^{\\circ}\\) image benchmarks for fine-grained gaze sequence generation as well as two downstream tasks, scanpath prediction and saliency prediction. Our evaluations show that DiffGaze outperforms the fine-grained gaze generation baselines in all tasks on both benchmarks. We also report a 21-participant survey study showing that our method generates gaze sequences that are indistinguishable from real human sequences. Taken together, our evaluations not only demonstrate the effectiveness of DiffGaze but also point towards a new generation of methods that faithfully model the rich spatial and temporal nature of natural human gaze behaviour.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415312888",
    "type": "article"
  },
  {
    "title": "Wearables and Social Signal Processing for Smarter Public Presentations",
    "doi": "https://doi.org/10.1145/3234507",
    "publication_date": "2019-03-18",
    "publication_year": 2019,
    "authors": "Alaeddine Mihoub; Grégoire Lefebvre",
    "corresponding_authors": "",
    "abstract": "Social Signal Processing techniques have given the opportunity to analyze in-depth human behavior in social face-to-face interactions. With recent advancements, it is henceforth possible to use these techniques to augment social interactions, especially human behavior in oral presentations. The goal of this study is to train a computational model able to provide a relevant feedback to a public speaker concerning his/her coverbal communication. Hence, the role of this model is to augment the social intelligence of the orator and then the relevance of his/her presentation. To this end, we present an original interaction setting in which the speaker is equipped with only wearable devices. Several coverbal modalities have been extracted and automatically annotated namely speech volume, intonation, speech rate, eye gaze, hand gestures, and body movements. In this article, which is an extension of our previous article published in IUI’17, we compare our Dynamic Bayesian Network design to classical J48/Multi-Layer Perceptron/Support Vector Machine classifiers, propose a subjective evaluation of presenter skills with a discussion in regards to our automatic evaluation, and we add a complementary study about using DBScan versus k -means algorithm in the design process of our Dynamic Bayesian Network.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2925226097",
    "type": "article"
  },
  {
    "title": "A User-adaptive Modeling for Eating Action Identification from Wristband Time Series",
    "doi": "https://doi.org/10.1145/3300149",
    "publication_date": "2019-10-10",
    "publication_year": 2019,
    "authors": "Junghyo Lee; Prajwal Paudyal; Ayan Banerjee; Sandeep K. S. Gupta",
    "corresponding_authors": "",
    "abstract": "Eating activity monitoring using wearable sensors can potentially enable interventions based on eating speed to mitigate the risks of critical healthcare problems such as obesity or diabetes. Eating actions are poly-componential gestures composed of sequential arrangements of three distinct components interspersed with gestures that may be unrelated to eating. This makes it extremely challenging to accurately identify eating actions. The primary reasons for the lack of acceptance of state-of-the-art eating action monitoring techniques include the following: (i) the need to install wearable sensors that are cumbersome to wear or limit the mobility of the user, (ii) the need for manual input from the user, and (iii) poor accuracy in the absence of manual inputs. In this work, we propose a novel methodology, IDEA, that performs accurate eating action identification within eating episodes with an average F1 score of 0.92. This is an improvement of 0.11 for precision and 0.15 for recall for the worst-case users as compared to the state of the art. IDEA uses only a single wristband and provides feedback on eating speed every 2 min without obtaining any manual input from the user.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2979875740",
    "type": "article"
  },
  {
    "title": "Human-in-the-loop Learning for Personalized Diet Monitoring from Unstructured Mobile Data",
    "doi": "https://doi.org/10.1145/3319370",
    "publication_date": "2019-11-14",
    "publication_year": 2019,
    "authors": "Niloofar Hezarjaribi; Sepideh Mazrouee; Saied Hemati; Naomi Chaytor; Martine M. Perrigue; Hassan Ghasemzadeh",
    "corresponding_authors": "",
    "abstract": "Lifestyle interventions with the focus on diet are crucial in self-management and prevention of many chronic conditions, such as obesity, cardiovascular disease, diabetes, and cancer. Such interventions require a diet monitoring approach to estimate overall dietary composition and energy intake. Although wearable sensors have been used to estimate eating context (e.g., food type and eating time), accurate monitoring of dietary intake has remained a challenging problem. In particular, because monitoring dietary intake is a self-administered task that involves the end-user to record or report their nutrition intake, current diet monitoring technologies are prone to measurement errors related to challenges of human memory, estimation, and bias. New approaches based on mobile devices have been proposed to facilitate the process of dietary intake recording. These technologies require individuals to use mobile devices such as smartphones to record nutrition intake by either entering text or taking images of the food. Such approaches, however, suffer from errors due to low adherence to technology adoption and time sensitivity to the dietary intake context. In this article, we introduce EZNutriPal , 1 an interactive diet monitoring system that operates on unstructured mobile data such as speech and free-text to facilitate dietary recording, real-time prompting, and personalized nutrition monitoring. EZNutriPal features a natural language processing unit that learns incrementally to add user-specific nutrition data and rules to the system. To prevent missing data that are required for dietary monitoring (e.g., calorie intake estimation), EZNutriPal devises an interactive operating mode that prompts the end-user to complete missing data in real-time. Additionally, we propose a combinatorial optimization approach to identify the most appropriate pairs of food names and food quantities in complex input sentences. We evaluate the performance of EZNutriPal using real data collected from 23 human subjects who participated in two user studies conducted in 13 days each. The results demonstrate that EZNutriPal achieves an accuracy of 89.7% in calorie intake estimation. We also assess the impacts of the incremental training and interactive prompting technologies on the accuracy of nutrient intake estimation and show that incremental training and interactive prompting improve the performance of diet monitoring by 49.6% and 29.1%, respectively, compared to a system without such computing units.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2984652321",
    "type": "article"
  },
  {
    "title": "Photo Sleuth",
    "doi": "https://doi.org/10.1145/3365842",
    "publication_date": "2020-10-16",
    "publication_year": 2020,
    "authors": "Vikram Mohanty; David Thames; Sneha Mehta; Kurt Luther",
    "corresponding_authors": "",
    "abstract": "Identifying people in historical photographs is important for preserving material culture, correcting the historical record, and creating economic value, but it is also a complex and challenging task. In this article, we focus on identifying portraits of soldiers who participated in the American Civil War (1861--65), the first widely photographed conflict. Many thousands of these portraits survive, but only 10%--20% are identified. We created Photo Sleuth, a web-based platform that combines crowdsourced human expertise and automated face recognition to support Civil War portrait identification. Our mixed-methods evaluations of Photo Sleuth one month and 11 months after its public launch showed that it helped users successfully identify unknown portraits and provided a sustainable model for volunteer contribution. We also discuss implications for crowd-AI interaction and person identification pipelines.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W3094413191",
    "type": "article"
  },
  {
    "title": "Regression Cube",
    "doi": "https://doi.org/10.1145/2590349",
    "publication_date": "2014-04-01",
    "publication_year": 2014,
    "authors": "Yu‐Hsuan Chan; Carlos D. Correa; Kwan‐Liu Ma",
    "corresponding_authors": "",
    "abstract": "Scatterplots are commonly used to visualize multidimensional data; however, 2D projections of data offer limited understanding of the high-dimensional interactions between data points. We introduce an interactive 3D extension of scatterplots called the Regression Cube (RC), which augments a 3D scatterplot with three facets on which the correlations between the two variables are revealed by sensitivity lines and sensitivity streamlines. The sensitivity visualization of local regression on the 2D projections provides insights about the shape of the data through its orientation and continuity cues. We also introduce a series of visual operations such as clustering, brushing, and selection supported in RC. By iteratively refining the selection of data points of interest, RC is able to reveal salient local correlation patterns that may otherwise remain hidden with a global analysis. We have demonstrated our system with two examples and a user-oriented evaluation, and we show how RCs enable interactive visual exploration of multidimensional datasets via a variety of classification and information retrieval tasks. A video demo of RC is available.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2070833402",
    "type": "article"
  },
  {
    "title": "Task model-driven realization of interactive application functionality through services",
    "doi": "https://doi.org/10.1145/2559979",
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "Kyriakos Kritikos; Dimitris Plexousakis; Fabio Paternò",
    "corresponding_authors": "",
    "abstract": "The Service-Oriented Computing (SOC) paradigm is currently being adopted by many developers, as it promises the construction of applications through reuse of existing Web Services (WSs). However, current SOC tools produce applications that interact with users in a limited way. This limitation is overcome by model-based Human-Computer Interaction (HCI) approaches that support the development of applications whose functionality is realized with WSs and whose User Interface (UI) is adapted to the user's context. Typically, such approaches do not consider various functional issues, such as the applications' semantics and their syntactic robustness in terms of the WSs selected to implement their functionality and the automation of the service discovery and selection processes. To this end, we propose a model-driven design method for interactive service-based applications that is able to consider the functional issues and their implications for the UI. This method is realized by a semiautomatic environment that can be integrated into current model-based HCI tools to complete the development of interactive service front-ends. The proposed method takes as input an HCI task model, which includes the user's view of the interactive system, and produces a concrete service model that describes how existing services can be combined to realize the application's functionality. To achieve its goal, our method first transforms system tasks into semantic service queries by mapping the task objects onto domain ontology concepts; then it sends each resulting query to a semantic service engine so as to discover the corresponding services. In the end, only one service from those associated with a system task is selected, through the execution of a novel service concretization algorithm that ensures message compatibility between the selected services.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2106511761",
    "type": "article"
  },
  {
    "title": "The VideoMob Interactive Art Installation Connecting Strangers through Inclusive Digital Crowds",
    "doi": "https://doi.org/10.1145/2768208",
    "publication_date": "2015-07-09",
    "publication_year": 2015,
    "authors": "Emily Grenader; Danilo Gasques Rodrigues; Fernando da Silva Nos; Nadir Weibel",
    "corresponding_authors": "",
    "abstract": "VideoMob is an interactive video platform and an artwork that enables strangers visiting different installation locations to interact across time and space through a computer interface that detects their presence, video-records their actions while automatically removing the video background through computer vision, and co-situates visitors as part of the same digital environment. Through the combination of individual user videos to form a digital crowd, strangers are connected through the graphic display. Our work is inspired by the way distant people can interact with each other through technology and influenced by artists working in the realm of interactive art. We deployed VideoMob in a variety of settings, locations, and contexts to observe hundreds of visitors’ reactions. By analyzing behavioral data collected through depth cameras from our 1,068 recordings across eight venues, we studied how participants behave when given the opportunity to record their own video portrait into the artwork. We report the specific activity performed in front of the camera and the influences that existing crowds impose on new participants. Our analysis informs the integration of a series of possible novel interaction paradigms based on real-time analysis of the visitors’ behavior through specific computer vision and machine learning techniques that have the potential to increase the engagement of the artwork's visitors and to impact user experience.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2259398566",
    "type": "article"
  },
  {
    "title": "Beyond the Touchscreen",
    "doi": "https://doi.org/10.1145/2954003",
    "publication_date": "2016-08-03",
    "publication_year": 2016,
    "authors": "Cheng Zhang; Anhong Guo; Dingtian Zhang; Yang Li; Caleb Southern; Rosa I. Arriaga; Gregory D. Abowd",
    "corresponding_authors": "",
    "abstract": "Most smartphones today have a rich set of sensors that could be used to infer input (e.g., accelerometer, gyroscope, microphone); however, the primary mode of interaction is still limited to the front-facing touchscreen and several physical buttons on the case. To investigate the potential opportunities for interactions supported by built-in sensors, we present the implementation and evaluation of BeyondTouch, a family of interactions to extend and enrich the input experience of a smartphone. Using only existing sensing capabilities on a commodity smartphone, we offer the user a wide variety of additional inputs on the case and the surface adjacent to the smartphone. Although most of these interactions are implemented with machine learning methods, compact and robust rule-based detection methods can also be applied for recognizing some interactions by analyzing physical characteristics of tapping events on the phone. This article is an extended version of Zhang et al. [2015], which solely covered gestures implemented by machine learning methods. We extended our previous work by adding gestures implemented with rule-based methods, which works well with different users across devices without collecting any training data. We outline the implementation of both machine learning and rule-based methods for these interaction techniques and demonstrate empirical evidence of their effectiveness and usability. We also discuss the practicality of BeyondTouch for a variety of application scenarios and compare the two different implementation methods.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2483639660",
    "type": "article"
  },
  {
    "title": "Exploring the Role of Common Model of Cognition in Designing Adaptive Coaching Interactions for Health Behavior Change",
    "doi": "https://doi.org/10.1145/3375790",
    "publication_date": "2021-03-31",
    "publication_year": 2021,
    "authors": "Shiwali Mohan",
    "corresponding_authors": "Shiwali Mohan",
    "abstract": "Our research aims to develop intelligent collaborative agents that are human-aware - they can model, learn, and reason about their human partner's physiological, cognitive, and affective states. In this paper, we study how adaptive coaching interactions can be designed to help people develop sustainable healthy behaviors. We leverage the common model of cognition - CMC [26] - as a framework for unifying several behavior change theories that are known to be useful in human-human coaching. We motivate a set of interactive system desiderata based on the CMC-based view of behavior change. Then, we propose PARCoach - an interactive system that addresses the desiderata. PARCoach helps a trainee pick a relevant health goal, set an implementation intention, and track their behavior. During this process, the trainee identifies a specific goal-directed behavior as well as the situational context in which they will perform it. PARCcoach uses this information to send notifications to the trainee, reminding them of their chosen behavior and the context. We report the results from a 4-week deployment with 60 participants. Our results support the CMC-based view of behavior change and demonstrate that the desiderata for proposed interactive system design is useful in producing behavior change.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W3159429795",
    "type": "article"
  },
  {
    "title": "Emerging ExG-based NUI Inputs in Extended Realities: A Bottom-up Survey",
    "doi": "https://doi.org/10.1145/3457950",
    "publication_date": "2021-06-30",
    "publication_year": 2021,
    "authors": "Kirill Shatilov; Dimitris Chatzopoulos; Lik‐Hang Lee; Pan Hui",
    "corresponding_authors": "",
    "abstract": "Incremental and quantitative improvements of two-way interactions with e x tended realities (XR) are contributing toward a qualitative leap into a state of XR ecosystems being efficient, user-friendly, and widely adopted. However, there are multiple barriers on the way toward the omnipresence of XR; among them are the following: computational and power limitations of portable hardware, social acceptance of novel interaction protocols, and usability and efficiency of interfaces. In this article, we overview and analyse novel natural user interfaces based on sensing electrical bio-signals that can be leveraged to tackle the challenges of XR input interactions. Electroencephalography-based brain-machine interfaces that enable thought-only hands-free interaction, myoelectric input methods that track body gestures employing electromyography, and gaze-tracking electrooculography input interfaces are the examples of electrical bio-signal sensing technologies united under a collective concept of ExG. ExG signal acquisition modalities provide a way to interact with computing systems using natural intuitive actions enriching interactions with XR. This survey will provide a bottom-up overview starting from (i) underlying biological aspects and signal acquisition techniques, (ii) ExG hardware solutions, (iii) ExG-enabled applications, (iv) discussion on social acceptance of such applications and technologies, as well as (v) research challenges, application directions, and open problems; evidencing the benefits that ExG-based Natural User Interfaces inputs can introduce to the area of XR.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W3183178213",
    "type": "article"
  },
  {
    "title": "VADAF: Visualization for Abnormal Client Detection and Analysis in Federated Learning",
    "doi": "https://doi.org/10.1145/3426866",
    "publication_date": "2021-09-03",
    "publication_year": 2021,
    "authors": "Linhao Meng; Yating Wei; Rusheng Pan; Shuyue Zhou; Jianwei Zhang; Wei Chen",
    "corresponding_authors": "",
    "abstract": "Federated Learning (FL) provides a powerful solution to distributed machine learning on a large corpus of decentralized data. It ensures privacy and security by performing computation on devices (which we refer to as clients) based on local data to improve the shared global model. However, the inaccessibility of the data and the invisibility of the computation make it challenging to interpret and analyze the training process, especially to distinguish potential client anomalies. Identifying these anomalies can help experts diagnose and improve FL models. For this reason, we propose a visual analytics system, VADAF, to depict the training dynamics and facilitate analyzing potential client anomalies. Specifically, we design a visualization scheme that supports massive training dynamics in the FL environment. Moreover, we introduce an anomaly detection method to detect potential client anomalies, which are further analyzed based on both the client model’s visual and objective estimation. Three case studies have demonstrated the effectiveness of our system in understanding the FL training process and supporting abnormal client detection and analysis.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W3196691156",
    "type": "article"
  },
  {
    "title": "Learning GUI Completions with User-defined Constraints",
    "doi": "https://doi.org/10.1145/3490034",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Lukas Brückner; Luis A. Leiva; Antti Oulasvirta",
    "corresponding_authors": "",
    "abstract": "A key objective in the design of graphical user interfaces (GUIs) is to ensure consistency across screens of the same product. However, designing a compliant layout is time-consuming and can distract designers from creative thinking. This paper studies layout recommendation methods that fulfill such consistency requirements using machine learning. Given a desired element type and size, the methods suggest element placements following real-world GUI design processes. Consistency requirements are given implicitly through previous layouts from which patterns are to be learned, comparable to existing screens of a software product. We adopt two recently proposed methods for this task, a Graph Neural Network (GNN) and a Transformer model, and compare them with a custom approach based on sequence alignment and nearest neighbor search (kNN) . The methods were tested on handcrafted datasets with explicit layout patterns, as well as large-scale public datasets of diverse mobile design layouts. Our results show that our instance-based learning algorithm outperforms both neural network approaches. Ultimately, this work contributes to establishing smarter design tools for professional designers with explainable algorithms that increase their efficacy.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4214872958",
    "type": "article"
  },
  {
    "title": "SmartShots: An Optimization Approach for Generating Videos with Data Visualizations Embedded",
    "doi": "https://doi.org/10.1145/3484506",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Tan Tang; Junxiu Tang; Jiewen Lai; Ying Lü; Yingcai Wu; Lingyun Yu; Peiran Ren",
    "corresponding_authors": "",
    "abstract": "Videos are well-received methods for storytellers to communicate various narratives. To further engage viewers, we introduce a novel visual medium where data visualizations are embedded into videos to present data insights. However, creating such data-driven videos requires professional video editing skills, data visualization knowledge, and even design talents. To ease the difficulty, we propose an optimization method and develop SmartShots, which facilitates the automatic integration of in-video visualizations. For its development, we first collaborated with experts from different backgrounds, including information visualization, design, and video production. Our discussions led to a design space that summarizes crucial design considerations along three dimensions: visualization, embedded layout, and rhythm. Based on that, we formulated an optimization problem that aims to address two challenges: (1) embedding visualizations while considering both contextual relevance and aesthetic principles and (2) generating videos by assembling multi-media materials. We show how SmartShots solves this optimization problem and demonstrate its usage in three cases. Finally, we report the results of semi-structured interviews with experts and amateur users on the usability of SmartShots.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4214902115",
    "type": "article"
  },
  {
    "title": "ForSense: Accelerating Online Research Through Sensemaking Integration and Machine Research Support",
    "doi": "https://doi.org/10.1145/3532853",
    "publication_date": "2022-05-11",
    "publication_year": 2022,
    "authors": "Gonzalo Ramos; Napol Rachatasumrit; Jina Suh; Rachel Ng; Christopher Meek",
    "corresponding_authors": "",
    "abstract": "Online research is a frequent and important activity people perform on the Internet, yet current support for this task is basic, fragmented and not well integrated into web browser experiences. Guided by sensemaking theory, we present ForSense, a browser extension for accelerating people’s online research experience. The two primary sources of novelty of ForSense are the integration of multiple stages of online research and providing machine assistance to the user by leveraging recent advances in neural-driven machine reading. We use ForSense as a design probe to explore (1) the benefits of integrating multiple stages of online research, (2) the opportunities to accelerate online research using current advances in machine reading, (3) the opportunities to support online research tasks in the presence of imprecise machine suggestions, and (4) insights about the behaviors people exhibit when performing online research, the pages they visit, and the artifacts they create. Through our design probe, we observe people performing online research tasks, and see that they benefit from ForSense’s integration and machine support for online research. From the information and insights we collected, we derive and share key recommendations for designing and supporting imprecise machine assistance for research tasks.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4280619408",
    "type": "article"
  },
  {
    "title": "The Impact of Intelligent Pedagogical Agents’ Interventions on Student Behavior and Performance in Open-Ended Game Design Environments",
    "doi": "https://doi.org/10.1145/3578523",
    "publication_date": "2023-01-04",
    "publication_year": 2023,
    "authors": "Özge Nilay Yalçın; Sébastien Lallé; Cristina Conati",
    "corresponding_authors": "",
    "abstract": "Research has shown that free-form Game-Design (GD) environments can be very effective in fostering Computational Thinking (CT) skills at a young age. However, some students can still need some guidance during the learning process due to the highly open-ended nature of these environments. Intelligent Pedagogical Agents (IPAs) can be used to provide personalized assistance in real-time to alleviate this challenge. This paper presents our results in evaluating such an agent deployed in a real-word free-form GD learning environment to foster CT in the early K-12 education, Unity-CT. We focus on the effect of repetition by comparing student behaviors between no intervention, 1-shot, and repeated intervention groups for two different errors that are known to be challenging in the online lessons of Unity-CT. Our findings showed that the agent was perceived very positively by the students and the repeated intervention showed promising results in terms of helping students make fewer errors and more correct behaviors, albeit only for one of the two target errors. Building from these results, we provide insights on how to provide IPA interventions in free-form GD environments.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4313590566",
    "type": "article"
  },
  {
    "title": "Enabling Efficient Web Data-Record Interaction for People with Visual Impairments via Proxy Interfaces",
    "doi": "https://doi.org/10.1145/3579364",
    "publication_date": "2023-01-10",
    "publication_year": 2023,
    "authors": "Javedul Ferdous; Hae-Na Lee; Sampath Jayarathna; Vikas Ashok",
    "corresponding_authors": "",
    "abstract": "Web data records are usually accompanied by auxiliary webpage segments, such as filters, sort options, search form, and multi-page links, to enhance interaction efficiency and convenience for end users. However, blind and visually impaired (BVI) persons are presently unable to fully exploit the auxiliary segments like their sighted peers, since these segments are scattered all across the screen, and as such assistive technologies used by BVI users, i.e., screen reader and screen magnifier, are not geared for efficient interaction with such scattered content. Specifically, for blind screen reader users, content navigation is predominantly one-dimensional despite the support for skipping content, and therefore navigating to-and-fro between different parts of the webpage is tedious and frustrating. Similarly, low vision screen magnifier users have to continuously pan back-and-forth between different portions of a webpage, given that only a portion of the screen is viewable at any instant due to content enlargement. The extant techniques to overcome inefficient web interaction for BVI users have mostly focused on general web-browsing activities, and as such they provide little to no support for data record-specific interaction activities such as filtering and sorting – activities that are equally important for facilitating quick and easy access to desired data records. To fill this void, we present InSupport, a browser extension that: (i) employs custom machine learning-based algorithms to automatically extract auxiliary segments on any webpage containing data records; and (ii) provides an instantly accessible proxy one-stop interface for easily navigating the extracted auxiliary segments using either basic keyboard shortcuts or mouse actions. Evaluation studies with 14 blind participants and 16 low vision participants showed significant improvement in web usability with InSupport, driven by increased reduction in interaction time and user effort, compared to the state-of-the-art solutions.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4315491096",
    "type": "article"
  },
  {
    "title": "Does this Explanation Help? Designing Local Model-agnostic Explanation Representations and an Experimental Evaluation Using Eye-tracking Technology",
    "doi": "https://doi.org/10.1145/3607145",
    "publication_date": "2023-07-13",
    "publication_year": 2023,
    "authors": "Miguel Ángel Martínez; Mario Nadj; Moritz Langner; Peyman Toreini; Alexander Maedche",
    "corresponding_authors": "",
    "abstract": "In Explainable Artificial Intelligence (XAI) research, various local model-agnostic methods have been proposed to explain individual predictions to users in order to increase the transparency of the underlying Artificial Intelligence (AI) systems. However, the user perspective has received less attention in XAI research, leading to a (1) lack of involvement of users in the design process of local model-agnostic explanations representations and (2) a limited understanding of how users visually attend them. Against this backdrop, we refined representations of local explanations from four well-established model-agnostic XAI methods in an iterative design process with users. Moreover, we evaluated the refined explanation representations in a laboratory experiment using eye-tracking technology as well as self-reports and interviews. Our results show that users do not necessarily prefer simple explanations and that their individual characteristics, such as gender and previous experience with AI systems, strongly influence their preferences. In addition, users find that some explanations are only useful in certain scenarios making the selection of an appropriate explanation highly dependent on context. With our work, we contribute to ongoing research to improve transparency in AI.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4384201261",
    "type": "article"
  },
  {
    "title": "MALACHITE - Enabling Users to Teach GUI-Aware Natural Language Interfaces",
    "doi": "https://doi.org/10.1145/3716141",
    "publication_date": "2025-02-07",
    "publication_year": 2025,
    "authors": "Marcel Ruoff; Brad A. Myers; Alexander Maedche",
    "corresponding_authors": "",
    "abstract": "Users can adapt contemporary natural language interfaces (NLIs) by teaching the NLIs how to handle new natural language (NL) inputs. One promising approach is interactive task learning (ITL), which enables users to teach new NL inputs for multi-modal systems. While recent advances enable users to teach the syntactic and semantic level of the NL inputs through ITL, NLIs are still not able to learn how to consider the context, such as the current state of the graphical user interface (GUI). To address this challenge, we designed MALACHITE through three formative studies. MALACHITE enables users to successfully teach NL inputs on a semantic and syntactic level leveraging the GUI screen of a data visualization tool. With two evaluative studies, we provide evidence that with MALACHITE 's suggestions users significantly improve their accuracy by a factor of 2.3 in teaching GUI-dependent NL inputs in contrast to those without MALACHITE's suggestions.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407246934",
    "type": "article"
  },
  {
    "title": "Comparative Analysis of Personality Recognition in Response to Virtual Reality and Two-Dimensional Emotional Stimulus Using ECG Signals",
    "doi": "https://doi.org/10.1145/3707648",
    "publication_date": "2025-02-13",
    "publication_year": 2025,
    "authors": "Jialan Xie; Ping Lan; Zhaonian Hu; Guangyuan Liu",
    "corresponding_authors": "",
    "abstract": "Personality primarily refers to the unique and stable way of a person’s thinking and behavior. A few studies have recently been conducted on personality recognition using physiological signals, most of which have used 2D emotional stimulus materials. Virtual reality (VR) has been utilized in many fields, and its superiority over 2D in emotion recognition has been proven. However, relevant research on VR scenes is lacking in the field of personality recognition. In this study, based on the psychological principle that emotional arousal can expose an individual’s personality, we attempt to explore the feasibility and effect of using electrocardiogram (ECG) signals in response to VR emotional stimuli for personality identification. For this purpose, a VR-2D emotion-induction experiment was conducted in which ECG signals were collected, and physiological datasets of emotional personalities were constructed through preprocessing and feature extraction. Statistical analysis of the emotion scale scores and ECG features of the participants showed that the VR group had a higher number of significantly correlated features. Meanwhile, VR- and 2D-based personality recognition models were constructed using machine learning algorithms. The results showed that the VR-based personality recognition model achieved better results for the four personality dimensions, with a maximum accuracy of 79.76%. These findings indicate that VR not only enhances the physiological correlation between emotion and personality but also improves the classification accuracy of personality recognition.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407431700",
    "type": "article"
  },
  {
    "title": "Feeds of Distrust: Investigating How AI-Powered News Chatbots Shape User Trust and Perceptions",
    "doi": "https://doi.org/10.1145/3722227",
    "publication_date": "2025-03-14",
    "publication_year": 2025,
    "authors": "Jarod Govers; Saumya Pareek; Eduardo Velloso; Jorge Gonçalves",
    "corresponding_authors": "",
    "abstract": "The start of the 2020s ushered in a new era of Artificial Intelligence through the rise of Generative AI Large Language Models (LLMs) such as Chat-GPT. These AI chatbots offer a form of interactive agency by enabling users to ask questions and query for more information. However, prior research only considers if LLMs have a political bias or agenda, and not how a biased LLM can impact a user's opinion and trust. Our study bridges this gap by investigating a scenario where users read online news articles and then engage with an interactive AI chatbot, where both the news and the AI are biased to hold a particular stance on a news topic. Interestingly, participants were far more likely to adopt the narrative of a biased chatbot over news articles with an opposing stance. Participants were also substantially more inclined to adopt the chatbot's narrative if its stance aligned with the news—all compared to a control news-article only group. Our findings suggest that the very interactive agency offered by an AI chatbot significantly enhances its perceived trust and persuasive ability compared to the ‘static’ articles from established news outlets, raising concerns about the potential for AI-driven indoctrination. We outline the reasons behind this phenomenon and conclude with the implications of biased LLMs for HCI research, as well as the risks of Generative AI undermining democratic integrity through AI-driven Information Warfare.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408449869",
    "type": "article"
  },
  {
    "title": "Inclusive design of AI’s Explanations: Just for Those Previously Left Out?",
    "doi": "https://doi.org/10.1145/3772074",
    "publication_date": "2025-10-22",
    "publication_year": 2025,
    "authors": "Md Montaser Hamid; Fatima Moussaoui; Jimena Noa Guevara; A. W. Anderson; Puja Agarwal; Jonathan Dodge; Margaret Burnett",
    "corresponding_authors": "",
    "abstract": "Motivations: Explainable Artificial Intelligence (XAI) systems aim to improve users’ understanding of AI, but XAI research has shown that many XAI explanations serve some users well while failing others. In non-AI systems, software practitioners have used inclusive design approaches to address similar problems, sometimes creating “curb-cut” improvements that benefit both underserved users and everyone else. This raises the possibility that inclusive design approaches can bring similar curb-cut improvements to AI explanations. Objectives: Our objective was to investigate possible curb-cut effects of inclusivity-driven fixes an AI product team made using an inclusive design approach (GenderMag) to improve their XAI prototype. Methods: We ran a between-subject study with 69 participants who had no formal AI background. 34 participants used the original version of the XAI prototype and the rest used the version with the AI team’s inclusivity fixes. We then compared the two groups’ mental model concepts scores and prediction accuracy, and the two prototypes’ inclusivity. Results: Our investigation produced four main results. First, the AI team’s inclusivity fixes were overall effective, resulting in overall better conceptual mental models with the new prototype. Further (second), the AI team’s inclusivity fixes were particularly beneficial to the underserved population’s conceptual mental models—which, together with the first result, constitutes a curb-cut effect. However (third), the inclusivity fixes did not improve participants’ prediction accuracy scores. Instead, it appears to have harmed them overall—a “curb-fence” effect (opposite of a curb-cut effect). Finally (fourth), the AI team’s fixes improved equity, reducing the gender gap by 45%.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415435775",
    "type": "article"
  },
  {
    "title": "Capturing Common Knowledge about Tasks",
    "doi": "https://doi.org/10.1145/2362394.2362397",
    "publication_date": "2012-09-01",
    "publication_year": 2012,
    "authors": "Yolanda Gil; Varun Ratnakar; Timothy Chklovski; Paul Groth; Denny Vrandečić",
    "corresponding_authors": "",
    "abstract": "Although to-do lists are a ubiquitous form of personal task management, there has been no work on intelligent assistance to automate, elaborate, or coordinate a user’s to-dos. Our research focuses on three aspects of intelligent assistance for to-dos. We investigated the use of intelligent agents to automate to-dos in an office setting. We collected a large corpus from users and developed a paraphrase-based approach to matching agent capabilities with to-dos. We also investigated to-dos for personal tasks and the kinds of assistance that can be offered to users by elaborating on them on the basis of substep knowledge extracted from the Web. Finally, we explored coordination of user tasks with other users through a to-do management application deployed in a popular social networking site. We discuss the emergence of Social Task Networks, which link users‘ tasks to their social network as well as to relevant resources on the Web. We show the benefits of using common sense knowledge to interpret and elaborate to-dos. Conversely, we also show that to-do lists are a valuable way to create repositories of common sense knowledge about tasks.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2145626381",
    "type": "article"
  },
  {
    "title": "An Active Sleep Monitoring Framework Using Wearables",
    "doi": "https://doi.org/10.1145/3185516",
    "publication_date": "2018-07-13",
    "publication_year": 2018,
    "authors": "H M Sajjad Hossain; Sreenivasan Ramasamy Ramamurthy; Md Abdullah Al Hafiz Khan; Nirmalya Roy",
    "corresponding_authors": "",
    "abstract": "Sleep is the most important aspect of healthy and active living. The right amount of sleep at the right time helps an individual to protect his or her physical, mental, and cognitive health and maintain his or her quality of life. The most durative of the Activities of Daily Living (ADL), sleep has a major synergic influence on a person’s fuctional, behavioral, and cognitive health. A deep understanding of sleep behavior and its relationship with its physiological signals, and contexts (such as eye or body movements), is necessary to design and develop a robust intelligent sleep monitoring system. In this article, we propose an intelligent algorithm to detect the microscopic states of sleep that fundamentally constitute the components of good and bad sleeping behaviors and thus help shape the formative assessment of sleep quality. Our initial analysis includes the investigation of several classification techniques to identify and correlate the relationship of microscopic sleep states with overall sleep behavior. Subsequently, we also propose an online algorithm based on change point detection to process and classify the microscopic sleep states. We also develop a lightweight version of the proposed algorithm for real-time sleep monitoring, recognition, and assessment at scale. For a larger deployment of our proposed model across a community of individuals, we propose an active-learning-based methodology to reduce the effort of ground-truth data collection and labeling. Finally, we evaluate the performance of our proposed algorithms on real data traces and demonstrate the efficacy of our models for detecting and assessing the fine-grained sleep states beyond an individual.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2882995261",
    "type": "article"
  },
  {
    "title": "A Roadmap to User-Controllable Social Exploratory Search",
    "doi": "https://doi.org/10.1145/3241382",
    "publication_date": "2019-08-30",
    "publication_year": 2019,
    "authors": "Cecilia di Sciascio; Peter Brusilovsky; Christoph Trattner; Eduardo Veas",
    "corresponding_authors": "",
    "abstract": "Information-seeking tasks with learning or investigative purposes are usually referred to as exploratory search. Exploratory search unfolds as a dynamic process where the user, amidst navigation, trial and error, and on-the-fly selections, gathers and organizes information (resources). A range of innovative interfaces with increased user control has been developed to support the exploratory search process. In this work, we present our attempt to increase the power of exploratory search interfaces by using ideas of social search—for instance, leveraging information left by past users of information systems. Social search technologies are highly popular today, especially for improving ranking. However, current approaches to social ranking do not allow users to decide to what extent social information should be taken into account for result ranking. This article presents an interface that integrates social search functionality into an exploratory search system in a user-controlled way that is consistent with the nature of exploratory search. The interface incorporates control features that allow the user to (i) express information needs by selecting keywords and (ii) to express preferences for incorporating social wisdom based on tag matching and user similarity. The interface promotes search transparency through color-coded stacked bars and rich tooltips. This work presents the full series of evaluations conducted to, first, assess the value of the social models in contexts independent to the user interface, in terms of objective and perceived accuracy. Then, in a study with the full-fledged system, we investigated system accuracy and subjective aspects with a structural model revealing that when users actively interacted with all of its control features, the hybrid system outperformed a baseline content-based–only tool and users were more satisfied.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2971907146",
    "type": "article"
  },
  {
    "title": "Employing a Parametric Model for Analytic Provenance",
    "doi": "https://doi.org/10.1145/2591510",
    "publication_date": "2014-04-01",
    "publication_year": 2014,
    "authors": "Yingjie Chen; Zhenyu Cheryl Qian; Robert Woodbury; John C. Dill; Chris Shaw",
    "corresponding_authors": "",
    "abstract": "We introduce a propagation-based parametric symbolic model approach to supporting analytic provenance. This approach combines a script language to capture and encode the analytic process and a parametrically controlled symbolic model to represent and reuse the logic of the analysis process. Our approach first appeared in a visual analytics system called CZSaw. Using a script to capture the analyst’s interactions at a meaningful system action level allows the creation of a parametrically controlled symbolic model in the form of a Directed Acyclic Graph (DAG). Using the DAG allows propagating changes. Graph nodes correspond to variables in CZSaw scripts, which are results (data and data visualizations) generated from user interactions. The user interacts with variables representing entities or relations to create the next step’s results. Graph edges represent dependency relationships among nodes. Any change to a variable triggers the propagation mechanism to update downstream dependent variables and in turn updates data views to reflect the change. The analyst can reuse parts of the analysis process by assigning new values to a node in the graph. We evaluated this symbolic model approach by solving three IEEE VAST Challenge contest problems (from IEEE VAST 2008, 2009, and 2010). In each of these challenges, the analyst first created a symbolic model to explore, understand, analyze, and solve a particular subproblem and then reused the model via its dependency graph propagation mechanism to solve similar subproblems. With the script and model, CZSaw supports the analytic provenance by capturing, encoding, and reusing the analysis process. The analyst can recall the chronological states of the analysis process with the CZSaw script and may interpret the underlying rationale of the analysis with the symbolic model.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W1988241068",
    "type": "article"
  },
  {
    "title": "An analysis of input-output relations in interaction with smart tangible objects",
    "doi": "https://doi.org/10.1145/2499474.2499478",
    "publication_date": "2013-07-01",
    "publication_year": 2013,
    "authors": "Evelien van de Garde-Perik; Serge Offermans; Koen van Boerdonk; K.-M. H. Lenssen; Elise van den Hoven",
    "corresponding_authors": "",
    "abstract": "This article focuses on the conceptual relation between the user's input and a system's output in interaction with smart tangible objects. Understanding this input-output relation (IO relation) is a prerequisite for the design of meaningful interaction. A meaningful IO relation allows the user to know what to do with a system to achieve a certain goal and to evaluate the outcome. The work discussed in this article followed a design research process in which four concepts were developed and prototyped. An evaluation was performed using these prototypes to investigate the effect of highly different IO relations on the user's understanding of the interaction. The evaluation revealed two types of IO relations differing in functionality and the number of mappings between the user and system actions. These two types of relations are described by two IO models that provide an overview of these mappings. Furthermore, they illustrate the role of the user and the influence of the system in the process of understanding the interaction. The analysis of the two types of IO models illustrates the value of understanding IO relations for the design of smart tangible objects.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2159503560",
    "type": "article"
  },
  {
    "title": "PromotionRank",
    "doi": "https://doi.org/10.1145/2584249",
    "publication_date": "2014-04-01",
    "publication_year": 2014,
    "authors": "Petteri Nurmi; Antti Salovaara; Andreas Forsblom; Fabian Bohnert; Patrik Floréen",
    "corresponding_authors": "",
    "abstract": "We present PromotionRank, a technique for generating a personalized ranking of grocery product promotions based on the contents of the customer’s personal shopping list. PromotionRank consists of four phases. First, information retrieval techniques are used to map shopping list items onto potentially relevant product categories. Second, since customers typically buy more items than what appear on their shopping lists, the set of potentially relevant categories is expanded using collaborative filtering. Third, we calculate a rank score for each category using a statistical interest criterion. Finally, the available promotions are ranked using the newly computed rank scores. To validate the different phases, we consider 12 months of anonymized shopping basket data from a large national supermarket. To demonstrate the effectiveness of PromotionRank, we also present results from two user studies. The first user study was conducted in a controlled setting using shopping lists of different lengths, whereas the second study was conducted within a large national supermarket using real customers and their personal shopping lists. The results of the two studies demonstrate that PromotionRank is able to identify promotions that are considered both relevant and interesting. As part of the second study, we used PromotionRank to identify relevant promotions to advertise and measure the influence of the advertisements on purchases. The results of this evaluation indicate that PromotionRank is also capable of targeting advertisements, improving sales compared to a baseline that selects random advertisements.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2168183036",
    "type": "article"
  },
  {
    "title": "Adaptive Contextualization Methods for Combating Selection Bias during High-Dimensional Visualization",
    "doi": "https://doi.org/10.1145/3009973",
    "publication_date": "2017-11-21",
    "publication_year": 2017,
    "authors": "David Gotz; Shun Sun; Nan Cao; Rita Kundu; Anne‐Marie Meyer",
    "corresponding_authors": "",
    "abstract": "Large and high-dimensional real-world datasets are being gathered across a wide range of application disciplines to enable data-driven decision making. Interactive data visualization can play a critical role in allowing domain experts to select and analyze data from these large collections. However, there is a critical mismatch between the very large number of dimensions in complex real-world datasets and the much smaller number of dimensions that can be concurrently visualized using modern techniques. This gap in dimensionality can result in high levels of selection bias that go unnoticed by users. The bias can in turn threaten the very validity of any subsequent insights. This article describes Adaptive Contextualization (AC), a novel approach to interactive visual data selection that is specifically designed to combat the invisible introduction of selection bias. The AC approach (1) monitors and models a user’s visual data selection activity, (2) computes metrics over that model to quantify the amount of selection bias after each step, (3) visualizes the metric results, and (4) provides interactive tools that help users assess and avoid bias-related problems. This article expands on an earlier article presented at ACM IUI 2016 [16] by providing a more detailed review of the AC methodology and additional evaluation results.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2768844221",
    "type": "article"
  },
  {
    "title": "Quantifying Collaboration with a Co-Creative Drawing Agent",
    "doi": "https://doi.org/10.1145/3009981",
    "publication_date": "2017-12-04",
    "publication_year": 2017,
    "authors": "Nicholas Davis; Chih-Pin Hsiao; Kunwar Yashraj Singh; Bo Lin; Brian Magerko",
    "corresponding_authors": "",
    "abstract": "This article describes a new technique for quantifying creative collaboration and applies it to the user study evaluation of a co-creative drawing agent. We present a cognitive framework called creative sense-making that provides a new method to visualize and quantify the interaction dynamics of creative collaboration, for example, the rhythm of interaction, style of turn taking, and the manner in which participants are mutually making sense of a situation. The creative sense-making framework includes a qualitative coding technique, interaction coding software, an analysis method, and the cognitive theory behind these applications. This framework and analysis method are applied to empirical studies of the Drawing Apprentice collaborative sketching system to compare human collaboration with a co-creative AI agent vs. a Wizard of Oz setup. The analysis demonstrates how the proposed technique can be used to analyze interaction data using continuous functions (e.g., integrations and moving averages) to measure and evaluate how collaborations unfold through time.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2771410528",
    "type": "article"
  },
  {
    "title": "Modeling and Computational Characterization of Twitter Customer Service Conversations",
    "doi": "https://doi.org/10.1145/3213014",
    "publication_date": "2019-03-18",
    "publication_year": 2019,
    "authors": "Shereen Oraby; Mansurul Bhuiyan; Pritam Gundecha; Jalal Mahmud; Rama Akkiraju",
    "corresponding_authors": "",
    "abstract": "Given the increasing popularity of customer service dialogue on Twitter, analysis of conversation data is essential to understanding trends in customer and agent behavior for the purpose of automating customer service interactions. In this work, we develop a novel taxonomy of fine-grained “dialogue acts” frequently observed in customer service, showcasing acts that are more suited to the domain than the more generic existing taxonomies. Using a sequential SVM-HMM model, we model conversation flow, predicting the dialogue act of a given turn in real time, and showcase this using our “PredDial” portal. We characterize differences between customer and agent behavior in Twitter customer service conversations and investigate the effect of testing our system on different customer service industries. Finally, we use a data-driven approach to predict important conversation outcomes: customer satisfaction, customer frustration, and overall problem resolution. We show that the type and location of certain dialogue acts in a conversation have a significant effect on the probability of desirable and undesirable outcomes and present actionable rules based on our findings. We explore the correlations between different dialogue acts and the outcome of the conversations in detail using an actionable-rule discovery task by leveraging a state-of-the-art sequential rule mining algorithm while modeling a set of conversations as a set of sequences. The patterns and rules we derive can be used as guidelines for outcome-driven automated customer service platforms.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2922594681",
    "type": "article"
  },
  {
    "title": "HILC",
    "doi": "https://doi.org/10.1145/3234508",
    "publication_date": "2019-03-18",
    "publication_year": 2019,
    "authors": "Thanapong Intharah; Daniyar Turmukhambetov; Gabriel Brostow",
    "corresponding_authors": "",
    "abstract": "Creating automation scripts for tasks involving Graphical User Interface (GUI) interactions is hard. It is challenging because not all software applications allow access to a program’s internal state, nor do they all have accessibility APIs. Although much of the internal state is exposed to the user through the GUI, it is hard to programmatically operate the GUI’s widgets. To that end, we developed a system prototype that learns by demonstration, called HILC (Help, It Looks Confusing). Users, both programmers and non-programmers, train HILC to synthesize a task script by demonstrating the task. A demonstration produces the needed screenshots and their corresponding mouse-keyboard signals. After the demonstration, the user answers follow-up questions. We propose a user-in-the-loop framework that learns to generate scripts of actions performed on visible elements of graphical applications. Although pure programming by demonstration is still unrealistic due to a computer’s limited understanding of user intentions, we use quantitative and qualitative experiments to show that non-programming users are willing and effective at answering follow-up queries posed by our system, to help with confusing parts of the demonstrations. Our models of events and appearances are surprisingly simple but are combined effectively to cope with varying amounts of supervision. The best available baseline, Sikuli Slides, struggled to assist users in the majority of the tests in our user study experiments. The prototype with our proposed approach successfully helped users accomplish simple linear tasks, complicated tasks (monitoring, looping, and mixed), and tasks that span across multiple applications. Even when both systems could ultimately perform a task, ours was trained and refined by the user in less time.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2922989098",
    "type": "article"
  },
  {
    "title": "PRIME: A Personalized Recommender System for Information Visualization Methods via Extended Matrix Completion",
    "doi": "https://doi.org/10.1145/3366484",
    "publication_date": "2021-03-15",
    "publication_year": 2021,
    "authors": "Xiaoyu Chen; Nathan Lau; Ran Jin",
    "corresponding_authors": "",
    "abstract": "Adapting user interface designs for specific tasks performed by different users is a challenging yet important problem. Automatically adapting visualization designs to users and contexts (e.g., tasks, display devices, environments, etc.) can theoretically improve human–computer interaction to acquire insights from complex datasets. However, effectiveness of any specific visualization is moderated by individual differences in knowledge, skills, and abilities for different contexts. A modeling framework called P ersonalized R ecommender System for I nformation visualization M ethods via E xtended matrix completion (PRIME) is proposed for recommending the optimal visualization designs for individual users in different contexts. PRIME quantitatively models covariates (e.g., psychological and behavioral measurements) to predict recommendation scores (e.g., perceived complexity, mental workload, etc.) for users to adapt the visualization specific to the context. An evaluation study was conducted and showed that PRIME can achieve satisfactory recommendation accuracy for adapting visualization, even when there are limited historical data. PRIME can make accurate recommendations even for new users or new tasks based on historical wearable sensor signals and recommendation scores. This capability contributes to designing a new generation of visualization systems that will adapt to users’ states. PRIME can support researchers in reducing the sample size requirements to quantify individual differences, and practitioners in adapting visualizations according to user states and contexts.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3136610526",
    "type": "article"
  },
  {
    "title": "Socially Aware Navigation: A Non-linear Multi-objective Optimization Approach",
    "doi": "https://doi.org/10.1145/3453445",
    "publication_date": "2021-06-30",
    "publication_year": 2021,
    "authors": "Santosh Balajee Banisetty; Scott Forer; Logan Yliniemi; Monica Nicolescu; David Feil-Seifer",
    "corresponding_authors": "",
    "abstract": "Mobile robots are increasingly populating homes, hospitals, shopping malls, factory floors, and other human environments. Human society has social norms that people mutually accept; obeying these norms is an essential signal that someone is participating socially with respect to the rest of the population. For robots to be socially compatible with humans, it is crucial for robots to obey these social norms. In prior work, we demonstrated a Socially-Aware Navigation (SAN) planner, based on Pareto Concavity Elimination Transformation (PaCcET), in a hallway scenario, optimizing two objectives so the robot does not invade the personal space of people. This article extends our PaCcET-based SAN planner to multiple scenarios with more than two objectives. We modified the Robot Operating System’s (ROS) navigation stack to include PaCcET in the local planning task. We show that our approach can accommodate multiple Human-Robot Interaction (HRI) scenarios. Using the proposed approach, we achieved successful HRI in multiple scenarios such as hallway interactions, an art gallery, waiting in a queue, and interacting with a group. We implemented our method on a simulated PR2 robot in a 2D simulator (Stage) and a pioneer-3DX mobile robot in the real-world to validate all the scenarios. A comprehensive set of experiments shows that our approach can handle multiple interaction scenarios on both holonomic and non-holonomic robots; hence, it can be a viable option for a Unified Socially-Aware Navigation (USAN).",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3185548788",
    "type": "article"
  },
  {
    "title": "Minimal Interaction Content Discovery in Recommender Systems",
    "doi": "https://doi.org/10.1145/2845090",
    "publication_date": "2016-07-20",
    "publication_year": 2016,
    "authors": "Branislav Kveton; Shlomo Berkovsky",
    "corresponding_authors": "",
    "abstract": "Many prior works in recommender systems focus on improving the accuracy of item rating predictions. In comparison, the areas of recommendation interfaces and user-recommender interaction remain underexplored. In this work, we look into the interaction of users with the recommendation list, aiming to devise a method that simplifies content discovery and minimizes the cost of reaching an item of interest. We quantify this cost by the number of user interactions (clicks and scrolls) with the recommendation list. To this end, we propose generalized linear search (GLS), an adaptive combination of the established linear and generalized search (GS) approaches. GLS leverages the advantages of these two approaches, and we prove formally that it performs at least as well as GS. We also conduct a thorough experimental evaluation of GLS and compare it to several baselines and heuristic approaches in both an offline and live evaluation. The results of the evaluation show that GLS consistently outperforms the baseline approaches and is also preferred by users. In summary, GLS offers an efficient and easy-to-use means for content discovery in recommender systems.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2475646629",
    "type": "article"
  },
  {
    "title": "Man and the Machine: Effects of AI-assisted Human Labeling on Interactive Annotation of Real-time Video Streams",
    "doi": "https://doi.org/10.1145/3649457",
    "publication_date": "2024-02-29",
    "publication_year": 2024,
    "authors": "Marko Radeta; Rúben Freitas; Claudio Rodrigues; Agustin Zuniga; Ngoc Thi Nguyen; Huber Flores; Petteri Nurmi",
    "corresponding_authors": "",
    "abstract": "AI-assisted interactive annotation is a powerful way to facilitate data annotation—a prerequisite for constructing robust AI models. While AI-assisted interactive annotation has been extensively studied in static settings, less is known about its usage in dynamic scenarios where the annotators operate under time and cognitive constraints, e.g., while detecting suspicious or dangerous activities from real-time surveillance feeds. Understanding how AI can assist annotators in these tasks and facilitate consistent annotation is paramount to ensure high performance for AI models trained on these data. We address this gap in interactive machine learning (IML) research, contributing an extensive investigation of the benefits, limitations, and challenges of AI-assisted annotation in dynamic application use cases. We address both the effects of AI on annotators and the effects of (AI) annotations on the performance of AI models trained on annotated data in real-time video annotations. We conduct extensive experiments that compare annotation performance at two annotator levels (expert and non-expert) and two interactive labeling techniques (with and without AI assistance). In a controlled study with \\(N=34\\) annotators and a follow-up study with 51,963 images and their annotation labels being input to the AI model, we demonstrate that the benefits of AI-assisted models are greatest for non-expert users and for cases where targets are only partially or briefly visible. The expert users tend to outperform or achieve similar performance as the AI model. Labels combining AI and expert annotations result in the best overall performance as the AI reduces overflow and latency in the expert annotations. We derive guidelines for the use of AI-assisted human annotation in real-time dynamic use cases.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4392294975",
    "type": "article"
  },
  {
    "title": "Insights into Natural Language Database Query Errors: From Attention Misalignment to User Handling Strategies",
    "doi": "https://doi.org/10.1145/3650114",
    "publication_date": "2024-03-02",
    "publication_year": 2024,
    "authors": "Zheng Ning; Yuan Tian; Zheng Zhang; Tianyi Zhang; Toby Jia-Jun Li",
    "corresponding_authors": "",
    "abstract": "Querying structured databases with natural language (NL2SQL) has remained a difficult problem for years. Recently, the advancement of machine learning (ML), natural language processing (NLP), and large language models (LLM) have led to significant improvements in performance, with the best model achieving ∼ 85% percent accuracy on the benchmark Spider dataset. However, there is a lack of a systematic understanding of the types, causes, and effectiveness of error-handling mechanisms of errors for erroneous queries nowadays. To bridge the gap, a taxonomy of errors made by four representative NL2SQL models was built in this work, along with an in-depth analysis of the errors. Second, the causes of model errors were explored by analyzing the model-human attention alignment to the natural language query. Last, a within-subjects user study with 26 participants was conducted to investigate the effectiveness of three interactive error-handling mechanisms in NL2SQL. Findings from this paper shed light on the design of model structure and error discovery and repair strategies for natural language data query interfaces in the future.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4392348090",
    "type": "article"
  },
  {
    "title": "Categorical and Continuous Features in Counterfactual Explanations of AI Systems",
    "doi": "https://doi.org/10.1145/3673907",
    "publication_date": "2024-06-20",
    "publication_year": 2024,
    "authors": "Greta Warren; Ruth M. J. Byrne; Mark T. Keane",
    "corresponding_authors": "",
    "abstract": "Recently, eXplainable AI (XAI) research has focused on the use of counterfactual explanations to address interpretability, algorithmic recourse, and bias in AI system decision-making. The developers of these algorithms claim they meet user requirements in generating counterfactual explanations with “plausible”, “actionable” or “causally important” features. However, few of these claims have been tested in controlled psychological studies. Hence, we know very little about which aspects of counterfactual explanations really help users understand the decisions of AI systems. Nor do we know whether counterfactual explanations are an advance on more traditional causal explanations that have a longer history in AI (e.g., in expert systems). Accordingly, we carried out three user studies to (i) test a fundamental distinction in feature-types, between categorical and continuous features, and (ii) compare the relative effectiveness of counterfactual and causal explanations. The studies used a simulated, automated decision-making app that determined safe driving limits after drinking alcohol, based on predicted blood alcohol content, where users’ responses were measured objectively (using predictive accuracy) and subjectively (using satisfaction and trust judgments). Study 1 (N = 127) showed that users understand explanations referring to categorical features more readily than those referring to continuous features. It also discovered a dissociation between objective and subjective measures: counterfactual explanations elicited higher accuracy than no-explanation controls but elicited no more accuracy than causal explanations, yet counterfactual explanations elicited greater satisfaction and trust than causal explanations. In Study 2 (N = 136) we transformed the continuous features of presented items to be categorical (i.e., binary) and found that these converted features led to highly accurate responding. Study 3 (N = 211) explicitly compared matched items involving either mixed features (i.e., a mix of categorical and continuous features) or categorical features (i.e., categorical and categorically-transformed continuous features), and found that users were more accurate when categorically-transformed features were used instead of continuous ones. It also replicated the dissociation between objective and subjective effects of explanations. The findings delineate important boundary conditions for current and future counterfactual explanation methods in XAI.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4399858197",
    "type": "article"
  },
  {
    "title": "Towards Understanding AI Delegation: The Role of Self-Efficacy and Visual Processing Ability",
    "doi": "https://doi.org/10.1145/3696423",
    "publication_date": "2024-10-15",
    "publication_year": 2024,
    "authors": "Monika Westphal; Patrick Hemmer; Michael Vössing; Max Schemmer; Sebastian Vetter; Gerhard Satzger",
    "corresponding_authors": "",
    "abstract": "Recent work has proposed artificial intelligence (AI) models that can learn to decide whether to make a prediction for a task instance or to delegate it to a human by considering both parties’ capabilities. In simulations with synthetically generated or context-independent human predictions, delegation can help improve the performance of human-AI teams—compared to humans or the AI model completing the task alone. However, so far, it remains unclear how humans perform and how they perceive the task when individual instances of a task are delegated to them by an AI model. In an experimental study with 196 participants, we show that task performance and task satisfaction improve for the instances delegated by the AI model, regardless of whether humans are aware of the delegation. Additionally, we identify humans’ increased levels of self-efficacy as the underlying mechanism for these improvements in performance and satisfaction, and one dimension of cognitive ability as a moderator to this effect. In particular, AI delegation can buffer potential negative effects on task performance and task satisfaction for humans with low visual processing ability. Our findings provide initial evidence that allowing AI models to take over more management responsibilities can be an effective form of human-AI collaboration in workplaces. 1",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4403429639",
    "type": "article"
  },
  {
    "title": "Visualizing Ubiquitously Sensed Measures of Motor Ability in Multiple Sclerosis",
    "doi": "https://doi.org/10.1145/3181670",
    "publication_date": "2018-06-30",
    "publication_year": 2018,
    "authors": "Cecily Morrison; Kit Huckvale; Bob Corish; Richard Banks; Martin Grayson; Jonas F. Dorn; Abigail Sellen; Sân Lindley",
    "corresponding_authors": "",
    "abstract": "Sophisticated ubiquitous sensing systems are being used to measure motor ability in clinical settings. Intended to augment clinical decision-making, the interpretability of the machine-learning measurements underneath becomes critical to their use. We explore how visualization can support the interpretability of machine-learning measures through the case of Assess MS, a system to support the clinical assessment of Multiple Sclerosis. A substantial design challenge is to make visible the algorithm's decision-making process in a way that allows clinicians to integrate the algorithm's result into their own decision process. To this end, we present a series of design iterations that probe the challenges in supporting interpretability in a real-world system. The key contribution of this article is to illustrate that simply making visible the algorithmic decision-making process is not helpful in supporting clinicians in their own decision-making process. It disregards that people and algorithms make decisions in different ways. Instead, we propose that visualisation can provide context to algorithmic decision-making, rendering observable a range of internal workings of the algorithm from data quality issues to the web of relationships generated in the machine-learning process.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2883406490",
    "type": "article"
  },
  {
    "title": "Cooperative augmentation of mobile smart objects with projected displays",
    "doi": "https://doi.org/10.1145/2499474.2499476",
    "publication_date": "2013-07-01",
    "publication_year": 2013,
    "authors": "David Molyneaux; Hans Gellersen; Joe Finney",
    "corresponding_authors": "",
    "abstract": "Sensors, processors, and radios can be integrated invisibly into objects to make them smart and sensitive to user interaction, but feedback is often limited to beeps, blinks, or buzzes. We propose to redress this input-output imbalance by augmentation of smart objects with projected displays, that—unlike physical displays—allow seamless integration with the natural appearance of an object. In this article, we investigate how, in a ubiquitous computing world, smart objects can acquire and control a projection. We consider that projectors and cameras are ubiquitous in the environment, and we develop a novel conception and system that enables smart objects to spontaneously associate with projector-camera systems for cooperative augmentation. Projector-camera systems are conceived as generic, supporting standard computer vision methods for different appearance cues, and smart objects provide a model of their appearance for method selection at runtime, as well as sensor observations to constrain the visual detection process. Cooperative detection results in accurate location and pose of the object, which is then tracked for visual augmentation in response to display requests by the smart object. In this article, we define the conceptual framework underlying our approach; report on computer vision experiments that give original insight into natural appearance-based detection of everyday objects; show how object sensing can be used to increase speed and robustness of visual detection; describe and evaluate a fully implemented system; and describe two smart object applications to illustrate the system's cooperative augmentation process and the embodied interactions it enables with smart objects.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2033171359",
    "type": "article"
  },
  {
    "title": "Introduction to the special issue on interaction with smart objects",
    "doi": "https://doi.org/10.1145/2499474.2499475",
    "publication_date": "2013-07-01",
    "publication_year": 2013,
    "authors": "Daniel Schreiber; Kris Luyten; Max Mühlhäuser; Oliver Brdiczka; Melanie Hartman",
    "corresponding_authors": "",
    "abstract": "Smart objects can be smart because of the information and communication technology that is added to human-made artifacts. It is not, however, the technology itself that makes them smart but rather the way in which the technology is integrated, and their smartness surfaces through how people are able to interact with these objects. Hence, the key challenge for making smart objects successful is to design usable and useful interactions with them. We list five features that can contribute to the smartness of an object, and we discuss how smart objects can help resolve the simplicity-featurism paradox. We conclude by introducing the three articles in this special issue, which dive into various aspects of smart object interaction: augmenting objects with projection, service-oriented interaction with smart objects via a mobile portal, and an analysis of input-output relations in interaction with tangible smart objects.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2041658154",
    "type": "article"
  },
  {
    "title": "LiveAction",
    "doi": "https://doi.org/10.1145/2533670.2533672",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Saleema Amershi; Jalal Mahmud; Jeffrey Nichols; Tessa Lau; German Attanasio Ruiz",
    "corresponding_authors": "",
    "abstract": "Task automation systems promise to increase human productivity by assisting us with our mundane and difficult tasks. These systems often rely on people to (1) identify the tasks they want automated and (2) specify the procedural steps necessary to accomplish those tasks (i.e., to create task models). However, our interviews with users of a Web task automation system reveal that people find it difficult to identify tasks to automate and most do not even believe they perform repetitive tasks worthy of automation. Furthermore, even when automatable tasks are identified, the well-recognized difficulties of specifying task steps often prevent people from taking advantage of these automation systems. In this research, we analyze real Web usage data and find that people do in fact repeat behaviors on the Web and that automating these behaviors, regardless of their complexity, would reduce the overall number of actions people need to perform when completing their tasks, potentially saving time. Motivated by these findings, we developed LiveAction, a fully-automated approach to generating task models from Web usage data. LiveAction models can be used to populate the task model repositories required by many automation systems, helping us take advantage of automation in our everyday lives.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2157279946",
    "type": "article"
  },
  {
    "title": "AttentiveVideo",
    "doi": "https://doi.org/10.1145/3232233",
    "publication_date": "2019-03-18",
    "publication_year": 2019,
    "authors": "Phuong Thao Pham; Jingtao Wang",
    "corresponding_authors": "",
    "abstract": "Understanding a target audience's emotional responses to a video advertisement is crucial to evaluate the advertisement's effectiveness. However, traditional methods for collecting such information are slow, expensive, and coarse grained. We propose AttentiveVideo, a scalable intelligent mobile interface with corresponding inference algorithms to monitor and quantify the effects of mobile video advertising in real time. Without requiring additional sensors, AttentiveVideo employs a combination of implicit photoplethysmography (PPG) sensing and facial expression analysis (FEA) to detect the attention, engagement , and sentiment of viewers as they watch video advertisements on unmodified smartphones. In a 24-participant study, AttentiveVideo achieved good accuracy on a wide range of emotional measures (the best average accuracy = 82.6% across nine measures). While feature fusion alone did not improve prediction accuracy with a single model, it significantly improved the accuracy when working together with model fusion. We also found that the PPG sensing channel and the FEA technique have different strength in data availability, latency detection, accuracy, and usage environment. These findings show the potential for both low-cost collection and deep understanding of emotional responses to mobile video advertisements.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2922789690",
    "type": "article"
  },
  {
    "title": "Profiling Personality Traits with Games",
    "doi": "https://doi.org/10.1145/3230738",
    "publication_date": "2019-03-18",
    "publication_year": 2019,
    "authors": "Carlos Pereira Santos; Kevin Hutchinson; Vassilis-Javed Khan; Panos Markopoulos",
    "corresponding_authors": "",
    "abstract": "Trying to understand a player's characteristics with regards to a computer game is a major line of research known as player modeling. The purpose of player modeling is typically the adaptation of the game itself. We present two studies that extend player modeling into player profiling by trying to identify abstract personality traits, such as the need for cognition and self-esteem , through a player's in-game behavior. We present evidence that game mechanics that can be broadly adopted by several game genres, such as hints and a player's self-evaluation at the end of a level, correlate with the aforementioned personality traits. We conclude by presenting future directions for research regarding this topic, discuss the direct applications for the games industry, and explore how games can be developed as profiling tools with applications to other contexts.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2924915734",
    "type": "article"
  },
  {
    "title": "Unobtrusive Activity Recognition and Position Estimation for Work Surfaces Using RF-Radar Sensing",
    "doi": "https://doi.org/10.1145/3241383",
    "publication_date": "2019-08-09",
    "publication_year": 2019,
    "authors": "Daniel Avrahami; Mitesh Patel; Yusuke Yamaura; Sven Kratz; Matthew Cooper",
    "corresponding_authors": "",
    "abstract": "Activity recognition is a core component of many intelligent and context-aware systems. We present a solution for discreetly and unobtrusively recognizing common work activities above a work surface without using cameras. We demonstrate our approach, which utilizes an RF-radar sensor mounted under the work surface, in three domains: recognizing work activities at a convenience-store counter, recognizing common office deskwork activities, and estimating the position of customers in a showroom environment. Our examples illustrate potential benefits for both post-hoc business analytics and for real-time applications. Our solution was able to classify seven clerk activities with 94.9% accuracy using data collected in a lab environment and able to recognize six common deskwork activities collected in real offices with 95.3% accuracy. Using two sensors simultaneously, we demonstrate coarse position estimation around a large surface with 95.4% accuracy. We show that using multiple projections of RF signal leads to improved recognition accuracy. Finally, we show how smartwatches worn by users can be used to attribute an activity, recognized with the RF sensor, to a particular user in multi-user scenarios. We believe our solution can mitigate some of users’ privacy concerns associated with cameras and is useful for a wide range of intelligent systems.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2967503331",
    "type": "article"
  },
  {
    "title": "Learning from Sets of Items in Recommender Systems",
    "doi": "https://doi.org/10.1145/3326128",
    "publication_date": "2019-07-25",
    "publication_year": 2019,
    "authors": "Mohit Sharma; F. Maxwell Harper; George Karypis",
    "corresponding_authors": "",
    "abstract": "Most of the existing recommender systems use the ratings provided by users on individual items. An additional source of preference information is to use the ratings that users provide on sets of items. The advantages of using preferences on sets are two-fold. First, a rating provided on a set conveys some preference information about each of the set's items, which allows us to acquire a user's preferences for more items that the number of ratings that the user provided. Second, due to privacy concerns, users may not be willing to reveal their preferences on individual items explicitly but may be willing to provide a single rating to a set of items, since it provides some level of information hiding. This paper investigates two questions related to using set-level ratings in recommender systems. First, how users' item-level ratings relate to their set-level ratings. Second, how collaborative filtering-based models for item-level rating prediction can take advantage of such set-level ratings. We have collected set-level ratings from active users of Movielens on sets of movies that they have rated in the past. Our analysis of these ratings shows that though the majority of the users provide the average of the ratings on a set's constituent items as the rating on the set, there exists a significant number of users that tend to consistently either under- or over-rate the sets. We have developed collaborative filtering-based methods to explicitly model these user behaviors that can be used to recommend items to users. Experiments on real data and on synthetic data that resembles the under- or over-rating behavior in the real data, demonstrate that these models can recover the overall characteristics of the underlying data and predict the user's ratings on individual items.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W3100318323",
    "type": "article"
  },
  {
    "title": "A Stimulus-Response Framework for Robot Control",
    "doi": "https://doi.org/10.1145/2677198",
    "publication_date": "2015-01-28",
    "publication_year": 2015,
    "authors": "Mario Gianni; Geert-Jan M. Kruijff; Fiora Pirri",
    "corresponding_authors": "",
    "abstract": "We propose in this article a new approach to robot cognitive control based on a stimulus-response framework that models both a robot’s stimuli and the robot’s decision to switch tasks in response to or inhibit the stimuli. In an autonomous system, we expect a robot to be able to deal with the whole system of stimuli and to use them to regulate its behavior in real-world applications. The proposed framework contributes to the state of the art of robot planning and high-level control in that it provides a novel perspective on the interaction between robot and environment. Our approach is inspired by Gibson’s constructive view of the concept of a stimulus and by the cognitive control paradigm of task switching. We model the robot’s response to a stimulus in three stages. We start by defining the stimuli as perceptual functions yielded by the active robot processes and learned via an informed logistic regression. Then we model the stimulus-response relationship by estimating a score matrix that leads to the selection of a single response task for each stimulus, basing the estimation on low-rank matrix factorization. The decision about switching takes into account both an interference cost and a reconfiguration cost. The interference cost weighs the effort of discontinuing the current robot mental state to switch to a new state, whereas the reconfiguration cost weighs the effort of activating the response task. A choice is finally made based on the payoff of switching. Because processes play such a crucial role both in the stimulus model and in the stimulus-response model, and because processes are activated by actions, we address also the process model, which is built on a theory of action. The framework is validated by several experiments that exploit a full implementation on an advanced robotic platform and is compared with two known approaches to replanning. Results demonstrate the practical value of the system in terms of robot autonomy, flexibility, and usability.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2076589559",
    "type": "article"
  },
  {
    "title": "Interpreting Natural Language Instructions Using Language, Vision, and Behavior",
    "doi": "https://doi.org/10.1145/2629632",
    "publication_date": "2014-08-11",
    "publication_year": 2014,
    "authors": "Luciana Benotti; Tessa Lau; Martín Villalba",
    "corresponding_authors": "",
    "abstract": "We define the problem of automatic instruction interpretation as follows. Given a natural language instruction, can we automatically predict what an instruction follower, such as a robot, should do in the environment to follow that instruction? Previous approaches to automatic instruction interpretation have required either extensive domain-dependent rule writing or extensive manually annotated corpora. This article presents a novel approach that leverages a large amount of unannotated, easy-to-collect data from humans interacting in a game-like environment. Our approach uses an automatic annotation phase based on artificial intelligence planning, for which two different annotation strategies are compared: one based on behavioral information and the other based on visibility information. The resulting annotations are used as training data for different automatic classifiers. This algorithm is based on the intuition that the problem of interpreting a situated instruction can be cast as a classification problem of choosing among the actions that are possible in the situation. Classification is done by combining language, vision, and behavior information. Our empirical analysis shows that machine learning classifiers achieve 77% accuracy on this task on available English corpora and 74% on similar German corpora. Finally, the inclusion of human feedback in the interpretation process is shown to boost performance to 92% for the English corpus and 90% for the German corpus.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2087163675",
    "type": "article"
  },
  {
    "title": "A Dynamic Pen-Based Interface for Writing and Editing Complex Mathematical Expressions With Math Boxes",
    "doi": "https://doi.org/10.1145/2946795",
    "publication_date": "2016-07-20",
    "publication_year": 2016,
    "authors": "Eugene M. Taranta; Andres Vargas; Spencer P. Compton; Joseph J. LaViola",
    "corresponding_authors": "",
    "abstract": "Math boxes is a recently introduced pen-based user interface for simplifying the task of hand writing difficult mathematical expressions. Visible bounding boxes around subexpressions are automatically generated as the system detects relevant spatial relationships between symbols including superscripts, subscripts, and fractions. Subexpressions contained in a math box can then be extended by adding new terms directly into its given bounds. When new characters are accepted, box boundaries are dynamically resized and neighboring terms are translated to make room for the larger box. Feedback on structural recognition is given via the boxes themselves. In this work, we extend the math boxes interface to include support for subexpression modifications via a new set of pen-based interactions. Specifically, techniques to expand and rearrange terms in a given expression are introduced. To evaluate the usefulness of our proposed methods, we first conducted a user study in which participants wrote a variety of equations ranging in complexity from a simple polynomial to the more difficult expected value of the logistic distribution. The math boxes interface is compared against the commonly used offset typeset (small) method, where recognized expressions are typeset in a system font near the user’s unmodified ink. In this initial study, we find that the fluidness of the offset method is preferred for simple expressions but that, as difficulty increases, our math boxes method is overwhelmingly preferred. We then conducted a second user study that focused only on modifying various mathematical expressions. In general, participants worked faster with the math boxes interface, and most new techniques were well received. On the basis of the two user studies, we discuss the implications of the math boxes interface and identify areas where improvements are possible.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2495726989",
    "type": "article"
  },
  {
    "title": "Combining the Projective Consciousness Model and Virtual Humans for Immersive Psychological Research: A Proof-of-concept Simulating a ToM Assessment",
    "doi": "https://doi.org/10.1145/3583886",
    "publication_date": "2023-02-21",
    "publication_year": 2023,
    "authors": "David Rudrauf; G. Sergeant-Perhtuis; Yvain Tisserand; Teerawat Monnor; Valentin Durand de Gevigney; O. Belli",
    "corresponding_authors": "",
    "abstract": "Relating explicit psychological mechanisms and observable behaviours is a central aim of psychological and behavioural science. One of the challenges is to understand and model the role of consciousness and, in particular, its subjective perspective as an internal level of representation (including for social cognition) in the governance of behaviour. Toward this aim, we implemented the principles of the Projective Consciousness Model (PCM) into artificial agents embodied as virtual humans, extending a previous implementation of the model. Our goal was to offer a proof-of-concept, based purely on simulations, as a basis for a future methodological framework. Its overarching aim is to be able to assess hidden psychological parameters in human participants, based on a model relevant to consciousness research, in the context of experiments in virtual reality. As an illustration of the approach, we focused on simulating the role of Theory of Mind (ToM) in the choice of strategic behaviours of approach and avoidance to optimise the satisfaction of agents’ preferences. We designed a main experiment in a virtual environment that could be used with real humans, allowing us to classify behaviours as a function of order of ToM, up to the second order. We show that agents using the PCM demonstrated expected behaviours with consistent parameters of ToM in this experiment. We also show that the agents could be used to estimate correctly each other’s order of ToM. Furthermore, in a supplementary experiment, we demonstrated how the agents could simultaneously estimate order of ToM and preferences attributed to others to optimize behavioural outcomes. Future studies will empirically assess and fine tune the framework with real humans in virtual reality experiments.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4321455189",
    "type": "article"
  },
  {
    "title": "Visual Analytics of Neuron Vulnerability to Adversarial Attacks on Convolutional Neural Networks",
    "doi": "https://doi.org/10.1145/3587470",
    "publication_date": "2023-03-15",
    "publication_year": 2023,
    "authors": "Yiran Li; Junpeng Wang; Takanori Fujiwara; Kwan‐Liu Ma",
    "corresponding_authors": "",
    "abstract": "Adversarial attacks on a convolutional neural network (CNN)—injecting human-imperceptible perturbations into an input image—could fool a high-performance CNN into making incorrect predictions. The success of adversarial attacks raises serious concerns about the robustness of CNNs, and prevents them from being used in safety-critical applications, such as medical diagnosis and autonomous driving. Our work introduces a visual analytics approach to understanding adversarial attacks by answering two questions: (1) Which neurons are more vulnerable to attacks? and (2) Which image features do these vulnerable neurons capture during the prediction? For the first question, we introduce multiple perturbation-based measures to break down the attacking magnitude into individual CNN neurons and rank the neurons by their vulnerability levels. For the second, we identify image features (e.g., cat ears) that highly stimulate a user-selected neuron to augment and validate the neuron’s responsibility. Furthermore, we support an interactive exploration of a large number of neurons by aiding with hierarchical clustering based on the neurons’ roles in the prediction. To this end, a visual analytics system is designed to incorporate visual reasoning for interpreting adversarial attacks. We validate the effectiveness of our system through multiple case studies as well as feedback from domain experts.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4324359912",
    "type": "article"
  },
  {
    "title": "LIMEADE: From AI Explanations to Advice Taking",
    "doi": "https://doi.org/10.1145/3589345",
    "publication_date": "2023-03-28",
    "publication_year": 2023,
    "authors": "Benjamin Charles Germain Lee; Doug Downey; Kyle Lo; Daniel S. Weld",
    "corresponding_authors": "",
    "abstract": "Research in human-centered AI has shown the benefits of systems that can explain their predictions. Methods that allow AI to take advice from humans in response to explanations are similarly useful. While both capabilities are well developed for transparent learning models (e.g., linear models and GA 2 Ms) and recent techniques (e.g., LIME and SHAP) can generate explanations for opaque models, little attention has been given to advice methods for opaque models. This article introduces LIMEADE, the first general framework that translates both positive and negative advice (expressed using high-level vocabulary such as that employed by post hoc explanations) into an update to an arbitrary, underlying opaque model. We demonstrate the generality of our approach with case studies on 70 real-world models across two broad domains: image classification and text recommendation. We show that our method improves accuracy compared to a rigorous baseline on the image classification domains. For the text modality, we apply our framework to a neural recommender system for scientific papers on a public website; our user study shows that our framework leads to significantly higher perceived user control, trust, and satisfaction.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4361213634",
    "type": "article"
  },
  {
    "title": "VERB: Visualizing and Interpreting Bias Mitigation Techniques Geometrically for Word Representations",
    "doi": "https://doi.org/10.1145/3604433",
    "publication_date": "2023-06-22",
    "publication_year": 2023,
    "authors": "Archit Rathore; Sunipa Dev; Jeff M. Phillips; Vivek Srikumar; Yan Zheng; Chin‐Chia Michael Yeh; Junpeng Wang; Wei Zhang; Bei Wang",
    "corresponding_authors": "",
    "abstract": "Word vector embeddings have been shown to contain and amplify biases in the data they are extracted from. Consequently, many techniques have been proposed to identify, mitigate, and attenuate these biases in word representations. In this article, we utilize interactive visualization to increase the interpretability and accessibility of a collection of state-of-the-art debiasing techniques. To aid this, we present the Visualization of Embedding Representations for deBiasing (VERB) system, an open-source web-based visualization tool that helps users gain a technical understanding and visual intuition of the inner workings of debiasing techniques, with a focus on their geometric properties. In particular, VERB offers easy-to-follow examples that explore the effects of these debiasing techniques on the geometry of high-dimensional word vectors. To help understand how various debiasing techniques change the underlying geometry, VERB decomposes each technique into interpretable sequences of primitive transformations and highlights their effect on the word vectors using dimensionality reduction and interactive visual exploration. VERB is designed to target natural language processing (NLP) practitioners who are designing decision-making systems on top of word embeddings and researchers working with the fairness and ethics of machine learning systems in NLP. It can also serve as a visual medium for education, which helps an NLP novice understand and mitigate biases in word embeddings.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4381612850",
    "type": "article"
  },
  {
    "title": "Explainable Activity Recognition in Videos using Deep Learning and Tractable Probabilistic Models",
    "doi": "https://doi.org/10.1145/3626961",
    "publication_date": "2023-10-12",
    "publication_year": 2023,
    "authors": "Chiradeep Roy; Mahsan Nourani; Shivvrat Arya; Mahesh Shanbhag; Tahrima Rahman; Eric D. Ragan; Nicholas Ruozzi; Vibhav Gogate",
    "corresponding_authors": "",
    "abstract": "We consider the following video activity recognition (VAR) task: given a video, infer the set of activities being performed in the video and assign each frame to an activity. Although VAR can be solved accurately using existing deep learning techniques, deep networks are neither interpretable nor explainable and as a result their use is problematic in high stakes decision-making applications (in healthcare, experimental Biology, aviation, law, etc.). In such applications, failure may lead to disastrous consequences and therefore it is necessary that the user is able to either understand the inner workings of the model or probe it to understand its reasoning patterns for a given decision. We address these limitations of deep networks by proposing a new approach that feeds the output of a deep model into a tractable, interpretable probabilistic model called a dynamic conditional cutset network that is defined over the explanatory and output variables and then performing joint inference over the combined model. The two key benefits of using cutset networks are: (a) they explicitly model the relationship between the output and explanatory variables and as a result, the combined model is likely to be more accurate than the vanilla deep model and (b) they can answer reasoning queries in polynomial time and as a result, they can derive meaningful explanations by efficiently answering explanation queries. We demonstrate the efficacy of our approach on two datasets, Textually Annotated Cooking Scenes (TACoS), and wet lab, using conventional evaluation measures such as the Jaccard Index and Hamming Loss, as well as a human-subjects study.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4387579383",
    "type": "article"
  },
  {
    "title": "Exploring Audience Response in Performing Arts with a Brain-Adaptive Digital Performance System",
    "doi": "https://doi.org/10.1145/3009974",
    "publication_date": "2017-12-04",
    "publication_year": 2017,
    "authors": "Shuo Yan; Gangyi Ding; Hongsong Li; Ningxiao Sun; Zheng Guan; Yufeng Wu; Longfei Zhang; Tianyu Huang",
    "corresponding_authors": "",
    "abstract": "Audience response is an important indicator of the quality of performing arts. Psychophysiological measurements enable researchers to perceive and understand audience response by collecting their bio-signals during a live performance. However, how the audience respond and how the performance is affected by these responses are the key elements but are hard to implement. To address this issue, we designed a brain-computer interactive system called Brain-Adaptive Digital Performance ( BADP ) for the measurement and analysis of audience engagement level through an interactive three-dimensional virtual theater. The BADP system monitors audience engagement in real time using electroencephalography (EEG) measurement and tries to improve it by applying content-related performing cues when the engagement level decreased. In this article, we generate EEG-based engagement level and build thresholds to determine the decrease and re-engage moments. In the experiment, we simulated two types of theatre performance to provide participants a high-fidelity virtual environment using the BADP system. We also create content-related performing cues for each performance under three different conditions. The results of these evaluations show that our algorithm could accurately detect the engagement status and the performing cues have a positive impact on regaining audience engagement across different performance types. Our findings open new perspectives in audience-based theatre performance design.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2772738338",
    "type": "article"
  },
  {
    "title": "Designing an AI Health Coach and Studying Its Utility in Promoting Regular Aerobic Exercise",
    "doi": "https://doi.org/10.1145/3366501",
    "publication_date": "2020-05-30",
    "publication_year": 2020,
    "authors": "Shiwali Mohan; Anusha Venkatakrishnan; Andrea L. Hartzler",
    "corresponding_authors": "",
    "abstract": "Our research aims to develop interactive, social agents that can coach people to learn new tasks, skills, and habits. In this paper, we focus on coaching sedentary, overweight individuals (i.e., trainees) to exercise regularly. We employ adaptive goal setting in which the intelligent health coach generates, tracks, and revises personalized exercise goals for a trainee. The goals become incrementally more difficult as the trainee progresses through the training program. Our approach is model-based - the coach maintains a parameterized model of the trainee's aerobic capability that drives its expectation of the trainee's performance. The model is continually revised based on trainee-coach interactions. The coach is embodied in a smartphone application, NutriWalking, which serves as a medium for coach-trainee interaction. We adopt a task-centric evaluation approach for studying the utility of the proposed algorithm in promoting regular aerobic exercise. We show that our approach can adapt the trainee program not only to several trainees with different capabilities, but also to how a trainee's capability improves as they begin to exercise more. Experts rate the goals selected by the coach better than other plausible goals, demonstrating that our approach is consistent with clinical recommendations. Further, in a 6-week observational study with sedentary participants, we show that the proposed approach helps increase exercise volume performed each week.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2980040225",
    "type": "article"
  },
  {
    "title": "Being the Center of Attention",
    "doi": "https://doi.org/10.1145/3338245",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "Dario Dotti; Mirela Popa; Stylianos Asteriadis",
    "corresponding_authors": "",
    "abstract": "This article proposes a novel study on personality recognition using video data from different scenarios. Our goal is to jointly model nonverbal behavioral cues with contextual information for a robust, multi-scenario, personality recognition system. Therefore, we build a novel multi-stream Convolutional Neural Network (CNN) framework, which considers multiple sources of information. From a given scenario, we extract spatio-temporal motion descriptors from every individual in the scene, spatio-temporal motion descriptors encoding social group dynamics, and proxemics descriptors to encode the interaction with the surrounding context. All the proposed descriptors are mapped to the same feature space facilitating the overall learning effort. Experiments on two public datasets demonstrate the effectiveness of jointly modeling the mutual Person-Context information, outperforming the state-of-the art-results for personality recognition in two different scenarios. Last, we present CNN class activation maps for each personality trait, shedding light on behavioral patterns linked with personality attributes.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3041121984",
    "type": "article"
  },
  {
    "title": "I Know What You Know: What Hand Movements Reveal about Domain Expertise",
    "doi": "https://doi.org/10.1145/3423049",
    "publication_date": "2021-03-15",
    "publication_year": 2021,
    "authors": "Sharon Oviatt; Jionghao Lin; Abishek Sriramulu",
    "corresponding_authors": "",
    "abstract": "This research investigates whether students’ level of domain expertise can be detected during authentic learning activities by analyzing their physical activity patterns. More expert students reduced their manual activity by a substantial 50%, which was evident in fine-grained signal analyses and total rate of gesturing. The quality of experts’ discrete hand movements also averaged shorter in distance, briefer in duration, and slower in velocity than those of non-experts. Interestingly, experts adapted by nearly eliminating gestures on easier problems, while selectively increasing them on harder ones. They also strategically produced 62% more iconic gestures, which serve to retain spatial information in working memory while extracting inferences required to solve problems correctly. These findings highlight the close relation between hand movements and mental state and, more specifically, that hand movements provide an unusually clear window on students’ level of domain expertise. Embodied Cognition and Limited Resource theories only partially account for the present findings, which specify future directions for theoretical work.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3139181933",
    "type": "article"
  },
  {
    "title": "Effect of Adaptive Guidance and Visualization Literacy on Gaze Attentive Behaviors and Sequential Patterns on Magazine-Style Narrative Visualizations",
    "doi": "https://doi.org/10.1145/3447992",
    "publication_date": "2021-09-03",
    "publication_year": 2021,
    "authors": "Oswald Barral; Sébastien Lallé; Alireza Iranpour; Cristina Conati",
    "corresponding_authors": "",
    "abstract": "We study the effectiveness of adaptive interventions at helping users process textual documents with embedded visualizations, a form of multimodal documents known as Magazine-Style Narrative Visualizations (MSNVs). The interventions are meant to dynamically highlight in the visualization the datapoints that are described in the textual sentence currently being read by the user, as captured by eye-tracking. These interventions were previously evaluated in two user studies that involved 98 participants reading excerpts of real-world MSNVs during a 1-hour session. Participants’ outcomes included their subjective feedback about the guidance, and well as their reading time and score on a set of comprehension questions. Results showed that the interventions can increase comprehension of the MSNV excerpts for users with lower levels of a cognitive skill known as visualization literacy. In this article, we aim to further investigate this result by leveraging eye-tracking to analyze in depth how the participants processed the interventions depending on their levels of visualization literacy. We first analyzed summative gaze metrics that capture how users process and integrate the key components of the narrative visualizations. Second, we mined the salient patterns in the users’ scanpaths to contextualize how users sequentially process these components. Results indicate that the interventions succeed in guiding attention to salient components of the narrative visualizations, especially by generating more transitions between key components of the visualization (i.e., datapoints, labels, and legend), as well as between the two modalities (text and visualization). We also show that the interventions help users with lower levels of visualization literacy to better map datapoints to the legend, which likely contributed to their improved comprehension of the documents. These findings shed light on how adaptive interventions help users with different levels of visualization literacy, informing the design of personalized narrative visualizations.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3197405043",
    "type": "article"
  },
  {
    "title": "Human Tutorial Instruction in the Raw",
    "doi": "https://doi.org/10.1145/2531920",
    "publication_date": "2015-03-25",
    "publication_year": 2015,
    "authors": "Yolanda Gil",
    "corresponding_authors": "Yolanda Gil",
    "abstract": "Humans learn procedures from one another through a variety of methods, such as observing someone do the task, practicing by themselves, reading manuals or textbooks, or getting instruction from a teacher. Some of these methods generate examples that require the learner to generalize appropriately. When procedures are complex, however, it becomes unmanageable to induce the procedures from examples alone. An alternative and very common method for teaching procedures is tutorial instruction, where a teacher describes in general terms what actions to perform and possibly includes explanations of the rationale for the actions. This article provides an overview of the challenges in using human tutorial instruction for teaching procedures to computers. First, procedures can be very complex and can involve many different types of interrelated information, including (1) situating the instruction in the context of relevant objects and their properties, (2) describing the steps involved, (3) specifying the organization of the procedure in terms of relationships among steps and substeps, and (4) conveying control structures. Second, human tutorial instruction is naturally plagued with omissions, oversights, unintentional inconsistencies, errors, and simply poor design. The article presents a survey of work from the literature that highlights the nature of these challenges and illustrates them with numerous examples of instruction in many domains. Major research challenges in this area are highlighted, including the difficulty of the learning task when procedures are complex, the need to overcome omissions and errors in the instruction, the design of a natural user interface to specify procedures, the management of the interaction of a human with a learning system, and the combination of tutorial instruction with other teaching modalities.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2022440215",
    "type": "article"
  },
  {
    "title": "Automatic Classification of Leading Interactions in a String Quartet",
    "doi": "https://doi.org/10.1145/2818739",
    "publication_date": "2016-03-09",
    "publication_year": 2016,
    "authors": "Floriane Dardard; Giorgio Gnecco; Donald Glowinski",
    "corresponding_authors": "",
    "abstract": "The aim of the present work is to analyze automatically the leading interactions between the musicians of a string quartet, using machine-learning techniques applied to nonverbal features of the musicians’ behavior, which are detected through the help of a motion-capture system. We represent these interactions by a graph of “influence” of the musicians, which displays the relations “is following” and “is not following” with weighted directed arcs. The goal of the machine-learning problem investigated is to assign weights to these arcs in an optimal way. Since only a subset of the available training examples are labeled, a semisupervised support vector machine is used, which is based on a linear kernel to limit its model complexity. Specific potential applications within the field of human-computer interaction are also discussed, such as e-learning, networked music performance, and social active listening.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2318879460",
    "type": "article"
  },
  {
    "title": "The Stability and Usability of Statistical Topic Models",
    "doi": "https://doi.org/10.1145/2954002",
    "publication_date": "2016-07-20",
    "publication_year": 2016,
    "authors": "Yi Yang; Shimei Pan; Jie Lü; Mercan Topkara; Yangqiu Song",
    "corresponding_authors": "",
    "abstract": "Statistical topic models have become a useful and ubiquitous tool for analyzing large text corpora. One common application of statistical topic models is to support topic-centric navigation and exploration of document collections. Existing work on topic modeling focuses on the inference of model parameters so the resulting model fits the input data. Since the exact inference is intractable, statistical inference methods, such as Gibbs Sampling, are commonly used to solve the problem. However, most of the existing work ignores an important aspect that is closely related to the end user experience: topic model stability. When the model is either re-trained with the same input data or updated with new documents, the topic previously assigned to a document may change under the new model, which may result in a disruption of end users’ mental maps about the relations between documents and topics, thus undermining the usability of the applications. In this article, we propose a novel user-directed non-disruptive topic model update method that balances the tradeoff between finding the model that fits the data and maintaining the stability of the model from end users’ perspective. It employs a novel constrained LDA algorithm to incorporate pairwise document constraints, which are converted from user feedback about topics, to achieve topic model stability. Evaluation results demonstrate the advantages of our approach over previous methods.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2494147140",
    "type": "article"
  },
  {
    "title": "Tribe or Not? Critical Inspection of Group Differences Using TribalGram",
    "doi": "https://doi.org/10.1145/3484509",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Yongsu Ahn; Muheng Yan; Yu‐Ru Lin; Wen‐Ting Chung; Rebecca Hwa",
    "corresponding_authors": "",
    "abstract": "With the rise of AI and data mining techniques, group profiling and group-level analysis have been increasingly used in many domains, including policy making and direct marketing. In some cases, the statistics extracted from data may provide insights to a group’s shared characteristics; in others, the group-level analysis can lead to problems, including stereotyping and systematic oppression. How can analytic tools facilitate a more conscientious process in group analysis? In this work, we identify a set of accountable group analytics design guidelines to explicate the needs for group differentiation and preventing overgeneralization of a group. Following the design guidelines, we develop TribalGram , a visual analytic suite that leverages interpretable machine learning algorithms and visualization to offer inference assessment, model explanation, data corroboration, and sense-making. Through the interviews with domain experts, we showcase how our design and tools can bring a richer understanding of “groups” mined from the data.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4214927243",
    "type": "article"
  },
  {
    "title": "<scp>ClioQuery</scp> : Interactive Query-oriented Text Analytics for Comprehensive Investigation of Historical News Archives",
    "doi": "https://doi.org/10.1145/3524025",
    "publication_date": "2022-04-29",
    "publication_year": 2022,
    "authors": "Abram Handler; Narges Mahyar; Brendan O’Connor",
    "corresponding_authors": "",
    "abstract": "Historians and archivists often find and analyze the occurrences of query words in newspaper archives to help answer fundamental questions about society. But much work in text analytics focuses on helping people investigate other textual units, such as events, clusters, ranked documents, entity relationships, or thematic hierarchies. Informed by a study into the needs of historians and archivists, we thus propose ClioQuery , a text analytics system uniquely organized around the analysis of query words in context. ClioQuery applies text simplification techniques from natural language processing to help historians quickly and comprehensively gather and analyze all occurrences of a query word across an archive. It also pairs these new NLP methods with more traditional features like linked views and in-text highlighting to help engender trust in summarization techniques. We evaluate ClioQuery with two separate user studies, in which historians explain how ClioQuery ’s novel text simplification features can help facilitate historical research. We also evaluate with a separate quantitative comparison study, which shows that ClioQuery helps crowdworkers find and remember historical information. Such results suggest possible new directions for text analytics in other query-oriented settings.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4226165090",
    "type": "article"
  },
  {
    "title": "The Influence of Personality Traits on User Interaction with Recommendation Interfaces",
    "doi": "https://doi.org/10.1145/3558772",
    "publication_date": "2022-08-24",
    "publication_year": 2022,
    "authors": "Dongning Yan; Li Chen",
    "corresponding_authors": "",
    "abstract": "Users’ personality traits can take an active role in affecting their behavior when they interact with a computer interface. However, in the area of recommender systems (RS) , though personality-based RS has been extensively studied, most works focus on algorithm design, with little attention paid to studying whether and how the personality may influence users’ interaction with the recommendation interface. In this manuscript, we report the results of a user study (with 108 participants) that not only measured the influence of users’ personality traits on their perception and performance when using the recommendation interface but also employed an eye-tracker to in-depth reveal how personality may influence users’ eye-movement behavior. Moreover, being different from related work that has mainly been conducted in a single product domain, our user study was performed in three typical application domains (i.e., electronics like smartphones, entertainment like movies, and tourism like hotels). Our results show that mainly three personality traits, i.e., Openness to experience , Conscientiousness , and Agreeableness , significantly influence users’ perception and eye-movement behavior, but the exact influences vary across the domains. Finally, we provide a set of guidelines that might be constructive for designing a more effective recommendation interface based on user personality.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4293173401",
    "type": "article"
  },
  {
    "title": "Evaluation and Refinement of Clustered Search Results with the Crowd",
    "doi": "https://doi.org/10.1145/3158226",
    "publication_date": "2018-06-13",
    "publication_year": 2018,
    "authors": "Amy X. Zhang; Jilin Chen; Wei Koong Chai; Jinjun Xu; Lichan Hong; Ed H.",
    "corresponding_authors": "",
    "abstract": "When searching on the web or in an app, results are often returned as lists of hundreds to thousands of items, making it difficult for users to understand or navigate the space of results. Research has demonstrated that using clustering to partition search results into coherent, topical clusters can aid in both exploration and discovery. Yet clusters generated by an algorithm for this purpose are often of poor quality and do not satisfy users. To achieve acceptable clustered search results, experts must manually evaluate and refine the clustered results for each search query, a process that does not scale to large numbers of search queries. In this article, we investigate using crowd-based human evaluation to inspect, evaluate, and improve clusters to create high-quality clustered search results at scale. We introduce a workflow that begins by using a collection of well-known clustering algorithms to produce a set of clustered search results for a given query. Then, we use crowd workers to holistically assess the quality of each clustered search result to find the best one. Finally, the workflow has the crowd spot and fix problems in the best result to produce a final output. We evaluate this workflow on 120 top search queries from the Google Play Store, some of whom have clustered search results as a result of evaluations and refinements by experts. Our evaluations demonstrate that the workflow is effective at reproducing the evaluation of expert judges and also improves clusters in a way that agrees with experts and crowds alike.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2807850020",
    "type": "article"
  },
  {
    "title": "Efficient Interactive Multiclass Learning from Binary Feedback",
    "doi": "https://doi.org/10.1145/2629631",
    "publication_date": "2014-08-11",
    "publication_year": 2014,
    "authors": "Hung Q. Ngo; Matthew Luciw; Jawas Nagi; Alexander Förster; Jürgen Schmidhuber; Ngo Anh Vien",
    "corresponding_authors": "",
    "abstract": "We introduce a novel algorithm called upper confidence - weighted learning (UCWL) for online multiclass learning from binary feedback (e.g., feedback that indicates whether the prediction was right or wrong). UCWL combines the upper confidence bound (UCB) framework with the soft confidence-weighted (SCW) online learning scheme. In UCB, each instance is classified using both score and uncertainty. For a given instance in the sequence, the algorithm might guess its class label primarily to reduce the class uncertainty. This is a form of informed exploration, which enables the performance to improve with lower sample complexity compared to the case without exploration. Combining UCB with SCW leads to the ability to deal well with noisy and nonseparable data, and state-of-the-art performance is achieved without increasing the computational cost. A potential application setting is human-robot interaction (HRI), where the robot is learning to classify some set of inputs while the human teaches it by providing only binary feedback—or sometimes even the wrong answer entirely. Experimental results in the HRI setting and with two benchmark datasets from other settings show that UCWL outperforms other state-of-the-art algorithms in the online binary feedback setting—and surprisingly even sometimes outperforms state-of-the-art algorithms that get full feedback (e.g., the true class label), whereas UCWL gets only binary feedback on the same data sequence.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2023280931",
    "type": "article"
  },
  {
    "title": "“I’ll Be There Next”",
    "doi": "https://doi.org/10.1145/2844542",
    "publication_date": "2016-01-07",
    "publication_year": 2016,
    "authors": "Keiichi Yamazaki; Akiko Yamazaki; Keiko Ikeda; Chen Liu; Mihoko Fukushima; Yoshinori Kobayashi; Yoshinori Kuno",
    "corresponding_authors": "",
    "abstract": "In this article, we discuss our findings from an ethnographic study at an elderly care center where we observed the utilization of two different functions of human gaze to convey service order (i.e., “who is served first and who is served next”). In one case, when an elderly person requested assistance, the gaze of the care worker communicated that he/she would serve that client next in turn. In the other case, the gaze conveyed a request to the service seeker to wait until the care worker finished attending the current client. Each gaze function depended on the care worker's current engagement and other behaviors. We sought to integrate these findings into the development of a robot that might function more effectively in multiple human-robot party settings. We focused on the multiple functions of gaze and bodily actions, implementing those functions into our robot. We conducted three experiments to gauge a combination of gestures and gazes performed by our robot. This article demonstrates that the employment of gaze is an important consideration when developing robots that can interact effectively in multiple human-robot party settings.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2237757129",
    "type": "article"
  },
  {
    "title": "Intelligent Biohazard Training Based on Real-Time Task Recognition",
    "doi": "https://doi.org/10.1145/2883617",
    "publication_date": "2016-09-21",
    "publication_year": 2016,
    "authors": "Helmut Prendinger; Nahum Álvarez; Antonio A. Sánchez‐Ruiz; Marc Cavazza; Jo�ão Ricardo Catarino; João Oliveira; Rui Prada; Shuji Fujimoto; Mika Shigematsu",
    "corresponding_authors": "",
    "abstract": "Virtual environments offer an ideal setting to develop intelligent training applications. Yet, their ability to support complex procedures depends on the appropriate integration of knowledge-based techniques and natural interaction. In this article, we describe the implementation of an intelligent rehearsal system for biohazard laboratory procedures, based on the real-time instantiation of task models from the trainee’s actions. A virtual biohazard laboratory has been recreated using the Unity3D engine, in which users interact with laboratory objects using keyboard/mouse input or hand gestures through a Kinect device. Realistic behavior for objects is supported by the implementation of a relevant subset of common sense and physics knowledge. User interaction with objects leads to the recognition of specific actions, which are used to progressively instantiate a task-based representation of biohazard procedures. The dynamics of this instantiation process supports trainee evaluation as well as real-time assistance. This system is designed primarily as a rehearsal system providing real-time advice and supporting user performance evaluation. We provide detailed examples illustrating error detection and recovery, and results from on-site testing with students from the Faculty of Medical Sciences at Kyushu University. In the study, we investigate the usability aspect by comparing interaction with mouse and Kinect devices and the effect of real-time task recognition on recovery time after user mistakes.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2523011934",
    "type": "article"
  },
  {
    "title": "A Real-time Interactive Visualizer for Large Classroom",
    "doi": "https://doi.org/10.1145/3418529",
    "publication_date": "2021-03-15",
    "publication_year": 2021,
    "authors": "Samit Bhattacharya; Viral B. Shah; Krishna Kumar; Ujjwal Biswas",
    "corresponding_authors": "",
    "abstract": "In improving the teaching and learning experience in a classroom environment, it is crucial for a teacher to have a fair idea about the students who need help during a lecture. However, teachers of large classes usually face difficulties in identifying the students who are in a critical state. The current methods for classroom visualization are limited in showing both the status and location of a large number of students in a limited display area. Additionally, comprehension of the states adds cognitive load on the teacher working in a time-constrained classroom environment. In this article, we propose a two-level visualizer for large classrooms to address the challenges. In the first level, the visualizer generates a colored matrix representation of the classroom. The colored matrix is a quantitative illustration of the status of the class in terms of student clusters. We use three colors: red, yellow, and green, indicating the most critical, less critical, and the normal cluster on the screen, respectively. With tap/click on the first level, detailed information for a cluster is visualized as the second level. We conducted extensive studies for our visualizer in a simulated classroom with 12 tasks and 27 teacher participants. The results show that the visualizer is efficient and usable.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3136034258",
    "type": "article"
  },
  {
    "title": "MI3: Machine-initiated Intelligent Interaction for Interactive Classification and Data Reconstruction",
    "doi": "https://doi.org/10.1145/3412848",
    "publication_date": "2021-09-03",
    "publication_year": 2021,
    "authors": "Yu Zhang; Bob Coecke; Min Chen",
    "corresponding_authors": "",
    "abstract": "In many applications, while machine learning (ML) can be used to derive algorithmic models to aid decision processes, it is often difficult to learn a precise model when the number of similar data points is limited. One example of such applications is data reconstruction from historical visualizations, many of which encode precious data, but their numerical records are lost. On the one hand, there is not enough similar data for training an ML model. On the other hand, manual reconstruction of the data is both tedious and arduous. Hence, a desirable approach is to train an ML model dynamically using interactive classification, and hopefully, after some training, the model can complete the data reconstruction tasks with less human interference. For this approach to be effective, the number of annotated data objects used for training the ML model should be as small as possible, while the number of data objects to be reconstructed automatically should be as large as possible. In this article, we present a novel technique for the machine to initiate intelligent interactions to reduce the user’s interaction cost in interactive classification tasks. The technique of machine-initiated intelligent interaction (MI3) builds on a generic framework featuring active sampling and default labeling. To demonstrate the MI3 approach, we use the well-known cholera map visualization by John Snow as an example, as it features three instances of MI3 pipelines. The experiment has confirmed the merits of the MI3 approach.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3198524080",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on Activity Recognition for Interaction",
    "doi": "https://doi.org/10.1145/2694858",
    "publication_date": "2015-01-28",
    "publication_year": 2015,
    "authors": "Andreas Bulling; Ulf Blanke; Desney Tan; Jun Rekimoto; Gregory D. Abowd",
    "corresponding_authors": "",
    "abstract": "This editorial introduction describes the aims and scope of the ACM Transactions on Interactive Intelligent Systems special issue on Activity Recognition for Interaction. It explains why activity recognition is becoming crucial as part of the cycle of interaction between users and computing systems, and it shows how the five articles selected for this special issue reflect this theme.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2004287234",
    "type": "article"
  },
  {
    "title": "Learning Semantically Rich Network-based Multi-modal Mobile User Interface Embeddings",
    "doi": "https://doi.org/10.1145/3533856",
    "publication_date": "2022-05-16",
    "publication_year": 2022,
    "authors": "Gary Ang; Ee‐Peng Lim",
    "corresponding_authors": "",
    "abstract": "Semantically rich information from multiple modalities—text, code, images, categorical and numerical data—co-exist in the user interface (UI) design of mobile applications. Moreover, each UI design is composed of inter-linked UI entities that support different functions of an application, e.g., a UI screen comprising a UI taskbar, a menu, and multiple button elements. Existing UI representation learning methods unfortunately are not designed to capture multi-modal and linkage structure between UI entities. To support effective search and recommendation applications over mobile UIs, we need UI representations that integrate latent semantics present in both multi-modal information and linkages between UI entities. In this article, we present a novel self-supervised model—Multi-modal Attention-based Attributed Network Embedding (MAAN) model. MAAN is designed to capture structural network information present within the linkages between UI entities, as well as multi-modal attributes of the UI entity nodes. Based on the variational autoencoder framework, MAAN learns semantically rich UI embeddings in a self-supervised manner by reconstructing the attributes of UI entities and the linkages between them. The generated embeddings can be applied to a variety of downstream tasks: predicting UI elements associated with UI screens, inferring missing UI screen and element attributes, predicting UI user ratings, and retrieving UIs. Extensive experiments, including user evaluations, conducted on datasets from RICO, a rich real-world mobile UI repository, demonstrate that MAAN out-performs other state-of-the-art models. The number of linkages between UI entities can provide further information on the role of different UI entities in UI designs. However, MAAN does not capture edge attributes. To extend and generalize MAAN to learn even richer UI embeddings, we further propose EMAAN to capture edge attributes. We conduct additional extensive experiments on EMAAN, which show that it improves the performance of MAAN and similarly out-performs state-of-the-art models.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4280501884",
    "type": "article"
  },
  {
    "title": "Special Issue on Conversational Agents for Healthcare and Wellbeing",
    "doi": "https://doi.org/10.1145/3532860",
    "publication_date": "2022-06-30",
    "publication_year": 2022,
    "authors": "A. Baki Kocaballı; Liliana Laranjo; Leigh Clark; Rafał Kocielnik; Robert J. Moore; Q. Vera Liao; Timothy Bickmore",
    "corresponding_authors": "",
    "abstract": "research-article Share on Special Issue on Conversational Agents for Healthcare and Wellbeing Authors: A. Baki Kocaballi The School of Computer Science, The University of Technology Sydney The School of Computer Science, The University of Technology SydneySearch about this author , Liliana Laranjo The Westmead Applied Research Centre (WARC), The University of Sydney The Westmead Applied Research Centre (WARC), The University of SydneySearch about this author , Leigh Clark The Department of Computer Science, Swansea University The Department of Computer Science, Swansea UniversitySearch about this author , Rafał Kocielnik The California Institute of Technology The California Institute of TechnologySearch about this author , Robert J. Moore IBM Research - Almaden IBM Research - AlmadenSearch about this author , Q. Vera Liao Microsoft Research Montréal Microsoft Research MontréalSearch about this author , Timothy Bickmore The Khoury College of Computer Sciences, Northeastern University The Khoury College of Computer Sciences, Northeastern UniversitySearch about this author Authors Info & Claims ACM Transactions on Interactive Intelligent SystemsVolume 12Issue 2June 2022 Article No.: 9pp 1–3https://doi.org/10.1145/3532860Published:12 July 2022Publication History 0citation255DownloadsMetricsTotal Citations0Total Downloads255Last 12 Months255Last 6 weeks54 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4285099116",
    "type": "article"
  },
  {
    "title": "Visual Analytics of Co-Occurrences to Discover Subspaces in Structured Data",
    "doi": "https://doi.org/10.1145/3579031",
    "publication_date": "2023-01-21",
    "publication_year": 2023,
    "authors": "Wolfgang Jentner; Giuliana Lindholz; Hanna Hauptmann; Mennatallah El‐Assady; Kwan‐Liu Ma; Daniel A. Keim",
    "corresponding_authors": "",
    "abstract": "We present an approach that shows all relevant subspaces of categorical data condensed in a single picture. We model the categorical values of the attributes as co-occurrences with data partitions generated from structured data using pattern mining. We show that these co-occurrences are a-priori , allowing us to greatly reduce the search space, effectively generating the condensed picture where conventional approaches filter out several subspaces as these are deemed insignificant. The task of identifying interesting subspaces is common but difficult due to exponential search spaces and the curse of dimensionality. One application of such a task might be identifying a cohort of patients defined by attributes such as gender, age, and diabetes type that share a common patient history, which is modeled as event sequences. Filtering the data by these attributes is common but cumbersome and often does not allow a comparison of subspaces. We contribute a powerful multi-dimensional pattern exploration approach (MDPE-approach) agnostic to the structured data type that models multiple attributes and their characteristics as co-occurrences, allowing the user to identify and compare thousands of subspaces of interest in a single picture. In our MDPE-approach, we introduce two methods to dramatically reduce the search space, outputting only the boundaries of the search space in the form of two tables. We implement the MDPE-approach in an interactive visual interface (MDPE-vis) that provides a scalable, pixel-based visualization design allowing the identification, comparison, and sense-making of subspaces in structured data. Our case studies using a gold-standard dataset and external domain experts confirm our approach’s and implementation’s applicability. A third use case sheds light on the scalability of our approach and a user study with 15 participants underlines its usefulness and power.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4317659182",
    "type": "article"
  },
  {
    "title": "Crowdsourcing Thumbnail Captions: Data Collection and Validation",
    "doi": "https://doi.org/10.1145/3589346",
    "publication_date": "2023-03-29",
    "publication_year": 2023,
    "authors": "Carlos Aguirre; Shiye Cao; Amama Mahmood; Chien‐Ming Huang",
    "corresponding_authors": "",
    "abstract": "Speech interfaces, such as personal assistants and screen readers, read image captions to users. Typically, however, only one caption is available per image, which may not be adequate for all situations (e.g., browsing large quantities of images). Long captions provide a deeper understanding of an image but require more time to listen to, whereas shorter captions may not allow for such thorough comprehension yet have the advantage of being faster to consume. We explore how to effectively collect both thumbnail captions—succinct image descriptions meant to be consumed quickly—and comprehensive captions—which allow individuals to understand visual content in greater detail. We consider text-based instructions and time-constrained methods to collect descriptions at these two levels of detail and find that a time-constrained method is the most effective for collecting thumbnail captions while preserving caption accuracy. Additionally, we verify that caption authors using this time-constrained method are still able to focus on the most important regions of an image by tracking their eye gaze. We evaluate our collected captions along human-rated axes—correctness, fluency, amount of detail, and mentions of important concepts—and discuss the potential for model-based metrics to perform large-scale automatic evaluations in the future.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4361297406",
    "type": "article"
  },
  {
    "title": "Simulation-Based Optimization of User Interfaces for Quality-Assuring Machine Learning Model Predictions",
    "doi": "https://doi.org/10.1145/3594552",
    "publication_date": "2023-05-17",
    "publication_year": 2023,
    "authors": "Yu Zhang; Martijn Tennekes; Tim de Jong; Lyana Curier; Bob Coecke; Min Chen",
    "corresponding_authors": "",
    "abstract": "Quality-sensitive applications of machine learning (ML) require quality assurance (QA) by humans before the predictions of an ML model can be deployed. QA for ML (QA4ML) interfaces require users to view a large amount of data and perform many interactions to correct errors made by the ML model. An optimized user interface (UI) can significantly reduce interaction costs. While UI optimization can be informed by user studies evaluating design options, this approach is not scalable, because there are typically numerous small variations that can affect the efficiency of a QA4ML interface. Hence, we propose using simulation to evaluate and aid the optimization of QA4ML interfaces. In particular, we focus on simulating the combined effects of human intelligence in initiating appropriate interaction commands and machine intelligence in providing algorithmic assistance for accelerating QA4ML processes. As QA4ML is usually labor-intensive, we use the simulated task completion time as the metric for UI optimization under different interface and algorithm setups. We demonstrate the usage of this UI design method in several QA4ML applications.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4376877409",
    "type": "article"
  },
  {
    "title": "Guest Editorial",
    "doi": "https://doi.org/10.1145/3178569",
    "publication_date": "2018-02-12",
    "publication_year": 2018,
    "authors": "Yu‐Ru Lin; Nan Cao",
    "corresponding_authors": "",
    "abstract": "The analysis of human behaviors has impacted many social and commercial domains. How could interactive visual analytic systems be used to further provide behavioral insights? This editorial introduction features emerging research trend related to this question. The four articles accepted for this special issue represent recent progress: they identify research challenges arising from analysis of human and crowd behaviors, and present novel methods in visual analysis to address those challenges and help make behavioral data more useful.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2786049873",
    "type": "editorial"
  },
  {
    "title": "A Bandit-Based Ensemble Framework for Exploration/Exploitation of Diverse Recommendation Components",
    "doi": "https://doi.org/10.1145/3237187",
    "publication_date": "2019-08-09",
    "publication_year": 2019,
    "authors": "Björn Brodén; Mikael Hammar; Bengt J. Nilsson; Dimitris Paraschakis",
    "corresponding_authors": "",
    "abstract": "This work presents an extension of Thompson Sampling bandit policy for orchestrating the collection of base recommendation algorithms for e-commerce. We focus on the problem of item-to-item recommendations, for which multiple behavioral and attribute-based predictors are provided to an ensemble learner. In addition, we detail the construction of a personalized predictor based on k -Nearest Neighbors ( k NN), with temporal decay capabilities and event weighting. We show how to adapt Thompson Sampling to realistic situations when neither action availability nor reward stationarity is guaranteed. Furthermore, we investigate the effects of priming the sampler with pre-set parameters of reward probability distributions by utilizing the product catalog and/or event history, when such information is available. We report our experimental results based on the analysis of three real-world e-commerce datasets.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2967847712",
    "type": "article"
  },
  {
    "title": "Theoretical, Measured, and Subjective Responsibility in Aided Decision Making",
    "doi": "https://doi.org/10.1145/3425732",
    "publication_date": "2021-03-15",
    "publication_year": 2021,
    "authors": "Nir Douer; Joachim Meyer",
    "corresponding_authors": "",
    "abstract": "When humans interact with intelligent systems, their causal responsibility for outcomes becomes equivocal. We analyze the descriptive abilities of a newly developed responsibility quantification model (ResQu) to predict actual human responsibility and perceptions of responsibility in the interaction with intelligent systems. In two laboratory experiments, participants performed a classification task. They were aided by classification systems with different capabilities. We compared the predicted theoretical responsibility values to the actual measured responsibility participants took on and to their subjective rankings of responsibility. The model predictions were strongly correlated with both measured and subjective responsibility. Participants’ behavior with each system was influenced by the system and human capabilities, but also by the subjective perceptions of these capabilities and the perception of the participant's own contribution. A bias existed only when participants with poor classification capabilities relied less than optimally on a system that had superior classification capabilities and assumed higher-than-optimal responsibility. The study implies that when humans interact with advanced intelligent systems, with capabilities that greatly exceed their own, their comparative causal responsibility will be small, even if formally the human is assigned major roles. Simply putting a human into the loop does not ensure that the human will meaningfully contribute to the outcomes. The results demonstrate the descriptive value of the ResQu model to predict behavior and perceptions of responsibility by considering the characteristics of the human, the intelligent system, the environment, and some systematic behavioral biases. The ResQu model is a new quantitative method that can be used in system design and can guide policy and legal decisions regarding human responsibility in events involving intelligent systems.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3138814662",
    "type": "article"
  },
  {
    "title": "Adapting the Interactive Activation Model for Context Recognition and Identification",
    "doi": "https://doi.org/10.1145/2873067",
    "publication_date": "2016-09-14",
    "publication_year": 2016,
    "authors": "Maya Sappelli; Suzan Verberne; Wessel Kraaij",
    "corresponding_authors": "",
    "abstract": "In this article, we propose and implement a new model for context recognition and identification . Our work is motivated by the importance of “working in context” for knowledge workers to stay focused and productive. A computer application that can identify the current context in which the knowledge worker is working can (among other things) provide the worker with contextual support, for example, by suggesting relevant information sources, or give an overview of how he or she spent his or her time during the day. We present a descriptive model for the context of a knowledge worker. This model describes the contextual elements in the work environment of the knowledge worker and how these elements relate to each other. This model is operationalized in an algorithm, the contextual interactive activation model (CIA), which is based on the interactive activation model by Rumelhart and McClelland. It consists of a layered connected network through which activation flows. We have tested CIA in a context identification setting. In this case, the data that we use as input is low-level computer interaction logging data. We found that topical information and entities were the most relevant types of information for context identification. Overall the proposed CIA model is more effective than traditional supervised methods in identifying the active context from sparse input data, with less labelled training data.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2518784017",
    "type": "article"
  },
  {
    "title": "Read What You Touch with Intelligent Audio System for Non-Visual Interaction",
    "doi": "https://doi.org/10.1145/2822908",
    "publication_date": "2016-10-10",
    "publication_year": 2016,
    "authors": "Yasmine N. Elglaly; Francis Quek",
    "corresponding_authors": "",
    "abstract": "Slate-type devices allow Individuals with Blindness or Severe Visual Impairment (IBSVI) to read in place with the touch of their fingertip by audio-rendering the words they touch. Such technologies are helpful for spatial cognition while reading. However, users have to move their fingers slowly, or they may lose their place on screen. Also, IBSVI may wander between lines without realizing they did. We addressed these two interaction problems by introducing a dynamic speech-touch interaction model and an intelligent reading support system. With this model, the speed of the speech will dynamically change with the user’s finger speed. The proposed model is composed of (1) an Audio Dynamics Model and (2) an Off-line Speech Synthesis Technique. The intelligent reading support system predicts the direction of reading, corrects the reading word if the user drifts, and notifies the user using a sonic gutter to help him/her from straying off the reading line. We tested the new audio dynamics model, the sonic gutter, and the reading support model in two user studies. The participants’ feedback helped us fine-tune the parameters of the two models. A decomposition study was conducted to evaluate the main components of the system. The results showed that both intelligent reading support with tactile feedback are required to achieve the best performance in terms of efficiency and effectiveness. Finally, we ran an evaluation study where the reading support system is compared to other VoiceOver technologies. The results showed preponderance to the reading support system with its audio dynamics and intelligent reading support components.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2530557073",
    "type": "article"
  },
  {
    "title": "MobInsight",
    "doi": "https://doi.org/10.1145/3158433",
    "publication_date": "2018-07-05",
    "publication_year": 2018,
    "authors": "Souneil Park; Joan Serrà; Enrique Frías Martínez; Nuria Oliver",
    "corresponding_authors": "",
    "abstract": "Collective urban mobility embodies the residents’ local insights on the city. Mobility practices of the residents are produced from their spatial choices , which involve various considerations such as the atmosphere of destinations, distance, past experiences, and preferences. The advances in mobile computing and the rise of geo-social platforms have provided the means for capturing the mobility practices; however, interpreting the residents’ insights is challenging due to the scale and complexity of an urban environment and its unique context. In this article, we present MobInsight, a framework for making localized interpretations of urban mobility that reflect various aspects of the urbanism. MobInsight extracts a rich set of neighborhood features through holistic semantic aggregation , and models the mobility between all-pairs of neighborhoods . We evaluate MobInsight with the mobility data of Barcelona and demonstrate diverse localized and semantically rich interpretations.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2962888788",
    "type": "article"
  },
  {
    "title": "The Shoutcasters, the Game Enthusiasts, and the AI: Foraging for Explanations of Real-time Strategy Players",
    "doi": "https://doi.org/10.1145/3396047",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "Sean Penney; Jonathan Dodge; A. W. Anderson; Claudia Hilderbrand; Logan Simpson; Margaret Burnett",
    "corresponding_authors": "",
    "abstract": "Assessing and understanding intelligent agents is a difficult task for users who lack an AI background. “Explainable AI” (XAI) aims to address this problem, but what should be in an explanation? One route toward answering this question is to turn to theories of how humans try to obtain information they seek. Information Foraging Theory (IFT) is one such theory. In this article, we present a series of studies 1 using IFT: the first investigates how expert explainers supply explanations in the RTS domain, the second investigates what explanations domain experts demand from agents in the RTS domain, and the last focuses on how both populations try to explain a state-of-the-art AI. Our results show that RTS environments like StarCraft offer so many options that change so rapidly, foraging tends to be very costly. Ways foragers attempted to manage such costs included “satisficing” approaches to reduce their cognitive load, such as focusing more on What information than on Why information, strategic use of language to communicate a lot of nuanced information in a few words, and optimizing their environment when possible to make their most valuable information patches readily available. Further, when a real AI entered the picture, even very experienced domain experts had difficulty understanding and judging some of the AI’s unconventional behaviors. Finally, our results reveal ways Information Foraging Theory can inform future XAI interactive explanation environments, and also how XAI can inform IFT.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3040729482",
    "type": "article"
  },
  {
    "title": "How Impactful Is Presentation in Email? The Effect of Avatars and Signatures",
    "doi": "https://doi.org/10.1145/3345641",
    "publication_date": "2020-09-30",
    "publication_year": 2020,
    "authors": "Joshua Hailpern; Mark W. Huber; Ronald Calvo",
    "corresponding_authors": "",
    "abstract": "A primary well-controlled study of 900 participants found that personal presentation choices in professional emails (non-content changes like Profile Avatar 8 Signature) impact the recipient’s perception of the sender’s personality and the quality of the email itself. By understanding the role these choices play, employees can gain better control over how they influence the recipient of their messages. Results further indicate that although some variations can positively impact the recipient’s view of the sender, these same variations often also have negative side effects. This implies that many seemingly innocuous presentation decisions should be made in the context of who is receiving the email, and if these effects negatively impact the content of the message. For example, although statements in a Signature about the email having been written on a phone are included to preemptively apologize for typing mistakes, this causes the sender to appear less agreeable, less conscientious, and less open, and the email itself appears less well written and more poorly formatted. This is surprising given that the email itself was not changed in the study.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3103445509",
    "type": "article"
  },
  {
    "title": "Embodying services into physical places",
    "doi": "https://doi.org/10.1145/2499474.2499477",
    "publication_date": "2013-07-01",
    "publication_year": 2013,
    "authors": "Pierrick Thébault; Dominique Decotter; Matthieu Boussard; Monique Lu",
    "corresponding_authors": "",
    "abstract": "The tremendous developments in mobile computing and handheld devices have allowed for an increasing usage of the resources of the World Wide Web. People today consume information and services on the go, through smart phones applications capable of exploiting their location in order to adapt the content according to the context of use. As location-based services gain traction and reveal their limitations, we argue there is a need for intelligent systems to be created to better support people's activities in their experience of the city, especially regarding their decision-making processes. In this article, we explore the opportunity to move closer to the realization of the ubiquitous computing vision by turning physical places into smart environments capable of cooperatively and autonomously collecting, processing, and transporting information about their characteristics (e.g., practical information, presence of people, and ambience). Following a multidisciplinary approach which leverages psychology, design, and computer science, we propose to investigate the potential of building communication and interaction spaces, called information spheres , on top of physical places such as businesses, homes, and institutions. We argue that, if the latter are exposed on the Web, they can act as a platform delivering information and services and mediating interactions with smart objects without requiring too much effort for the deployment of the architecture. After presenting the inherent challenges of our vision, we go through the protocol of two preliminary experiments that aim to evaluate users' perception of different types of information (i.e., reviews, check-in information, video streams, and real-time representations) and their influence on the decision-making process. Results of this study lead us to elaborate the design considerations that must be taken into account to ensure the intelligibility and user acceptance of information spheres. We finally describe a research prototype application called Environment Browser (Env-B) and present the underlying smart space middleware, before evaluating the user experience with our system through quantitative and qualitative methods.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W1968955721",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on Machine Learning for Multiple Modalities in Interactive Systems and Robots",
    "doi": "https://doi.org/10.1145/2670539",
    "publication_date": "2014-10-14",
    "publication_year": 2014,
    "authors": "Heriberto Cuayáhuitl; Lutz Frommberger; Nina Dethlefs; Antoine Raux; Mathew Marge; Hendrik Zender",
    "corresponding_authors": "",
    "abstract": "This special issue highlights research articles that apply machine learning to robots and other systems that interact with users through more than one modality, such as speech, gestures, and vision. For example, a robot may coordinate its speech with its actions, taking into account (audio-)visual feedback during their execution. Machine learning provides interactive systems with opportunities to improve performance not only of individual components but also of the system as a whole. However, machine learning methods that encompass multiple modalities of an interactive system are still relatively hard to find. The articles in this special issue represent examples that contribute to filling this gap.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2054899333",
    "type": "article"
  },
  {
    "title": "The WOZ Recognizer",
    "doi": "https://doi.org/10.1145/2743029",
    "publication_date": "2015-10-16",
    "publication_year": 2015,
    "authors": "Jared N. Bott; Joseph J. LaViola",
    "corresponding_authors": "",
    "abstract": "Sketch recognition has the potential to be an important input method for computers in the coming years, particularly for STEM (science, technology, engineering, and math) education. However, designing and building an accurate and sophisticated sketch recognition system is a time-consuming and daunting task. Since sketch recognition mistakes are still common, it is important to understand how users perceive and tolerate recognition errors and other user interface elements with these imperfect systems. In order to solve this problem, we developed a Wizard of Oz sketch recognition tool, the WOZ Recognizer, that supports controlled recognition accuracy, multiple recognition modes, and multiple sketching domains for performing controlled experiments. We present the design of the WOZ Recognizer and our process for representing recognition domains using graphs and symbol alphabets. In addition, we discuss how sketches are altered, how to control the WOZ Recognizer, and how users interact with it. Finally, we present an expert user case study that examines the WOZ Recognizer’s usability.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2091225631",
    "type": "article"
  },
  {
    "title": "Expressive Latent Feature Modelling for Explainable Matrix Factorisation-based Recommender Systems",
    "doi": "https://doi.org/10.1145/3530299",
    "publication_date": "2022-05-02",
    "publication_year": 2022,
    "authors": "Abdullah Alhejaili; Shaheen Fatima",
    "corresponding_authors": "",
    "abstract": "The traditional matrix factorisation (MF)-based recommender system methods, despite their success in making the recommendation, lack explainable recommendations as the produced latent features are meaningless and cannot explain the recommendation. This article introduces an MF-based explainable recommender system framework that utilises the user-item rating data and the available item information to model meaningful user and item latent features. These features are exploited to enhance the rating prediction accuracy and the recommendation explainability. Our proposed feature-based explainable recommender system framework utilises these meaningful user and item latent features to explain the recommendation without relying on private or outer data. The recommendations are explained to the user using text message and bar chart. Our proposed model has been evaluated in terms of the rating prediction accuracy and the reasonableness of the explanation using six real-world benchmark datasets for movies, books, video games, and fashion recommendation systems. The results show that the proposed model can produce accurate explainable recommendations.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4225275672",
    "type": "article"
  },
  {
    "title": "Relational Agents for the Homeless with Tuberculosis Experience: Providing Social Support Through Human–agent Relationships",
    "doi": "https://doi.org/10.1145/3488056",
    "publication_date": "2022-05-20",
    "publication_year": 2022,
    "authors": "Yihyun Jang; Soo Han Im; Younah Kang; Joon Sang Baek",
    "corresponding_authors": "",
    "abstract": "In human–computer interaction (HCI) research, relational agents (RAs) are increasingly used to improve social support for vulnerable groups including people exposed to stigmas, alienation, and isolation. However, technical support for tuberculosis (TB) patients, one such vulnerable group, remains insufficient due to the nature of the infectious disease and difficulties in accessing the homeless community. To derive design considerations for developing RAs targeting homeless TB patients, we conducted an empirical study on the patients. Data were collected through participatory observations and interviews and were processed using deductive thematic analysis. The patients’ environmental and behavioral characteristics were classified, which showed that understanding these factors in the design of an RA is important because the patients’ perception, attitudes, and expectations towards the agent are shaped by (and also shape) their environmental and behavioral characteristics, which consequently affect the nature of relationships formed between them. Therefore, we drew the following design considerations: (1) protection of privacy is a prerequisite to the use of an RA for homeless TB patients and can be addressed from both short-term (technical) and long-term (sociotechnical) perspectives; (2) the homeless group emphasized affective support from the agent, suggesting that relationships per se are already valuable to people who have been socially isolated and stigmatized; (3) consideration of the past memories in selecting social cues can facilitate the exchange of affective expressions in user–agent interaction; and (4) an RA should clarify to its interlocuters its identity as a machine to avoid confusing people with low technological literacy.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4280651696",
    "type": "article"
  },
  {
    "title": "Improving Office Workers’ Workspace Using a Self-adjusting Computer Screen",
    "doi": "https://doi.org/10.1145/3545993",
    "publication_date": "2022-06-28",
    "publication_year": 2022,
    "authors": "Rotem Kronenberg; Tsvi Kuflik; Ilan Shimshoni",
    "corresponding_authors": "",
    "abstract": "With the rapid evolution of technology, computers and their users’ workspaces have become an essential part of our life in general. Today, many people use computers both for work and for personal needs, spending long hours sitting at a desk in front of a computer screen, changing their pose slightly from time to time. This phenomenon impacts people’s health negatively, adversely affecting their musculoskeletal and ocular systems. To mitigate these risks, several different ergonomic solutions have been suggested. This study proposes, demonstrates, and evaluates a technological solution that automatically adjusts the computer screen position and orientation to its user’s current pose, using a simple RGB camera and robotic arm. The automatic adjustment will reduce the physical load on users and better fit their changing poses. The user’s pose is extracted from images continuously acquired by the system’s camera. The most suitable screen position is calculated according to the user’s pose and ergonomic guidelines. Thereafter, the robotic arm adjusts the screen accordingly. The evaluation was done through a user study with 35 users who rated both the idea and the prototype system itself highly.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4283657272",
    "type": "article"
  },
  {
    "title": "Learning and Understanding User Interface Semantics from Heterogeneous Networks with Multimodal and Positional Attributes",
    "doi": "https://doi.org/10.1145/3578522",
    "publication_date": "2022-12-23",
    "publication_year": 2022,
    "authors": "Gary Ang; Ee‐Peng Lim",
    "corresponding_authors": "",
    "abstract": "User interfaces (UI) of desktop, web, and mobile applications involve a hierarchy of objects (e.g., applications, screens, view class, and other types of design objects) with multimodal (e.g., textual and visual) and positional (e.g., spatial location, sequence order, and hierarchy level) attributes. We can therefore represent a set of application UIs as a heterogeneous network with multimodal and positional attributes. Such a network not only represents how users understand the visual layout of UIs but also influences how users would interact with applications through these UIs. To model the UI semantics well for different UI annotation, search, and evaluation tasks, this article proposes the novel Heterogeneous Attention-based Multimodal Positional (HAMP) graph neural network model. HAMP combines graph neural networks with the scaled dot-product attention used in transformers to learn the embeddings of heterogeneous nodes and associated multimodal and positional attributes in a unified manner. HAMP is evaluated with classification and regression tasks conducted on three distinct real-world datasets. Our experiments demonstrate that HAMP significantly out-performs other state-of-the-art models on such tasks. To further provide interpretations of the contribution of heterogeneous network information for understanding the relationships between the UI structure and prediction tasks, we propose Adaptive HAMP (AHAMP), which adaptively learns the importance of different edges linking different UI objects. Our experiments demonstrate AHAMP’s superior performance over HAMP on a number of tasks, and its ability to provide interpretations of the contribution of multimodal and positional attributes, as well as heterogeneous network information to different tasks.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4312179316",
    "type": "article"
  },
  {
    "title": "Predicting Group Choices from Group Profiles",
    "doi": "https://doi.org/10.1145/3639710",
    "publication_date": "2024-01-10",
    "publication_year": 2024,
    "authors": "Hanif Emamgholizadeh; Amra Delić; Francesco Ricci⋆",
    "corresponding_authors": "",
    "abstract": "Group recommender systems (GRSs) identify items to recommend to a group of people by aggregating group members’ individual preferences into a group profile, and selecting the items that have the largest score in the group profile. The GRS predicts that these recommendations would be chosen by the group, by assuming that the group is applying the same preference aggregation strategy as the one adopted by the GRS. However, predicting the choice of a group is more complex since the GRS is not aware of the exact preference aggregation strategy that is going to be used by the group. To this end, the aim of this paper is to validate the research hypothesis that, by using a machine learning approach and a data set of observed group choices, it is possible to predict a group’s final choice, better than by using a standard preference aggregation strategy. Inspired by the Decision Scheme theory, which first tried to address the group choice prediction problem, we search for a group profile definition that, in conjunction with a machine learning model, can be used to accurately predict a group choice. Moreover, to cope with the data scarcity problem, we propose two data augmentation methods, which add synthetic group profiles to the training data, and we hypothesize they can further improve the choice prediction accuracy. We validate our research hypotheses by using a data set containing 282 participants organized in 79 groups. The experiments indicate that the proposed method outperforms baseline aggregation strategies when used for group choice prediction. The method we propose is robust with the presence of missing preference data and achieves a performance superior to what humans can achieve on the group choice prediction task. Finally, the proposed data augmentation method can also improve the prediction accuracy. Our approach can be exploited in novel GRSs to identify the items that the group is likely to choose and to help groups to make even better and fairer choices.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4390695788",
    "type": "article"
  },
  {
    "title": "Entity Footprinting: Modeling Contextual User States via Digital Activity Monitoring",
    "doi": "https://doi.org/10.1145/3643893",
    "publication_date": "2024-02-06",
    "publication_year": 2024,
    "authors": "Zeinab R. Yousefi; Tung Vuong; Marie Al-Ghossein; Tuukka Ruotsalo; Giulio Jaccuci; Samuel Kaski",
    "corresponding_authors": "",
    "abstract": "Our digital life consists of activities that are organized around tasks and exhibit different user states in the digital contexts around these activities. Previous works have shown that digital activity monitoring can be used to predict entities that users will need to perform digital tasks. There have been methods developed to automatically detect the tasks of a user. However, these studies typically support only specific applications and tasks, and relatively little research has been conducted on real-life digital activities. This article introduces user state modeling and prediction with contextual information captured as entities, recorded from real-world digital user behavior, called entity footprinting - a system that records users' digital activities on their screens and proactively provides useful entities across application boundaries without requiring explicit query formulation. Our methodology is to detect contextual user states using latent representations of entities occurring in digital activities. Using topic models and recurrent neural networks, the model learns the latent representation of concurrent entities and their sequential relationships. We report a field study in which the digital activities of 13 people were recorded continuously for 14 days. The model learned from this data is used to (1) predict contextual user states and (2) predict relevant entities for the detected states. The results show improved user state detection accuracy and entity prediction performance compared to static, heuristic, and basic topic models. Our findings have implications for the design of proactive recommendation systems that can implicitly infer users' contextual state by monitoring users' digital activities and proactively recommending the right information at the right time.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4391568830",
    "type": "article"
  },
  {
    "title": "-generAItor: Tree-in-the-loop Text Generation for Language Model Explainability and Adaptation",
    "doi": "https://doi.org/10.1145/3652028",
    "publication_date": "2024-03-14",
    "publication_year": 2024,
    "authors": "Thilo Spinner; Rebecca Kehlbeck; Rita Sevastjanova; Tobias Stähle; Daniel A. Keim; Oliver Deußen; Mennatallah El‐Assady",
    "corresponding_authors": "",
    "abstract": "Large language models (LLMs) are widely deployed in various downstream tasks, e.g., auto-completion, aided writing, or chat-based text generation. However, the considered output candidates of the underlying search algorithm are under-explored and under-explained. We tackle this shortcoming by proposing a tree-in-the-loop approach, where a visual representation of the beam search tree is the central component for analyzing, explaining, and adapting the generated outputs. To support these tasks, we present generAItor, a visual analytics technique, augmenting the central beam search tree with various task-specific widgets, providing targeted visualizations and interaction possibilities. Our approach allows interactions on multiple levels and offers an iterative pipeline that encompasses generating, exploring, and comparing output candidates, as well as fine-tuning the model based on adapted data. Our case study shows that our tool generates new insights in gender bias analysis beyond state-of-the-art template-based methods. Additionally, we demonstrate the applicability of our approach in a qualitative user study. Finally, we quantitatively evaluate the adaptability of the model to few samples, as occurring in text-generation use cases.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4392781242",
    "type": "article"
  },
  {
    "title": "Accelerating Scientific Paper Skimming with Augmented Intelligence Through Customizable Faceted Highlights",
    "doi": "https://doi.org/10.1145/3665648",
    "publication_date": "2024-05-23",
    "publication_year": 2024,
    "authors": "Raymond Fok; Luca Soldaini; Cassidy Trier; Erin Bransom; Kelsey MacMillan; Evie Yu-Yen Cheng; Hita Kambhamettu; Jonathan Bragg; Kyle Lo; Marti A. Hearst; Andrew Head; Daniel S. Weld",
    "corresponding_authors": "",
    "abstract": "Scholars need to keep up with an exponentially increasing flood of scientific papers. To aid this challenge, we introduce Scim, a novel intelligent interface that helps scholars skim papers to rapidly review and gain a cursory understanding of its contents. Scim supports the skimming process by highlighting salient content within a paper, directing a scholar’s attention. These automatically-extracted highlights are faceted by content type, evenly distributed across a paper, and have a density configurable by scholars. We evaluate Scim with an in-lab usability study and a longitudinal diary study, revealing how its highlights facilitate the more efficient construction of a conceptualization of a paper. Finally, we describe the process of scaling highlights from their conception within Scim, a research prototype, to production on over 521,000 papers within the Semantic Reader, a publicly-available augmented reading interface for scientific papers. We conclude by discussing design considerations and tensions for the design of future skimming tools with augmented intelligence.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4398252958",
    "type": "article"
  },
  {
    "title": "Exploring the Effects of Self-Correction Behavior of an Intelligent Virtual Character during a Jigsaw Puzzle Co-Solving Task",
    "doi": "https://doi.org/10.1145/3688006",
    "publication_date": "2024-08-10",
    "publication_year": 2024,
    "authors": "Minsoo Choi; Siqi Guo; Alexandros Koilias; Matias Volonte; Dominic Kao; Christos Mousas",
    "corresponding_authors": "",
    "abstract": "Although researchers have explored how humans perceive the intelligence of virtual characters, few studies have focused on the ability of intelligent virtual characters to fix their mistakes. Thus, we explored the self-correction behavior of a virtual character with different intelligence capabilities in a within-group design ( \\(N=23\\) ) study. For this study, we developed a virtual character that can solve a jigsaw puzzle whose self-correction behavior is controlled by two parameters, namely, Intelligence and Accuracy of Self-correction . Then, we integrated the virtual character into our virtual reality experience and asked participants to co-solve a jigsaw puzzle. During the study, our participants were exposed to five experimental conditions resulting from combinations of the Intelligence and Accuracy of Self-correction parameters. In each condition, we asked our participants to respond to a survey examining their perceptions of the virtual character’s intelligence and awareness (private, public, and surroundings awareness) and user experiences, including trust, enjoyment, performance, frustration, and desire for future interaction. We also collected application logs, including participants’ dwell gaze data, completion times, and the number of puzzle pieces they placed to co-solve the jigsaw puzzle. The results of all the survey ratings and the completion time were statistically significant. Our results indicated that higher levels of Intelligence and Accuracy of Self-correction enhanced not only our participants’ perceptions of the virtual character’s intelligence, awareness (private, public, and surroundings), trustworthiness, and performance but also increased their enjoyment and desire for future interaction with the virtual character while reducing their frustration and completion time. Moreover, we found that as the Intelligence and Accuracy of Self-correction increased, participants had to place fewer puzzle pieces and needed less time to complete the jigsaw puzzle. Finally, regardless of the experimental condition to which we exposed our participants, they gazed at the virtual character for more time compared to the puzzle pieces and puzzle goal in the virtual environment.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4401484957",
    "type": "article"
  },
  {
    "title": "Taking into Account Opponent’s Arguments in Human-Agent Negotiations",
    "doi": "https://doi.org/10.1145/3691643",
    "publication_date": "2024-09-10",
    "publication_year": 2024,
    "authors": "Anıl Doğru; Onur Keskin; Reyhan Aydoğan",
    "corresponding_authors": "",
    "abstract": "Autonomous negotiating agents, which can interact with other agents, aim to solve decision-making problems involving participants with conflicting interests. Designing agents capable of negotiating with human partners requires considering some factors, such as emotional states and arguments. For this purpose, we introduce an extended taxonomy of argument types capturing human speech acts during the negotiation. We propose an argument-based automated negotiating agent that can extract human arguments from a chat-based environment using a hierarchical classifier. Consequently, the proposed agent can understand the received arguments and adapt its strategy accordingly while negotiating with its human counterparts. We initially conducted human-agent negotiation experiments to construct a negotiation corpus to train our classifier. According to the experimental results, it is seen that the proposed hierarchical classifier successfully extracted the arguments from the given text. Moreover, we conducted a second experiment where we tested the performance of the designed negotiation strategy considering the human opponent’s arguments and emotions. Our results showed that the proposed agent beats the human negotiator and gains higher utility than the baseline agent.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4402404957",
    "type": "article"
  },
  {
    "title": "ConfusionLens: Focus+Context Visualization Interface for Performance Analysis of Multiclass Image Classifiers",
    "doi": "https://doi.org/10.1145/3700139",
    "publication_date": "2024-10-17",
    "publication_year": 2024,
    "authors": "Kazuyuki Fujita; Keito Uwaseki; Hongyu Bu; Kazuki Takashima; Yoshifumi Kitamura",
    "corresponding_authors": "",
    "abstract": "Building higher-quality image classification models requires better performance analysis (PA) to help understand their behaviors. We propose ConfusionLens, a dynamic and interactive visualization interface that augments a conventional confusion matrix with focus+context visualization. This interface allows users to seamlessly switch table layouts among three views (overall view, class-level view, and between-class view) while observing all of the instance images in a single screen. We designed and implemented a ConfusionLens prototype that supports hundreds of instances, and then conducted a user study (N=14) to evaluate it compared to the conventional confusion matrix with a split view of instances. Results show that ConfusionLens achieved faster task-completion time in observing instance-level performance and higher accuracy in observing between-class confusion. Moreover, we conducted an expert interview (N=6) to investigate the applicability of our interface to practical PA tasks, and then implemented several extensions of ConfusionLens based on the feedback. Feedback on these extensions from users experienced in image classification (N=5) demonstrated their general usefulness and highlighted their beneficial use in PA tasks.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4403486374",
    "type": "article"
  },
  {
    "title": "ConverSearch: Supporting Experts in Human Behavior Analysis of Conversational Videos with a Multimodal Scene Search Tool",
    "doi": "https://doi.org/10.1145/3709012",
    "publication_date": "2024-12-23",
    "publication_year": 2024,
    "authors": "Riku Arakawa; Kiyosu Maeda; Hiromu Yakura",
    "corresponding_authors": "",
    "abstract": "Multimodal scene search of conversations is essential for unlocking valuable insights into social dynamics and enhancing our communication. While experts in conversational analysis have their own knowledge and skills to find key scenes, a lack of comprehensive, user-friendly tools that streamline the processing of diverse multimodal queries impedes efficiency and objectivity. To address this gap, we developed ConverSearch , a visual-programming-based tool based on insights for effective interface and implementation design derived from a formative study with experts. The tool allows experts to integrate various machine-learning algorithms to capture human behavioral cues without the need for coding. Our user study, employing the System Usability Scale (SUS) and satisfaction metrics, demonstrated high user preference, reflecting the tool’s ease of use and effectiveness in supporting scene search tasks. Additionally, through a deployment trial within industrial organizations, we confirmed the tool’s objectivity, reusability, and potential to enhance expert workflows. This suggests the advantages of expert-AI collaboration in domains requiring human contextual understanding and demonstrates how customizable, transparent tools yielding reusable artifacts can support expert-driven tasks in complex, multimodal environments.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4405724262",
    "type": "article"
  },
  {
    "title": "Have You Lost the Thread? Discovering Ongoing Conversations in Scattered Dialog Blocks",
    "doi": "https://doi.org/10.1145/2885501",
    "publication_date": "2017-06-30",
    "publication_year": 2017,
    "authors": "Fabio Massimo Zanzotto; Lorenzo Ferrone",
    "corresponding_authors": "",
    "abstract": "Finding threads in textual dialogs is emerging as a need to better organize stored knowledge. We capture this need by introducing the novel task of discovering ongoing conversations in scattered dialog blocks. Our aim in this article is twofold. First, we propose a publicly available testbed for the task by solving the insurmountable problem of privacy of Big Personal Data. In fact, we showed that personal dialogs can be surrogated with theatrical plays. Second, we propose a suite of computationally light learning models that can use syntactic and semantic features. With this suite, we showed that models for this challenging task should include features capturing shifts in language use and, possibly, modeling underlying scripts.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2569369828",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on Interactive Computational Visual Analytics",
    "doi": "https://doi.org/10.1145/2594648",
    "publication_date": "2014-04-01",
    "publication_year": 2014,
    "authors": "Remco Chang; David S. Ebert; Daniel A. Keim",
    "corresponding_authors": "",
    "abstract": "This editorial introduction describes the aims and scope of ACM Transactions on Interactive Intelligent Systems 's special issue on interactive computational visual analytics. It explains why visual analytics is crucial to the growing needs surrounding data analysis, and it shows how the four articles selected for this issue reflect this theme.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2064870012",
    "type": "article"
  },
  {
    "title": "A General-Purpose Sensing Floor Architecture for Human-Environment Interaction",
    "doi": "https://doi.org/10.1145/2751566",
    "publication_date": "2015-06-30",
    "publication_year": 2015,
    "authors": "Roberto Vezzani; Martino Lombardi; A. Pieracci; Paolo Santinelli; Rita Cucchiara",
    "corresponding_authors": "",
    "abstract": "Smart environments are now designed as natural interfaces to capture and understand human behavior without a need for explicit human-computer interaction. In this article, we present a general-purpose architecture that acquires and understands human behaviors through a sensing floor. The pressure field generated by moving people is captured and analyzed. Specific actions and events are then detected by a low-level processing engine and sent to high-level interfaces providing different functions. The proposed architecture and sensors are modular, general-purpose, cheap, and suitable for both small- and large-area coverage. Some sample entertainment and virtual reality applications that we developed to test the platform are presented.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2216556207",
    "type": "article"
  },
  {
    "title": "Human Interaction with Artificial Advice Givers",
    "doi": null,
    "publication_date": "2016-07-02",
    "publication_year": 2016,
    "authors": "Nava Tintarev; John O’Donovan; Alexander Felfernig",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2913959523",
    "type": "article"
  },
  {
    "title": "Conversational Context-sensitive Ad Generation with a Few Core-Queries",
    "doi": "https://doi.org/10.1145/3588578",
    "publication_date": "2023-03-23",
    "publication_year": 2023,
    "authors": "Ryoichi Shibata; Shoya Matsumori; Yosuke Fukuchi; Tomoyuki Maekawa; Mitsuhiko Kimoto; Michita Imai",
    "corresponding_authors": "",
    "abstract": "When people are talking together in front of digital signage, advertisements that are aware of the context of the dialogue will work the most effectively. However, it has been challenging for computer systems to retrieve the appropriate advertisement from among the many options presented in large databases. Our proposed system, the Conversational Context-sensitive Advertisement generator (CoCoA), is the first attempt to apply masked word prediction to web information retrieval that takes into account the dialogue context. The novelty of CoCoA is that advertisers simply need to prepare a few abstract phrases, called Core-Queries, and then CoCoA automatically generates a context-sensitive expression as a complete search query by utilizing a masked word prediction technique that adds a word related to the dialogue context to one of the prepared Core-Queries. This automatic generation frees the advertisers from having to come up with context-sensitive phrases to attract users’ attention. Another unique point is that the modified Core-Query offers users speaking in front of the CoCoA system a list of context-sensitive advertisements. CoCoA was evaluated by crowd workers regarding the context-sensitivity of the generated search queries against the dialogue text of multiple domains prepared in advance. The results indicated that CoCoA could present more contextual and practical advertisements than other web-retrieval systems. Moreover, CoCoA acquired a higher evaluation in a particular conversation that included many travel topics to which the Core-Queries were designated, implying that it succeeded in adapting the Core-Queries for the specific ongoing context better than the compared method without any effort on the part of the advertisers. In addition, case studies with users and advertisers revealed that the context-sensitive advertisements generated by CoCoA also had an effect on the content of the ongoing dialogue. Specifically, since pairs unfamiliar with each other more frequently referred to the advertisement CoCoA displayed, the advertisements had an effect on the topics about which the pairs spoke. Moreover, participants of an advertiser role recognized that some of the search queries generated by CoCoA fit the context of a conversation and that CoCoA improved the effect of the advertisement. In particular, they learned how to design of designing a good Core-Query at ease by observing the users’ response to the advertisements retrieved with the generated search queries.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4360612654",
    "type": "article"
  },
  {
    "title": "When Biased Humans Meet Debiased AI: A Case Study in College Major Recommendation",
    "doi": "https://doi.org/10.1145/3611313",
    "publication_date": "2023-08-01",
    "publication_year": 2023,
    "authors": "Clarice Wang; Kathryn Wang; Andrew Bian; Rashidul Islam; Kamrun Naher Keya; James R. Foulds; Shimei Pan",
    "corresponding_authors": "",
    "abstract": "Currently, there is a surge of interest in fair Artificial Intelligence (AI) and Machine Learning (ML) research which aims to mitigate discriminatory bias in AI algorithms, e.g., along lines of gender, age, and race. While most research in this domain focuses on developing fair AI algorithms, in this work, we examine the challenges which arise when humans and fair AI interact. Our results show that due to an apparent conflict between human preferences and fairness, a fair AI algorithm on its own may be insufficient to achieve its intended results in the real world. Using college major recommendation as a case study, we build a fair AI recommender by employing gender debiasing machine learning techniques. Our offline evaluation showed that the debiased recommender makes fairer career recommendations without sacrificing its accuracy in prediction. Nevertheless, an online user study of more than 200 college students revealed that participants on average prefer the original biased system over the debiased system. Specifically, we found that perceived gender disparity is a determining factor for the acceptance of a recommendation. In other words, we cannot fully address the gender bias issue in AI recommendations without addressing the gender bias in humans. We conducted a follow-up survey to gain additional insights into the effectiveness of various design options that can help participants to overcome their own biases. Our results suggest that making fair AI explainable is crucial for increasing its adoption in the real world.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4385456523",
    "type": "article"
  },
  {
    "title": "Perceptual Validation for the Generation of Expressive Movements from End-Effector Trajectories",
    "doi": "https://doi.org/10.1145/3150976",
    "publication_date": "2018-07-05",
    "publication_year": 2018,
    "authors": "Pamela Carreno‐Medrano; Sylvie Gibet; Pierre-François Marteau",
    "corresponding_authors": "",
    "abstract": "Endowing animated virtual characters with emotionally expressive behaviors is paramount to improving the quality of the interactions between humans and virtual characters. Full-body motion, in particular, with its subtle kinematic variations, represents an effective way of conveying emotionally expressive content. However, before synthesizing expressive full-body movements, it is necessary to identify and understand what qualities of human motion are salient to the perception of emotions and how these qualities can be exploited to generate novel and equally expressive full-body movements. Based on previous studies, we argue that it is possible to perceive and generate expressive full-body movements from a limited set of joint trajectories, including end-effector trajectories and additional constraints such as pelvis and elbow trajectories. Hence, these selected trajectories define a significant and reduced motion space, which is adequate for the characterization of the expressive qualities of human motion and that is both suitable for the analysis and generation of emotionally expressive full-body movements. The purpose and main contribution of this work is the methodological framework we defined and used to assess the validity and applicability of the selected trajectories for the perception and generation of expressive full-body movements. This framework consists of the creation of a motion capture database of expressive theatrical movements, the development of a motion synthesis system based on trajectories re-played or re-sampled and inverse kinematics, and two perceptual studies.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2837249313",
    "type": "article"
  },
  {
    "title": "Enhancing Deep Learning with Visual Interactions",
    "doi": "https://doi.org/10.1145/3150977",
    "publication_date": "2019-03-01",
    "publication_year": 2019,
    "authors": "Eric Krokos; Hsueh-Chen Cheng; Jessica S Chang; Bohdan Nebesh; Celeste Lyn Paul; Kirsten Whitley; Amitabh Varshney",
    "corresponding_authors": "",
    "abstract": "Deep learning has emerged as a powerful tool for feature-driven labeling of datasets. However, for it to be effective, it requires a large and finely labeled training dataset. Precisely labeling a large training dataset is expensive, time-consuming, and error prone. In this article, we present a visually driven deep-learning approach that starts with a coarsely labeled training dataset and iteratively refines the labeling through intuitive interactions that leverage the latent structures of the dataset. Our approach can be used to (a) alleviate the burden of intensive manual labeling that captures the fine nuances in a high-dimensional dataset by simple visual interactions, (b) replace a complicated (and therefore difficult to design) labeling algorithm by a simpler (but coarse) labeling algorithm supplemented by user interaction to refine the labeling, or (c) use low-dimensional features (such as the RGB colors) for coarse labeling and turn to higher-dimensional latent structures that are progressively revealed by deep learning, for fine labeling. We validate our approach through use cases on three high-dimensional datasets and a user study.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2919112843",
    "type": "article"
  },
  {
    "title": "Interactive Quality Analytics of User-generated Content",
    "doi": "https://doi.org/10.1145/3150973",
    "publication_date": "2019-03-27",
    "publication_year": 2019,
    "authors": "Cecilia di Sciascio; David Strohmaier; Marcelo Luis Errecalde; Eduardo Veas",
    "corresponding_authors": "",
    "abstract": "Digital libraries and services enable users to access large amounts of data on demand. Yet, quality assessment of information encountered on the Internet remains an elusive open issue. For example, Wikipedia, one of the most visited platforms on the Web, hosts thousands of user-generated articles and undergoes 12 million edits/contributions per month. User-generated content is undoubtedly one of the keys to its success but also a hindrance to good quality. Although Wikipedia has established guidelines for the “perfect article,” authors find it difficult to assert whether their contributions comply with them and reviewers cannot cope with the ever-growing amount of articles pending review. Great efforts have been invested in algorithmic methods for automatic classification of Wikipedia articles (as featured or non-featured) and for quality flaw detection. Instead, our contribution is an interactive tool that combines automatic classification methods and human interaction in a toolkit, whereby experts can experiment with new quality metrics and share them with authors that need to identify weaknesses to improve a particular article. A design study shows that experts are able to effectively create complex quality metrics in a visual analytics environment. In turn, a user study evidences that regular users can identify flaws, as well as high-quality content based on the inspection of automatic quality scores.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2934274461",
    "type": "article"
  },
  {
    "title": "PolicyFlow",
    "doi": "https://doi.org/10.1145/3385729",
    "publication_date": "2020-06-11",
    "publication_year": 2020,
    "authors": "Yongsu Ahn; Yu‐Ru Lin",
    "corresponding_authors": "",
    "abstract": "Stability in social, technical, and financial systems, as well as the capacity of organizations to work across borders, requires consistency in public policy across jurisdictions. The diffusion of laws and regulations across political boundaries can reduce the tension that arises between innovation and consistency. Policy diffusion has been a topic of focus across the social sciences for several decades, but due to limitations of data and computational capacity, researchers have not taken a comprehensive and data-intensive look at the aggregate, cross-policy patterns of diffusion. This work combines visual analytics and text and network analyses to help understand how policies, as represented in digitized text, spread across states. As a result, our approach can quickly guide analysts to progressively gain insights into policy adoption data. We evaluate the effectiveness of our system via case studies with a real-world policy dataset and qualitative interviews with domain experts.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3035580015",
    "type": "article"
  },
  {
    "title": "picoTrans",
    "doi": "https://doi.org/10.1145/2448116.2448121",
    "publication_date": "2013-04-01",
    "publication_year": 2013,
    "authors": "Wei Song; Andrew Finch; Kumiko Tanaka‐Ishii; Keiji Yasuda; Eiichiro Sumita",
    "corresponding_authors": "",
    "abstract": "picoTrans is a prototype system that introduces a novel icon-based paradigm for cross-lingual communication on mobile devices. Our approach marries a machine translation system with the popular picture book. Users interact with picoTrans by pointing at pictures as if it were a picture book; the system generates natural language from these icons and the user is able to interact with the icon sequence to refine the meaning of the words that are generated. When users are satisfied that the sentence generated represents what they wish to express, they tap a translate button and picoTrans displays the translation. Structuring the process of communication in this way has many advantages. First, tapping icons is a very natural method of user input on mobile devices; typing is cumbersome and speech input errorful. Second, the sequence of icons which is annotated both with pictures and bilingually with words is meaningful to both users, and it opens up a second channel of communication between them that conveys the gist of what is being expressed. We performed a number of evaluations of picoTrans to determine: its coverage of a corpus of in-domain sentences; the input efficiency in terms of the number of key presses required relative to text entry; and users' overall impressions of using the system compared to using a picture book. Our results show that we are able to cover 74% of the expressions in our test corpus using a 2000-icon set; we believe that this icon set size is realistic for a mobile device. We also found that picoTrans requires fewer key presses than typing the input and that the system is able to predict the correct, intended natural language sentence from the icon sequence most of the time, making user interaction with the icon sequence often unnecessary. In the user evaluation, we found that in general users prefer using picoTrans and are able to communicate more rapidly and expressively. Furthermore, users had more confidence that they were able to communicate effectively using picoTrans.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W1968834608",
    "type": "article"
  },
  {
    "title": "Introduction to the special issue on eye gaze in intelligent human-machine interaction",
    "doi": "https://doi.org/10.1145/2070719.2070720",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "Elisabeth André; Joyce Chai",
    "corresponding_authors": "",
    "abstract": "Given the recent advances in eye tracking technology and the availability of nonintrusive and high-performance eye tracking devices, there has never been a better time to explore new opportunities to incorporate eye gaze in intelligent and natural human-machine communication. In this special issue, we present six articles that cover various aspects of eye gaze in human-machine interaction, including applications of gaze tracking in human-machine interaction, techniques that recognize gaze gestures and render gaze behaviors, and the analysis of gaze behaviors in social interactions.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2047559669",
    "type": "article"
  },
  {
    "title": "On the Detection of Structural Aesthetic Defects of Android Mobile User Interfaces with a Metrics-based Tool",
    "doi": "https://doi.org/10.1145/3410468",
    "publication_date": "2021-03-15",
    "publication_year": 2021,
    "authors": "Narjes Bessghaier; Makram Soui; Christophe Kolski; Mabrouka Chouchane",
    "corresponding_authors": "",
    "abstract": "Smartphone users are striving for easy-to-learn and use mobile apps user interfaces. Accomplishing these qualities demands an iterative evaluation of the Mobile User Interface (MUI). Several studies stress the value of providing a MUI with a pleasing look and feel to engaging end-users. The MUI, therefore, needs to be free from all kinds of structural aesthetic defects. Such defects are indicators of poor design decisions interfering with the consistency of a MUI and making it more difficult to use. To this end, we are proposing a tool (Aesthetic Defects DEtection Tool (ADDET)) to determine the structural aesthetic dimension of MUIs. Automating this process is useful to designers in evaluating the quality of their designs. Our approach is composed of two modules. (1) Metrics assessment is based on the static analysis of a tree-structured layout of the MUI. We used 15 geometric metrics (also known as structural or aesthetic metrics) to check various structural properties before a defect is triggered. (2) Defects detection: The manual combination of metrics and defects are time-consuming and user-dependent when determining a detection rule. Thus, we perceive the process of identification of defects as an optimization problem. We aim to automatically combine the metrics related to a particular defect and optimize the accuracy of the rules created by assigning a weight, representing the metric importance in detecting a defect. We conducted a quantitative and qualitative analysis to evaluate the accuracy of the proposed tool in computing metrics and detecting defects. The findings affirm the tool’s reliability when assessing a MUI’s structural design problems with 71% accuracy.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3137424160",
    "type": "article"
  },
  {
    "title": "Learn, Generate, Rank, Explain: A Case Study of Visual Explanation by Generative Machine Learning",
    "doi": "https://doi.org/10.1145/3465407",
    "publication_date": "2021-09-03",
    "publication_year": 2021,
    "authors": "Chris Kim; Lin Xiao; Christopher Collins; Graham W. Taylor; Mohamed R. Amer",
    "corresponding_authors": "",
    "abstract": "While the computer vision problem of searching for activities in videos is usually addressed by using discriminative models, their decisions tend to be opaque and difficult for people to understand. We propose a case study of a novel machine learning approach for generative searching and ranking of motion capture activities with visual explanation. Instead of directly ranking videos in the database given a text query, our approach uses a variant of Generative Adversarial Networks (GANs) to generate exemplars based on the query and uses them to search for the activity of interest in a large database. Our model is able to achieve comparable results to its discriminative counterpart, while being able to dynamically generate visual explanations. In addition to our searching and ranking method, we present an explanation interface that enables the user to successfully explore the model’s explanations and its confidence by revealing query-based, model-generated motion capture clips that contributed to the model’s decision. Finally, we conducted a user study with 44 participants to show that by using our model and interface, participants benefit from a deeper understanding of the model’s conceptualization of the search query. We discovered that the XAI system yielded a comparable level of efficiency, accuracy, and user-machine synchronization as its black-box counterpart, if the user exhibited a high level of trust for AI explanation.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3198189810",
    "type": "article"
  },
  {
    "title": "Cues of Violent Intergroup Conflict Diminish Perceptions of Robotic Personhood",
    "doi": "https://doi.org/10.1145/3181674",
    "publication_date": "2018-11-19",
    "publication_year": 2018,
    "authors": "Colin Holbrook",
    "corresponding_authors": "Colin Holbrook",
    "abstract": "Convergent lines of evidence indicate that anthropomorphic robots are represented using neurocognitive mechanisms typically employed in social reasoning about other people. Relatedly, a growing literature documents that contexts of threat can exacerbate coalitional biases in social perceptions. Integrating these research programs, the present studies test whether cues of violent intergroup conflict modulate perceptions of the intelligence, emotional experience, or overall personhood of robots. In Studies 1 and 2, participants evaluated a large, bipedal all-terrain robot; in Study 3, participants evaluated a small, social robot with humanlike facial and vocal characteristics. Across all studies, cues of violent conflict caused significant decreases in perceived robotic personhood, and these shifts were mediated by parallel reductions in emotional connection with the robot (with no significant effects of threat on attributions of intelligence/skill). In addition, in Study 2, participants in the conflict condition estimated the large bipedal robot to be less effective in military combat, and this difference was mediated by the reduction in perceived robotic personhood. These results are discussed as they motivate future investigation into the links among threat, coalitional bias and human–robot interaction.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2901929783",
    "type": "article"
  },
  {
    "title": "Toward Universal Spatialization Through Wikipedia-Based Semantic Enhancement",
    "doi": "https://doi.org/10.1145/3213769",
    "publication_date": "2019-04-09",
    "publication_year": 2019,
    "authors": "Shilad Sen; Anja Beth Swoap; Qisheng Li; Ilse N Dippenaar; Monica Ngo; Sarah Pujol; Rebecca Gold; Brooke Boatman; Brent Hecht; Bret Jackson",
    "corresponding_authors": "",
    "abstract": "This article introduces Cartograph, a visualization system that harnesses the vast world knowledge encoded within Wikipedia to create thematic maps of almost any data. Cartograph extends previous systems that visualize non-spatial data using geographic approaches. Although these systems required data with an existing semantic structure, Cartograph unlocks spatial visualization for a much larger variety of datasets by enhancing input datasets with semantic information extracted from Wikipedia. Cartograph’s map embeddings use neural networks trained on Wikipedia article content and user navigation behavior. Using these embeddings, the system can reveal connections between points that are unrelated in the original datasets but are related in meaning and therefore embedded close together on the map. We describe the design of the system and key challenges we encountered. We present findings from two user studies exploring design choices and use of the system.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2938084301",
    "type": "article"
  },
  {
    "title": "Learning Context-dependent Personal Preferences for Adaptive Recommendation",
    "doi": "https://doi.org/10.1145/3359755",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "Keita Higuchi; Hiroki Tsuchida; Eshed Ohn-Bar; Yoichi Sato; Kris Kitani",
    "corresponding_authors": "",
    "abstract": "We propose two online-learning algorithms for modeling the personal preferences of users of interactive systems. The proposed algorithms leverage user feedback to estimate user behavior and provide personalized adaptive recommendation for supporting context-dependent decision-making. We formulate preference modeling as online prediction algorithms over a set of learned policies, i.e., policies generated via supervised learning with interaction and context data collected from previous users. The algorithms then adapt to a target user by learning the policy that best predicts that user’s behavior and preferences. We also generalize the proposed algorithms for a more challenging learning case in which they are restricted to a limited number of trained policies at each timestep, i.e., for mobile settings with limited resources. While the proposed algorithms are kept general for use in a variety of domains, we developed an image-filter-selection application. We used this application to demonstrate how the proposed algorithms can quickly learn to match the current user’s selections. Based on these evaluations, we show that (1) the proposed algorithms exhibit better prediction accuracy compared to traditional supervised learning and bandit algorithms, (2) our algorithms are robust under challenging limited prediction settings in which a smaller number of expert policies is assumed. Finally, we conducted a user study to demonstrate how presenting users with the prediction results of our algorithms significantly improves the efficiency of the overall interaction experience.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3041496970",
    "type": "article"
  },
  {
    "title": "A Method and Analysis to Elicit User-Reported Problems in Intelligent Everyday Applications",
    "doi": "https://doi.org/10.1145/3370927",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "Malin Eiband; Sarah Theres Völkel; Daniel Buschek; Sophia Cook; Heinrich Hußmann",
    "corresponding_authors": "",
    "abstract": "The complex nature of intelligent systems motivates work on supporting users during interaction, for example, through explanations. However, as of yet, there is little empirical evidence in regard to specific problems users face when applying such systems in everyday situations. This article contributes a novel method and analysis to investigate such problems as reported by users: We analysed 45,448 reviews of four apps on the Google Play Store (Facebook, Netflix, Google Maps, and Google Assistant) with sentiment analysis and topic modelling to reveal problems during interaction that can be attributed to the apps’ algorithmic decision-making. We enriched this data with users’ coping and support strategies through a follow-up online survey (N = 286). In particular, we found problems and strategies related to content, algorithm, user choice, and feedback. We discuss corresponding implications for designing user support, highlighting the importance of user control and explanations of output rather than processes.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3041677164",
    "type": "article"
  },
  {
    "title": "Affect-Aware Word Clouds",
    "doi": "https://doi.org/10.1145/3370928",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "Tugba Kulahcioglu; Gerard de Melo",
    "corresponding_authors": "",
    "abstract": "Word clouds are widely used for non-analytic purposes, such as introducing a topic to students, or creating a gift with personally meaningful text. Surveys show that users prefer tools that yield word clouds with a stronger emotional impact. Fonts and color palettes are powerful typographical signals that may determine this impact. Typically, these signals are assigned randomly, or expected to be chosen by the users. We present an affect-aware font and color palette selection methodology that aims to facilitate more informed choices. We infer associations of fonts with a set of eight affects, and evaluate the resulting data in a series of user studies both on individual words as well as in word clouds. Relying on a recent study to procure affective color palettes, we carry out a similar user study to understand the impact of color choices on word clouds. Our findings suggest that both fonts and color palettes are powerful tools contributing to the affects evoked by a word cloud. The experiments further confirm that the novel datasets we propose are successful in enabling this. We also find that, for the majority of the affects, both signals need to be congruent to create a stronger impact. Based on this data, we implement a prototype that allows users to specify a desired affect and recommends congruent fonts and color palettes for the word.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3042189966",
    "type": "article"
  },
  {
    "title": "Introduction to the special section on eye gaze and conversation",
    "doi": "https://doi.org/10.1145/2499474.2499479",
    "publication_date": "2013-07-01",
    "publication_year": 2013,
    "authors": "Elisabeth André; Joyce Chai",
    "corresponding_authors": "",
    "abstract": "This editorial introduction first explains the origin of this special section. It then outlines how each of the two articles included sheds light on possibilities for conversational dialog systems to use eye gaze as a signal that reflects aspects of participation in the dialog: degree of engagement and turn taking behavior, respectively.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W1980711366",
    "type": "article"
  },
  {
    "title": "Behavior Understanding for Arts and Entertainment",
    "doi": "https://doi.org/10.1145/2817208",
    "publication_date": "2015-09-23",
    "publication_year": 2015,
    "authors": "Albert Ali Salah; Hayley Hung; Oya Aran; Hatice Güneş; Matthew Turk",
    "corresponding_authors": "",
    "abstract": "This editorial introduction complements the shorter introduction to the first part of the two-part special issue on Behavior Understanding for Arts and Entertainment. It offers a more expansive discussion of the use of behavior analysis for interactive systems that involve creativity, either for the producer or the consumer of such a system. We first summarise the two articles that appear in this second part of the special issue. We then discuss general questions and challenges in this domain that were suggested by the entire set of seven articles of the special issue and by the comments of the reviewers of these articles.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2020810517",
    "type": "article"
  },
  {
    "title": "Active multiple kernel learning for interactive 3D object retrieval systems",
    "doi": "https://doi.org/10.1145/2030365.2030368",
    "publication_date": "2011-10-01",
    "publication_year": 2011,
    "authors": "Steven C. H. Hoi; Rong Jin",
    "corresponding_authors": "",
    "abstract": "An effective relevance feedback solution plays a key role in interactive intelligent 3D object retrieval systems. In this work, we investigate the relevance feedback problem for interactive intelligent 3D object retrieval, with the focus on studying effective machine learning algorithms for improving the user's interaction in the retrieval task. One of the key challenges is to learn appropriate kernel similarity measure between 3D objects through the relevance feedback interaction with users. We address this challenge by presenting a novel framework of Active multiple kernel learning (AMKL), which exploits multiple kernel learning techniques for relevance feedback in interactive 3D object retrieval. The proposed framework aims to efficiently identify an optimal combination of multiple kernels by asking the users to label the most informative 3D images. We evaluate the proposed techniques on a dataset of over 10,000 3D models collected from the World Wide Web. Our experimental results show that the proposed AMKL technique is significantly more effective for 3D object retrieval than the regular relevance feedback techniques widely used in interactive content-based image retrieval, and thus is promising for enhancing user's interaction in such interactive intelligent retrieval systems.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2034873715",
    "type": "article"
  },
  {
    "title": "Collaborative Language Models for Localized Query Prediction",
    "doi": "https://doi.org/10.1145/2622617",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "Yi Fang; Ziad Al Bawab; Jean-François Crespo",
    "corresponding_authors": "",
    "abstract": "Localized query prediction (LQP) is the task of estimating web query trends for a specific location. This problem subsumes many interesting personalized web applications such as personalization for buzz query detection, for query expansion, and for query recommendation. These personalized applications can greatly enhance user interaction with web search engines by providing more customized information discovered from user input (i.e., queries), but the LQP task has rarely been investigated in the literature. Although exist abundant work on estimating global web search trends does exist, it often encounters the big challenge of data sparsity when personalization comes into play. In this article, we tackle the LQP task by proposing a series of collaborative language models (CLMs). CLMs alleviate the data sparsity issue by collaboratively collecting queries and trend information from the other locations. The traditional statistical language models assume a fixed background language model, which loses the taste of personalization. In contrast, CLMs are personalized language models with flexible background language models customized to various locations. The most sophisticated CLM enables the collaboration to adapt to specific query topics, which further advances the personalization level. An extensive set of experiments have been conducted on a large-scale web query log to demonstrate the effectiveness of the proposed models.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2084248356",
    "type": "article"
  },
  {
    "title": "In Memoriam",
    "doi": "https://doi.org/10.1145/2533670.2533671",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Anthony Jameson",
    "corresponding_authors": "Anthony Jameson",
    "abstract": "This recollection of John Riedl, founding coeditor-in-chief of the ACM Transactions on Interactive Intelligent Systems , presents a picture by editors of the journal of what it was like to collaborate and interact with him.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2085728873",
    "type": "article"
  },
  {
    "title": "Planning for Reasoning with Multiple Common Sense Knowledge Bases",
    "doi": "https://doi.org/10.1145/2362394.2362399",
    "publication_date": "2012-09-01",
    "publication_year": 2012,
    "authors": "Yen‐Ling Kuo; Jane Yung-jen Hsu",
    "corresponding_authors": "",
    "abstract": "Intelligent user interfaces require common sense knowledge to bridge the gap between the functionality of applications and the user’s goals. While current reasoning methods have been used to provide contextual information for interface agents, the quality of their reasoning results is limited by the coverage of their underlying knowledge bases. This article presents reasoning composition , a planning-based approach to integrating reasoning methods from multiple common sense knowledge bases to answer queries. The reasoning results of one reasoning method are passed to other reasoning methods to form a reasoning chain to the target context of a query. By leveraging different weak reasoning methods, we are able to find answers to queries that cannot be directly answered by querying a single common sense knowledge base. By conducting experiments on ConceptNet and WordNet, we compare the reasoning results of reasoning composition, directly querying merged knowledge bases, and spreading activation. The results show an 11.03% improvement in coverage over directly querying merged knowledge bases and a 49.7% improvement in accuracy over spreading activation. Two case studies are presented, showing how reasoning composition can improve performance of retrieval in a video editing system and a dialogue assistant.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2093978740",
    "type": "article"
  },
  {
    "title": "Holistic Transfer to Rank for Top-N Recommendation",
    "doi": "https://doi.org/10.1145/3434360",
    "publication_date": "2021-03-15",
    "publication_year": 2021,
    "authors": "Wanqi Ma; Xiaoxiao Liao; Wei Dai; Weike Pan; Zhong Ming",
    "corresponding_authors": "",
    "abstract": "Recommender systems have been a valuable component in various online services such as e-commerce and entertainment. To provide an accurate top-N recommendation list of items for each target user, we have to answer a very basic question of how to model users’ feedback effectively. In this article, we focus on studying users’ explicit feedback, which is usually assumed to contain more preference information than the counterpart, i.e., implicit feedback. In particular, we follow two very recent transfer to rank algorithms by converting the original feedback to three different but related views of examinations, scores, and purchases, and then propose a novel solution called holistic transfer to rank (HoToR), which is able to address the uncertainty challenge and the inconvenience challenge in the existing works. More specifically, we take the rating scores as a weighting strategy to alleviate the uncertainty of the examinations, and we design a holistic one-stage solution to address the inconvenience of the two/three-stage training and prediction procedures in previous works. We then conduct extensive empirical studies in a direct comparison with the two closely related transfer learning algorithms and some very competitive factorization- and neighborhood-based methods on three public datasets and find that our HoToR performs significantly better than the other methods in terms of several ranking-oriented evaluation metrics.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3137268186",
    "type": "article"
  },
  {
    "title": "Photo Sequences of Varying Emotion: Optimization with a Valence-Arousal Annotated Dataset",
    "doi": "https://doi.org/10.1145/3458844",
    "publication_date": "2021-06-30",
    "publication_year": 2021,
    "authors": "Christos Mousas; Claudia Krogmeier; Wang Zhi-quan",
    "corresponding_authors": "",
    "abstract": "Synthesizing photo products such as photo strips and slideshows using a database of images is a time-consuming and tedious process that requires significant manual work. To overcome this limitation, we developed a method that automatically synthesizes photo sequences based on several design parameters. Our method considers the valence and arousal ratings of images in conjunction with parameters related to both the visual consistency of the synthesized photo sequence and the progression of valence and arousal throughout the photo sequence. Our method encodes valence, arousal, and visual consistency parameters as cost terms into a total cost function while applying a Markov chain Monte Carlo optimization techniques called simulated annealing to synthesize the photo sequence based on user-defined target objectives in a few seconds. As our method was developed for the synthesis of photo sequences using the valence-arousal emotional model, a user study was conducted to evaluate the efficacy of the synthesized photo sequences in triggering valence-arousal ratings as expected. Our results indicate that the proposed method synthesizes photo sequences in which valence and arousal dimensions are perceived as expected by participants; however, valence may be more appropriately perceived than arousal.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3184118861",
    "type": "article"
  },
  {
    "title": "Expressive Cognitive Architecture for a Curious Social Robot",
    "doi": "https://doi.org/10.1145/3451531",
    "publication_date": "2021-06-30",
    "publication_year": 2021,
    "authors": "Maor Rosenberg; Hae Won Park; Rinat B. Rosenberg‐Kima; Safinah Ali; Anastasia K. Ostrowski; Cynthia Breazeal; Goren Gordon",
    "corresponding_authors": "",
    "abstract": "Artificial curiosity, based on developmental psychology concepts wherein an agent attempts to maximize its learning progress, has gained much attention in recent years. Similarly, social robots are slowly integrating into our daily lives, in schools, factories, and in our homes. In this contribution, we integrate recent advances in artificial curiosity and social robots into a single expressive cognitive architecture. It is composed of artificial curiosity and social expressivity modules and their unique link, i.e., the robot verbally and non-verbally communicates its internally estimated learning progress, or learnability, to its human companion. We implemented this architecture in an interaction where a fully autonomous robot took turns with a child trying to select and solve tangram puzzles on a tablet. During the curious robot’s turn, it selected its estimated most learnable tangram to play, communicated its selection to the child, and then attempted at solving it. We validated the implemented architecture and showed that the robot learned, estimated its learnability, and improved when its selection was based on its learnability estimation. Moreover, we ran a comparison study between curious and non-curious robots, and showed that the robot’s curiosity-based behavior influenced the child’s selections. Based on the artificial curiosity module of the robot, we have formulated an equation that estimates each child’s moment-by-moment curiosity based on their selections. This analysis revealed an overall significant decrease in estimated curiosity during the interaction. However, this drop in estimated curiosity was significantly larger with the non-curious robot, compared to the curious one. These results suggest that the new architecture is a promising new approach to integrate state-of-the-art curiosity-based algorithms to the growing field of social robots.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3185900819",
    "type": "article"
  },
  {
    "title": "Projection Path Explorer: Exploring Visual Patterns in Projected Decision-making Paths",
    "doi": "https://doi.org/10.1145/3387165",
    "publication_date": "2021-09-03",
    "publication_year": 2021,
    "authors": "Andreas Hinterreiter; Christian Steinparz; Moritz Schöfl; Holger Stitz; Marc Streit",
    "corresponding_authors": "",
    "abstract": "In problem-solving, a path towards solutions can be viewed as a sequence of decisions. The decisions, made by humans or computers, describe a trajectory through a high-dimensional representation space of the problem. By means of dimensionality reduction, these trajectories can be visualized in lower-dimensional space. Such embedded trajectories have previously been applied to a wide variety of data, but analysis has focused almost exclusively on the self-similarity of single trajectories. In contrast, we describe patterns emerging from drawing many trajectories -- for different initial conditions, end states, and solution strategies -- in the same embedding space. We argue that general statements about the problem-solving tasks and solving strategies can be made by interpreting these patterns. We explore and characterize such patterns in trajectories resulting from human and machine-made decisions in a variety of application domains: logic puzzles (Rubik's cube), strategy games (chess), and optimization problems (neural network training). We also discuss the importance of suitably chosen representation spaces and similarity metrics for the embedding.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3197822619",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on Big Personal Data in Interactive Intelligent Systems",
    "doi": "https://doi.org/10.1145/3101102",
    "publication_date": "2017-06-30",
    "publication_year": 2017,
    "authors": "Federica Cena; Cristina Gena; G.J.P.M. Houben; Markus Strohmaier",
    "corresponding_authors": "",
    "abstract": "This brief introduction begins with an overview of the types of research that are relevant to the special issue on Big Personal Data in Interactive Intelligent Systems. The overarching question is: How can big personal data be collected, analyzed, and exploited so as to provide new or improved forms of interaction with intelligent systems, and what new issues have to be taken into account? The three articles accepted for the special issue are then characterized in terms of the concepts of this overview.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2739646587",
    "type": "article"
  },
  {
    "title": "FourEyes",
    "doi": "https://doi.org/10.1145/3237188",
    "publication_date": "2019-08-09",
    "publication_year": 2019,
    "authors": "Jean Song; Raymond Fok; Juho Kim; Walter S. Lasecki",
    "corresponding_authors": "",
    "abstract": "Crowdsourcing is a common means of collecting image segmentation training data for use in a variety of computer vision applications. However, designing accurate crowd-powered image segmentation systems is challenging, because defining object boundaries in an image requires significant fine motor skills and hand-eye coordination, which makes these tasks error-prone. Typically, special segmentation tools are created and then answers from multiple workers are aggregated to generate more accurate results. However, individual tool designs can bias how and where people make mistakes, resulting in shared errors that remain even after aggregation. In this article, we introduce a novel crowdsourcing approach that leverages tool diversity as a means of improving aggregate crowd performance. Our idea is that given a diverse set of tools, answer aggregation done across tools can help improve the collective performance by offsetting systematic biases induced by the individual tools themselves. To demonstrate the effectiveness of the proposed approach, we design four different tools and present FourEyes, a crowd-powered image segmentation system that uses aggregation across different tools. We then conduct a series of studies that evaluate different aggregation conditions and show that using multiple tools can significantly improve aggregate accuracy. Furthermore, we investigate the idea of applying post-processing for multi-tool aggregation in terms of correction mechanism. We introduce a novel region-based method for synthesizing more accurate bounds for image segmentation tasks through averaging surrounding annotations. In addition, we explore the effect of adjusting the threshold parameter of an EM-based aggregation method. Our results suggest that not only the individual tool’s design, but also the correction mechanism, can affect the performance of multi-tool aggregation. This article extends a work presented at ACM IUI 2018 [46] by providing a novel region-based error-correction method and additional in-depth evaluation of the proposed approach.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2967708711",
    "type": "article"
  },
  {
    "title": "<i>Textflow</i> : Toward Supporting Screen-free Manipulation of Situation-Relevant Smart Messages",
    "doi": "https://doi.org/10.1145/3519263",
    "publication_date": "2022-04-28",
    "publication_year": 2022,
    "authors": "Pegah Karimi; Emanuele Plebani; Aqueasha Martin-Hammond; Davide Bolchini",
    "corresponding_authors": "",
    "abstract": "Texting relies on screen-centric prompts designed for sighted users, still posing significant barriers to people who are blind and visually impaired (BVI). Can we re-imagine texting untethered from a visual display? In an interview study, 20 BVI adults shared situations surrounding their texting practices, recurrent topics of conversations, and challenges. Informed by these insights, we introduce TextFlow , a mixed-initiative context-aware system that generates entirely auditory message options relevant to the users’ location, activity, and time of the day. Users can browse and select suggested aural messages using finger-taps supported by an off-the-shelf finger-worn device without having to hold or attend to a mobile screen. In an evaluative study, 10 BVI participants successfully interacted with TextFlow to browse and send messages in screen-free mode. The experiential response of the users shed light on the importance of bypassing the phone and accessing rapidly controllable messages at their fingertips while preserving privacy and accuracy with respect to speech or screen-based input. We discuss how non-visual access to proactive, contextual messaging can support the blind in a variety of daily scenarios.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4293236982",
    "type": "article"
  },
  {
    "title": "A Personalized Interaction Mechanism Framework for Micro-moment Recommender Systems",
    "doi": "https://doi.org/10.1145/3569586",
    "publication_date": "2022-10-29",
    "publication_year": 2022,
    "authors": "Yiling Lin; Shao-Wei Lee",
    "corresponding_authors": "",
    "abstract": "The emergence of the micro-moment concept highlights the influence of context; recommender system design should reflect this trend. In response to different contexts, a micro-moment recommender system (MMRS) requires an effective interaction mechanism that allows users to easily interact with the system in a way that supports autonomy and promotes the creation and expression of self. We study four types of interaction mechanisms to understand which personalization approach is the most suitable design for MMRSs. We assume that designs that support micro-moment needs well are those that give users more control over the system and constitute a lighter user burden. We test our hypothesis via a two-week between-subject field study in which participants used our system and provided feedback. User-initiated and mix-initiated intention mechanisms show higher perceived active control, and the additional controls do not add to user burdens. Therefore, these two designs suit the MMRS interaction mechanism.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4307811148",
    "type": "article"
  },
  {
    "title": "Brief Introduction to the Special Issue on Behavior Understanding for Arts and Entertainment",
    "doi": "https://doi.org/10.1145/2786762",
    "publication_date": "2015-07-09",
    "publication_year": 2015,
    "authors": "Albert Ali Salah; Hayley Hung; Oya Aran; Hatice Güneş; Matthew Turk",
    "corresponding_authors": "",
    "abstract": "This editorial introduction describes the aims and scope of the special issue of the ACM Transactions on Interactive Intelligent Systems on Behavior Understanding for Arts and Entertainment, which is being published in issues 2 and 3 of volume 5 of the journal. Here we offer a brief introduction to the use of behavior analysis for interactive systems that involve creativity in either the creator or the consumer of a work of art. We then characterize each of the five articles included in this first part of the special issue, which span a wide range of applications.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2240213860",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on New Directions in Eye Gaze for Interactive Intelligent Systems",
    "doi": "https://doi.org/10.1145/2893485",
    "publication_date": "2016-04-21",
    "publication_year": 2016,
    "authors": "Yukiko Nakano; Roman Bednarik; Hung‐Hsuan Huang; Kristiina Jokinen",
    "corresponding_authors": "",
    "abstract": "Eye gaze has been used broadly in interactive intelligent systems. The research area has grown in recent years to cover emerging topics that go beyond the traditional focus on interaction between a single user and an interactive system. This special issue presents five articles that explore new directions of gaze-based interactive intelligent systems, ranging from communication robots in dyadic and multiparty conversations to a driving simulator that uses eye gaze evidence to critique learners’ behavior.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2337230303",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2744352",
    "publication_date": "2015-03-25",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We present a systematic exploration of how to utilize video game context (e.g., player and environmental state) to modify and augment existing 3D gesture recognizers to improve accuracy for large gesture sets. Specifically, our work develops and ...",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4235787379",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2602757",
    "publication_date": "2014-04-01",
    "publication_year": 2014,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We present PromotionRank, a technique for generating a personalized ranking of grocery product promotions based on the contents of the customer’s personal shopping list. PromotionRank consists of four phases. First, information retrieval techniques are ...",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4247538636",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2638542",
    "publication_date": "2014-07-01",
    "publication_year": 2014,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Recommender systems are firmly established as a standard technology for assisting users with their choices; however, little attention has been paid to the application of the user model in recommender systems, particularly the variability and noise that ...",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4251086380",
    "type": "paratext"
  },
  {
    "title": "Introduction to the Special Issue on Common Sense for Interactive Systems",
    "doi": "https://doi.org/10.1145/2362394.2362396",
    "publication_date": "2012-09-01",
    "publication_year": 2012,
    "authors": "Henry Lieberman; Catherine Havasi",
    "corresponding_authors": "",
    "abstract": "This editorial introduction describes the aims and scope of the special issue on Common Sense for Interactive Systems of the ACM Transactions on Interactive Intelligent Systems. It explains why the common sense knowledge problem is crucial for both artificial intelligence and human-computer interaction, and it shows how the four articles selected for this issue fit into the theme.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W1969053949",
    "type": "article"
  },
  {
    "title": "Introduction to the special section on internet-scale human problem solving",
    "doi": "https://doi.org/10.1145/2448116.2448117",
    "publication_date": "2013-04-01",
    "publication_year": 2013,
    "authors": "Fausto Giunchiglia; David Robertson",
    "corresponding_authors": "",
    "abstract": "This editorial introduction first outlines some of the research challenges raised by the emerging forms of internet-scale human problem solving. It then explains how the two articles in this special section can serve as illuminating complementary case studies, providing concrete examples embedded in general conceptual frameworks.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W1990945103",
    "type": "article"
  },
  {
    "title": "Rating Bias and Preference Acquisition",
    "doi": "https://doi.org/10.1145/2499673",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Jill Freyne; Shlomo Berkovsky; Gregory A. Smith",
    "corresponding_authors": "",
    "abstract": "Personalized systems and recommender systems exploit implicitly and explicitly provided user information to address the needs and requirements of those using their services. User preference information, often in the form of interaction logs and ratings data, is used to identify similar users, whose opinions are leveraged to inform recommendations or to filter information. In this work we explore a different dimension of information trends in user bias and reasoning learned from ratings provided by users to a recommender system. Our work examines the characteristics of a dataset of 100,000 user ratings on a corpus of recipes, which illustrates stable user bias towards certain features of the recipes (cuisine type, key ingredient, and complexity). We exploit this knowledge to design and evaluate a personalized rating acquisition tool based on active learning, which leverages user biases in order to obtain ratings bearing high-value information and to reduce prediction errors with new users.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2009464737",
    "type": "article"
  },
  {
    "title": "Introduction to the special issue on affective interaction in natural environments",
    "doi": "https://doi.org/10.1145/2133366.2133367",
    "publication_date": "2012-03-01",
    "publication_year": 2012,
    "authors": "Ginevra Castellano; Laurel D. Riek; Christopher Peters; Kostas Karpouzis; Jean‐Claude Martin; Louis‐Philippe Morency",
    "corresponding_authors": "",
    "abstract": "Affect-sensitive systems such as social robots and virtual agents are increasingly being investigated in real-world settings. In order to work effectively in natural environments, these systems require the ability to infer the affective and mental states of humans and to provide appropriate timely output that helps to sustain long-term interactions. This special issue, which appears in two parts, includes articles on the design of socio-emotional behaviors and expressions in robots and virtual agents and on computational approaches for the automatic recognition of social signals and affective states.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2109794557",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2070719",
    "publication_date": "2012-01-01",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Given the recent advances in eye tracking technology and the availability of nonintrusive and high-performance eye tracking devices, there has never been a better time to explore new opportunities to incorporate eye gaze in intelligent and natural human-...",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4232286146",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2209310",
    "publication_date": "2012-06-01",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Bias in the news media is an inherent flaw of the news production process. The bias often causes a sharp increase in political polarization and in the cost of conflict on social issues such as the Iraq war. This article presents NewsCube, a novel ...",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4239638278",
    "type": "paratext"
  },
  {
    "title": "Estimating Collective Attention toward a Public Display",
    "doi": "https://doi.org/10.1145/3230715",
    "publication_date": "2018-07-24",
    "publication_year": 2018,
    "authors": "Wolfgang Narzt; Otto Weichselbaum; Gustav Pomberger; Markus Hofmarcher; Michael Strauß; Peter Holzkorn; Roland Haring; Monika Sturm",
    "corresponding_authors": "",
    "abstract": "Enticing groups of passers-by to focused interaction with a public display requires the display system to take appropriate action that depends on how much attention the group is already paying to the display. In the design of such a system, we might want to present the content so that it indicates that a part of the group that is looking head-on at the display has already been registered and is addressed individually, whereas it simultaneously emits a strong audio signal that makes the inattentive rest of the group turn toward it. The challenge here is to define and delimit adequate mixed attention states for groups of people, allowing for classifying collective attention based on inhomogeneous variants of individual attention, i.e., where some group members might be highly attentive, others even interacting with the public display, and some unperceptive. In this article, we present a model for estimating collective human attention toward a public display and investigate technical methods for practical implementation that employs measurement of physical expressive features of people appearing within the display's field of view (i.e., the basis for deriving a person's attention). We delineate strengths and weaknesses and prove the potentials of our model by experimentally exerting influence on the attention of groups of passers-by in a public gaming scenario.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2883531380",
    "type": "article"
  },
  {
    "title": "AutoRL X: Automated Reinforcement Learning on the Web",
    "doi": "https://doi.org/10.1145/3670692",
    "publication_date": "2024-06-03",
    "publication_year": 2024,
    "authors": "Loraine Franke; Daniel Karl I. Weidele; Nima Dehmamy; Lipeng Ning; Daniel Haehn",
    "corresponding_authors": "",
    "abstract": "Reinforcement Learning (RL) is crucial in decision optimization, but its inherent complexity often presents challenges in interpretation and communication. Building upon AutoDOViz — an interface that pushed the boundaries of Automated RL for Decision Optimization — this paper unveils an open-source expansion with a web-based platform for RL. Our work introduces a taxonomy of RL visualizations and launches a dynamic web platform, leveraging backend flexibility for AutoRL frameworks like ARLO and Svelte.js for a smooth interactive user experience in the front end. Since AutoDOViz is not open-source, we present AutoRL X, a new interface designed to visualize RL processes. AutoRL X is shaped by the extensive user feedback and expert interviews from AutoDOViz studies, and it brings forth an intelligent interface with real-time, intuitive visualization capabilities that enhance understanding, collaborative efforts, and personalization of RL agents. Addressing the gap in accurately representing complex real-world challenges within standard RL environments, we demonstrate our tool's application in healthcare, explicitly optimizing brain stimulation trajectories. A user study contrasts the performance of human users optimizing electric fields via a 2D interface with RL agents’ behavior that we visually analyze in AutoRL X, assessing the practicality of automated RL. All our data and code is openly available at: https://github.com/lorifranke/autorlx .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4399285924",
    "type": "article"
  },
  {
    "title": "2023 TiiS Best Paper announcement",
    "doi": "https://doi.org/10.1145/3690000",
    "publication_date": "2024-08-30",
    "publication_year": 2024,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4402047245",
    "type": "article"
  },
  {
    "title": "Market-based recommendation: Agents that compete for consumer attention",
    "doi": null,
    "publication_date": "2004-01-01",
    "publication_year": 2004,
    "authors": "SM Sander Bohté; EH Gerding; JA Han La Poutré",
    "corresponding_authors": "",
    "abstract": "The amount of attention available for recommending suppliers to consumers on e-commerce sites is typically limited. We present a competitive distributed recommendation mechanism based on adaptive software agents for efficiently allocating the attention space, or banners. In the example of an electronic shopping mall, the task is delegated to the individual shops, each of which evaluates the information that is available about the consumer and his or her interests (e.g. keywords, product queries, and available parts of a profile). Shops make a monetary bid in an auction where a limited amount of attention space for the arriving consumer is sold. Each shop is represented by a software agent that bids for each consumer. This allows shops to rapidly adapt their bidding strategy to focus on consumers interested in their offerings.For various basic and simple models for on-line consumers, shops, and profiles, we demonstrate the feasibility of our system by evolutionary simulations as in the field of agent-based computational economics (ACE). We also develop adaptive software agents that learn bidding-strategies, based on neural networks and strategy exploration heuristics. Furthermore, we address the commercial and technological advantages of this distributed market-based approach. The mechanism we describe is not limited to the example of the electronic shopping mall, but can easily be extended to other domains.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2584401655",
    "type": "article"
  },
  {
    "title": "Introduction to the TiiS Special Column",
    "doi": "https://doi.org/10.1145/3427592",
    "publication_date": "2020-11-23",
    "publication_year": 2020,
    "authors": "Michele X. Zhou",
    "corresponding_authors": "Michele X. Zhou",
    "abstract": "No abstract available.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3108149801",
    "type": "article"
  },
  {
    "title": "Special Issue on Data-Driven Personality Modeling for Intelligent Human-Computer Interaction",
    "doi": "https://doi.org/10.1145/3402522",
    "publication_date": "2020-09-30",
    "publication_year": 2020,
    "authors": "Shimei Pan; Oliver Brdiczka; Andrea Kleinsmith; Yangqiu Song",
    "corresponding_authors": "",
    "abstract": "research-article Special Issue on Data-Driven Personality Modeling for Intelligent Human-Computer Interaction Share on Authors: Shimei Pan University of Maryland, Baltimore County University of Maryland, Baltimore CountyView Profile , Oliver Brdiczka Adobe Inc. Adobe Inc.View Profile , Andrea Kleinsmith University of Maryland, Baltimore County University of Maryland, Baltimore CountyView Profile , Yangqiu Song Hong Kong University of Science and Technology Hong Kong University of Science and TechnologyView Profile Authors Info & Affiliations ACM Transactions on Interactive Intelligent SystemsVolume 10Issue 3November 2020 Article No.: 17pp 1–3https://doi.org/10.1145/3402522Published:18 November 2020 0citation129DownloadsMetricsTotal Citations0Total Downloads129Last 12 Months129Last 6 weeks14 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3109359975",
    "type": "article"
  },
  {
    "title": "Evaluation of a Multi-agent “Human-in-the-loop” Game Design System",
    "doi": "https://doi.org/10.1145/3531009",
    "publication_date": "2022-04-30",
    "publication_year": 2022,
    "authors": "Jan Peter Kruse; Andy M. Connor; Stefan Marks",
    "corresponding_authors": "",
    "abstract": "Designing games is a complicated and time-consuming process, where developing new levels for existing games can take weeks. Procedural content generation offers the potential to shorten this timeframe, however, automated design tools are not adopted widely in the game industry. This article presents an expert evaluation of a human-in-the-loop generative design approach for commercial game maps that incorporates multiple computational agents. The evaluation aims to gauge the extent to which such an approach could support and be accepted by human game designers and to determine whether the computational agents improve the overall design. To evaluate the approach, 11 game designers utilized the approach to design game levels with the computational agents both active and inactive. Eye-tracking, observational, and think-aloud data was collected to determine whether designers favored levels suggested by the computational agents. This data was triangulated with qualitative data from semi-structured interviews that were used to gather overall opinions of the approach. The eye-tracking data indicates that the participating game level designers showed a clear preference for levels suggested by the computational agents, however, expert designers in particular appeared to reject the idea that the computational agents are helpful. The perception of computational tools not being useful needs to be addressed if procedural content generation approaches are to fulfill their potential for the game industry.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4225126461",
    "type": "article"
  },
  {
    "title": "A Spatial Constraint Model for Manipulating Static Visualizations",
    "doi": "https://doi.org/10.1145/3657642",
    "publication_date": "2024-04-11",
    "publication_year": 2024,
    "authors": "Can Liu; Yu Zhang; C.-M. Wu; Changlian Li; Xiaoru Yuan",
    "corresponding_authors": "",
    "abstract": "We introduce a spatial constraint model to characterize the positioning and interactions in visualizations, thereby facilitating the activation of static visualizations. Our model provides users with the capability to manipulate visualizations through operations such as selection, filtering, navigation, arrangement, and aggregation. Building upon this conceptual framework, we propose a prototype system designed to activate pre-existing visualizations by imbuing them with intelligent interactions. This augmentation is accomplished through the integration of visual objects with forces. The instantiation of our spatial constraint model enables seamless animated transitions between distinct visualization layouts. To demonstrate the efficacy of our approach, we present usage scenarios that involve the activation of visualizations within real-world contexts.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4394727834",
    "type": "article"
  },
  {
    "title": "Human Interaction with Artificial Advice Givers: no subtitle",
    "doi": null,
    "publication_date": "2016-12-31",
    "publication_year": 2016,
    "authors": "Nava Tintarev; John O’Donovan; Alexander Felfernig",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2971373952",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2799389",
    "publication_date": "2015-07-09",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "This editorial introduction describes the aims and scope of the special issue of the ACM Transactions on Interactive Intelligent Systems on Behavior Understanding for Arts and Entertainment, which is being published in issues 2 and 3 of volume 5 of the ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4230045736",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2997043",
    "publication_date": "2016-10-17",
    "publication_year": 2016,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Virtual environments offer an ideal setting to develop intelligent training applications. Yet, their ability to support complex procedures depends on the appropriate integration of knowledge-based techniques and natural interaction. In this article, we ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4235030610",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2974721",
    "publication_date": "2016-08-03",
    "publication_year": 2016,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Users’ online behaviors such as ratings and examination of items are recognized as one of the most valuable sources of information for learning users’ preferences in order to make personalized recommendations. But most previous works focus on modeling ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4236287963",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3015563",
    "publication_date": "2016-12-26",
    "publication_year": 2016,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Many interactive systems in today’s world can be viewed as providing advice to their users. Commercial examples include recommender systems, satellite navigation systems, intelligent personal assistants on smartphones, and automated checkout systems in ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4236682652",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2866565",
    "publication_date": "2016-01-07",
    "publication_year": 2016,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We propose a real-time system that continuously recognizes emotions from body movements. The combined low-level 3D postural features and high-level kinematic and geometrical features are fed to a Random Forests classifier through summarization (...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4242295146",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2896319",
    "publication_date": "2016-05-05",
    "publication_year": 2016,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Eye gaze has been used broadly in interactive intelligent systems. The research area has grown in recent years to cover emerging topics that go beyond the traditional focus on interaction between a single user and an interactive system. This special ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4243984506",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2567808",
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Indoor localization and navigation systems for individuals with Visual Impairments (VIs) typically rely upon extensive augmentation of the physical space, significant computational resources, or heavy and expensive sensors; thus, few systems have been ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4244286695",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2660857",
    "publication_date": "2014-11-21",
    "publication_year": 2014,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "This special issue highlights research articles that apply machine learning to robots and other systems that interact with users through more than one modality, such as speech, gestures, and vision. For example, a robot may coordinate its speech with ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4244632708",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2688469",
    "publication_date": "2015-01-28",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "This editorial introduction describes the aims and scope of the ACM Transactions on Interactive Intelligent Systems special issue on Activity Recognition for Interaction. It explains why activity recognition is becoming crucial as part of the cycle of ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4246118016",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2821459",
    "publication_date": "2015-10-16",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "This editorial introduction complements the shorter introduction to the first part of the two-part special issue on Behavior Understanding for Arts and Entertainment. It offers a more expansive discussion of the use of behavior analysis for interactive ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4250922097",
    "type": "paratext"
  },
  {
    "title": "AOI-shapes: An Efficient Footprint Algorithm to Support Visualization of User-defined Urban Areas of Interest",
    "doi": "https://doi.org/10.1145/3431817",
    "publication_date": "2021-09-03",
    "publication_year": 2021,
    "authors": "Mingzhao Li; Zhifeng Bao; Farhana Choudhury; Hanan Samet; Matt Duckham; Timos Sellis",
    "corresponding_authors": "",
    "abstract": "Understanding urban areas of interest (AOIs) is essential in many real-life scenarios, and such AOIs can be computed based on the geographic points that satisfy user queries. In this article, we study the problem of efficient and effective visualization of user-defined urban AOIs in an interactive manner. In particular, we first define the problem of user-defined AOI visualization based on a real estate data visualization scenario, and we illustrate why a novel footprint method is needed to support the visualization. After extensively reviewing existing “footprint” methods, we propose a parameter-free footprint method, named AOI-shapes, to capture the boundary information of a user-defined urban AOI. Next, to allow interactive query refinements by the user, we propose two efficient and scalable algorithms to incrementally generate urban AOIs by reusing existing visualization results. Finally, we conduct extensive experiments with both synthetic and real-world datasets to demonstrate the quality and efficiency of the proposed methods.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3196741562",
    "type": "article"
  },
  {
    "title": "Special Issue on Interactive Visual Analytics for Making Explainable and Accountable Decisions",
    "doi": "https://doi.org/10.1145/3471903",
    "publication_date": "2021-09-03",
    "publication_year": 2021,
    "authors": "Çağatay Turkay; Tatiana von Landesberger; Daniel Archambault; Shi‐Xia Liu; Remco Chang",
    "corresponding_authors": "",
    "abstract": "research-article Share on Special Issue on Interactive Visual Analytics for Making Explainable and Accountable Decisions Authors: Cagatay Turkay University of Warwick, Coventry, UK University of Warwick, Coventry, UKView Profile , Tatiana Von Landesberger University of Cologne and University of Rostock, Cologne, Germany University of Cologne and University of Rostock, Cologne, GermanyView Profile , Daniel Archambault Swansea University, Swansea, Wales, UK Swansea University, Swansea, Wales, UKView Profile , Shixia Liu Tsinghua University, Beijing, People’s Republic of China Tsinghua University, Beijing, People’s Republic of ChinaView Profile , Remco Chang Tufts University, Medford, USA Tufts University, Medford, USAView Profile Authors Info & Claims ACM Transactions on Interactive Intelligent SystemsVolume 11Issue 3-4December 2021 Article No.: 17pp 1–4https://doi.org/10.1145/3471903Online:03 September 2021Publication History 0citation187DownloadsMetricsTotal Citations0Total Downloads187Last 12 Months187Last 6 weeks20 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3197076550",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2533670",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "This recollection of John Riedl, founding coeditor-in-chief of the ACM Transactions on Interactive Intelligent Systems, presents a picture by editors of the journal of what it was like to collaborate and interact with him.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4239209513",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2030365",
    "publication_date": "2011-10-01",
    "publication_year": 2011,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "This editorial introduction describes the aims and scope of the ACM Transactions on Interactive Intelligent Systems, explains how it aims to constitute a landmark addition to the publication landscape, and shows how the five articles in this inaugural ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4246116550",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2499474",
    "publication_date": "2013-07-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Smart objects can be smart because of the information and communication technology that is added to human-made artifacts. It is not, however, the technology itself that makes them smart but rather the way in which the technology is integrated, and their ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4253909296",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2448116",
    "publication_date": "2013-04-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4256093773",
    "type": "paratext"
  },
  {
    "title": "Introduction to the special issue on highlights of the decade in interactive intelligent systems",
    "doi": "https://doi.org/10.1145/2395123.2395124",
    "publication_date": "2012-12-01",
    "publication_year": 2012,
    "authors": "Anthony Jameson; John Riedl",
    "corresponding_authors": "",
    "abstract": "This editorial introduction explains the motivation and origin of the TiiS special issue on Highlights of the Decade in Interactive Intelligent Systems and shows how its five articles exemplify the types of research contribution that TiiS aims to encourage and publish.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W1973324724",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2133366",
    "publication_date": "2012-03-01",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Affect-sensitive systems such as social robots and virtual agents are increasingly being investigated in real-world settings. In order to work effectively in natural environments, these systems require the ability to infer the affective and mental ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4242442804",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2395123",
    "publication_date": "2012-12-01",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "This editorial introduction explains the motivation and origin of the TiiS special issue on Highlights of the Decade in Interactive Intelligent Systems and shows how its five articles exemplify the types of research contribution that TiiS aims to ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4245705491",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2362394",
    "publication_date": "2012-09-01",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "This article introduces the tag genome, a data structure that extends the traditional tagging model to provide enhanced forms of user interaction. Just as a biological genome encodes an organism based on a sequence of genes, the tag genome encodes an ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4254029672",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3143523",
    "publication_date": "2017-10-09",
    "publication_year": 2017,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4233559967",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3028254",
    "publication_date": "2017-03-23",
    "publication_year": 2017,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Recommender systems form the backbone of many interactive systems. They incorporate user feedback to personalize the user experience typically via personalized recommendation lists. As users interact with a system, an increasing amount of data about a ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4235638774",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3129288",
    "publication_date": "2017-07-29",
    "publication_year": 2017,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "This brief introduction begins with an overview of the types of research that are relevant to the special issue on Big Personal Data in Interactive Intelligent Systems. The overarching question is: How can big personal data be collected, analyzed, and ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4239272063",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3166060",
    "publication_date": "2017-12-19",
    "publication_year": 2017,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "This article presents a novel smart eyewear that recognizes the wearer’s facial expressions in daily scenarios. Our device uses embedded photo-reflective sensors and machine learning to recognize the wearer’s facial expressions. Our approach focuses on ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4246555978",
    "type": "paratext"
  },
  {
    "title": "Towards Addressing Ambiguous Interactions and Inferring User Intent with Dimension Reduction and Clustering Combinations in Visual Analytics",
    "doi": "https://doi.org/10.1145/3588565",
    "publication_date": "2023-04-17",
    "publication_year": 2023,
    "authors": "John Wenskovitch; Michelle Dowling; Chris North",
    "corresponding_authors": "",
    "abstract": "Direct manipulation interactions on projections are often incorporated in visual analytics applications. These interactions enable analysts to provide incremental feedback to the system in a semi-supervised manner, demonstrating relationships that the analyst wishes to find within the data. However, determining the precise intent of the analyst is a challenge. When an analyst interacts with a projection, the inherent ambiguity of interactions can lead to a variety of possible interpretations that the system can infer. Previous work has demonstrated the utility of clusters as an interaction target to address this “With Respect to What” problem in dimension-reduced projections. However, the introduction of clusters introduces interaction inference challenges as well. In this work, we discuss the interaction space for the simultaneous use of semi-supervised dimension reduction and clustering algorithms. We introduce a novel pipeline representation to disambiguate between interactions on observations and clusters, as well as which underlying model is responding to those analyst interactions. We use a prototype visual analytics tool to demonstrate the effects of these ambiguous interactions, their properties, and the insights that an analyst can glean from each.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4366087604",
    "type": "article"
  },
  {
    "title": "Generalisable Dialogue-based Approach for Active Learning of Activities of Daily Living",
    "doi": "https://doi.org/10.1145/3616017",
    "publication_date": "2023-08-14",
    "publication_year": 2023,
    "authors": "Ronnie Smith; Mauro Dragone",
    "corresponding_authors": "",
    "abstract": "While Human Activity Recognition systems may benefit from Active Learning by allowing users to self-annotate their Activities of Daily Living (ADLs), many proposed methods for collecting such annotations are for short-term data collection campaigns for specific datasets. We present a reusable dialogue-based approach to user interaction for active learning in activity recognition systems, which utilises semantic similarity measures and a dataset of natural language descriptions of common activities (which we make publicly available). Our approach involves system-initiated dialogue, including follow-up questions to reduce ambiguity in user responses where appropriate. We apply this approach to two active learning scenarios: (i) using an existing CASAS dataset, demonstrating long-term usage; and (ii) using an online activity recognition system, which tackles the issue of online segmentation and labelling. We demonstrate our work in context, in which a natural language interface provides knowledge that can help interpret other multi-modal sensor data. We provide results highlighting the potential of our dialogue- and semantic similarity-based approach. We evaluate our work: (i) quantitatively, as an efficient way to seek users’ input for active learning of ADLs; and (ii) qualitatively, through a user study in which users were asked to compare our approach and an established method. Results show the potential of our approach as a hands-free interface for annotation of sensor data as part of an active learning system. We provide insights into the challenges of active learning for activity recognition under real-world conditions and identify potential ways to address them.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4385806844",
    "type": "article"
  },
  {
    "title": "2022 TiiS Best Paper Announcement",
    "doi": "https://doi.org/10.1145/3615590",
    "publication_date": "2023-09-11",
    "publication_year": 2023,
    "authors": "Michelle X. Zhou; Shlomo Berkovsky",
    "corresponding_authors": "",
    "abstract": "The IEEE TRANSACTIONS ON SIGNAL PROCESSING is fortunate to attract submissions of the highest quality and to publish articles that deal with topics that are at the forefront of what is happening in the field of signal processing and its adjacent areas. ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4386607431",
    "type": "article"
  },
  {
    "title": "ACM Transactions on Interactive Intelligent Systems (TiiS) Special Issue on Trust and Influence in Intelligent Human-Machine Interaction",
    "doi": "https://doi.org/10.1145/3281451",
    "publication_date": "2018-11-16",
    "publication_year": 2018,
    "authors": "Benjamin A. Knott; Jonathan Gratch; Angelo Cangelosi; James Caverlee",
    "corresponding_authors": "",
    "abstract": "research-article Share on ACM Transactions on Interactive Intelligent Systems (TiiS) Special Issue on Trust and Influence in Intelligent Human-Machine Interaction Authors: Benjamin A. Knott The Office of Naval Research Global, Roppongi, Tokyo, Japan The Office of Naval Research Global, Roppongi, Tokyo, JapanView Profile , Jonathan Gratch University of Southern California, USA University of Southern California, USAView Profile , Angelo Cangelosi Plymouth University, USA Plymouth University, USAView Profile , James Caverlee Texas A8M University, USA Texas A8M University, USAView Profile Authors Info & Claims ACM Transactions on Interactive Intelligent SystemsVolume 8Issue 4December 2018 Article No.: 25pp 1–3https://doi.org/10.1145/3281451Published:16 November 2018Publication History 0citation395DownloadsMetricsTotal Citations0Total Downloads395Last 12 Months62Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2901751437",
    "type": "article"
  },
  {
    "title": "Special Issue on Highlights of ACM Intelligent User Interface (IUI) 2017",
    "doi": "https://doi.org/10.1145/3301292",
    "publication_date": "2019-04-18",
    "publication_year": 2019,
    "authors": "Fang Chen; Carlos Duarte; Wai‐Tat Fu",
    "corresponding_authors": "",
    "abstract": "research-article Share on Special Issue on Highlights of ACM Intelligent User Interface (IUI) 2017 Authors: Fang Chen University of Technology Sydney, NSW, Australia University of Technology Sydney, NSW, AustraliaView Profile , Carlos Duarte Universidade de Lisboa, Lisboa, Portugal Universidade de Lisboa, Lisboa, PortugalView Profile , Wai-Tat Fu University of Illinois at Urbana-Champaign, IL, USA University of Illinois at Urbana-Champaign, IL, USAView Profile Authors Info & Claims ACM Transactions on Interactive Intelligent SystemsVolume 9Issue 2-3September 2019 Article No.: 7pp 1–3https://doi.org/10.1145/3301292Published:18 April 2019Publication History 2citation163DownloadsMetricsTotal Citations2Total Downloads163Last 12 Months12Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2939789290",
    "type": "article"
  },
  {
    "title": "Special Issue on Highlights of ACM Intelligent User Interface (IUI) 2018",
    "doi": "https://doi.org/10.1145/3357206",
    "publication_date": "2019-10-12",
    "publication_year": 2019,
    "authors": "Mark Billinghurst; Margaret Burnett; Aaron Quigley",
    "corresponding_authors": "",
    "abstract": "research-article Share on Special Issue on Highlights of ACM Intelligent User Interface (IUI) 2018 Authors: Mark Billinghurst School of ITMS, University of South Australia, Adelaide, South Australia, Australia School of ITMS, University of South Australia, Adelaide, South Australia, AustraliaView Profile , Margaret Burnett School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, Oregon, USA School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, Oregon, USAView Profile , Aaron Quigley School of Computer Science, University of St. Andrews, St. Andrews, Scotland, United Kingdom School of Computer Science, University of St. Andrews, St. Andrews, Scotland, United KingdomView Profile Authors Info & Claims ACM Transactions on Interactive Intelligent SystemsVolume 10Issue 1March 2020 Article No.: 1pp 1–3https://doi.org/10.1145/3357206Published:12 October 2019Publication History 0citation192DownloadsMetricsTotal Citations0Total Downloads192Last 12 Months27Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3034644144",
    "type": "article"
  },
  {
    "title": "Distinguished Reviewers",
    "doi": "https://doi.org/10.1145/3283374",
    "publication_date": "2018-11-16",
    "publication_year": 2018,
    "authors": "Daniel Afergan",
    "corresponding_authors": "Daniel Afergan",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3127705785",
    "type": "article"
  },
  {
    "title": "コンピュータビジョンとフォローアップ質問によるHILCドメイン独立PBDシステム【JST・京大機械翻訳】",
    "doi": null,
    "publication_date": "2019-01-01",
    "publication_year": 2019,
    "authors": "Intharah Thanapong; Turmukhambetov Daniyar; J Brostow Gabriel",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3163942831",
    "type": "article"
  },
  {
    "title": "動的手書き信号特徴はドメイン専門知識を予測する【JST・京大機械翻訳】",
    "doi": null,
    "publication_date": "2018-01-01",
    "publication_year": 2018,
    "authors": "S Oviatt; K Hang; J Zhou; K Yu; F Chen",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3175562994",
    "type": "article"
  },
  {
    "title": "手首帯域時系列からの摂食行動同定のためのユーザ適応モデリング【JST・京大機械翻訳】",
    "doi": null,
    "publication_date": "2019-01-01",
    "publication_year": 2019,
    "authors": "Lee Jung-hyo; Prajwal Paudyal; Ayan Banerjee; K S Gupta Sandeep",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3187887546",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3236465",
    "publication_date": "2018-08-08",
    "publication_year": 2018,
    "authors": "H Sajjad Hossain; Sreenivasan Ramasamy Ramamurthy; Abdullah Al Hafiz Khan; Nirmalya Roy; Hamid Hossain; H Sajjad",
    "corresponding_authors": "Nirmalya Roy",
    "abstract": "This article reports on the development of capabilities for (on-screen) virtual agents and robots to support isolated older adults in their homes. A real-time architecture was developed to use a virtual agent or a robot interchangeably to interact via ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4230946804",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3292532",
    "publication_date": "2018-11-22",
    "publication_year": 2018,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "This article presents a conceptual framework for human-robot trust which uses computational representations inspired by game theory to represent a definition of trust, derived from social psychology. This conceptual framework generates several testable ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4233809761",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3351880",
    "publication_date": "2019-12-05",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Most of the existing recommender systems use the ratings provided by users on individual items. An additional source of preference information is to use the ratings that users provide on sets of items. The advantages of using preferences on sets are ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4240281268",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3185338",
    "publication_date": "2018-03-13",
    "publication_year": 2018,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The analysis of human behaviors has impacted many social and commercial domains. How could interactive visual analytic systems be used to further provide behavioral insights? This editorial introduction features emerging research trend related to this ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4241262884",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3352585",
    "publication_date": "2019-10-12",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4248415519",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3312745",
    "publication_date": "2019-03-01",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Full-body human movement is characterized by fine-grain expressive qualities that humans are easily capable of exhibiting and recognizing in others’ movement. In sports (e.g., martial arts) and performing arts (e.g., dance), the same sequence of ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4254803362",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3320251",
    "publication_date": "2019-04-25",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Understanding a target audience's emotional responses to a video advertisement is crucial to evaluate the advertisement's effectiveness. However, traditional methods for collecting such information are slow, expensive, and coarse grained. We propose ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4255229919",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3232718",
    "publication_date": "2018-07-14",
    "publication_year": 2018,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Machine learning is one of the most important and successful techniques in contemporary computer science. Although it can be applied to myriad problems of human interest, research in machine learning is often framed in an impersonal way, as merely ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4255783643",
    "type": "paratext"
  },
  {
    "title": "Introduction to the Special Issue on Highlights of ACM Intelligent User Interface (IUI) 2019",
    "doi": "https://doi.org/10.1145/3429946",
    "publication_date": "2020-12-02",
    "publication_year": 2020,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3108664361",
    "type": "article"
  },
  {
    "title": "プログレッシブ・ディスクロージャー,なぜ,そしてユーザはアルゴリズム的透明度情報を待つか?【JST・京大機械翻訳】",
    "doi": null,
    "publication_date": "2020-01-01",
    "publication_year": 2020,
    "authors": "Springer Aaron; Whittaker Steve",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3204932627",
    "type": "article"
  },
  {
    "title": "Editorial Introduction to TiiS Special Category Article: Practitioners’ Toolbox",
    "doi": "https://doi.org/10.1145/3519381",
    "publication_date": "2022-03-24",
    "publication_year": 2022,
    "authors": "Michelle X. Zhou",
    "corresponding_authors": "Michelle X. Zhou",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4220815121",
    "type": "article"
  },
  {
    "title": "GO-Finder: A Registration-free Wearable System for Assisting Users in Finding Lost Hand-held Objects",
    "doi": "https://doi.org/10.1145/3519268",
    "publication_date": "2022-04-28",
    "publication_year": 2022,
    "authors": "Takuma Yagi; Takumi Nishiyasu; Kunimasa Kawasaki; Moe Matsuki; Yoichi Sato",
    "corresponding_authors": "",
    "abstract": "People spend an enormous amount of time and effort looking for lost objects. To help remind people of the location of lost objects, various computational systems that provide information on their locations have been developed. However, prior systems for assisting people in finding objects require users to register the target objects in advance. This requirement imposes a cumbersome burden on the users, and the system cannot help remind them of unexpectedly lost objects. We propose GO-Finder (“Generic Object Finder”), a registration-free wearable camera-based system for assisting people in finding an arbitrary number of objects based on two key features: automatic discovery of hand-held objects and image-based candidate selection. Given a video taken from a wearable camera, GO-Finder automatically detects and groups hand-held objects to form a visual timeline of the objects. Users can retrieve the last appearance of the object by browsing the timeline through a smartphone app. We conducted user studies to investigate how users benefit from using GO-Finder. In the first study, we asked participants to perform an object retrieval task and confirmed improved accuracy and reduced mental load in the object search task by providing clear visual cues on object locations. In the second study, the system’s usability on a longer and more realistic scenario was verified, accompanied by an additional feature of context-based candidate filtering. Participant feedback suggested the usefulness of GO-Finder also in realistic scenarios where more than one hundred objects appear.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4293236945",
    "type": "article"
  },
  {
    "title": "Special Issue on Highlights of IUI 2021: Introduction",
    "doi": "https://doi.org/10.1145/3561516",
    "publication_date": "2022-12-31",
    "publication_year": 2022,
    "authors": "Tracy Hammond; Bart P. Knijnenburg; John O’Donovan; Paul Taele",
    "corresponding_authors": "",
    "abstract": "introduction Share on Special Issue on Highlights of IUI 2021: Introduction Authors: Tracy Hammond Texas A&M University, College Station, Texas, United States Texas A&M University, College Station, Texas, United StatesView Profile , Bart Knijnenburg Clemson University, Clemson, South Carolina, United States Clemson University, Clemson, South Carolina, United StatesView Profile , John O’Donovan University of California, Santa Barbara, California, United States University of California, Santa Barbara, California, United StatesView Profile , Paul Taele Texas A&M University, College Station, Texas, United States Texas A&M University, College Station, Texas, United StatesView Profile Authors Info & Claims ACM Transactions on Interactive Intelligent SystemsVolume 12Issue 403 February 2023Article No.: 25pp 1–4https://doi.org/10.1145/3561516Published:03 February 2023Publication History 0citation47DownloadsMetricsTotal Citations0Total Downloads47Last 12 Months47Last 6 weeks9 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4320018857",
    "type": "article"
  },
  {
    "title": "BONNIE: Building Online Narratives from Noteworthy Interaction Events",
    "doi": "https://doi.org/10.1145/3423048",
    "publication_date": "2021-09-03",
    "publication_year": 2021,
    "authors": "Vinícius Segura; Simone Diniz Junqueira Barbosa",
    "corresponding_authors": "",
    "abstract": "Nowadays, we have access to data of unprecedented volume, high dimensionality, and complexity. To extract novel insights from such complex and dynamic data, we need effective and efficient strategies. One such strategy is to combine data analysis and visualization techniques, which are the essence of visual analytics applications. After the knowledge discovery process, a major challenge is to filter the essential information that has led to a discovery and to communicate the findings to other people, explaining the decisions they may have made based on the data. We propose to record and use the trace left by the exploratory data analysis, in the form of user interaction history, to aid this process. With the trace, users can choose the desired interaction steps and create a narrative, sharing the acquired knowledge with readers. To achieve our goal, we have developed the BONNIE ( Building Online Narratives from Noteworthy Interaction Events ) framework. BONNIE comprises a log model to register the interaction events, auxiliary code to help developers instrument their own code, and an environment to view users’ own interaction history and build narratives. This article presents our proposal for communicating discoveries in visual analytics applications, the BONNIE framework, and the studies we conducted to evaluate our solution. After two user studies (the first one focused on history visualization and the second one focused on narrative creation), our solution has showed to be promising, with mostly positive feedback and results from a Technology Acceptance Model ( TAM ) questionnaire.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3197156282",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Column for Human-Centered Artificial Intelligence",
    "doi": "https://doi.org/10.1145/3490553",
    "publication_date": "2021-10-25",
    "publication_year": 2021,
    "authors": "Michelle X. Zhou",
    "corresponding_authors": "Michelle X. Zhou",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3210163360",
    "type": "article"
  }
]