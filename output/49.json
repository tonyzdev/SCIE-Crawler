[
  {
    "title": "VTR 7.0",
    "doi": "https://doi.org/10.1145/2617593",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "Jason Luu; Jeffrey Goeders; Michael Wainberg; Andrew Somerville; Thien Yu; Konstantin Nasartschuk; Miad Nasr; Sen Wang; Tim Liu; Nooruddin Ahmed; Kenneth B. Kent; Jason H. Anderson; Jonathan Rose; Vaughn Betz",
    "corresponding_authors": "",
    "abstract": "Exploring architectures for large, modern FPGAs requires sophisticated software that can model and target hypothetical devices. Furthermore, research into new CAD algorithms often requires a complete and open source baseline CAD flow. This article describes recent advances in the open source Verilog-to-Routing (VTR) CAD flow that enable further research in these areas. VTR now supports designs with multiple clocks in both timing analysis and optimization. Hard adder/carry logic can be included in an architecture in various ways and significantly improves the performance of arithmetic circuits. The flow now models energy consumption, an increasingly important concern. The speed and quality of the packing algorithms have been significantly improved. VTR can now generate a netlist of the final post-routed circuit which enables detailed simulation of a design for a variety of purposes. We also release new FPGA architecture files and models that are much closer to modern commercial architectures, enabling more realistic experiments. Finally, we show that while this version of VTR supports new and complex features, it has a 1.5× compile time speed-up for simple architectures and a 6× speed-up for complex architectures compared to the previous release, with no degradation to timing or wire-length quality.",
    "cited_by_count": 340,
    "openalex_id": "https://openalex.org/W2005602803",
    "type": "article"
  },
  {
    "title": "FINN- <i>R</i>",
    "doi": "https://doi.org/10.1145/3242897",
    "publication_date": "2018-09-30",
    "publication_year": 2018,
    "authors": "Michaela Blott; Thomas B. Preußer; Nicholas J. Fraser; Giulio Gambardella; Kenneth M. O'Brien; Yaman Umuroglu; Miriam Leeser; Kees Vissers",
    "corresponding_authors": "",
    "abstract": "Convolutional Neural Networks have rapidly become the most successful machine-learning algorithm, enabling ubiquitous machine vision and intelligent decisions on even embedded computing systems. While the underlying arithmetic is structurally simple, compute and memory requirements are challenging. One of the promising opportunities is leveraging reduced-precision representations for inputs, activations, and model parameters. The resulting scalability in performance, power efficiency, and storage footprint provides interesting design compromises in exchange for a small reduction in accuracy. FPGAs are ideal for exploiting low-precision inference engines leveraging custom precisions to achieve the required numerical accuracy for a given application. In this article, we describe the second generation of the FINN framework, an end-to-end tool that enables design-space exploration and automates the creation of fully customized inference engines on FPGAs. Given a neural network description, the tool optimizes for given platforms, design targets, and a specific precision. We introduce formalizations of resource cost functions and performance predictions and elaborate on the optimization algorithms. Finally, we evaluate a selection of reduced precision neural networks ranging from CIFAR-10 classifiers to YOLO-based object detection on a range of platforms including PYNQ and AWS F1, demonstrating new unprecedented measured throughput at 50 TOp/s on AWS F1 and 5 TOp/s on embedded devices.",
    "cited_by_count": 319,
    "openalex_id": "https://openalex.org/W2891946740",
    "type": "article"
  },
  {
    "title": "VTR 8",
    "doi": "https://doi.org/10.1145/3388617",
    "publication_date": "2020-06-01",
    "publication_year": 2020,
    "authors": "Kevin E. Murray; Oleg Petelin; Sheng Zhong; Jia Min Wang; Mohamed Eldafrawy; Jean-Philippe Legault; Eugene Sha; Aaron G. Graham; Jean Wu; Matthew Walker; Hanqing Zeng; Panagiotis Patros; Jason Luu; Kenneth B. Kent; Vaughn Betz",
    "corresponding_authors": "",
    "abstract": "Developing Field-programmable Gate Array (FPGA) architectures is challenging due to the competing requirements of various application domains and changing manufacturing process technology. This is compounded by the difficulty of fairly evaluating FPGA architectural choices, which requires sophisticated high-quality Computer Aided Design (CAD) tools to target each potential architecture. This article describes version 8.0 of the open source Verilog to Routing (VTR) project, which provides such a design flow. VTR 8 expands the scope of FPGA architectures that can be modelled, allowing VTR to target and model many details of both commercial and proposed FPGA architectures. The VTR design flow also serves as a baseline for evaluating new CAD algorithms. It is therefore important, for both CAD algorithm comparisons and the validity of architectural conclusions, that VTR produce high-quality circuit implementations. VTR 8 significantly improves optimization quality (reductions of 15% minimum routable channel width, 41% wirelength, and 12% critical path delay), run-time (5.3× faster) and memory footprint (3.3× lower). Finally, we demonstrate VTR is run-time and memory footprint efficient, while producing circuit implementations of reasonable quality compared to highly-tuned architecture-specific industrial tools—showing that architecture generality, good implementation quality, and run-time efficiency are not mutually exclusive goals.",
    "cited_by_count": 234,
    "openalex_id": "https://openalex.org/W3033033241",
    "type": "article"
  },
  {
    "title": "[DL] A Survey of FPGA-based Neural Network Inference Accelerators",
    "doi": "https://doi.org/10.1145/3289185",
    "publication_date": "2019-03-28",
    "publication_year": 2019,
    "authors": "Kaiyuan Guo; Shulin Zeng; Jincheng Yu; Yu Wang; Huazhong Yang",
    "corresponding_authors": "",
    "abstract": "Recent research on neural networks has shown a significant advantage in machine learning over traditional algorithms based on handcrafted features and models. Neural networks are now widely adopted in regions like image, speech, and video recognition. But the high computation and storage complexity of neural network inference poses great difficulty on its application. It is difficult for CPU platforms to offer enough computation capacity. GPU platforms are the first choice for neural network processes because of its high computation capacity and easy-to-use development frameworks. However, FPGA-based neural network inference accelerator is becoming a research topic. With specifically designed hardware, FPGA is the next possible solution to surpass GPU in speed and energy efficiency. Various FPGA-based accelerator designs have been proposed with software and hardware optimization techniques to achieve high speed and energy efficiency. In this article, we give an overview of previous work on neural network inference accelerators based on FPGA and summarize the main techniques used. An investigation from software to hardware, from circuit level to system level is carried out to complete analysis of FPGA-based neural network inference accelerator design and serves as a guide to future work.",
    "cited_by_count": 224,
    "openalex_id": "https://openalex.org/W2997106510",
    "type": "article"
  },
  {
    "title": "FPGA HLS Today: Successes, Challenges, and Opportunities",
    "doi": "https://doi.org/10.1145/3530775",
    "publication_date": "2022-04-21",
    "publication_year": 2022,
    "authors": "Jason Cong; Jason Lau; Gai Liu; Stephen Neuendorffer; Peichen Pan; Kees Vissers; Zhiru Zhang",
    "corresponding_authors": "",
    "abstract": "The year 2011 marked an important transition for FPGA high-level synthesis (HLS), as it went from prototyping to deployment. A decade later, in this article, we assess the progress of the deployment of HLS technology and highlight the successes in several application domains, including deep learning, video transcoding, graph processing, and genome sequencing. We also discuss the challenges faced by today’s HLS technology and the opportunities for further research and development, especially in the areas of achieving high clock frequency, coping with complex pragmas and system integration, legacy code transformation, building on open source HLS infrastructures, supporting domain-specific languages, and standardization. It is our hope that this article will inspire more research on FPGA HLS and bring it to a new height.",
    "cited_by_count": 109,
    "openalex_id": "https://openalex.org/W4224265992",
    "type": "article"
  },
  {
    "title": "The Future of FPGA Acceleration in Datacenters and the Cloud",
    "doi": "https://doi.org/10.1145/3506713",
    "publication_date": "2022-02-04",
    "publication_year": 2022,
    "authors": "Christophe Bobda; Joel Mandebi Mbongue; Paul Chow; Mohammad Ewais; Naif Tarafdar; Juan Camilo Vega; Ken Eguro; Dirk Koch; Suranga Handagala; Miriam Leeser; Martin Herbordt; Hafsah Shahzad; Peter Hofste; Burkhard Ringlein; Jakub Szefer; Ahmed Sanaullah; Russell Tessier",
    "corresponding_authors": "",
    "abstract": "In this article, we survey existing academic and commercial efforts to provide Field-Programmable Gate Array (FPGA) acceleration in datacenters and the cloud. The goal is a critical review of existing systems and a discussion of their evolution from single workstations with PCI-attached FPGAs in the early days of reconfigurable computing to the integration of FPGA farms in large-scale computing infrastructures. From the lessons learned, we discuss the future of FPGAs in datacenters and the cloud and assess the challenges likely to be encountered along the way. The article explores current architectures and discusses scalability and abstractions supported by operating systems, middleware, and virtualization. Hardware and software security becomes critical when infrastructure is shared among tenants with disparate backgrounds. We review the vulnerabilities of current systems and possible attack scenarios and discuss mitigation strategies, some of which impact FPGA architecture and technology. The viability of these architectures for popular applications is reviewed, with a particular focus on deep learning and scientific computing. This work draws from workshop discussions, panel sessions including the participation of experts in the reconfigurable computing field, and private discussions among these experts. These interactions have harmonized the terminology, taxonomy, and the important topics covered in this manuscript.",
    "cited_by_count": 100,
    "openalex_id": "https://openalex.org/W4210258659",
    "type": "article"
  },
  {
    "title": "Techniques for Design and Implementation of Secure Reconfigurable PUFs",
    "doi": "https://doi.org/10.1145/1502781.1502786",
    "publication_date": "2009-03-01",
    "publication_year": 2009,
    "authors": "Mehrdad Majzoobi; Farinaz Koushanfar; Miodrag Potkonjak",
    "corresponding_authors": "",
    "abstract": "Physically unclonable functions (PUFs) provide a basis for many security and digital rights management protocols. PUF-based security approaches have numerous comparative strengths with respect to traditional cryptography-based techniques, including resilience against physical and side channel attacks and suitability for lightweight protocols. However, classical delay-based PUF structures have a number of drawbacks including susceptibility to guessing, reverse engineering, and emulation attacks, as well as sensitivity to operational and environmental variations. To address these limitations, we have developed a new set of techniques for FPGA-based PUF design and implementation. We demonstrate how reconfigurability can be exploited to eliminate the stated PUF limitations. We also show how FPGA-based PUFs can be used for privacy protection. Furthermore, reconfigurability enables the introduction of new techniques for PUF testing. The effectiveness of all the proposed techniques is validated using extensive implementations, simulations, and statistical analysis.",
    "cited_by_count": 204,
    "openalex_id": "https://openalex.org/W2070196900",
    "type": "article"
  },
  {
    "title": "RIFFA 2.1",
    "doi": "https://doi.org/10.1145/2815631",
    "publication_date": "2015-09-13",
    "publication_year": 2015,
    "authors": "Matthew Jacobsen; Dustin Richmond; Matthew Hogains; Ryan Kastner",
    "corresponding_authors": "",
    "abstract": "We present RIFFA 2.1, a reusable integration framework for Field-Programmable Gate Array (FPGA) accelerators. RIFFA provides communication and synchronization for FPGA accelerated applications using simple interfaces for hardware and software. Our goal is to expand the use of FPGAs as an acceleration platform by releasing, as open source, a framework that easily integrates software running on commodity CPUs with FPGA cores. RIFFA uses PCI Express (PCIe) links to connect FPGAs to a CPU’s system bus. RIFFA 2.1 supports FPGAs from Xilinx and Altera, Linux and Windows operating systems, and allows multiple FPGAs to connect to a single host PC system. It has software bindings for C/C++, Java, Python, and Matlab. Tests show that data transfers between hardware and software can reach 97% of the achievable PCIe link bandwidth.",
    "cited_by_count": 126,
    "openalex_id": "https://openalex.org/W2125703639",
    "type": "article"
  },
  {
    "title": "Performance of partial reconfiguration in FPGA systems",
    "doi": "https://doi.org/10.1145/2068716.2068722",
    "publication_date": "2011-12-01",
    "publication_year": 2011,
    "authors": "Kyprianos Papadimitriou; Apostolos Dollas; Scott Hauck",
    "corresponding_authors": "",
    "abstract": "Fine-grain reconfigurable devices suffer from the time needed to load the configuration bitstream. Even for small bitstreams in partially reconfigurable FPGAs this time cannot be neglected. In this article we survey the performance of the factors that contribute to the reconfiguration speed. Then, we study an FPGA-based system architecture and with real experiments we produce a cost model of Partial Reconfiguration (PR). This model is introduced to calculate the expected reconfiguration time and throughput. In order to develop a realistic model we take into account all the physical components that participate in the reconfiguration process. We analyze the parameters that affect the generality of the model and the adjustments needed per system for error-free evaluation. We verify it with real measurements, and then we employ it to evaluate existing systems presented in previous publications. The percentage error of the cost model when comparing its results with the actual values of those publications varies from 36% to 63%, whereas existing works report differences up to two orders of magnitude. Present work enables a user to evaluate PR and decide whether it is suitable for a certain application prior entering the complex PR design flow.",
    "cited_by_count": 119,
    "openalex_id": "https://openalex.org/W2081381252",
    "type": "article"
  },
  {
    "title": "Throughput-Optimized FPGA Accelerator for Deep Convolutional Neural Networks",
    "doi": "https://doi.org/10.1145/3079758",
    "publication_date": "2017-07-19",
    "publication_year": 2017,
    "authors": "Zhi-Qiang Liu; Yong Dou; Jingfei Jiang; Jinwei Xu; Shijie Li; Yongmei Zhou; Yingnan Xu",
    "corresponding_authors": "",
    "abstract": "Deep convolutional neural networks (CNNs) have gained great success in various computer vision applications. State-of-the-art CNN models for large-scale applications are computation intensive and memory expensive and, hence, are mainly processed on high-performance processors like server CPUs and GPUs. However, there is an increasing demand of high-accuracy or real-time object detection tasks in large-scale clusters or embedded systems, which requires energy-efficient accelerators because of the green computation requirement or the limited battery restriction. Due to the advantages of energy efficiency and reconfigurability, Field-Programmable Gate Arrays (FPGAs) have been widely explored as CNN accelerators. In this article, we present an in-depth analysis of computation complexity and the memory footprint of each CNN layer type. Then a scalable parallel framework is proposed that exploits four levels of parallelism in hardware acceleration. We further put forward a systematic design space exploration methodology to search for the optimal solution that maximizes accelerator throughput under the FPGA constraints such as on-chip memory, computational resources, external memory bandwidth, and clock frequency. Finally, we demonstrate the methodology by optimizing three representative CNNs (LeNet, AlexNet, and VGG-S) on a Xilinx VC709 board. The average performance of the three accelerators is 424.7, 445.6, and 473.4GOP/s under 100MHz working frequency, which outperforms the CPU and previous work significantly.",
    "cited_by_count": 100,
    "openalex_id": "https://openalex.org/W2737762472",
    "type": "article"
  },
  {
    "title": "Timing-Driven Titan",
    "doi": "https://doi.org/10.1145/2629579",
    "publication_date": "2015-03-25",
    "publication_year": 2015,
    "authors": "Kevin E. Murray; Scott Whitty; Suya Liu; Jason Luu; Vaughn Betz",
    "corresponding_authors": "",
    "abstract": "Benchmarks play a key role in Field-Programmable Gate Array (FPGA) architecture and CAD research, enabling the quantitative comparison of tools and architectures. It is important that these benchmarks reflect modern large-scale systems that make use of heterogeneous resources; however, most current FPGA benchmarks are both small and simple. In this artile, we present Titan, a hybrid CAD flow that addresses these issues. The flow uses Altera’s Quartus II FPGA CAD software to perform HDL synthesis and a conversion tool to translate the result into the academic Berkeley Logic Interchange Format (BLIF). Using this flow, we created the Titan23 benchmark set, which consists of 23 large (90K--1.8M block) benchmark circuits covering a wide range of application domains. Using the Titan23 benchmarks and an enhanced model of Altera’s Stratix IV architecture, including a detailed timing model, we compare the performance and quality of VPR and Quartus II targeting the same architecture. We found that VPR is at least 2.8 × slower, uses 6.2 × more memory, 2.2 × more wire, and produces critical paths 1.5 × slower compared to Quartus II. Finally, we identified that VPR’s focus on achieving a dense packing and an inability to take apart clusters is responsible for a large portion of the wire length and critical path delay gap.",
    "cited_by_count": 94,
    "openalex_id": "https://openalex.org/W2023428606",
    "type": "article"
  },
  {
    "title": "FPGAD <scp>efender</scp>",
    "doi": "https://doi.org/10.1145/3402937",
    "publication_date": "2020-09-01",
    "publication_year": 2020,
    "authors": "Tuan La; Kaspar Matas; Nikola Grunchevski; Khoa Dang Pham; Dirk Koch",
    "corresponding_authors": "",
    "abstract": "Sharing configuration bitstreams rather than netlists is a very desirable feature to protect IP or to share IP without longer CAD tool processing times. Furthermore, an increasing number of systems could hugely benefit from serving multiple users on the same FPGA, for example, for resource pooling in cloud infrastructures. This article researches the threat that a malicious application can impose on an FPGA-based system in a multi-tenancy scenario from a hardware security point of view. In particular, this article evaluates the risk systematically for FPGA power-hammering through short-circuits and self-oscillating circuits, which potentially may cause harm to a system. This risk includes implementing, tuning, and evaluating all FPGA self-oscillators known from the literature but also developing a large number of new power-hammering designs that have not been considered before. Our experiments demonstrate that malicious circuits can be tuned to the point that just 3% of the logic available on an Ultra96 FPGA board can draw the power budget of the entire FPGA board. This fact suggests a waste power potential for datacenter FPGAs in the range of kilowatts. In addition to carefully analyzing FPGA hardware security threats, we present the FPGA virus scanner FPGAD efender , which can detect (possibly) any self-oscillating FPGA circuit, as well as detecting short-circuits, high fanout nets, and a tapping onto signals outside the scope of a module for protecting data center FPGAs, such as Xilinx UltraScale+ devices at the bitstream level.",
    "cited_by_count": 80,
    "openalex_id": "https://openalex.org/W3036146501",
    "type": "article"
  },
  {
    "title": "Mercury BLASTP",
    "doi": "https://doi.org/10.1145/1371579.1371581",
    "publication_date": "2008-06-01",
    "publication_year": 2008,
    "authors": "Arpith C. Jacob; Joseph M. Lancaster; Jeremy Buhler; Brandon Harris; Roger D. Chamberlain",
    "corresponding_authors": "",
    "abstract": "Large-scale protein sequence comparison is an important but compute-intensive task in molecular biology. BLASTP is the most popular tool for comparative analysis of protein sequences. In recent years, an exponential increase in the size of protein sequence databases has required either exponentially more running time or a cluster of machines to keep pace. To address this problem, we have designed and built a high-performance FPGA-accelerated version of BLASTP, Mercury BLASTP. In this paper, we describe the architecture of the portions of the application that are accelerated in the FPGA, and we also describe the integration of these FPGA-accelerated portions with the existing BLASTP software. We have implemented Mercury BLASTP on a commodity workstation with two Xilinx Virtex-II 6000 FPGAs. We show that the new design runs 11-15 times faster than software BLASTP on a modern CPU while delivering close to 99% identical results.",
    "cited_by_count": 99,
    "openalex_id": "https://openalex.org/W1978885894",
    "type": "article"
  },
  {
    "title": "The Instruction-Set Extension Problem",
    "doi": "https://doi.org/10.1145/1968502.1968509",
    "publication_date": "2011-05-01",
    "publication_year": 2011,
    "authors": "Carlo Galuzzi; Koen Bertels",
    "corresponding_authors": "",
    "abstract": "The extension of a given instruction-set with specialized instructions has become a common technique used to speed up the execution of applications. By identifying computationally intensive portions of an application to be partitioned in segments of code to execute in software and segments of code to execute in hardware, the execution of an application can be considerably speeded up. Each segment of code implemented in hardware can then be seen as a specialized application-specific instruction extending a given instruction-set. Although a number of approaches exist in literature proposing different methodologies to customize an instruction-set, the description of the problem consists only of sporadic comparisons limited to isolated problems. This survey presents a unique detailed description of the problem and provides an exhaustive overview of the research in the past years in instruction-set extension. This article presents a thorough analysis of the issues involved during the customization of an instruction-set by means of a set of specialized application-specific instructions. The investigation of the problem covers both instruction generation and instruction selection and different kinds of customizations are analyzed in a great detail.",
    "cited_by_count": 96,
    "openalex_id": "https://openalex.org/W2153798176",
    "type": "article"
  },
  {
    "title": "NCBI BLASTP on High-Performance Reconfigurable Computing Systems",
    "doi": "https://doi.org/10.1145/2629691",
    "publication_date": "2015-01-23",
    "publication_year": 2015,
    "authors": "Atabak Mahram; Martin Herbordt",
    "corresponding_authors": "",
    "abstract": "The BLAST sequence alignment program is a central application in bioinformatics. The de facto standard version, NCBI BLAST, uses complex heuristics that make it challenging to simultaneously achieve both high performance and exact agreement. We propose a system that uses novel FPGA-based filters that reduce the input database by over 99.97% without loss of sensitivity. There are several contributions. First is design of the filters themselves, which perform two-hit seeding, exhaustive ungapped alignment, and exhaustive gapped alignments, respectively. Second is the coupling of the filters, especially the two-hit seeding and the ungapped alignment. Third is pipelining the filters in a single design, including maintaining load balancing as data are reduced by orders of magnitude at each stage. Fourth is the optimization required to maintain operating frequency for the resulting complex design. And finally, there is system integration both in hardware (the Convey HC1-EX) and software (NCBI BLASTP). We present results for various usage scenarios and find complete agreement and a factor of nearly 5x speedup over a fully parallel implementation of the reference code on a contemporaneous CPU. We believe that the resulting system is the leading per-socket-accelerated NCBI BLAST.",
    "cited_by_count": 88,
    "openalex_id": "https://openalex.org/W2083564022",
    "type": "article"
  },
  {
    "title": "Scalable don't-care-based logic optimization and resynthesis",
    "doi": "https://doi.org/10.1145/2068716.2068720",
    "publication_date": "2011-12-01",
    "publication_year": 2011,
    "authors": "Alan Mishchenko; Robert K. Brayton; Jie-Hong R. Jiang; Stephen Jang",
    "corresponding_authors": "",
    "abstract": "We describe an optimization method for combinational and sequential logic networks, with emphasis on scalability. The proposed resynthesis (a) is capable of substantial logic restructuring, (b) is customizable to solve a variety of optimization tasks, and (c) has reasonable runtime on industrial designs. The approach uses don't-cares computed for a window surrounding a node and can take into account external don't-cares (e.g., unreachable states). It uses a SAT solver for all aspects of Boolean manipulation: computing don't-cares for a node in the window, and deriving a new Boolean function of the node after resubstitution. Experimental results on 6-input LUT networks after a high effort synthesis show substantial reductions in area and delay. When applied to 20 large academic benchmarks, the LUT counts and logic levels are reduced by 45.0% and 12.2%, respectively. The longest runtime for synthesis and mapping is about two minutes. When applied to a set of 14 industrial benchmarks ranging up to 83K 6-LUTs, the LUT counts and logic levels are reduced by 11.8% and 16.5%, respectively. The longest runtime is about 30 minutes.",
    "cited_by_count": 84,
    "openalex_id": "https://openalex.org/W1993932586",
    "type": "article"
  },
  {
    "title": "Optimizing CNN-based Segmentation with Deeply Customized Convolutional and Deconvolutional Architectures on FPGA",
    "doi": "https://doi.org/10.1145/3242900",
    "publication_date": "2018-09-30",
    "publication_year": 2018,
    "authors": "Shuanglong Liu; Hongxiang Fan; Xinyu Niu; Ho-cheung Ng; Yang Chu; Wayne Luk",
    "corresponding_authors": "",
    "abstract": "Convolutional Neural Networks-- (CNNs) based algorithms have been successful in solving image recognition problems, showing very large accuracy improvement. In recent years, deconvolution layers are widely used as key components in the state-of-the-art CNNs for end-to-end training and models to support tasks such as image segmentation and super resolution. However, the deconvolution algorithms are computationally intensive, which limits their applicability to real-time applications. Particularly, there has been little research on the efficient implementations of deconvolution algorithms on FPGA platforms that have been widely used to accelerate CNN algorithms by practitioners and researchers due to their high performance and power efficiency. In this work, we propose and develop deconvolution architecture for efficient FPGA implementation. FPGA-based accelerators are proposed for both deconvolution and CNN algorithms. Besides, memory sharing between the computation modules is proposed for the FPGA-based CNN accelerator as well as for other optimization techniques. A non-linear optimization model based on the performance model is introduced to efficiently explore the design space to achieve optimal processing speed of the system and improve power efficiency. Furthermore, a hardware mapping framework is developed to automatically generate the low-latency hardware design for any given CNN model on the target device. Finally, we implement our designs on Xilinx Zynq ZC706 board and the deconvolution accelerator achieves a performance of 90.1 giga operations per second (GOPS) under 200MHz working frequency and a performance density of 0.10 GOPS/DSP using 32-bit quantization, which significantly outperforms previous designs on FPGAs. A real-time application of scene segmentation on Cityscapes Dataset is used to evaluate our CNN accelerator on Zynq ZC706 board, and the system achieves a performance of 107 GOPS and 0.12 GOPS/DSP using 16-bit quantization and supports up to 17 frames per second for 512 × 512 image inputs with a power consumption of only 9.6W.",
    "cited_by_count": 70,
    "openalex_id": "https://openalex.org/W2906403229",
    "type": "article"
  },
  {
    "title": "ULP-SRP",
    "doi": "https://doi.org/10.1145/2629610",
    "publication_date": "2014-08-01",
    "publication_year": 2014,
    "authors": "Changmoo Kim; Moo-Kyoung Chung; Yeongon Cho; Mario Konijnenburg; Soojung Ryu; Jeongwook Kim",
    "corresponding_authors": "",
    "abstract": "The latest biomedical applications require low energy consumption, high performance, and wide energy-performance scalability to adapt to various working environments. In this study, we present ULP-SRP, an energy-efficient reconfigurable processor for biomedical applications. ULP-SRP uses a Coarse-Grained Reconfigurable Array (CGRA) for high-performance data processing with low energy consumption. We adopted a compact-size CGRA and modified it to support dynamically switchable three performance modes with fine-grained power gating in order to further optimize the energy consumption. The energy-performance scalability is also accomplished with multiple performance modes and a Unified Memory Architecture (UMA). Experimental results show that ULP-SRP achieved 59% energy reduction compared to previous works. A technique of dynamic CGRA mode changing gives 18.9% energy reduction. ULP-SRP is a good candidate for future mobile healthcare devices.",
    "cited_by_count": 69,
    "openalex_id": "https://openalex.org/W2047049615",
    "type": "article"
  },
  {
    "title": "Recent Attacks and Defenses on FPGA-based Systems",
    "doi": "https://doi.org/10.1145/3340557",
    "publication_date": "2019-08-21",
    "publication_year": 2019,
    "authors": "Jiliang Zhang; Gang Qu",
    "corresponding_authors": "",
    "abstract": "Field-programmable gate array (FPGA) is a kind of programmable chip that is widely used in many areas, including automotive electronics, medical devices, military and consumer electronics, and is gaining more popularity. Unlike the application specific integrated circuits (ASIC) design, an FPGA-based system has its own supply-chain model and design flow, which brings interesting security and trust challenges. In this survey, we review the security and trust issues related to FPGA-based systems from the market perspective, where we model the market with the following parties: FPGA vendors, foundries, IP vendors, EDA tool vendors, FPGA-based system developers, and end-users. For each party, we show the security and trust problems they need to be aware of and the associated solutions that are available. We also discuss some challenges and opportunities in the security and trust of FPGA-based systems used in large-scale cloud and datacenters.",
    "cited_by_count": 69,
    "openalex_id": "https://openalex.org/W3030932237",
    "type": "article"
  },
  {
    "title": "Mitigating Electrical-level Attacks towards Secure Multi-Tenant FPGAs in the Cloud",
    "doi": "https://doi.org/10.1145/3328222",
    "publication_date": "2019-08-13",
    "publication_year": 2019,
    "authors": "Jonas Krautter; Dennis R. E. Gnad; Mehdi B. Tahoori",
    "corresponding_authors": "",
    "abstract": "A rising trend is the use of multi-tenant FPGAs, particularly in cloud environments, where partial access to the hardware is given to multiple third parties. This leads to new types of attacks in FPGAs, which operate not only on the logic level, but also on the electrical level through the common power delivery network. Since FPGAs are configured from the software-side, attackers are enabled to launch hardware attacks from software, impacting the security of an entire system. In this article, we show the first attempt of a countermeasure against attacks on the electrical level, which is based on a bitstream checking methodology. Bitstreams are translated back into flat technology mapped netlists, which are then checked for properties that indicate potential malicious runtime behavior of FPGA logic. Our approach can provide a metric of potential risk of the FPGA bitstream being used in active fault or passive side-channel attacks against other users of the FPGA fabric or the entire SoC platform.",
    "cited_by_count": 67,
    "openalex_id": "https://openalex.org/W2966980899",
    "type": "article"
  },
  {
    "title": "NEURA <scp>ghe</scp>",
    "doi": "https://doi.org/10.1145/3284357",
    "publication_date": "2018-09-30",
    "publication_year": 2018,
    "authors": "Paolo Meloni; Alessandro Capotondi; Gianfranco Deriu; Michele Brian; Francesco Conti; Davide Rossi; Luigi Raffo; Luca Benini",
    "corresponding_authors": "",
    "abstract": "Deep convolutional neural networks (CNNs) obtain outstanding results in tasks that require human-level understanding of data, like image or speech recognition. However, their computational load is significant, motivating the development of CNN-specialized accelerators. This work presents NEURA ghe , a flexible and efficient hardware/software solution for the acceleration of CNNs on Zynq SoCs. NEURA ghe leverages the synergistic usage of Zynq ARM cores and of a powerful and flexible Convolution-Specific Processor deployed on the reconfigurable logic. The Convolution-Specific Processor embeds both a convolution engine and a programmable soft core, releasing the ARM processors from most of the supervision duties and allowing the accelerator to be controlled by software at an ultra-fine granularity. This methodology opens the way for cooperative heterogeneous computing: While the accelerator takes care of the bulk of the CNN workload, the ARM cores can seamlessly execute hard-to-accelerate parts of the computational graph, taking advantage of the NEON vector engines to further speed up computation. Through the companion NeuDNN SW stack, NEURA ghe supports end-to-end CNN-based classification with a peak performance of 169GOps/s, and an energy efficiency of 17GOps/W. Thanks to our heterogeneous computing model, our platform improves upon the state-of-the-art, achieving a frame rate of 5.5 frames per second (fps) on the end-to-end execution of VGG-16 and 6.6fps on ResNet-18.",
    "cited_by_count": 64,
    "openalex_id": "https://openalex.org/W3100768276",
    "type": "article"
  },
  {
    "title": "FPGA-based Deep Learning Inference Accelerators: Where Are We Standing?",
    "doi": "https://doi.org/10.1145/3613963",
    "publication_date": "2023-09-04",
    "publication_year": 2023,
    "authors": "Anouar Nechi; Lukas Groth; Saleh Mulhem; Farhad Merchant; Rainer Buchty; Mladen Bereković",
    "corresponding_authors": "",
    "abstract": "Recently, artificial intelligence applications have become part of almost all emerging technologies around us. Neural networks, in particular, have shown significant advantages and have been widely adopted over other approaches in machine learning. In this context, high processing power is deemed a fundamental challenge and a persistent requirement. Recent solutions facing such a challenge deploy hardware platforms to provide high computing performance for neural networks and deep learning algorithms. This direction is also rapidly taking over the market. Here, FPGAs occupy the middle ground regarding flexibility, reconfigurability, and efficiency compared to general-purpose CPUs, GPUs, on one side, and manufactured ASICs on the other. FPGA-based accelerators exploit the features of FPGAs to increase the computing performance for specific algorithms and algorithm features. Filling a gap, we provide holistic benchmarking criteria and optimization techniques that work across several classes of deep learning implementations. This article summarizes the current state of deep learning hardware acceleration: More than 120 FPGA-based neural network accelerator designs are presented and evaluated based on a matrix of performance and acceleration criteria, and corresponding optimization techniques are presented and discussed. In addition, the evaluation criteria and optimization techniques are demonstrated by benchmarking ResNet-2 and LSTM-based accelerators.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W4386422402",
    "type": "article"
  },
  {
    "title": "TATAA: Programmable Mixed-Precision Transformer Acceleration with a Transformable Arithmetic Architecture",
    "doi": "https://doi.org/10.1145/3714416",
    "publication_date": "2025-01-24",
    "publication_year": 2025,
    "authors": "Jiajun Wu; Mo Song; Jingmin Zhao; Yizhao Gao; Jia Li; Hayden Kwok‐Hay So",
    "corresponding_authors": "",
    "abstract": "Modern transformer-based deep neural networks present unique technical challenges for effective acceleration in real-world applications. Apart from the vast amount of linear operations needed due to their sizes, modern transformer models are increasingly reliance on precise non-linear computations that make traditional low-bitwidth quantization methods and fixed-dataflow matrix accelerators ineffective for end-to-end acceleration. To address this need to accelerate both linear and non-linear operations in a unified and programmable framework, this article introduces TATAA. TATAA employs 8-bit integer ( int8 ) arithmetic for quantized linear layer operations through post-training quantization, while it relies on bfloat16 floating-point arithmetic to approximate non-linear layers of a transformer model. TATAA hardware features a transformable arithmetic architecture that supports both formats during runtime with minimal overhead, enabling it to switch between a systolic array mode for int8 matrix multiplications and a SIMD mode for vectorized bfloat16 operations. An end-to-end compiler is presented to enable flexible mapping from emerging transformer models to the proposed hardware. Experimental results indicate that our mixed-precision design incurs only 0.14% to 1.16% accuracy drop when compared with the pre-trained single-precision transformer models across a range of vision, language, and generative text applications. Our prototype implementation on the Alveo U280 FPGA currently achieves 2,935.2 GOPS throughput on linear layers and a maximum of 189.5 GFLOPS for non-linear operations, outperforming related works by up to \\(1.45\\times\\) in end-to-end throughput and \\(2.29\\times\\) in DSP efficiency, while achieving \\(2.19\\times\\) higher power efficiency than modern NVIDIA RTX4090 GPU.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4406783634",
    "type": "article"
  },
  {
    "title": "ProtoFlex",
    "doi": "https://doi.org/10.1145/1534916.1534925",
    "publication_date": "2009-06-01",
    "publication_year": 2009,
    "authors": "Eric S. Chung; Michael Papamichael; Eriko Nurvitadhi; James C. Hoe; Ken Mai; Babak Falsafi",
    "corresponding_authors": "",
    "abstract": "Functional full-system simulators are powerful and versatile research tools for accelerating architectural exploration and advanced software development. Their main shortcoming is limited throughput when simulating large multiprocessor systems with hundreds or thousands of processors or when instrumentation is introduced. We propose the ProtoFlex simulation architecture, which uses FPGAs to accelerate full-system multiprocessor simulation and to facilitate high-performance instrumentation. Prior FPGA approaches that prototype a complete system in hardware are either too complex when scaling to large-scale configurations or require significant effort to provide full-system support. In contrast, ProtoFlex virtualizes the execution of many logical processors onto a consolidated number of multiple-context execution engines on the FPGA. Through virtualization, the number of engines can be judiciously scaled, as needed, to deliver on necessary simulation performance at a large savings in complexity. Further, to achieve low-complexity full-system support, a hybrid simulation technique called transplanting allows implementing in the FPGA only the frequently encountered behaviors, while a software simulator preserves the abstraction of a complete system. We have created a first instance of the ProtoFlex simulation architecture, which is an FPGA-based, full-system functional simulator for a 16-way UltraSPARC III symmetric multiprocessor server, hosted on a single Xilinx Virtex-II XCV2P70 FPGA. On average, the simulator achieves a 38x speedup (and as high as 49×) over comparable software simulation across a suite of applications, including OLTP on a commercial database server. We also demonstrate the advantages of minimal-overhead FPGA-accelerated instrumentation through a CMP cache simulation technique that runs orders-of-magnitude faster than software.",
    "cited_by_count": 86,
    "openalex_id": "https://openalex.org/W2155764480",
    "type": "article"
  },
  {
    "title": "Molecular Dynamics Simulations on High-Performance Reconfigurable Computing Systems",
    "doi": "https://doi.org/10.1145/1862648.1862653",
    "publication_date": "2010-11-01",
    "publication_year": 2010,
    "authors": "Matt Chiu; Martin Herbordt",
    "corresponding_authors": "",
    "abstract": "The acceleration of molecular dynamics (MD) simulations using high-performance reconfigurable computing (HPRC) has been much studied. Given the intense competition from multicore and GPUs, there is now a question whether MD on HPRC can be competitive. We concentrate here on the MD kernel computation: determining the short-range force between particle pairs. In one part of the study, we systematically explore the design space of the force pipeline with respect to arithmetic algorithm, arithmetic mode, precision, and various other optimizations. We examine simplifications and find that some have little effect on simulation quality. In the other part, we present the first FPGA study of the filtering of particle pairs with nearly zero mutual force, a standard optimization in MD codes. There are several innovations, including a novel partitioning of the particle space, and new methods for filtering and mapping work onto the pipelines. As a consequence, highly efficient filtering can be implemented with only a small fraction of the FPGA's resources. Overall, we find that, for an Altera Stratix-III EP3ES260, 8 force pipelines running at nearly 200 MHz can fit on the FPGA, and that they can perform at 95% efficiency. This results in an 80-fold per core speed-up for the short-range force, which is likely to make FPGAs highly competitive for MD.",
    "cited_by_count": 85,
    "openalex_id": "https://openalex.org/W2086364561",
    "type": "article"
  },
  {
    "title": "Low-cost sensing with ring oscillator arrays for healthier reconfigurable systems",
    "doi": "https://doi.org/10.1145/2133352.2133353",
    "publication_date": "2012-03-01",
    "publication_year": 2012,
    "authors": "Kenneth M. Zick; John P. Hayes",
    "corresponding_authors": "",
    "abstract": "Electronic systems on a chip increasingly suffer from component variation, voltage noise, thermal hotspots, and other subtle physical phenomena. Systems with reconfigurability have unique opportunities for adapting to such effects. Required, however, are low-cost, fine-grained methods for sensing physical parameters. This article presents powerful, novel approaches to online sensing, including methods for designing compact reconfigurable sensors, low-cost threshold detection, and several enhanced measurement procedures. Together, the approaches help enable systems to autonomously uncover a wealth of physical information. A highly efficient counter and improved ring oscillator are introduced, enabling an entire sensor node in just 8 Virtex-5 LUTs. We describe how variations can be measured in delay, temperature, switching-induced IR drop, and leakage-induced IR drop. We demonstrate the proposed approach with an experimental system based on a Virtex-5, instrumented with over 100 sensors at an overhead of only 1.3%. Results from thermally controlled experiments provide some surprising insights and illustrate the utility of the approach. Online sensing can help open the door to physically adaptive computing, including fine-grained power, reliability, and health management schemes for systems on a chip.",
    "cited_by_count": 76,
    "openalex_id": "https://openalex.org/W2050145588",
    "type": "article"
  },
  {
    "title": "Reconfigurable Fault Tolerance",
    "doi": "https://doi.org/10.1145/2392616.2392619",
    "publication_date": "2012-12-01",
    "publication_year": 2012,
    "authors": "Adam Jacobs; Grzegorz Cieslewski; Alan D. George; Ross Gordon; Herman Lam",
    "corresponding_authors": "",
    "abstract": "Commercial SRAM-based, field-programmable gate arrays (FPGAs) have the potential to provide space applications with the necessary performance to meet next-generation mission requirements. However, mitigating an FPGA’s susceptibility to single-event upset (SEU) radiation is challenging. Triple-modular redundancy (TMR) techniques are traditionally used to mitigate radiation effects, but TMR incurs substantial overheads such as increased area and power requirements. In order to reduce these overheads while still providing sufficient radiation mitigation, we propose a reconfigurable fault tolerance (RFT) framework that enables system designers to dynamically adjust a system’s level of redundancy and fault mitigation based on the varying radiation incurred at different orbital positions. This framework includes an adaptive hardware architecture that leverages FPGA reconfigurable techniques to enable significant processing to be performed efficiently and reliably when environmental factors permit. To accurately estimate upset rates, we propose an upset rate modeling tool that captures time-varying radiation effects for arbitrary satellite orbits using a collection of existing, publically available tools and models. We perform fault-injection testing on a prototype RFT platform to validate the RFT architecture and RFT performability models. We combine our RFT hardware architecture and the modeled upset rates using phased-mission Markov modeling to estimate performability gains achievable using our framework for two case-study orbits.",
    "cited_by_count": 71,
    "openalex_id": "https://openalex.org/W1980602225",
    "type": "article"
  },
  {
    "title": "VPR 5.0",
    "doi": "https://doi.org/10.1145/2068716.2068718",
    "publication_date": "2011-12-01",
    "publication_year": 2011,
    "authors": "Jason Luu; Ian Kuon; Peter Jamieson; Ted Campbell; Andy Ye; Wei Mark Fang; Kenneth B. Kent; Jonathan Rose",
    "corresponding_authors": "",
    "abstract": "The VPR toolset has been widely used in FPGA architecture and CAD research, but has not evolved over the past decade. This article describes and illustrates the use of a new version of the toolset that includes four new features: first, it supports a broad range of single-driver routing architectures, which have superior architectural and electrical properties over the prior multidriver approach (and which is now employed in the majority of FPGAs sold). Second, it can now model, for placement and routing a heterogeneous selection of hard logic blocks. This is a key (but not final) step toward the incluion of blocks such as memory and multipliers. Third, we provide optimized electrical models for a wide range of architectures in different process technologies, including a range of area-delay trade-offs for each single architecture. Finally, to maintain robustness and support future development the release includes a set of regression tests for the software. To illustrate the use of the new features, we explore several architectural issues: the FPGA area efficiency versus logic block granularity, the effect of single-driver routing, and a simple use of the heterogeneity to explore the impact of hard multipliers on wiring track count.",
    "cited_by_count": 67,
    "openalex_id": "https://openalex.org/W2014316444",
    "type": "article"
  },
  {
    "title": "Improved Reliability of FPGA-Based PUF Identification Generator Design",
    "doi": "https://doi.org/10.1145/3053681",
    "publication_date": "2017-05-27",
    "publication_year": 2017,
    "authors": "Chongyan Gu; Neil Hanley; Máire O’Neill",
    "corresponding_authors": "",
    "abstract": "Physical unclonable functions (PUFs), a form of physical security primitive, enable digital identifiers to be extracted from devices, such as field programmable gate arrays (FPGAs). Many PUF implementations have been proposed to generate these unique n -bit binary strings. However, they often offer insufficient uniqueness and reliability when implemented on FPGAs and can consume excessive resources. To address these problems, in this article we present an efficient, lightweight, and scalable PUF identification (ID) generator circuit that offers a compact design with good uniqueness and reliability properties and is specifically designed for FPGAs. A novel post-characterisation methodology is also proposed that improves the reliability of a PUF without the need for any additional hardware resources. Moreover, the proposed post-characterisation method can be generally used for any FPGA-based PUF designs. The PUF ID generator consumes 8.95% of the hardware resources of a low-cost Xilinx Spartan-6 LX9 FPGA and 0.81% of a Xilinx Artix-7 FPGA. Experimental results show good uniqueness, reliability, and uniformity with no occurrence of bit-aliasing. In particular, the reliability of the PUF is close to 100% over an environmental temperature range of 25°C to 70°C with ± 10% variation in the supply voltage.",
    "cited_by_count": 60,
    "openalex_id": "https://openalex.org/W2609040851",
    "type": "article"
  },
  {
    "title": "You Cannot Improve What You Do not Measure",
    "doi": "https://doi.org/10.1145/3242898",
    "publication_date": "2018-09-30",
    "publication_year": 2018,
    "authors": "Andrew Boutros; Sadegh Yazdanshenas; Vaughn Betz",
    "corresponding_authors": "",
    "abstract": "Recently, deep learning (DL) has become best-in-class for numerous applications but at a high computational cost that necessitates high-performance energy-efficient acceleration. The reconfigurability of FPGAs is appealing due to the rapid change in DL models but also causes lower performance and area-efficiency compared to ASICs. In this article, we implement three state-of-the-art computing architectures (CAs) for convolutional neural network (CNN) inference on FPGAs and ASICs. By comparing the FPGA and ASIC implementations, we highlight the area and performance costs of programmability to pinpoint the inefficiencies in current FPGA architectures. We perform our experiments using three variations of these CAs for AlexNet, VGG-16 and ResNet-50 to allow extensive comparisons. We find that the performance gap varies significantly from 2.8× to 6.3×, while the area gap is consistent across CAs with an 8.7 average FPGA-to-ASIC area ratio. Among different blocks of the CAs, the convolution engine, constituting up to 60% of the total area, has a high area ratio ranging from 13 to 31. Motivated by our FPGA vs. ASIC comparisons, we suggest FPGA architectural changes such as increasing DSP block count, enhancing low-precision support in DSP blocks and rethinking the on-chip memories to reduce the programmability gap for DL applications.",
    "cited_by_count": 55,
    "openalex_id": "https://openalex.org/W2903688003",
    "type": "article"
  },
  {
    "title": "COFFE 2",
    "doi": "https://doi.org/10.1145/3301298",
    "publication_date": "2019-01-30",
    "publication_year": 2019,
    "authors": "Sadegh Yazdanshenas; Vaughn Betz",
    "corresponding_authors": "",
    "abstract": "FPGAs are becoming more heteregeneous to better adapt to different markets, motivating rapid exploration of different blocks/tiles for FPGAs. To evaluate a new FPGA architectural idea, one should be able to accurately obtain the area, delay, and energy consumption of the block of interest. However, current FPGA circuit design tools can only model simple, homogeneous FPGA architectures with basic logic blocks and also lack DSP and other heterogeneous block support. Modern FPGAs are instead composed of many different tiles, some of which are designed in a full custom style and some of which mix standard cell and full custom styles. To fill this modelling gap, we introduce COFFE 2, an open-source FPGA design toolset for automatic FPGA circuit design. COFFE 2 uses a mix of full custom and standard cell flows and supports not only complex logic blocks with fracturable lookup tables and hard arithmetic but also arbitrary heterogeneous blocks. To validate COFFE 2 and demonstrate its features, we design and evaluate a multi-mode Stratix III-like DSP block and several logic tiles with fracturable LUTs and hard arithmetic. We also demonstrate how COFFE 2’s interface to VTR allows full evaluation of block-routing interfaces and various fracturable 6-LUT architectures.",
    "cited_by_count": 48,
    "openalex_id": "https://openalex.org/W2911751195",
    "type": "article"
  },
  {
    "title": "FlexCNN: An End-to-end Framework for Composing CNN Accelerators on FPGA",
    "doi": "https://doi.org/10.1145/3570928",
    "publication_date": "2022-12-20",
    "publication_year": 2022,
    "authors": "Suhail Basalama; Atefeh Sohrabizadeh; Jie Wang; Licheng Guo; Jason Cong",
    "corresponding_authors": "",
    "abstract": "With reduced data reuse and parallelism, recent convolutional neural networks (CNNs) create new challenges for FPGA acceleration. Systolic arrays (SAs) are efficient, scalable architectures for convolutional layers, but without proper optimizations, their efficiency drops dramatically for reasons: (1) the different dimensions within same-type layers, (2) the different convolution layers especially transposed and dilated convolutions, and (3) CNN’s complex dataflow graph. Furthermore, significant overheads arise when integrating FPGAs into machine learning frameworks. Therefore, we present a flexible, composable architecture called FlexCNN, which delivers high computation efficiency by employing dynamic tiling, layer fusion, and data layout optimizations. Additionally, we implement a novel versatile SA to process normal, transposed, and dilated convolutions efficiently. FlexCNN also uses a fully pipelined software-hardware integration that alleviates the software overheads. Moreover, with an automated compilation flow, FlexCNN takes a CNN in the ONNX 1 representation, performs a design space exploration, and generates an FPGA accelerator. The framework is tested using three complex CNNs: OpenPose, U-Net, and E-Net. The architecture optimizations achieve 2.3× performance improvement. Compared to a standard SA, the versatile SA achieves close-to-ideal speedups, with up to 5.98× and 13.42× for transposed and dilated convolutions, with a 6% average area overhead. The pipelined integration leads to a 5× speedup for OpenPose.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W4312037452",
    "type": "article"
  },
  {
    "title": "LW-GCN: A Lightweight FPGA-based Graph Convolutional Network Accelerator",
    "doi": "https://doi.org/10.1145/3550075",
    "publication_date": "2022-08-04",
    "publication_year": 2022,
    "authors": "Zhuofu Tao; C.-L. Wu; Yuan Liang; Kun Wang; Lei He",
    "corresponding_authors": "",
    "abstract": "Graph convolutional networks (GCNs) have been introduced to effectively process non-Euclidean graph data. However, GCNs incur large amounts of irregularity in computation and memory access, which prevents efficient use of traditional neural network accelerators. Moreover, existing dedicated GCN accelerators demand high memory volumes and are difficult to implement onto resource limited edge devices. In this work, we propose LW-GCN, a lightweight FPGA-based accelerator with a software-hardware co-designed process to tackle irregularity in computation and memory access in GCN inference. LW-GCN decomposes the main GCN operations into Sparse Matrix-Matrix Multiplication (SpMM) and Matrix-Matrix Multiplication (MM). We propose a novel compression format to balance workload across PEs and prevent data hazards. Moreover, we apply data quantization and workload tiling, and map both SpMM and MM of GCN inference onto a uniform architecture on resource limited hardware. Evaluation on GCN and GraphSAGE are performed on Xilinx Kintex-7 FPGA with three popular datasets. Compared to existing CPU, GPU, and state-of-the-art FPGA-based accelerator, LW-GCN reduces latency by up to 60×, 12×, and 1.7× and increases power efficiency by up to 912×, 511×, and 3.87×, respectively. Furthermore, compared with NVIDIA’s latest edge GPU Jetson Xavier NX, LW-GCN achieves speedup and energy savings of 32× and 84×, respectively.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W4226439885",
    "type": "article"
  },
  {
    "title": "TAPA: A Scalable Task-parallel Dataflow Programming Framework for Modern FPGAs with Co-optimization of HLS and Physical Design",
    "doi": "https://doi.org/10.1145/3609335",
    "publication_date": "2023-09-18",
    "publication_year": 2023,
    "authors": "Licheng Guo; Yuze Chi; Jason Lau; Linghao Song; Xingyu Tian; Moazin Khatti; Weikang Qiao; Jie Wang; Ecenur Ustun; Zhenman Fang; Zhiru Zhang; Jason Cong",
    "corresponding_authors": "",
    "abstract": "In this article, we propose TAPA, an end-to-end framework that compiles a C++ task-parallel dataflow program into a high-frequency FPGA accelerator. Compared to existing solutions, TAPA has two major advantages. First, TAPA provides a set of convenient APIs that allows users to easily express flexible and complex inter-task communication structures. Second, TAPA adopts a coarse-grained floorplanning step during HLS compilation for accurate pipelining of potential critical paths. In addition, TAPA implements several optimization techniques specifically tailored for modern HBM-based FPGAs. In our experiments with a total of 43 designs, we improve the average frequency from 147 MHz to 297 MHz (a 102% improvement) with no loss of throughput and a negligible change in resource utilization. Notably, in 16 experiments, we make the originally unroutable designs achieve 274 MHz, on average. The framework is available at https://github.com/UCLA-VAST/tapa and the core floorplan module is available at https://github.com/UCLA-VAST/AutoBridge",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W4386830976",
    "type": "article"
  },
  {
    "title": "Understanding the Potential of FPGA-Based Spatial Acceleration for Large Language Model Inference",
    "doi": "https://doi.org/10.1145/3656177",
    "publication_date": "2024-04-04",
    "publication_year": 2024,
    "authors": "Hongzheng Chen; Jiahao Zhang; Yixiao Du; Shaojie Xiang; Zichao Yue; Niansong Zhang; Yaohui Cai; Zhiru Zhang",
    "corresponding_authors": "",
    "abstract": "Recent advancements in large language models (LLMs) boasting billions of parameters have generated a significant demand for efficient deployment in inference workloads. While hardware accelerators for Transformer-based models have been extensively studied, the majority of existing approaches rely on temporal architectures that reuse hardware units for different network layers and operators. However, these methods often encounter challenges in achieving low latency due to considerable memory access overhead. This paper investigates the feasibility and potential of model-specific spatial acceleration for LLM inference on FPGAs. Our approach involves the specialization of distinct hardware units for specific operators or layers, facilitating direct communication between them through a dataflow architecture while minimizing off-chip memory accesses. We introduce a comprehensive analytical model for estimating the performance of a spatial LLM accelerator, taking into account the on-chip compute and memory resources available on an FPGA. This model can be extended to multi-FPGA settings for distributed inference. Through our analysis, we can identify the most effective parallelization and buffering schemes for the accelerator and, crucially, determine the scenarios in which FPGA-based spatial acceleration can outperform its GPU-based counterpart. To enable more productive implementations of an LLM model on FPGAs, we further provide a library of high-level synthesis (HLS) kernels that are composable and reusable. This library will be made available as open-source. To validate the effectiveness of both our analytical model and HLS library, we have implemented BERT and GPT2 on an AMD Xilinx Alveo U280 FPGA device. Experimental results demonstrate our approach can achieve up to 13.4 × speedup when compared to previous FPGA-based accelerators for the BERT model. For GPT generative inference, we attain a 2.2 × speedup compared to DFX, an FPGA overlay, in the prefill stage, while achieving a 1.9 × speedup and a 5.7 × improvement in energy efficiency compared to the NVIDIA A100 GPU in the decode stage.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4393949386",
    "type": "article"
  },
  {
    "title": "Vector Processing as a Soft Processor Accelerator",
    "doi": "https://doi.org/10.1145/1534916.1534922",
    "publication_date": "2009-06-01",
    "publication_year": 2009,
    "authors": "Jason Yu; Christopher Eagleston; Christopher Han-Yu Chou; Maxime Perreault; Guy Lemieux",
    "corresponding_authors": "",
    "abstract": "Current FPGA soft processor systems use dedicated hardware modules or accelerators to speed up data-parallel applications. This work explores an alternative approach of using a soft vector processor as a general-purpose accelerator. The approach has the benefits of a purely software-oriented development model, a fixed ISA allowing parallel software and hardware development, a single accelerator that can accelerate multiple applications, and scalable performance from the same source code. With no hardware design experience needed, a software programmer can make area-versus-performance trade-offs by scaling the number of functional units and register file bandwidth with a single parameter. A soft vector processor can be further customized by a number of secondary parameters to add or remove features for a specific application to optimize resource utilization. This article introduces VIPERS, a soft vector processor architecture that maps efficiently into an FPGA and provides a scalable amount of performance for a reasonable amount of area. Compared to a Nios II/s processor, instances of VIPERS with 32 processing lanes achieve up to 44× speedup using up to 26× the area.",
    "cited_by_count": 69,
    "openalex_id": "https://openalex.org/W2167897136",
    "type": "article"
  },
  {
    "title": "Self-Measurement of Combinatorial Circuit Delays in FPGAs",
    "doi": "https://doi.org/10.1145/1534916.1534920",
    "publication_date": "2009-06-01",
    "publication_year": 2009,
    "authors": "Justin S. J. Wong; Pete Sedcole; Peter Y. K. Cheung",
    "corresponding_authors": "",
    "abstract": "This article proposes a Built-In Self-Test (BIST) method to accurately measure the combinatorial circuit delays on an FPGA. The flexibility of the on-chip clock generation capability found in modern FPGAs is employed to step through a range of frequencies until timing failure in the combinatorial circuit is detected. In this way, the delay of any combinatorial circuit can be determined with a timing resolution of the order of picoseconds. Parallel and optimized implementations of the method for self-characterization of the delay of all the LUTs on an FPGA are also proposed. The method was applied to Altera Cyclone II and III FPGAs . A complete self-characterization of LUTs on a Cyclone II was achieved in 2.5 seconds, utilizing only 13kbit of block RAM to store the results. More extensive tests were carried out on the Cyclone III and the delays of adder circuits and embedded multiplier blocks were successfully measured. This self-measurement method paves the way for matching timing requirements in designs to FPGAs as a means of combating the problem of process variations.",
    "cited_by_count": 64,
    "openalex_id": "https://openalex.org/W2022202043",
    "type": "article"
  },
  {
    "title": "High-Performance Quasi-Monte Carlo Financial Simulation",
    "doi": "https://doi.org/10.1145/1862648.1862656",
    "publication_date": "2010-11-01",
    "publication_year": 2010,
    "authors": "Xiang Tian; Khaled Benkrid",
    "corresponding_authors": "",
    "abstract": "Quasi-Monte Carlo simulation is a special Monte Carlo simulation method that uses quasi-random or low-discrepancy numbers as random sample sets. In many applications, this method has proved advantageous compared to the traditional Monte Carlo simulation method, which uses pseudo-random numbers, thanks to its faster convergence and higher level of accuracy. This article presents the design and implementation of a massively parallelized Quasi-Monte Carlo simulation engine on an FPGA-based supercomputer, called Maxwell. It also compares this implementation with equivalent graphics processing units (GPUs) and general purpose processors (GPP)-based implementations. The detailed comparison between these three implementations (FPGA vs. GPP vs. GPU) is done in the context of financial derivatives pricing based on our Quasi-Monte Carlo simulation engine. Real hardware implementations on the Maxwell machine show that FPGAs outperform equivalent GPP-based software implementations by 2 orders of magnitude, with the speed-up figure scaling linearly with the number of processing nodes used (FPGAs/GPPs). The same implementations show that FPGAs achieve a ~ 3x speedup compared to equivalent GPU-based implementations. Power consumption measurements also show FPGAs to be 336x more energy efficient than CPUs, and 16x more energy efficient than GPUs.",
    "cited_by_count": 54,
    "openalex_id": "https://openalex.org/W2030005191",
    "type": "article"
  },
  {
    "title": "FPGA-Based Dynamically Reconfigurable SQL Query Processing",
    "doi": "https://doi.org/10.1145/2845087",
    "publication_date": "2016-08-22",
    "publication_year": 2016,
    "authors": "Daniel Ziener; Florian J. Bauer; Andreas Becher; Christopher Dennl; Klaus Meyer-Wegener; Ute Schürfeld; Jürgen Teich; J.-S. Vogt; Helmut Weber",
    "corresponding_authors": "",
    "abstract": "In this article, we propose an FPGA-based SQL query processing approach exploiting the capabilities of partial dynamic reconfiguration of modern FPGAs. After the analysis of an incoming query, a query-specific hardware processing unit is generated on the fly and loaded on the FPGA for immediate query execution. For each query, a specialized hardware accelerator pipeline is composed and configured on the FPGA from a set of presynthesized hardware modules. These partially reconfigurable hardware modules are gathered in a library covering all major SQL operations like restrictions and aggregations, as well as more complex operations such as joins and sorts. Moreover, this holistic query processing approach in hardware supports different data processing strategies including row- as column-wise data processing in order to optimize data communication and processing. This article gives an overview of the proposed query processing methodology and the corresponding library of modules. Additionally, a performance analysis is introduced that is able to estimate the processing time of a query for different processing strategies and different communication and processing architecture configurations. With the help of this performance analysis, architectural bottlenecks may be exposed and future optimized architectures, besides the two prototypes presented here, may be determined.",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W2512638702",
    "type": "article"
  },
  {
    "title": "In-Depth Analysis on Microarchitectures of Modern Heterogeneous CPU-FPGA Platforms",
    "doi": "https://doi.org/10.1145/3294054",
    "publication_date": "2019-02-17",
    "publication_year": 2019,
    "authors": "Young Choi; Jason Cong; Zhenman Fang; Yuchen Hao; Glenn Reinman; Peng Wei",
    "corresponding_authors": "",
    "abstract": "Conventional homogeneous multicore processors are not able to provide the continued performance and energy improvement that we have expected from past endeavors. Heterogeneous architectures that feature specialized hardware accelerators are widely considered a promising paradigm for resolving this issue. Among different heterogeneous devices, FPGAs that can be reconfigured to accelerate a broad class of applications with orders-of-magnitude performance/watt gains, are attracting increased attention from both academia and industry. As a consequence, a variety of CPU-FPGA acceleration platforms with diversified microarchitectural features have been supplied by industry vendors. Such diversity, however, poses a serious challenge to application developers in selecting the appropriate platform for a specific application or application domain. This article aims to address this challenge by determining which microarchitectural characteristics affect performance, and in what ways. Specifically, we conduct a quantitative comparison and an in-depth analysis on five state-of-the-art CPU-FPGA acceleration platforms: (1) the Alpha Data board and (2) the Amazon F1 instance that represent the traditional PCIe-based platform with private device memory; (3) the IBM CAPI that represents the PCIe-based system with coherent shared memory; (4) the first generation of the Intel Xeon+FPGA Accelerator Platform that represents the QPI-based system with coherent shared memory; and (5) the second generation of the Intel Xeon+FPGA Accelerator Platform that represents a hybrid PCIe-based (non-coherent) and QPI-based (coherent) system with shared memory. Based on the analysis of their CPU-FPGA communication latency and bandwidth characteristics, we provide a series of insights for both application developers and platform designers. Furthermore, we conduct two case studies to demonstrate how these insights can be leveraged to optimize accelerator designs. The microbenchmarks used for evaluation have been released for public use.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W2917675277",
    "type": "article"
  },
  {
    "title": "Leakier Wires",
    "doi": "https://doi.org/10.1145/3322483",
    "publication_date": "2019-08-23",
    "publication_year": 2019,
    "authors": "Ilias Giechaskiel; Ken Eguro; Kasper Rasmussen",
    "corresponding_authors": "",
    "abstract": "In complex FPGA designs, implementations of algorithms and protocols from third-party sources are common. However, the monolithic nature of FPGAs means that all sub-circuits share common on-chip infrastructure, such as routing resources. This presents an attack vector for all FPGAs that contain designs from multiple vendors, especially for FPGAs used in multi-tenant cloud environments, or integrated into multi-core processors. In this article, we show that “long” routing wires present a new source of information leakage on FPGAs, by influencing the delay of adjacent long wires. We show that the effect is measurable for both static and dynamic signals and that it can be detected using small on-board circuits. We characterize the channel in detail and show that it is measurable even when multiple competing circuits (including multiple long-wire transmitters) are present and can be replicated on different generations and families of Xilinx devices (Virtex 5, Virtex 6, Artix 7, and Spartan 7). We exploit the leakage to create a covert channel with 6kbps of bandwidth and 99.9% accuracy, and a side channel, which can recover signals kept constant for only 1.3sμs, with an accuracy of more than 98.4%. Finally, we propose countermeasures to reduce the impact of this leakage. 1",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W2969965903",
    "type": "article"
  },
  {
    "title": "A Software/Hardware Co-Design of Crystals-Dilithium Signature Scheme",
    "doi": "https://doi.org/10.1145/3447812",
    "publication_date": "2021-06-05",
    "publication_year": 2021,
    "authors": "Zhen Zhou; Debiao He; Zhe Liu; Min Luo; Kim‐Kwang Raymond Choo",
    "corresponding_authors": "",
    "abstract": "As quantum computers become more affordable and commonplace, existing security systems that are based on classical cryptographic primitives, such as RSA and Elliptic Curve Cryptography ( ECC ), will no longer be secure. Hence, there has been interest in designing post-quantum cryptographic ( PQC ) schemes, such as those based on lattice-based cryptography ( LBC ). The potential of LBC schemes is evidenced by the number of such schemes passing the selection of NIST PQC Standardization Process Round-3. One such scheme is the Crystals-Dilithium signature scheme, which is based on the hard module-lattice problem. However, there is no efficient implementation of the Crystals-Dilithium signature scheme. Hence, in this article, we present a compact hardware architecture containing elaborate modular multiplication units using the Karatsuba algorithm along with smart generators of address sequence and twiddle factors for NTT, which can complete polynomial addition/multiplication with the parameter setting of Dilithium in a short clock period. Also, we propose a fast software/hardware co-design implementation on Field Programmable Gate Array ( FPGA ) for the Dilithium scheme with a tradeoff between speed and resource utilization. Our co-design implementation outperforms a pure C implementation on a Nios-II processor of the platform Altera DE2-115, in the sense that our implementation is 11.2 and 7.4 times faster for signature and verification, respectively. In addition, we also achieve approximately 51% and 31% speed improvement for signature and verification, in comparison to the pure C implementation on processor ARM Cortex-A9 of ZYNQ-7020 platform.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W3166957364",
    "type": "article"
  },
  {
    "title": "Elastic-DF: Scaling Performance of DNN Inference in FPGA Clouds through Automatic Partitioning",
    "doi": "https://doi.org/10.1145/3470567",
    "publication_date": "2021-12-06",
    "publication_year": 2021,
    "authors": "Tobías Alonso; Lucian Petrică; Mario Ruiz; Jakoba Petri-Koenig; Yaman Umuroglu; Ioannis Stamelos; Elias Koromilas; Michaela Blott; Kees Vissers",
    "corresponding_authors": "",
    "abstract": "Customized compute acceleration in the datacenter is key to the wider roll-out of applications based on deep neural network (DNN) inference. In this article, we investigate how to maximize the performance and scalability of field-programmable gate array (FPGA)-based pipeline dataflow DNN inference accelerators (DFAs) automatically on computing infrastructures consisting of multi-die, network-connected FPGAs. We present Elastic-DF, a novel resource partitioning tool and associated FPGA runtime infrastructure that integrates with the DNN compiler FINN. Elastic-DF allocates FPGA resources to DNN layers and layers to individual FPGA dies to maximize the total performance of the multi-FPGA system. In the resulting Elastic-DF mapping, the accelerator may be instantiated multiple times, and each instance may be segmented across multiple FPGAs transparently, whereby the segments communicate peer-to-peer through 100 Gbps Ethernet FPGA infrastructure, without host involvement. When applied to ResNet-50, Elastic-DF provides a 44% latency decrease on Alveo U280. For MobileNetV1 on Alveo U200 and U280, Elastic-DF enables a 78% throughput increase, eliminating the performance difference between these cards and the larger Alveo U250. Elastic-DF also increases operating frequency in all our experiments, on average by over 20%. Elastic-DF therefore increases performance portability between different sizes of FPGA and increases the critical throughput per cost metric of datacenter inference.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W4200023416",
    "type": "article"
  },
  {
    "title": "SyncNN: Evaluating and Accelerating Spiking Neural Networks on FPGAs",
    "doi": "https://doi.org/10.1145/3514253",
    "publication_date": "2022-02-09",
    "publication_year": 2022,
    "authors": "Sathish Panchapakesan; Zhenman Fang; Jian Li",
    "corresponding_authors": "",
    "abstract": "Compared to conventional artificial neural networks, spiking neural networks (SNNs) are more biologically plausible and require less computation due to their event-driven nature of spiking neurons. However, the default asynchronous execution of SNNs also poses great challenges to accelerate their performance on FPGAs. In this work, we present a novel synchronous approach for rate-encoding-based SNNs, which is more hardware friendly than conventional asynchronous approaches. We first quantitatively evaluate and mathematically prove that the proposed synchronous approach and asynchronous implementation alternatives of rate-encoding-based SNNs are similar in terms of inference accuracy, and we highlight the computational performance advantage of using SyncNN over an asynchronous approach. We also design and implement the SyncNN framework to accelerate SNNs on Xilinx ARM-FPGA SoCs in a synchronous fashion. To improve the computation and memory access efficiency, we first quantize the network weights to 16-bit, 8-bit, and 4-bit fixed-point values with the SNN-friendly quantization techniques. Moreover, we encode only the activated neurons by recording their positions and corresponding number of spikes to fully utilize the event-driven characteristics of SNNs, instead of using the common binary encoding (i.e., 1 for a spike and 0 for no spike). For the encoded neurons that have dynamic and irregular access patterns, we design parameterized compute engines to accelerate their performance on the FPGA, where we explore various parallelization strategies and memory access optimizations. Our experimental results on multiple Xilinx ARM-FPGA SoC boards demonstrate that our SyncNN is scalable to run multiple networks, such as LeNet, Network in Network, and VGG, on various datasets such as MNIST, SVHN, and CIFAR-10. SyncNN not only achieves competitive accuracy (99.6%) but also achieves state-of-the-art performance (13,086 frames per second) for the MNIST dataset. Finally, we compare the performance of SyncNN with conventional CNNs using the Vitis AI and find that SyncNN can achieve similar accuracy and better performance compared to Vitis AI for image classification using small networks.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W4226340880",
    "type": "article"
  },
  {
    "title": "Electromagnetic Radiations of FPGAs",
    "doi": "https://doi.org/10.1145/1502781.1502785",
    "publication_date": "2009-03-01",
    "publication_year": 2009,
    "authors": "Laurent Sauvage; Sylvain Guilley; Yves Mathieu",
    "corresponding_authors": "",
    "abstract": "Since the first announcement of a Side Channel Analysis (SCA) about ten years ago, considerable research has been devoted to studying these attacks on Application Specific Integrated Circuits (ASICs), such as smart cards or TPMs. In this article, we compare power-line attacks with ElectroMagnetic (EM) attacks, specifically targeting Field Programmable Gate Array devices (FPGAs), as they are becoming widely used for sensitive applications involving cryptography. We show experimentally that ElectroMagnetic Analysis (EMA) is always faster than the historical Differential Power Analysis (DPA) in retrieving keys of symmetric ciphers. In addition, these analyses prove to be very convenient to conduct, as they are totally non-invasive. Research reports indicate that EMA can be conducted globally, typically with macroscopic home-made coils circling the device under attack, with fair results. However, as accurate professional EM antennas are now becoming more accessible, it has become commonplace to carry out EM analyses locally. Cartography has been carried out by optical means on circuits realized with technology greater than 250 nanometers. Nonetheless, for deep submicron technologies, the feature size of devices that are spied upon is too small to be visible with photographic techniques. In addition, the presence of the 6+ metallization layers obviously prevents a direct observation of the layout. Therefore, EM imaging is emerging as a relevant means to discover the underlying device structure. In this article, we present the first images of deep-submicron FPGAs. The resolution is not as accurate as photographic pictures: we notably compare the layout of toy design examples placed at the four corners of the FPGAs with the EM images we collected. We observe that EM imaging has the advantage of revealing active regions, which can be useful in locating a particular processor (visible while active---invisible when inactive). In the context of EM attacks, we stress that the exact localization of the cryptographic target is not necessary: the coarse resolution we obtain is sufficient. We note that the EM imaging does not reveal the exact layout of the FPGA, but instead directly guides the attacker towards the areas which are leaking the most. We achieve attacks with an accurate sensor, both far from (namely on a SMC capacitor on the board) and close to (namely directly over the FPGA) the encryption co-processor. As compared to the previously published attacks, we report a successful attack on a DES module in fewer than 6,300 measurements, which is currently the best cracking performance against this encryption algorithm implemented in FPGAs.",
    "cited_by_count": 52,
    "openalex_id": "https://openalex.org/W2017165380",
    "type": "article"
  },
  {
    "title": "VFloat",
    "doi": "https://doi.org/10.1145/1839480.1839486",
    "publication_date": "2010-09-01",
    "publication_year": 2010,
    "authors": "Xiaojun Wang; Miriam Leeser",
    "corresponding_authors": "",
    "abstract": "Optimal reconfigurable hardware implementations may require the use of arbitrary floating-point formats that do not necessarily conform to IEEE specified sizes. We present a variable precision floating-point library (VFloat) that supports general floating-point formats including IEEE standard formats. Most previously published floating-point formats for use with reconfigurable hardware are subsets of our format. Custom datapaths with optimal bitwidths for each operation can be built using the variable precision hardware modules in the VFloat library, enabling a higher level of parallelism. The VFloat library includes three types of hardware modules for format control, arithmetic operations, and conversions between fixed-point and floating-point formats. The format conversions allow for hybrid fixed- and floating-point operations in a single design. This gives the designer control over a large number of design possibilities including format as well as number range within the same application. In this article, we give an overview of the components in the VFloat library and demonstrate their use in an implementation of the K-means clustering algorithm applied to multispectral satellite images.",
    "cited_by_count": 51,
    "openalex_id": "https://openalex.org/W2032195315",
    "type": "article"
  },
  {
    "title": "A High Throughput FPGA-Based Floating Point Conjugate Gradient Implementation for Dense Matrices",
    "doi": "https://doi.org/10.1145/1661438.1661439",
    "publication_date": "2010-01-01",
    "publication_year": 2010,
    "authors": "António Roldão; George A. Constantinides",
    "corresponding_authors": "",
    "abstract": "Recent developments in the capacity of modern Field Programmable Gate Arrays (FPGAs) have significantly expanded their applications. One such field is the acceleration of scientific computation and one type of calculation that is commonplace in scientific computation is the solution of systems of linear equations. A method that has proven in software to be very efficient and robust for finding such solutions is the Conjugate Gradient (CG) algorithm. In this article we present a widely parallel and deeply pipelined hardware CG implementation, targeted at modern FPGA architectures. This implementation is particularly suited for accelerating multiple small-to-medium-sized dense systems of linear equations and can be used as a stand-alone solver or as building block to solve higher-order systems. In this article it is shown that through parallelization it is possible to convert the computation time per iteration for an order n matrix from Θ ( n 2 ) clock cycles on a microprocessor to Θ ( n ) on a FPGA. Through deep pipelining it is also possible to solve several problems in parallel and maximize both performance and efficiency. I/O requirements are shown to be scalable and convergent to a constant value with the increase of matrix order. Post place-and-route results on a readily available VirtexII-6000 demonstrate sustained performance of 5 GFlops, and results on a Virtex5-330 indicate sustained performance of 35 GFlops. A comparison with an optimized software implementation running on a high-end CPU demonstrate that this FPGA implementation represents a significant speedup of at least an order of magnitude.",
    "cited_by_count": 51,
    "openalex_id": "https://openalex.org/W2077428505",
    "type": "article"
  },
  {
    "title": "DSPs, BRAMs, and a Pinch of Logic",
    "doi": "https://doi.org/10.1145/1661438.1661441",
    "publication_date": "2010-01-01",
    "publication_year": 2010,
    "authors": "Saar Drimer; Tim Güneysu; Christof Paar",
    "corresponding_authors": "",
    "abstract": "We present three lookup-table-based AES implementations that efficiently use the BlockRAM and DSP units embedded within Xilinx Virtex-5 FPGAs. An iterative module outputs a 32-bit AES round column every clock cycle, with a throughput of 1.67 Gbit/s when processing two 128-bit inputs. This construct is then replicated four times to provide a complete AES round per cycle with 6.7 Gbit/s throughput when processing eight input streams. This, in turn, is replicated ten times for a fully unrolled design providing over 52 Gbit/s of throughput. We also present implementations of a BRAM-based AES key-expansion, CMAC, and CTR modes of operation. Results for designs where DSPs are replaced by regular logic are also presented. The combination and arrangement of the specialized embedded functions available in the FPGA allows us to implement our designs using very few traditional user logic elements such as flip-flops and lookup tables, yet still achieve these high throughputs. HDL source code, simulation testbenches, and software tool commands to reproduce reported results for the three AES variants and CMAC mode are made publicly available. Our contribution concludes with a discussion on comparing cipher implementations in the literature, and why these comparisons can be meaningless without a common reporting methodology, or within the context of a constrained target application.",
    "cited_by_count": 50,
    "openalex_id": "https://openalex.org/W2061908058",
    "type": "article"
  },
  {
    "title": "Characterization of Fixed and Reconfigurable Multi-Core Devices for Application Acceleration",
    "doi": "https://doi.org/10.1145/1862648.1862649",
    "publication_date": "2010-11-01",
    "publication_year": 2010,
    "authors": "Jason Williams; Chris Massie; Alan D. George; Justin Richardson; Kunal Gosrani; Herman Lam",
    "corresponding_authors": "",
    "abstract": "As on-chip transistor counts increase, the computing landscape has shifted to multi- and many-core devices. Computational accelerators have adopted this trend by incorporating both fixed and reconfigurable many-core and multi-core devices. As more, disparate devices enter the market, there is an increasing need for concepts, terminology, and classification techniques to understand the device tradeoffs. Additionally, computational performance, memory performance, and power metrics are needed to objectively compare devices. These metrics will assist application scientists in selecting the appropriate device early in the development cycle. This article presents a hierarchical taxonomy of computing devices, concepts and terminology describing reconfigurability, and computational density and internal memory bandwidth metrics to compare devices.",
    "cited_by_count": 50,
    "openalex_id": "https://openalex.org/W2076186064",
    "type": "article"
  },
  {
    "title": "Isolated WDDL",
    "doi": "https://doi.org/10.1145/1502781.1502784",
    "publication_date": "2009-03-01",
    "publication_year": 2009,
    "authors": "Robert P. McEvoy; Colin C. Murphy; William P. Marnane; Michael Tunstall",
    "corresponding_authors": "",
    "abstract": "Security protocols are frequently accelerated by implementing the underlying cryptographic functions in reconfigurable hardware. However, unprotected hardware implementations are susceptible to side-channel attacks, and Differential Power Analysis (DPA) has been shown to be especially powerful. In this work, we evaluate and compare the effectiveness of common hiding countermeasures against DPA in FPGA-based designs, using the Whirlpool hash function as a case study. In particular, we develop a new design flow called Isolated WDDL (IWDDL). In contrast with previous works, IWDDL isolates the direct and complementary circuit paths, and also provides DPA resistance in the Hamming distance power model. The analysis is supported using actual implementation results.",
    "cited_by_count": 48,
    "openalex_id": "https://openalex.org/W2001577466",
    "type": "article"
  },
  {
    "title": "Compressor tree synthesis on commercial high-performance FPGAs",
    "doi": "https://doi.org/10.1145/2068716.2068725",
    "publication_date": "2011-12-01",
    "publication_year": 2011,
    "authors": "Hadi Parandeh-Afshar; Arkosnato Neogy; Philip Brisk; Paolo Ienne",
    "corresponding_authors": "",
    "abstract": "Compressor trees are a class of circuits that generalizes multioperand addition and the partial product reduction trees of parallel multipliers using carry-save arithmetic. Compressor trees naturally occur in many DSP applications, such as FIR filters, and, in the more general case, their use can be maximized through the application of high-level transformations to arithmetically intensive data flow graphs. Due to the presence of carry-chains, it has long been thought that trees of 2- or 3-input carry-propagate adders are more efficient than compressor trees for FPGA synthesis; however, this is not the case. This article presents a heuristic for FPGA synthesis of compressor trees that outperforms adder trees and exploits carry-chains when possible. The experimental results show that, on average, the use of compressor trees can reduce critical path delay by 33% and 45% respectively, compared to adder trees synthesized on the Xilinx Virtex-5 and Altera Stratix III FPGAs.",
    "cited_by_count": 46,
    "openalex_id": "https://openalex.org/W2120474114",
    "type": "article"
  },
  {
    "title": "Portable and scalable FPGA-based acceleration of a direct linear system solver",
    "doi": "https://doi.org/10.1145/2133352.2133358",
    "publication_date": "2012-03-01",
    "publication_year": 2012,
    "authors": "Wei Zhang; Vaughn Betz; Jonathan Rose",
    "corresponding_authors": "",
    "abstract": "FPGAs have the potential to serve as a platform for accelerating many computations including scientific applications. However, the large development cost and short life span for FPGA designs have limited their adoption by the scientific computing community. FPGA-based scientific computing and many kinds of embedded computing could become more practical if there were hardware libraries that were portable to any FPGA-based system with performance that scaled with the size of the FPGA. To illustrate this idea we have implemented one common super-computing library function: the LU factorization method for solving systems of linear equations. This paper describes a method for making the design both portable and scalable that should be illustrative if such libraries are to be built in the future. The design is a software-based generator that leverages both the flexibility of a software programming language and the parameters inherent in an hardware description language. The generator accepts parameters that describe the FPGA capacity and external memory capabilities. We compare the performance of our engine executing on the largest FPGA available at the time of this work (an Altera Stratix III 3S340) to a single processor core fabricated in the same 65nm IC process running a highly optimized software implementation from the processor vendor. For single precision matrices on the order of 10,000 × 10,000 elements, the FPGA implementation is 2.2 times faster and the energy dissipated per useful GFLOP operation is a factor of 5 times less. For double precision, the FPGA implementation is 1.7 times faster and 3.5 times more energy efficient.",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W2159844683",
    "type": "article"
  },
  {
    "title": "On the exploitation of a high-throughput SHA-256 FPGA design for HMAC",
    "doi": "https://doi.org/10.1145/2133352.2133354",
    "publication_date": "2012-03-01",
    "publication_year": 2012,
    "authors": "Harris E. Michail; George S. Athanasiou; Vasilios Kelefouras; George Theodoridis; Costas E. Goutis",
    "corresponding_authors": "",
    "abstract": "High-throughput and area-efficient designs of hash functions and corresponding mechanisms for Message Authentication Codes (MACs) are in high demand due to new security protocols that have arisen and call for security services in every transmitted data packet. For instance, IPv6 incorporates the IPSec protocol for secure data transmission. However, the IPSec's performance bottleneck is the HMAC mechanism which is responsible for authenticating the transmitted data. HMAC's performance bottleneck in its turn is the underlying hash function. In this article a high-throughput and small-size SHA-256 hash function FPGA design and the corresponding HMAC FPGA design is presented. Advanced optimization techniques have been deployed leading to a SHA-256 hashing core which performs more than 30% better, compared to the next better design. This improvement is achieved both in terms of throughput as well as in terms of throughput/area cost factor. It is the first reported SHA-256 hashing core that exceeds 11Gbps (after place and route in Xilinx Virtex 6 board).",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W2162853544",
    "type": "article"
  },
  {
    "title": "The iDEA DSP Block-Based Soft Processor for FPGAs",
    "doi": "https://doi.org/10.1145/2629443",
    "publication_date": "2014-08-01",
    "publication_year": 2014,
    "authors": "Hui Yan Cheah; Fredrik Brosser; Suhaib A. Fahmy; Douglas L. Maskell",
    "corresponding_authors": "",
    "abstract": "DSP blocks in modern FPGAs can be used for a wide range of arithmetic functions, offering increased performance while saving logic resources for other uses. They have evolved to better support a plethora of signal processing tasks, meaning that in other application domains they may be underutilised. The DSP48E1 primitives in new Xilinx devices support dynamic programmability that can help extend their usefulness; the specific function of a DSP block can be modified on a cycle-by-cycle basis. However, the standard synthesis flow does not leverage this flexibility in the vast majority of cases. The lean DSP Extension Architecture (iDEA) presented in this article builds around the dynamic programmability of a single DSP48E1 primitive, with minimal additional logic to create a general-purpose processor supporting a full instruction-set architecture. The result is a very compact, fast processor that can execute a full gamut of general machine instructions. We show a number of simple applications compiled using an MIPS compiler and translated to the iDEA instruction set, comparing with a Xilinx MicroBlaze to show estimated performance figures. Being based on the DSP48E1, this processor can be deployed across next-generation Xilinx Artix-7, Kintex-7, Virtex-7, and Zynq families.",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W1970047515",
    "type": "article"
  },
  {
    "title": "Physical Security Evaluation of the Bitstream Encryption Mechanism of Altera Stratix II and Stratix III FPGAs",
    "doi": "https://doi.org/10.1145/2629462",
    "publication_date": "2014-12-15",
    "publication_year": 2014,
    "authors": "Pawel Swierczynski; Amir Moradi; David Oswald; Christof Paar",
    "corresponding_authors": "",
    "abstract": "To protect Field-Programmable Gate Array (FPGA) designs against Intellectual Property (IP) theft and related issues such as product cloning, all major FPGA manufacturers offer a mechanism to encrypt the bitstream that is used to configure the FPGA. From a mathematical point of view, the employed encryption algorithms (e.g., Advanced Encryption Standard (AES) or 3DES) are highly secure. However, it has been shown that the bitstream encryption feature of several FPGA families is susceptible to side-channel attacks based on measuring the power consumption of the cryptographic module. In this article, we present the first successful attack on the bitstream encryption of the Altera Stratix II and Stratix III FPGA families. To this end, we analyzed the Quartus II software and reverse engineered the details of the proprietary and unpublished schemes used for bitstream encryption on Stratix II and Stratix III. Using this knowledge, we demonstrate that the full 128-bit AES key of a Stratix II as well as the full 256-bit AES key of a Stratix III can be recovered by means of side-channel attacks. In both cases, the attack can be conducted in a few hours. The complete bitstream of these FPGAs that are (seemingly) protected by the bitstream encryption feature can hence fall into the hands of a competitor or criminal—possibly implying system-wide damage if confidential information such as proprietary encryption schemes or secret keys programmed into the FPGA are extracted. In addition to lost IP, reprogramming the attacked FPGA with modified code, for instance, to secretly plant a hardware Trojan, is a particularly dangerous scenario for many security-critical applications.",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W1995447946",
    "type": "article"
  },
  {
    "title": "The Effect of Compiler Optimizations on High-Level Synthesis-Generated Hardware",
    "doi": "https://doi.org/10.1145/2629547",
    "publication_date": "2015-05-11",
    "publication_year": 2015,
    "authors": "Qijing Huang; Ruolong Lian; Andrew Canis; Jongsok Choi; Ryan Xi; Nazanin Calagar; Stephen D. Brown; Jason H. Anderson",
    "corresponding_authors": "",
    "abstract": "We consider the impact of compiler optimizations on the quality of high-level synthesis (HLS)-generated field-programmable gate array (FPGA) hardware. Using an HLS tool implemented within the state-of-the-art LLVM compiler, we study the effect of compiler optimizations on the hardware metrics of circuit area, execution cycles, FMax , and wall-clock time. We evaluate 56 different compiler optimizations implemented within LLVM and show that some optimizations significantly affect hardware quality. Moreover, we show that hardware quality is also affected by some optimization parameter values, as well as the order in which optimizations are applied. We then present a new HLS-directed approach to compiler optimizations, wherein we execute partial HLS and profiling at intermittent points in the optimization process and use the results to judiciously undo the impact of optimization passes predicted to be damaging to the generated hardware quality. Results show that our approach produces circuits with 16% better speed performance, on average, versus using the standard -O3 optimization level.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W1884563702",
    "type": "article"
  },
  {
    "title": "FPGA Logic Block Architectures for Efficient Deep Learning Inference",
    "doi": "https://doi.org/10.1145/3393668",
    "publication_date": "2020-06-03",
    "publication_year": 2020,
    "authors": "Mohamed Eldafrawy; Andrew Boutros; Sadegh Yazdanshenas; Vaughn Betz",
    "corresponding_authors": "",
    "abstract": "Reducing the precision of deep neural network (DNN) inference accelerators can yield large efficiency gains with little or no accuracy degradation compared to half or single precision floating-point by enabling more multiplication operations per unit area. A wide range of precisions fall on the pareto-optimal curve of hardware efficiency vs. accuracy with no single precision dominating, making the variable precision capabilities of FPGAs very valuable. We propose three types of logic block architectural enhancements and fully evaluate a total of six architectures that improve the area efficiency of multiplications and additions implemented in the soft fabric. Increasing the LUT fracturability and adding two adders to the ALM (4-bit Adder Double Chain architecture) leads to a 1.5× area reduction for arithmetic heavy machine learning (ML) kernels, while increasing their speed. In addition, this architecture also reduces the logic area of general applications by 6%, while increasing the critical path delay by only 1%. However, our highest impact option, which adds a 9-bit shadow multiplier to the logic clusters, reduces the area and critical path delay of ML kernels by 2.4× and 1.2×, respectively. These large gains come at a cost of 15% logic area increase for general applications.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W3033506121",
    "type": "article"
  },
  {
    "title": "FOS",
    "doi": "https://doi.org/10.1145/3405794",
    "publication_date": "2020-09-09",
    "publication_year": 2020,
    "authors": "Anuj Vaishnav; Khoa Dang Pham; Joseph Powell; Dirk Koch",
    "corresponding_authors": "",
    "abstract": "With FPGAs now being deployed in the cloud and at the edge, there is a need for scalable design methods that can incorporate the heterogeneity present in the hardware and software components of FPGA systems. Moreover, these FPGA systems need to be maintainable and adaptable to changing workloads while improving accessibility for the application developers. However, current FPGA systems fail to achieve modularity and support for multi-tenancy due to dependencies between system components and the lack of standardised abstraction layers. To solve this, we introduce a modular FPGA operating system – FOS, which adopts a modular FPGA development flow to allow each system component to be changed and be agnostic to the heterogeneity of EDA tool versions, hardware and software layers. Further, to dynamically maximise the utilisation transparently from the users, FOS employs resource-elastic scheduling to arbitrate the FPGA resources in both time and spatial domain for any type of accelerators. Our evaluation on different FPGA boards shows that FOS can provide performance improvements in both single-tenant and multi-tenant environments while substantially reducing the development time and, at the same time, improving flexibility.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W3085331912",
    "type": "article"
  },
  {
    "title": "Programming and Synthesis for Software-defined FPGA Acceleration: Status and Future Prospects",
    "doi": "https://doi.org/10.1145/3469660",
    "publication_date": "2021-09-13",
    "publication_year": 2021,
    "authors": "Yi‐Hsiang Lai; Ecenur Ustun; Shaojie Xiang; Zhenman Fang; Hongbo Rong; Zhiru Zhang",
    "corresponding_authors": "",
    "abstract": "FPGA-based accelerators are increasingly popular across a broad range of applications, because they offer massive parallelism, high energy efficiency, and great flexibility for customizations. However, difficulties in programming and integrating FPGAs have hindered their widespread adoption. Since the mid 2000s, there has been extensive research and development toward making FPGAs accessible to software-inclined developers, besides hardware specialists. Many programming models and automated synthesis tools, such as high-level synthesis, have been proposed to tackle this grand challenge. In this survey, we describe the progression and future prospects of the ongoing journey in significantly improving the software programmability of FPGAs. We first provide a taxonomy of the essential techniques for building a high-performance FPGA accelerator, which requires customizations of the compute engines, memory hierarchy, and data representations. We then summarize a rich spectrum of work on programming abstractions and optimizing compilers that provide different trade-offs between performance and productivity. Finally, we highlight several additional challenges and opportunities that deserve extra attention by the community to bring FPGA-based computing to the masses.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W3200826900",
    "type": "article"
  },
  {
    "title": "Low-precision Floating-point Arithmetic for High-performance FPGA-based CNN Acceleration",
    "doi": "https://doi.org/10.1145/3474597",
    "publication_date": "2021-11-09",
    "publication_year": 2021,
    "authors": "Chen Wu; Mingyu Wang; Xinyuan Chu; Kun Wang; Lei He",
    "corresponding_authors": "",
    "abstract": "Low-precision data representation is important to reduce storage size and memory access for convolutional neural networks (CNNs). Yet, existing methods have two major limitations: (1) requiring re-training to maintain accuracy for deep CNNs and (2) needing 16-bit floating-point or 8-bit fixed-point for a good accuracy. In this article, we propose a low-precision (8-bit) floating-point (LPFP) quantization method for FPGA-based acceleration to overcome the above limitations. Without any re-training, LPFP finds an optimal 8-bit data representation with negligible top-1/top-5 accuracy loss (within 0.5%/0.3% in our experiments, respectively, and significantly better than existing methods for deep CNNs). Furthermore, we implement one 8-bit LPFP multiplication by one 4-bit multiply-adder and one 3-bit adder, and therefore implement four 8-bit LPFP multiplications using one DSP48E1 of Xilinx Kintex-7 family or DSP48E2 of Xilinx Ultrascale/Ultrascale+ family, whereas one DSP can implement only two 8-bit fixed-point multiplications. Experiments on six typical CNNs for inference show that on average, we improve throughput by over existing FPGA accelerators. Particularly for VGG16 and YOLO, compared to six recent FPGA accelerators, we improve average throughput by 3.5 and 27.5 and average throughput per DSP by 4.1 and 5 , respectively.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W3212505503",
    "type": "article"
  },
  {
    "title": "NASCENT2: Generic Near-Storage Sort Accelerator for Data Analytics on SmartSSD",
    "doi": "https://doi.org/10.1145/3472769",
    "publication_date": "2022-01-28",
    "publication_year": 2022,
    "authors": "Sahand Salamat; Hui Zhang; Yang Seok Ki; Tajana Rosing",
    "corresponding_authors": "",
    "abstract": "As the size of data generated every day grows dramatically, the computational bottleneck of computer systems has shifted toward storage devices. The interface between the storage and the computational platforms has become the main limitation due to its limited bandwidth, which does not scale when the number of storage devices increases. Interconnect networks do not provide simultaneous access to all storage devices and thus limit the performance of the system when executing independent operations on different storage devices. Offloading the computations to the storage devices eliminates the burden of data transfer from the interconnects. Near-storage computing offloads a portion of computations to the storage devices to accelerate big data applications. In this article, we propose a generic near-storage sort accelerator for data analytics, NASCENT2, which utilizes Samsung SmartSSD, an NVMe flash drive with an on-board FPGA chip that processes data in situ. NASCENT2 consists of dictionary decoder, sort, and shuffle FPGA-based accelerators to support sorting database tables based on a key column with any arbitrary data type. It exploits data partitioning applied by data processing management systems, such as SparkSQL, to breakdown the sort operations on colossal tables to multiple sort operations on smaller tables. NASCENT2 generic sort provides 2 × speedup and 15.2 × energy efficiency improvement as compared to the CPU baseline. It moreover considers the specifications of the SmartSSD (e.g., the FPGA resources, interconnect network, and solid-state drive bandwidth) to increase the scalability of computer systems as the number of storage devices increases. With 12 SmartSSDs, NASCENT2 is 9.9× (137.2 ×) faster and 7.3 × (119.2 ×) more energy efficient in sorting the largest tables of TPCC and TPCH benchmarks than the FPGA (CPU) baseline.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W4210675237",
    "type": "article"
  },
  {
    "title": "AutoScaleDSE: A Scalable Design Space Exploration Engine for High-Level Synthesis",
    "doi": "https://doi.org/10.1145/3572959",
    "publication_date": "2023-02-15",
    "publication_year": 2023,
    "authors": "Hyegang Jun; Hanchen Ye; Hyunmin Jeong; Deming Chen",
    "corresponding_authors": "",
    "abstract": "High-Level Synthesis (HLS) has enabled users to rapidly develop designs targeted for FPGAs from the behavioral description of the design. However, to synthesize an optimal design capable of taking better advantage of the target FPGA, a considerable amount of effort is needed to transform the initial behavioral description into a form that can capture the desired level of parallelism. Thus, a design space exploration (DSE) engine capable of optimizing large complex designs is needed to achieve this goal. We present a new DSE engine capable of considering code transformation, compiler directives (pragmas), and the compatibility of these optimizations. To accomplish this, we initially express the structure of the input code as a graph to guide the exploration process. To appropriately transform the code, we take advantage of ScaleHLS based on the multi-level compiler infrastructure (MLIR). Finally, we identify problems that limit the scalability of existing DSEs, which we name the “design space merging problem.” We address this issue by employing a Random Forest classifier that can successfully decrease the number of invalid design points without invoking the HLS compiler as a validation tool. We evaluated our DSE engine against the ScaleHLS DSE, outperforming it by a maximum of 59×. We additionally demonstrate the scalability of our design by applying our DSE to large-scale HLS designs, achieving a maximum speedup of 12× for the benchmarks in the MachSuite and Rodinia set.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4320891761",
    "type": "article"
  },
  {
    "title": "Design and Implementation of Hardware-Software Architecture Based on Hashes for SPHINCS+",
    "doi": "https://doi.org/10.1145/3653459",
    "publication_date": "2024-03-27",
    "publication_year": 2024,
    "authors": "Jonathan Lopez-Valdivieso; René Cumplido",
    "corresponding_authors": "",
    "abstract": "Advances in quantum computing have posed a future threat to today’s cryptography. With the advent of these quantum computers, security could be compromised. Therefore, the National Institute of Standards and Technology (NIST) has issued a request for proposals to standardize algorithms for post-quantum cryptography (PQC), which is considered difficult to solve for both classical and quantum computers. Among the proposed technologies, the most popular choices are lattice-based (shortest vector problem) and hash-based approaches. Other important categories are public key cryptography (PKE) and digital signatures. Within the realm of digital signatures lies SPHINCS+. However, there are few implementations of this scheme in hardware architectures. In this article, we present a hardware-software architecture for the SPHINCS+ scheme. We utilized a free RISC-V (Reduced Instruction Set Computer) processor synthesized on a Field Programmable Gate Array (FPGA), primarily integrating two accelerator modules for Keccak-1600 and the Haraka hash function. Additionally, modifications were made to the processor to accommodate the execution of these added modules. Our implementation yielded a 15-fold increase in performance with the SHAKE-256 function and nearly 90-fold improvement when using Haraka, compared to the reference software. Moreover, it is more compact compared to related works. This implementation was realized on a Xilinx FPGA Arty S7: Spartan-7.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4393224433",
    "type": "article"
  },
  {
    "title": "A Systematic Review of Fast, Scalable and Efficient Hardware Implementations of Elliptic Curve Cryptography for Blockchain",
    "doi": "https://doi.org/10.1145/3696422",
    "publication_date": "2024-09-19",
    "publication_year": 2024,
    "authors": "Rareș-Cristian Ifrim; Dumitrel Loghin; Decebal Popescu",
    "corresponding_authors": "",
    "abstract": "Blockchain technology entered the enterprise domain under the name of permissioned blockchains and hybrid or verifiable database systems, as they provide a distributed solution that allows multiple distrusting parties to share common information. One drawback of these systems is the overhead added by the cryptographic functions which impacts the throughput in terms of transactions per second and increases the latency of transaction processing. Many of the cryptographic functions and protocols used in blockchains are based on Elliptic Curve Cryptography (ECC). Unfortunately, ECC operations such as modulo inverse or scalar point multiplication have considerable latency which causes the slowdown of the entire system. In such situations, reconfigurable computing architectures, such as FPGAs, can be used to offload these tasks to overcome the performance loss. This survey analyzes the current state-of-the-art designs and implementations of ECC from a hardware perspective. We use a PRISMA-based approach to filter recent publications and to reduce their number from over 16,000 to only 43 highly relevant designs. In the end, we show that very few designs are able to fulfill all three properties of high performance, scalability, and efficiency.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4402637784",
    "type": "review"
  },
  {
    "title": "Productively Generating a High-Performance Linear Algebra Library on FPGAs",
    "doi": "https://doi.org/10.1145/3723046",
    "publication_date": "2025-03-11",
    "publication_year": 2025,
    "authors": "X. Q. Hao; Mingzhe Zhang; Ce Sun; Zhuofu Tao; Hongbo Rong; Yu Zhang; Lei He; Eric Petit; Wenguang Chen; Yun Liang",
    "corresponding_authors": "",
    "abstract": "Linear algebra computations can be greatly accelerated using spatial accelerators on FPGAs. As a standard building block of linear algebra applications, BLAS covers a wide range of compute patterns that vary vastly in data reuse, bottleneck resources, matrix storage layouts, and data types. However, existing implementations of BLAS routines on FPGAs are stuck in the dilemma of productivity and performance. They either require extensive human effort or fail to leverage the properties of routines for acceleration. We introduce Lasa, a framework composed of a programming model and a compiler, designed to address the dilemma by abstracting (for productivity) and specializing (for performance) the architecture of a spatial accelerator. The programming model realizes systolic arrays using uniform recurrence equations and space-time transforms. Streaming tensors, an intuitive dataflow-style abstraction, is proposed to uniformly describe the movement, storage, and transpose of input and output data across the spatial components. According to streaming tensors, a customized memory hierarchy is automatically built on an FPGA by our compiler. The compiler further specializes the architecture with transparent optimizations on FPGAs. Using this framework, we develop a complete BLAS library, demonstrating performance in parity with expert-written HLS code for BLAS level 3 routines, 76%-94% machine peak for level 1 and 2 routines, and 1.6X-13X speedup by leveraging the matrix properties such as symmetry, triangularity, and bandness.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4408327158",
    "type": "article"
  },
  {
    "title": "Rock the <u>QASBA</u> : <u>Q</u> uantum Error Correction <u>A</u> cceleration via the <u>S</u> parse <u>B</u> lossom <u>A</u> lgorithm on FPGAs",
    "doi": "https://doi.org/10.1145/3723168",
    "publication_date": "2025-03-18",
    "publication_year": 2025,
    "authors": "M. Vénere; Beatrice Branchini; Davide Conficconi; Donatella Sciuto; Marco D. Santambrogio",
    "corresponding_authors": "",
    "abstract": "Quantum computing is a new paradigm of computation that exploits principles from quantum mechanics to achieve an exponential speedup compared to classical logic. However, noise strongly limits current quantum hardware, reducing achievable performance and limiting the scaling of the applications. For this reason, current noisy intermediate-scale quantum devices require Quantum Error Correction (QEC) mechanisms to identify errors occurring in the computation and correct them in real time. Nevertheless, the high computational complexity of QEC algorithms is incompatible with the tight time constraints of quantum devices. Thus, hardware acceleration is paramount to achieving real-time QEC. This work presents QASBA, an FPGA-based hardware accelerator for the Sparse Blossom Algorithm (SBA), a state-of-the-art decoding algorithm. After profiling the state-of-the-art software counterpart, we developed a design methodology for hardware development based on the SBA. We also devised an automation process to help users without expertise in hardware design in deploying architectures based on QASBA. We implement QASBA on different FPGA architectures and experimentally evaluate resource usage, execution time, and energy efficiency of our solution. Our solution attains up to 25.05× speedup and 304.16× improvement in energy efficiency compared to the software baseline.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4408561484",
    "type": "article"
  },
  {
    "title": "DA-VinCi: A Deeplearning Accelerator Overlay using in-Memory Computing",
    "doi": "https://doi.org/10.1145/3770756",
    "publication_date": "2025-10-07",
    "publication_year": 2025,
    "authors": "MD Arafat Kabir; Nathaniel Fredricks; Tendayi Kamucheka; Joel Mandebi Mbongue; Miaoqing Huang; Jason D. Bakos; David Andrews",
    "corresponding_authors": "",
    "abstract": "The matrix operations that underpin today’s deep learning models are routinely implemented in SIMD domain specific accelerators. [1–19]. SIMD accelerators including GPUs and array processors can effectively leverage parallelism in models that are compute-bound, but their effectiveness can be diminished for models that are memory-bound. Processing-in-Memory (PIM) architectures are being explored to provide better energy efficiency and scalable performance for these memory-bound models [20–33]. Modern Field Programmable Gate Arrays (FPGAs) feature hundreds of megabits of SRAM distributed across the device as disaggregated memory resources. This makes FPGAs ideal programmable platforms for developing custom Processor In/Near Memory accelerators. Several PIM array-based accelerator designs [24–31] have been proposed to leverage this substantial internal bandwidth. However, results reported to date show the FPGA based PIM architectures operating at system clock frequencies well below a chips BRAM Fmax clock frequency. Results also show that the compute densities of the designs do not scale linearly with BRAM densities. These results indicate that FPGA PIM architectures will never be competitive with their custom Application-Specific Integrated Circuit (ASIC) counterparts. In this paper, we introduce DA-VinCi, a D eeplearning A ccelerator O v erlay using in -Memory C omput i ng. DA-VinCi is the first scalable FPGA based PIM deep-learning accelerator overlay capable of clocking at the maximum frequency of a device’s BRAM. Further, the architecture of DA-VinCi allows the number of compute units to scale linearly up to the maximum capacity of a devices BRAM, and at the maximum clock frequency of the BRAM. The DA-VinCi overlay has a programmable Instruction Set Architecture (ISA) that allows the same synthesized design to provide low-latency inferencing of a range of memory-bound deep-learning models, including MLP, RNN, LSTM, and GRU networks. The scalability and high clocking frequency of DA-VinCi is achieved through a new Processor In Memory (PIM) Tile architecture and a highly scalable system-level framework. We present results showing DA-VinCi linearly scaling the number of PEs to 100% of the BRAM capacity (over 60K PEs) on an Alveo U55 clocking at 737 MHz, the chips BRAM Fmax. We provide comparative studies on inference latency across multiple deep-learning applications that show DA-VinCi achieves up to a 201× improvement over a state-of-the-art PIM overlay accelerator, up to 87× improvement over existing PIM-based FPGA accelerators, and up to 57× improvement over custom deep-learning accelerators on FPGAs.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4414913085",
    "type": "article"
  },
  {
    "title": "Exploiting Partial Runtime Reconfiguration for High-Performance Reconfigurable Computing",
    "doi": "https://doi.org/10.1145/1462586.1462590",
    "publication_date": "2009-01-01",
    "publication_year": 2009,
    "authors": "Esam El‐Araby; Iván González; Tarek El‐Ghazawi",
    "corresponding_authors": "",
    "abstract": "Runtime Reconfiguration (RTR) has been traditionally utilized as a means for exploiting the flexibility of High-Performance Reconfigurable Computers (HPRCs). However, the RTR feature comes with the cost of high configuration overhead which might negatively impact the overall performance. Currently, modern FPGAs have more advanced mechanisms for reducing the configuration overheads, particularly Partial Runtime Reconfiguration (PRTR). It has been perceived that PRTR on HPRC systems can be the trend for improving the performance. In this work, we will investigate the potential of PRTR on HPRC by formally analyzing the execution model and experimentally verifying our analytical findings by enabling PRTR for the first time, to the best of our knowledge, on one of the current HPRC systems, Cray XD1. Our approach is general and can be applied to any of the available HPRC systems. The paper will conclude with recommendations and conditions, based on our conceptual and experimental work, for the optimal utilization of PRTR as well as possible future usage in HPRC.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W2063534452",
    "type": "article"
  },
  {
    "title": "FPGA-Based Hardware Acceleration of Lithographic Aerial Image Simulation",
    "doi": "https://doi.org/10.1145/1575774.1575776",
    "publication_date": "2009-09-01",
    "publication_year": 2009,
    "authors": "Jason Cong; Yi Zou",
    "corresponding_authors": "",
    "abstract": "Lithography simulation, an essential step in design for manufacturability (DFM), is still far from computationally efficient. Most leading companies use large clusters of server computers to achieve acceptable turn-around time. Thus coprocessor acceleration is very attractive for obtaining increased computational performance with a reduced power consumption. This article describes the implementation of a customized accelerator on FPGA using a polygon-based simulation model. An application-specific memory partitioning scheme is designed to meet the bandwidth requirements for a large number of processing elements. Deep loop pipelining and ping-pong buffer based function block pipelining are also implemented in our design. Initial results show a 15X speedup versus the software implementation running on a microprocessor, and more speedup is expected via further performance tuning. The implementation also leverages state-of-art C-to-RTL synthesis tools. At the same time, we also identify the need for manual architecture-level exploration for parallel implementations. Moreover, we implement the algorithm on NVIDIA GPUs using the CUDA programming environment, and provide some useful comparisons for different kinds of accelerators.",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W2031135506",
    "type": "article"
  },
  {
    "title": "MPI as a Programming Model for High-Performance Reconfigurable Computers",
    "doi": "https://doi.org/10.1145/1862648.1862652",
    "publication_date": "2010-11-01",
    "publication_year": 2010,
    "authors": "Manuel Saldaña; Arun Singh Patel; Christopher Madill; D. C. Nunes; Danyao Wang; Paul Chow; Ralph Wittig; Henry Styles; Andrew Putnam",
    "corresponding_authors": "",
    "abstract": "High-Performance Reconfigurable Computers (HPRCs) consist of one or more standard microprocessors tightly-coupled with one or more reconfigurable FPGAs. HPRCs have been shown to provide good speedups and good cost/performance ratios, but not necessarily ease of use, leading to a slow acceptance of this technology. HPRCs introduce new design challenges, such as the lack of portability across platforms, incompatibilities with legacy code, users reluctant to change their code base, a prolonged learning curve, and the need for a system-level Hardware/Software co-design development flow. This article presents the evolution and current work on TMD-MPI, which started as an MPI-based programming model for Multiprocessor Systems-on-Chip implemented in FPGAs, and has now evolved to include multiple X86 processors. TMD-MPI is shown to address current design challenges in HPRC usage, suggesting that the MPI standard has enough syntax and semantics to program these new types of parallel architectures. Also presented is the TMD-MPI Ecosystem , which consists of research projects and tools that are developed around TMD-MPI to further improve HPRC usability. Finally, we present preliminary communication performance measurements.",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W1982516565",
    "type": "article"
  },
  {
    "title": "Placement and Floorplanning in Dynamically Reconfigurable FPGAs",
    "doi": "https://doi.org/10.1145/1862648.1862654",
    "publication_date": "2010-11-01",
    "publication_year": 2010,
    "authors": "A. Montone; Marco D. Santambrogio; Donatella Sciuto; Seda Öǧrenci Memik",
    "corresponding_authors": "",
    "abstract": "The aim of this article is to describe a complete partitioning and floorplanning algorithm tailored for reconfigurable architectures deployable on FPGAs and considering communication infrastructure feasibility. This article proposes a novel approach for resource- and reconfiguration- aware floorplanning. Different from existing approaches, our floorplanning algorithm takes specific physical constraints such as resource distribution and the granularity of reconfiguration possible for a given FPGA device into account. Due to the introduction of constraints typical of other problems like partitioning and placement, the proposed approach is named floorplacer in order to underline the great differences with respect to traditional floorplanners. These physical constraints are typically considered at the later placement stage. Different aspects of the problems have been described, focusing particularly on the FPGAs resource heterogeneity and the temporal dimension typical of reconfigurable systems. Once the problem is introduced a comparison among related works has been provided and their limits have been pointed out. Experimental results proved the validity of the proposed approach.",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W2070215512",
    "type": "article"
  },
  {
    "title": "Fast and Accurate Stereo Vision System on FPGA",
    "doi": "https://doi.org/10.1145/2567659",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "Minxi Jin; Tsutomu Maruyama",
    "corresponding_authors": "",
    "abstract": "In this article, we present a fast and high quality stereo matching algorithm on FPGA using cost aggregation (CA) and fast locally consistent (FLC) dense stereo. In many software programs, global matching algorithms are used in order to obtain accurate disparity maps. Although their error rates are considerably low, their processing speeds are far from that required for real-time processing because of their complex processing sequences. In order to realize real-time processing, many hardware systems have been proposed to date. They have achieved considerably high processing speeds; however, their error rates are not as good as those of software programs, because simple local matching algorithms have been widely used in those systems. In our system, sophisticated local matching algorithms (CA and FLC) that are suitable for FPGA implementation are used to achieve low error rate while maintaining the high processing speed. We evaluate the performance of our circuit on Xilinx Vertex-6 FPGAs. Its error rate is comparable to that of top-level software algorithms, and its processing speed is nearly 2 clock cycles per pixel, which reaches 507.9 fps for 640 480 pixel images.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W1988745519",
    "type": "article"
  },
  {
    "title": "A Fully Pipelined FPGA Architecture of a Factored Restricted Boltzmann Machine Artificial Neural Network",
    "doi": "https://doi.org/10.1145/2539125",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "Lok-Won Kim; Sameh Asaad; Ralph Linsker",
    "corresponding_authors": "",
    "abstract": "Artificial neural networks (ANNs) are a natural target for hardware acceleration by FPGAs and GPGPUs because commercial-scale applications can require days to weeks to train using CPUs, and the algorithms are highly parallelizable. Previous work on FPGAs has shown how hardware parallelism can be used to accelerate a “Restricted Boltzmann Machine” (RBM) ANN algorithm, and how to distribute computation across multiple FPGAs. Here we describe a fully pipelined parallel architecture that exploits “mini-batch” training (combining many input cases to compute each set of weight updates) to further accelerate ANN training. We implement on an FPGA, for the first time to our knowledge, a more powerful variant of the basic RBM, the “Factored RBM” (fRBM). The fRBM has proved valuable in learning transformations and in discovering features that are present across multiple types of input. We obtain (in simulation) a 100-fold acceleration (vs. CPU software) for an fRBM having N = 256 units in each of its four groups (two input, one output, one intermediate group of units) running on a Virtex-6 LX760 FPGA. Many of the architectural features we implement are applicable not only to fRBMs, but to basic RBMs and other ANN algorithms more broadly.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W2171318387",
    "type": "article"
  },
  {
    "title": "Remote FPGA Lab for Enhancing Learning of Digital Systems",
    "doi": "https://doi.org/10.1145/2362374.2362382",
    "publication_date": "2012-10-01",
    "publication_year": 2012,
    "authors": "Fearghal Morgan; Seamus Cawley; D. J. Newell",
    "corresponding_authors": "",
    "abstract": "Learning in digital systems can be enhanced through applying a learn-by-doing approach on practical hardware systems and by using Web-based technology to visualize and animate hardware behavior. The authors have reported the Web-based Remote FPGA Lab (RFL) which provides a novel, real-time control and visualization interface to a remote, always-on FPGA hardware implementation. The RFL helps students to understand and reason about digital systems operation, using interactive animation of signal behavior in an executing digital logic system, at any level of the design hierarchy. The RFL supports the creation of real-time interactive digital systems teaching demos. The article presents student RFL usage data and survey data which highlight improved student engagement, learning and achievement. The article describes the RFL architecture, communication interface, Web page functionality, user access administration and database management. The article also describes the RFLGen program, developed to automate user design integration into the Xilinx ISE VHDL-based RFL project wrapper for creation of FPGA configuration bitstreams and RFL animations.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2011792419",
    "type": "article"
  },
  {
    "title": "Implementing Curve25519 for Side-Channel--Protected Elliptic Curve Cryptography",
    "doi": "https://doi.org/10.1145/2700834",
    "publication_date": "2015-11-04",
    "publication_year": 2015,
    "authors": "Pascal Sasdrich; Tim Güneysu",
    "corresponding_authors": "",
    "abstract": "For security-critical embedded applications Elliptic Curve Cryptography (ECC) has become the predominant cryptographic system for efficient key agreement and digital signatures. However, ECC still involves complex modular arithmetic that is a particular burden for small processors. In this context, Bernstein proposed the highly efficient ECC instance Curve25519 that particularly enables efficient software implementations at a security level comparable to AES-128 with inherent resistance to simple power analysis (SPA) and timing attacks. In this work, we show that Curve25519 is likewise competitive on FPGAs even when countermeasures to thwart side-channel power analysis are included. Our basic multicore DSP-based architectures achieves a maximal performance of more than 32,000 point multiplications per second on a Xilinx Zynq 7020 FPGA. Including a mix of side-channel countermeasures to impede simple and differential power analysis, we still achieve more than 27,500 point multiplications per second with a moderate increase in logic resources.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2294895936",
    "type": "article"
  },
  {
    "title": "Hoplite",
    "doi": "https://doi.org/10.1145/3027486",
    "publication_date": "2017-03-22",
    "publication_year": 2017,
    "authors": "Nachiket Kapre; Jan Gray",
    "corresponding_authors": "",
    "abstract": "We can design an FPGA-optimized lightweight network-on-chip (NoC) router for flit-oriented packet-switched communication that is an order of magnitude smaller (in terms of LUTs and FFs) than state-of-the-art FPGA overlay routers available today. We present Hoplite, an efficient, lightweight, and fast FPGA overlay NoC that is designed to be small and compact by (1) using deflection routing instead of buffered switching to eliminate expensive FIFO buffers and (2) using a torus topology to reduce the cost of switch crossbar. Buffering and crossbar implementation complexities have traditionally limited speeds and imposed heavy resource costs in conventional FPGA overlay NoCs. We take care to exploit the fracturable lookup tables (LUT) organization of the FPGA to further improve the resource efficiency of mapping the expensive crossbar multiplexers. Hoplite can outperform classic, bidirectional, buffered mesh networks for single-flit-oriented FPGA applications by as much as 1.5 × (best achievable throughputs for a 10 × 10 system) or 2.5 × (allocating same amount of FPGA resources to both NoCs) for uniform random traffic. When compared to buffered mesh switches, FPGA-based deflection routers are ≈ 3.5 × smaller (HLS-generated switch) and 2.5 × faster (clock period) for 32b payloads. In a separate experiment, we hand-crafted an RTL version of our switch with location constraints that requires only 60 LUTs and 100 FFs per router and runs at 2.9ns. We conduct additional layout experiments on modern Xilinx and Altera FPGAs and demonstrate wide-channel chip-spanning layouts that run in excess of 300MHz while consuming 10--15% of overall chip resources. We also demonstrate a clustered RISC-V multiprocessor organization that uses Hoplite to help deliver the high processing throughputs of the FPGA architecture to user applications.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2600117321",
    "type": "article"
  },
  {
    "title": "The Cibola Flight Experiment",
    "doi": "https://doi.org/10.1145/2629556",
    "publication_date": "2015-03-06",
    "publication_year": 2015,
    "authors": "Heather Quinn; D. Roussel-Dupré; M. Caffrey; Paul Graham; Michael Wirthlin; Keith Morgan; Anthony Salazar; Tony Nelson; Will Howes; Eric K. Johnson; Jon Johnson; Brian Pratt; Nathan Rollins; Jim Krone",
    "corresponding_authors": "",
    "abstract": "Over the past 15 years many organizations have researched the use of Static-Random Access Memory (SRAM)-based Field-Programmable Gate Arrays (FPGAs) in space. Although the components can provide a performance improvement over radiation-hardened processing components, random soft errors can occur from the naturally occurring space radiation environment. Many organizations have been developing methods for characterizing, emulating, and simulating radiation-induced events; mitigating and removing radiation-induced computational errors; and designing fault-tolerant reconfigurable spacecraft. Los Alamos National Laboratory has fielded one of the longest space-based FPGAs experiments, called the Cibola Flight Experiment (CFE), using Xilinx Virtex FPGAs. CFE has successfully deployed commercial SRAM FPGAs into a low-Earth orbit with Single-Event Upset (SEU) mitigation and was able to exploit effectively the reconfigurability and customization of FPGAs in a harsh radiation environment. Although older than current state-of-the-art FPGAs, these same concepts are used to deploy newer FPGA-based space systems since the launch of the CFE satellite and will continue to be useful for newer systems. In this article, we present how the system was designed to be fault tolerant, prelaunch predictions of expected on-orbit behaviors, and on-orbit results.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2093412576",
    "type": "article"
  },
  {
    "title": "Dynamic Power and Thermal Management of NoC-Based Heterogeneous MPSoCs",
    "doi": "https://doi.org/10.1145/2567658",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "George Kornaros; Dionisios Pnevmatikatos",
    "corresponding_authors": "",
    "abstract": "Advances in silicon process technology have made it possible to include multiple processor cores on a single die. Billion transistor architectures usually in the form of networks-on-chip present a wide range of challenges in design, microarchitecture, and algorithmic levels with significant impact to system performance and power consumption. In this article, we propose efficient methods and mechanisms that exploit a heterogeneous network-on-chip (NoC) to achieve a power- and thermal-aware coherent system. To this end, we utilize different management techniques which employ dynamic frequency scaling circuitry and power and temperature sensors per node to achieve real-time workload prediction and allocation at node and system level by low-cost threads. The developed heterogeneous multicoprocessing infrastructure is utilized to evaluate diverse policies for power-aware computing in terms of effectiveness and in relation to distributed sensor-conscious management. The proposed reconfigurable architecture supports coprocessor accelerators per node, monitors the program’s power profile on-the-fly, and balances power and thermal behavior at the NoC level. Overall, these techniques form a system exploration methodology using a multi-FPGA emulation platform showing a minimum complexity overhead.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W2015835319",
    "type": "article"
  },
  {
    "title": "Voltage Sensor Implementations for Remote Power Attacks on FPGAs",
    "doi": "https://doi.org/10.1145/3555048",
    "publication_date": "2022-08-08",
    "publication_year": 2022,
    "authors": "Shayan Moini; Aleksa Deric; Xiang Li; George Provelengios; Wayne Burleson; Russell Tessier; Daniel Holcomb",
    "corresponding_authors": "",
    "abstract": "This article presents a study of two types of on-chip FPGA voltage sensors based on ring oscillators (ROs) and time-to-digital converter (TDCs), respectively. It has previously been shown that these sensors are often used to extract side-channel information from FPGAs without physical access. The performance of the sensors is evaluated in the presence of circuits that deliberately waste power, resulting in localized voltage drops. The effects of FPGA power supply features and sensor sensitivity in detecting voltage drops in an FPGA power distribution network (PDN) are evaluated for Xilinx Artix-7, Zynq 7000, and Zynq UltraScale+ FPGAs. We show that both sensor types are able to detect supply voltage drops, and that their measurements are consistent with each other. Our findings show that TDC-based sensors are more sensitive and can detect voltage drops that are shorter in duration, while RO sensors are easier to implement because calibration is not required. Furthermore, we present a new time-interleaved TDC design that sweeps the sensor phase. The new sensor generates data that can reconstruct voltage transients on the order of tens of picoseconds.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W4290648676",
    "type": "article"
  },
  {
    "title": "Constraint Programming Approach to Reconfigurable Processor Extension Generation and Application Compilation",
    "doi": "https://doi.org/10.1145/2209285.2209289",
    "publication_date": "2012-06-01",
    "publication_year": 2012,
    "authors": "Kévin Martin; Christophe Wolinski; Krzysztof Kuchciński; Antoine Floch; François Charot",
    "corresponding_authors": "",
    "abstract": "In this article, we present a constraint programming approach for solving hard design problems present when automatically designing specialized processor extensions. Specifically, we discuss our approach for automatic selection and synthesis of processor extensions as well as efficient application compilation for these newly generated extensions. The discussed approach is implemented in our integrated design framework, IFPEC , built using Constraint Programming (CP). In our framework, custom instructions, implemented as processor extensions, are defined as computational patterns and represented as graphs. This, along with the graph representation of an application, provides a way to use our CP framework equipped with subgraph isomorphism and connected component constraints for identification of processor extensions as well as their selection, application scheduling, binding, and routing. All design steps assume architectures composed of runtime reconfigurable cells, implementing selected extensions, tightly connected to a processor. An advantage of our approach is the possibility of combining different heterogeneous constraints to represent and solve all our design problems. Moreover, the flexibility and expressiveness of the CP framework makes it possible to solve simultaneously extension selection, application scheduling, and binding and improve the quality of the generated results. The article is largely illustrated with experimental results.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2101637975",
    "type": "article"
  },
  {
    "title": "Composing Multi-Ported Memories on FPGAs",
    "doi": "https://doi.org/10.1145/2629629",
    "publication_date": "2014-08-01",
    "publication_year": 2014,
    "authors": "Charles Eric LaForest; Zimo Li; Tristan O'rourke; Ming G. Liu; J. Gregory Steffan",
    "corresponding_authors": "",
    "abstract": "Multi-ported memories are challenging to implement on FPGAs since the block RAMs included in the fabric typically have only two ports. Hence we must construct memories requiring more than two ports, either out of logic elements or by combining multiple block RAMs. We present a thorough exploration and evaluation of the design space of FPGA-based soft multi-ported memories for conventional solutions, and also for the recently proposed Live Value Table (LVT) [LaForest and Steffan 2010] and XOR [LaForest et al. 2012] approaches to unidirectional port memories, reporting results for both Altera and Xilinx FPGAs. Additionally, we thoroughly evaluate and compare with a recent LVT-based approach to bidirectional port memories [Choi et al. 2012].",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2137545744",
    "type": "article"
  },
  {
    "title": "A Hash Table for Line-Rate Data Processing",
    "doi": "https://doi.org/10.1145/2629582",
    "publication_date": "2015-03-24",
    "publication_year": 2015,
    "authors": "Zsolt István; Gustavo Alonso; Michaela Blott; Kees Vissers",
    "corresponding_authors": "",
    "abstract": "FPGA-based data processing is becoming increasingly relevant in data centers, as the transformation of existing applications into dataflow architectures can bring significant throughput and power benefits. Furthermore, a tighter integration of computing and network is appealing, as it overcomes traditional bottlenecks between CPUs and network interfaces, and dramatically reduces latency. In this article, we present the design of a novel hash table, a fundamental building block used in many applications, to enable data processing on FPGAs close to the network. We present a fully pipelined design capable of sustaining consistent 10Gbps line-rate processing by deploying a concurrent mechanism to handle hash collisions. We address additional design challenges such as support for a broad range of key sizes without stalling the pipeline through careful matching of lookup time with packet reception time. Finally, the design is based on a scalable architecture that can be easily parameterized to work with different memory types operating at different access speeds and latencies. We have tested the proposed hash table in an FPGA-based memcached appliance implementing a main-memory key-value store in hardware. The hash table is used to index 2 million entries in 24GB of external DDR3 DRAM while sustaining 13 million requests per second, the maximum packet rate that can be achieved with UDP packets on a 10Gbps link for this application.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2036940992",
    "type": "article"
  },
  {
    "title": "Acceleration of k-Means Algorithm Using Altera SDK for OpenCL",
    "doi": "https://doi.org/10.1145/2964910",
    "publication_date": "2016-09-24",
    "publication_year": 2016,
    "authors": "Qing Tang; Mohammed Khalid",
    "corresponding_authors": "",
    "abstract": "A K-means clustering algorithm involves partitioning of data iteratively into k clusters. It is one of the most popular data-mining algorithms [Wu et al. 2007], and is widely used in other applications, such as image processing and machine learning. However, k-means is highly time-consuming when data or cluster size is large. Traditionally, FPGAs have shown great promise for accelerating computationally intensive algorithms, but they are harder to use for acceleration if we rely on traditional HD-based design methods. The recent introduction of Altera SDK for the OpenCL high-level synthesis tool allows developers to utilize FPGA's potential without long development periods and extensive hardware knowledge. This article presents an optimized implementation of a k-means clustering algorithm on an FPGA using Altera SDK for OpenCL. Performance and power consumption is measured with various data, cluster, and dimension sizes. When compared to state-of-the-art solutions, this implementation supports larger cluster sizes, offers up to 21x speed over a CPU and is more power efficient than a GPU. Unlike previous implementations, it can deliver consistently high throughput across large or small feature dimensions given reasonable cluster sizes and large enough data size.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2524901373",
    "type": "article"
  },
  {
    "title": "A Tradeoff Analysis of FPGAs, GPUs, and Multicores for Sliding-Window Applications",
    "doi": "https://doi.org/10.1145/2659000",
    "publication_date": "2015-03-06",
    "publication_year": 2015,
    "authors": "Patrick Cooke; Jeremy Fowers; G.R. Brown; Greg Stitt",
    "corresponding_authors": "",
    "abstract": "The increasing usage of hardware accelerators such as Field-Programmable Gate Arrays (FPGAs) and Graphics Processing Units (GPUs) has significantly increased application design complexity. Such complexity results from a larger design space created by numerous combinations of accelerators, algorithms, and hw/sw partitions. Exploration of this increased design space is critical due to widely varying performance and energy consumption for each accelerator when used for different application domains and different use cases. To address this problem, numerous studies have evaluated specific applications across different architectures. In this article, we analyze an important domain of applications, referred to as sliding-window applications , implemented on FPGAs, GPUs, and multicore CPUs. For each device, we present optimization strategies and analyze use cases where each device is most effective. The results show that, for large input sizes, FPGAs can achieve speedups of up to 5.6× and 58× compared to GPUs and multicore CPUs, respectively, while also using up to an order of magnitude less energy. For small input sizes and applications with frequency-domain algorithms, GPUs generally provide the best performance and energy.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W1995926296",
    "type": "article"
  },
  {
    "title": "Low-Level Flexible Architecture with Hybrid Reconfiguration for Evolvable Hardware",
    "doi": "https://doi.org/10.1145/2700414",
    "publication_date": "2015-05-11",
    "publication_year": 2015,
    "authors": "Roland Dobai; Lukáš Sekanina",
    "corresponding_authors": "",
    "abstract": "Field-programmable gate arrays (FPGAs) can be considered to be the most popular and successful platform for evolvable hardware. They allow one to establish and later reconfigure candidate solutions. Recent work in the field of evolvable hardware includes the use of virtual and native reconfigurations. Virtual reconfiguration is based on the change of functionality by hardware components implemented on top of FPGA resources. Native reconfiguration changes the FPGA resources directly by means provided by the FPGA manufacturer. Both of these approaches have their disadvantages. The virtual reconfiguration is characterized by lower maximal operational frequency of the resulting solutions, and the native reconfiguration is slower. In this work, a hybrid approach is used merging the advantages while limiting the disadvantages of the virtual and native reconfigurations. The main contribution is the new low-level architecture for evolvable hardware in the new Zynq-7000 all-programmable system-on-chip. The proposed architecture offers high flexibility in comparison with other evolvable hardware systems by considering direct modification of the reconfigurable resources. The impact of the higher reconfiguration time of the native approach is limited by the dense placement of the proposed reconfigurable processing elements. These processing elements also ensure fast evaluation of candidate solutions. The proposed architecture is evaluated by evolutionary design of switching image filters and edge detectors. The experimental results demonstrate advantages over the previous approaches considering the time required for evolution, area overhead, and flexibility.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2270187207",
    "type": "article"
  },
  {
    "title": "Microkernel Architecture and Hardware Abstraction Layer of a Reliable Reconfigurable Real-Time Operating System (R3TOS)",
    "doi": "https://doi.org/10.1145/2629639",
    "publication_date": "2015-03-06",
    "publication_year": 2015,
    "authors": "Xabier Iturbe; Khaled Benkrid; Chuan Hong; Ali Ebrahim; Raúl Torrego; Tughrul Arslan",
    "corresponding_authors": "",
    "abstract": "This article presents a new solution for easing the development of reconfigurable applications using Field-Programable Gate Arrays (FPGAs). Namely, our Reliable Reconfigurable Real-Time Operating System (R3TOS) provides OS-like support for partially reconfigurable FPGAs. Unlike related works, R3TOS is founded on the basis of resource reusability and computation ephemerality. It makes intensive use of reconfiguration at very fine FPGA granularity, keeping the logic resources used only while performing computation and releasing them as soon as it is completed. To achieve this goal, R3TOS goes beyond the traditional approach of using reconfigurable slots with fixed boundaries interconnected by means of a static communication infrastructure. Instead, R3TOS approaches a static route-free system where nearly everything is reconfigurable. The tasks are concatenated to form a computation chain through which partial results naturally flow, and data are exchanged among remotely located tasks using FPGA’s reconfiguration mechanism or by means of “removable” routing circuits. In this article, we describe the R3TOS microkernel architecture as well as its hardware abstraction services and programming interface. Notably, the article presents a set of novel circuits and mechanisms to overcome the limitations and exploit the opportunities of Xilinx reconfigurable technology in the scope of hardware multitasking and dependability.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2057340342",
    "type": "article"
  },
  {
    "title": "Unrolling Ternary Neural Networks",
    "doi": "https://doi.org/10.1145/3359983",
    "publication_date": "2019-10-18",
    "publication_year": 2019,
    "authors": "Stephen Tridgell; Martin Kumm; Martin Hardieck; David Boland; Duncan J. M. Moss; Peter Zipf; Philip H. W. Leong",
    "corresponding_authors": "",
    "abstract": "The computational complexity of neural networks for large-scale or real-time applications necessitates hardware acceleration. Most approaches assume that the network architecture and parameters are unknown at design time, permitting usage in a large number of applications. This article demonstrates, for the case where the neural network architecture and ternary weight values are known a priori , that extremely high throughput implementations of neural network inference can be made by customising the datapath and routing to remove unnecessary computations and data movement. This approach is ideally suited to FPGA implementations as a specialized implementation of a trained network improves efficiency while still retaining generality with the reconfigurability of an FPGA. A VGG-style network with ternary weights and fixed point activations is implemented for the CIFAR10 dataset on Amazon’s AWS F1 instance. This article demonstrates how to remove 90% of the operations in convolutional layers by exploiting sparsity and compile-time optimizations. The implementation in hardware achieves 90.9 ± 0.1% accuracy and 122k frames per second, with a latency of only 29µs, which is the fastest CNN inference implementation reported so far on an FPGA.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2981240861",
    "type": "article"
  },
  {
    "title": "Mitigating Voltage Attacks in Multi-Tenant FPGAs",
    "doi": "https://doi.org/10.1145/3451236",
    "publication_date": "2021-06-30",
    "publication_year": 2021,
    "authors": "George Provelengios; Daniel Holcomb; Russell Tessier",
    "corresponding_authors": "",
    "abstract": "Recent research has exposed a number of security issues related to the use of FPGAs in embedded system and cloud computing environments. Circuits that deliberately waste power can be carefully crafted by a malicious cloud FPGA user and deployed to cause denial-of-service and fault injection attacks. The main defense strategy used by FPGA cloud services involves checking user-submitted designs for circuit structures that are known to aggressively consume power. Unfortunately, this approach is limited by an attacker’s ability to conceive new designs that defeat existing checkers. In this work, our contributions are twofold. We evaluate a variety of circuit power wasting techniques that typically are not flagged by design rule checks imposed by FPGA cloud computing vendors. The efficiencies of five power wasting circuits, including our new design, are evaluated in terms of power consumed per logic resource. We then show that the source of voltage attacks based on power wasters can be identified. Our monitoring approach localizes the attack and suppresses the clock signal for the target region within 21 μs, which is fast enough to stop an attack before it causes a board reset. All experiments are performed using a state-of-the-art Intel Stratix 10 FPGA.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W3190249143",
    "type": "article"
  },
  {
    "title": "An Optimized GIB Routing Architecture with Bent Wires for FPGA",
    "doi": "https://doi.org/10.1145/3519599",
    "publication_date": "2022-03-05",
    "publication_year": 2022,
    "authors": "Kaichuang Shi; Xuegong Zhou; Hao Zhou; Lingli Wang",
    "corresponding_authors": "",
    "abstract": "Field-programmable gate arrays (FGPAs) are widely used because of the superiority in flexibility and lower non-recurring engineering cost. How to optimize the routing architecture is a key problem for FPGA architects because it has a large impact on FPGA area, delay, and routability. In academia, the routing architecture is mainly based on the connection blocks (CBs) and switch blocks (SBs), whereas most research has focused on SB architectures, such as Wilton, Universal, and Disjoint SB patterns. In this article, we propose a novel unidirectional routing architecture—general interconnection block (GIB)—to improve FPGA performance. With the GIB architecture, logic block (LB) pins can directly connect with the adjacent GIBs without programmable switches. Inside a GIB, LB pins can connect to the routing channel tracks on the four sides of a GIB. In particular, the logic pins from different neighboring LBs that connect to the same GIB can connect with each other with only one programmable switch. In addition, we enhance VTR to support the GIB with bent wires and develop a searching framework based on the simulated annealing algorithm to search for a near-optimal distribution of wire types. We evaluate the GIB architecture on VTR 8 with the provided benchmark circuits. The experimental results show that the GIB architecture with length-4 wires can achieve 9.5% improvement on the critical path delay and 11.1% improvement on the area-delay product compared to the VTR CB-SB architecture with length-4 wires. After exploring mixed wire types, the optimized GIB architecture can further improve the delay by 16.4% and area-delay product by 17.1% compared to the CB-SB architecture with length-4 wires.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W4220769335",
    "type": "article"
  },
  {
    "title": "A Survey on FPGA Cybersecurity Design Strategies",
    "doi": "https://doi.org/10.1145/3561515",
    "publication_date": "2022-09-15",
    "publication_year": 2022,
    "authors": "Alexandre Proulx; Jean‐Yves Chouinard; Paul Fortier; Amine Miled",
    "corresponding_authors": "",
    "abstract": "This article presents a critical literature review on the security aspects of field-programmable gate array (FPGA) devices. FPGA devices present unique challenges to cybersecurity through their reconfigurable nature. The article also pays special attention to emerging system-on-chip (SoC) FPGA devices that incorporate a hard processing system (HPS) on the same die as the FPGA logic. While this incorporation reduces the need for vulnerable external signals, the HPS in SoC FPGA devices adds a level of complexity that is not present for stand-alone FPGA devices. This added complexity necessarily hands over the task of securing the device to developers. Even with standard security features in place, the HPS might still have unhindered access to the FPGA logic. A single software flaw could open up a breach that might allow an attacker to extract the FPGA’s configuration data. A robust cybersecurity strategy is thus required for developers. As such, this work aims to provide the groundwork to build a solid threat-based cybersecurity design strategy that is specially adapted to SoC FPGA devices.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W4295943008",
    "type": "article"
  },
  {
    "title": "FPGA Implementation of Compact Hardware Accelerators for Ring-Binary-LWE-based Post-quantum Cryptography",
    "doi": "https://doi.org/10.1145/3569457",
    "publication_date": "2022-10-27",
    "publication_year": 2022,
    "authors": "Pengzhou He; Tianyou Bao; Jiafeng Xie; Moeness G. Amin",
    "corresponding_authors": "",
    "abstract": "Post-quantum cryptography (PQC) has recently drawn substantial attention from various communities owing to the proven vulnerability of existing public-key cryptosystems against the attacks launched from well-established quantum computers. The Ring-Binary-Learning-with-Errors (RBLWE), a variant of Ring-LWE, has been proposed to build PQC for lightweight applications. As more Field-Programmable Gate Array (FPGA) devices are being deployed in lightweight applications like Internet-of-Things (IoT) devices, it would be interesting if the RBLWE-based PQC can be implemented on the FPGA with ultra-low complexity and flexible processing. However, thus far, limited information is available for such implementations. In this article, we propose novel RBLWE-based PQC accelerators on the FPGA with ultra-low implementation complexity and flexible timing. We first present the process of deriving the key operation of the RBLWE-based scheme into the proposed algorithmic operation. The corresponding hardware accelerator is then efficiently mapped from the proposed algorithm with the help of algorithm-to-architecture implementation techniques and extended to obtain higher-throughput designs. The final complexity analysis and implementation results (on a variety of FPGAs) show that the proposed accelerators have significantly smaller area-time complexities than the state-of-the-art designs. Overall, the proposed accelerators feature low implementation complexity and flexible processing, making them desirable for emerging FPGA-based lightweight applications.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W4307640781",
    "type": "article"
  },
  {
    "title": "Accelerating Weather Prediction Using Near-Memory Reconfigurable Fabric",
    "doi": "https://doi.org/10.1145/3501804",
    "publication_date": "2022-02-09",
    "publication_year": 2022,
    "authors": "Gagandeep Singh; Dionysios Diamantopoulos; Juan Gómez-Luna; Christoph Hagleitner; Sander Stuijk; Henk Corporaal; Onur Mutlu",
    "corresponding_authors": "",
    "abstract": "Ongoing climate change calls for fast and accurate weather and climate modeling. However, when solving large-scale weather prediction simulations, state-of-the-art CPU and GPU implementations suffer from limited performance and high energy consumption. These implementations are dominated by complex irregular memory access patterns and low arithmetic intensity that pose fundamental challenges to acceleration. To overcome these challenges, we propose and evaluate the use of near-memory acceleration using a reconfigurable fabric with high-bandwidth memory (HBM). We focus on compound stencils that are fundamental kernels in weather prediction models. By using high-level synthesis techniques, we develop NERO, an FPGA+HBM-based accelerator connected through OCAPI (Open Coherent Accelerator Processor Interface) to an IBM POWER9 host system. Our experimental results show that NERO outperforms a 16-core POWER9 system by 5.3x and 12.7x when running two different compound stencil kernels. NERO reduces the energy consumption by 12x and 35x for the same two kernels over the POWER9 system with an energy efficiency of 1.61 GFLOPS/Watt and 21.01 GFLOPS/Watt. We conclude that employing near-memory acceleration solutions for weather prediction modeling is promising as a means to achieve both high performance and high energy efficiency.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W4210934944",
    "type": "article"
  },
  {
    "title": "Guest Editors’ Introduction to Security in Reconfigurable Systems Design",
    "doi": "https://doi.org/10.1145/1502781.1502782",
    "publication_date": "2009-03-01",
    "publication_year": 2009,
    "authors": "Patrick Schaumont; Alex K. Jones; Steve Trimberger",
    "corresponding_authors": "",
    "abstract": "This special issue on Security in Reconfigurable Systems Design reports on recent research results in the design and implementation of trustworthy reconfigurable systems. Five articles cover topics including power-efficient implementation of public-key cryptography, side-channel analysis of electromagnetic radiation, side-channel resistant design, design of robust unclonable functions on an FPGA, and Trojan detection in an FPGA bitstream.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2038686471",
    "type": "article"
  },
  {
    "title": "Multivariate Gaussian Random Number Generation Targeting Reconfigurable Hardware",
    "doi": "https://doi.org/10.1145/1371579.1371584",
    "publication_date": "2008-06-01",
    "publication_year": 2008,
    "authors": "David B. Thomas; Wayne Luk",
    "corresponding_authors": "",
    "abstract": "The multivariate Gaussian distribution is often used to model correlations between stochastic time-series, and can be used to explore the effect of these correlations across N time-series in Monte-Carlo simulations. However, generating random correlated vectors is an O ( N 2 ) process, and quickly becomes a computational bottleneck in software simulations. This article presents an efficient method for generating vectors in parallel hardware, using N parallel pipelined components to generate a new vector every N cycles. This method maps well to the embedded block RAMs and multipliers in contemporary FPGAs, particularly as extensive testing shows that the limited bit-width arithmetic does not reduce the statistical quality of the generated vectors. An implementation of the architecture in the Virtex-4 architecture achieves a 500MHz clock-rate, and can support vector lengths up to 512 in the largest devices. The combination of a high clock-rate and parallelism provides a significant performance advantage over conventional processors, with an xc4vsx55 device at 500MHz providing a 200 times speedup over an Opteron 2.6GHz using an AMD optimised BLAS package. In a case study in Delta-Gamma Value-at Risk, an RC2000 accelerator card using an xc4vsx55 at 400MHz is 26 times faster than a quad Opteron 2.6GHz SMP.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2040213768",
    "type": "article"
  },
  {
    "title": "RAT",
    "doi": "https://doi.org/10.1145/1462586.1462591",
    "publication_date": "2009-01-01",
    "publication_year": 2009,
    "authors": "Brian Holland; Karthik Nagarajan; Alan D. George",
    "corresponding_authors": "",
    "abstract": "While the promise of achieving speedup and additional benefits such as high performance per watt with FPGAs continues to expand, chief among the challenges with the emerging paradigm of reconfigurable computing is the complexity in application design and implementation. Before a lengthy development effort is undertaken to map a given application to hardware, it is important that a high-level parallel algorithm crafted for that application first be analyzed relative to the target platform, so as to ascertain the likelihood of success in terms of potential speedup. This article presents the RC Amenability Test, or RAT, a methodology and model developed for this purpose, supporting rapid exploration and prediction of strategic design tradeoffs during the formulation stage of application development.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2059113806",
    "type": "article"
  },
  {
    "title": "Reconfiguration and Communication-Aware Task Scheduling for High-Performance Reconfigurable Computing",
    "doi": "https://doi.org/10.1145/1862648.1862650",
    "publication_date": "2010-11-01",
    "publication_year": 2010,
    "authors": "Miaoqing Huang; Vikram K. Narayana; H. Simmler; Olivier Serres; Tarek El‐Ghazawi",
    "corresponding_authors": "",
    "abstract": "High-performance reconfigurable computing involves acceleration of significant portions of an application using reconfigurable hardware. When the hardware tasks of an application cannot simultaneously fit in an FPGA, the task graph needs to be partitioned and scheduled into multiple FPGA configurations, in a way that minimizes the total execution time. This article proposes the Reduced Data Movement Scheduling (RDMS) algorithm that aims to improve the overall performance of hardware tasks by taking into account the reconfiguration time, data dependency between tasks, intertask communication as well as task resource utilization. The proposed algorithm uses the dynamic programming method. A mathematical analysis of the algorithm shows that the execution time would at most exceed the optimal solution by a factor of around 1.6, in the worst-case. Simulations on randomly generated task graphs indicate that RDMS algorithm can reduce interconfiguration communication time by 11% and 44% respectively, compared with two other approaches that consider data dependency and hardware resource utilization only. The practicality, as well as efficiency of the proposed algorithm over other approaches, is demonstrated by simulating a task graph from a real-life application - N-body simulation - along with constraints for bandwidth and FPGA parameters from existing high-performance reconfigurable computers. Experiments on SRC-6 are carried out to validate the approach.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2058480247",
    "type": "article"
  },
  {
    "title": "An Approach for Solving Large SAT Problems on FPGA",
    "doi": "https://doi.org/10.1145/1857927.1857937",
    "publication_date": "2010-12-01",
    "publication_year": 2010,
    "authors": "Kenji Kanazawa; Tsutomu Maruyama",
    "corresponding_authors": "",
    "abstract": "WSAT and its variants are one of the best performing stochastic local search algorithms for the satisfiability (SAT) problem. In this article, we propose an approach for solving large 3-SAT problems on FPGA using a WSAT algorithm. In hardware solvers, it is important to solve large problems efficiently. In WSAT algorithms, an assignment of binary values to the variables that satisfy all clauses is searched by repeatedly choosing a variable in an unsatisfied clause using a heuristic, and flipping its value. In our solver, (1) only the clauses that may be unsatisfied by the flipping are evaluated in parallel to minimize the circuit size, and (2) several independent tries are executed at the same time on the pipelined circuit to achieve high performance. Our FPGA solver can solve larger problems than previous works with less hardware resources, and shows higher performance.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2085156669",
    "type": "article"
  },
  {
    "title": "WireMap",
    "doi": "https://doi.org/10.1145/1534916.1534924",
    "publication_date": "2009-06-01",
    "publication_year": 2009,
    "authors": "Stephen Jang; Billy Chan; Kevin Chung; Alan Mishchenko",
    "corresponding_authors": "",
    "abstract": "This article presents a new technology mapper, WireMap. The mapper uses an edge flow heuristic to improve the routability of a mapped design. The heuristic is applied during the iterative mapping optimization to reduce the total number of pin-to-pin connections (or edges). On academic benchmark (ISCAS, MCNC, and ITC designs), the average edge reduction of 9.3% is achieved while maintaining depth and LUT count compared to state-of-the-art technology mapping. Placing and routing the resulting netlists leads to an 8.5% reduction in the total wirelength, a 6.0% reduction in minimum channel width, and a 2.3% reduction in critical path delay. This technique is applied in the Xilinx ISE Design tool to evaluate its effect on industrial Virtex5 circuits. In a set of 20 large designs, we find the edge reduction is 6.8% while total wirelength measured in the placer is reduced by 3.6%. Applying WireMap has an additional advantage of reducing an average number of inputs of LUTs without increasing the total LUT count and depth. The percentages of 5- and 6-LUTs in a typical design are reduced, while the percentages of 2-, 3-, and 4-LUTs are increased. These smaller LUTs can be merged into pairs and implemented using the dual-output LUT structure found in commercial FPGAs. For academic benchmarks, WireMap leads to 9.4% fewer dual-output LUTs after merging. For the industrial designs, WireMap leads to 6.3% fewer dual-output Virtex5 LUTs.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2126814059",
    "type": "article"
  },
  {
    "title": "Improving the Robustness of Ring Oscillator TRNGs",
    "doi": "https://doi.org/10.1145/1754386.1754390",
    "publication_date": "2010-05-01",
    "publication_year": 2010,
    "authors": "Sang-Kyung Yoo; Deniz Karakoyunlu; Berk Birand; Berk Sunar",
    "corresponding_authors": "",
    "abstract": "A ring oscillator-based true-random number generator design (Rings design) was introduced in Sunar et al. [2007]. The design was rigorously analyzed under a simple mathematical model and its performance characteristics were established. In this article we focus on the practical aspects of the Rings design on a reconfigurable logic platform and determine their implications on the earlier analysis framework. We make recommendations for avoiding pitfalls in real-life implementations by considering ring interaction, transistor-level effects, narrow signal rejection, transmission line attenuation, and sampler bias. Furthermore, we present experimental results showing that changing operating conditions such as the power supply voltage or the operating temperature may affect the output quality when the signal is subsampled. Hence, an attacker may shift the operating point via a simple noninvasive influence and easily bias the TRNG output. Finally, we propose modifications to the design which significantly improve its robustness against attacks, alleviate implementation-related problems, and simultaneously improve its area, throughput, and power performance.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2073766244",
    "type": "article"
  },
  {
    "title": "Fast, Efficient Floating-Point Adders and Multipliers for FPGAs",
    "doi": "https://doi.org/10.1145/1839480.1839481",
    "publication_date": "2010-09-01",
    "publication_year": 2010,
    "authors": "K. Scott Hemmert; Keith D. Underwood",
    "corresponding_authors": "",
    "abstract": "Floating-point applications are a growing trend in the FPGA community. As such, it has become critical to create floating-point units optimized for standard FPGA technology. Unfortunately, the FPGA design space is very different from the VLSI design space; thus, optimizations for FPGAs can differ significantly from optimizations for VLSI. In particular, the FPGA environment constrains the design space such that only limited parallelism can be effectively exploited to reduce latency. Obtaining the right balances between clock speed, latency, and area in FPGAs can be particularly challenging. This article presents implementation details for an IEEE-754 standard floating-point adder and multiplier for FPGAs. The designs presented here enable a Xilinx Virtex4 FPGA (-11 speed grade) to achieve 270 MHz IEEE compliant double precision floating-point performance with a 9-stage adder pipeline and 14-stage multiplier pipeline. The area requirement is approximately 500 slices for the adder and under 750 slices for the multiplier.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2087898301",
    "type": "article"
  },
  {
    "title": "Adaptive Voltage Scaling in a Dynamically Reconfigurable FPGA-Based Platform",
    "doi": "https://doi.org/10.1145/2392616.2392618",
    "publication_date": "2012-12-01",
    "publication_year": 2012,
    "authors": "Atukem Nabina; Jose Nunez‐Yanez",
    "corresponding_authors": "",
    "abstract": "Power is an important issue limiting the applicability of Field Programmable Gate Arrays (FPGAs) since it is considered to be up to one order of magnitude higher than in ASICs. Recently, dynamic reconfiguration in FPGAs has emerged as a viable technique able to achieve power and cost reductions by time-multiplexing the required functionality at runtime. In this article, the applicability of Adaptive Voltage Scaling (AVS) to FPGAs is considered together with dynamic reconfiguration of logic and clock management resources to further improve the power profile of these devices. AVS is a popular power-saving technique in ASICs that enables a device to regulate its own voltage and frequency based on workload, fabrication, and operating conditions. The resulting processing platform exploits the available application-dependent timing margins to achieve a power reduction up to 85% operating at 0.58 volts compared with operating at a nominal voltage of 1 volt. The results also show that the energy requirements at 0.58 volts are aproximately five times lower compared with nominal voltage and this can be explained by the approximate cubic relation of static energy with voltage and the fact that the static component dominates power consumption in the considered FPGA devices.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W1978124838",
    "type": "article"
  },
  {
    "title": "FPGA-Array with Bandwidth-Reduction Mechanism for Scalable and Power-Efficient Numerical Simulations Based on Finite Difference Methods",
    "doi": "https://doi.org/10.1145/1862648.1862651",
    "publication_date": "2010-11-01",
    "publication_year": 2010,
    "authors": "Kentaro Sano; Luzhou Wang; Yoshiaki Hatsuda; Takanori Iizuka; Satoru Yamamoto",
    "corresponding_authors": "",
    "abstract": "For scientific numerical simulation that requires a relatively high ratio of data access to computation, the scalability of memory bandwidth is the key to performance improvement, and therefore custom-computing machines (CCMs) are one of the promising approaches to provide bandwidth-aware structures tailored for individual applications. In this article, we propose a scalable FPGA-array with bandwidth-reduction mechanism (BRM) to implement high-performance and power-efficient CCMs for scientific simulations based on finite difference methods. With the FPGA-array, we construct a systolic computational-memory array (SCMA), which is given a minimum of programmability to provide flexibility and high productivity for various computing kernels and boundary computations. Since the systolic computational-memory architecture of SCMA provides scalability of both memory bandwidth and arithmetic performance according to the array size, we introduce a homogeneously partitioning approach to the SCMA so that it is extensible over a 1D or 2D array of FPGAs connected with a mesh network. To satisfy the bandwidth requirement of inter-FPGA communication, we propose BRM based on time-division multiplexing. BRM decreases the required number of communication channels between the adjacent FPGAs at the cost of delay cycles. We formulate the trade-off between bandwidth and delay of inter-FPGA data-transfer with BRM. To demonstrate feasibility and evaluate performance quantitatively, we design and implement the SCMA of 192 processing elements over two ALTERA Stratix II FPGAs. The implemented SCMA running at 106MHz has the peak performance of 40.7 GFlops in single precision. We demonstrate that the SCMA achieves the sustained performances of 32.8 to 35.7 GFlops for three benchmark computations with high utilization of computing units. The SCMA has complete scalability to the increasing number of FPGAs due to the highly localized computation and communication. In addition, we also demonstrate that the FPGA-based SCMA is power-efficient: it consumes 69% to 87% power and requires only 2.8% to 7.0% energy of those for the same computations performed by a 3.4-GHz Pentium4 processor. With software simulation, we show that BRM works effectively for benchmark computations, and therefore commercially available low-end FPGAs with relatively narrow I/O bandwidth can be utilized to construct a scalable FPGA-array.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W1989336131",
    "type": "article"
  },
  {
    "title": "An FPGA-Based Accelerator for Frequent Itemset Mining",
    "doi": "https://doi.org/10.1145/2457443.2457445",
    "publication_date": "2013-05-01",
    "publication_year": 2013,
    "authors": "Yan Zhang; Fan Zhang; Zheming Jin; Jason D. Bakos",
    "corresponding_authors": "",
    "abstract": "In this article we describe a Field Programmable Gate Array (FPGA)-based coprocessor architecture for Frequent Itemset Mining (FIM). FIM is a common data mining task used to find frequently occurring subsets amongst a database of sets. FIM is a nonnumerical, data intensive computation and is used in machine learning and computational biology. FIM is particularly expensive---in terms of execution time and memory---when performed on large and/or sparse databases or when applied using a low appearance frequency threshold. Because of this, the development of increasingly efficient FIM algorithms and their mapping to parallel architectures is an active field. Previous attempts to accelerate FIM using FPGAs have relied on performance-limiting strategies such as iterative database loading and runtime logic unit reconfiguration. In this article, we present a novel architecture to implement Eclat, a well-known FIM algorithm. Unlike previous efforts, our technique does not impose limits on the maximum set size as a function of available FPGA logic resources and our design scales well to multiple FPGAs. In addition to a novel hardware design, we also present a corresponding compression scheme for intermediate results that are stored in on-chip memory. On a four-FPGA board, experimental results show up to 68X speedup compared to a highly optimized software implementation.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W1990253906",
    "type": "article"
  },
  {
    "title": "Fast Optical Reconfiguration of a Nine-Context DORGA Using a Speed Adjustment Control",
    "doi": "https://doi.org/10.1145/1968502.1968506",
    "publication_date": "2011-05-01",
    "publication_year": 2011,
    "authors": "Mao Nakajima; Minoru Watanabe",
    "corresponding_authors": "",
    "abstract": "Demand for fast dynamic reconfiguration has increased since dynamic reconfiguration can accelerate the performance of implementation circuits. Such dynamic reconfiguration requires two important features: fast reconfiguration and numerous reconfiguration contexts. However, fast reconfiguration and numerous reconfiguration contexts share a trade-off relation on current VLSIs. Therefore, Optically Reconfigurable Gate Arrays (ORGAs) have been developed to resolve this dilemma. An ORGA architecture allows many configuration contexts by exploiting the large storage capacity of a holographic memory and fast reconfiguration using wide-bandwidth optical connections between a holographic memory and a programmable gate array VLSI. In addition, Dynamic Optically Reconfigurable Gate Arrays (DORGAs) using a photodiode memory architecture have already been developed to realize a high-gate-density VLSI. Therefore, this article presents the first demonstration of a nanosecond-order configuration of a nine-context DORGA architecture. Moreover, this article presents a proposal of a reconfiguration period adjustment technique to control each reconfiguration period to its best setting.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2171030356",
    "type": "article"
  },
  {
    "title": "Synthesizable Standard Cell FPGA Fabrics Targetable by the Verilog-to-Routing CAD Flow",
    "doi": "https://doi.org/10.1145/3024063",
    "publication_date": "2017-04-06",
    "publication_year": 2017,
    "authors": "Jin Hee Kim; Jason H. Anderson",
    "corresponding_authors": "",
    "abstract": "In this article, we consider implementing field-programmable gate arrays (FPGAs) using a standard cell design methodology and present a framework for the automated generation of synthesizable FPGA fabrics. The open-source Verilog-to-Routing (VTR) FPGA architecture evaluation framework [Rose et al. 2012] is extended to generate synthesizable Verilog for its in-memory FPGA architectural device model. The Verilog can subsequently be synthesized into standard cells, placed and routed using an ASIC design flow. A second extension to VTR generates a configuration bitstream for the FPGA, where the bitstream configures the FPGA to realize a user-provided placed and routed design. The proposed framework and methodology makes possible the silicon implementation of a wide range of VTR-modeled FPGA fabrics. In an experimental study, area and timing-optimized FPGA implementations in 65nm TSMC standard cells are compared to a 65nm Altera commercial FPGA. In addition, we consider augmenting the generic standard-cell library from TSMC with a manually designed and laid-out FPGA-specific cell. We demonstrate the utility of the custom cell in reducing the area of the synthesized FPGA fabric.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2604877941",
    "type": "article"
  },
  {
    "title": "GROK-LAB",
    "doi": "https://doi.org/10.1145/2597889",
    "publication_date": "2014-12-29",
    "publication_year": 2014,
    "authors": "Benjamin Gojman; Sirisha Nalmela; Nikil Mehta; Nicholas Howarth; André DeHon",
    "corresponding_authors": "",
    "abstract": "Timing Extraction identifies the delay of fine-grained components within an FPGA. From these computed delays, the delay of any path can be calculated. Moreover, a comparison of the fine-grained delays allows a detailed understanding of the amount and type of process variation that exists in the FPGA. To obtain these delays, Timing Extraction measures, using only resources already available in the FPGA, the delay of a small subset of the total paths in the FPGA. We apply Timing Extraction to the Logic Array Block (LAB) on an Altera Cyclone III FPGA to obtain a view of the delay down to near-individual LUT SRAM cell granularity, characterizing components with delays on the order of tens to a few hundred picoseconds with a resolution of ±3.2ps, matching the expected error bounds. This information reveals that the 65nm process used has, on average, random variation of σ μ =4.0% with components having an average maximum spread of 83ps. Timing Extraction also shows that as V DD decreases from 1.2V to 0.9V in a Cyclone IV 60nm FPGA, paths slow down, and variation increases from σ μ =4.3% to σ μ =5.8%, a clear indication that lowering V DD magnifies the impact of random variation.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2006434824",
    "type": "article"
  },
  {
    "title": "Networks-on-Chip for FPGAs",
    "doi": "https://doi.org/10.1145/2629442",
    "publication_date": "2014-08-01",
    "publication_year": 2014,
    "authors": "Mohamed S. Abdelfattah; Vaughn Betz",
    "corresponding_authors": "",
    "abstract": "As FPGA capacity increases, a growing challenge is connecting ever-more components with the current low-level FPGA interconnect while keeping designers productive and on-chip communication efficient. We propose augmenting FPGAs with networks-on-chip (NoCs) to simplify design, and we show that this can be done while maintaining or even improving silicon efficiency. We compare the area and speed efficiency of each NoC component when implemented hard versus soft to explore the space and inform our design choices. We then build on this component-level analysis to architect hard NoCs and integrate them into the FPGA fabric; these NoCs are on average 20--23× smaller and 5--6× faster than soft NoCs. A 64-node hard NoC uses only ∼2% of an FPGA's silicon area and metallization. We introduce a new communication efficiency metric: silicon area required per realized communication bandwidth. Soft NoCs consume 4960 mm 2 /TBps, but hard NoCs are 84× more efficient at 59 mm 2 /TBps. Informed design can further reduce the area overhead of NoCs to 23 mm 2 /TBps, which is only 2.6× less efficient than the simplest point-to-point soft links (9 mm 2 /TBps). Despite this almost comparable efficiency, NoCs can switch data across the entire FPGA while point-to-point links are very limited in capability; therefore, hard NoCs are expected to improve FPGA efficiency for more complex styles of communication.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2026844540",
    "type": "article"
  },
  {
    "title": "SATTA",
    "doi": "https://doi.org/10.1145/2659001",
    "publication_date": "2015-03-06",
    "publication_year": 2015,
    "authors": "Stefano Di Carlo; Giulio Gambardella; P. Prinetto; Daniele Rolfo; Pascal Trotta",
    "corresponding_authors": "",
    "abstract": "Dependability issues due to nonfunctional properties are emerging as a major cause of faults in modern digital systems. Effective countermeasures have to be developed to properly manage their critical timing effects. This article presents a methodology to avoid transition delay faults in field-programmable gate array (FPGA)-based systems, with low area overhead. The approach is able to exploit temperature information and aging characteristics to minimize the cost in terms of performances degradation and power consumption. The architecture of a hardware manager able to avoid delay faults is presented and analyzed extensively, as well as its integration in the standard implementation design flow.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2044042022",
    "type": "article"
  },
  {
    "title": "Open-Source Variable-Precision Floating-Point Library for Major Commercial FPGAs",
    "doi": "https://doi.org/10.1145/2851507",
    "publication_date": "2016-07-19",
    "publication_year": 2016,
    "authors": "Xin Fang; Miriam Leeser",
    "corresponding_authors": "",
    "abstract": "There is increased interest in implementing floating-point designs for different precisions that take advantage of the flexibility offered by Field-Programmable Gate Arrays (FPGAs). In this article, we present updates to the Variable-precision FLOATing Point Library (VFLOAT) developed at Northeastern University and highlight recent improvements in implementations for implementing reciprocal, division, and square root components that scale to double precision for FPGAs from the two major vendors: Altera and Xilinx. Our library is open source and flexible and provides the user with many options. A designer has many tradeoffs to consider including clock frequency, total latency, and resource usage as well as target architecture. We compare the generated cores to those produced by each vendor and to another popular open-source tool: FloPoCo. VFLOAT has the advantage of not tying the user’s design to a specific target architecture and of providing the maximum flexibility for all options including clock frequency and latency compared to other alternatives. Our results show that variable-precision as well as double-precision designs can easily be accommodated and the resulting components are competitive and in many cases superior to the alternatives.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2482880624",
    "type": "article"
  },
  {
    "title": "High-Efficiency Convolutional Ternary Neural Networks with Custom Adder Trees and Weight Compression",
    "doi": "https://doi.org/10.1145/3270764",
    "publication_date": "2018-09-30",
    "publication_year": 2018,
    "authors": "Adrien Prost-Boucle; Alban Bourge; Frédéric Pétrot",
    "corresponding_authors": "",
    "abstract": "Although performing inference with artificial neural networks (ANN) was until quite recently considered as essentially compute intensive, the emergence of deep neural networks coupled with the evolution of the integration technology transformed inference into a memory bound problem. This ascertainment being established, many works have lately focused on minimizing memory accesses, either by enforcing and exploiting sparsity on weights or by using few bits for representing activations and weights, to be able to use ANNs inference in embedded devices. In this work, we detail an architecture dedicated to inference using ternary {−1, 0, 1} weights and activations. This architecture is configurable at design time to provide throughput vs. power trade-offs to choose from. It is also generic in the sense that it uses information drawn for the target technologies (memory geometries and cost, number of available cuts, etc.) to adapt at best to the FPGA resources. This allows to achieve up to 5.2k frames per second per Watt for classification on a VC709 board using approximately half of the resources of the FPGA.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2905006747",
    "type": "article"
  },
  {
    "title": "General-Purpose Computing with Soft GPUs on FPGAs",
    "doi": "https://doi.org/10.1145/3173548",
    "publication_date": "2018-01-24",
    "publication_year": 2018,
    "authors": "Muhammed Al Kadi; Benedikt Janßen; Jones Yudi; Michael Huebner",
    "corresponding_authors": "",
    "abstract": "Using field-programmable gate arrays (FPGAs) as a substrate to deploy soft graphics processing units (GPUs) would enable offering the FPGA compute power in a very flexible GPU-like tool flow. Application-specific adaptations like selective hardening of floating-point operations and instruction set subsetting would mitigate the high area and power demands of soft GPUs. This work explores the capabilities and limitations of soft General Purpose Computing on GPUs (GPGPU) for both fixed- and floating point arithmetic. For this purpose, we have developed FGPU: a configurable, scalable, and portable GPU architecture designed especially for FPGAs. FGPU is open-source and implemented entirely in RTL. It can be programmed in OpenCL and controlled through a Python API. This article introduces its hardware architecture as well as its tool flow. We evaluated the proposed GPGPU approach against multiple other solutions. In comparison to homogeneous Multi-Processor System-On-Chips (MPSoCs), we found that using a soft GPU is a Pareto-optimal solution regarding throughput per area and energy consumption. On average, FGPU has a 2.9× better compute density and 11.2× less energy consumption than a single MicroBlaze processor when computing in IEEE-754 floating-point format. An average speedup of about 4× over the ARM Cortex-A9 supported with the NEON vector co-processor has been measured for fixed- or floating-point benchmarks. In addition, the biggest FGPU cores we could implement on a Xilinx Zynq-7000 System-On-Chip (SoC) can deliver similar performance to equivalent implementations with High-Level Synthesis (HLS).",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2996795721",
    "type": "article"
  },
  {
    "title": "RIPL",
    "doi": "https://doi.org/10.1145/3180481",
    "publication_date": "2018-03-14",
    "publication_year": 2018,
    "authors": "Robert Stewart; Kirsty Duncan; Greg Michaelson; Paulo Garcia; Deepayan Bhowmik; Andrew Wallace",
    "corresponding_authors": "",
    "abstract": "Specialized FPGA implementations can deliver higher performance and greater power efficiency than embedded CPU or GPU implementations for real-time image processing. Programming challenges limit their wider use, because the implementation of FPGA architectures at the register transfer level is time consuming and error prone. Existing software languages supported by high-level synthesis (HLS), although providing a productivity improvement, are too general purpose to generate efficient hardware without the use of hardware-specific code optimizations. Such optimizations leak hardware details into the abstractions that software languages are there to provide, and they require knowledge of FPGAs to generate efficient hardware, such as by using language pragmas to partition data structures across memory blocks. This article presents a thorough account of the Rathlin image processing language (RIPL), a high-level image processing domain-specific language for FPGAs. We motivate its design, based on higher-order algorithmic skeletons, with requirements from the image processing domain. RIPL’s skeletons suffice to elegantly describe image processing stencils, as well as recursive algorithms with nonlocal random access patterns. At its core, RIPL employs a dataflow intermediate representation. We give a formal account of the compilation scheme from RIPL skeletons to static and cyclostatic dataflow models to describe their data rates and static scheduling on FPGAs. RIPL compares favorably to the Vivado HLS OpenCV library and C++ compiled with Vivado HLS. RIPL achieves between 54 and 191 frames per second (FPS) at 100MHz for four synthetic benchmarks, faster than HLS OpenCV in three cases. Two real-world algorithms are implemented in RIPL: visual saliency and mean shift segmentation. For the visual saliency algorithm, RIPL achieves 71 FPS compared to optimized C++ at 28 FPS. RIPL is also concise, being 5x shorter than C++ and 111x shorter than an equivalent direct dataflow implementation. For mean shift segmentation, RIPL achieves 7 FPS compared to optimized C++ on 64 CPU cores at 1.1, and RIPL is 10x shorter than the direct dataflow FPGA implementation.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2788102204",
    "type": "article"
  },
  {
    "title": "Accelerating FPGA Routing Through Algorithmic Enhancements and Connection-aware Parallelization",
    "doi": "https://doi.org/10.1145/3406959",
    "publication_date": "2020-08-25",
    "publication_year": 2020,
    "authors": "Yun Zhou; Dries Vercruyce; Dirk Stroobandt",
    "corresponding_authors": "",
    "abstract": "Routing is a crucial step in Field Programmable Gate Array (FPGA) physical design, as it determines the routes of signals in the circuit, which impacts the design implementation quality significantly. It can be very time-consuming to successfully route all the signals of large circuits that utilize many FPGA resources. Attempts have been made to shorten the routing runtime for efficient design exploration while expecting high-quality implementations. In this work, we elaborate on the connection-based routing strategy and algorithmic enhancements to improve the serial FPGA routing. We also explore a recursive partitioning-based parallelization technique to further accelerate the routing process. To exploit more parallelism by a finer granularity in both spatial partitioning and routing, a connection-aware routing bounding box model is proposed for the source-sink connections of the nets. It is built upon the location information of each connection’s source, sink, and the geometric center of the net that the connection belongs to, different from the existing net-based routing bounding box that covers all the pins of the entire net. We present that the proposed connection-aware routing bounding box is more beneficial for parallel routing than the existing net-based routing bounding box. The quality and runtime of the serial and multi-threaded routers are compared to the router in VPR 7.0.7. The large heterogeneous Titan23 designs that are targeted to a detailed representation of the Stratix IV FPGA are used for benchmarking. With eight threads, the parallel router using the connection-aware routing bounding box model reaches a speedup of 6.1× over the serial router in VPR 7.0.7, which is 1.24× faster than the one using the existing net-based routing bounding box model, while reducing the total wire-length by 10% and the critical path delay by 7%.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W3080438181",
    "type": "article"
  },
  {
    "title": "RWRoute: An Open-source Timing-driven Router for Commercial FPGAs",
    "doi": "https://doi.org/10.1145/3491236",
    "publication_date": "2021-11-29",
    "publication_year": 2021,
    "authors": "Yun Zhou; Pongstorn Maidee; Christopher Lavin; Alireza Kaviani; Dirk Stroobandt",
    "corresponding_authors": "",
    "abstract": "One of the key obstacles to pervasive deployment of FPGA accelerators in data centers is their cumbersome programming model. Open source tooling is suggested as a way to develop alternative EDA tools to remedy this issue. Open source FPGA CAD tools have traditionally targeted academic hypothetical architectures, making them impractical for commercial devices. Recently, there have been efforts to develop open source back-end tools targeting commercial devices. These tools claim to follow an alternate data-driven approach that allows them to be more adaptable to the domain requirements such as faster compile time. In this paper, we present RWRoute, the first open source timing-driven router for UltraScale+ devices. RWRoute is built on the RapidWright framework and includes the essential and pragmatic features found in commercial FPGA routers that are often missing from open source tools. Another valuable contribution of this work is an open-source lightweight timing model with high fidelity timing approximations. By leveraging a combination of architectural knowledge, repeating patterns, and extensive analysis of Vivado timing reports, we obtain a slightly pessimistic, lumped delay model within 2% average accuracy of Vivado for UltraScale+ devices. Compared to Vivado, RWRoute results in a 4.9× compile time improvement at the expense of 10% Quality of Results (QoR) loss for 665 synthetic and six real designs. A main benefit of our router is enabling fast partial routing at the back-end of a domain-specific flow. Our initial results indicate that more than 9× compile time improvement is achievable for partial routing. The results of this paper show how such a router can be beneficial for a low touch flow to reduce dependency on commercial tools.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W3215473801",
    "type": "article"
  },
  {
    "title": "Cross-VM Covert- and Side-Channel Attacks in Cloud FPGAs",
    "doi": "https://doi.org/10.1145/3534972",
    "publication_date": "2022-05-12",
    "publication_year": 2022,
    "authors": "Ilias Giechaskiel; Shanquan Tian; Jakub Szefer",
    "corresponding_authors": "",
    "abstract": "The availability of FPGAs in cloud data centers offers rapid, on-demand access to reconfigurable hardware compute resources that users can adapt to their own needs. However, the low-level access to the FPGA hardware and associated resources such as the PCIe bus, SSD drives, or DRAM modules also opens up threats of malicious attackers uploading designs that are able to infer information about other users or about the cloud infrastructure itself. In particular, this work presents a new, fast PCIe-contention-based channel that is able to transmit data between FPGA-accelerated virtual machines (VMs) by modulating the PCIe bus usage. This channel further works with different operating systems and achieves bandwidths reaching 20 kbps with 99% accuracy. This is the first cross-FPGA covert channel demonstrated on commercial clouds and has a bandwidth which is over 2000× larger than prior voltage- or temperature-based cross-board attacks. This article further demonstrates that the PCIe receivers are able to not just receive covert transmissions, but can also perform fine-grained monitoring of the PCIe bus, including detecting when co-located VMs are initialized, even prior to their associated FPGAs being used. Moreover, the proposed mechanism can be used to infer the activities of other users, or even slow down the programming of the co-located FPGAs as well as other data transfers between the host and the FPGA. Beyond leaking information across different virtual machines, the ability to monitor the PCIe bandwidth over hours or days can be used to estimate the data center utilization and map the behavior of the other users. The article also introduces further novel threats in FPGA-accelerated instances, including contention due to network traffic, contention due to shared NVMe SSDs, as well as thermal monitoring to identify FPGA co-location using the DRAM modules attached to the FPGA boards. This is the first work to demonstrate that it is possible to break the separation of privilege in FPGA-accelerated cloud environments, and highlights that defenses for public clouds using FPGAs need to consider PCIe, SSD, and DRAM resources as part of the attack surface that should be protected.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W4280590166",
    "type": "article"
  },
  {
    "title": "Algorithm-hardware Co-optimization for Energy-efficient Drone Detection on Resource-constrained FPGA",
    "doi": "https://doi.org/10.1145/3583074",
    "publication_date": "2023-02-15",
    "publication_year": 2023,
    "authors": "Han-Sok Suh; Jian Meng; Ty Nguyen; Vijay Kumar; Yu Cao; Jae-sun Seo",
    "corresponding_authors": "",
    "abstract": "Convolutional neural network (CNN)-based object detection has achieved very high accuracy; e.g., single-shot multi-box detectors (SSDs) can efficiently detect and localize various objects in an input image. However, they require a high amount of computation and memory storage, which makes it difficult to perform efficient inference on resource-constrained hardware devices such as drones or unmanned aerial vehicles (UAVs). Drone/UAV detection is an important task for applications including surveillance, defense, and multi-drone self-localization and formation control. In this article, we designed and co-optimized an algorithm and hardware for energy-efficient drone detection on resource-constrained FPGA devices. We trained an SSD object detection algorithm with a custom drone dataset. For inference, we employed low-precision quantization and adapted the width of the SSD CNN model. To improve throughput, we use dual-data rate operations for DSPs to effectively double the throughput with limited DSP counts. For different SSD algorithm models, we analyze accuracy or mean average precision (mAP) and evaluate the corresponding FPGA hardware utilization, DRAM communication, and throughput optimization. We evaluated the FPGA hardware for a custom drone dataset, Pascal VOC, and COCO2017. Our proposed design achieves a high mAP of 88.42% on the multi-drone dataset, with a high energy efficiency of 79 GOPS/W and throughput of 158 GOPS using the Xilinx Zynq ZU3EG FPGA device on the Open Vision Computer version 3 (OVC3) platform. Our design achieves 1.1 to 8.7× higher energy efficiency than prior works that used the same Pascal VOC dataset, using the same FPGA device, but at a low-power consumption of 2.54 W. For the COCO dataset, our MobileNet-V1 implementation achieved an mAP of 16.8, and 4.9 FPS/W for energy-efficiency, which is ∼ 1.9× higher than prior FPGA works or other commercial hardware platforms.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4320891764",
    "type": "article"
  },
  {
    "title": "RapidStream 2.0: Automated Parallel Implementation of Latency–Insensitive FPGA Designs Through Partial Reconfiguration",
    "doi": "https://doi.org/10.1145/3593025",
    "publication_date": "2023-04-26",
    "publication_year": 2023,
    "authors": "Licheng Guo; Pongstorn Maidee; Yun Zhou; Christopher Lavin; Eddie Hung; Wuxi Li; Jason Lau; Weikang Qiao; Yuze Chi; Linghao Song; Yuanlong Xiao; Alireza Kaviani; Zhiru Zhang; Jason Cong",
    "corresponding_authors": "",
    "abstract": "Field-programmable gate arrays (FPGAs) require a much longer compilation cycle than conventional computing platforms such as CPUs. In this article, we shorten the overall compilation time by co-optimizing the HLS compilation (C-to-RTL) and the back-end physical implementation (RTL-to-bitstream). We propose a split compilation approach based on the pipelining flexibility at the HLS level, which allows us to partition designs for parallel placement and routing. We outline a number of technical challenges and address them by breaking the conventional boundaries between different stages of the traditional FPGA tool flow and reorganizing them to achieve a fast end-to-end compilation. Our research produces RapidStream, a parallelized and physical-integrated compilation framework that takes in a latency-insensitive program in C/C++ and generates a fully placed and routed implementation. We present two approaches. The first approach (RapidStream 1.0) resolves inter-partition routing conflicts at the end when separate partitions are stitched together. When tested on the Xilinx U250 FPGA with a set of realistic HLS designs, RapidStream achieves a 5 to 7× reduction in compile time and up to 1.3× increase in frequency when compared with a commercial off-the-shelf toolchain. In addition, we provide preliminary results using a customized open-source router to reduce the compile time up to an order of magnitude in cases with lower performance requirements. The second approach (RapidStream 2.0) prevents routing conflicts using virtual pins. Testing on Xilinx U280 FPGA, we observed 5 to 7× compile time reduction and 1.3× frequency increase.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4367050909",
    "type": "article"
  },
  {
    "title": "High-efficiency TRNG Design Based on Multi-bit Dual-ring Oscillator",
    "doi": "https://doi.org/10.1145/3624991",
    "publication_date": "2023-09-21",
    "publication_year": 2023,
    "authors": "Yingchun Lu; Yun Qing Yang; Rong Hu; Huaguo Liang; Maoxiang Yi; Zhengfeng Huang; Yanyang MA; Tian Chen; Liang Yao",
    "corresponding_authors": "",
    "abstract": "Unpredictable true random numbers are required in security technology fields such as information encryption, key generation, mask generation for anti-side-channel analysis, algorithm initialization, and so on. At present, the true random number generator (TRNG) is not enough to provide fast random bits by low-speed bits generation. Therefore, it is necessary to design a faster TRNG. This work presents an ultra-compact TRNG with high throughput based on a novel extendable dual-ring oscillator (DRO). Owing to multiple bits output per cycle in DRO can be used to obtain the original random sequence, the proposed DRO achieves a maximum resource utilization to build a more efficient TRNG, compared with the conventional TRNG system based on ring oscillator (RO), which only has a single output and needs to build multiple groups of ring oscillators. TRNG based on the 2-bit DRO and its 8-bit derivative structure has been verified on Xilinx Artix-7 and Kintex-7 FPGA under the automatic layout and routing and has achieved a throughput of 550 Mbps and 1,100 Mbps, respectively. Moreover, in terms of throughput performance over operating frequency, hardware consumption, and entropy, the proposed scheme has obvious advantages. Finally, the generated sequences show good randomness in the test of NIST SP800-22 and Dieharder test suite and pass the entropy estimation test kit NIST SP800-90B and AIS-31.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4386928861",
    "type": "article"
  },
  {
    "title": "HLPerf: Demystifying the Performance of HLS-based Graph Neural Networks with Dataflow Architectures",
    "doi": "https://doi.org/10.1145/3655627",
    "publication_date": "2024-04-02",
    "publication_year": 2024,
    "authors": "Chenfeng Zhao; Clayton J. Faber; Roger D. Chamberlain; Xuan Zhang",
    "corresponding_authors": "",
    "abstract": "The development of FPGA-based applications using HLS is fraught with performance pitfalls and large design space exploration times. These issues are exacerbated when the application is complicated and its performance is dependent on the input data set, as is often the case with graph neural network approaches to machine learning. Here, we introduce HLPerf, an open-source, simulation-based performance evaluation framework for dataflow architectures that both supports early exploration of the design space and shortens the performance evaluation cycle. We apply the methodology to GNNHLS, an HLS-based graph neural network benchmark containing 6 commonly used graph neural network models and 4 datasets with distinct topologies and scales. The results show that HLPerf achieves over 10 000 × average simulation acceleration relative to RTL simulation and over 400 × acceleration relative to state-of-the-art cycle-accurate tools at the cost of 7% mean error rate relative to actual FPGA implementation performance. This acceleration positions HLPerf as a viable component in the design cycle.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4393421265",
    "type": "article"
  },
  {
    "title": "Hardware Decompression Techniques for FPGA-Based Embedded Systems",
    "doi": "https://doi.org/10.1145/1534916.1534919",
    "publication_date": "2009-06-01",
    "publication_year": 2009,
    "authors": "Dirk Koch; Christian Beckhoff; Jürgen Teich",
    "corresponding_authors": "",
    "abstract": "In this work, we present hardware decompression accelerators for widening the bottleneck between slow nonvolatile memories on the one side and high-speed FPGA configuration interfaces and fast softcore CPUs on the other side. We discuss different compression algorithms suitable for a hardware accelerated decompression on FPGAs as well as on CPLDs. The algorithms will be investigated with respect to the achievable compression ratio, throughput, and hardware overhead. This leads to various decompressor implementations with one capable to decompress at high data rates of up to 400 megabytes per second under optimal conditions while only requiring slightly more than a hundred lookup tables. We will evaluate how these decompressors perform on configuration bitstreams for different FPGAs as well as for softcore CPU binaries.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W1970616362",
    "type": "article"
  },
  {
    "title": "An open-source HyperTransport core",
    "doi": "https://doi.org/10.1145/1391732.1391734",
    "publication_date": "2008-09-01",
    "publication_year": 2008,
    "authors": "David Slogsnat; Alexander Giese; Mondrian Nüssle; Ulrich Brüning",
    "corresponding_authors": "",
    "abstract": "This article presents the design of a generic HyperTransport (HT) core. HyperTransport is a packet-based interconnect technology for low-latency, high-bandwidth point-to-point connections. It is specially optimized to achieve a very low latency. The core has been verified in system using an FPGA. This exhaustive verification and the generic design allow the mapping to both ASICs and FPGAs. The implementation described in this work supports a 16-bit link width, as used by Opteron processors. On a Xilinx Virtex-4 FX60, the core supports a link frequency of 400 MHz DDR and offers a maximum bidirectional bandwidth of 3.2GB/s. The in-system verification has been performed using a custom FPGA board that has been plugged into a HyperTransport extension connector (HTX) of a standard Opteron-based motherboard. HTX slots in Opteron-based motherboards allow very high-bandwidth, low-latency communication, since the HTX device is directly connected to one of the HyperTransport links of the processor. Performance analysis shows a unidirectional payload bandwidth of 1.4GB/s and a read latency of 180 ns. The HT core in combination with the HTX board is an ideal base for prototyping systems and implementing FPGA coprocessors. The HT core is available as open source.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2065949640",
    "type": "article"
  },
  {
    "title": "Applying dynamic reconfiguration in the mobile robotics domain",
    "doi": "https://doi.org/10.1145/2000832.2000841",
    "publication_date": "2011-08-01",
    "publication_year": 2011,
    "authors": "Federico Nava; Donatella Sciuto; Marco D. Santambrogio; Stefan Herbrechtsmeier; Mario Porrmann; Ulf Witkowski; Ulrich Rueckert",
    "corresponding_authors": "",
    "abstract": "Mobile robots are widely used in industrial environments and are expected to be widely available in human environments in the near future, for example, in the area of care and service robots. This article proposes an implementation for a highly customizable color recognition module based on Field Programmable Gate Array (FPGA) hardware to accomplish tasks like real-time frame processing for image streams. In comparison to a pure software solution on a CPU, an attached FPGA-based hardware accelerator enables real-time image processing and significantly reduces the required computing power of the CPU. Instead, the CPU can be used for tasks that cannot be efficiently implemented on FPGAs, for example, because of a large control overhead. We concentrate on a multirobot scenario where a group of robots follows a human team member by keeping a specific formation in order to support the human in exploration and object detection. Additionally, the robots provide a communication infrastructure to maintain a stable multihop communication network between the human and a base station recording all actions and evaluating the captured images and transmitted data. Depending on the current operating conditions, the robot system has to be able to execute a wide variety of different tasks. Since only a small number of tasks have to be executed concurrently, dynamic reconfiguration of the FPGA can be used to avoid the parallel implementation of all tasks on the FPGA. Within this context, this article discusses application fields where dynamic reconfiguration of FPGA-based coprocessors significantly reduces the CPU load and presents examples of how dynamic reconfiguration can be used in exploration.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2048578917",
    "type": "article"
  },
  {
    "title": "A Mapping-Scheduling Algorithm for Hardware Acceleration on Reconfigurable Platforms",
    "doi": "https://doi.org/10.1145/2611562",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "Juan Antonio Clemente; Ivan Beretta; Vincenzo Rana; David Atienza; Donatella Sciuto",
    "corresponding_authors": "",
    "abstract": "Reconfigurable platforms are a promising technology that offers an interesting trade-off between flexibility and performance, which many recent embedded system applications demand, especially in fields such as multimedia processing. These applications typically involve multiple ad-hoc tasks for hardware acceleration, which are usually represented using formalisms such as Data Flow Diagrams (DFDs), Data Flow Graphs (DFGs), Control and Data Flow Graphs (CDFGs) or Petri Nets. However, none of these models is able to capture at the same time the pipeline behavior between tasks (that therefore can coexist in order to minimize the application execution time), their communication patterns, and their data dependencies. This article proves that the knowledge of all this information can be effectively exploited to reduce the resource requirements and the timing performance of modern reconfigurable systems, where a set of hardware accelerators is used to support the computation. For this purpose, this article proposes a novel task representation model, named Temporal Constrained Data Flow Diagram (TCDFD), which includes all this information. This article also presents a mapping-scheduling algorithm that is able to take advantage of the new TCDFD model. It aims at minimizing the dynamic reconfiguration overhead while meeting the communication requirements among the tasks. Experimental results show that the presented approach achieves up to 75% of resources saving and up to 89% of reconfiguration overhead reduction with respect to other state-of-the-art techniques for reconfigurable platforms.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W1972374915",
    "type": "article"
  },
  {
    "title": "Bandwidth Compression of Floating-Point Numerical Data Streams for FPGA-Based High-Performance Computing",
    "doi": "https://doi.org/10.1145/3053688",
    "publication_date": "2017-05-27",
    "publication_year": 2017,
    "authors": "Tomohiro Ueno; Kentaro Sano; Satoru Yamamoto",
    "corresponding_authors": "",
    "abstract": "Although computational performance is often limited by insufficient bandwidth to/from an external memory, it is not easy to physically increase off-chip memory bandwidth. In this study, we propose a hardware-based bandwidth compression technique that can be applied to field-programmable gate array-- (FPGA) based high-performance computation with a logically wider effective memory bandwidth. Our proposed hardware approach can boost the performance of FPGA-based stream computations by applying a data compression technique to effectively transfer more data streams. To apply this data compression technique to bandwidth compression via hardware, several requirements must first be satisfied, including an acceptable level of compression performance and a sufficiently small hardware footprint. Our proposed hardware-based bandwidth compressor utilizes an efficient prediction-based data compression algorithm. Moreover, we propose a multichannel serializer and deserializer that enable applications to use multiple channels of computational data with the bandwidth compression. The serializer encodes compressed data blocks of multiple channels into a data stream, which is efficiently written to an external memory. Based on preliminary evaluation, we define an encoding format considering both high compression ratio and small hardware area. As a result, we demonstrate that our area saving bandwidth compressor increases performance of an FPGA-based fluid dynamics simulation by deploying more processing elements to exploit spatial parallelism with the enhanced memory bandwidth.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2619524412",
    "type": "article"
  },
  {
    "title": "Preemption of the Partial Reconfiguration Process to Enable Real-Time Computing With FPGAs",
    "doi": "https://doi.org/10.1145/3182183",
    "publication_date": "2018-06-30",
    "publication_year": 2018,
    "authors": "Rossi Enrico; Marvin Damschen; Lars Bauer; Giorgio Buttazzo; Jörg Henkel",
    "corresponding_authors": "",
    "abstract": "To improve computing performance in real-time applications, modern embedded platforms comprise hardware accelerators that speed up the task’s most compute-intensive parts. A recent trend in the design of real-time embedded systems is to integrate field-programmable gate arrays (FPGA) that are reconfigured with different accelerators at runtime, to cope with dynamic workloads that are subject to timing constraints. One of the major limitations when dealing with partial FPGA reconfiguration in real-time systems is that the reconfiguration port can only perform one reconfiguration at a time: if a high-priority task issues a reconfiguration request while the reconfiguration port is already occupied by a lower-priority task, the high-priority task has to wait until the current reconfiguration is completed (a phenomenon known as priority inversion ), unless the current reconfiguration is aborted (introducing unbounded delays in low-priority tasks, a phenomenon known as starvation ). This article shows how priority inversion and starvation can be solved by making the reconfiguration process preemptive —that is, allowing it to be interrupted at any time and resumed at a later time without restarting it from scratch. Such a feature is crucial for the design of runtime reconfigurable real-time systems but not yet available in today’s platforms. Furthermore, the trade-off of achieving a guaranteed bound on the reconfiguration delay for low-priority tasks and the maximum delay induced for high-priority tasks when preempting an ongoing reconfiguration has been identified and analyzed. Experimental results on the Xilinx Zynq-7000 platform show that the proposed implementation of preemptive reconfiguration introduces a low runtime overhead, thus effectively solving priority inversion and starvation.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2884923139",
    "type": "article"
  },
  {
    "title": "Instruction Driven Cross-layer CNN Accelerator for Fast Detection on FPGA",
    "doi": "https://doi.org/10.1145/3283452",
    "publication_date": "2018-09-30",
    "publication_year": 2018,
    "authors": "Jincheng Yu; Guangjun Ge; Yiming Hu; Xuefei Ning; Jiantao Qiu; Kaiyuan Guo; Yu Wang; Huazhong Yang",
    "corresponding_authors": "",
    "abstract": "In recent years, Convolutional Neural Networks (CNNs) have been widely applied in computer vision and have achieved significant improvements in object detection tasks. Although there are many optimizing methods to speed up CNN-based detection algorithms, it is still difficult to deploy detection algorithms on real-time low-power systems. Field-Programmable Gate Array (FPGA) has been widely explored as a platform for accelerating CNN due to its promising performance, high energy efficiency, and flexibility. Previous works show that the energy consumption of CNN accelerators is dominated by the memory access. By fusing multiple layers in CNN, the intermediate data transfer can be reduced. However, previous accelerators with the cross-layer scheduling are designed for a particular CNN model. In addition to the memory access optimization, the Winograd algorithm can greatly improve the computational performance of convolution. In this article, to improve the flexibility of hardware, we design an instruction-driven CNN accelerator, supporting the Winograd algorithm and the cross-layer scheduling, for object detection. We modify the loop unrolling order of CNN, so that we can schedule a CNN across different layers with instructions and eliminate the intermediate data transfer. We propose a hardware architecture to support the instructions with Winograd computation units and reach the state-of-the-art energy efficiency. To deploy image detection algorithms onto the proposed accelerator with fixed-point computation units, we adopt the fixed-point fine-tune method, which can guarantee the accuracy of the detection algorithms. We evaluate our accelerator and scheduling policy on the Xilinx KU115 FPGA platform. The intermediate data transfer can be reduced by more than 90% on the VGG-D CNN model with the cross-layer strategy. Thus, the performance of our hardware accelerator reaches 1700GOP/s on the classification model VGG-D. We also implement a framework for object detection algorithms, which achieves 2.3× and 50× in energy efficiency compared with GPU and CPU, respectively. Compared with floating-point algorithms, the accuracy of the fixed-point detection algorithms only drops by less than 1%.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2903778355",
    "type": "article"
  },
  {
    "title": "High-Level Abstractions and Modular Debugging for FPGA Design Validation",
    "doi": "https://doi.org/10.1145/2567662",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "Yousef Iskander; Cameron Patterson; Stephen Craven",
    "corresponding_authors": "",
    "abstract": "Design validation is the most time-consuming task in the FPGA design cycle. Although manufacturers and third-party vendors offer a range of tools that provide visibility and control of the different stages of a design, many require that the design be fully re-implemented for even simple parameter modifications or do not allow the design to be run at full speed. Designs are typically first modeled using a high-level language then later rewritten in a hardware description language, first for simulation and then later modified for synthesis. IP and third-party cores may differ during these final two stages complicating development and validation. The developed approach provides two means of directly validating synthesized hardware designs. The first allows the original high-level model written in C or C++ to be directly coupled to the synthesized hardware, abstracting away the traditional gate-level view of designs. A high-level programmatic interface allows the synthesized design to be validated directly by the software reference model. The second approach provides an alternative view to FPGAs within the scope of a traditional software debugger. This debug framework leverages partially reconfigurable regions to accelerate the modification of dynamic, software-like breakpoints for low-level analysis and provides a automatable, scriptable, command-line interface directly to a running design on an FPGA.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W1994052066",
    "type": "article"
  },
  {
    "title": "Feel Free to Interrupt",
    "doi": "https://doi.org/10.1145/3372491",
    "publication_date": "2020-01-28",
    "publication_year": 2020,
    "authors": "Sameh Attia; Vaughn Betz",
    "corresponding_authors": "",
    "abstract": "Saving and restoring an FPGA task state in an orderly manner is essential to enable hardware checkpointing, which is highly desirable to improve the ability to debug cloud-scale hardware services, and context switching, which allows multiple users to share FPGA resources. However, these features require task interruption, and stopping a task at an arbitrary time can cause several hazards including deadlock and data loss. In this article, we build a context saving and restoring simulator to simulate and identify these hazards. In addition, we derive design rules that should be followed to achieve safe task interruption. Finally, we propose task wrappers that can be placed around an FPGA task to implement these rules. The timing and area overheads added by these wrappers are very small; they add 1.8% area and no timing overhead to a full Memcached system. Taken together, these design rules and wrappers enable safe checkpointing and context switching in a wide variety of FPGA tasks, including those with multiple clocks, multi-cycle I/O transactions, and interface dependencies.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W3013676662",
    "type": "article"
  },
  {
    "title": "MEG",
    "doi": "https://doi.org/10.1145/3409114",
    "publication_date": "2020-09-30",
    "publication_year": 2020,
    "authors": "Jialiang Zhang; Yue Zha; Nicholas Beckwith; Bangya Liu; Jing Li",
    "corresponding_authors": "",
    "abstract": "Emerging three-dimensional (3D) memory technologies, such as the Hybrid Memory Cube (HMC) and High Bandwidth Memory (HBM), provide high-bandwidth and massive memory-level parallelism. With the growing heterogeneity and complexity of computer systems (CPU cores and accelerators, etc.), efficiently integrating emerging memories into existing systems poses new challenges and requires detailed evaluation in a realistic computing environment. In this article, we propose MEG, an open source, configurable, cycle-exact, and RISC-V-based full-system emulation infrastructure using FPGA and HBM. MEG provides a highly modular hardware design and includes a bootable Linux image for a realistic software flow, so that users can perform cross-layer software-hardware co-optimization in a full-system environment. To improve the observability and debuggability of the system, MEG also provides a flexible performance monitoring scheme to guide the performance optimization. The proposed MEG infrastructure can potentially benefit broad communities across computer architecture, system software, and application software. Leveraging MEG, we present two cross-layer system optimizations as illustrative cases to demonstrate the usability of MEG. In the first case study, we present a reconfigurable memory controller to improve the address mapping of standard memory controller. This reconfigurable memory controller along with its OS support allows us to optimize the address mapping scheme to fully exploit the massive parallelism provided by the emerging three-dimensional (3D) memories. In the second case study, we present a lightweight IOMMU design to tackle the unique challenges brought by 3D memory in providing virtual memory support for near-memory accelerators. We provide a prototype implementation of MEG on a Xilinx VU37P FPGA and demonstrate its capability, fidelity, and flexibility on real-world benchmark applications. We hope MEG fills a gap in the space of publicly available FPGA-based full-system emulation infrastructures, specifically targeting memory systems, and inspires further collaborative software/hardware innovations.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W3091524708",
    "type": "article"
  },
  {
    "title": "Near-memory Computing on FPGAs with 3D-stacked Memories: Applications, Architectures, and Optimizations",
    "doi": "https://doi.org/10.1145/3547658",
    "publication_date": "2022-07-18",
    "publication_year": 2022,
    "authors": "Veronia Iskandar; Mohamed A. Abd El Ghany; Diana Göhringer",
    "corresponding_authors": "",
    "abstract": "The near-memory computing (NMC) paradigm has transpired as a promising method for overcoming the memory wall challenges of future computing architectures. Modern systems integrating 3D-stacked DRAM memory can be leveraged to prevent unnecessary data movement between the main memory and the CPU. FPGA vendors have started introducing 3D memories to their products in an effort to remain competitive on bandwidth requirements of modern memory-intensive applications. Recent NMC proposals target various types of data processing workloads such as graph processing, MapReduce, sorting, machine learning, and database analytics. In this article, we conduct a literature survey on previous proposals of NMC systems on FPGAs integrated with 3D memories. By leveraging the high bandwidth offered from such memories together with specifically designed hardware, FPGA architectures have become a competitor to GPU solutions in terms of speed and energy efficiency. Various FPGA-based NMC designs have been proposed with software and hardware optimization methods to achieve high performance and energy efficiency. Our review investigates various aspects of NMC designs such as platforms, architectures, workloads, and tools. We identify the key challenges and open issues with future research directions.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4285730081",
    "type": "article"
  },
  {
    "title": "High-performance and Configurable SW/HW Co-design of Post-quantum Signature CRYSTALS-Dilithium",
    "doi": "https://doi.org/10.1145/3569456",
    "publication_date": "2022-10-25",
    "publication_year": 2022,
    "authors": "Gaoyu Mao; Donglong Chen; Guangyan Li; Wangchen Dai; Abdurrashid Ibrahim Sanka; Çetin Kaya Koç; Ray C. C. Cheung",
    "corresponding_authors": "",
    "abstract": "CRYSTALS-Dilithium is a lattice-based post-quantum digital signature scheme that is resistant to attacks by quantum computers and has been selected to be standardized in the NIST post-quantum cryptography (PQC) standardization process. However, the speed performance and design flexibility of the Dilithium still need to be evaluated. This article presents a high-performance software/hardware co-design of CRYSTALS-Dilithium based on the NIST PQC round-3 parameters. High-speed pipelined hardware modules for NTT/INTT, point-wise multiplication/addition, and for SHAKE are included in the design to accelerate the time-consuming operations in Dilithium. All hardware modules are parameterized, thus allowing full support of runtime configuration to increase versatility. Moreover, the proposed software/hardware architecture and tight operating workflows reduce the data transmission overhead between the processor and other hardware modules. The hardware accelerator is implemented with a reconfigurable logic on FPGA and is integrated with the high-performance ARM Cortex-A9 processor in the Xilinx Zynq Architecture. We measure the performance of the software/hardware system for Dilithium in NIST security levels 2, 3, and 5. Compared to pure software implementations, we achieve 8.7–12.5 times speedup in Key generation, 6.3–7.3 times speedup in Sign, and 9.1–12.2 times speedup in Verify operations.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W4307380322",
    "type": "article"
  },
  {
    "title": "High Throughput FPGA-Based Object Detection via Algorithm-Hardware Co-Design",
    "doi": "https://doi.org/10.1145/3634919",
    "publication_date": "2023-12-04",
    "publication_year": 2023,
    "authors": "Anupreetham Anupreetham; Mohamed Ibrahim; Mathew Hall; Andrew Boutros; Ajay Kuzhively; Abinash Mohanty; Eriko Nurvitadhi; Vaughn Betz; Yu Cao; Jae-sun Seo",
    "corresponding_authors": "",
    "abstract": "Object detection and classification is a key task in many computer vision applications such as smart surveillance and autonomous vehicles. Recent advances in deep learning have significantly improved the quality of results achieved by these systems, making them more accurate and reliable in complex environments. Modern object detection systems make use of lightweight convolutional neural networks (CNNs) for feature extraction, coupled with single-shot multi-box detectors (SSDs) that generate bounding boxes around the identified objects along with their classification confidence scores. Subsequently, a non-maximum suppression (NMS) module removes any redundant detection boxes from the final output. Typical NMS algorithms must wait for all box predictions to be generated by the SSD-based feature extractor before processing them. This sequential dependency between box predictions and NMS results in a significant latency overhead and degrades the overall system throughput, even if a high-performance CNN accelerator is used for the SSD feature extraction component. In this paper, we present a novel pipelined NMS algorithm that eliminates this sequential dependency and associated NMS latency overhead. We then use our novel NMS algorithm to implement an end-to-end fully pipelined FPGA system for low-latency SSD-MobileNet-V1 object detection. Our system, implemented on an Intel Stratix 10 FPGA, runs at 400 MHz and achieves a throughput of 2,167 frames per second with an end-to-end batch-1 latency of 2.13 ms. Our system achieves 5.3× higher throughput and 5× lower latency compared to the best prior FPGA-based solution with comparable accuracy.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4389299466",
    "type": "article"
  },
  {
    "title": "Perfecto",
    "doi": "https://doi.org/10.1145/1391732.1391737",
    "publication_date": "2008-09-01",
    "publication_year": 2008,
    "authors": "Pao‐Ann Hsiung; Chao-Sheng Lin; Chih-Feng Liao",
    "corresponding_authors": "",
    "abstract": "To cope with increasing demands for higher computational power and greater system flexibility, dynamically and partially reconfigurable logic has started to play an important role in embedded systems and systems-on-chip (SoC). However, when using traditional design methods and tools, it is difficult to estimate or analyze the performance impact of including such reconfigurable logic devices into a system design. In this work, we present a system-level framework, called Perfecto, which is able to perform rapid exploration of different reconfigurable design alternatives and to detect system performance bottlenecks. This framework is based on the popular IEEE standard system-level design language SystemC, which is supported by most EDA and ESL tools. Given an architecture model and an application model, Perfecto uses SystemC transaction-level models (TLMs) to simulate the system design alternatives automatically. Different hardware-software copartitioning, coscheduling, and placement algorithms can be embedded into the framework for analysis; thus, Perfecto can also be used to design the algorithms to be used in an operating system for reconfigurable systems. Applications to a simple illustration example and a network security system have shown how Perfecto helps a designer make intelligent partition decisions, optimize system performance, and evaluate task placements.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W1971600554",
    "type": "article"
  },
  {
    "title": "A-Port Networks",
    "doi": "https://doi.org/10.1145/1575774.1575775",
    "publication_date": "2009-09-01",
    "publication_year": 2009,
    "authors": "Michael Pellauer; Muralidaran Vijayaraghavan; Michael Adler; Arvind Arvind; Joel Emer",
    "corresponding_authors": "",
    "abstract": "Computer architects need to run cycle-accurate performance models of processors orders of magnitude faster. We discuss why the speedup on traditional multicores is limited, and why FPGAs represent a good vehicle to achieve a dramatic performance improvement over software models. This article introduces A-Port Networks, a simulation scheme designed to expose the fine-grained parallelism inherent in performance models and efficiently exploit them using FPGAs.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2057962492",
    "type": "article"
  },
  {
    "title": "TAS-MRAM-Based Low-Power High-Speed Runtime Reconfiguration (RTR) FPGA",
    "doi": "https://doi.org/10.1145/1534916.1534918",
    "publication_date": "2009-06-01",
    "publication_year": 2009,
    "authors": "Weisheng Zhao; Eric Belhaire; Claude Chappert; B. Diény; Guillaume Prenat",
    "corresponding_authors": "",
    "abstract": "As one of the most promising Spintronics applications, MRAM combines the advantages of high writing and reading speed, limitless endurance, and nonvolatility. The integration of MRAM in FPGAs allows the logic circuit to rapidly configure the algorithm, the routing and logic functions, and easily realize the Runtime Reconfiguration (RTR) and multicontext configuration. However, the conventional MRAM technology based on the Field Induced Magnetic Switching (FIMS) writing approach consumes very high power, large circuit surfaces, and produces high disturbance between memory cells. These drawbacks prevent FIMS-MRAM’s further development in memory and logic circuit. Thermally Assisted Switching (TAS)-based MRAM is then evaluated to address these issues. In this article, some design techniques, novel computing architecture, and logic components for FPGA logic circuits based on TAS-MRAM technology are presented. By using STMicroelectronics CMOS 90nm technology and a complete TAS-MTJ spice model, some chip characteristic results such as the programming latency (~25ns) and power dissipation (~124pJ) have been calculated or simulated to demonstrate the expected performance of TAS-MRAM-based FPGA logic circuits.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2171051477",
    "type": "article"
  },
  {
    "title": "TR-FSM",
    "doi": "https://doi.org/10.1145/2000832.2000835",
    "publication_date": "2011-08-01",
    "publication_year": 2011,
    "authors": "Johann Glaser; Markus Damm; Jan Haase; Christoph Grimm",
    "corresponding_authors": "",
    "abstract": "Finite State Machines (FSMs) are a key element of integrated circuits. Hard-coded FSMs do not allow changes after the ASIC production. While an embedded FPGA IP core provides flexibility, it is a complex circuit, requires difficult synthesis tools, and is expensive. This article presents and evaluates a novel architecture that is specifically optimized for implementing reconfigurable finite state machines: Transition-based Reconfigurable FSM (TR-FSM). The architecture shows a considerable reduction in area, delay, and power consumption compared to FPGA architectures with a (nearly) FPGA-like reconfigurability.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W1970426991",
    "type": "article"
  },
  {
    "title": "A Comparison Study on Implementing Optical Flow and Digital Communications on FPGAs and GPUs",
    "doi": "https://doi.org/10.1145/1754386.1754387",
    "publication_date": "2010-05-01",
    "publication_year": 2010,
    "authors": "John Bodily; Brent Nelson; Zhaoyi Wei; Dah-Jye Lee; Jeff Chase",
    "corresponding_authors": "",
    "abstract": "FPGA devices have often found use as higher-performance alternatives to programmable processors for implementing computations. Applications successfully implemented on FPGAs typically contain high levels of parallelism and often use simple statically scheduled control and modest arithmetic. Recently introduced computing devices such as coarse-grain reconfigurable arrays, multi-core processors, and graphical processing units promise to significantly change the computational landscape and take advantage of many of the same application characteristics that fit well on FPGAs. One real-time computing task, optical flow, is difficult to apply in robotic vision applications because of its high computational and data rate requirements, and so is a good candidate for implementation on FPGAs and other custom computing architectures. This article reports on a series of experiments mapping a collection of different algorithms onto both an FPGA and a GPU. For two different optical flow algorithms the GPU had better performance, while for a set of digital comm MIMO computations, they had similar performance. In all cases the FPGA implementations required 10x the development time. Finally, a discussion of the two technology’s characteristics is given to show they achieve high performance in different ways.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2024447729",
    "type": "article"
  },
  {
    "title": "Choose-your-own-adventure routing",
    "doi": "https://doi.org/10.1145/2068716.2068719",
    "publication_date": "2011-12-01",
    "publication_year": 2011,
    "authors": "Raphael Rubin; André DeHon",
    "corresponding_authors": "",
    "abstract": "Aggressive scaling increases the number of devices we can integrate per square millimeter but makes it increasingly difficult to guarantee that each device fabricated has the intended operational characteristics. Without careful mitigation, component yield rates will fall, potentially negating the economic benefits of scaling. The fine-grained reconfigurability inherent in FPGAs is a powerful tool that can allow us to drop the stringent requirement that every device be fabricated perfectly in order for a component to be useful. To exploit inherent FPGA reconfigurability while avoiding full CAD mapping, we propose lightweight techniques compatible with the current single bitstream model that can avoid defective devices, reducing yield loss at high defect rates. In particular, by embedding testing operations and alternative path configurations into the bitstream, each FPGA can avoid defects by making only simple, greedy decisions at bitstream load time. With 20% additional tracks above the minimum routable channel width, routes can tolerate 0.01% switch and wire defect rates, raising yield from essentially 0% to near 100%.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2090396933",
    "type": "article"
  },
  {
    "title": "An analytical model for multilevel performance prediction of Multi-FPGA systems",
    "doi": "https://doi.org/10.1145/2000832.2000839",
    "publication_date": "2011-08-01",
    "publication_year": 2011,
    "authors": "Brian Holland; Alan D. George; Herman Lam; Melissa C. Smith",
    "corresponding_authors": "",
    "abstract": "Power limitations in semiconductors have made explicitly parallel device architectures such as Field-Programmable Gate Arrays (FPGAs) increasingly attractive for use in scalable systems. However, mitigating the significant cost of FPGA development requires efficient design-space exploration to plan and evaluate a range of potential algorithm and platform choices prior to implementation. The authors propose the RC Amenability Test for Scalable Systems (RATSS), an analytical model which enables straightforward, fast, and reasonably accurate performance prediction prior to implementation by extending current modeling concepts to multi-FPGA designs. RATSS provides a comprehensive strategic model to evaluate applications based on the computation and communication requirements of the algorithm and capabilities of the FPGA platform. The RATSS model targets data-parallel applications on current scalable FPGA systems. Three case studies with RATSS demonstrate nearly 90% prediction accuracy as compared to corresponding implementations.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2031423893",
    "type": "article"
  },
  {
    "title": "A novel framework for exploring 3-D FPGAs with heterogeneous interconnect fabric",
    "doi": "https://doi.org/10.1145/2133352.2133356",
    "publication_date": "2012-03-01",
    "publication_year": 2012,
    "authors": "Kostas Siozios; Vasilis F. Pavlidis; Dimitrios Soudris",
    "corresponding_authors": "",
    "abstract": "A heterogeneous interconnect architecture can be a useful approach for the design of 3-D FPGAs. A methodology to investigate heterogeneous interconnection schemes for 3-D FPGAs under different 3-D fabrication technologies is proposed. Application of the proposed methodology on benchmark circuits demonstrates an improvement in delay, power consumption, and total wire-length of approximately 41%, 32%, and 36%, respectively, as compared to 2-D FPGAs. These improvements are additional to reducing the number of interlayer connections. The fewer interlayer connections are traded off for a higher yield. An area model to evaluate this trade-off is presented. Results indicate that a heterogeneous 3-D FPGA requires 37% less area as compared to a homogeneous 3-D FPGA. Consequently, the heterogeneous FPGAs can exhibit a higher manufacturing yield. A design toolset is also developed to support the design and exploration of various performance metrics for the proposed 3-D FPGAs.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2103755335",
    "type": "article"
  },
  {
    "title": "Self-Awareness as a Model for Designing and Operating Heterogeneous Multicores",
    "doi": "https://doi.org/10.1145/2617596",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "Andreas Agne; Markus Happe; Achim Lösch; Christian Plessl; Marco Platzner",
    "corresponding_authors": "",
    "abstract": "Self-aware computing is a paradigm for structuring and simplifying the design and operation of computing systems that face unprecedented levels of system dynamics and thus require novel forms of adaptivity. The generality of the paradigm makes it applicable to many types of computing systems and, previously, researchers started to introduce concepts of self-awareness to multicore architectures. In our work we build on a recent reference architectural framework as a model for self-aware computing and instantiate it for an FPGA-based heterogeneous multicore running the ReconOS reconfigurable architecture and operating system. After presenting the model for self-aware computing and ReconOS, we demonstrate with a case study how a multicore application built on the principle of self-awareness, autonomously adapts to changes in the workload and system state. Our work shows that the reference architectural framework as a model for self-aware computing can be practically applied and allows us to structure and simplify the design process, which is essential for designing complex future computing systems.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2151600945",
    "type": "article"
  },
  {
    "title": "Reconfigurable Hardware Architecture for Authenticated Key Agreement Protocol Over Binary Edwards Curve",
    "doi": "https://doi.org/10.1145/3231743",
    "publication_date": "2018-06-30",
    "publication_year": 2018,
    "authors": "N. Nalla Anandakumar; M. Prem Laxman Das; Somitra Kumar Sanadhya; Mohammad Hashmi",
    "corresponding_authors": "",
    "abstract": "In this article, we present a high-performance hardware architecture for Elliptic curve based (authenticated) key agreement protocol “Elliptic Curve Menezes, Qu and Vanstone” (ECMQV) over Binary Edwards Curve (BEC). We begin by analyzing inversion module on a 251-bit binary field. Subsequently, we present Field Programmable Gate Array (FPGA) implementations of the unified formula for computing elliptic curve point addition on BEC in affine and projective coordinates and investigate the relative performance of these two coordinates. Then, we implement the w -coordinate based differential addition formulae suitable for usage in Montgomery ladder. Next, we present a novel hardware architecture of BEC point multiplication using mixed w -coordinates of the Montgomery laddering algorithm and analyze it in terms of resistance to Simple Power Analysis (SPA) attack. In order to improve the performance, the architecture utilizes registers efficiently and uses efficient scheduling mechanisms for the BEC arithmetic implementations. Our implementation results show that the proposed architecture is resistant against SPA attack and yields a better performance when compared to the existing state-of-the-art BEC designs for computing point multiplication (PM). Finally, we present an FPGA design of ECMQV key agreement protocol using BEC defined over GF(2 251 ). The execution of ECMQV protocol takes 66.47μs using 32,479 slices on Virtex-4 FPGA and 52.34μs using 15,988 slices on Virtex-5 FPGA. To the best of our knowledge, this is the first FPGA design of the ECMQV protocol using BEC.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2899912039",
    "type": "article"
  },
  {
    "title": "Optimizing Bit-Serial Matrix Multiplication for Reconfigurable Computing",
    "doi": "https://doi.org/10.1145/3337929",
    "publication_date": "2019-08-20",
    "publication_year": 2019,
    "authors": "Yaman Umuroglu; Davide Conficconi; Lahiru Rasnayake; Thomas B. Preußer; Magnus Själander",
    "corresponding_authors": "",
    "abstract": "Matrix-matrix multiplication is a key computational kernel for numerous applications in science and engineering, with ample parallelism and data locality that lends itself well to high-performance implementations. Many matrix multiplication-dependent applications can use reduced-precision integer or fixed-point representations to increase their performance and energy efficiency while still offering adequate quality of results. However, precision requirements may vary between different application phases or depend on input data, rendering constant-precision solutions ineffective. BISMO, a vectorized bit-serial matrix multiplication overlay for reconfigurable computing, previously utilized the excellent binary-operation performance of FPGAs to offer a matrix multiplication performance that scales with required precision and parallelism. We show how BISMO can be scaled up on Xilinx FPGAs using an arithmetic architecture that better utilizes 6-LUTs. The improved BISMO achieves a peak performance of 15.4 binary TOPS on the Ultra96 board with a Xilinx UltraScale+ MPSoC.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W3102383356",
    "type": "article"
  },
  {
    "title": "Dynamic Energy, Performance, and Accuracy Optimization and Management Using Automatically Generated Constraints for Separable 2D FIR Filtering for Digital Video Processing",
    "doi": "https://doi.org/10.1145/2629623",
    "publication_date": "2014-12-29",
    "publication_year": 2014,
    "authors": "Daniel Llamocca; Marios S. Pattichis",
    "corresponding_authors": "",
    "abstract": "There is strong interest in the development of dynamically reconfigurable systems that can meet real-time constraints on energy, performance, and accuracy. The generation of real-time constraints will significantly expand the applicability of dynamically reconfigurable systems to new domains, such as digital video processing. We develop a dynamically reconfigurable 2D FIR filtering system that can meet real-time constraints in energy, performance, and accuracy (EPA). The real-time constraints are automatically generated based on user input, image types associated with video communications, and video content. We first generate a set of Pareto-optimal realizations, described by their EPA values and associated 2D FIR hardware description bitstreams. Dynamic management is then achieved by selecting Pareto-optimal realizations that meet the automatically generated time-varying EPA constraints. We validate our approach using three different 2D Gaussian filters. Filter realizations are evaluated in terms of the required energy per frame, accuracy of the resulting image, and performance in frames per second. We demonstrate dynamic EPA management by applying a Difference of Gaussians (DOG) filter to standard video sequences. For video frame sizes that are equal to or larger than the VGA resolution, compared to a static implementation, our dynamic system provides significant reduction in the total energy consumption (&gt;30%).",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2085256114",
    "type": "article"
  },
  {
    "title": "CoEx",
    "doi": "https://doi.org/10.1145/2629563",
    "publication_date": "2015-05-04",
    "publication_year": 2015,
    "authors": "Juan Fernando Eusse; Christopher Williams; Rainer Leupers",
    "corresponding_authors": "",
    "abstract": "Application-Specific Instruction Set Processors (ASIPs) provide the adequate performance/efficiency tradeoff for their particular application domain. Nevertheless, their design methodologies have stagnated during the past decade and are still based on a series of manual and time-consuming iterative steps. Furthermore, there exists a productivity gap between the point where an application is given as the target for processor customization and the time a customized architecture is available. Therefore, new tools are required that reduce the number of design iterations and bridge the aforementioned productivity gap. This can be achieved by (1) profiling technologies that, by adapting to the designer’s needs, help to gain insight into application specifications, and (2) prearchitectural design technologies that give early yet accurate feedback on the impact of algorithmic/architectural design decisions. The first requirement is addressed in this article by proposing the multigrained profiling approach, which identifies the profiling needs at each step of ASIP design and lets the designer tailor the level of detail for application inspection. CoEx, a practical implementation of the approach, is also introduced. The second requirement is addressed by creating a prearchitectural estimation engine . This engine couples CoEx reports for an application with an abstract processor model and generates an estimate of the achievable performance. Both CoEx and the performance estimation engine are respectively evaluated for instrumentation-induced execution overhead and accuracy. Finally, the development of an ASIP architecture for an augmented reality computer vision application is presented. The ASIP achieves a gain of six times compared to the original application performance, after being developed in only 2 days.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2201491784",
    "type": "article"
  },
  {
    "title": "Modular Switched Multiported SRAM-Based Memories",
    "doi": "https://doi.org/10.1145/2851506",
    "publication_date": "2016-07-14",
    "publication_year": 2016,
    "authors": "Ameer Abdelhadi; Guy Lemieux",
    "corresponding_authors": "",
    "abstract": "Multiported RAMs are essential for high-performance parallel computation systems. VLIW and vector processors, CGRAs, DSPs, CMPs, and other processing systems often rely upon multiported memories for parallel access. Although memories with a large number of read and write ports are important, their high implementation cost means that they are used sparingly. As a result, FPGA vendors only provide dual-ported block RAMs (BRAMs) to handle the majority of usage patterns. Furthermore, recent attempts to create FPGA-based multiported memories suffer from low storage utilization. Whereas most approaches provide simple unidirectional ports with a fixed read or write, others propose true bidirectional ports where each port dynamically switches read and write. True RAM ports are useful for systems with transceivers and provide high RAM flexibility; however, this flexibility incurs high BRAM consumption. In this article, a novel, modular, and BRAM-based switched multiported RAM architecture is proposed. In addition to unidirectional ports with fixed read/write, this switched architecture allows a group of write ports to switch with another group of read ports dynamically, hence altering the number of active ports. The proposed switched-ports architecture is less flexible than a true-multiported RAM where each port is switched individually. Nevertheless, switched memories can dramatically reduce BRAM consumption compared to true ports for systems with alternating port requirements. Previous live-value-table (LVT) and XOR approaches are merged and optimized into a generalized and modular structure that we call an invalidation-based live-value-table (I-LVT). Like a regular LVT, the I-LVT determines the correct bank to read from, but it differs in how updates to the table are made; the LVT approach requires multiple write ports, often leading to an area-intensive register-based implementation, whereas the XOR approach suffers from excessive storage overhead since wider memories are required to accommodate the XOR-ed data. Two specific I-LVT implementations are proposed and evaluated: binary and thermometer coding. The I-LVT approach is especially suitable for deep memories because the table is implemented only in SRAM cells. The I-LVT method gives higher performance while occupying fewer BRAMs than earlier approaches: for several configurations, BRAM usage is reduced by greater than 44% and clock speed is improved by greater than 76%. The I-LVT can be used with fixed ports, true ports, or the proposed switched ports architectures. Formal proofs for the suggested methods, resources consumption analysis, usage guidelines, and analytic comparison to other methods are provided. A fully parameterized Verilog implementation is released as an open source library. The library has been extensively tested using Altera’s EDA tools.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2469520437",
    "type": "article"
  },
  {
    "title": "Generating Efficient Context-Switch Capable Circuits through Autonomous Design Flow",
    "doi": "https://doi.org/10.1145/2996199",
    "publication_date": "2016-12-09",
    "publication_year": 2016,
    "authors": "Alban Bourge; Olivier Muller; Frédéric Rousseau",
    "corresponding_authors": "",
    "abstract": "Commercial off-the-shelf (COTS) Field-Programmable Gate Arrays (FPGAs) are becoming increasingly powerful. In addition to their huge hardware resources, they are also integrated into complete systems on chips (SOCs), e.g., in the latest Xilinx Zynq or Altera Stratix platforms. However, cooperation between FPGAs and their surroundings, and the flexibility of hardware task management could still be improved. For instance, mechanisms have yet to be automated to allow multi-user approaches. A reconfigurable resource can be shared between applications or users only if it has a context-switch ability allowing applications to be paused and resumed in response to system demands. Here, we present a high-level synthesis (HLS) design flow producing a context-switch-capable circuit. The design flow manipulates the intermediate representation of an HLS tool to build the context extraction mechanism and to optimize performance for the circuit produced. The method is based on efficient checkpoint selection and insertion of a powerful scan-chain into the initial circuit. This scan-chain can extract flip-flops or memory content. Experiments with the system produced show that it has a low hardware overhead for many benchmark applications, and that the hardware added has a negligible impact on application performance. Comparisons with current standard methods highlight the efficiency of our contributions.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2559960820",
    "type": "article"
  },
  {
    "title": "ThunderGP: Resource-Efficient Graph Processing Framework on FPGAs with HLS",
    "doi": "https://doi.org/10.1145/3517141",
    "publication_date": "2022-03-05",
    "publication_year": 2022,
    "authors": "Xinyu Chen; Feng Cheng; Hongshi Tan; Yao Chen; Bingsheng He; Weng‐Fai Wong; Deming Chen",
    "corresponding_authors": "",
    "abstract": "FPGA has been an emerging computing infrastructure in datacenters benefiting from fine-grained parallelism, energy efficiency, and reconfigurability. Meanwhile, graph processing has attracted tremendous interest in data analytics, and its performance is in increasing demand with the rapid growth of data. Many works have been proposed to tackle the challenges of designing efficient FPGA-based accelerators for graph processing. However, the largely overlooked programmability still requires hardware design expertise and sizable development efforts from developers. ThunderGP , a high-level synthesis based graph processing framework on FPGAs, is hence proposed to close the gap, with which developers could enjoy high performance of FPGA-accelerated graph processing by writing only a few high-level functions with no knowledge of the hardware. ThunderGP adopts the gather-apply-scatter model as the abstraction of various graph algorithms and realizes the model by a built-in highly parallel and memory-efficient accelerator template. With high-level functions as inputs, ThunderGP automatically explores massive resources of multiple super-logic regions of modern FPGA platforms to generate and deploy accelerators, as well as schedule tasks for them. Although ThunderGP on DRAM-based platforms is memory bandwidth bounded, recent high bandwidth memory (HBM) brings large potentials to performance. However, the system bottleneck shifts from memory bandwidth to resource consumption on HBM-enabled platforms. Therefore, we further propose to improve resource efficiency of ThunderGP to utilize more memory bandwidth from HBM. We conduct evaluation with seven common graph applications and 19 graphs. ThunderGP on DRAM-based hardware platforms provides 1.9× ∼ 5.2× improvement on bandwidth efficiency over the state of the art, whereas ThunderGP on HBM-based hardware platforms delivers up to 5.2× speedup over the state-of-the-art RTL-based approach. This work is open sourced on GitHub at https://github.com/Xtra-Computing/ThunderGP .",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4220883019",
    "type": "article"
  },
  {
    "title": "Tensor Slices: FPGA Building Blocks For The Deep Learning Era",
    "doi": "https://doi.org/10.1145/3529650",
    "publication_date": "2022-04-13",
    "publication_year": 2022,
    "authors": "Aman Arora; Moinak Ghosh; Samidh Mehta; Vaughn Betz; Lizy K. John",
    "corresponding_authors": "",
    "abstract": "FPGAs are well-suited for accelerating deep learning (DL) applications owing to the rapidly changing algorithms, network architectures and computation requirements in this field. However, the generic building blocks available on traditional FPGAs limit the acceleration that can be achieved. Many modifications to FPGA architecture have been proposed and deployed including adding specialized artificial intelligence (AI) processing engines, adding support for smaller precision math like 8-bit fixed point and IEEE half-precision (fp16) in DSP slices, adding shadow multipliers in logic blocks, etc. In this paper, we describe replacing a portion of the FPGA’s programmable logic area with Tensor Slices. These slices have a systolic array of processing elements at their heart that support multiple tensor operations, multiple dynamically-selectable precisions and can be dynamically fractured into individual multipliers and MACs (multiply-and-accumulate). These slices have a local crossbar at the inputs that helps with easing the routing pressure caused by a large block on the FPGA. Adding these DL-specific coarse-grained hard blocks to FPGAs increases their compute density and makes them even better hardware accelerators for DL applications, while still keeping the vast majority of the real estate on the FPGA programmable at fine-grain.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4224010033",
    "type": "article"
  },
  {
    "title": "A Reconfigurable Architecture for Real-time Event-based Multi-Object Tracking",
    "doi": "https://doi.org/10.1145/3593587",
    "publication_date": "2023-04-21",
    "publication_year": 2023,
    "authors": "Yizhao Gao; Song Wang; Hayden Kwok‐Hay So",
    "corresponding_authors": "",
    "abstract": "Although advances in event-based machine vision algorithms have demonstrated unparalleled capabilities in performing some of the most demanding tasks, their implementations under stringent real-time and power constraints in edge systems remain a major challenge. In this work, a reconfigurable hardware-software architecture called REMOT, which performs real-time event-based multi-object tracking on FPGAs, is presented. REMOT performs vision tasks by defining a set of actions over attention units (AUs). These actions allow AUs to track an object candidate autonomously by adjusting its region of attention and allow information gathered by each AU to be used for making algorithmic-level decisions. Taking advantage of this modular structure, algorithm-architecture codesign can be performed by implementing different parts of the algorithm in either hardware or software for different tradeoffs. Results show that REMOT can process 0.43–2.91 million events per second at 1.75–5.45 W. Compared with the software baseline, our implementation achieves up to 44 times higher throughput and 35.4 times higher power efficiency. Migrating the Merge operation to hardware further reduces the worst-case latency to be 95 times shorter than the software baseline. By varying the AU configuration and operation, a reduction of 0.59–0.77 mW per AU on the programmable logic has also been demonstrated.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4366774363",
    "type": "article"
  },
  {
    "title": "CoMeFa: Deploying Compute-in-Memory on FPGAs for Deep Learning Acceleration",
    "doi": "https://doi.org/10.1145/3603504",
    "publication_date": "2023-06-05",
    "publication_year": 2023,
    "authors": "Aman Arora; Atharva Rahul Bhamburkar; Aatman Borda; Tanmay Anand; Rishabh Sehgal; Bagus Hanindhito; Pierre‐Emmanuel Gaillardon; Jaydeep P. Kulkarni; Lizy K. John",
    "corresponding_authors": "",
    "abstract": "Block random access memories (BRAMs) are the storage houses of FPGAs, providing extensive on-chip memory bandwidth to the compute units implemented using logic blocks and digital signal processing slices. We propose modifying BRAMs to convert them to CoMeFa ( Co mpute-in- Me mory Blocks for F PG A s) random access memories (RAMs). These RAMs provide highly parallel compute-in-memory by combining computation and storage capabilities in one block. CoMeFa RAMs utilize the true dual-port nature of FPGA BRAMs and contain multiple configurable single-bit bit-serial processing elements. CoMeFa RAMs can be used to compute with any precision, which is extremely important for applications like deep learning (DL). Adding CoMeFa RAMs to FPGAs significantly increases their compute density while also reducing data movement. We explore and propose two architectures of these RAMs: CoMeFa-D (optimized for delay) and CoMeFa-A (optimized for area). Compared to existing proposals, CoMeFa RAMs do not require changing the underlying static RAM technology like simultaneously activating multiple wordlines on the same port, and are practical to implement. CoMeFa RAMs are especially suitable for parallel and compute-intensive applications like DL, but these versatile blocks find applications in diverse applications like signal processing and databases, among others. By augmenting an Intel Arria 10–like FPGA with CoMeFa-D (CoMeFa-A) RAMs at the cost of 3.8% (1.2%) area, and with algorithmic improvements and efficient mapping, we observe a geomean speedup of 2.55× (1.85×) across microbenchmarks from various applications and a geomean speedup of up to 2.5× across multiple deep neural networks. Replacing all or some BRAMs with CoMeFa RAMs in FPGAs can make them better accelerators of DL workloads.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4379388660",
    "type": "article"
  },
  {
    "title": "Programmable Analog System Benchmarks Leading to Efficient Analog Computation Synthesis",
    "doi": "https://doi.org/10.1145/3625298",
    "publication_date": "2023-09-29",
    "publication_year": 2023,
    "authors": "Jennifer Hasler; Cong Hao",
    "corresponding_authors": "",
    "abstract": "This effort develops the first rich suite of analog and mixed-signal benchmark of various sizes and domains, intended for use with contemporary analog and mixed-signal designs and synthesis tools. Benchmarking enables analog-digital co-design exploration as well as extensive evaluation of analog synthesis tools and the generated analog/mixed-signal circuit or device. The goals of this effort are defining analog computation system benchmarks, developing the required concepts for higher-level analog and mixed-signal tools to utilize these benchmarks, and enabling future automated architectural design space exploration (DSE) to determine the best configurable architecture (e.g., a new FPAA) for a certain family of applications. The benchmarks comprise multiple levels of an acoustic , a vision , a communications , and an analog filter system that must be simultaneously satisfied for a complete system.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4387188574",
    "type": "article"
  },
  {
    "title": "HierCGRA: A Novel Framework for Large-scale CGRA with Hierarchical Modeling and Automated Design Space Exploration",
    "doi": "https://doi.org/10.1145/3656176",
    "publication_date": "2024-04-08",
    "publication_year": 2024,
    "authors": "Sichao Chen; Chang Cai; Su Zheng; Jiangnan Li; Guowei Zhu; Jingyuan Li; Yazhou Yan; Yuan Dai; Wenbo Yin; Lingli Wang",
    "corresponding_authors": "",
    "abstract": "Coarse-grained reconfigurable arrays (CGRAs) are promising design choices in computation-intensive domains, since they can strike a balance between energy efficiency and flexibility. A typical CGRA comprises processing elements (PEs) that can execute operations in applications and interconnections between them. Nevertheless, most CGRAs suffer from the ineffectiveness of supporting flexible architecture design and solving large-scale mapping problems. To address these challenges, we introduce HierCGRA, a novel framework that integrates hierarchical CGRA modeling, Chisel-based Verilog generation, LLVM-based data flow graph (DFG) generation, DFG mapping, and design space exploration (DSE). With the graph homomorphism (GH) mapping algorithm, HierCGRA achieves a faster mapping speed and higher PE utilization rate compared with the existing state-of-the-art CGRA frameworks. The proposed hierarchical mapping strategy achieves 41× speedup on average compared with the ILP mapping algorithm in CGRA-ME. Furthermore, the automated DSE based on Bayesian optimization achieves a significant performance improvement by the heterogeneity of PEs and interconnections. With these features, HierCGRA enables the agile development for large-scale CGRA and accelerates the process of finding a better CGRA architecture.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4394569809",
    "type": "article"
  },
  {
    "title": "RD-FAXID: Ransomware Detection with FPGA-Accelerated XGBoost",
    "doi": "https://doi.org/10.1145/3688396",
    "publication_date": "2024-08-12",
    "publication_year": 2024,
    "authors": "Archit Gajjar; Priyank Kashyap; Aydın Aysu; Paul D. Franzon; Yongjin Choi; Chris Cheng; Giacomo Pedretti; Jim Ignowski",
    "corresponding_authors": "",
    "abstract": "Over the last decade, there has been a rise in cyberattacks, particularly ransomware, causing significant disruption and financial repercussions across public and private sectors. Tremendous efforts have been spent on developing techniques to detect ransomware to, ideally, protect data or have as minimum data loss as possible. Ransomware attacks are becoming more frequent and sophisticated as there is a constant tussle between attackers and cybersecurity defenders. Machine Learning (ML) approaches have proven more effective in detecting ransomware than classical signature-based detection. In particular, tree-based algorithms such as Decision Trees (DT), Random Forest (RF), and eXtreme Gradient Boosting (XGBoost) spike up interest among cybersecurity researchers. However, due to the nature of the problem, traditional CPUs and GPUs fail to keep up with the desired performance, especially for large data workloads. Thus, the problem demands a customized solution to detect the ransomware. Here, we propose an FPGA accelerated tree-based ML model for multi-dataset ransomware detection. We show the capability of the proposed prototype to address the problem from more than one set of features, reducing false positive and negative rates to have robust predictions by looking at Hardware Performance Counters (HPCs), Operating System (OS) calls, and network traffic information simultaneously. With 1,000 samples per batch, the FPGA prototype has 65.8 \\({\\times}\\) and 4.1 \\({\\times}\\) lower latency over the CPU and GPU, respectively. Moreover, the FPGA design is up to 11.3 \\({\\times}\\) cost-effective and 643 \\({\\times}\\) energy-efficient compared to the CPU and 3 \\({\\times}\\) cost-effective and 16.8 \\({\\times}\\) energy-efficient over the GPU.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4401527056",
    "type": "article"
  },
  {
    "title": "Graph-OPU: A Highly Flexible FPGA-Based Overlay Processor for Graph Neural Networks",
    "doi": "https://doi.org/10.1145/3691636",
    "publication_date": "2024-09-02",
    "publication_year": 2024,
    "authors": "Enhao Tang; Shun Li; Ruiqi Chen; Hao Zhou; Yuhanxiao Ma; Haoyang Zhang; Jun Yu; Kun Wang",
    "corresponding_authors": "",
    "abstract": "Field-programmable gate arrays (FPGAs) are an ideal candidate for accelerating graph neural networks (GNNs). However, the FPGA redeployment process is time-consuming when updating or switching between diverse GNN models across different applications. Existing GNN processors eliminate the need for FPGA redeployment when switching between different GNN models. However, adapting matrix multiplication types by switching processing units decreases hardware utilization. In addition, the bandwidth of DDR limits further improvements in hardware performance. This article proposes a highly flexible FPGA-based overlay processor for GNN accelerations. Graph-OPU provides excellent flexibility and programmability for users, as the executable code of GNN models is automatically compiled and reloaded without requiring FPGA redeployment. First, we customize the compiler and instruction sets for the inference process of different GNN models. Second, we customize the datapath and optimize the data format in the microarchitecture to fully leverage the advantages of high bandwidth memory (HBM). Third, we design a unified matrix multiplication to handle both sparse-dense matrix multiplication (SpMM) and general matrix multiplication (GEMM), enhancing Graph-OPU performance. During Graph-OPU execution, the computational units are shared between SpMM and GEMM instead of being switched, which improves the hardware utilization. Finally, we implement a hardware prototype on the Xilinx Alveo U50 and test the mainstream GNN models using various datasets. Experimental results show that Graph-OPU achieves up to 1,654 \\(\\times\\) and 63 \\(\\times\\) speedup, as well as up to 5,305 \\(\\times\\) and 422 \\(\\times\\) energy efficiency boosts, compared to implementations on CPU and GPU, respectively. Graph-OPU outperforms state-of-the-art (SOTA) end-to-end overlay accelerators for GNN, reducing latency by an average of 1.36 \\(\\times\\) and improving energy efficiency by 1.41 \\(\\times\\) on average. Moreover, Graph-OPU exhibits an average 1.45 \\(\\times\\) speed improvement in end-to-end latency over the SOTA GNN processor. Graph-OPU represents an in-depth study of an FPGA-based overlay processor for GNNs, offering high flexibility, speedup, and energy efficiency.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4402129561",
    "type": "article"
  },
  {
    "title": "Designing Efficient Input Interconnect Blocks for LUT Clusters Using Counting and Entropy",
    "doi": "https://doi.org/10.1145/1331897.1331902",
    "publication_date": "2008-03-01",
    "publication_year": 2008,
    "authors": "Wenyi Feng; Sinan Kaptanoglu",
    "corresponding_authors": "",
    "abstract": "In a cluster-based FPGA, the interconnect from external routing tracks and cluster feedbacks to the LUT inputs consumes significant area, and no consensus has emerged among different implementations (e.g., 1-level or 2-level). In this paper, we model this interconnect as a unified input interconnect block (IIB). We identify three types of IIBs and develop general combinatorial techniques to count the number of distinct functional configurations for them. We use entropy, defined as the logarithm of this count, to estimate an IIB's routing flexibility. This enables us to analytically evaluate different IIBs without the customary time-consuming place and route experiments. We show that both depopulated 1-level IIBs and VPR-style 2-level IIBs achieve high routing flexibility but lack area efficiency. We propose a novel class of highly efficient, yet still simple, IIBs that use substantially fewer switches with only a small degradation in routing flexibility. Experimental results verify the routability of these IIBs, and confirm that entropy is a good predictor of routability.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2040166817",
    "type": "article"
  },
  {
    "title": "Packing Techniques for Virtex-5 FPGAs",
    "doi": "https://doi.org/10.1145/1575774.1575777",
    "publication_date": "2009-09-01",
    "publication_year": 2009,
    "authors": "Taneem Ahmed; Paul D. Kundarewich; Jason H. Anderson",
    "corresponding_authors": "",
    "abstract": "Packing is a key step in the FPGA tool flow that straddles the boundaries between synthesis, technology mapping and placement. Packing strongly influences circuit speed, density, and power, and in this article, we consider packing in the commercial FPGA context and examine the area and performance trade-offs associated with packing in a state-of-the-art FPGA---the Xilinx ® Virtex TM -5 FPGA. In addition to look-up-table (LUT)-based logic blocks, modern FPGAs also contain large IP blocks. We discuss packing techniques for both types of blocks. Virtex-5 logic blocks contain dual-output 6-input LUTs. Such LUTs can implement any single logic function of up to 6 inputs, or any two logic functions requiring no more than 5 distinct inputs. The second LUT output has reduced speed, and therefore, must be used judiciously. We present techniques for dual-output LUT packing that lead to improved area-efficiency, with minimal performance degradation. We then describe packing techniques for large IP blocks, namely, block RAMs and DSPs. We pack circuits into the large blocks in a way that leverages the unique block RAM and DSP layout/architecture in Virtex-5, achieving significantly improved design performance.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W1991576731",
    "type": "article"
  },
  {
    "title": "FPGA Acceleration of RankBoost in Web Search Engines",
    "doi": "https://doi.org/10.1145/1462586.1462588",
    "publication_date": "2009-01-01",
    "publication_year": 2009,
    "authors": "Ningyi Xu; Xiongfei Cai; Rui Gao; Lei Zhang; Feng-hsiung Hsu",
    "corresponding_authors": "",
    "abstract": "Search relevance is a key measurement for the usefulness of search engines. Shift of search relevance among search engines can easily change a search company's market cap by tens of billions of dollars. With the ever-increasing scale of the Web, machine learning technologies have become important tools to improve search relevance ranking. RankBoost is a promising algorithm in this area, but it is not widely used due to its long training time. To reduce the computation time for RankBoost, we designed a FPGA-based accelerator system and its upgraded version. The accelerator, plugged into a commodity PC, increased the training speed on MSN search engine data up to 1800x compared to the original software implementation on a server. The proposed accelerator has been successfully used by researchers in the search relevance ranking.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W1964796092",
    "type": "article"
  },
  {
    "title": "Synthesis and Optimization of 2D Filter Designs for Heterogeneous FPGAs",
    "doi": "https://doi.org/10.1145/1462586.1462593",
    "publication_date": "2009-01-01",
    "publication_year": 2009,
    "authors": "Christos-Savvas Bouganis; Sung-Boem Park; George A. Constantinides; Peter Y. K. Cheung",
    "corresponding_authors": "",
    "abstract": "Many image processing applications require fast convolution of an image with one or more 2D filters. Field-Programmable Gate Arrays (FPGAs) are often used to achieve this goal due to their fine grain parallelism and reconfigurability. However, the heterogeneous nature of modern reconfigurable devices is not usually considered during design optimization. This article proposes an algorithm that explores the space of possible implementation architectures of 2D filters, targeting the minimization of the required area, by optimizing the usage of the different components in a heterogeneous device. This is achieved by exploring the heterogeneous nature of modern reconfigurable devices using a Singular Value Decomposition based algorithm, which provides an efficient mapping of filter's implementation requirements to the heterogeneous components of modern FPGAs. In the case of multiple 2D filters, the proposed algorithm also exploits any redundancy that exists within each filter and between different filters in the set, leading to designs with minimized area. Experiments with real filter sets from computer vision applications demonstrate an average of up to 38% reduction in the required area.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W1985340652",
    "type": "article"
  },
  {
    "title": "Exploring Reconfigurable Architectures for Tree-Based Option Pricing Models",
    "doi": "https://doi.org/10.1145/1575779.1575781",
    "publication_date": "2009-09-01",
    "publication_year": 2009,
    "authors": "Qiwei Jin; David B. Thomas; Wayne Luk; Benjamin Cope",
    "corresponding_authors": "",
    "abstract": "This article explores the application of reconfigurable hardware to the acceleration of financial computation using tree-based pricing models. Two parallel pipelined architectures have been developed for option valuation using binomial trees and trinomial trees, with support for concurrent evaluation of independent options to achieve high pricing throughput. Our results show that the tree-based models executing on a Virtex 4 field programmable gate array (FPGA) at 82.7 MHz with fixed-point arithmetic can run over 160 times faster than a Core2 Duo processor at 2.2 GHz. The FPGA implementation is two times faster than the nVidia Geforce 7900GTX processor with 24 pipelines at 650 MHz, and 27%--35% slower than the nVidia Geforce 8600GTS processor with 32 Pipelines at 1450 MHz. Our preliminary experiments also indicate that while an FPGA implementation can be slower than a GPU, it could be more efficient when power consumption is taken into account.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2094316810",
    "type": "article"
  },
  {
    "title": "Trust-Based Design and Check of FPGA Circuits Using Two-Level Randomized ECC Structures",
    "doi": "https://doi.org/10.1145/1502781.1508209",
    "publication_date": "2009-03-01",
    "publication_year": 2009,
    "authors": "Shantanu Dutt; Li Li",
    "corresponding_authors": "",
    "abstract": "A novel trust-based design method for FPGA circuits that uses error-correcting code (ECC) structures for detecting design tampers (changes, deletion of existing logic, and addition of extradesign logic-like Trojans) is proposed in this article. We determine ECC-based CLB (configuration logic block) parity groups and embed the check CLBs for each parity group in the FPGA circuit. During a trust-checking phase, a Test-Pattern Generator (TPG) and an Output Response Analyzer (ORA), configured in the FPGA, are used to check that each parity group of CLB outputs produce the expected parities. We use two levels of randomization to thwart attempts by an adversary to discover the parity groups and inject tampers that mask each other, or to tamper with the TPG and ORA so that design tampers remain undetected: (a) randomization of the mapping of the ECC parity groups to the CLB array; (b) randomization within each parity group of odd and even parities for different input combinations (classically, all ECC parity groups have even parities across all inputs). These randomizations along with the error-detecting property of the underlying ECC lead to design tampers being uncovered with very high probabilities, as we show both analytically and empirically. We also classify different CLB function structures and impose a parity group selection in which only similarly structured functions are randomly selected to be in the same parity group in order to minimize check function complexity. Using the 2D code as our underlying ECC and its 2-level randomization, our experiments with inserting 1-10 circuit CLB tampers and 1-5 extraneous logic CLBs in two medium-size circuits and a RISC processor circuit implemented on a Xilinx Spartan-3 FPGA show promising results of 100% tamper detection and 0% false alarms, obtained at a hardware overhead of only 7-10%.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2142911447",
    "type": "article"
  },
  {
    "title": "Security Primitives for Reconfigurable Hardware-Based Systems",
    "doi": "https://doi.org/10.1145/1754386.1754391",
    "publication_date": "2010-05-01",
    "publication_year": 2010,
    "authors": "Ted Huffmire; Timothy E. Levin; Thuy D. Nguyen; Cynthia Irvine; Brett Brotherton; Gang Wang; Timothy Sherwood; Ryan Kastner",
    "corresponding_authors": "",
    "abstract": "Computing systems designed using reconfigurable hardware are increasingly composed using a number of different Intellectual Property (IP) cores, which are often provided by third-party vendors that may have different levels of trust. Unlike traditional software where hardware resources are mediated using an operating system, IP cores have fine-grain control over the underlying reconfigurable hardware. To address this problem, the embedded systems community requires novel security primitives that address the realities of modern reconfigurable hardware. In this work, we propose security primitives using ideas centered around the notion of “moats and drawbridges.” The primitives encompass four design properties: logical isolation, interconnect traceability, secure reconfigurable broadcast, and configuration scrubbing. Each of these is a fundamental operation with easily understood formal properties, yet they map cleanly and efficiently to a wide variety of reconfigurable devices. We carefully quantify the required overheads of the security techniques on modern FPGA architectures across a number of different applications.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2002826523",
    "type": "article"
  },
  {
    "title": "Performance Analysis Framework for High-Level Language Applications in Reconfigurable Computing",
    "doi": "https://doi.org/10.1145/1661438.1661443",
    "publication_date": "2010-01-01",
    "publication_year": 2010,
    "authors": "John Curreri; Seth Koehler; Alan D. George; Brian Holland; Rafael García",
    "corresponding_authors": "",
    "abstract": "High-Level Languages (HLLs) for Field-Programmable Gate Arrays (FPGAs) facilitate the use of reconfigurable computing resources for application developers by using familiar, higher-level syntax, semantics, and abstractions, typically enabling faster development times than with traditional Hardware Description Languages (HDLs). However, programming at a higher level of abstraction is typically accompanied by some loss of performance as well as reduced transparency of application behavior, making it difficult to understand and improve application performance. While runtime tools for performance analysis are often featured in development with traditional HLLs for sequential and parallel programming, HLL-based development for FPGAs has an equal or greater need yet lacks these tools. This article presents a novel and portable framework for runtime performance analysis of HLL applications for FPGAs, including an automated tool for performance analysis of designs created with Impulse C, a commercial HLL for FPGAs. As a case study, this tool is used to successfully locate performance bottlenecks in a molecular dynamics kernel in order to gain speedup.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2054414374",
    "type": "article"
  },
  {
    "title": "Scheduling and Placement of Hardware/Software Real-Time Relocatable Tasks in Dynamically Partially Reconfigurable Systems",
    "doi": "https://doi.org/10.1145/1857927.1857936",
    "publication_date": "2010-12-01",
    "publication_year": 2010,
    "authors": "Pao‐Ann Hsiung; Chun-Hsian Huang; Jih-Sheng Shen; Chen-Chi Chiang",
    "corresponding_authors": "",
    "abstract": "With the gradually fading distinction between hardware and software, it is now possible to relocate tasks from a microprocessor to reconfigurable logic and vice versa. However, existing hardware-software scheduling can rarely cope with such runtime task relocation. In this work, we propose a new Relocatable Hardware-Software Scheduling (RHSS) method that not only can be applied to dynamically relocatable hardware-software tasks, but also increases the reconfigurable hardware resource utilization, reduces the reconfigurable hardware resource fragmentation with realistic placement methods, and makes best efforts at meeting the real-time constraints of tasks. The feasibility of the proposed relocatable hardware-software scheduling algorithm was proved by applying it to some randomly generated examples and a real dynamically reconfigurable network security system example. Compared to the quadratic time complexity of the state-of-the-art Adaptive Hardware-Software Allocation (AHSA) method, RHSS is linear in time complexity, and improves the reconfigurable hardware utilization by as much as 117.8%. The scheduling and placement time and the memory usage are also drastically reduced by as much as 89.5% and 96.4%, respectively.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2084112981",
    "type": "article"
  },
  {
    "title": "On the Evolution of Hardware Circuits via Reconfigurable Architectures",
    "doi": "https://doi.org/10.1145/2392616.2392620",
    "publication_date": "2012-12-01",
    "publication_year": 2012,
    "authors": "Fabio Cancare; Davide B. Bartolini; Matteo Carminati; Donatella Sciuto; Marco D. Santambrogio",
    "corresponding_authors": "",
    "abstract": "Traditionally, hardware circuits are realized according to techniques that follow the classical phases of design and testing. A completely new approach in the creation of hardware circuits has been proposed---the Evolvable Hardware (EHW) paradigm, which bases the circuit synthesis on a goal-oriented evolutionary process inspired by biological evolution in Nature. FPGA-based approaches have emerged as the main architectural solution to implement EHW systems. Various EHW systems have been proposed by researchers but most of them, being based on outdated chips, do not take advantage of the interesting features introduced in newer FPGAs. This article describes a project named Hardware Evolution over Reconfigurable Architectures (HERA), which aims at creating a complete and performance-oriented framework for the evolution of digital circuits, leveraging the reconfiguration technology available in FPGAs. The project is described from its birth to its current state, presenting its evolutionary technique tailored for FPGA-based circuits and the most recent enhancements to improve the scalability with respect to problem size. The developed EHW system outperforms the state of the art, proving its effectiveness in evolving both standard benchmarks and more complex real-world applications.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2010132259",
    "type": "article"
  },
  {
    "title": "Secure, Remote, Dynamic Reconfiguration of FPGAs",
    "doi": "https://doi.org/10.1145/2629423",
    "publication_date": "2014-12-15",
    "publication_year": 2014,
    "authors": "Jo Vliegen; Nele Mentens; Ingrid Verbauwhede",
    "corresponding_authors": "",
    "abstract": "With the widespread availability of broadband Internet, Field-Programmable Gate Arrays (FPGAs) can get remote updates in the field. This provides hardware and software updates, and enables issue solving and upgrade ability without device modification. In order to prevent an attacker from eavesdropping or manipulating the configuration data, security is a necessity. This work describes an architecture that allows the secure, remote reconfiguration of an FPGA. The architecture is partially dynamically reconfigurable and it consists of a static partition that handles the secure communication protocol and a single reconfigurable partition that holds the main application. Our solution distinguishes itself from existing work in two ways: it provides entity authentication and it avoids the use of a trusted third party. The former provides protection against active attackers on the communication channel, while the latter reduces the number of reliable entities. Additionally, this work provides basic countermeasures against simple power-oriented side-channel analysis attacks. The result is an implementation that is optimized toward minimal resource occupation. Because configuration updates occur infrequently, configuration speed is of minor importance with respect to area. A prototype of the proposed design is implemented, using 5,702 slices and having minimal downtime.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2162634789",
    "type": "article"
  },
  {
    "title": "Solving the Global Atmospheric Equations through Heterogeneous Reconfigurable Platforms",
    "doi": "https://doi.org/10.1145/2629581",
    "publication_date": "2015-03-25",
    "publication_year": 2015,
    "authors": "Lin Gan; Haohuan Fu; Wayne Luk; Chao Yang; Wei Xue; Xiaomeng Huang; Youhui Zhang; Guangwen Yang",
    "corresponding_authors": "",
    "abstract": "One of the most essential and challenging components in climate modeling is the atmospheric model. To solve multiphysical atmospheric equations, developers have to face extremely complex stencil kernels that are costly in terms of both computing and memory resources. This article aims to accelerate the solution of global shallow water equations (SWEs), which is one of the most essential equation sets describing atmospheric dynamics. We first design a hybrid methodology that employs both the host CPU cores and the field-programmable gate array (FPGA) accelerators to work in parallel. Through a careful adjustment of the computational domains, we achieve a balanced resource utilization and a further improvement of the overall performance. By decomposing the resource-demanding SWE kernel, we manage to map the double-precision algorithm into three FPGAs. Moreover, by using fixed-point and reduced-precision floating point arithmetic, we manage to build a fully pipelined mixed-precision design on a single FPGA, which can perform 428 floating-point and 235 fixed-point operations per cycle. The mixed-precision design with four FPGAs running together can achieve a speedup of 20 over a fully optimized design on a CPU rack with two eight-core processorsand is 8 times faster than the fully optimized Kepler GPU design. As for power efficiency, the mixed-precision design with four FPGAs is 10 times more power efficient than a Tianhe-1A supercomputer node.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2076377833",
    "type": "article"
  },
  {
    "title": "Distributed Inference over Decision Tree Ensembles on Clusters of FPGAs",
    "doi": "https://doi.org/10.1145/3340263",
    "publication_date": "2019-09-09",
    "publication_year": 2019,
    "authors": "Muhsen Owaida; Amit Kulkarni; Gustavo Alonso",
    "corresponding_authors": "",
    "abstract": "Given the growth in data inputs and application complexity, it is often the case that a single hardware accelerator is not enough to solve a given problem. In particular, the computational demands and I/O of many tasks in machine learning often require a cluster of accelerators to make a relevant difference in performance. In this article, we explore the efficient construction of FPGA clusters using inference over Decision Tree Ensembles as the target application. The article explores several levels of the problem: (1) a lightweight inter-FPGA communication protocol and routing layer to facilitate the communication between the different FPGAs, (2) the data partitioning and distribution strategies maximizing performance, (3) and an in depth analysis on how applications can be efficiently distributed over such a cluster. The experimental analysis shows that the resulting system can support inference over decision tree ensembles at a significantly higher throughput than that achieved by existing systems.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2972661888",
    "type": "article"
  },
  {
    "title": "Low-Overhead FPGA Middleware for Application Portability and Productivity",
    "doi": "https://doi.org/10.1145/2746404",
    "publication_date": "2015-09-11",
    "publication_year": 2015,
    "authors": "Robert Kirchgessner; Alan D. George; Greg Stitt",
    "corresponding_authors": "",
    "abstract": "Reconfigurable computing devices such as field-programmable gate arrays (FPGAs) offer advantages over fixed-logic CPU and GPU architectures, including improved performance, superior power efficiency, and reconfigurability. The challenge of FPGA application development, however, has limited their acceptance in high-performance computing and high-performance embedded computing applications. FPGA development carries similar difficulties to hardware design, requiring that developers iterate through register-transfer level designs with cycle-level accuracy. Furthermore, the lack of hardware and software standards between FPGA platforms limits productivity and application portability, and makes porting applications between heterogeneous platforms a time-consuming and often challenging process. Recent efforts to improve FPGA productivity using high-level synthesis tools and languages show promise, but platform support remains limited and typically is left as a challenge for developers. To address these issues, we present RC Middleware (RCMW), a novel middleware that improves productivity and enables application and tool portability by abstracting away platform-specific details. RCMW provides an application-centric development environment, exposing only the resources and standardized interfaces required by an application, independent of the underlying platform. We demonstrate the portability and productivity benefits of RCMW using four heterogeneous platforms from three vendors. Our results indicate that RCMW enables application productivity and improves developer productivity, and that these benefits are achieved with less than 7% performance and 3% area overhead on average.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W1981869558",
    "type": "article"
  },
  {
    "title": "Enhancing the Scalability of Multi-FPGA Stencil Computations via Highly Optimized HDL Components",
    "doi": "https://doi.org/10.1145/3461478",
    "publication_date": "2021-08-12",
    "publication_year": 2021,
    "authors": "Enrico Reggiani; Emanuele Del Sozzo; Davide Conficconi; Giuseppe Natale; Carlo Moroni; Marco D. Santambrogio",
    "corresponding_authors": "",
    "abstract": "Stencil-based algorithms are a relevant class of computational kernels in high-performance systems, as they appear in a plethora of fields, from image processing to seismic simulations, from numerical methods to physical modeling. Among the various incarnations of stencil-based computations, Iterative Stencil Loops (ISLs) and Convolutional Neural Networks (CNNs) represent two well-known examples of kernels belonging to the stencil class. Indeed, ISLs apply the same stencil several times until convergence, while CNN layers leverage stencils to extract features from an image. The computationally intensive essence of ISLs, CNNs, and in general stencil-based workloads, requires solutions able to produce efficient implementations in terms of throughput and power efficiency. In this context, FPGAs are ideal candidates for such workloads, as they allow design architectures tailored to the stencil regular computational pattern. Moreover, the ever-growing need for performance enhancement leads FPGA-based architectures to scale to multiple devices to benefit from a distributed acceleration. For this reason, we propose a library of HDL components to effectively compute ISLs and CNNs inference on FPGA, along with a scalable multi-FPGA architecture, based on custom PCB interconnects. Our solution eases the design flow and guarantees both scalability and performance competitive with state-of-the-art works.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W3195130375",
    "type": "article"
  },
  {
    "title": "Jitter-based Adaptive True Random Number Generation Circuits for FPGAs in the Cloud",
    "doi": "https://doi.org/10.1145/3487554",
    "publication_date": "2022-09-05",
    "publication_year": 2022,
    "authors": "Xiang Li; Peter Stanwicks; George Provelengios; Russell Tessier; Daniel Holcomb",
    "corresponding_authors": "",
    "abstract": "In this article, we present and evaluate a true random number generator (TRNG) design that is compatible with the restrictions imposed by cloud-based Field Programmable Gate Array (FPGA) providers such as Amazon Web Services (AWS) EC2 F1. Because cloud FPGA providers disallow the ring oscillator circuits that conventionally generate TRNG entropy, our design is oscillator-free and uses clock jitter as its entropy source. The clock jitter is harvested with a time-to-digital converter (TDC) and a controllable delay line that is continuously tuned to compensate for process, voltage, and temperature variations. After describing the design, we present and validate a stochastic model that conservatively quantifies its worst-case entropy. We deploy and model the design in the cloud on 60 EC2 F1 FPGA instances to ensure sufficient randomness is captured. TRNG entropy is further validated using NIST test suites, and experiments are performed to understand how the TRNG responds to on-die power attacks that disturb the FPGA supply voltage in the vicinity of the TRNG. After introducing and validating our basic TRNG design, we introduce and validate a new variant that uses four instances of a linkable sampling module to increase the entropy per sample and improve throughput. The new variant improves throughput by 250% at a modest 17% increase in CLB count.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4294599620",
    "type": "article"
  },
  {
    "title": "Multi-FPGA Designs and Scaling of HPC Challenge Benchmarks via MPI and Circuit-switched Inter-FPGA Networks",
    "doi": "https://doi.org/10.1145/3576200",
    "publication_date": "2023-01-09",
    "publication_year": 2023,
    "authors": "Marius Meyer; Tobias Kenter; Christian Plessl",
    "corresponding_authors": "",
    "abstract": "While FPGA accelerator boards and their respective high-level design tools are maturing, there is still a lack of multi-FPGA applications, libraries, and not least, benchmarks and reference implementations towards sustained HPC usage of these devices. As in the early days of GPUs in HPC, for workloads that can reasonably be decoupled into loosely coupled working sets, multi-accelerator support can be achieved by using standard communication interfaces like MPI on the host side. However, for performance and productivity, some applications can profit from a tighter coupling of the accelerators. FPGAs offer unique opportunities here when extending the dataflow characteristics to their communication interfaces. In this work, we extend the HPCC FPGA benchmark suite by multi-FPGA support and three missing benchmarks that particularly characterize or stress inter-device communication: b_eff, PTRANS, and LINPACK. With all benchmarks implemented for current boards with Intel and Xilinx FPGAs, we established a baseline for multi-FPGA performance. Additionally, for the communication-centric benchmarks, we explored the potential of direct FPGA-to-FPGA communication with a circuit-switched inter-FPGA network that is currently only available for one of the boards. The evaluation with parallel execution on up to 26 FPGA boards makes use of one of the largest academic FPGA installations.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4313894250",
    "type": "article"
  },
  {
    "title": "An Empirical Approach to Enhance Performance for Scalable CORDIC-Based Deep Neural Networks",
    "doi": "https://doi.org/10.1145/3596220",
    "publication_date": "2023-05-08",
    "publication_year": 2023,
    "authors": "Gopal Raut; Saurabh Karkun; Santosh Kumar Vishvakarma",
    "corresponding_authors": "",
    "abstract": "Practical implementation of deep neural networks (DNNs) demands significant hardware resources, necessitating high computational power and memory bandwidth. While existing field-programmable gate array (FPGA)–based DNN accelerators are primarily optimized for fast single-task performance, cost, energy efficiency, and overall throughput are crucial considerations for their practical use in various applications. This article proposes a performance-centric pipeline Coordinate Rotation Digital Computer (CORDIC)–based MAC unit and implements a scalable CORDIC-based DNN architecture that is area- and power-efficient and has high throughput. The CORDIC-based neuron engine uses bit-rounding to maintain input-output precision and minimal hardware resource overhead. The results demonstrate the versatility of the proposed pipelined MAC, which operates at 460 MHz and allows for higher network throughput. A software-based implementation platform evaluates the proposed MAC operation’s accuracy for more extensive neural networks and complex datasets. The DNN accelerator with parameterized and modular layer-multiplexed architecture is designed. Empirical evaluation through Pareto analysis is used to improve the efficiency of DNN implementations by fixing the arithmetic precision and optimal pipeline stages. The proposed architecture utilizes layer-multiplexing, a technique that effectively reuses a single DNN layer to enhance efficiency while maintaining modularity and adaptability for integrating various network configurations. The proposed CORDIC MAC-based DNN architecture is scalable for any bit-precision network size, and the DNN accelerator is prototyped using the Xilinx Virtex-7 VC707 FPGA board, operating at 66 MHz. The proposed design does not use any Xilinx macros, making it easily adaptable for ASIC implementation. Compared with state-of-the-art designs, the proposed design reduces resource use by 45% and power consumption by 4× without sacrificing performance. The accelerator is validated using the MNIST dataset, achieving 95.06% accuracy, only 0.35% less than other cutting-edge implementations.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4375861680",
    "type": "article"
  },
  {
    "title": "CHIP-KNNv2: A<u>C</u>onfigurable and<u>Hi</u>gh-<u>P</u>erformance<u>K</u>-<u>N</u>earest<u>N</u>eighbors Accelerator on HBM-based FPGAs",
    "doi": "https://doi.org/10.1145/3616873",
    "publication_date": "2023-09-13",
    "publication_year": 2023,
    "authors": "Kenneth Liu; Alec Lu; Kartik Samtani; Zhenman Fang; Licheng Guo",
    "corresponding_authors": "",
    "abstract": "The k-nearest neighbors (KNN) algorithm is an essential algorithm in many applications, such as similarity search, image classification, and database query. With the rapid growth in the dataset size and the feature dimension of each data point, processing KNN becomes more compute and memory hungry. Most prior studies focus on accelerating the computation of KNN using the abundant parallel resource on FPGAs. However, they often overlook the memory access optimizations on FPGA platforms and only achieve a marginal speedup over a multi-thread CPU implementation for large datasets. In this article, we design and implement CHIP-KNN: an HLS-based, configurable, and high-performance KNN accelerator. CHIP-KNN optimizes the off-chip memory access on modern HBM-based FPGAs such as the AMD/Xilinx Alveo U280 FPGA board. CHIP-KNN is configurable for all essential parameters used in the algorithm, including the size of the search dataset, the feature dimension and data type representation of each data point, the distance metric, and the number of nearest neighbors - K. In terms of design architecture, we explore and discuss the tradeoffs between two design versions: CHIP-KNNv1 (Ping-Pong buffer based) and CHIP-KNNv2 (streaming-based). Moreover, we investigate the routing congestion issue in our accelerator design, implement hierarchical structures to shorten critical paths, and integrate an open-source floorplanning optimization tool called TAPA/AutoBridge to eliminate the place-and-route issues. To explore the design space and balance the computation and memory access performance, we also build an analytical performance model. Given a user configuration of the KNN parameters, our tool can automatically generate TAPA HLS C code for the optimal accelerator design and the corresponding host code, on the HBM-based FPGA platform. Our experimental results on the Alveo U280 show that, compared to a 48-thread CPU implementation, CHIP-KNNv2 achieves a geomean performance speedup of 15×, with a maximum speedup of 45×. Additionally, we show that CHIP-KNNv2 achieves up to 2.1× performance speedup over CHIP-KNNv1 while increasing configurability. Compared with the state-of-the-art Facebook AI Similarity Search (FAISS) [ 23 ] GPU implementation running on a Nvidia Tesla V100 GPU, CHIP-KNNv2 achieves an average latency reduction of 30.6× while requiring 34.3% of GPU power consumption.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4386710522",
    "type": "article"
  },
  {
    "title": "R-Blocks: an Energy-Efficient, Flexible, and Programmable CGRA",
    "doi": "https://doi.org/10.1145/3656642",
    "publication_date": "2024-04-08",
    "publication_year": 2024,
    "authors": "Barry de Bruin; Kanishkan Vadivel; Mark Wijtvliet; Pekka Jääskeläinen; Henk Corporaal",
    "corresponding_authors": "",
    "abstract": "Emerging data-driven applications in the embedded, e-Health, and internet of things (IoT) domain require complex on-device signal analysis and data reduction to maximize energy efficiency on these energy-constrained devices. Coarse-grained reconfigurable architectures (CGRAs) have been proposed as a good compromise between flexibility and energy efficiency for ultra-low power (ULP) signal processing. Existing CGRAs are often specialized and domain-specific or can only accelerate simple kernels, which makes accelerating complete applications on a CGRA while maintaining high energy efficiency an open issue. Moreover, the lack of instruction set architecture (ISA) standardization across CGRAs makes code generation using current compiler technology a major challenge. This work introduces R-Blocks; a ULP CGRA with HW/SW co-design tool-flow based on the OpenASIP toolset. This CGRA is extremely flexible due to its well-established VLIW-SIMD execution model and support for flexible SIMD-processing, while maintaining an extremely high energy efficiency using software bypassing, optimized instruction delivery, and local scratchpad memories. R-Blocks is synthesized in a commercial 22-nm FD-SOI technology and achieves a full-system energy efficiency of 115 MOPS/mW on a common FFT benchmark, 1.45× higher than a highly tuned embedded RISC-V processor. Comparable energy efficiency is obtained on multiple complex workloads, making R-Blocks a promising acceleration target for general-purpose computing.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4394569784",
    "type": "article"
  },
  {
    "title": "PQA: Exploring the Potential of Product Quantization in DNN Hardware Acceleration",
    "doi": "https://doi.org/10.1145/3656643",
    "publication_date": "2024-04-18",
    "publication_year": 2024,
    "authors": "Ahmed F. AbouElhamayed; Angela Cui; Javier Fernández-Marqués; Nicholas D. Lane; Mohamed S. Abdelfattah",
    "corresponding_authors": "",
    "abstract": "Conventional multiply-accumulate (MAC) operations have long dominated computation time for deep neural networks (DNNs), especially convolutional neural networks (CNNs). Recently, product quantization (PQ) has been applied to these workloads, replacing MACs with memory lookups to pre-computed dot products. To better understand the efficiency tradeoffs of product-quantized DNNs (PQ-DNNs), we create a custom hardware accelerator to parallelize and accelerate nearest-neighbor search and dot-product lookups. Additionally, we perform an empirical study to investigate the efficiency–accuracy tradeoffs of different PQ parameterizations and training methods. We identify PQ configurations that improve performance-per-area for ResNet20 by up to 3.1 ×, even when compared to a highly optimized conventional DNN accelerator, with similar improvements on two additional compact DNNs. When comparing to recent PQ solutions, we outperform prior work by 4 × in terms of performance-per-area with a 0.6% accuracy degradation. Finally, we reduce the bitwidth of PQ operations to investigate the impact on both hardware efficiency and accuracy. With only 2–6-bit precision on three compact DNNs, we were able to maintain DNN accuracy eliminating the need for DSPs.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4394931310",
    "type": "article"
  },
  {
    "title": "End-to-end codesign of Hessian-aware quantized neural networks for FPGAs",
    "doi": "https://doi.org/10.1145/3662000",
    "publication_date": "2024-05-11",
    "publication_year": 2024,
    "authors": "Javier Campos; Jovan Mitrevski; Nhan Viet Tran; Zhen Dong; Amir Gholaminejad; Michael W. Mahoney; J. Duarte",
    "corresponding_authors": "",
    "abstract": "We develop an end-to-end workflow for the training and implementation of co-designed neural networks (NNs) for efficient field-programmable gate array (FPGA) hardware. Our approach leverages Hessian-aware quantization of NNs, the Quantized Open Neural Network Exchange intermediate representation, and the hls4ml tool flow for transpiling NNs into FPGA firmware. This makes efficient NN implementations in hardware accessible to nonexperts in a single open sourced workflow that can be deployed for real-time machine-learning applications in a wide range of scientific and industrial settings. We demonstrate the workflow in a particle physics application involving trigger decisions that must operate at the 40-MHz collision rate of the CERN Large Hadron Collider (LHC). Given the high collision rate, all data processing must be implemented on FPGA hardware within the strict area and latency requirements. Based on these constraints, we implement an optimized mixed-precision NN classifier for high-momentum particle jets in simulated LHC proton-proton collisions.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4396834201",
    "type": "article"
  },
  {
    "title": "SQL2FPGA: Automated Acceleration of SQL Query Processing on Modern CPU-FPGA Platforms",
    "doi": "https://doi.org/10.1145/3674843",
    "publication_date": "2024-07-02",
    "publication_year": 2024,
    "authors": "Alec Lu; Jahanvi Narendra Agrawal; Zhenman Fang",
    "corresponding_authors": "",
    "abstract": "Today’s big data query engines are constantly under pressure to keep up with the rapidly increasing demand for faster processing of more complex workloads. In the past few years, FPGA-based database acceleration efforts have demonstrated promising performance improvement with good energy efficiency. However, few studies target the programming and design automation support to leverage the FPGA accelerator benefits in query processing. Most of them rely on the SQL query plan generated by CPU query engines and manually map the query plan onto the FPGA accelerators, which is tedious and error-prone. Moreover, such CPU-oriented query plans do not consider the utilization of FPGA accelerators and could lose more optimization opportunities. In this article, we present SQL2FPGA, an FPGA accelerator-aware compiler to automatically map SQL queries onto the heterogeneous CPU-FPGA platforms. Our SQL2FPGA front-end takes an optimized logical plan of an SQL query from a database query engine and transforms it into a unified operator-level intermediate representation. To generate an optimized FPGA-aware physical plan, SQL2FPGA implements a set of compiler optimization passes to (1) improve operator acceleration coverage by the FPGA, (2) eliminate redundant computation during physical execution, and (3) minimize data transfer overhead between operators on the CPU and FPGA. Furthermore, it also leverages machine learning techniques to predict and identify the optimal platform, either CPU or FPGA, for the physical execution of individual query operations. Finally, SQL2FPGA generates the associated query acceleration code for heterogeneous CPU-FPGA system deployment. Compared to the widely used Apache Spark SQL framework running on the CPU, SQL2FPGA—using two AMD/Xilinx HBM-based Alveo U280 FPGA boards and Ver.2020 AMD/Xilinx FPGA overlay designs—achieves an average performance speedup of 10.1x and 13.9x across all 22 TPC-H benchmark queries in a scale factor of 1 GB (SF1) and 30 GB (SF30), respectively. While evaluated on AMD/Xilinx Alveo U50 FPGA boards, SQL2FPGA using Ver. 2022 AMD/Xilinx FPGA overlay designs also achieve an average speedup of 9.6x at SF1 scale factor.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4400227515",
    "type": "article"
  },
  {
    "title": "Special-Purpose Hardware for Solving the Elliptic Curve Discrete Logarithm Problem",
    "doi": "https://doi.org/10.1145/1371579.1371580",
    "publication_date": "2008-06-01",
    "publication_year": 2008,
    "authors": "Tim Güneysu; Christof Paar; Jan Pelzl",
    "corresponding_authors": "",
    "abstract": "The resistance against powerful index-calculus attacks makes Elliptic Curve Cryptosystems (ECC) an interesting alternative to conventional asymmetric cryptosystems, like RSA. Operands in ECC require significantly less bits at the same level of security, resulting in a higher computational efficiency compared to RSA. With growing computational capabilities and continuous technological improvements over the years, however, the question of the security of ECC against attacks based on special-purpose hardware arises. In this context, recently emerged low-cost FPGAs demand for attention in the domain of hardware-based cryptanalysis: the extraordinary efficiency of modern programmable hardware devices allow for a low-budget implementation of hardware-based ECC attacks---without the requirement of the expensive development of ASICs. With focus on the aspect of cost-efficiency, this contribution presents and analyzes an FPGA-based architecture of an attack against ECC over prime fields. A multi-processing hardware architecture for Pollard's Rho method is described. We provide results on actually used key lengths of ECC (128 bits and above) and estimate the expected runtime for a successful attack. As a first result, currently used elliptic curve cryptosystems with a security of 160 bit and above turn out to be infeasible to break with available computational and financial resources. However, some of the security standards proposed by the Standards for Efficient Cryptography Group (SECG) become subject to attacks based on low-cost FPGAs.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W1977131936",
    "type": "article"
  },
  {
    "title": "Statistical Analysis and Process Variation-Aware Routing and Skew Assignment for FPGAs",
    "doi": "https://doi.org/10.1145/1331897.1331900",
    "publication_date": "2008-03-01",
    "publication_year": 2008,
    "authors": "Satish Sivaswamy; Kia Bazargan",
    "corresponding_authors": "",
    "abstract": "With constant scaling of process technologies, chip design is becoming increasingly difficult due to process variations. The FPGA community has only recently started focusing on the effects of variations. In this work we present a statistical analysis to compare the effects of variations on designs mapped to FPGAs and ASICs. We also present CAD and architecture techniques to mitigate the impact of variations. First we present a variation-aware router that optimizes statistical criticality. We then propose a modification to the clock network to deliver programmable skews to different flip-flops. Finally, we combine the two techniques and the result is a 9x reduction in yield loss that translates to a 12% improvement in timing yield. When the desired timing yield is set to 99%, our combined statistical routing and skew assignment technique results in a delay improvement of about 10% over a purely deterministic approach.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W1977505713",
    "type": "article"
  },
  {
    "title": "Robust Real-Time Super-Resolution on FPGA and an Application to Video Enhancement",
    "doi": "https://doi.org/10.1145/1575779.1575782",
    "publication_date": "2009-09-01",
    "publication_year": 2009,
    "authors": "Maria E. Angelopoulou; Christos-Savvas Bouganis; Peter Y. K. Cheung; George A. Constantinides",
    "corresponding_authors": "",
    "abstract": "The high density image sensors of state-of-the-art imaging systems provide outputs with high spatial resolution, but require long exposure times. This limits their applicability, due to the motion blur effect. Recent technological advances have lead to adaptive image sensors that can combine several pixels together in real time to form a larger pixel. Larger pixels require shorter exposure times and produce high-frame-rate samples with reduced motion blur. This work proposes combining an FPGA with an adaptive image sensor to produce an output of high resolution both in space and time. The FPGA is responsible for the spatial resolution enhancement of the high-frame-rate samples using super-resolution (SR) techniques in real time. To achieve it, this article proposes utilizing the Iterative Back Projection (IBP) SR algorithm. The original IBP method is modified to account for the presence of noise, leading to an algorithm more robust to noise. An FPGA implementation of this algorithm is presented. The proposed architecture can serve as a general purpose real-time resolution enhancement system, and its performance is evaluated under various noise levels.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2107638114",
    "type": "article"
  },
  {
    "title": "Evaluation of Random Delay Insertion against DPA on FPGAs",
    "doi": "https://doi.org/10.1145/1857927.1857938",
    "publication_date": "2010-12-01",
    "publication_year": 2010,
    "authors": "Yingxi Lu; Máire O’Neill; J.V. McCanny",
    "corresponding_authors": "",
    "abstract": "Side-channel attacks (SCA) threaten electronic cryptographic devices and can be carried out by monitoring the physical characteristics of security circuits. Differential Power Analysis (DPA) is one the most widely studied side-channel attacks. Numerous countermeasure techniques, such as Random Delay Insertion (RDI), have been proposed to reduce the risk of DPA attacks against cryptographic devices. The RDI technique was first proposed for microprocessors but it was shown to be unsuccessful when implemented on smartcards as it was vulnerable to a variant of the DPA attack known as the Sliding-Window DPA attack. Previous research by the authors investigated the use of the RDI countermeasure for Field Programmable Gate Array (FPGA) based cryptographic devices. A split-RDI technique was proposed to improve the security of the RDI countermeasure. A set of critical parameters was also proposed that could be utilized in the design stage to optimize a security algorithm design with RDI in terms of area, speed and power. The authors also showed that RDI is an efficient countermeasure technique on FPGA in comparison to other countermeasures. In this article, a new RDI logic design is proposed that can be used to cost-efficiently implement RDI on FPGA devices. Sliding-Window DPA and realignment attacks, which were shown to be effective against RDI implemented on smartcard devices, are performed on the improved RDI FPGA implementation. We demonstrate that these attacks are unsuccessful and we also propose a realignment technique that can be used to demonstrate the weakness of RDI implementations.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W1965436647",
    "type": "article"
  },
  {
    "title": "Exploiting Self-Reconfiguration Capability to Improve SRAM-based FPGA Robustness in Space and Avionics Applications",
    "doi": "https://doi.org/10.1145/1857927.1857935",
    "publication_date": "2010-12-01",
    "publication_year": 2010,
    "authors": "Marco Lanuzza; Paolo Zicari; Fabio Frustaci; Stefania Perri; Pasquale Corsonello",
    "corresponding_authors": "",
    "abstract": "This article presents a novel configuration scrubbing core, used for internal detection and correction of radiation-induced configuration single and multiple bit errors, without requiring external scrubbing. The proposed technique combines the benefits of fast radiation-induced fault detection with fast restoration of the device functionality and small area and power overheads. Experimental results demonstrate that the novel approach significantly improves the availability in hostile radiation environments of FPGA-based designs. When implemented using a Xilinx XC2V1000 Virtex-II device, the presented technique detects and corrects single bit upsets and double, triple and quadruple multi bit upsets, occupying just 1488 slices and dissipating less than 30 mW at a 50MHz running frequency.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2085589133",
    "type": "article"
  },
  {
    "title": "Floating-Point Exponentiation Units for Reconfigurable Computing",
    "doi": "https://doi.org/10.1145/2457443.2457447",
    "publication_date": "2013-05-01",
    "publication_year": 2013,
    "authors": "Florent de Dinechin; Pedro Echeverría; Marisa López‐Vallejo; Bogdan Pasca",
    "corresponding_authors": "",
    "abstract": "The high performance and capacity of current FPGAs makes them suitable as acceleration co-processors. This article studies the implementation, for such accelerators, of the floating-point power function x y as defined by the C99 and IEEE 754-2008 standards, generalized here to arbitrary exponent and mantissa sizes. Last-bit accuracy at the smallest possible cost is obtained thanks to a careful study of the various subcomponents: a floating-point logarithm, a modified floating-point exponential, and a truncated floating-point multiplier. A parameterized architecture generator in the open-source FloPoCo project is presented in details and evaluated.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2013659828",
    "type": "article"
  },
  {
    "title": "Secure Extension of FPGA General Purpose Processors for Symmetric Key Cryptography with Partial Reconfiguration Capabilities",
    "doi": "https://doi.org/10.1145/2362374.2362380",
    "publication_date": "2012-10-01",
    "publication_year": 2012,
    "authors": "Lubos Gaspar; Viktor Fischer; Lilian Bossuet; Robert Fouquet",
    "corresponding_authors": "",
    "abstract": "In data security systems, general purpose processors (GPPs) are often extended by a cryptographic accelerator. The article presents three ways of extending GPPs for symmetric key cryptography applications. Proposed extensions guarantee secure key storage and management even if the system is facing protocol, software and cache memory attacks. The system is partitioned into processor, cipher, and key memory zones. The three security zones are separated at protocol, system, architecture and physical levels. The proposed principle was validated on Altera NIOS II, Xilinx MicroBlaze and Microsemi Cortex M1 soft-core processor extensions. We show that stringent separation of the cipher zone is helpful for partial reconfiguration of the security module, if the enciphering algorithm needs to be dynamically changed. However, the key zone including reconfiguration controller must remain static in order to maintain the high level of security required. We demonstrate that the principle is feasible in partially reconfigurable field programmable gate arrays (FPGAs) such as Altera Stratix V or Xilinx Virtex 6 and also to some extent in FPGAs featuring hardwired general purpose processors such as Cortex M3 in Microsemi SmartFusion FPGA. Although the three GPPs feature different data interfaces, we show that the processors with their extensions reach the required high security level while maintaining partial reconfiguration capability.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2034015693",
    "type": "article"
  },
  {
    "title": "Optimizing memory bandwidth use and performance for matrix-vector multiplication in iterative methods",
    "doi": "https://doi.org/10.1145/2000832.2000834",
    "publication_date": "2011-08-01",
    "publication_year": 2011,
    "authors": "David Boland; George A. Constantinides",
    "corresponding_authors": "",
    "abstract": "Computing the solution to a system of linear equations is a fundamental problem in scientific computing, and its acceleration has drawn wide interest in the FPGA community [Morris et al. 2006; Zhang et al. 2008; Zhuo and Prasanna 2006]. One class of algorithms to solve these systems, iterative methods, has drawn particular interest, with recent literature showing large performance improvements over General-Purpose Processors (GPPs) [Lopes and Constantinides 2008]. In several iterative methods, this performance gain is largely a result of parallelization of the matrix-vector multiplication, an operation that occurs in many applications and hence has also been widely studied on FPGAs [Zhuo and Prasanna 2005; El-Kurdi et al. 2006]. However, whilst the performance of matrix-vector multiplication on FPGAs is generally I/O bound [Zhuo and Prasanna 2005], the nature of iterative methods allows the use of on-chip memory buffers to increase the bandwidth, providing the potential for significantly more parallelism [deLorimier and DeHon 2005]. Unfortunately, existing approaches have generally only either been capable of solving large matrices with limited improvement over GPPs [Zhuo and Prasanna 2005; El-Kurdi et al. 2006; deLorimier and DeHon 2005], or achieve high performance for relatively small matrices [Lopes and Constantinides 2008; Boland and Constantinides 2008]. This article proposes hardware designs to take advantage of symmetrical and banded matrix structure, as well as methods to optimize the RAM use, in order to both increase the performance and retain this performance for larger-order matrices.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2091059698",
    "type": "article"
  },
  {
    "title": "KAPow",
    "doi": "https://doi.org/10.1145/3129789",
    "publication_date": "2018-01-09",
    "publication_year": 2018,
    "authors": "James J. Davis; Eddie Hung; Joshua M. Levine; Edward Stott; Peter Y. K. Cheung; George A. Constantinides",
    "corresponding_authors": "",
    "abstract": "In an FPGA system-on-chip design, it is often insufficient to merely assess the power consumption of the entire circuit by compile-time estimation or runtime power measurement. Instead, to make better decisions, one must understand the power consumed by each module in the system. In this work, we combine measurements of register-level switching activity and system-level power to build an adaptive online model that produces live breakdowns of power consumption within the design. Online model refinement avoids time-consuming characterization while also allowing the model to track long-term operating condition changes. Central to our method is an automated flow that selects signals predicted to be indicative of high power consumption, instrumenting them for monitoring. We named this technique KAPow, for ‘K’ounting Activity for Power estimation, which we show to be accurate and to have low overheads across a range of representative benchmarks. We also propose a strategy allowing for the identification and subsequent elimination of counters found to be of low significance at runtime, reducing algorithmic complexity without sacrificing significant accuracy. Finally, we demonstrate an application example in which a module-level power breakdown can be used to determine an efficient mapping of tasks to modules and reduce system-wide power consumption by up to 7%.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2783532808",
    "type": "article"
  },
  {
    "title": "Lightening the Load with Highly Accurate Storage- and Energy-Efficient LightNNs",
    "doi": "https://doi.org/10.1145/3270689",
    "publication_date": "2018-09-30",
    "publication_year": 2018,
    "authors": "Ruizhou Ding; Zeye Liu; R.D. Blanton; Diana Marculescu",
    "corresponding_authors": "",
    "abstract": "Hardware implementations of deep neural networks (DNNs) have been adopted in many systems because of their higher classification speed. However, while they may be characterized by better accuracy, larger DNNs require significant energy and area, thereby limiting their wide adoption. The energy consumption of DNNs is driven by both memory accesses and computation. Binarized neural networks (BNNs), as a tradeoff between accuracy and energy consumption, can achieve great energy reduction and have good accuracy for large DNNs due to their regularization effect. However, BNNs show poor accuracy when a smaller DNN configuration is adopted. In this article, we propose a new DNN architecture, LightNN, which replaces the multiplications to one shift or a constrained number of shifts and adds. Our theoretical analysis for LightNNs shows that their accuracy is maintained while dramatically reducing storage and energy requirements. For a fixed DNN configuration, LightNNs have better accuracy at a slight energy increase than BNNs, yet are more energy efficient with only slightly less accuracy than conventional DNNs. Therefore, LightNNs provide more options for hardware designers to trade off accuracy and energy. Moreover, for large DNN configurations, LightNNs have a regularization effect, making them better in accuracy than conventional DNNs. These conclusions are verified by experiment using the MNIST and CIFAR-10 datasets for different DNN configurations. Our FPGA implementation for conventional DNNs and LightNNs confirms all theoretical and simulation results and shows that LightNNs reduce latency and use fewer FPGA resources compared to conventional DNN architectures.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2905175929",
    "type": "article"
  },
  {
    "title": "Self-Alignment Schemes for the Implementation of Addition-Related Floating-Point Operators",
    "doi": "https://doi.org/10.1145/2457443.2457444",
    "publication_date": "2013-05-01",
    "publication_year": 2013,
    "authors": "Tarek Ould‐Bachir; Jean‐Pierre David",
    "corresponding_authors": "",
    "abstract": "Advances in semiconductor technology brings to the market incredibly dense devices, capable of handling tens to hundreds floating-point operators on a single chip; so do the latest field programmable gate arrays (FPGAs). In order to alleviate the complexity of resorting to these devices in computationally intensive applications, this article proposes hardware schemes for the realization of addition-related floating-point operators based on the self-alignment technique (SAT). The article demonstrates that the schemes guarantee an accuracy as if summation was computed accurately in the precision of operator’s internal mantissa, then faithfully rounded to working precision. To achieve such performance, the article adopts the redundant high radix carry-save (HRCS) format for the rapid addition of wide mantissas. Implementation results show that combining the SAT and the HRCS format allows the implementation of complex operators with reduced area and latency, more so when a fused-path approach is adopted. The article also proposes a new hardware operator for performing endomorphic HRCS additions and presents a new technique for speeding up the conversion from the redundant HRCS to a conventional binary format.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W1967476488",
    "type": "article"
  },
  {
    "title": "Parallelizing Data Processing on FPGAs with Shifter Lists",
    "doi": "https://doi.org/10.1145/2629551",
    "publication_date": "2015-03-31",
    "publication_year": 2015,
    "authors": "Louis Woods; Gustavo Alonso; Jens Teubner",
    "corresponding_authors": "",
    "abstract": "Parallelism is currently seen as a mechanism to minimize the impact of the power and heat dissipation problems encountered in modern hardware. Data parallelism—based on partitioning the data—and pipeline parallelism—based on partitioning the computation—are the two main approaches to leverage parallelism on a wide range of hardware platforms. Unfortunately, not all data processing problems are susceptible to either of those strategies. An example is the skyline operator [Börzsönyi et al. 2001], which computes the set of Pareto-optimal points within a multidimensional dataset. Existing approaches to parallelize the skyline operator are based on data parallelism. As a result, they suffer from a high overhead when merging intermediate results because of the lack of a global view of the problem inherent to partitioning the input data. In this article, we show how to combine pipeline with data parallelism on a Field-Programmable Gate Array (FPGA) for a more efficient utilization of the available hardware parallelism. As we show in our experiments, skyline computation using our proposed technique scales linearly with the number of processing elements, and the performance we achieve on a rather small FPGA is comparable to that of a 64-core high-end server running a state-of-the-art data parallel implementation of skyline [Park et al. 2009]. The proposed approach to parallelize the skyline operator can be generalized to a wider range of data processing problems. We demonstrate this through a novel, highly parallel data structure, a shifter list , that can be efficiently implemented on an FPGA. The resulting template is easy to parametrize to implement a variety of computationally intensive operators such as frequent items , n -closest pairs , or K-means .",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2052166264",
    "type": "article"
  },
  {
    "title": "Exploiting FPGA Block Memories for Protected Cryptographic Implementations",
    "doi": "https://doi.org/10.1145/2629552",
    "publication_date": "2015-05-11",
    "publication_year": 2015,
    "authors": "Shivam Bhasin; Jean‐Luc Danger; Sylvain Guilley; Wei He",
    "corresponding_authors": "",
    "abstract": "Modern field programmable gate arrays (FPGAs) are power packed with features to facilitate designers. Availability of features like large block memory (BRAM), digital signal processing cores, and embedded CPU makes the design strategy of FPGAs quite different from ASICs. FPGAs are also widely used in security-critical applications where protection against known attacks is of prime importance. We focus on physical attacks that target physical implementations. To design countermeasures against such attacks, the strategy for FPGA designers should be different from that in ASIC. The available features should be exploited to design compact and strong countermeasures. In this article, we propose methods to exploit the BRAMs in FPGAs for designing compact countermeasures. Internal BRAM can be used to optimize intrinsic countermeasures such as masking and dual-rail logics, which otherwise have significant overhead (at least 2 × ) compared to unprotected ones. The optimizations are applied on a real AES-128 co-processor and tested for area overhead and resistance on Xilinx Virtex-5 chips. The presented masking countermeasure has an overhead of only 16% when applied on AES. Moreover, the dual-rail precharge logic (DPL) countermeasure has been optimized to pack the whole sequential part in the BRAM, hence enhancing the security. Proper robustness evaluations are conducted to analyze the optimization in terms of area and security.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2294130056",
    "type": "article"
  },
  {
    "title": "Compact and On-the-Fly Secure Dynamic Reconfiguration for Volatile FPGAs",
    "doi": "https://doi.org/10.1145/2816822",
    "publication_date": "2016-01-12",
    "publication_year": 2016,
    "authors": "Hirak J. Kashyap; Ricardo Chaves",
    "corresponding_authors": "",
    "abstract": "The dynamic partial reconfiguration functionality of FPGAs can be attacked, particularly when the FPGA is remotely located or the configuration bitstreams are sent through insecure networks. The existing FPGA technologies provide some built-in security mechanisms; however, these are often inadequate. The existing solutions still impose a significant impact on the reconfiguration process and on the available resources. This article proposes a solution to improve the security of dynamic partial reconfiguration of FPGAs, without significantly affecting the reconfiguration performance. The proposed solution changes the encryption key of the remotely received bitstream by a randomly generated key, unique for each configuration, when storing them in the external unsecured memory. The native frame-wise error detection mechanism combined with an additional CBC-MAC authentication mechanism, allows for an improved countermeasure against replay attack and wrongful bitstream usage. The proposed solution introduces an overhead of 1% of the available resources on the target FPGA and provides the lowest impact on the reconfiguration process when compared to the state of the art, achieving a reconfiguration throughput of 2.5Gbps. Regarding the built-in security mechanism provided by the Xilinx FPGAs, the solution herein proposed provides better security and improves the reconfiguration performance by more than 3 times.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2296779301",
    "type": "article"
  },
  {
    "title": "Automata Processing in Reconfigurable Architectures",
    "doi": "https://doi.org/10.1145/3314576",
    "publication_date": "2019-05-17",
    "publication_year": 2019,
    "authors": "Chunkun Bo; Vinh Dang; Ted Xie; Jack Wadden; Mircea R. Stan; Kevin Skadron",
    "corresponding_authors": "",
    "abstract": "We present a general automata processing framework on FPGAs, which generates an RTL kernel for automata processing together with an AXI and PCIe based I/O circuitry. We implement the framework on both local nodes and cloud platforms (Amazon AWS and Nimbix) with novel features. A full performance comparison of the proposed framework is conducted against state-of-the-art automata processing engines on CPUs, GPUs, and Micron’s Automata Processor using the ANMLZoo benchmark suite and some real-world datasets. Results show that FPGAs enable extremely high-throughput automata processing compared to von Neumann architectures. We also collect the resource utilization and power consumption on the two cloud platforms, and find that the I/O circuitry consumes most of the hardware resources and power. Furthermore, we propose a fast, symbol-only reconfiguration mechanism based on the framework for large pattern sets that cannot fit on a single device and need to be partitioned. The proposed method supports multiple passes of the input stream and reduces the re-compilation cost from hours to seconds.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2945350303",
    "type": "article"
  },
  {
    "title": "Execution Trace--Driven Energy-Reliability Optimization for Multimedia MPSoCs",
    "doi": "https://doi.org/10.1145/2665071",
    "publication_date": "2015-05-04",
    "publication_year": 2015,
    "authors": "Anup Das; Amit Kumar Singh; Akash Kumar",
    "corresponding_authors": "",
    "abstract": "Multiprocessor systems-on-chip (MPSoCs) are becoming a popular design choice in current and future technology nodes to accommodate the heterogeneous computing demand of a multitude of applications enabled on these platform. Streaming multimedia and other communication-centric applications constitute a significant fraction of the application space of these devices. The mapping of an application on an MPSoC is an NP-hard problem. This has attracted researchers to solve this problem both as stand-alone (best-effort) and in conjunction with other optimization objectives, such as energy and reliability. Most existing studies on energy-reliability joint optimization are static—that is, design time based. These techniques fail to capture runtime variability such as resource unavailability and dynamism associated with application behaviors, which are typical of multimedia applications. The few studies that consider dynamic mapping of applications do not consider throughput degradation, which directly impacts user satisfaction. This article proposes a runtime technique to analyze the execution trace of an application modeled as Synchronous Data Flow Graphs (SDFGs) to determine its mapping on a multiprocessor system with heterogeneous processing units for different fault scenarios. Further, communication energy is minimized for each of these mappings while satisfying the throughput constraint. Experiments conducted with synthetic and real SDFGs demonstrate that the proposed technique achieves significant improvement with respect to the state-of-the-art approaches in terms of throughput and storage overhead with less than 20% energy overhead.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W1848619664",
    "type": "article"
  },
  {
    "title": "Extending UML/MARTE to Support Discrete Controller Synthesis, Application to Reconfigurable Systems-on-Chip Modeling",
    "doi": "https://doi.org/10.1145/2629628",
    "publication_date": "2014-08-01",
    "publication_year": 2014,
    "authors": "Sébastien Guillet; Florent de Lamotte; Nicolas Le Griguer; Éric Rutten; Guy Gogniat; Jean-Philippe Diguet",
    "corresponding_authors": "",
    "abstract": "This article presents the first framework to design and synthesize a formal controller managing dynamic reconfiguration, using a model-driven engineering methodology based on an extension of UML/MARTE. The implementation technique highlights the combination of hard configuration constraints using weights ( control part )—ensured statically and fulfilled by the system at runtime—and soft constraints ( decision part ) that, given a set of correct and accessible configurations, choose one of them. An application model of an image processing application is presented, then transformed and synthesized to be executed on a Xilinx platform to show how the controller, executed on a Microblaze, manages the hardware reconfigurations.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2039712832",
    "type": "article"
  },
  {
    "title": "Multi-Application Network-on-Chip Design using Global Mapping and Local Reconfiguration",
    "doi": "https://doi.org/10.1145/2556944",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "J. Soumya; Ashish Sharma; Santanu Chattopadhyay",
    "corresponding_authors": "",
    "abstract": "This article proposes a reconfigurable Network-on-Chip (NoC) architecture based on mesh topology. It provides a local reconfiguration of cores to connect to any of the neighboring routers, depending upon the currently executing application. The area overhead for this local reconfiguration has been shown to be very small. We have also presented the strategy to map the cores of an application set onto this architecture. This has been achieved via a two-phase procedure. In the first phase, the cores of the combined application set are mapped tentatively to individual routers, minimizing the communication cost. In the second phase, for each application, positions of individual cores are finalized. A core gets attached to any neighbor of its tentative allocation. We have proposed Integer Linear Programming (ILP) formulation of both the phases. Since ILP takes large amount of CPU time, we have also formulated a Particle Swarm Optimization (PSO)-based solution for the two phases. A heuristic approach has also been developed for the reconfiguration. Comparison of communication cost, latency and network energy have been carried out for the applications, before and after reconfiguration. It shows significant improvement in performance via reconfiguration.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2050254448",
    "type": "article"
  },
  {
    "title": "Separation Logic for High-Level Synthesis",
    "doi": "https://doi.org/10.1145/2836169",
    "publication_date": "2015-12-17",
    "publication_year": 2015,
    "authors": "Felix Winterstein; Samuel Bayliss; George A. Constantinides",
    "corresponding_authors": "",
    "abstract": "High-Level Synthesis (HLS) promises a significant shortening of the FPGA design cycle by raising the abstraction level of the design entry to high-level languages such as C/C++. However, applications using dynamic, pointer-based data structures and dynamic memory allocation remain difficult to implement well, yet such constructs are widely used in software. Automated optimizations that leverage the memory bandwidth of FPGAs by distributing the application data over separate banks of on-chip memory are often ineffective in the presence of dynamic data structures due to the lack of an automated analysis of pointer-based memory accesses. In this work, we take a step toward closing this gap. We present a static analysis for pointer-manipulating programs that automatically splits heap-allocated data structures into disjoint, independent regions. The analysis leverages recent advances in separation logic , a theoretical framework for reasoning about heap-allocated data that has been successfully applied in recent software verification tools. Our algorithm focuses on dynamic data structures accessed in loops and is accompanied by automated source-to-source transformations that enable automatic loop parallelization and memory partitioning by off-the-shelf HLS tools. We demonstrate the successful loop parallelization and memory partitioning by our tool flow using three real-life applications that build, traverse, update, and dispose of dynamically allocated data structures. Our case studies, comparing the automatically parallelized to the direct HLS implementations, show an average latency reduction by a factor of 2 × across our benchmarks.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2337362532",
    "type": "article"
  },
  {
    "title": "Deploying Multi-tenant FPGAs within Linux-based Cloud Infrastructure",
    "doi": "https://doi.org/10.1145/3474058",
    "publication_date": "2021-12-01",
    "publication_year": 2021,
    "authors": "Joel Mandebi Mbongue; Danielle Tchuinkou Kwadjo; Alex Shuping; Christophe Bobda",
    "corresponding_authors": "",
    "abstract": "Cloud deployments now increasingly exploit Field-Programmable Gate Array (FPGA) accelerators as part of virtual instances. While cloud FPGAs are still essentially single-tenant, the growing demand for efficient hardware acceleration paves the way to FPGA multi-tenancy. It then becomes necessary to explore architectures, design flows, and resource management features that aim at exposing multi-tenant FPGAs to the cloud users. In this article, we discuss a hardware/software architecture that supports provisioning space-shared FPGAs in Kernel-based Virtual Machine (KVM) clouds. The proposed hardware/software architecture introduces an FPGA organization that improves hardware consolidation and support hardware elasticity with minimal data movement overhead. It also relies on VirtIO to decrease communication latency between hardware and software domains. Prototyping the proposed architecture with a Virtex UltraScale+ FPGA demonstrated near specification maximum frequency for on-chip data movement and high throughput in virtual instance access to hardware accelerators. We demonstrate similar performance compared to single-tenant deployment while increasing FPGA utilization, which is one of the goals of virtualization. Overall, our FPGA design achieved about 2× higher maximum frequency than the state of the art and a bandwidth reaching up to 28 Gbps on 32-bit data width.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W3216247449",
    "type": "article"
  },
  {
    "title": "Leveraging Incremental Machine Learning for Reconfigurable Systems Modeling under Dynamic Workloads",
    "doi": "https://doi.org/10.1145/3715154",
    "publication_date": "2025-01-27",
    "publication_year": 2025,
    "authors": "Juan Encinas; Alfonso Rodríguez; A. Otero",
    "corresponding_authors": "",
    "abstract": "Dynamic workload orchestration is one of the main concerns when working with heterogeneous computing infrastructures in the edge-cloud continuum. In this context, FPGA-based computing nodes can take advantage of their improved flexibility, performance and energy efficiency provided that they use proper resource management strategies. In this regard, many state-of-the-art systems rely on proactive power management techniques and task scheduling decisions, which in turn require deep knowledge about the applications to be accelerated and the actual response of the target reconfigurable fabrics when executing them. While acquiring this knowledge at design time was more or less feasible in the past, with applications mostly being static task graphs that did not change at run time, the highly dynamic nature of current workloads in the edge-cloud continuum, where tasks can be deployed on any node and at any time, has removed this possibility. As a result, being able to derive such information at run time to make informed decisions has become a must. This paper presents an infrastructure to build incremental ML models that can be used to obtain run-time power consumption and performance estimations in FPGA-based reconfigurable multi-accelerator systems operating under dynamic workloads. The proposed infrastructure features a novel stop-and-restart resource-aware mechanism to monitor and control the model training and evaluation stages during normal system operation, enabling low-overhead updates in the models to account for either unexpected acceleration requests (i.e., tasks not considered previously by the models) or model drift (e.g., fabric degradation). Experimental results show that the proposed approach induces a maximal additional error of 3.66% compared to a continuous training alternative. Furthermore, the proposed approach incurs only a 4.49% execution time overhead, compared to the 20.91% overhead induced by the continuous training alternative. The proposed modeling strategy enables innovative scheduling approaches in reconfigurable systems. This is exemplified by the conflict-aware scheduler introduced in this work, which achieves up to a 1.35 times speedup in executing the experimental workload. Additionally, the proposed approach demonstrates superior adaptability compared to other methods in the literature, particularly in response to significant changes in workload and to mitigate the effects of model overfitting. The portability of the proposed modeling methodology and monitoring infrastructure is also shown through their application to both Zynq-7000 and Zynq UltraScale+ devices.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406875108",
    "type": "article"
  },
  {
    "title": "Multi-Tenant Cloud FPGA: A Survey on Security, Trust and Privacy",
    "doi": "https://doi.org/10.1145/3713078",
    "publication_date": "2025-01-27",
    "publication_year": 2025,
    "authors": "Muhammed Kawser Ahmed; Max Panoff; Joel Mandebi Mbongue; Sujan Kumar Saha; Erman Nghonda Tchinda; Peter Mbua; Christophe Bobda",
    "corresponding_authors": "",
    "abstract": "With the growing demand for enhanced performance and scalability in cloud applications and systems, data center architectures are evolving to incorporate heterogeneous computing fabrics that leverage CPUs, GPUs, and FPGAs. Unlike traditional processing platforms like CPUs and GPUs, FPGAs offer the unique ability for hardware reconfiguration at run-time, enabling improved and tailored performance, flexibility, and acceleration. FPGAs excel at executing large-scale search optimization, acceleration, and signal processing tasks while consuming low power and minimizing latency. Major public cloud providers, such as Amazon, Huawei, Microsoft, Alibaba, and others, have already begun integrating FPGA-based cloud acceleration services into their offerings. Although FPGAs in cloud applications facilitate customized hardware acceleration, they also introduce new security challenges that demand attention. Granting cloud users the capability to reconfigure hardware designs after deployment may create potential vulnerabilities for malicious users, thereby jeopardizing entire cloud platforms. In particular, multi-tenant FPGA services, where a single FPGA is divided spatially among multiple users, are highly vulnerable to such attacks. This paper examines the security concerns associated with multi-tenant cloud FPGAs, provides a comprehensive overview of the related security, privacy and trust issues, and discusses forthcoming challenges in this evolving field of study.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406875205",
    "type": "article"
  },
  {
    "title": "CHIRP: <u>C</u> ompact and <u>H</u> igh-Performance FPGA Implementation of Un <u>i</u> fied Hardware Accelerators for <u>R</u> ing-Binary-LWE-based <u>P</u> QC",
    "doi": "https://doi.org/10.1145/3715153",
    "publication_date": "2025-01-28",
    "publication_year": 2025,
    "authors": "Tianyou Bao; Pengzhou He; Daisuke Fujimoto; Yu‐ichi Hayashi; Jiafeng Xie",
    "corresponding_authors": "",
    "abstract": "Post-quantum cryptography (PQC) has drawn significant attention from the hardware design research community, especially on field-programmable gate array (FPGA) platforms. In line with this trend, in this paper, we present a novel FPGA-based PQC design work (CHIRP), i.e., C ompact and high- P erformance FPGA implementation of un I fied accelerators for R ing-Binary-Learning-with-Errors (RBLWE)-based P QC, a promising lightweight PQC suited for related applications like Internet-of-Things. The proposed accelerators offer flexibility across the available two security levels, thus expanding their application potential. In total, we presented four distinct hardware accelerators tailored to different performance and resource requirements, ranging from resource-constrained devices to high-throughput applications. Our innovation encompasses three key efforts: (i) we derived four optimized algorithms for RBLWE-ENC’s unified operation (covering the available two security levels), allowing flexible switching of security sizes while boosting calculations; (ii) we then presented the four novel accelerators (CHIRP) targeting FPGA platforms, featuring dedicated hardware structures; (iii) we finally conducted a comprehensive evaluation to validate the efficiency of the proposed accelerators on various FPGA devices. Compared to the existing unified design, the proposed accelerator demonstrated up to 91.4% reduction in area-delay product (ADP) on the Straix-V device. Even when compared with the state-of-the-art single security designs, the proposed accelerator (best version) obtains much better resource usage and ADP performance while unified operation (flexibly switching between two security levels) is considered on both AMD-Xilinx and Intel devices. We anticipate the findings of this research will foster advancements in FPGA implementation techniques for lightweight PQC development.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4406905715",
    "type": "article"
  },
  {
    "title": "The ExaNeSt Prototype: Evaluation of Efficient HPC Communication Hardware in an ARM-based Multi-FPGA Rack",
    "doi": "https://doi.org/10.1145/3715152",
    "publication_date": "2025-02-04",
    "publication_year": 2025,
    "authors": "Manolis Ploumidis; Fabien Chaix; Nikolaos Chrysos; Marios Assiminakis; Nikolaos D. Kallimanis; Nick Kossifidis; Mickael Nikoloudakis; Nikolaos Dimou; Michalis Gianioudis; Georgios Ieronymakis; Άγγελος Ιωάννου; George Kalokerinos; Pantelis Xirouchakis; Astrinos Damianakis; Mickael Ligerakis; Theocharis Vavouris; Manolis Katevenis; Vassilis Papaefstathiou; Manolis Marazakis; Iakovos Mavroidis",
    "corresponding_authors": "",
    "abstract": "We present and evaluate the ExaNeSt Prototype, which compactly packages 128 Xilinx ZU9EG MPSoCs, 2 TBytes of DRAM, and 8 TBytes of SSD into a liquid-cooled rack, using a custom interconnection hardware based on 10 Gbps links. We developed this testbed in 2016-2019 in order to leverage the flexibility of FPGAs for experimenting with efficient hardware support for HPC communication among tens of thousands of processors and accelerators in the quest towards Exascale systems and beyond. In the years since then, we carefully studied this system, and we present our key design choices and insights resulting from our measurement and analysis. We developed this testbed, from architecture to the PCBs and the runtime software, within the ExaNeSt project. It is fully operational in configurations with up to 8x4x4 MPSoC nodes. It achieves high density through tight board design, while also leveraging state-of-the-art liquid cooling technology. In this paper, we present a thorough architectural analysis, along with important aspects of our infrastructure development. Our custom interconnect includes a low-cost low-latency network interface, offering user-level, zero-copy RDMA, which we coupled with the ARMv8 processors in the MPSoCs. We further developed the corresponding runtimes that allow us to test real MPI applications on the large-scale testbed. We evaluated our platform through MPI microbenchmarks, mini, and full MPI applications. Single hop, one way latency is \\(1.3\\) \\(\\mu\\) s; approximately \\(0.47\\) \\(\\mu\\) s out of these are attributed to network interface and the user-space library that exposes its functionality to the runtime. Latency over longer paths increases as expected, reaching \\(2.55\\) \\(\\mu\\) s for a five-hop path. Bandwidth tests show that, for single hop, link utilization reaches \\(82\\%\\) of the theoretical capacity. Microbenchmarks based on MPI collectives reveal that broadcast latency scales as expected when the number of participating ranks increases. We also implemented a custom MPI_Allreduce accelerator in the network interface, which reduces the latency of such collectives by up to \\(88\\%\\) . We assess performance scaling through weak and strong scaling tests for HPCG, LAMMPS, and the miniFE mini application; for all these tests, parallelization efficiency is at least \\(69\\%\\) , or better.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407135757",
    "type": "article"
  },
  {
    "title": "FPGA-Based Large-Scale Sorting with Optimized Bandwidth Utilization",
    "doi": "https://doi.org/10.1145/3716392",
    "publication_date": "2025-02-05",
    "publication_year": 2025,
    "authors": "Mingqian Sun; Guangwei Xie; Fan Zhang; Wei Guo; Xitian Fan; Li Chen; Jiayu Du",
    "corresponding_authors": "",
    "abstract": "Fast sorting of large-scale data is an essential task for data centers. In previous works, the existing computational model of sorting kernel still results in lower bandwidth utilization on the external memory bus. And the execution of merge operations in merge sort circuit on FPGAs depends on control commands from the host CPU. In this case, the merge sort circuit is not fully offloaded to hardware layer for acceleration, resulting in a performance loss. We design an on-chip merge sort controller to efficiently command the merge sort process. The proposed controller has the ability to schedule multiple on-chip computing kernels simultaneously in a more efficient mode, thus ensuring that the circuit has a better bandwidth utilization. Meanwhile, fundamental factors affecting the performance of merge sort are studied and analyzed, and we propose a high-performance merge sort architecture. Results show that using the proposed controller-centered architecture, an overall improvement of 20-30% in sorting throughput can be achieved. Compared with the state-of-the-art previous merge sorting implementation on FPGA, our circuit can achieve 1.22/1.46 \\(\\times\\) speedup.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407172994",
    "type": "article"
  },
  {
    "title": "FPGA-Accelerated Correspondence-free Point Cloud Registration with PointNet Features",
    "doi": "https://doi.org/10.1145/3717836",
    "publication_date": "2025-02-14",
    "publication_year": 2025,
    "authors": "Keisuke Sugiura; Hiroki Matsutani",
    "corresponding_authors": "",
    "abstract": "Point cloud registration serves as a basis for vision and robotic applications including 3D reconstruction and mapping. Despite significant improvements on the quality of results, recent deep learning approaches are computationally expensive and power-hungry, making them difficult to deploy on resource-constrained edge devices. To tackle this problem, in this paper, we propose a fast, accurate, and robust registration for low-cost embedded FPGAs. Based on a parallel and pipelined PointNet feature extractor, we develop custom accelerator cores namely PointLKCore and ReAgentCore, for two different learning-based methods. They are both correspondence-free and computationally efficient as they avoid the costly feature matching step involving nearest-neighbor search. The proposed cores are implemented on the Xilinx ZCU104 board and evaluated using both synthetic and real-world datasets, showing substantial improvements in the trade-off between runtime and registration quality. They run 58.27–63.21x faster than ARM Cortex-A53 CPU and offer 1.54–10.91x speedups over Intel Xeon CPU and Nvidia Jetson boards, while consuming less than 1W and achieving 133.63–267.57x energy-efficiency compared to Nvidia GeForce GPU. The proposed cores are more robust to noise and large initial misalignments than the classical methods and quickly find reasonable solutions in less than 4–12ms, demonstrating real-time performance.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407579009",
    "type": "article"
  },
  {
    "title": "<tt>DFlows</tt> : <i>A Flow-based Programming</i> Approach for a Polyglot Design-Space Exploration Framework",
    "doi": "https://doi.org/10.1145/3717837",
    "publication_date": "2025-02-18",
    "publication_year": 2025,
    "authors": "Francesco Peverelli; Daniele Paletti; Davide Conficconi",
    "corresponding_authors": "",
    "abstract": "Current architectural Design-Space Exploration (DSE) tools specify the exploration problem through annotations or pragmas. However, this approach is inherently language-dependent and limits the applicability to one specific target language and synthesis toolchain. Additionally, the rapid development of new hardware Domain-Specific Languages, programming models, and different exploration heuristics calls for a language-agnostic and modular approach. To address this need, we present a DSE formalization to facilitate the integration of new components and customized flows and leverage it to implement DFlows , a flow-based-programming DSE tool that decouples problem definition, code generation, exploration, and evaluation strategies. DFlows’s compiler-based frontend provides language-agnostic generation of design points through Abstract Syntax Tree manipulation. We show how DFlows can integrate custom performance models from complex state-of-the-art accelerators for Verilog, VHDL, Chisel, and HLS designs. We compare the runtimes of our DSE process against a state-of-the-art Chisel-based DSE tool, achieving up to 3.74× speedup while identifying the same set of optimal solutions. Additionally, we integrate in DFlows a custom exploration heuristic leveraging genetic algorithms and a novel online learning fitness function approximation methodology. This approximation yields a negligible hypervolume difference with the exhaustive search Pareto-front while improving DSE runtime by up to 2.67×.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407698449",
    "type": "article"
  },
  {
    "title": "CTScan: A CGRA-based Platform for the Emulation of Power Side-Channel Attacks on Edge CPUs",
    "doi": "https://doi.org/10.1145/3721294",
    "publication_date": "2025-03-03",
    "publication_year": 2025,
    "authors": "Yaswanth Tavva; Rohan Juneja; Trevor E. Carlson; Li-Shiuan Peh",
    "corresponding_authors": "",
    "abstract": "Cryptographic algorithms can be exploited by power side-channel attacks. Thus, it is imperative to perform a thorough pre-silicon security evaluation to minimize these potential threats. Conventional methods using FPGAs and CAD tools for pre-silicon power side-channel evaluation of CPUs can take a long time to complete. In this work, we propose CTScan, a novel platform that uses Coarse-Grained Reconfigurable Arrays (CGRAs) to speedup this evaluation. CTScan first maps the CPU microarchitecture onto the underlying CGRA hardware to mimic the execution patterns. Next, using the CPU instruction trace profiles obtained from a high-level simulator, we translate and then run these traces on the CGRA which allows for the emulated CPU power traces to be obtained for analysis. Our CGRA-based CTScan platform shows an end-to-end speedup improvement of up to 67 \\(\\times\\) speedup over state-of-the-art FPGAs for CPA attack, with comparable correlation to hypothesis. To the best of our knowledge, this is the first proposal that uses CGRAs as a platform for pre-silicon CPU security evaluation. CTScan has been validated against real silicon measurements on the Sakura-X FPGA board and a commercial RISC-V processor (SiFive FE310). Additionally, we present case studies to evaluate the applicability of CTScan when running two commonly used power side-channels 1 .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408112564",
    "type": "article"
  },
  {
    "title": "High-throughput TRNG design with novelty adjustable TDC based on STR",
    "doi": "https://doi.org/10.1145/3722118",
    "publication_date": "2025-03-07",
    "publication_year": 2025,
    "authors": "Yongkang Feng; Liang Yao; Hongli Zhou; M.Z. Wu; Shuai Xiang; Wanting Sun; Xiumin Xu; Yingchun Lu",
    "corresponding_authors": "",
    "abstract": "In IoT devices, true random number generators (TRNGs) play an increasingly important role, and advanced TRNGs must possess high throughput, low resource overhead, and high stability. In this paper, we propose a fine-grained entropy extraction circuit based on self-timed ring (STR), which can change the entropy extraction capability by varying the stages of STRs to extract randomness from different entropy sources. Importantly, the throughput of the proposed TRNG can be automatically adjusted according to the frequency of the entropy source, adapting to user requirements. The proposed TRNG is validated on Xilinx Spartan-6, Xilinx Artix-7 and Xilinx Virtex-6 FPGA development boards. It utilizes a three-stage Ring Oscillator (RO) and a five-stage Ring Oscillator (RO) for entropy extraction, requiring only 53 LUTs, 32 DFFs, and 62 registers. The generated random numbers of the TRNG, without any post-processing, achieve excellent results in NIST SP 800-22, NIST SP 800-90B, robustness test, universality test, AIS-31, and TEST U01, demonstrating a throughput of 280 Mbps.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408220068",
    "type": "article"
  },
  {
    "title": "PRISA: A Potential Region-based Intelligent Search Algorithm for Data-Flow Graph Mapping in Spatial CGRAs",
    "doi": "https://doi.org/10.1145/3723045",
    "publication_date": "2025-03-18",
    "publication_year": 2025,
    "authors": "Seyed Mehdi Mohtavipour; Hadi Shahriar Shahhoseini",
    "corresponding_authors": "",
    "abstract": "Coarse-Grained Reconfigurable Architectures (CGRAs) offer energy efficiency and programmability, making them integral to modern high-performance computing. However, complicated compilation when mapping the Data-Flow Graph (DFG) to CGRA components leads to significant time overhead, area inefficiency, and routing challenges, especially for large-scale applications today. In this paper, a fast and accurate DFG mapping approach called PRISA is proposed to reduce the compilation delay and obtain mapping solutions with higher qualities in a more reasonable time. This approach analytically identifies the potential and weak regions in the space of mapping solutions to guide the search algorithm and prevent ineffective examinations. Moreover, using the potential solutions and a novel sparse matrix permutation technique, we introduced Selective Initial Solutions (SIS) to further improve the performance of PRISA without needing long-time optimizations. As the computational requirements in the PRISA are performed analytically, there is no additional time overhead. We conducted extensive experiments on the benchmark graphs of CGRA-ME and VPR-8 toolkits and obtained outstanding results compared to integer linear programming, evolutionary, and graph traversal approaches in terms of compilation time and mapping communication cost. Our approach could map the DFGs of VPR-8 toolkit with 3.64 maximal FIFO size requirement and 53.5ms time overhead, on average.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408561525",
    "type": "article"
  },
  {
    "title": "Algorithmic-Level Design Partitioning for Latency Minimization in Multi-Chip and Multi-Die Systems",
    "doi": "https://doi.org/10.1145/3727646",
    "publication_date": "2025-04-03",
    "publication_year": 2025,
    "authors": "Cheng Zuo; Chang Wu",
    "corresponding_authors": "",
    "abstract": "In the post-Moore's Law era, multi-chip and chiplet designs have become important trends. However, due to the limited inter-chip routing resources and large delays, partition a design into such multi-chips may lead to performance degradation. Existing netlist-based partitioning algorithms mostly targeting for cut size minimization do not handle performance well. In this paper, we propose an Integer Linear Programming (ILP) based algorithmic-level design partitioning algorithm for latency minimization. Our ILP formulation considers both partition resource constraints and inter-chip routing resource constraints. Multi-cycle communication is considered to address large inter-chip delay, together with local cache design for inter-chip data transmission bandwidth. For 2.5D FPGAs, our approach can improve the final clock frequency by 74.3% and circuit runtime by 60.2% when compared to Vivado. For multi-FPGA systems, we achieve 5 to 29 times higher clock frequency and an average of 11 times better performance over Synopsys HAPS software.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409142069",
    "type": "article"
  },
  {
    "title": "QUEKUF: an FPGA Union Find Decoder for Quantum Error Correction on the Toric Code",
    "doi": "https://doi.org/10.1145/3733239",
    "publication_date": "2025-04-29",
    "publication_year": 2025,
    "authors": "Federico Valentino; Beatrice Branchini; Davide Conficconi; Donatella Sciuto; Marco D. Santambrogio",
    "corresponding_authors": "",
    "abstract": "Quantum computing represents an exciting computing paradigm that promises to solve problems untractable for a classical computer. The main limiting factor for quantum devices is the noise impacting qubits, which hinders the superpolynomial speedup promise. Thus, although Quantum Error Correction (QEC) mechanisms are paramount, QEC demands high speed and low latency to scale quantum computations to real-life-sized problems. Within this context, hardware accelerators, such as Field Programmable Gate Arrays (FPGAs), represent a valuable approach to fulfilling QEC requirements. Nevertheless, the literature falls short in proposing solutions targeting the toric code, a type of quantum Low-Density Parity Check code capable of encoding two logical qubits, thus requiring fewer physical qubits. This manuscript presents QUEKUF , an FPGA-based QEC dataflow architecture dealing with the toric code. QUEKUF disposes of parallel processing units to spatially parallelize QEC, which a centralized controller orchestrates for data movement and operation decisions. We also provide a latency-oriented resource optimization model to identify the best theoretical configuration of QUEKUF that minimizes latency and optimizes resource requirements based upon high-level quantum parameters. Experimental results show that QUEKUF attains up to 7.30× speedup and 81.51× improvement in energy efficiency over a C++ implementation with error-free syndromes while keeping high accuracy.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409917924",
    "type": "article"
  },
  {
    "title": "DVHetero: A Framework for Designing and Validating Heterogeneous SoC with RISC-V Processor and CGRA",
    "doi": "https://doi.org/10.1145/3733721",
    "publication_date": "2025-05-02",
    "publication_year": 2025,
    "authors": "Guowei Zhu; Liming Deng; Kaisen Zhang; Wang Fan; Boyin Jin; Wei Cao; Fengzhe Zhang; Xuegong Zhou; Fan Zhang; Xinsheng Yu",
    "corresponding_authors": "",
    "abstract": "CGRA, as a coprocessor in SoCs, has been widely studied. However, there is limited research on how to efficiently debug and verify SoCs composed of CGRAs and processors during the design process. To address this gap, we introduce DVHetero. DVHetero incorporates a simulation and validation framework, SoCDiff, which enables comprehensive SoC simulation, debugging, and rapid error localization. Using this verification framework, we successfully implemented and validated the entire SoC. The SoC includes a Chisel-based CGRA generator and provides a pipelined CGRA architecture template. The CGRA is tightly integrated with the RISC-V processor, allowing for efficient DMA-based data transfer and MMIO support within the SoC.The pipelined CGRA architecture generated by DVHetero shows a 1.27x improvement in area efficiency and a 10.54x increase in mapping speed compared to the state-of-the-art CGRA framework, HierCGRA. Additionally, compared to state-of-the-art CGRA-SoC systems FDRA, DVHetero demonstrates a 1.67x increase in execution speed and a 4.34x improvement in area efficiency.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410028113",
    "type": "article"
  },
  {
    "title": "R-LUT: A Reduced LUT Architecture with Fine-Grained Scalability and its Automatic Design Flow for Large Frequent Functions",
    "doi": "https://doi.org/10.1145/3737291",
    "publication_date": "2025-05-23",
    "publication_year": 2025,
    "authors": "Moucheng Yang; C J Zeng; Kaixiang Zhu; Lingli Wang",
    "corresponding_authors": "",
    "abstract": "As technology scaling exacerbates interconnect resistance in advanced nodes, FPGA architectures demand enhanced programmable logic blocks (PLBs) to minimize global metal routing. However, it is expensive to raise the functionality of LUTs due to exponential area growth with the number of inputs, resulting in poor scalability. Moreover, LUTs are redundant since practical functions in real-world benchmarks only account for an extremely small proportion of all the functions. For example, only 16424 out of more than 100 trillion NPN classes of 6-input functions are used in the mapped netlists of the VTR8 and KOIOS benchmarks. Therefore, we propose a reduced LUT architecture, named RLUT, to efficiently implement most of the frequent functions. The compact structure of the MUX tree in LUTs is preserved and reduced, while the reduced programmable bits are connected to the MUX tree according to the bit assignment generated automatically by the proposed algorithms. Results of evaluations by a full EDA flow show that, compared with the modified Stratix10 baseline, the proposed 8-input PLB with 75 SRAM bits, named Dual-RLUT6, reduces the maximum logic levels significantly by 20.85%, while the critical path delay is improved by 10.11% at the cost of 4.65% area overhead.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410644240",
    "type": "article"
  },
  {
    "title": "SPDFA: A Novel Dataflow Fusion Sparse Deep Neural Network Accelerator",
    "doi": "https://doi.org/10.1145/3737462",
    "publication_date": "2025-05-30",
    "publication_year": 2025,
    "authors": "Jinwei Xu; Jingfei Jiang; Lei Gao; Xifu Qian; Yong Dou",
    "corresponding_authors": "",
    "abstract": "Unstructured sparse pruning significantly reduces the computational and parametric complexities of deep neural network models. Nevertheless, the highly irregular nature of sparse models limits its performance and efficiency on traditional computing platforms, thereby prompting the development of specialized hardware solutions. To improve computational efficiency, we introduce the Sparse Dataflow Fusion Accelerator (SPDFA), a specialized architecture meticulously designed for sparse deep neural networks. Firstly, we present a non-blocking data distribution-computing engine that integrates inner product and column product. This engine boosts computational efficiency by decomposing matrix multiplication and convolution into rectangular matrix-vector multiplications. Secondly, we implement a computation array to further exploit the parallelism, and design an on-chip buffer structure that supports multi-line memory access mode. Lastly, to bolster the adaptability of our accelerator, we propose an innovative macroinstruction set coupled with a micro-kernel scheme. Furthermore, we refine the macroinstruction issue strategy, thereby further enhancing computational efficiency. Our evaluation results demonstrate that SPDFA achieves an average 1.29 \\(\\times\\) -2.38 \\(\\times\\) improvement in computational efficiency compared to the state-of-the-art SpMM accelerators when applied to unstructured sparse deep neural network models. Furthermore, its performance outperforms existing sparse neural network accelerators by a factor of 1.03 \\(\\times\\) -1.83 \\(\\times\\) . Additionally, SPDFA exhibits excellent scalability with a scaling efficiency exceeding 80%.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410893427",
    "type": "article"
  },
  {
    "title": "Accelerated Phylogenetics on the AMD <sup>®</sup> Versal™ Adaptive SoC",
    "doi": "https://doi.org/10.1145/3747592",
    "publication_date": "2025-07-11",
    "publication_year": 2025,
    "authors": "Geert Roks; Mario Ruiz; Nikolaos Alachiotis",
    "corresponding_authors": "",
    "abstract": "Phylogenetics study the evolutionary history of organisms using an iterative procedure of creating and evaluating phylogenetic trees. This procedure is highly compute-intensive; constructing a large phylogenetic tree requires hundreds to thousands of CPU hours. Most phylogenetic analyses today rely either on maximum likelihood (ML) or Bayesian inference (BI) methods for inferring phylogenetic trees; the phylogenetic likelihood function (PLF) is employed in both ML and BI approaches as the tree-evaluation function, accounting for up to 95% of the overall analysis time. In this work, we explore the AMD ® Versal™ Adaptive SoC architecture for accelerating the PLF that heavily relies on matrix multiplication operations. We find that the tight integration of domain-specific processors (AI Engines) with programmable logic (PL) in the Versal architecture is highly suitable for the needs of the PLF: we map the core operation of the PLF (matrix multiplication) to the AI Engines and deploy special-purpose units in the PL for PLF-specific data caching and input/output management, as well as numerical scaling that is a prerequisite for yielding numerically stable solutions for large-scale phylogenetic studies. We conducted a thorough performance analysis to leverage the platform capabilities to guide matrix-multiplication acceleration that requires close PL-AIE cooperation. We observed between \\(23.8\\times\\) and \\(47.0\\times\\) higher computational power of the Versal SoC than one x86 CPU core (both AMD ® and Intel ® ) using AVX2 intrinsics, and between \\(3.7\\times\\) and \\(5.9\\times\\) higher performance than eight cores. For the full system, we observe comparable performance ( \\(\\pm\\ 30\\%\\) ) with 8 CPU cores due to device memory access and PCIe ® limitations, showing the potential of the Versal architecture in accelerating a distinct application from DSP/AI, its primary design focus.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412203320",
    "type": "article"
  },
  {
    "title": "Exploring the Contribution of Hardware Shuffling in Securing Low-Cost Symmetric Encryption Devices against Power-Based Side-Channel Attacks: Case Study of an AES-128 on FPGA",
    "doi": "https://doi.org/10.1145/3758100",
    "publication_date": "2025-08-04",
    "publication_year": 2025,
    "authors": "Vianney Lapôtre; Cyrille Chavet; Ghita Harcha; Philippe Coussy",
    "corresponding_authors": "",
    "abstract": "In the era of the Internet of Things (IoT), embedded systems are massively spreading in critical infrastructures. Low-cost and low-power components are used to build such devices, which manipulate sensitive data and communicate at continuously growing throughput. To protect these data, IoT nodes embed cryptographic primitives including countermeasures against Side-Channel Attacks (SCA). In this article, we explore the interest of hardware-based shuffling to protect AES ciphers against power-based side-channel attacks in the context of low-cost IoT devices. Shuffling is performed via a dedicated hardware module, wherein a Pseudo-Random Number Generator provides a random vector used to control a permutation network, generating a permutation which determines the AES computation sequence. The approach has been explored and evaluated through several FPGA-based design solutions in term of area, timing performance, and security. Compared to an unprotected design, the best solution leads to a minimum area overhead factor of 1.2 or a maximum throughput of 45.23 Mbit/s. Furthermore, compared to existing works that also depend on hardware shuffling, the proposed solution is up to 10.4 times faster. Results show that hardware-based shuffling solution as implemented increases the Measure-to-Disclosure metric by a factor greater than 10,000 when considering Correlation Power Analysis-based SCA.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4412928633",
    "type": "article"
  },
  {
    "title": "VERSATILE: Very fast partial reconfiguration controller",
    "doi": "https://doi.org/10.1145/3748728",
    "publication_date": "2025-07-17",
    "publication_year": 2025,
    "authors": "M. A. Ibrahim; Sébastien Pillement; Andrea Pinna; Sébastien Le Nours",
    "corresponding_authors": "",
    "abstract": "Dynamically reconfigurable architectures allow sharing of hardware resources, which is particularly beneficial for small low-end FPGAs. Based on the online modification of parts of the circuit, these architectures require partial reconfiguration of the chip. Reducing resource availability or usage comes at the cost of a performance penalty affecting execution time. The challenge lies in bitstream management, especially for complex applications that often exceed the internal memory capacity of the FPGA (BRAM). Consequently, the time penalty arises from the need to retrieve partial bitstreams from external memory (e.g., often a DDR) each time it is necessary. Current state-of-the-art reconfiguration controllers are limited to a throughput of 400 MB/s, significantly penalizing reconfiguration times and making dynamic reconfiguration unattractive for real life applications (e.g., video processing, machine learning applications, continual and federated learning for embedded systems, …). This paper introduces a novel partial reconfiguration controller architecture that achieves a throughput of up to 1.396 GB/s, a 3.49x acceleration over existing controllers. The reduced reconfiguration time allows the practical use of dynamic reconfiguration with fewer performance penalties. Additionally, the paper compares various reconfiguration controllers in terms of time penalties and offers a trade-off between algorithm complexity, FPGA resources, and performance.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413078307",
    "type": "article"
  },
  {
    "title": "Editorial: A Message from the New Editor-in-Chief",
    "doi": "https://doi.org/10.1145/3765289",
    "publication_date": "2025-08-29",
    "publication_year": 2025,
    "authors": "Vaughn Betz",
    "corresponding_authors": "Vaughn Betz",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413816464",
    "type": "editorial"
  },
  {
    "title": "GreyConE+: Efficient Rare-Target Test Generation for FPGA HLS Designs",
    "doi": "https://doi.org/10.1145/3769295",
    "publication_date": "2025-09-25",
    "publication_year": 2025,
    "authors": "Mukta Debnath; Animesh Basak Chowdhury; Debasri Saha; Susmita Sur‐Kolay",
    "corresponding_authors": "",
    "abstract": "High-Level Synthesis (HLS) has transformed the development of complex hardware IPs (HWIPs) by enabling abstraction and configurability through languages such as SystemC and C/C++, particularly for FPGA-based high-performance and cloud computing applications. HLS streamlines design space exploration and functional verification. It allows efficient IP synthesis across various FPGA platforms. However, it also introduces security risks, such as hidden circuitry and hardware Trojans being embedded by untrusted third-party vendors. These threats can lead to data leaks, functionality disruptions and hardware damage. The risks are particularly concerning in cloud environments with multi-tenant architectures, where multiple FPGA-based IPs operate on shared infrastructure. Detecting such threats before synthesis requires robust security validation frameworks. This work presents GreyConE+ , an advanced security testing framework for FPGA-based HLS IPs, designed to detect rare-trigger vulnerabilities that often evade conventional verification methods. By integrating selective instrumentation, greybox fuzzing, and concolic execution, GreyConE+ enhances test generation and efficiently uncovers hidden Trojans and functional anomalies. Evaluations on diverse HLS benchmarks, including SystemC and ML-based C++ designs, demonstrate higher coverage, faster Trojan detection, reduced memory overhead, and lower testing costs compared to existing techniques, reinforcing its effectiveness in securing FPGA-based HLS designs.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414510953",
    "type": "article"
  },
  {
    "title": "ISRLUT: Integer-Only FHD Image Super-Resolution based on Neural Lookup Table and Near-Memory Computing",
    "doi": "https://doi.org/10.1145/3770759",
    "publication_date": "2025-10-03",
    "publication_year": 2025,
    "authors": "Tsung-Che Lu; Jianyang Ding; Bowen Jiang; Huachen Zhang; Wei Xu; Zhilei Chai",
    "corresponding_authors": "",
    "abstract": "While Deep Neural Networks (DNNs) have achieved remarkable progress in Image Super-Resolution (SR) task, they face significant challenges for edge processing FHD images. Complex DNN operators lead to high hardware resource consumption and latency. Computational inefficiency of FPU increases energy consumption, while DDR access overhead and on-chip memory overflow further constrain real-time capabilities. To address this, we propose ISRLUT, a novel accelerator architecture focused on integer-only inference and near-memory computing. Its core contributions include: 1) Fusion of Neural LUT arithmetic with reconfigurable compute units, transforming unified LUT operators from DNN operators and enhancing hardware utilization; 2) An integer-only inference and parallel architecture, eliminating floating-point dependencies and significantly reducing energy consumption; 3) An innovative internal operator memory management scheme coupled with Tile-based Buffer Overlap and Private Cache Mechanism. We deploy ISRLUT on FPGA and ASIC platforms. Experiments demonstrate that ISRLUT achieves efficient performance: For 4 \\(\\times\\) upscaling, it requires only 36.9KB of storage and achieves a PSNR of 30.21dB on Set5. Hardware implementation using a 55nm ASIC consumes merely 0.0337W power, delivers an energy efficiency of 7278.6 Mpixels/s/W, and achieves a real-time frame rate of 118 FPS for 4 \\(\\times\\) FHD processing, validating its superiority in energy efficiency and hardware utilization.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414783225",
    "type": "article"
  },
  {
    "title": "EMINEM: <u>E</u> fficient FPGA Implementation of <u>M</u> ixed-RadIx <u>N</u> TT Hardware Acc <u>E</u> lerators for NIST Post-Quantu <u>M</u> Cryptography Falcon, Dilithium, and HAWK",
    "doi": "https://doi.org/10.1145/3771287",
    "publication_date": "2025-10-09",
    "publication_year": 2025,
    "authors": "Yazheng Tu; Jiafeng Xie",
    "corresponding_authors": "",
    "abstract": "The advent of quantum computing poses a significant threat to modern cryptography. To address this challenge, the National Institute of Standards and Technology (NIST) has initiated the PQC standardization process, and several algorithms have been selected (a few are still under consideration in the additional standardization process). Among these schemes, lattice-based post-quantum cryptography (PQC) has emerged as a promising approach and garnered substantial attention from the implementation community, especially on the hardware platforms. Notably, the field-programmable gate array (FPGA) has gained considerable attention as a convenient platform for hardware implementation, not only from NIST but also from the research community, as reflected by the related recommendations from NIST and the number of works reported recently. This work follows the existing trend of developing novel FPGA implementations for PQC. It is worth mentioning that the polynomial multiplication in these NIST lattice-based PQC algorithms can be implemented with Number Theoretic Transform (NTT) for efficiency. Nevertheless, there remains a lack of novel and universal NTT methods for polynomial multiplication at different sizes. For instance, for \\(n=512\\) (F alcon and HAWK), the existing works are mostly limited to the Radix-2 NTT (other methods like Radix-4 or Radix-8 cannot be directly applied). To fill the research gap, this paper presents a novel design framework, i.e., E fficient M ixed-Rad I x N TT hardware acc E lerators for NIST post-quantu M cryptography (EMINEM, specially tailored for targeted schemes). Our design leverages Radix-4 for polynomial sizes of 256 and 1,024, while introducing a hybrid Radix-2/4 strategy for NTT of length 512 and achieving comparable performance to pure Radix-4 at other lengths. In total, our contributions include: (i) a generic Radix-4/Mixed-Radix NTT algorithm is proposed for \\(n=256\\) , 512, and 1,024; (ii) an efficient NTT hardware accelerator is designed with the help of a new memory access pattern and some optimization techniques; (iii) two types of butterfly architectures are developed to obtain pure Radix-4 time complexity and low resource usage, respectively; (iv) a detailed implementation and comparison showcase the superior performance of the proposed design strategy. Overall, the proposed strategy enables the efficient deployment of Mixed-Radix NTT in targeted NIST schemes, surpassing the limitations of the conventional Radix-2 approach for 512-length NTT designs. The proposed design offers a significant advancement in the field, facilitating efficient FPGA acceleration of PQC standards.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414993639",
    "type": "article"
  },
  {
    "title": "AHCA: Agile Design Framework for Hashcat Acceleration based on FPGA",
    "doi": "https://doi.org/10.1145/3770760",
    "publication_date": "2025-10-10",
    "publication_year": 2025,
    "authors": "Liming Deng; Guowei Zhu; Xitian Fan; Wei Cao; Xuegong Zhou; Fan Zhang; Shaobo Yang",
    "corresponding_authors": "",
    "abstract": "This paper presents AHCA, an agile design framework for FPGA-based Hashcat acceleration that automates the generation of optimized register transfer level (RTL) code. Our approach is centered on a proposed automated design method using a parameterized domain-specific template (DST) and a specific hardware operator library. The framework analyzes an algorithm’s graph to extract key hardware operators and their interconnection network. To support diverse user inputs, we introduce an innovative operator matching strategy using subgraph isomorphism, which maps algorithms to our operator library. This matched information, combined with Design Space Exploration (DSE), is used to configure the DST and generate the final RTL code, avoiding redundancy for previously implemented algorithms. Compared to state-of-the-art high-level synthesis (HLS) tools, AHCA demonstrates a maximum performance enhancement of 797×, a LUT efficiency improvement of up to 105×, and an energy efficiency gain of up to 676×. When deployed on an FPGA for password cracking, the AHCA-generated hardware achieves a 63.95× enhancement in energy efficiency over CPUs and a 4.71× improvement over GPUs.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415047079",
    "type": "article"
  },
  {
    "title": "An Energy-Efficient and Real-Time FPGA-Based Point Cloud Registration Framework with Ultra-Fast and Configurable Multi-Mode Correspondence Search",
    "doi": "https://doi.org/10.1145/3771768",
    "publication_date": "2025-10-15",
    "publication_year": 2025,
    "authors": "Qi Deng; Hao Sun; Yuhao Shu; Jian-Zhong Xiao; Weixiong Jiang; Hui Wang; Yajun Ha",
    "corresponding_authors": "",
    "abstract": "Point cloud registration is a fundamental task in LiDAR-based localization and mapping, widely employed in robotics and autonomous vehicles. However, existing registration solutions lose geometric topology continuity and lack scanline-aware, configurable correspondence search, restricting their real-time applicability. To solve these issues, we propose a fundamentally re-architected, energy-efficient FPGA framework for real-time point cloud registration, featuring a configurable, multi-mode correspondence search engine. First, we introduce a scanline-aided range-projection structure (SA-RPS) that reorganizes LiDAR points within configurable segmentation domains into contiguous memory while preserving scanline topology, enabling efficient and flexible multi-mode correspondence search. Second, we develop a deeply pipelined, ultra-fast SA-RPS-based correspondence search (SA-RPS-CS) accelerator that supports dynamic configuration of search mode and parallelism, and incorporates a sliding-window cache and scanline-aware K-selection module for high-throughput, multi-mode correspondence extraction. Third, we present a co-designed registration framework that integrates the accelerator with dynamic parameter configuration, enabling adaptive, real-time processing across diverse SLAM scenarios. Experimental results demonstrate that the proposed SA-RPS-CS accelerator delivers \\(2.3\\times\\) – \\(32.4\\times\\) faster search and \\(1.8\\times\\) – \\(26.2\\times\\) higher energy efficiency than previous state-of-the-art FPGA designs, achieving real-time registration for 64-channel LiDAR at 21.5 FPS with negligible loss in accuracy.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415228492",
    "type": "article"
  },
  {
    "title": "LHAM: Low-Cost and High-Accuracy Approximate Multiplier for FPGA-Based Computing",
    "doi": "https://doi.org/10.1145/3770757",
    "publication_date": "2025-10-16",
    "publication_year": 2025,
    "authors": "Mingyu Shu; Qiang Liu",
    "corresponding_authors": "",
    "abstract": "The rapid development of artificial intelligence raises higher demands on the performance of intelligent devices, requiring new computing units with lower resource usage and power consumption. Approximate multipliers (AMs) meet this need by reducing resource and power consumption at the cost of computational accuracy, and are widely used in fields like image processing and deep neural networks. In this paper, we present a low-cost and high-performance approximate multiplier (LHAM) design methodology targeting Field Programmable Gate Arrays (FPGAs). The expressions of carry propagation and carry generation for FPGA-based arry-look-ahead adders (CLA) are optimized, which effectively reducing the errors associated with discarding carry generation information. Using these expressions together with a logic fusion based approximation strategy, we design both accurate and approximate adders. These adders can be selectively configured during the partial product accumulation stage of the multiplier, allowing a tunable trade-off between computational accuracy and hardware resource utilization. Finally, we model the AM design space as a 0-1 Knapsack problem to efficiently generate optimized designs under varying accuracy and area requirements. Experimental results show that, compared to the Xilinx accurate multiplier IP core, the proposed LHAM reduces LUT usage, delay and power consumption by 50.7%, 21.7%, 20.6% for \\(8\\times 8\\) multiplication, respectively. Compared to existing AMs, LHAM uses the fewest LUTs and achieves the best trade-off between accuracy and area. The proposed LHAM is also applied to two applications to demonstrate its efficiency and effectiveness.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415249094",
    "type": "article"
  },
  {
    "title": "HiFA: A High-Performance and Flexible Acceleration Framework for Large-Size Number Theoretic Transform",
    "doi": "https://doi.org/10.1145/3771769",
    "publication_date": "2025-10-18",
    "publication_year": 2025,
    "authors": "Qilin Hu; Haotian Wang; Chubo Liu; Keqin Li; Kenli Li",
    "corresponding_authors": "",
    "abstract": "Zero-knowledge proofs (ZKP) and homomorphic encryption (HE) are crucial for data privacy in applications like cloud, blockchain, and analytics. However, the real-world adoption often faces performance challenges, particularly in the execution of the Number Theoretic Transform (NTT) required for polynomial multiplication involving sizes beyond 2 20 and large integer widths (e.g., 256 bits). FPGAs offer a promising platform for acceleration, but efficiently implementing large-size NTTs remains difficult due to the limited on-chip resources. The widely adopted four-step NTT method, used to relieve the need for large on-chip memory, introduces performance bottlenecks. Firstly, the traditional dataflow NTT architecture may not fully exploit available compute capability, which hinders achieving peak performance. Furthermore, during the matrix transpose phase, the non-sequential access to external high-bandwidth memory (HBM) causes inefficiency. To address these challenges, we introduce HiFA, an FPGA-based automatic accelerator framework designed for high performance and flexible large-size NTT computations. HiFA utilizes a stacked NTT architecture for high parallelism, maximizing HBM throughput. It supports various decomposed polynomial sizes via a novel reordering module. Additionally, a specialized cyclic shuffle module is integrated to optimize data movement during the matrix transpose step, alleviating random memory access delay. HiFA also provides an automatic Design Space Exploration (DSE) framework that identifies optimal four-step decomposition parameters and generates corresponding hardware configurations. Our experiments show that the FPGA implementation of HiFA achieves an average speedup of 2.97× and up to 7.25× improvement in latency over prior state-of-the-art FPGA solutions. Compared to prior GPU-based methods, HiFA achieves an average energy efficiency gain of 2.24×.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415312917",
    "type": "article"
  },
  {
    "title": "Parametric Yield Modeling and Simulations of FPGA Circuits Considering Within-Die Delay Variations",
    "doi": "https://doi.org/10.1145/1371579.1371582",
    "publication_date": "2008-06-01",
    "publication_year": 2008,
    "authors": "Pete Sedcole; Peter Y. K. Cheung",
    "corresponding_authors": "",
    "abstract": "Variations in the semiconductor fabrication process results in differences in parameters between transistors on the same die, a problem exacerbated by lithographic scaling. Field-Programmable Gate Arrays may be able to compensate for within-die delay variability, by judicious use of reconfigurability. This article presents two strategies for compensating within-die stochastic delay variability by using reconfiguration: reconfiguring the entire FPGA, and relocating subcircuits within an FPGA. Analytical models for the theoretical bounds on the achievable gains are derived for both strategies and compared to models for worst-case design as well as statistical static timing analysis (SSTA). All models are validated by comparison to circuit-level Monte Carlo simulations. It is demonstrated that significant improvements in circuit yield and timing are possible using SSTA alone, and these improvements can be enhanced by employing reconfiguration-based techniques.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2031871682",
    "type": "article"
  },
  {
    "title": "Elliptic Curve Cryptography on FPGA for Low-Power Applications",
    "doi": "https://doi.org/10.1145/1502781.1502783",
    "publication_date": "2009-03-01",
    "publication_year": 2009,
    "authors": "Maurice Keller; Andrew W. Byrne; William P. Marnane",
    "corresponding_authors": "",
    "abstract": "Elliptic curve cryptography has generated a lot of research interest due to its ability to provide greater security per bit compared to public key systems such as RSA. The designer of an elliptic curve hardware accelerator is faced with many choices at design time, each of which can impact the performance of the accelerator in different ways. There are many examples in the literature of how these design choices can effect the area and/or speed of an elliptic curve hardware accelerator. The effect of design choices on power and energy consumption in elliptic curve hardware has been less well studied. This article studies the effect of design choices on the power and energy consumption of an FPGA-based reconfigurable elliptic curve hardware accelerator. A reconfigurable processor has been used for different system parameters and the power and energy consumption measured. The power and energy results are presented and compared.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2132513685",
    "type": "article"
  },
  {
    "title": "Compute Bound and I/O Bound Cellular Automata Simulations on FPGA Logic",
    "doi": "https://doi.org/10.1145/1462586.1462592",
    "publication_date": "2009-01-01",
    "publication_year": 2009,
    "authors": "S. Murtaza; Alfons G. Hoekstra; P.M.A. Sloot",
    "corresponding_authors": "",
    "abstract": "FPGA-based computation engines have been used as Cellular Automata accelerators in the scientific community for some time now. With the recent availability of more advanced FPGA logic it becomes necessary to better understand the mapping of Cellular Automata to these systems. There are many trade-offs to consider when mapping a Cellular Automata algorithm from an abstract system to the physical implementation using FPGA logic. The trade-offs include both the available FPGA resources and the Cellular Automata algorithm's execution time. The most important aspect is to fully understand the behavior of the specified CA algorithm in terms of its execution times which are either compute bound or I/O bound. In this article, we present a methodology to categorize a specified CA algorithm as a compute bound or an I/O bound. We take the methodology further by presenting rigorous analysis for each of the two cases identifying the various parameters that control the mapping process and are defined both by the Cellular Automata algorithm and the given FPGA hardware specifications. This methodology helps to predict the performance of running Cellular Automata algorithms on specific FPGA hardware and to determine optimal values for the various parameters that control the mapping process. The model is validated for both compute and I/O bound two-dimensional Cellular Automata algorithms. We find that our model predictions are accurate within 7%.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W1996725964",
    "type": "article"
  },
  {
    "title": "Fine-Grained Module-Based Error Recovery in FPGA-Based TMR Systems",
    "doi": "https://doi.org/10.1145/3173549",
    "publication_date": "2018-01-24",
    "publication_year": 2018,
    "authors": "Zhuoran Zhao; Nguyen T. H. Nguyen; Dimitris Agiakatsikas; Ganghee Lee; Ediz Cetin; Oliver Diessel",
    "corresponding_authors": "",
    "abstract": "Space processing applications deployed on SRAM-based Field Programmable Gate Arrays (FPGAs) are vulnerable to radiation-induced Single Event Upsets (SEUs). Compared with the well-known SEU mitigation solution—Triple Modular Redundancy (TMR) with configuration memory scrubbing—TMR with module-based error recovery (MER) is notably more energy efficient and responsive in repairing soft-errors in the system. Unfortunately, TMR-MER systems also need to resort to scrubbing when errors occur between sub-components, such as in interconnection nets, which are not recovered by MER. This article addresses this problem by proposing a fine-grained module-based error recovery technique, which can localize and correct errors that classic MER fails to do without additional system hardware. We evaluate our proposal via fault-injection campaigns on three types of circuits implemented in Xilinx 7-Series devices. With respect to scrubbing, we observed reductions in the mean time to repair configuration memory errors of between 48.5% and 89.4%, while reductions in energy used recovering from configuration memory errors were estimated at between 77.4% and 96.1%. These improvements result in higher reliability for systems employing TMR with fine-grained reconfiguration than equivalent systems relying on scrubbing for configuration error recovery.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2794208964",
    "type": "article"
  },
  {
    "title": "An Evaluation on the Accuracy of the Minimum-Width Transistor Area Models in Ranking the Layout Area of FPGA Architectures",
    "doi": "https://doi.org/10.1145/3182394",
    "publication_date": "2018-03-14",
    "publication_year": 2018,
    "authors": "Farheen Khan; Andy Ye",
    "corresponding_authors": "",
    "abstract": "This work provides an evaluation on the accuracy of the minimum-width transistor area models in ranking the actual layout area of FPGA architectures. Both the original VPR area model and the new COFFE area model are compared against the actual layouts with up to three metal layers for the various FPGA building blocks. We found that both models have significant variations with respect to the accuracy of their predictions across the building blocks. In particular, the original VPR model overestimates the layout area of larger buffers, full adders, and multiplexers by as much as 38%, while they underestimate the layout area of smaller buffers and multiplexers by as much as 58%, for an overall prediction error variation of 96%. The newer COFFE model also significantly overestimates the layout area of full adders by 13% and underestimates the layout area of multiplexers by a maximum of 60% for a prediction error variation of 73%. Such variations are particularly significant considering sensitivity analyses are not routinely performed in FPGA architectural studies. Our results suggest that such analyses are extremely important in studies that employ the minimum-width area models so the tolerance of the architectural conclusions against the prediction error variations can be quantified. Furthermore, an open-source version of the layouts of the actual FPGA building blocks should be created so their actual layout area can be used to achieve a highly accurate ranking of the implementation area of FPGA architectures built upon these layouts.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2794259726",
    "type": "article"
  },
  {
    "title": "Dynamic Defragmentation of Reconfigurable Devices",
    "doi": "https://doi.org/10.1145/2209285.2209287",
    "publication_date": "2012-06-01",
    "publication_year": 2012,
    "authors": "Sándor P. Fekete; Tom Kamphans; Nils Schweer; Christopher Tessars; Jan C. van der Veen; Josef Angermeier; Dirk Koch; Jürgen Teich",
    "corresponding_authors": "",
    "abstract": "We propose a new method for defragmenting the module layout of a reconfigurable device, enabled by a novel approach for dealing with communication needs between relocated modules and with inhomogeneities found in commonly used FPGAs. Our method is based on dynamic relocation of module positions during runtime, with only very little reconfiguration overhead; the objective is to maximize the length of contiguous free space that is available for new modules. We describe a number of algorithmic aspects of good defragmentation, and present an optimization method based on tabu search. Experimental results indicate that we can improve the quality of module layout by roughly 50% over the static layout. Among other benefits, this improvement avoids unnecessary rejections of modules.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W1978905553",
    "type": "article"
  },
  {
    "title": "ReDCrypt",
    "doi": "https://doi.org/10.1145/3242899",
    "publication_date": "2018-09-30",
    "publication_year": 2018,
    "authors": "Bita Darvish Rouhani; Siam U. Hussain; Kristin Lauter; Farinaz Koushanfar",
    "corresponding_authors": "",
    "abstract": "Artificial Intelligence (AI) is increasingly incorporated into the cloud business in order to improve the functionality (e.g., accuracy) of the service. The adoption of AI as a cloud service raises serious privacy concerns in applications where the risk of data leakage is not acceptable. Examples of such applications include scenarios where clients hold potentially sensitive private information such as medical records, financial data, and/or location. This article proposes ReDCrypt, the first reconfigurable hardware-accelerated framework that empowers privacy-preserving inference of deep learning models in cloud servers. ReDCrypt is well-suited for streaming (a.k.a., real-time AI) settings where clients need to dynamically analyze their data as it is collected over time without having to queue the samples to meet a certain batch size. Unlike prior work, ReDCrypt neither requires to change how AI models are trained nor relies on two non-colluding servers to perform. The privacy-preserving computation in ReDCrypt is executed using Yao’s Garbled Circuit (GC) protocol. We break down the deep learning inference task into two phases: (i) privacy-insensitive (local) computation, and (ii) privacy-sensitive (interactive) computation. We devise a high-throughput and power-efficient implementation of GC protocol on FPGA for the privacy-sensitive phase. ReDCrypt’s accompanying API provides support for seamless integration of ReDCrypt into any deep learning framework. Proof-of-concept evaluations for different DL applications demonstrate up to 57-fold higher throughput per core compared to the best prior solution with no drop in the accuracy.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2904873688",
    "type": "article"
  },
  {
    "title": "FeatherNet",
    "doi": "https://doi.org/10.1145/3306202",
    "publication_date": "2019-03-28",
    "publication_year": 2019,
    "authors": "Raghid Morcel; Hazem Hajj; Mazen A. R. Saghir; Haitham Akkary; Hassan Artail; Rahul Khanna; Anil Keshavamurthy",
    "corresponding_authors": "",
    "abstract": "Convolutional Neural Network (ConvNet or CNN) algorithms are characterized by a large number of model parameters and high computational complexity. These two requirements have made it challenging for implementations on resource-limited FPGAs. The challenges are magnified when considering designs for low-end FPGAs. While previous work has demonstrated successful ConvNet implementations with high-end FPGAs, this article presents a ConvNet accelerator design that enables the implementation of complex deep ConvNet architectures on resource-constrained FPGA platforms aimed at the IoT market. We call the design “FeatherNet” for its light resource utilization. The implementations are VHDL-based providing flexibility in design optimizations. As part of the design process, new methods are introduced to address several design challenges. The first method is a novel stride-aware graph-based method targeted at ConvNets that aims at achieving efficient signal processing with reduced resource utilization. The second method addresses the challenge of determining the minimal precision arithmetic needed while preserving high accuracy. For this challenge, we propose variable-width dynamic fixed-point representations combined with a layer-by-layer design-space pruning heuristic across the different layers of the deep ConvNet model. The third method aims at achieving a modular design that can support different types of ConvNet layers while ensuring low resource utilization. For this challenge, we propose the modules to be relatively small and composed of computational filters that can be interconnected to build an entire accelerator design. These model elements can be easily configured through HDL parameters (e.g., layer type, mask size, stride, etc.) to meet the needs of specific ConvNet implementations and thus they can be reused to implement a wide variety of ConvNet architectures. The fourth method addresses the challenge of design portability between two different FPGA vendor platforms, namely, Intel/Altera and Xilinx. For this challenge, we propose to instantiate the device-specific hardware blocks needed in each computational filter, rather than relying on the synthesis tools to infer these blocks, while keeping track of the similarities and differences between the two platforms. We believe that the solutions to these design challenges further advance knowledge as they can benefit designers and other researchers using similar devices or facing similar challenges. Our results demonstrated the success of addressing the design challenges and achieving low (30%) resource utilization for the low-end FPGA platforms: Zedboard and Cyclone V. The design overcame the limitation of designs targeted for high-end platforms and that cannot fit on low-end IoT platforms. Furthermore, our design showed superior performance results (measured in terms of [Frame/s/W] per Dollar) compared to high-end optimized designs.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2926546867",
    "type": "article"
  },
  {
    "title": "Reconfigurable Framework for Environmentally Adaptive Resilience in Hybrid Space Systems",
    "doi": "https://doi.org/10.1145/3398380",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "Sebastian Sabogal; Alan D. George; C. M. Wilson",
    "corresponding_authors": "",
    "abstract": "Due to ongoing innovations in both sensor technology and spacecraft autonomy, onboard space processing continues to be outpaced by the escalating computational demands required for next-generation missions. Commercial-off-the-shelf, hybrid system-on-chips, combining fixed-logic CPUs with reconfigurable-logic FPGAs, present numerous architectural advantages that address onboard computing challenges. However, commercial devices are highly susceptible to space radiation and require dependable computing strategies to mitigate radiation-induced single-event effects. Depending upon the mission, the dynamics of the near-Earth space-radiation environment expose spacecraft to radiation fluxes that can vary by several orders of magnitude. By adopting an adaptive approach to dependable computing, spacecraft computers can reconfigure system resources to efficiently accommodate changing environmental conditions to maximize system performance while satisfying availability constraints throughout the mission. In this article, we propose Hybrid, Adaptive, Reconfigurable Fault Tolerance (HARFT), a reconfigurable framework for environmentally adaptive resilience in hybrid space systems. Furthermore, we describe a methodology to model adaptive systems, represented as phased-mission systems using Markov chains, subject to the near-Earth space-radiation environment, using a combination of orbital perturbation, geomagnetic field, and single-event effect rate prediction tools. We apply this methodology to evaluate the HARFT architecture using various static and adaptive strategies for several orbital case studies and demonstrate the achievable performability gains.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W3041051302",
    "type": "article"
  },
  {
    "title": "Area-Efficient Near-Associative Memories on FPGAs",
    "doi": "https://doi.org/10.1145/2629471",
    "publication_date": "2015-01-23",
    "publication_year": 2015,
    "authors": "Udit Dhawan; André DeHon",
    "corresponding_authors": "",
    "abstract": "Associative memories can map sparsely used keys to values with low latency but can incur heavy area overheads. The lack of customized hardware for associative memories in today’s mainstream FPGAs exacerbates the overhead cost of building these memories using the fixed address match BRAMs. In this article, we develop a new, FPGA-friendly, memory system architecture based on a multiple hash scheme that is able to achieve near-associative performance without the area-delay overheads of a fully associative memory on FPGAs. At the same time, we develop a novel memory management algorithm that allows us to statistically mimic an associative memory. Using the proposed architecture as a 64KB L1 data cache, we show that it is able to achieve near-associative miss rates while consuming 3--13 × fewer FPGA memory resources for a set of benchmark programs from the SPEC CPU2006 suite than fully associative memories generated by the Xilinx Coregen tool. Benefits for our architecture increase with key width, allowing area reduction up to 100 ×. Mapping delay is also reduced to 3.7ns for a 1,024-entry flat version or 6.1ns for an area-efficient version compared to 17.6ns for a fully associative memory for a 64-bit key on a Xilinx Virtex 6 device.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2049489754",
    "type": "article"
  },
  {
    "title": "Safe Dynamic Reshaping of Reconfigurable MPSoC Embedded Systems for Self-Healing and Self-Adaption Purposes",
    "doi": "https://doi.org/10.1145/2700416",
    "publication_date": "2015-09-11",
    "publication_year": 2015,
    "authors": "Alexander Biedermann; Sorin A. Huss; Adeel Israr",
    "corresponding_authors": "",
    "abstract": "Multiprocessor system-on-chip (MPSoC) architectures are a huge challenge in embedded system design. This situation arises from the fact that available MPSoCs and related designs flows are not tailored to the specific needs of embedded systems. This work demonstrates how to provide self-healing properties in embedded MPSoC design. This is achieved by combining the features of a generic approach to create virtualizable MPSoCs out of off-the-shelf embedded processors with a methodology to derive system configurations, such as task-processor bindings, which are optimal in terms of safety and execution time. The virtualization properties enable a reshaping of the MPSoC at runtime. Thus, system configurations may be exchanged rapidly in a dynamic fashion. As a main result of this work, embedded multiprocessor systems are introduced, which dynamically adapt to changing operating conditions, possible module defects, and internal state changes. We demonstrate the figures of merit of such reconfigurable MPSoC embedded systems by means of a complex automotive application scenario mapped to an FPGA featuring a virtualizable array of eight soft-core processors.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2068098775",
    "type": "article"
  },
  {
    "title": "Impact of Parallelism and Memory Architecture on FPGA Communication Energy",
    "doi": "https://doi.org/10.1145/2857057",
    "publication_date": "2016-08-22",
    "publication_year": 2016,
    "authors": "Edin Kadrić; David Lakata; André DeHon",
    "corresponding_authors": "",
    "abstract": "The energy in FPGA computations is dominated by data communication energy, either in the form of memory references or data movement on interconnect. In this article, we explore how to use data placement and parallelism to reduce communication energy. We show that parallelism can reduce energy and that the optimal level of parallelism increases with the problem size. We further explore how FPGA memory architecture (memory block size(s), memory banking, and spacing between memory banks) can impact communication energy, and determine how to organize the memory architecture to guarantee that the energy overhead compared to the optimally matched architecture for the design is never more than 60%. We specifically show that an architecture with 32 bit wide, 16Kb internally banked memories placed every 8 columns of 10 4-LUT logic blocks is within 61% of the optimally matched architecture across the VTR 7 benchmark set and a set of parallelism-tunable benchmarks. Without internal banking, the worst-case overhead is 98%, achieved with an architecture with 32 bit wide, 8Kb memories placed every 9 columns, roughly comparable to the memory organization on the Cyclone V (where memories are placed about every 10 columns). Monolithic 32 bit wide, 16Kb memories placed every 10 columns (comparable to 18Kb and 20Kb memories used in Virtex 4 and Stratix V FPGAs) have a 180% worst-case energy overhead. Furthermore, we show practical cases where designs mapped for optimal parallelism use 4.7 × less energy than designs using a single processing element.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2507742534",
    "type": "article"
  },
  {
    "title": "Substream-Centric Maximum Matchings on FPGA",
    "doi": "https://doi.org/10.1145/3377871",
    "publication_date": "2020-04-24",
    "publication_year": 2020,
    "authors": "Maciej Besta; Marc Fischer; Tal Ben‐Nun; Dimitri Stanojevic; Johannes de Fine Licht; Torsten Hoefler",
    "corresponding_authors": "",
    "abstract": "Developing high-performance and energy-efficient algorithms for maximum matchings is becoming increasingly important in social network analysis, computational sciences, scheduling, and others. In this work, we propose the first maximum matching algorithm designed for FPGAs; it is energy-efficient and has provable guarantees on accuracy, performance, and storage utilization. To achieve this, we forego popular graph processing paradigms, such as vertex-centric programming, that often entail large communication costs. Instead, we propose a substream-centric approach, in which the input stream of data is divided into substreams processed independently to enable more parallelism while lowering communication costs. We base our work on the theory of streaming graph algorithms and analyze 14 models and 28 algorithms. We use this analysis to provide theoretical underpinning that matches the physical constraints of FPGA platforms. Our algorithm delivers high performance (more than 4× speedup over tuned parallel CPU variants), low memory, high accuracy, and effective usage of FPGA resources. The substream-centric approach could easily be extended to other algorithms to offer low-power and high-performance graph processing on FPGAs.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W3021319949",
    "type": "article"
  },
  {
    "title": "The Strong Scaling Advantage of FPGAs in HPC for N-body Simulations",
    "doi": "https://doi.org/10.1145/3491235",
    "publication_date": "2021-11-29",
    "publication_year": 2021,
    "authors": "J. Menzel; Christian Plessl; Tobias Kenter",
    "corresponding_authors": "",
    "abstract": "N-body methods are one of the essential algorithmic building blocks of high-performance and parallel computing. Previous research has shown promising performance for implementing n-body simulations with pairwise force calculations on FPGAs. However, to avoid challenges with accumulation and memory access patterns, the presented designs calculate each pair of forces twice, along with both force sums of the involved particles. Also, they require large problem instances with hundreds of thousands of particles to reach their respective peak performance, limiting the applicability for strong scaling scenarios. This work addresses both issues by presenting a novel FPGA design that uses each calculated force twice and overlaps data transfers and computations in a way that allows to reach peak performance even for small problem instances, outperforming previous single precision results even in double precision, and scaling linearly over multiple interconnected FPGAs. For a comparison across architectures, we provide an equally optimized CPU reference, which for large problems actually achieves higher peak performance per device, however, given the strong scaling advantages of the FPGA design, in parallel setups with few thousand particles per device, the FPGA platform achieves highest performance and power efficiency.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W3216064418",
    "type": "article"
  },
  {
    "title": "<i>AIgean</i> : An Open Framework for Deploying Machine Learning on Heterogeneous Clusters",
    "doi": "https://doi.org/10.1145/3482854",
    "publication_date": "2021-12-27",
    "publication_year": 2021,
    "authors": "Naif Tarafdar; Giuseppe Di Guglielmo; Philip Harris; J. Krupa; Vladimir Lončar; Dylan Rankin; Nhan Viet Tran; Z. Wu; Qianfeng Shen; Paul Chow",
    "corresponding_authors": "",
    "abstract": "AIgean , pronounced like the sea, is an open framework to build and deploy machine learning (ML) algorithms on a heterogeneous cluster of devices (CPUs and FPGAs). We leverage two open source projects: Galapagos , for multi-FPGA deployment, and hls4ml , for generating ML kernels synthesizable using Vivado HLS. AIgean provides a full end-to-end multi-FPGA/CPU implementation of a neural network. The user supplies a high-level neural network description, and our tool flow is responsible for the synthesizing of the individual layers, partitioning layers across different nodes, as well as the bridging and routing required for these layers to communicate. If the user is an expert in a particular domain and would like to tinker with the implementation details of the neural network, we define a flexible implementation stack for ML that includes the layers of Algorithms, Cluster Deployment &amp; Communication, and Hardware. This allows the user to modify specific layers of abstraction without having to worry about components outside of their area of expertise, highlighting the modularity of AIgean . We demonstrate the effectiveness of AIgean with two use cases: an autoencoder, and ResNet-50 running across 10 and 12 FPGAs. AIgean leverages the FPGA’s strength in low-latency computing, as our implementations target batch-1 implementations.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W4200489299",
    "type": "article"
  },
  {
    "title": "Dynamic Task Mapping with Congestion Speculation for Reconfigurable Network-on-Chip",
    "doi": "https://doi.org/10.1145/2892633",
    "publication_date": "2016-09-24",
    "publication_year": 2016,
    "authors": "Hung-Lin Chao; Sheng-Ya Tung; Pao‐Ann Hsiung",
    "corresponding_authors": "",
    "abstract": "Network-on-Chip (NoC) has been proposed as a promising communication architecture to replace the dedicated interconnections and shared buses for future embedded system platforms. In such a parallel platform, mapping application tasks to the NoC is a key issue because it affects throughput significantly due to the problem of communication congestion. Increased communication latency, low system performance, and low resource utilization are some side-effects of a bad mapping. Current mapping algorithms either do not consider link utilizations or consider only the current utilizations. Besides, to design an efficient NoC platform, mapping task to computation nodes and scheduling communication should be taken into consideration. In this work, we propose an efficient algorithm for dynamic task mapping with congestion speculation (DTMCS) that not only includes the conventional application mapping, but also further considers future traffic patterns based on the link utilization. The proposed algorithm can reduce overall congestion, instead of only improving the current packet blocking situation. Our experiment results have demonstrated that compared to the state-of-the-art congestion-aware Path Load algorithm, the proposed DTMCS algorithm can reduce up to 40.5% of average communication latency, while the maximal communication latency can be reduced by up to 67.7%.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2525888813",
    "type": "article"
  },
  {
    "title": "Automated Real-Time Analysis of Streaming Big and Dense Data on Reconfigurable Platforms",
    "doi": "https://doi.org/10.1145/2974023",
    "publication_date": "2016-12-19",
    "publication_year": 2016,
    "authors": "Bita Darvish Rouhani; Azalia Mirhoseini; Ebrahim M. Songhori; Farinaz Koushanfar",
    "corresponding_authors": "",
    "abstract": "We propose SSketch, a novel automated framework for efficient analysis of dynamic big data with dense (non-sparse) correlation matrices on reconfigurable platforms. SSketch targets streaming applications where each data sample can be processed only once and storage is severely limited. Our framework adaptively learns from the stream of input data and updates a corresponding ensemble of lower-dimensional data structures, a.k.a., a sketch matrix . A new sketching methodology is introduced that tailors the problem of transforming the big data with dense correlations to an ensemble of lower-dimensional subspaces such that it is suitable for hardware-based acceleration performed by reconfigurable hardware. The new method is scalable, while it significantly reduces costly memory interactions and enhances matrix computation performance by leveraging coarse-grained parallelism existing in the dataset. SSketch provides an automated optimization methodology for creating the most accurate data sketch for a given set of user-defined constraints, including runtime and power as well as platform constraints such as memory. To facilitate automation, SSketch takes advantage of a Hardware/Software (HW/SW) co-design approach: It provides an Application Programming Interface that can be customized for rapid prototyping of an arbitrary matrix-based data analysis algorithm. Proof-of-concept evaluations on a variety of visual datasets with more than 11 million non-zeros demonstrate up to a 200-fold speedup on our hardware-accelerated realization of SSketch compared to a software-based deployment on a general-purpose processor.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2565558006",
    "type": "article"
  },
  {
    "title": "xDNN: Inference for Deep Convolutional Neural Networks",
    "doi": "https://doi.org/10.1145/3473334",
    "publication_date": "2022-01-11",
    "publication_year": 2022,
    "authors": "Paolo D’Alberto; Victor Wu; Aaron N. Ng; Rahul Nimaiyar; Elliott Delaye; Ashish Sirasao",
    "corresponding_authors": "",
    "abstract": "We present xDNN, an end-to-end system for deep-learning inference based on a family of specialized hardware processors synthesized on Field-Programmable Gate Array (FPGAs) and Convolution Neural Networks (CNN). We present a design optimized for low latency, high throughput, and high compute efficiency with no batching. The design is scalable and a parametric function of the number of multiply-accumulate units, on-chip memory hierarchy, and numerical precision. The design can produce a scale-down processor for embedded devices, replicated to produce more cores for larger devices, or resized to optimize efficiency. On Xilinx Virtex Ultrascale+ VU13P FPGA, we achieve 800 MHz that is close to the Digital Signal Processing maximum frequency and above 80% efficiency of on-chip compute resources. On top of our processor family, we present a runtime system enabling the execution of different networks for different input sizes (i.e., from 224× 224 to 2048× 1024). We present a compiler that reads CNNs from native frameworks (i.e., MXNet, Caffe, Keras, and Tensorflow), optimizes them, generates codes, and provides performance estimates. The compiler combines quantization information from the native environment and optimizations to feed the runtime with code as efficient as any hardware expert could write. We present tools partitioning a CNN into subgraphs for the division of work to CPU cores and FPGAs. Notice that the software will not change when or if the FPGA design becomes an ASIC, making our work vertical and not just a proof-of-concept FPGA project. We show experimental results for accuracy, latency, and power for several networks: In summary, we can achieve up to 4 times higher throughput, 3 times better power efficiency than the GPUs, and up to 20 times higher throughput than the latest CPUs. To our knowledge, we provide solutions faster than any previous FPGA-based solutions and comparable to any other top-of-the-shelves solutions.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4205171991",
    "type": "article"
  },
  {
    "title": "BlastFunction: A Full-stack Framework Bringing FPGA Hardware Acceleration to Cloud-native Applications",
    "doi": "https://doi.org/10.1145/3472958",
    "publication_date": "2022-01-11",
    "publication_year": 2022,
    "authors": "Andrea Damiani; Giorgia Fiscaletti; Marco Bacis; Rolando Brondolin; Marco D. Santambrogio",
    "corresponding_authors": "",
    "abstract": "“Cloud-native” is the umbrella adjective describing the standard approach for developing applications that exploit cloud infrastructures’ scalability and elasticity at their best. As the application complexity and user-bases grow, designing for performance becomes a first-class engineering concern. As an answer to these needs, heterogeneous computing platforms gained widespread attention as powerful tools to continue meeting SLAs for compute-intensive cloud-native workloads. We propose BlastFunction, an FPGA-as-a-Service full-stack framework to ease FPGAs’ adoption for cloud-native workloads, integrating with the vast spectrum of fundamental cloud models. At the IaaS level, BlastFunction time-shares FPGA-based accelerators to provide multi-tenant access to accelerated resources without any code rewriting. At the PaaS level, BlastFunction accelerates functionalities leveraging the serverless model and scales functions proactively, depending on the workload’s performance. Further lowering the FPGAs’ adoption barrier, an accelerators’ registry hosts accelerated functions ready to be used within cloud-native applications, bringing the simplicity of a SaaS-like approach to the developers. After an extensive experimental campaign against state-of-the-art cloud scenarios, we show how BlastFunction leads to higher performance metrics (utilization and throughput) against native execution, with minimal latency and overhead differences. Moreover, the scaling scheme we propose outperforms the main serverless autoscaling algorithms in workload performance and scaling operation amount.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4206662528",
    "type": "article"
  },
  {
    "title": "VCSN: Virtual Circuit-Switching Network for Flexible and Simple-to-Operate Communication in HPC FPGA Cluster",
    "doi": "https://doi.org/10.1145/3579848",
    "publication_date": "2023-01-14",
    "publication_year": 2023,
    "authors": "Tomohiro Ueno; Kentaro Sano",
    "corresponding_authors": "",
    "abstract": "FPGA clusters promise to play a critical role in high-performance computing (HPC) systems in the near future due to their flexibility and high power efficiency. The operation of large-scale general-purpose FPGA clusters on which multiple users run diverse applications requires flexible network topology to be divided and reconfigured. This paper proposes Virtual Circuit-Switching Network (VCSN) that provides an arbitrarily reconfigurable network topology and simple-to-operate network system among FPGA nodes. With virtualization, user logic on FPGAs can communicate with each other as if a circuit-switching network was available. This paper demonstrates that VCSN with 100 Gbps Ethernet achieves highly-efficient point-to-point communication among FPGAs due to its unique and efficient communication protocol. We compare VCSN with a direct connection network (DCN) that connects FPGAs directly. We also show a concrete procedure to realize collective communication on an FPGA cluster with VCSN. We demonstrate that the flexible virtual topology provided by VCSN can accelerate collective communication with simple operations. Furthermore, based on experimental results, we model and estimate communication performance by DCN and VCSN in a large FPGA cluster. The result shows that VCSN has the potential to accelerate gather communication up to about 1.97 times more than DCN.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4315977508",
    "type": "article"
  },
  {
    "title": "SASA: A Scalable and Automatic Stencil Acceleration Framework for Optimized Hybrid Spatial and Temporal Parallelism on HBM-based FPGAs",
    "doi": "https://doi.org/10.1145/3572547",
    "publication_date": "2023-01-31",
    "publication_year": 2023,
    "authors": "Xingyu Tian; Zhifan Ye; Alec Lu; Licheng Guo; Yuze Chi; Zhenman Fang",
    "corresponding_authors": "",
    "abstract": "Stencil computation is one of the fundamental computing patterns in many application domains such as scientific computing and image processing. While there are promising studies that accelerate stencils on FPGAs, there lacks an automated acceleration framework to systematically explore both spatial and temporal parallelisms for iterative stencils that could be either computation-bound or memory-bound. In this article, we present SASA, a scalable and automatic stencil acceleration framework on modern HBM-based FPGAs. SASA takes the high-level stencil DSL and FPGA platform as inputs, automatically exploits the best spatial and temporal parallelism configuration based on our accurate analytical model, and generates the optimized FPGA design with the best parallelism configuration in TAPA high-level synthesis C++ as well as its corresponding host code. Compared to state-of-the-art automatic stencil acceleration framework SODA that only exploits temporal parallelism, SASA achieves an average speedup of 3.41× and up to 15.73× speedup on the HBM-based Xilinx Alveo U280 FPGA board for a wide range of stencil kernels.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4318618048",
    "type": "article"
  },
  {
    "title": "ZyPR: End-to-end Build Tool and Runtime Manager for Partial Reconfiguration of FPGA SoCs at the Edge",
    "doi": "https://doi.org/10.1145/3585521",
    "publication_date": "2023-02-27",
    "publication_year": 2023,
    "authors": "Alex R. Bucknall; Suhaib A. Fahmy",
    "corresponding_authors": "",
    "abstract": "Partial reconfiguration (PR) is a key enabler to the design and development of adaptive systems on modern Field Programmable Gate Array (FPGA) Systems-on-Chip (SoCs), allowing hardware to be adapted dynamically at runtime. Vendor-supported PR infrastructure is performance-limited and blocking, drivers entail complex memory management, and software/hardware design requires bespoke knowledge of the underlying hardware. This article presents ZyPR: a complete end-to-end framework that provides high-performance reconfiguration of hardware from within a software abstraction in the Linux userspace, automating the process of building PR applications with support for the Xilinx Zynq and Zynq UltraScale+ architectures, aimed at enabling non-expert application designers to leverage PR for edge applications. We compare ZyPR against traditional vendor tooling for PR management as well as recent open source tools that support PR under Linux. The framework provides a high-performance runtime along with low overhead for its provided abstractions. We introduce improvements to our previous work, increasing the provisioning throughput for PR bitstreams on the Zynq Ultrascale+ by 2× and 5.4× compared to Xilinx’s FPGA Manager.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4322496341",
    "type": "article"
  },
  {
    "title": "An Efficient FPGA-based Depthwise Separable Convolutional Neural Network Accelerator with Hardware Pruning",
    "doi": "https://doi.org/10.1145/3615661",
    "publication_date": "2023-09-13",
    "publication_year": 2023,
    "authors": "Zhengyan Liu; Qiang Liu; Shun Yan; Ray C. C. Cheung",
    "corresponding_authors": "",
    "abstract": "Convolutional neural networks (CNNs) have been widely deployed in computer vision tasks. However, the computation and resource intensive characteristics of CNN bring obstacles to its application on embedded systems. This article proposes an efficient inference accelerator on Field Programmable Gate Array (FPGA) for CNNs with depthwise separable convolutions. To improve the accelerator efficiency, we make four contributions: (1) an efficient convolution engine with multiple strategies for exploiting parallelism and a configurable adder tree are designed to support three types of convolution operations; (2) a dedicated architecture combined with input buffers is designed for the bottleneck network structure to reduce data transmission time; (3) a hardware padding scheme to eliminate invalid padding operations is proposed; and (4) a hardware-assisted pruning method is developed to support online tradeoff between model accuracy and power consumption. Experimental results show that for MobileNetV2 the accelerator achieves 10× and 6× energy efficiency improvement over the CPU and GPU implementation, and 302.3 frames per second and 181.8 GOPS performance that is the best among several existing single-engine accelerators on FPGAs. The proposed hardware-assisted pruning method can effectively reduce 59.7% power consumption at the accuracy loss within 5%.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4386710183",
    "type": "article"
  },
  {
    "title": "XVDPU: A High-Performance CNN Accelerator on the Versal Platform Powered by the AI Engine",
    "doi": "https://doi.org/10.1145/3617836",
    "publication_date": "2023-09-13",
    "publication_year": 2023,
    "authors": "Xijie Jia; Yu Zhang; Guangdong Liu; Xinlin Yang; Tianyu Zhang; Jia Zheng; Dongdong Xu; Zhuohuan Liu; Mengke Liu; Xiaoyang Yan; Hong Wang; Rongzhang Zheng; Wang Li; Dong Li; Satyaprakash Pareek; Jian Weng; Lu Tian; Dongliang Xie; Hong Luo; Yi Shan",
    "corresponding_authors": "",
    "abstract": "Today, convolutional neural networks (CNNs) are widely used in computer vision applications. However, the trends of higher accuracy and higher resolution generate larger networks. The requirements of computation or I/O are the key bottlenecks. In this article, we propose XVDPU: the AI Engine (AIE)-based CNN accelerator on Versal chips to meet heavy computation requirements. To resolve the IO bottleneck, we adopt several techniques to improve data reuse and reduce I/O requirements. An arithmetic logic unit is further proposed that can better balance resource utilization, new feature support, and efficiency of the whole system. We have successfully deployed more than 100 CNN models with our accelerator. Our experimental results show that the 96-AIE-core implementation can achieve 1,653 frames per second (FPS) for ResNet50 on VCK190, which is 9.8× faster than the design on ZCU102 running at 168.5 FPS. The 256-AIE-core implementation can further achieve 4,050 FPS. We propose a tilling strategy to achieve feature-map-stationary for high-definition CNN with the accelerator, achieving 3.8× FPS improvement on the residual channel attention network and 3.1× on super-efficient super-resolution. This accelerator can also solve the 3D convolution task in disparity estimation, achieving end-to-end performance of 10.1 FPS with all the optimizations.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4386710216",
    "type": "article"
  },
  {
    "title": "<scp>Tailor</scp> : Altering Skip Connections for Resource-Efficient Inference",
    "doi": "https://doi.org/10.1145/3624990",
    "publication_date": "2023-09-22",
    "publication_year": 2023,
    "authors": "Olivia Weng; Gabriel Marcano; Vladimir Lončar; Alireza Khodamoradi; G Abarajithan; Nojan Sheybani; Andres Meza; Farinaz Koushanfar; K. Denolf; J. Duarte; Ryan Kastner",
    "corresponding_authors": "",
    "abstract": "Deep neural networks use skip connections to improve training convergence. However, these skip connections are costly in hardware, requiring extra buffers and increasing on- and off-chip memory utilization and bandwidth requirements. In this article, we show that skip connections can be optimized for hardware when tackled with a hardware-software codesign approach. We argue that while a network’s skip connections are needed for the network to learn, they can later be removed or shortened to provide a more hardware-efficient implementation with minimal to no accuracy loss. We introduce Tailor , a codesign tool whose hardware-aware training algorithm gradually removes or shortens a fully trained network’s skip connections to lower the hardware cost. Tailor improves resource utilization by up to 34% for block random access memories (BRAMs), 13% for flip-flops (FFs), and 16% for look-up tables (LUTs) for on-chip, dataflow-style architectures. Tailor increases performance by 30% and reduces memory bandwidth by 45% for a two-dimensional processing element array architecture.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4386958118",
    "type": "article"
  },
  {
    "title": "FDRA: A Framework for a Dynamically Reconfigurable Accelerator Supporting Multi-Level Parallelism",
    "doi": "https://doi.org/10.1145/3614224",
    "publication_date": "2023-10-10",
    "publication_year": 2023,
    "authors": "Yunhui Qiu; Yiqing Mao; Xuchen Gao; Sichao Chen; Jiangnan Li; Wenbo Yin; Lingli Wang",
    "corresponding_authors": "",
    "abstract": "Coarse-grained reconfigurable architectures (CGRAs) have emerged as promising accelerators due to their high flexibility and energy efficiency. However, existing open source works often lack integration of CGRAs with CPU systems and corresponding toolchains. Moreover, there is rare support for the accelerator instruction pipelining to overlap data communication, computation, and configuration across multiple tasks. In this article, we propose FDRA, an open source exploration framework for a heterogeneous system-on-chip (SoC) with a RISC-V processor and a dynamically reconfigurable accelerator (DRA) supporting loop, instruction, and task levels of parallelism. FDRA encompasses parameterized SoC modeling, Verilog generation, source-to-source application code transformation using frontend and DRA compilers, SoC simulation, and FPGA prototyping. FDRA incorporates the extraction of periodic accumulative operators and multi-dimensional linear load/store operators from nested loops. The DRA enables accessing the shared L2 cache with virtual addresses and supports direct memory access with arbitrary start addresses and data lengths. Integrated into the RISC-V Rocket SoC, our DRA achieves a remarkable 55× acceleration for loop kernels and improves energy efficiency by 29×. Compared to state-of-the-art RISC-V vector units, our DRA demonstrates a 2.9× speed improvement and 3.5× greater energy efficiency. In contrast to previous CGRA+RISC-V SoCs, our SoC achieves a minimum speedup of 5.2×.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4387495975",
    "type": "article"
  },
  {
    "title": "VTR 9: Open-Source CAD for Fabric and Beyond FPGA Architecture Exploration",
    "doi": "https://doi.org/10.1145/3734798",
    "publication_date": "2025-05-13",
    "publication_year": 2025,
    "authors": "Mohamed A. Elgammal; Amin Mohaghegh; Soheil Gholami Shahrouz; Fatemehsadat Mahmoudi; Fahrican Koşar; Kimia Talaei; Joshua Fife; Daniel Khadivi; Kevin E. Murray; Andrew Boutros; Kenneth B. Kent; Jeffrey Goeders; Vaughn Betz",
    "corresponding_authors": "",
    "abstract": "This work details the capabilities of a major new release of the Verilog-to-Routing (VTR) open-source FPGA CAD tool flow. Enhancements include generalizations of VTR’s architecture modeling language and optimizers to enable a more diverse set of programmable routing fabrics, FPGAs with embedded hard Networks-on-Chip (NoCs) and three-dimensional FPGA systems that leverage stacked silicon integration. The new Parmys logic synthesis flow improves language coverage and result quality, and the physical implementation flow includes a more efficient placement engine, floorplanning constraints to guide placement, the ability to perform single-stage (flat) routing to improve quality, and parallel routing algorithms to reduce CPU time. This release also includes new architecture captures of recent commercial devices (Xilinx’s 7-series and Altera’s Stratix 10) and new benchmark suites ( Titanium25 and Hermes) to aid FPGA architecture investigation. Verilog language coverage is greatly improved with the new Parmys logic synthesis flow, enabling more designs to be used with VTR. Finally, the placement and routing engines have beeen sped up by 4 \\(\\times\\) and 2.2 \\(\\times\\) vs. VTR 8, respectively, leading to an overall physical implementation flow CPU time reduction of 48% with better result quality on average compared to VTR 8.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410344137",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on RAW 2024",
    "doi": "https://doi.org/10.1145/3742478",
    "publication_date": "2025-06-02",
    "publication_year": 2025,
    "authors": "Zhenman Fang",
    "corresponding_authors": "Zhenman Fang",
    "abstract": "The Reconfigurable Architectures Workshop (RAW) is one of the major meetings for researchers to present ideas, results, and on-going research on both theoretical and practical advances in reconfigurable computing. The 31st RAW (RAW 2024) was held in San Francisco, CA, USA in May 2024, which is associated with the 38th Annual IEEE International Parallel &amp; Distributed Processing Symposium (IPDPS 2024) and is sponsored by the IEEE Computer Society and the Technical Committee on Parallel Processing. This special issue comprises extended versions for a selected set of two of the 10 papers accepted and presented at RAW 2024. These two articles present the potential of reconfigurable computing, especially commercial FPGAs (field-programmable gate arrays), in the emerging field of quantum computing. Quantum computing represents a new computing paradigm that exploits quantum mechanical phenomena and aims to solve problems intractable for a classical computer. While promising, current quantum devices are often limited by the noise impacting qubits and usually require quantum error correction (QEC) mechanisms to identify and correct the errors in real time. Due to its low power, high performance, and high reconfigurability, FPGA technology is a natural fit to accelerate such QEC computations. We summarize the research contributions and highlights of each article below. In the first paper “Rock the QASBA: Quantum Error Correction Acceleration via the Sparse Blossom Algorithm on FPGAs” (the recipient of RAW 2024 best poster award), the authors present QASBA, an FPGA-based hardware accelerator as well as the automation process to accelerate the sparse Blossom algorithm, a state-of-the-art decoding algorithm for QEC. Experimental results demonstrate up to 10.0× and 100.7× improvements in the performance and energy efficiency over the software baseline, respectively, for a toric code with distance d = 3. In the second paper “QUEKUF: An FPGA Union Find Decoder for Quantum Error Correction on the Toric Code”, the authors present QUEKUF, an open-source FPGA-based dataflow accelerator for QEC on the toric code, which is equipped with a centralized controller orchestrating the computation and optimized for correcting quantum errors using the union find algorithm. Experimental results demonstrate up to 9.6× and 98.6× improvements respectively in the performance and energy efficiency over the software baseline, while keeping a high accuracy similar to the theoretical framework. As guest editor, I would like to thank the authors for submitting and presenting high-quality articles as well as the anonymous reviewers for their reviews and insightful suggestions for improving the papers. I would also like to acknowledge support from ACM TRETS, including but not limited to Vaughn Betz (current Editor-in-Chief), Deming Chen (prior Editor-in-Chief), Jeff Goeders (Information Director), and Clarissa Nemeth (Journal Administrator), for their continued hard work and support. We hope that the articles included in this special issue will inspire more research in this new area.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410958633",
    "type": "article"
  },
  {
    "title": "MCoreOPU: An FPGA-based Multi-Core Overlay Processor for Transformer-based Models",
    "doi": "https://doi.org/10.1145/3742437",
    "publication_date": "2025-06-03",
    "publication_year": 2025,
    "authors": "Shaoqiang Lu; Tiandong Zhao; Ting-Jung Lin; Rumin Zhang; C.-L. Wu; Lei He",
    "corresponding_authors": "",
    "abstract": "Transformer-based models have achieved extensive success with increasingly large numbers of parameters and computations, for which many multi-core accelerators have been developed. Nevertheless, they suffer from limited throughput due to either low operating frequency or high communication overhead between cores. This paper proposes an FPGA-based multi-core overlay processor, named MCoreOPU , to optimize intra-core computation and inter-core communication. First, we boost the operating frequency of the processing element(PE) array to double the rest of the processor to improve the intra-core throughput. Second, we develop on-chip synchronization routers to reduce off-chip memory traffic, where only the partial sum and maximum are communicated between cores rather than entire vectors for layer normalization and softmax. Moreover, we pipeline synchronization to reduce synchronization latency and develop a bypass of the interconnect bus to reduce the off-chip memory access latency. Finally, we optimize the multi-core model allocation and scheduling to minimize the inter-core communications and maximize the intra-core computation efficiency. The MCoreOPU is implemented in 8-bit fixed-point precision with four cores and four DDRs on the Xilinx U200 FPGA, where the PE array runs at 600MHz while the rest runs at 300MHz. Experimental results show that the throughput per MAC of MCoreOPU for BERT, ViT, GPT-2, and LLaMA inference is 1.31× -7.18× higher than other FPGA-based accelerators. Compared with the A100 GPU, the throughput per equivalent MAC efficiency is improved by 22.52×-27.12×.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410995768",
    "type": "article"
  },
  {
    "title": "DGMF: a unified dynamic mapping framework for graph neural networks",
    "doi": "https://doi.org/10.1145/3744345",
    "publication_date": "2025-06-12",
    "publication_year": 2025,
    "authors": "Zimeng Fan; Min Peng",
    "corresponding_authors": "",
    "abstract": "Graph Neural Networks (GNNs) have seen considerable advancements across various applications. However, the computationally and storage-intensive nature of GNNs presents unique challenges for hardware design. Existing GNN accelerators frequently grapple with low execution efficiency and workload imbalance. Therefore, this paper introduces DGMF, a unified dynamic mapping approach that optimizes data flow and computation mode based on GNN models and datasets. To support this dynamic mapping approach, we also present a dedicated hardware architecture. This architecture dynamically adjusts resource allocation and parallelism based on models and datasets, thereby ensuring optimal utilization. Additionally, we propose a Design Space Exploration (DSE) algorithm that traverses the defined design space to identify the optimal design solution, effectively managing diverse design constraints and objectives. To validate our approach, we developed an accelerator based on three datasets and five types of GNN models. Experimental results demonstrate that DGMF achieves performance by 10.5x-2548x, and 1.01x-51.7x compared to GPUs and existing accelerators. It also improves resource efficiency, energy efficiency and DRAM access by 0.87x-1.47x, 0.92x-2.18x and 1.01x-47.25x respectively compared to other works.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411238437",
    "type": "article"
  },
  {
    "title": "A Taxonomy of the High-Level Synthesis Ecosystem for Heterogeneous FPGA Systems",
    "doi": "https://doi.org/10.1145/3764664",
    "publication_date": "2025-08-28",
    "publication_year": 2025,
    "authors": "Paul Gottschaldt; Ariel Podlubne; Diana Göhringer",
    "corresponding_authors": "",
    "abstract": "Domain-specific accelerators on Field-Programmable Gate Arrays (FPGAs) have been identified as one potential solution to continue the performance scaling after Moore’s Law ends. However, the design of such accelerators is cumbersome, leading to limited productivity and reduced adoption rates, especially in heterogeneous FPGA systems. Thus, this survey investigated hardware design approaches suited for heterogeneous system developers. High-Level Synthesis (HLS) has been identified as the most fitting category for this objective. Currently, we see the creation of many new HLS-related tools that are hard to classify according to conventional taxonomies. Therefore, this work establishes an explicit definition for HLS approaches based on intended usage. The definition combines the classification of hardware design abstractions with parallel programming models to identify suitable approaches unambiguously. The resulting HLS-related tools are categorized and presented according to a newly developed taxonomy. This taxonomy unifies the current vast ecosystem of HLS-related frameworks, including conventional HLS tools as Backends, embedded Domain Space Exploration (DSE) approaches and system-level integrating tools.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4413788946",
    "type": "article"
  },
  {
    "title": "MAD-HiSpMV: <u>M</u> atrix <u>A</u> daptive <u>D</u> esign with <u>H</u> ybrid Row Distribution for <u>I</u> mbalanced <u>SpMV</u> Acceleration on FPGAs",
    "doi": "https://doi.org/10.1145/3772082",
    "publication_date": "2025-10-21",
    "publication_year": 2025,
    "authors": "Manoj B. Rajashekar; Akhil Raj Baranwal; Xingyu Tian; Zhenman Fang",
    "corresponding_authors": "",
    "abstract": "Sparse matrix-vector multiplication (SpMV) is fundamental in numerous applications such as scientific computing, machine learning (ML), and graph analytics. While recent studies have made tremendous progress in accelerating SpMV on HBM-equipped FPGAs, there are still multiple remaining challenges to accelerate imbalanced SpMV where the distribution of nonzeros in the sparse matrix is imbalanced across different rows. These include (1) imbalanced workload distribution among the parallel processing elements (PEs), (2) long-distance dependency for floating-point accumulation on the output vector, (3) a new bottleneck due to the often-overlooked dense vectors’ off-chip access after the SpMV acceleration, and (4) sub-optimal performance of generic accelerators for various types of sparse matrices. (5) Additionally, ML workloads often consist of both SpMV and general matrix-vector multiplication (GeMV), which suffer from kernel switching inefficiencies. To address those challenges, we propose MAD-HiSpMV to accelerate imbalanced SpMV on HBM-equipped FPGAs with the following novel solutions: (1) a hybrid row distribution network to enable both inter-row and intra-row distribution for better balance, (2) a fully pipelined floating-point accumulation on the output vector using a combination of an adder chain and register-based circular buffer, (3) matrix adaptive design configurations generated by our automation framework via design space exploration (DSE) to maximize performance for the given matrix, and (4) a GeMV overlay built into the same kernel for efficient acceleration of mixed workloads. Experimental results demonstrate that the DSE-picked configuration of MAD-HiSpMV achieves a geomean speedup of 1.3x (up to 2.12x) for the SpMV benchmark matrices and achieves a geomean 1.15x (up to 1.54x) better performance per watt, when compared to state-of-the-art generic designs. For the SpMV benchmark matrices, compared to Intel MKL running on a 24-core Xeon Silver 4214 CPU, MAD-HiSpMV achieves a geomean speedup of 8.80x. Compared to cuSparse running on an Nvidia GTX 1080ti GPU, MAD-HiSpMV achieves a geomean of 2.57x better performance per watt. Additionally, a GeMV overlay built into MAD-HiSpMV achieves a peak throughput of 156.7 GFLOPS, which is 2.64x better than the Vitis L2 GeMV benchmark on U280, and performs 2.7x better for an end-to-end mixed workload, when compared to Intel MKL running on a 24-core Xeon Silver 4214 CPU. MAD-HiSpMV is available at https://github.com/SFU-HiAccel/HiSpMV .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415382450",
    "type": "article"
  },
  {
    "title": "PoCo: Extending Task-Parallel HLS Programming with Shared Multi- <u>P</u> r <u>o</u> ducer Multi- <u>Co</u> nsumer Buffer Support",
    "doi": "https://doi.org/10.1145/3771938",
    "publication_date": "2025-10-21",
    "publication_year": 2025,
    "authors": "Akhil Raj Baranwal; Zhenman Fang",
    "corresponding_authors": "",
    "abstract": "Advancements in High-Level Synthesis (HLS) tools have enabled task-level parallelism on FPGAs. However, prevailing frameworks predominantly employ single-producer-single-consumer (SPSC) models for task communication, thus limiting application scenarios. Analysis of designs becomes non-trivial with an increasing number of tasks in task-parallel systems. Adding features to existing designs often requires re-profiling of several task interfaces, redesign of the overall inter-task connectivity, and describing a new floorplan. This paper proposes PoCo, a novel framework to design scalable multi-producermulti- consumer (MPMC) models on task-parallel systems. PoCo introduces a shared-buffer abstraction that facilitates dynamic and high-bandwidth access to shared on-chip memory resources, incorporates latency-insensitive communication, and implements placement-aware design strategies to mitigate routing congestion. The frontend provides convenient APIs to access the buffer memory, while the backend features an optimized and pipelined datapath. Empirical evaluations demonstrate that PoCo achieves up to 50% reduction in on-chip memory utilization on SPSC models without performance degradation. Additionally, three case studies on distinct real-world applications reveal up to 1.5× frequency improvements and simplified dataflow management in heterogeneous FPGA accelerator designs.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415406059",
    "type": "article"
  },
  {
    "title": "CD-LLM: A Heterogeneous Multi-FPGA System for Batched Decoding of 70B+ LLMs using a Compute-Dedicated Architecture",
    "doi": "https://doi.org/10.1145/3771288",
    "publication_date": "2025-10-22",
    "publication_year": 2025,
    "authors": "Wenheng Ma; Xinhao Yang; Shulin Zeng; Tengxuan Liu; Libo Shen; Hongyi Wang; Shiyao Li; Ke Hong; Zhenhua Zhu; Xuefei Ning; Tsung-Yi Ho; Guohao Dai; Yu Wang",
    "corresponding_authors": "",
    "abstract": "Large language models (LLMs) with 70 billion or more parameters are increasingly being deployed in cloud-based Model-as-a-Service (MaaS) scenarios. To meet the demands of such deployments, MaaS providers require batched LLM decoding systems that can deliver high system throughput (STP) while minimizing total cost of ownership (TCO). However, existing FPGA-based solutions predominantly focus on small-batch or single-batch inference, which fails to meet the computational requirements of batched LLM decoding, resulting in performance gaps of up to 7.96 \\(\\times\\) . Moreover, the low utilization of multi-head attention operations in batched decoding scenarios, e.g., only 3.72% on A100 GPUs, further constrains throughput and inflates TCO. To address these challenges, this paper introduces CD-LLM , a heterogeneous multi-FPGA system designed for efficient batched decoding of LLMs with 70B+ parameters, built upon a C ompute- D edicated architecture. First, we propose a memory-aligned mixed-precision quantization engine to reduce workload. By employing importance-aware quantization, we compress Llama-3.1-70B to an effective 3.45-bit representation and achieve 72.33% bandwidth utilization through memory-aligned data packing. Second, we present a compute-dedicated FPGA architecture that maximizes peak performance by leveraging FPGA-specific resources such as DSPs, BRAMs, and LUTs. The compute-dedicated architecture enables CD-LLM to reach a peak performance of 59.90 TOPS at 600 MHz on U250 FPGA. At last, we introduce a heterogeneous master-slave multi-FPGA system to achieve higher utilization. By pipelining attention and linear layer computations across master and slave FPGAs, CD-LLM achieves utilization rates of 83.08% for linear layers and 68.30% for attention layers. CD-LLM is designed with a heterogeneous multi-FPGA architecture, with an HBM-enabled FPGA as the master accelerator and eight DDR-based FPGAs as slave accelerators. When deployed for inference on the Llama-3.1-70B model with a batch size of 256, CD-LLM achieves a throughput of 2721.79 tokens/s. This represents a 6.11 \\(\\times\\) improvement in STP and a 4.71 \\(\\times\\) reduction in TCO compared to an eight-card RTX3090 GPU system. Furthermore, CD-LLM substantially outperforms the state-of-the-art eight-card FPGA accelerator FlightLLM, delivering 16.15 \\(\\times\\) higher STP and 14.56 \\(\\times\\) lower TCO.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4415435760",
    "type": "article"
  },
  {
    "title": "Suppression of Intrinsic Delay Variation in FPGAs using Multiple Configurations",
    "doi": "https://doi.org/10.1145/1331897.1331899",
    "publication_date": "2008-03-01",
    "publication_year": 2008,
    "authors": "Yohei Matsumoto; Masakazu Hioki; Takashi Kawanami; Hanpei Koike; Toshiyuki Tsutsumi; Tadashi Nakagawa; Toshihiro Sekigawa",
    "corresponding_authors": "",
    "abstract": "A new method for improving the timing yield of field-programmable gate array (FPGA) devices affected by intrinsic within-die variation is proposed. The timing variation is reduced by selecting an appropriate configuration for each chip from a set of independent configurations, the critical paths of which do not share the same circuit resources on the FPGA. In this article, the actual method used to generate independent multiple configurations by simply repeating the routing phase is shown, along with the results of Monte Carlo simulation with 10,000 samples. One simulation result showed that the standard deviations of maximum critical path delays are reduced by 28% and 49% for 10% and 30% V th variations ( σ/ μ ), respectively, with 10 independent configurations. Therefore, the proposed method is especially effective for larger V th variation and is expected to be useful for suppressing the performance variation of FPGAs due to the future increase of parameter variation. Another simulation result showed that the effectiveness of the proposed technique was saturated at the use of 10 or more configurations because of the degradation of the quality of the configurations. Therefore, the use of 10 or fewer configurations is reasonable.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W1978700278",
    "type": "article"
  },
  {
    "title": "FPGA Acceleration of MultiFactor CDO Pricing",
    "doi": "https://doi.org/10.1145/1968502.1968511",
    "publication_date": "2011-05-01",
    "publication_year": 2011,
    "authors": "Alexander Kaganov; Asif Lakhany; Paul Chow",
    "corresponding_authors": "",
    "abstract": "The last decade has seen a significant growth in the financial industry. The recent widespread use of Internet technology has increased the accessibility of the general population to financial data, thereby increasing the average portfolio size. This increase, compounded by the need for accurate real-time results, has led to a rising demand for faster risk simulations. Often, accurately pricing widespread instruments, such as Collateralized Debt Obligations (CDOs), can take excessively long due to their multifactor assets dependency. We present a hardware implementation for a MultiFactor Gaussian Copula (MFGC) CDO pricing algorithm. Through a detailed benchmark exploration we demonstrate how reconfigurable hardware could be used to exploit fine-grain parallelism. Our results show that our implementation mapped onto a Xilinx Virtex 5 (XC5VSX50T) FPGA is over 71 times faster than corresponding software running on a single core 3.4 GHz Intel Xeon processor.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W1983637205",
    "type": "article"
  },
  {
    "title": "Sparse Matrix-Vector Multiplication on a Reconfigurable Supercomputer with Application",
    "doi": "https://doi.org/10.1145/1661438.1661440",
    "publication_date": "2010-01-01",
    "publication_year": 2010,
    "authors": "David Dubois; Andrew DuBois; Thomas M Boorman; Carolyn Connor; Steve Poole",
    "corresponding_authors": "",
    "abstract": "Double precision floating point Sparse Matrix-Vector Multiplication (SMVM) is a critical computational kernel used in iterative solvers for systems of sparse linear equations. The poor data locality exhibited by sparse matrices along with the high memory bandwidth requirements of SMVM result in poor performance on general purpose processors. Field Programmable Gate Arrays (FPGAs) offer a possible alternative with their customizable and application-targeted memory sub-system and processing elements. In this work we investigate two separate implementations of the SMVM on an SRC-6 MAPStation workstation. The first implementation investigates the peak performance capability, while the second implementation balances the amount of instantiated logic with the available sustained bandwidth of the FPGA subsystem. Both implementations yield the same sustained performance with the second producing a much more efficient solution. The metrics of processor and application balance are introduced to help provide some insight into the efficiencies of the FPGA and CPU based solutions explicitly showing the tight coupling of the available bandwidth to peak floating point performance. Due to the FPGAs ability to balance the amount of implemented logic to the available memory bandwidth it can provide a much more efficient solution. Finally, making use of the lessons learned implementing the SMVM, we present a fully implemented non-preconditioned Conjugate Gradient Algorithm utilizing the second SMVM design.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2007606233",
    "type": "article"
  },
  {
    "title": "A Simulation Framework for Rapid Analysis of Reconfigurable Computing Systems",
    "doi": "https://doi.org/10.1145/1862648.1862655",
    "publication_date": "2010-11-01",
    "publication_year": 2010,
    "authors": "Casey Reardon; Eric Grobelny; Alan D. George; Gongyu Wang",
    "corresponding_authors": "",
    "abstract": "Reconfigurable computing (RC) is rapidly emerging as a promising technology for the future of high-performance and embedded computing, enabling systems with the computational density and power of custom-logic hardware and the versatility of software-driven hardware in an optimal mix. Novel methods for rapid virtual prototyping, performance prediction, and evaluation are of critical importance in the engineering of complex reconfigurable systems and applications. These techniques can yield insightful tradeoff analyses while saving valuable time and resources for researchers and engineers alike. The research described herein provides a methodology for mapping arbitrary applications to targeted reconfigurable platforms in a simulation environment called RCSE. By splitting the process into two domains, the application and simulation domains, characterization of each element can occur independently and in parallel, leading to fast and accurate performance prediction results for large and complex systems. This article presents the design of a novel framework for system-level simulative performance prediction of RC systems and applications. The article also presents a set of case studies analyzing two applications, Hyperspectral Imaging (HSI) and Molecular Dynamics (MD), across three disparate RC platforms within the simulation framework. The validation results using each of these applications and systems show that our framework can quickly obtain performance prediction results with reasonable accuracy on a variety of platforms. Finally, a set of simulative case studies are presented to illustrate the various capabilities of the framework to quickly obtain a wide range of performance prediction results and power consumption estimates.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2049644798",
    "type": "article"
  },
  {
    "title": "Hardware-Accelerated RNA Secondary-Structure Alignment",
    "doi": "https://doi.org/10.1145/1839480.1839484",
    "publication_date": "2010-09-01",
    "publication_year": 2010,
    "authors": "James Moscola; Ron K. Cytron; Young H. Cho",
    "corresponding_authors": "",
    "abstract": "The search for homologous RNA molecules---sequences of RNA that might behave simiarly due to similarity in their physical (secondary) structure---is currently a computationally intensive task. Moreover, RNA sequences are populating genome databases at a pace unmatched by gains in standard processor performance. While software tools such as Infernal can efficiently find homologies among RNA families and genome databases of modest size, the continuous advent of new RNA families and the explosive growth in volume of RNA sequences necessitate a faster approach. This work introduces two different architectures for accelerating the task of finding homologous RNA molecules in a genome database. The first architecture takes advantage of the tree-like configuration of the covariance models used to represent the consensus secondary structure of an RNA family and converts it directly into a highly-pipelined processing engine. Results for this architecture show a 24× speedup over Infernal when processing a small RNA model. It is estimated that the architecture could potentially offer several thousands of times speedup over Infernal on larger models, provided that there are sufficient hardware resources available. The second architecture is introduced to address the steep resource requirements of the first architecture. It utilizes a uniform array of processing elements and schedules all of the computations required to scan for an RNA homolog onto those processing elements. The estimated speedup for this architecture over the Infernal software package ranges from just under 20× to over 2,350×.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2054926865",
    "type": "article"
  },
  {
    "title": "Reducing Memory Constraints in Modulo Scheduling Synthesis for FPGAs",
    "doi": "https://doi.org/10.1145/1839480.1839485",
    "publication_date": "2010-09-01",
    "publication_year": 2010,
    "authors": "Yosi Ben-Asher; Danny Meisler; Nadav Rotem",
    "corresponding_authors": "",
    "abstract": "In High-Level Synthesis (HLS), extracting parallelism in order to create small and fast circuits is the main advantage of HLS over software execution. Modulo Scheduling (MS) is a technique in which a loop is parallelized by overlapping different parts of successive iterations. This ability to extract parallelism makes MS an attractive synthesis technique for loop acceleration. In this work we consider two problems involved in the use of MS which are central when targeting FPGAs. Current MS scheduling techniques sacrifice execution times in order to meet resource and delay constraints. Let “ideal” execution times be the ones that could have been obtained by MS had we ignored resource and delay constraints. Here we pose the opposite problem, which is more suitable for HLS, namely, how to reduce resource constraints without sacrificing the ideal execution time. We focus on reducing the number of memory ports used by the MS synthesis, which we believe is a crucial resource for HLS. In addition to reducing the number of memory ports we consider the need to develop MS techniques that are fast enough to allow interactive synthesis times and repeated applications of the MS to explore different possibilities of synthesizing the circuits. Current solutions for MS synthesis that can handle memory constraints are too slow to support interactive synthesis. We formalize the problem of reducing the number of parallel memory references in every row of the kernel by a novel combinatorial setting. The proposed technique is based on inserting dummy operations in the kernel and by doing so, performing modulo-shift operations such that the maximal number of parallel memory references in a row is reduced. Experimental results suggest improved execution times for the synthesized circuit. The synthesis takes only a few seconds even for large-size loops.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2075198393",
    "type": "article"
  },
  {
    "title": "(FPL 2015) Scavenger",
    "doi": "https://doi.org/10.1145/3009971",
    "publication_date": "2017-03-22",
    "publication_year": 2017,
    "authors": "Hsin-Jung Yang; Kermin Fleming; Felix Winterstein; Michael Adler; Joel Emer",
    "corresponding_authors": "",
    "abstract": "High-level abstractions separate algorithm design from platform implementation, allowing programmers to focus on algorithms while building complex systems. This separation also provides system programmers and compilers an opportunity to optimize platform services on an application-by-application basis. In field-programmable gate arrays (FPGAs), platform-level malleability extends to the memory system: Unlike general-purpose processors, in which memory hardware is fixed at design time, the capacity, associativity, and topology of FPGA memory systems may all be tuned to improve application performance. Since application kernels may only explicitly use few memory resources, substantial memory capacity may be available to the platform for use on behalf of the user program. In this work, we present Scavenger, which utilizes spare resources to construct program-optimized memories, and we also perform an initial exploration of methods for automating the construction of these application-specific memory hierarchies. Although exploiting spare resources can be beneficial, naïvely consuming all memory resources may cause frequency degradation. To relieve timing pressure in large block RAM (BRAM) structures, we provide microarchitectural techniques to trade memory latency for design frequency. We demonstrate, by examining a set of benchmarks, that our scalable cache microarchitecture achieves performance gains of 7% to 74% (with a 26% geometric mean on average) over the baseline cache microarchitecture when scaling the size of first-level caches to the maximum.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2601686331",
    "type": "article"
  },
  {
    "title": "GraVF-M",
    "doi": "https://doi.org/10.1145/3357596",
    "publication_date": "2019-11-21",
    "publication_year": 2019,
    "authors": "Nina Engelhardt; Hayden Kwok‐Hay So",
    "corresponding_authors": "",
    "abstract": "Due to the irregular nature of connections in most graph datasets, partitioning graph analysis algorithms across multiple computational nodes that do not share a common memory inevitably leads to large amounts of interconnect traffic. Previous research has shown that FPGAs can outcompete software-based graph processing in shared memory contexts, but it remains an open question if this advantage can be maintained in distributed systems. In this work, we present GraVF-M, a framework designed to ease the implementation of FPGA-based graph processing accelerators for multi-FPGA platforms with distributed memory. Based on a lightweight description of the algorithm kernel, the framework automatically generates optimized RTL code for the whole multi-FPGA design. We exploit an aspect of the programming model to present a familiar message-passing paradigm to the user, while under the hood implementing a more efficient architecture that can reduce the necessary inter-FPGA network traffic by a factor equal to the average degree of the input graph. A performance model based on a theoretical analysis of the factors influencing performance serves to evaluate the efficiency of our implementation. With a throughput of up to 5.8GTEPS (billions of traversed edges per second) on a 4-FPGA system, the designs generated by GraVF-M compare favorably to state-of-the-art frameworks from the literature and reach 94% of the projected performance limit of the system.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2980681317",
    "type": "article"
  },
  {
    "title": "ReShape",
    "doi": "https://doi.org/10.1145/2457443.2457448",
    "publication_date": "2013-05-01",
    "publication_year": 2013,
    "authors": "Christopher E. Neely; Gordon Brebner; Weijia Shang",
    "corresponding_authors": "",
    "abstract": "The latest FPGA devices provide the headroom to implement large-scale and complex systems. A key requirement is the integration of modules from diverse sources to promote modular design and reuse. A contrary factor is that using dynamic partial reconfiguration typically requires low-level planning of the system implementation. In this article, we introduce ReShape: a high-level approach for designing reconfigurable systems by interconnecting modules, which gives a “plug and play” look and feel, is supported by tools that carry out implementation functions, and is carried through to support system reconfiguration during operation. The emphasis is on the inter-module connections and abstracting the communication patterns that are typical between modules: for example, the streaming of data, or the reading and writing of data to and from memory modules. The details of wiring and signaling are hidden from view, via metadata associated with individual modules. This setting allows system reconfiguration at the module level, both by supporting type checking of replacement modules and by managing the overall system implementation, via metadata associated with its FPGA floorplan. The methodology and tools have been implemented in a prototype targeted to a domain-specific setting---high-speed networking---and have been validated on real telecommunications design projects.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W1966195532",
    "type": "article"
  },
  {
    "title": "Virtual networks -- distributed communication resource management",
    "doi": "https://doi.org/10.1145/2492186",
    "publication_date": "2013-07-01",
    "publication_year": 2013,
    "authors": "Jan Heißwolf; Aurang Zaib; Andreas Weichslgartner; Ralf König; Thomas Wild; Jürgen Teich; Andreas Herkersdorf; Jürgen Becker",
    "corresponding_authors": "",
    "abstract": "Networks-on-Chip (NoC) enable scalability for future manycore architectures, facilitating parallel communication between multiple cores. Applications running in parallel on a NoC-based architecture can affect each other due to overlapping communication. Quality-of-Service (QoS) must be supported by the communication infrastructure to execute communication-, real-time- and safety-critical applications on such an architecture. Different strategies have been proposed to provide QoS for point-to-point connections. These strategies allow each node to set up a limited number of connections to other nodes. In this work Virtual Networks (VN) are proposed to enable QoS for regions of a NoC-based architecture. Virtual Networks overcome the limitation of point-to-point connections. A VN behaves like an exclusive physical network. Virtual Networks can be defined and configured during runtime. The size of the VN region and the assigned bandwidth can be adjusted depending on the application requirements. Virtual Networks enable the decoupling of local from global communication. Therefore, the communication of the application mapped into the region is assigned to a Virtual Network established in that specific region. This concept targets packet-switched networks with virtual channels and is realized by an intelligent hardware unit that manages the virtual channel reservation process at system runtime. Virtual Networks can be established and administrated independent of each other, enabling distributed communication resource management. The proposed concept is implemented as a cycle-accurate SystemC simulation model. The simulation results of executing communicating graphs obtained from real application highlight the usefulness of Virtual Networks by showing improved throughput and reduced delay in the respective scenarios. A hardware implementation demonstrates a low impact on area utilization and power consumption.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W1998911494",
    "type": "article"
  },
  {
    "title": "Platform-aware bottleneck detection for reconfigurable computing applications",
    "doi": "https://doi.org/10.1145/2000832.2000842",
    "publication_date": "2011-08-01",
    "publication_year": 2011,
    "authors": "Seth Koehler; Greg Stitt; Alan D. George",
    "corresponding_authors": "",
    "abstract": "Reconfigurable Computing (RC) has the potential to provide substantial performance benefits and yet simultaneously consume less power than traditional microprocessors or GPUs. While experimental performance analysis of RC applications has previously been shown crucial for achieving this potential, existing methods still require application designers to manually locate bottlenecks and determine appropriate optimizations, typically requiring significant designer expertise and effort. Worse, the diversity of platforms employed by RC applications further complicates the process of detecting bottlenecks and formulating optimizations. To address these shortcomings, we first discuss our platform-template system, which enables a performance analysis tool to perform more accurate bottleneck detection and achieve a higher degree of portability across diverse FPGA systems. We then provide details for our implementation of these concepts and techniques in the Reconfigurable Computing Application Performance (ReCAP) tool. Next, we present a taxonomy of common RC bottlenecks, providing associated detection and optimization strategies for each bottleneck, which we use to populate ReCAP's knowledge base for bottleneck detection. Finally, we demonstrate the utility of our approach via two application case studies across a total of three platforms.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2059536715",
    "type": "article"
  },
  {
    "title": "Fast and Cycle-Accurate Emulation of Large-Scale Networks-on-Chip Using a Single FPGA",
    "doi": "https://doi.org/10.1145/3151758",
    "publication_date": "2017-12-13",
    "publication_year": 2017,
    "authors": "Thiem Van Chu; Shimpei Sato; Kenji Kise",
    "corresponding_authors": "",
    "abstract": "Modeling and simulation/emulation play a major role in research and development of novel Networks-on-Chip (NoCs). However, conventional software simulators are so slow that studying NoCs for emerging many-core systems with hundreds to thousands of cores is challenging. State-of-the-art FPGA-based NoC emulators have shown great potential in speeding up the NoC simulation, but they cannot emulate large-scale NoCs due to the FPGA capacity constraints. Moreover, emulating large-scale NoCs under synthetic workloads on FPGAs typically requires a large amount of memory and thus involves the use of off-chip memory, which makes the overall design much more complicated and may substantially degrade the emulation speed. This article presents methods for fast and cycle-accurate emulation of NoCs with up to thousands of nodes using a single FPGA. We first describe how to emulate a NoC under a synthetic workload using only FPGA on-chip memory (BRAMs). We next present a novel use of time-division multiplexing where BRAMs are effectively used for emulating a network using a small number of nodes, thereby overcoming the FPGA capacity constraints. We propose methods for emulating both direct and indirect networks, focusing on the commonly used meshes and fat-trees ( k -ary n -trees). This is different from prior work that considers only direct networks. Using the proposed methods, we build a NoC emulator, called FNoC, and demonstrate the emulation of some mesh-based and fat-tree-based NoCs with canonical router architectures. Our evaluation results show that (1) the size of the largest NoC that can be emulated depends on only the FPGA on-chip memory capacity; (2) a mesh-based NoC with 16,384 nodes (128×128 NoC) and a fat-tree-based NoC with 6,144 switch nodes and 4,096 terminal nodes (4-ary 6-tree NoC) can be emulated using a single Virtex-7 FPGA; and (3) when emulating these two NoCs, we achieve, respectively, 5,047× and 232× speedups over BookSim, one of the most widely used software-based NoC simulators, while maintaining the same level of accuracy.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2771713864",
    "type": "article"
  },
  {
    "title": "A Deep Learning Framework to Predict Routability for FPGA Circuit Placement",
    "doi": "https://doi.org/10.1145/3465373",
    "publication_date": "2021-08-12",
    "publication_year": 2021,
    "authors": "Abeer Al-Hyari; Hannah Szentimrey; Ahmed Shamli; Timothy J. Martin; Gary Gréwal; Shawki Areibi",
    "corresponding_authors": "",
    "abstract": "The ability to accurately and efficiently estimate the routability of a circuit based on its placement is one of the most challenging and difficult tasks in the Field Programmable Gate Array (FPGA) flow. In this article, we present a novel, deep learning framework based on a Convolutional Neural Network (CNN) model for predicting the routability of a placement. Since the performance of the CNN model is strongly dependent on the hyper-parameters selected for the model, we perform an exhaustive parameter tuning that significantly improves the model’s performance and we also avoid overfitting the model. We also incorporate the deep learning model into a state-of-the-art placement tool and show how the model can be used to (1) avoid costly, but futile, place-and-route iterations, and (2) improve the placer’s ability to produce routable placements for hard-to-route circuits using feedback based on routability estimates generated by the proposed model. The model is trained and evaluated using over 26K placement images derived from 372 benchmarks supplied by Xilinx Inc. We also explore several opportunities to further improve the reliability of the predictions made by the proposed DLRoute technique by splitting the model into two separate deep learning models for (a) global and (b) detailed placement during the optimization process. Experimental results show that the proposed framework achieves a routability prediction accuracy of 97% while exhibiting runtimes of only a few milliseconds.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3193692689",
    "type": "article"
  },
  {
    "title": "CGRA-EAM—Rapid Energy and Area Estimation for Coarse-grained Reconfigurable Architectures",
    "doi": "https://doi.org/10.1145/3468874",
    "publication_date": "2021-09-13",
    "publication_year": 2021,
    "authors": "Mark Wijtvliet; Henk Corporaal; Akash Kumar",
    "corresponding_authors": "",
    "abstract": "Reconfigurable architectures are quickly gaining in popularity due to their flexibility and ability to provide high energy efficiency. However, reconfigurable systems allow for a huge design space. Iterative design space exploration (DSE) is often required to achieve good Pareto points with respect to some combination of performance, area, and/or energy. DSE tools depend on information about hardware characteristics in these aspects. These characteristics can be obtained from hardware synthesis and net-list simulation, but this is very time-consuming. Therefore, architecture models are common. This work introduces CGRA-EAM (Coarse-Grained Reconfigurable Architecture - Energy &amp; Area Model), a model for energy and area estimation framework for coarse-grained reconfigurable architectures. The model is evaluated for the Blocks CGRA. The results demonstrate that the mean absolute percentage error is 15.5% and 2.1% for energy and area, respectively, while the model achieves a speedup of close to three orders of magnitude compared to synthesis.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W3201108873",
    "type": "article"
  },
  {
    "title": "Fast Design Exploration for Performance, Power and Accuracy Tradeoffs in FPGA-Based Accelerators",
    "doi": "https://doi.org/10.1145/2567661",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "Onur Can Ulusel; Kumud Nepal; R. Iris Bahar; Sherief Reda",
    "corresponding_authors": "",
    "abstract": "The ease-of-use and reconfigurability of FPGAs makes them an attractive platform for accelerating algorithms. However, accelerating becomes a challenging task as the large number of possible design parameters lead to different accelerator variants. In this article, we propose techniques for fast design exploration and multi-objective optimization to quickly identify both algorithmic and hardware parameters that optimize these accelerators. This information is used to run regression analysis and train mathematical models within a nonlinear optimization framework to identify the optimal algorithm and design parameters under various objectives and constraints. To automate and improve the model generation process, we propose the use of L 1 -regularized least squares regression techniques.We implement two real-time image processing accelerators as test cases: one for image deblurring and one for block matching. For these designs, we demonstrate that by sampling only a small fraction of the design space (0.42% and 1.1%), our modeling techniques are accurate within 2%--4% for area and throughput, 8%--9% for power, and 5%--6% for arithmetic accuracy. We show speedups of 340× and 90× in time for the test cases compared to brute-force enumeration. We also identify the optimal set of parameters for a number of scenarios (e.g., minimizing power under arithmetic inaccuracy bounds).",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2095589290",
    "type": "article"
  },
  {
    "title": "Efficient Fault-Tolerant Topology Reconfiguration Using a Maximum Flow Algorithm",
    "doi": "https://doi.org/10.1145/2700417",
    "publication_date": "2015-05-19",
    "publication_year": 2015,
    "authors": "Yu Ren; Leibo Liu; Shouyi Yin; Jie Han; Shaojun Wei",
    "corresponding_authors": "",
    "abstract": "With an increasing number of processing elements (PEs) integrated on a single chip, fault-tolerant techniques are critical to ensure the reliability of such complex systems. In current reconfigurable architectures, redundant PEs are utilized for fault tolerance. In the presence of faulty PEs, the physical topologies of various chips may be different, so the concept of virtual topology from network embedding problem has been used to alleviate the burden for the operating systems. With limited hardware resources, how to reconfigure a system into the most effective virtual topology such that the maximum repair rate can be reached presents a significant challenge. In this article, a new approach using a maximum flow (MF) algorithm is proposed for an efficient topology reconfiguration in reconfigurable architectures. In this approach, topology reconfiguration is converted into a network flow problem by constructing a directed graph; the solution is then found by using the MF algorithm. This approach optimizes the use of spare PEs with minimal impacts on area, throughput, and delay, and thus it significantly improves the repair rate of faulty PEs. In addition, it achieves a polynomial reconfiguration time. Experimental results show that compared to previous methods, the MF approach increases the probability to repair faulty PEs by up to 50% using the same redundant resources. Compared to a fault-free system, the throughput only decreases by less than 2.5% and latency increases by less than 4%. To consider various types of PEs in a practical application, a cost factor is introduced into the MF algorithm. An enhanced approach using a minimum-cost MF algorithm is further shown to be efficient in the fault-tolerant reconfiguration of heterogeneous reconfigurable architectures.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2197062872",
    "type": "article"
  },
  {
    "title": "RAW 2014",
    "doi": "https://doi.org/10.1145/2807699",
    "publication_date": "2015-12-04",
    "publication_year": 2015,
    "authors": "Michael Raitza; Markus Vogt; J. Hochberger; Thilo Pionteck",
    "corresponding_authors": "",
    "abstract": "Random numbers are important ingredients in a number of applications. Especially in a security context, they must be well distributed and unpredictable. We investigate the practical use of random number generators (RNGs) that are built from digital elements found in FPGAs. For this, we implement different types of ring oscillators (ROs) and memory collision-based circuits on FPGAs from major vendors. Implementing RNGs on the same device as the rest of the system benefits an overall reduction of vulnerability to attacks and wire tapping. Nevertheless, we investigate different attacks by tampering with power supply, chip temperature, and by exposition to strong magnetic fields and X-radiation. We also consider their usability as massively deployed components, whose functionality cannot be tested individually anymore, by conducting a technology invariance experiment. Our experiments show that BlockRAM-based RNGs cannot be considered as a suitable entropy source. We further show that RO-based RNGs work reliably under a wide range of operating conditions. While magnetic fields and X-rays did not induce any notable change, voltage and temperature variations caused an increase in propagation delays within the circuits. We show how reliable RNGs can be constructed and deployed on FPGAs.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2293871667",
    "type": "article"
  },
  {
    "title": "ScalaBFS2: A High-performance BFS Accelerator on an HBM-enhanced FPGA Chip",
    "doi": "https://doi.org/10.1145/3650037",
    "publication_date": "2024-02-29",
    "publication_year": 2024,
    "authors": "Kexin Li; Shaoxian Xu; Zhiyuan Shao; Ran Zheng; Xiaofei Liao; Hai Jin",
    "corresponding_authors": "",
    "abstract": "The introduction of High Bandwidth Memory (HBM) to the FPGA chip makes it possible for an FPGA-based accelerator to leverage the huge memory bandwidth of HBM to improve its performance when implementing a specific algorithm, which is especially true for the Breadth-First Search (BFS) algorithm that demands a high bandwidth for accessing the graph data stored in memory. Different from traditional FPGA-DRAM platforms where memory bandwidth is the precious resource due to the limited DRAM channels, FPGA chips equipped with HBM have much higher memory bandwidths provided by the large quantities of HBM channels, but still a limited amount of logic (LUT, FF, and BRAM/URAM) resources. Therefore, the key to design a high-performance BFS accelerator on an HBM-enhanced FPGA chip is to efficiently use the logic resources to build as many as possible Processing Elements (PEs) and configure them flexibly to obtain as high as possible effective memory bandwidth that is useful to the algorithm from the HBM, rather than partially emphasizing the absolute memory bandwidth. To exploit as high as possible effective bandwidth from the HBM, ScalaBFS2 conducts BFS in graphs in a vertex-centric manner and proposes designs, including the independent module (HBM Reader) for memory accessing, multi-layer crossbar, and PEs that implement hybrid mode (i.e., capable of working in both push and pull modes) algorithm processing, to utilize the FPGA logic resources efficiently. Consequently, ScalaBFS2 is able to build up to 128 PEs on the XCU280 FPGA chip (produced with the 16 nm process and configured with two HBM2 stacks) of a Xilinx Alveo U280 board and achieves performance of 56.92 Giga Traversed Edges Per Second (GTEPS) by fully using its 32 HBM memory channels. Compared with the state-of-the-art graph processing system (i.e., ReGraph) built on top of the same board, ScalaBFS2 achieves 2.52x~4.40x performance speedups. Moreover, when compared with Gunrock running on an Nvidia A100 GPU that is produced with the 7 nm process and configured with five HBM2e stacks, ScalaBFS2 achieves 1.34x~2.40x speedups on absolute performance, and 7.35x~13.18x speedups on power efficiency.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4392295490",
    "type": "article"
  },
  {
    "title": "PTME: A Regular Expression Matching Engine Based on Speculation and Enumerative Computation on FPGA",
    "doi": "https://doi.org/10.1145/3655626",
    "publication_date": "2024-04-01",
    "publication_year": 2024,
    "authors": "Mingqian Sun; Guangwei Xie; Fan Zhang; Wei Guo; Xitian Fan; Tianyang Li; Li Chen; Jiayu Du",
    "corresponding_authors": "",
    "abstract": "Fast regular expression matching is an essential task for deep packet inspection. In previous works, the regular expression matching engine on FPGA struggled to achieve an ideal balance between resource consumption and throughput. Speculation and enumerative computation exploits the statistical properties of deterministic finite automata, allowing for more efficient pattern matching. Existing related designs mostly revolve around vector instructions and multiple processors/cores or SIMD instruction sets, with a lack of implementation on FPGA platforms. We design a parallelized two-character matching engine on FPGA for efficiently fast filtering off fields with no pattern features. We transform the state transitions with sequential dependencies to the existing problem of elements in one set, enabling the proposed design to achieve high throughput with low resource consumption and support dynamic updates. Results show that compared with the traditional DFA matching, with a maximum resource consumption of 25% for on-chip FFs (74323/1045440) and LUTs (123902/522720), there is an improvement in throughput of 8.08-229.96 × speedup and 87.61-99.56% speed-up(percentage improvement) for normal traffic, and 11.73-39.59 × speedup and 91.47-97.47% speed-up(percentage improvement) for traffic with high-frequency match hits. Compared with the state-of-the-art similar implementation, our circuit on a single FPGA chip is superior to existing multi-core designs.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4393378871",
    "type": "article"
  },
  {
    "title": "Turn on, Tune in, and Listen up: Maximizing Side-Channel Recovery in Cross-Platform Time-to-Digital Converters",
    "doi": "https://doi.org/10.1145/3666092",
    "publication_date": "2024-06-07",
    "publication_year": 2024,
    "authors": "Colin Drewes; Tyler Sheaves; Olivia Weng; Keegan Ryan; Bill Hunter; Christopher McCarty; Ryan Kastner; Dustin Richmond",
    "corresponding_authors": "",
    "abstract": "Voltage fluctuation sensors measure minute changes in an FPGA power distribution network, allowing attackers to extract information from concurrently executing computations. Previous voltage fluctuation sensors make assumptions about the co-tenant computation and require the attacker have a priori access or system knowledge to tune the sensor parameters statically. Additionally, prior voltage fluctuation sensors make use of proprietary vendor intellectual property and do not provide guidance on sensor migration to other vendors. We present the open-source design of the Tunable Dual-Polarity Time-to-Digital Converter, which introduces three dynamically tunable parameters that optimize signal measurement, including the transition polarity, sample window, frequency, and phase. We show that a properly tuned sensor improves co-tenant classification accuracy by 2.5 \\(\\times\\) over prior work and increases the ability to identify the co-tenant computation and its microarchitectural implementation. Across 13 varying applications, our techniques yield an 80% classification accuracy that generalizes beyond a single board. Our sensor improves the ability of a correlation power analysis attack to rank correct subkey values by 2 \\(\\times\\) . As an extension to our prior work, we show that the voltage fluctuation sensor is portable to multiple FPGA vendors, and we demonstrate implementations on both Xilinx and Intel FPGA systems.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4399424832",
    "type": "article"
  },
  {
    "title": "Codesign of reactor-oriented hardware and software for cyber-physical systems",
    "doi": "https://doi.org/10.1145/3672083",
    "publication_date": "2024-06-12",
    "publication_year": 2024,
    "authors": "Erling Jellum; Martin Schoeberl; Edward A. Lee; Milica Orlandić",
    "corresponding_authors": "",
    "abstract": "Modern cyber-physical systems often make use of heterogeneous systems-on-chip with reconfigurable logic to provide adequate computing power and flexible I/O. However, modeling, verifying, and implementing the computations spanning CPUs and reconfigurable logic are still challenging. The hardware and software components are often designed by different teams and at different levels of abstraction, making it hard to reason about the resulting computation. We propose to lift both hardware and software design to the same level of abstraction by using the Lingua Franca coordination language. Lingua Franca is based on a sparse synchronous model that allows modeling concurrency and timing while keeping a sequential model for the actual computation. We define hardware reactors as a subset of the reactor model of computation underlying Lingua Franca. We also present and evaluate reactor-chisel, a hardware runtime implementing the semantics of hardware reactors, and an extension to the Lingua Franca compiler enabling reactor-oriented hardware–software codesign.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4399557549",
    "type": "article"
  },
  {
    "title": "Hardware Acceleration for High-Volume Operations of CRYSTALS-Kyber and CRYSTALS-Dilithium",
    "doi": "https://doi.org/10.1145/3675172",
    "publication_date": "2024-07-02",
    "publication_year": 2024,
    "authors": "Xavier Carril; Charalampos Kardaris; Jordi Ribes-González; Oriol Farràs; Carles Hernàndez; Vatistas Kostalabros; Joel Ulises González-Jiménez; Miquel Moretó",
    "corresponding_authors": "",
    "abstract": "Many high-demand digital services need to perform several cryptographic operations, such as key exchange or security credentialing, in a concise amount of time. In turn, the security of some of these cryptographic schemes is threatened by advances in quantum computing, as quantum computer could break their security in the near future. Post-quantum cryptography (PQC) is an emerging field that studies cryptographic algorithms that resist such attacks. The National Institute of Standards and Technology (NIST) has selected the CRYSTALS-Kyber Key Encapsulation Mechanism and the CRYSTALS-Dilithium Digital Signature algorithm as primary PQC standards. In this article, we present field-programmable gate array (FPGA)-based hardware accelerators for high-volume operations of both schemes. We apply high-level synthesis (HLS) for hardware optimization, leveraging a batch processing approach to maximize the memory throughput and applying custom HLS logic to specific algorithmic components. Using reconfigurable FPGAs, we show that our hardware accelerators achieve speedups between 3 \\(\\times\\) and 9 \\(\\times\\) over software baseline implementations, even over ones leveraging CPU vector architectures. Furthermore, the methods used in this study can also be extended to the new CRYSTALS-based NIST FIPS drafts, ML-KEM and ML-DSA, with similar acceleration results.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4400227539",
    "type": "article"
  },
  {
    "title": "FPGA-Based Sparse Matrix Multiplication Accelerators: From State-of-the-Art to Future Opportunities",
    "doi": "https://doi.org/10.1145/3687480",
    "publication_date": "2024-08-28",
    "publication_year": 2024,
    "authors": "Yajing Liu; Ruiqi Chen; Shuyang Li; Jing Yang; Shun Li; Bruno da Silva",
    "corresponding_authors": "",
    "abstract": "Sparse matrix multiplication (SpMM) plays a critical role in high-performance computing applications, such as deep learning, image processing, and physical simulation. Field-Programmable Gate Arrays (FPGAs), with their configurable hardware resources, can be tailored to accelerate SpMMs. There has been considerable research on deploying sparse matrix multipliers across various FPGA platforms. However, the FPGA-based design of sparse matrix multipliers still presents numerous challenges. Therefore, it is necessary to summarize and organize the current work to provide a reference for further research. This article first introduces the computational method of SpMM and categorizes the different challenges of FPGA deployment. Following this, we introduce and analyze a variety of state-of-the-art FPGA-based accelerators tailored for SpMMs. In addition, a comparative analysis of these accelerators is performed, examining metrics including compression rate, throughput, and resource utilization. Finally, we propose potential research directions and challenges for further study of FPGA-based SpMM accelerators.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4401945586",
    "type": "article"
  },
  {
    "title": "A Survey on Architectures, Hardware Acceleration and Challenges for In-Network Computing",
    "doi": "https://doi.org/10.1145/3699514",
    "publication_date": "2024-10-10",
    "publication_year": 2024,
    "authors": "Matthias Nickel; Diana Göhringer",
    "corresponding_authors": "",
    "abstract": "By moving data and computation away from the end user to more powerful servers in the cloud or to cloudlets at the edge, end user devices only need to compute locally for small amounts of data and when low latency is required. However, with the advent of 6G and Internet-of-Everything, the demand for more powerful networks continues to grow. The introduction of Software-Defined Networking and Network Function Virtualization has allowed us to rethink networks and use them for more than just routing data to servers. In addition, the use of more powerful network devices is bringing new life to the concept of active networks in the form of in-network computing. In-Network Computing provides the ability to move applications into the network and process data on programmable network devices as they are transmitted. In this work, we provide an overview of in-network computing and its enabling technologies. We take a look at the programmability and different hardware architectures for SmartNICs and switches, focusing primarily on accelerators such as FPGAs. We discuss the state of the art and challenges in this area, and look at CGRAs, a class of hardware accelerators that hase not been widely discussed in this context.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4403295802",
    "type": "article"
  },
  {
    "title": "On the trade-off between power and flexibility of FPGA clock networks",
    "doi": "https://doi.org/10.1145/1391732.1391733",
    "publication_date": "2008-09-01",
    "publication_year": 2008,
    "authors": "Julien Lamoureux; Steven J. E. Wilton",
    "corresponding_authors": "",
    "abstract": "FPGA clock networks consume a significant amount of power, since they toggle every clock cycle and must be flexible enough to implement the clocks for a wide range of different applications. The efficiency of FPGA clock networks can be improved by reducing this flexibility; however, reducing the flexibility introduces stricter constraints during the clustering and placement stages of the FPGA CAD flow. These constraints can reduce the overall efficiency of the final implementation. This article examines the trade-off between the power consumption and flexibility of FPGA clock networks. Specifically, this article makes three contributions. First, it presents a new parameterized clock-network framework for describing and comparing FPGA clock networks. Second, it describes new clock-aware placement techniques that are needed to find a legal placement satisfying the constraints imposed by the clock network. Finally, it performs an empirical study to examine the trade-off between the power consumption of the clock network and the impact of the CAD constraints for a number of different clock networks with varying amounts of flexibility. The results show that the techniques used to produce a legal placement can have a significant influence on power and the ability of the placer to find a legal solution. On average, circuits placed using the most effective techniques dissipate 5% less overall energy and are significantly more likely to be legal than circuits placed using other techniques. Moreover, the results show that the architecture of the clock network is also important. On average, FPGAs with an efficient clock network are up to 14.6% more energy efficient compared to other FPGAs.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W1965195759",
    "type": "article"
  },
  {
    "title": "Optimal Loop Unrolling and Shifting for Reconfigurable Architectures",
    "doi": "https://doi.org/10.1145/1575779.1575785",
    "publication_date": "2009-09-01",
    "publication_year": 2009,
    "authors": "Ozana Silvia Dragomir; Todor Stefanov; Koen Bertels",
    "corresponding_authors": "",
    "abstract": "In this article, we present a new technique for optimizing loops that contain kernels mapped on a reconfigurable fabric. We assume the Molen machine organization as our framework. We propose combining loop unrolling with loop shifting, which is used to relocate the function calls contained in the loop body such that in every iteration of the transformed loop, software functions (running on GPP) execute in parallel with multiple instances of the kernel (running on FPGA). The algorithm computes the optimal unroll factor and determines the most appropriate transformation (which can be the combination of unrolling plus shifting or either of the two). This method is based on profiling information about the kernel’s execution times on GPP and FPGA, memory transfers and area utilization. In the experimental part, we apply this method to several kernels from loop nests extracted from real-life applications (DCT and SAD from MPEG2 encoder, Quantizer from JPEG, and Sobel’s Convolution) and perform an analysis of the results, comparing them with the theoretical maximum speedup by Amdahl’s Law and showing when and how our transformations are beneficial.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2039186546",
    "type": "article"
  },
  {
    "title": "Design Assurance Strategy and Toolset for Partially Reconfigurable FPGA Systems",
    "doi": "https://doi.org/10.1145/1857927.1857931",
    "publication_date": "2010-12-01",
    "publication_year": 2010,
    "authors": "Krzysztof Kȩpa; Fearghal Morgan; Krzysztof Kościuszkiewicz; L. Braun; Michael Hübner; Jürgen Becker",
    "corresponding_authors": "",
    "abstract": "The growth of the Reconfigurable Computing (RC) systems community exposes diverse requirements with regard to functionality of Electronic Design Automation (EDA) tools. Low-level design tools are increasingly required for RC bitstream debugging and IP core design assurance, particularly in multiparty Partially Reconfigurable (PR) designs. While tools for low-level analysis of design netlists do exist, there is increasing demand for automated and customisable bitstream analysis tools. This article discusses the need for low-level IP core verification within PR-enabled FPGA systems and reports FDAT (FPGA Design Analysis Tool), a versatile, modular and open tools framework for low-level analysis and verification of FPGA designs. FDAT provides a set of high-level Application Programming Interfaces (APIs) abstracting the Xilinx FPGA fabric, the implemented design (e.g., placed and routed netlist) and the related bitstream. A lightweight graphic front-end allows custom visualisation of the design within the FPGA fabric. The operation of FDAT is governed by “recipe” scripts which support rapid prototyping of the abstract algorithms for system-level design verification. FDAT recipes, being Python scripts, can be ported to embedded FPGA systems, for example, the previously reported Secure Reconfiguration Controller (SeReCon) which enforces an IP core spatial isolation policy in order to provide run-time protection to the PR system. The paper illustrates the application of FDAT for bit-pattern analysis of Virtex-II Pro and Virtex-5 inter-tile routing and verification of the spatial isolation between designs.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2024402038",
    "type": "article"
  },
  {
    "title": "Guest Editorial ARC 2009",
    "doi": "https://doi.org/10.1145/1857927.1857928",
    "publication_date": "2010-12-01",
    "publication_year": 2010,
    "authors": "Roger Woods; Jürgen Becker; Peter Athanas; Fearghal Morgan",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2081775969",
    "type": "editorial"
  },
  {
    "title": "UNTANGLED",
    "doi": "https://doi.org/10.1145/2517325",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Gayatri Mehta; Carson Crawford; Xiaozhong Luo; Natalie Parde; Krunalkumar Patel; Brandon Rodgers; Anil Kumar Sistla; Anil Kumar Yadav; Marc Reisner",
    "corresponding_authors": "",
    "abstract": "The problem of creating efficient mappings of dataflow graphs onto specific architectures (i.e., solving the place and route problem) is incredibly challenging. The difficulty is especially acute in the area of Coarse-Grained Reconfigurable Architectures (CGRAs) to the extent that solving the mapping problem may remove a significant bottleneck to adoption. We believe that the next generation of mapping algorithms will exhibit pattern recognition, the ability to learn from experience, and identification of creative solutions, all of which are human characteristics. This manuscript describes our game UNTANGLED, developed and fine-tuned over the course of a year to allow us to capture and analyze human mapping strategies. It also describes our results to date. We find that the mapping problem can be crowdsourced very effectively, that players can outperform existing algorithms, and that successful player strategies share many elements in common. Based on our observations and analysis, we make concrete recommendations for future research directions for mapping onto CGRAs.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2003527430",
    "type": "article"
  },
  {
    "title": "Enhancing Reconfigurable Platforms Programmability for Synchronous Data-Flow Applications",
    "doi": "https://doi.org/10.1145/2362374.2362378",
    "publication_date": "2012-10-01",
    "publication_year": 2012,
    "authors": "Laurent Gantel; Amel Khiar; Benoît Miramond; Mohamed El Amine Benkhelifa; Lounis Kessal; Fabrice Lemonnier; Jimmy Le Rhun",
    "corresponding_authors": "",
    "abstract": "Recent FPGAs allow the design of efficient and complex Heterogeneous Systems-on-Chip (HSoC). Namely, these systems are composed of several processors, hardware accelerators as well as communication media between all these components. Performances provided by HSoCs make them really interesting for data-flow applications, especially image processing applications. The use of this kind of architecture provides good performances but the drawback is an increase of the programming complexity. This complexity is due to the heterogeneous deployment of the application on the platform. Some functions are implemented in software to run on a processor, whereas other functions are implemented in hardware to run in a reconfigurable partition of the FPGA. This article aims to define a programming model based on the Synchronous Data-Flow model, in order to abstract the heterogeneity of the implementation and to leverage the communication issue between software and hardware actors.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2010052271",
    "type": "article"
  },
  {
    "title": "Microarchitectural Comparison of the MXP and Octavo Soft-Processor FPGA Overlays",
    "doi": "https://doi.org/10.1145/3053679",
    "publication_date": "2017-05-27",
    "publication_year": 2017,
    "authors": "Charles Eric LaForest; Jason H. Anderson",
    "corresponding_authors": "",
    "abstract": "Field-Programmable Gate Arrays (FPGAs) can yield higher performance and lower power than software solutions on CPUs or GPUs. However, designing with FPGAs requires specialized hardware design skills and hours-long CAD processing times. To reduce and accelerate the design effort, we can implement an overlay architecture on the FPGA, on which we then more easily construct the desired system but at a large cost in performance and area relative to a direct FPGA implementation. In this work, we compare the micro-architecture, performance, and area of two soft-processor overlays: the Octavo multi-threaded soft-processor and the MXP soft vector processor. To measure the area and performance penalties of these overlays relative to the underlying FPGA hardware, we compare direct FPGA implementations of the micro-benchmarks written in C synthesized with the LegUp HLS tool and also written in the Verilog HDL. Overall, Octavo’s higher operating frequency and MXP’s more efficient code execution results in similar performance from both, within an order of magnitude of direct FPGA implementations, but with a penalty of an order of magnitude greater area.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2620282837",
    "type": "article"
  },
  {
    "title": "PIMap",
    "doi": "https://doi.org/10.1145/3268344",
    "publication_date": "2018-12-31",
    "publication_year": 2018,
    "authors": "Gai Liu; Zhiru Zhang",
    "corresponding_authors": "",
    "abstract": "Modern FPGA synthesis tools typically apply a predetermined sequence of logic optimizations on the input logic network before carrying out technology mapping. While the “known recipes” of logic transformations often lead to improved mapping results, there remains a nontrivial gap between the quality metrics driving the pre-mapping logic optimizations and those targeted by the actual technology mapping. Needless to mention, such miscorrelations would eventually result in suboptimal quality of results. In this article, we propose PIMap, which couples logic transformations and technology mapping under an iterative improvement framework for LUT-based FPGAs. In each iteration, PIMap randomly proposes a transformation on the given logic network from an ensemble of candidate optimizations; it then invokes technology mapping and makes use of the mapping result to determine the likelihood of accepting the proposed transformation. By adjusting the optimization objective and incorporating required time constraints during the iterative process, PIMap can flexibly optimize for different objectives including area minimization, delay optimization, and delay-constrained area reduction. To mitigate the runtime overhead, we further introduce parallelization techniques to decompose a large design into multiple smaller sub-netlists that can be optimized simultaneously. Experimental results show that PIMap achieves promising quality improvement over a set of commonly used benchmarks, including improving the majority of the best-known area and delay records for the EPFL benchmark suite.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2908839997",
    "type": "article"
  },
  {
    "title": "Design of Distributed Reconfigurable Robotics Systems with ReconROS",
    "doi": "https://doi.org/10.1145/3494571",
    "publication_date": "2021-12-27",
    "publication_year": 2021,
    "authors": "Christian Lienen; Marco Platzner",
    "corresponding_authors": "",
    "abstract": "Robotics applications process large amounts of data in real time and require compute platforms that provide high performance and energy efficiency. FPGAs are well suited for many of these applications, but there is a reluctance in the robotics community to use hardware acceleration due to increased design complexity and a lack of consistent programming models across the software/hardware boundary. In this article, we present ReconROS , a framework that integrates the widely used robot operating system (ROS) with ReconOS, which features multithreaded programming of hardware and software threads for reconfigurable computers. This unique combination gives ROS 2 developers the flexibility to transparently accelerate parts of their robotics applications in hardware. We elaborate on the architecture and the design flow for ReconROS and report on a set of experiments that underline the feasibility and flexibility of our approach.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W3181418547",
    "type": "article"
  },
  {
    "title": "The Table-Hadamard GRNG",
    "doi": "https://doi.org/10.1145/2629607",
    "publication_date": "2015-09-24",
    "publication_year": 2015,
    "authors": "David B. Thomas",
    "corresponding_authors": "David B. Thomas",
    "abstract": "Gaussian random number generators (GRNGs) are an important component in parallel Monte Carlo simulations using FPGAs, where tens or hundreds of high-quality Gaussian samples must be generated per cycle using very few logic resources. This article describes the Table-Hadamard generator, which is a GRNG designed to generate multiple streams of random numbers in parallel. It uses discrete table distributions to generate pseudo-Gaussian base samples, then a parallel Hadamard transform to efficiently apply the central limit theorem. When generating 64 output samples, the Table-Hadamard requires just 130 slices per generated sample, which is a third of the resources needed by the next best technique, while still providing higher statistical quality.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2060615856",
    "type": "article"
  },
  {
    "title": "Design Tools for Implementing Self-Aware and Fault-Tolerant Systems on FPGAs",
    "doi": "https://doi.org/10.1145/2617597",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "Christian Beckhoff; Dirk Koch; Jim Tørresen",
    "corresponding_authors": "",
    "abstract": "To fully exploit the capabilities of runtime reconfigurable FPGAs in self-aware systems, design tools are required that exceed the capabilities of present vendor design tools. Such tools must allow the implementation of scalable reconfigurable systems with various different partial modules that might be loaded to different positions of the device at runtime. This comprises several complex tasks, including floorplanning, communication architecture synthesis, physical constraints generation, physical implementation, and timing verification all the way down to the final bitstream generation. In this article, we present how our GoAhead framework helps in implementing self-aware systems on FPGAs with a minimum of user interaction.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2160274482",
    "type": "article"
  },
  {
    "title": "A Framework for Evaluating and Optimizing FPGA-Based SoCs for Aerospace Computing",
    "doi": "https://doi.org/10.1145/2888400",
    "publication_date": "2016-09-24",
    "publication_year": 2016,
    "authors": "Nicholas Wulf; Alan D. George; Ross Gordon",
    "corresponding_authors": "",
    "abstract": "On-board processing systems are often deployed in harsh aerospace environments and must therefore adhere to stringent constraints such as low power, small size, and high dependability in the presence of faults. Field-programmable gate arrays (FPGAs) are often an attractive option for designers seeking low-power, high-performance devices. However, unlike nonreconfigurable devices, radiation effects can alter an FPGA’s functionality instead of just the device’s data, requiring designers to consider fault-tolerant strategies to mitigate these effects. In this article, we present a framework to ease these system design challenges and aid designers in considering a broad range of devices and fault-tolerant strategies for on-board processing, highlighting the most promising options and tradeoffs early in the design process. This article focuses on the power, dependability, and lifetime evaluation metrics, which our framework calculates and leverages to evaluate the effectiveness of varying system-on-chip (SoC) designs. Finally, we use our framework to evaluate SoC designs for a case study on a hyperspectral-imaging (HSI) mission to demonstrate our framework’s ability to identify efficient and effective SoC designs.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2524004831",
    "type": "article"
  },
  {
    "title": "Exploiting HBM on FPGAs for Data Processing",
    "doi": "https://doi.org/10.1145/3491238",
    "publication_date": "2022-02-09",
    "publication_year": 2022,
    "authors": "Runbin Shi; Kaan Kara; Christoph Hagleitner; Dionysios Diamantopoulos; Dimitris Syrivelis; Gustavo Alonso",
    "corresponding_authors": "",
    "abstract": "Field Programmable Gate Arrays (FPGAs) are increasingly being used in data centers and the cloud due to their potential to accelerate certain workloads as well as for their architectural flexibility, since they can be used as accelerators, smart-NICs, or stand-alone processors. To meet the challenges posed by these new use cases, FPGAs are quickly evolving in terms of their capabilities and organization. The utilization of High Bandwidth Memory (HBM) in FPGA devices is one recent example of such a trend. In this article, we study the potential of FPGAs equipped with HBM from a data analytics perspective. We consider three workloads common in analytics-oriented databases and implement them on an FPGA showing in which cases they benefit from HBM: range selection, hash join, and stochastic gradient descent for linear model training. We integrate our designs into a columnar database (MonetDB) and show the trade-offs arising from the integration related to data movement and partitioning. We consider two possible configurations of the HBM, using a single and a dual clock version design. With the right design, FPGA+HBM-based solutions are able to surpass the highest performance provided by either a two-socket POWER9 1 system or a 14-core Xeon 2 E5 by up to 5.9× (range selection), 18.3× (hash join), and 6.1× (SGD).",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4210829446",
    "type": "article"
  },
  {
    "title": "A BNN Accelerator Based on Edge-skip-calculation Strategy and Consolidation Compressed Tree",
    "doi": "https://doi.org/10.1145/3494569",
    "publication_date": "2022-05-10",
    "publication_year": 2022,
    "authors": "Gaoming Du; Bangyi Chen; Zhenmin Li; Zhenxing Tu; Junjie Zhou; Shenya Wang; Qinghao Zhao; Yongsheng Yin; Xiaolei Wang",
    "corresponding_authors": "",
    "abstract": "Binarized neural networks (BNNs) and batch normalization (BN) have already become typical techniques in artificial intelligence today. Unfortunately, the massive accumulation and multiplication in BNN models bring challenges to field-programmable gate array (FPGA) implementations, because complex arithmetics in BN consume too much computing resources. To relax FPGA resource limitations and speed up the computing process, we propose a BNN accelerator architecture based on consolidation compressed tree scheme by combining both XNOR and accumulation operation of the low bit into a systematic one. During the compression process, we adopt 0-padding (not ±1) to achieve no-accuracy-loss from software modeling to hardware implementation. Moreover, we introduce shift-addition-BN free binarization technique to shorten the delay path and optimize on-chip storage. To sum up, we drastically cut down the hardware consumption while maintaining great speed performance with the same model complexity as the previous design. We evaluate our accelerator on MNIST and CIFAR-10 dataset and implement the whole system on the ARTIX-7 100T FPGA with speed performance of 2052.65 GOP/s and area efficiency of 70.15 GOPS/KLUT.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4229460177",
    "type": "article"
  },
  {
    "title": "Automatic Creation of High-bandwidth Memory Architectures from Domain-specific Languages: The Case of Computational Fluid Dynamics",
    "doi": "https://doi.org/10.1145/3563553",
    "publication_date": "2022-09-15",
    "publication_year": 2022,
    "authors": "Stephanie Soldavini; Karl F. A. Friebel; Mattia Tibaldi; Gerald Hempel; Jerónimo Castrillón; Christian Pilato",
    "corresponding_authors": "",
    "abstract": "Numerical simulations can help solve complex problems. Most of these algorithms are massively parallel and thus good candidates for FPGA acceleration thanks to spatial parallelism. Modern FPGA devices can leverage high-bandwidth memory technologies, but when applications are memory-bound designers must craft advanced communication and memory architectures for efficient data movement and on-chip storage. This development process requires hardware design skills that are uncommon in domain-specific experts. In this paper, we propose an automated tool flow from a domain-specific language (DSL) for tensor expressions to generate massively-parallel accelerators on HBM-equipped FPGAs. Designers can use this flow to integrate and evaluate various compiler or hardware optimizations. We use computational fluid dynamics (CFD) as a paradigmatic example. Our flow starts from the high-level specification of tensor operations and combines an MLIR-based compiler with an in-house hardware generation flow to generate systems with parallel accelerators and a specialized memory architecture that moves data efficiently, aiming at fully exploiting the available CPU-FPGA bandwidth. We simulated applications with millions of elements, achieving up to 103 GFLOPS with one compute unit and custom precision when targeting a Xilinx Alveo U280. Our FPGA implementation is up to 25x more energy-efficient than expert-crafted Intel CPU implementations.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4296131215",
    "type": "article"
  },
  {
    "title": "Eciton: Very Low-power Recurrent Neural Network Accelerator for Real-time Inference at the Edge",
    "doi": "https://doi.org/10.1145/3629979",
    "publication_date": "2023-11-02",
    "publication_year": 2023,
    "authors": "Jeffrey Chen; Sang-Woo Jun; Sehwan Hong; Warrick He; Jinyeong Moon",
    "corresponding_authors": "",
    "abstract": "This article presents Eciton, a very low-power recurrent neural network accelerator for time series data within low-power edge sensor nodes, achieving real-time inference with a power consumption of 17 mW under load. Eciton reduces memory and chip resource requirements via 8-bit quantization and hard sigmoid activation, allowing the accelerator as well as the recurrent neural network model parameters to fit in a low-cost, low-power Lattice iCE40 UP5K FPGA. We evaluate Eciton on multiple, established time-series classification applications including predictive maintenance of mechanical systems, sound classification, and intrusion detection for IoT nodes. Binary and multi-class classification edge models are explored, demonstrating that Eciton can adapt to a variety of deployable environments and remote use cases. Eciton demonstrates real-time processing at a very low power consumption with minimal loss of accuracy on multiple inference scenarios with differing characteristics, while achieving competitive power efficiency against the state-of-the-art of similar scale. We show that the addition of this accelerator actually reduces the power budget of the sensor node by reducing power-hungry wireless transmission. The resulting power budget of the sensor node is small enough to be powered by a power harvester, potentially allowing it to run indefinitely without a battery or periodic maintenance.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4388229172",
    "type": "article"
  },
  {
    "title": "A Hardware Design Framework for Computer Vision Models Based on Reconfigurable Devices",
    "doi": "https://doi.org/10.1145/3635157",
    "publication_date": "2023-12-05",
    "publication_year": 2023,
    "authors": "Zimeng Fan; Wei Hu; Fang Liu; Dian Xu; Hong Guo; Yanxiang He; Min Peng",
    "corresponding_authors": "",
    "abstract": "In computer vision, the joint development of the algorithm and computing dimensions cannot be separated. Models and algorithms are constantly evolving, while hardware designs must adapt to new or updated algorithms. Reconfigurable devices are recognized as important platforms for computer vision applications because of their reconfigurability. There are two typical design approaches: customized and overlay design. However, existing work is unable to achieve both efficient performance and scalability to adapt to a wide range of models. To address both considerations, we propose a design framework based on reconfigurable devices to provide unified support for computer vision models. It provides software-programmable modules while leaving unit design space for problem-specific algorithms. Based on the proposed framework, we design a model mapping method and a hardware architecture with two processor arrays to enable dynamic and static reconfiguration, thereby relieving redesign pressure. In addition, resource consumption and efficiency can be balanced by adjusting the hyperparameter. In experiments on CNN, vision Transformer, and vision MLP models, our work’s throughput is improved by 18.8x–33.6x and 1.4x–2.0x compared to CPU and GPU. Compared to others on the same platform, accelerators based on our framework can better balance resource consumption and efficiency.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4389337739",
    "type": "article"
  },
  {
    "title": "A Desktop Computer with a Reconfigurable Pentium®",
    "doi": "https://doi.org/10.1145/1331897.1331901",
    "publication_date": "2008-03-01",
    "publication_year": 2008,
    "authors": "Shih‐Lien L. Lu; Peter Yiannacouras; Taeweon Suh; Rolf Kassa; Michael Konow",
    "corresponding_authors": "",
    "abstract": "Advancements in reconfigurable technologies, specifically FPGAs, have yielded faster, more power-efficient reconfigurable devices with enormous capacities. In our work, we provide testament to the impressive capacity of recent FPGAs by hosting a complete Pentium ® in a single FPGA chip. In addition we demonstrate how FPGAs can be used for microprocessor design space exploration while overcoming the tension between simulation speed, model accuracy, and model completeness found in traditional software simulator environments. Specifically, we perform preliminary experimentation/prototyping with an original Socket 7 based desktop processor system with typical hardware peripherals running modern operating systems such as Fedora Core 4 and Windows XP; however we have inserted a Xilinx Virtex-4 in place of the processor that should sit in the motherboard and have used the Virtex-4 to host a complete version of the Pentium ® microprocessor (which consumes less than half its resources). We can therefore apply architectural changes to the processor and evaluate their effects on the complete desktop system. We use this FPGA-based emulation system to conduct preliminary architectural experiments including growing the branch target buffer and the level 1 caches. In addition, we experimented with interfacing hardware accelerators such as DES and AES engines which resulted in a 27x speedup.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2018593616",
    "type": "article"
  },
  {
    "title": "SARFUM",
    "doi": "https://doi.org/10.1145/1754386.1754389",
    "publication_date": "2010-05-01",
    "publication_year": 2010,
    "authors": "Benoît Badrignans; David Champagne; Reouven Elbaz; Catherine H. Gebotys; Lionel Torres",
    "corresponding_authors": "",
    "abstract": "Remote update of hardware platforms or embedded systems is a convenient service enabled by Field Programmable Gate Array (FPGA)-based systems. This service is often essential in applications like space-based FPGA systems or set-top boxes. However, having the source of the update be remote from the FPGA system opens the door to a set of attacks that may challenge the confidentiality and integrity of the FPGA configuration, the bitstream. Existing schemes propose to encrypt and authenticate the bitstream to thwart these attacks. However, we show that they do not prevent the replay of old bitstream versions, and thus give adversaries an opportunity for downgrading the system. In this article, we propose a new architecture called sarfum that, in addition to ensuring bitstream confidentiality and integrity, precludes the replay of old bitstreams. sarfum also includes a protocol for the system designer to remotely monitor the running configuration of the FPGA. Following our presentation and analysis of the security protocols, we propose an example of implementation with the CCM (Counter with CBC-MAC) authenticated encryption standard. We also evaluate the impact of our architecture on the configuration time for different FPGA devices.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W1989678405",
    "type": "article"
  },
  {
    "title": "Field Programmable Compressor Trees",
    "doi": "https://doi.org/10.1145/1534916.1534923",
    "publication_date": "2009-06-01",
    "publication_year": 2009,
    "authors": "Alessandro Cevrero; Panagiotis Athanasopoulos; Hadi Parandeh-Afshar; Ajay Verma; Seyed-Hosein Attarzadeh-Niaki; Chrysostomos Nicopoulos; Frank K. Gürkaynak; Philip Brisk; Yusuf Leblebici; Paolo Ienne",
    "corresponding_authors": "",
    "abstract": "Multi-input addition occurs in a variety of arithmetically intensive signal processing applications. The DSP blocks embedded in high-performance FPGAs perform fixed bitwidth parallel multiplication and Multiply-ACcumulate (MAC) operations. In theory, the compressor trees contained within the multipliers could implement multi-input addition; however, they are not exposed to the programmer. To improve FPGA performance for these applications, this article introduces the Field Programmable Compressor Tree (FPCT) as an alternative to the DSP blocks. By providing just a compressor tree, the FPCT can perform multi-input addition along with parallel multiplication and MAC in conjunction with a small amount of FPGA general logic. Furthermore, the user can configure the FPCT to precisely match the bitwidths of the operands being summed. Although an FPCT cannot beat the performance of a well-designed ASIC compressor tree of fixed bitwidth, for example, 9×9 and 18×18-bit multipliers/MACs in DSP blocks, its configurable bitwidth and ability to perform multi-input addition is ideal for reconfigurable devices that are used across a variety of applications.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2004940452",
    "type": "article"
  },
  {
    "title": "A New Timing Driven Placement Algorithm for Dependable Circuits on SRAM-based FPGAs",
    "doi": "https://doi.org/10.1145/1857927.1857934",
    "publication_date": "2010-12-01",
    "publication_year": 2010,
    "authors": "Luca Sterpone",
    "corresponding_authors": "Luca Sterpone",
    "abstract": "Electronic systems for safety critical applications such as space and avionics need the maximum level of dependability for guarantee the success of their missions. Simultaneously the computation capabilities required in these fields are constantly increasing for afford the implementation of different kind of applications ranging from signal processing to networking. SRAM-based FPGAs are the candidate devices to achieve this goal thanks to their high versatility of implementing complex circuits with a very short development time. However, in critical environments, the presence of Single Event Upsets (SEUs) affecting the FPGA’s functionalities, requires the adoption of specific fault tolerant techniques, like Triple Modular Redundancy (TMR), able to increase the protection capability against radiation effects, but on the other side, introducing a dramatic penalty in terms of performances. In this paper, it is proposed a new timing-driven placement algorithm for implementing soft-errors resilient circuits on SRAM-based FPGAs with a negligible degradation of performance. The algorithm is based on a placement heuristic able to remove the crossing error domains while decreasing the routing congestions and delay inserted by the TMR routing and voting scheme. Experimental analysis performed by timing analysis and SEU static analysis point out a performance improvement of 29% on the average with respect to standard TMR approach and an increased robustness against SEU affecting the FPGA’s configuration memory. Accurate analyses of SEUs sensitivity and performance optimization have been performed on a real microprocessor core demonstrating an improvement of performances of more than 62%.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2121314934",
    "type": "article"
  },
  {
    "title": "Searching for Transient Pulses with the ETA Radio Telescope",
    "doi": "https://doi.org/10.1145/1462586.1462589",
    "publication_date": "2009-01-01",
    "publication_year": 2009,
    "authors": "Cameron D. Patterson; Steven W. Ellingson; B. Martin; K. Deshpande; J. H. Simonetti; Michael Kavic; S. Cutchin",
    "corresponding_authors": "",
    "abstract": "Array-based, direct-sampling radio telescopes have computational and communication requirements unsuited to conventional computer and cluster architectures. Synchronization must be strictly maintained across a large number of parallel data streams, from A/D conversion, through operations such as beamforming, to dataset recording. FPGAs supporting multigigabit serial I/O are ideally suited to this application. We describe a recently-constructed radio telescope called ETA having all-sky observing capability for detecting low frequency pulses from transient events such as gamma ray bursts and primordial black hole explosions. Signals from 24 dipole antennas are processed by a tiered arrangement of 28 commercial FPGA boards and 4 PCs with FPGA-based data acquisition cards, connected with custom I/O adapter boards supporting InfiniBand and LVDS physical links. ETA is designed for unattended operation, allowing configuration and recording to be controlled remotely.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W3101372596",
    "type": "article"
  },
  {
    "title": "Application-Specific FPGA using heterogeneous logic blocks",
    "doi": "https://doi.org/10.1145/2000832.2000836",
    "publication_date": "2011-08-01",
    "publication_year": 2011,
    "authors": "Husain Parvez; Zied Marrakchi; Alp Kiliç; Habib Mehrez",
    "corresponding_authors": "",
    "abstract": "This work presents a new automatic mechanism to explore the solution space between Field Programmable Gate Arrays (FPGAs) and Application-Specific Integrated Circuits (ASICs). This new solution is termed as an Application-Specific Inflexible FPGA (ASIF) [Parvez et al. 2009]. An ASIF can be considered as an FPGA with reduced flexibility, or as a reconfigurable ASIC that can implement a set of application circuits which will operate at mutually exclusive times. Execution of different application circuits can be switched by loading their respective bitstream on an ASIF. An ASIF that is reduced from a heterogeneous FPGA is termed as a heterogeneous ASIF. It is shown that a standard-cell-based heterogeneous ASIF for a set of 10 opencore application circuits is 9.6 times smaller than a single-driver mesh-based heterogeneous FPGA. The area gap between ASIC and ASIF is not too significant; however, it can be reduced by designing repeatedly used components of ASIF in full-custom. Unlike an ASIC, an ASIF is a reprogrammable device that can be used to reprogram new or modified circuits at a limited scale.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2021255422",
    "type": "article"
  },
  {
    "title": "Virtualizable hardware/software design infrastructure for dynamically partially reconfigurable systems",
    "doi": "https://doi.org/10.1145/2499625.2499628",
    "publication_date": "2013-07-01",
    "publication_year": 2013,
    "authors": "Chun-Hsian Huang; Pao‐Ann Hsiung",
    "corresponding_authors": "",
    "abstract": "In most existing works, reconfigurable hardware modules are still managed as conventional hardware devices. Further, the software reconfiguration overhead incurred by loading corresponding device drivers into the kernel of an operating system has been overlooked until now. As a result, the enhancement of system performance and the utilization of reconfigurable hardware modules are still quite limited. This work proposes a virtualizable hardware/software design infrastructure (VDI) for dynamically partially reconfigurable systems. Besides the gate-level hardware virtualization provided by the partial reconfiguration technology, VDI supports the device-level hardware virtualization. In VDI, a reconfigurable hardware module can be virtualized such that it can be accessed efficiently by multiple applications in an interleaving way. A Hot-Plugin Connector (HPC) replaces the conventional device driver, such that it not only assists the device-level hardware virtualization but can also be reused across different hardware modules. To facilitate hardware/software communication and to enhance system scalability, the proposed VDI is realized as a hierarchical design framework. User-designed reconfigurable hardware modules can be easily integrated into VDI, and are then executed as hardware tasks in an operating system for reconfigurable systems (OS4RS). A dynamically partially reconfigurable network security system was designed using VDI, which demonstrated a higher utilization of reconfigurable hardware modules and a reduction by up to 12.83% of the processing time required by using the conventional method in a dynamically partially reconfigurable system.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2067332732",
    "type": "article"
  },
  {
    "title": "Quipu",
    "doi": "https://doi.org/10.1145/2457443.2457446",
    "publication_date": "2013-05-01",
    "publication_year": 2013,
    "authors": "Roel Meeuws; S. Arash Ostadzadeh; Carlo Galuzzi; Vlad-Mihai Sima; Răzvan Nane; Koen Bertels",
    "corresponding_authors": "",
    "abstract": "There has been a steady increase in the utilization of heterogeneous architectures to tackle the growing need for computing performance and low-power systems. The execution of computation-intensive functions on specialized hardware enables to achieve substantial speedups and power savings. However, with a large legacy code base and software engineering experts, it is not at all obvious how to easily utilize these new architectures. As a result, there is a need for comprehensive tool support to bridge the knowledge gap of many engineers as well as to retarget legacy code. In this article, we present the Quipu modeling approach, which consists of a set of tools and a modeling methodology that can generate hardware estimation models, which provide valuable information for developers. This information helps to focus their efforts, to partition their application, and to select the right heterogeneous components. We present Quipu ’s capability to generate domain-specific models, that are up to several times more accurate within their particular domain (error: 4.6%) as compared to domain-agnostic models (error: 23%). Finally, we show how Quipu can generate models for a new toolchain and platform within a few days.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2158161722",
    "type": "article"
  },
  {
    "title": "Efficient and Versatile FPGA Acceleration of Support Counting for Stream Mining of Sequences and Frequent Itemsets",
    "doi": "https://doi.org/10.1145/3027485",
    "publication_date": "2017-05-27",
    "publication_year": 2017,
    "authors": "Adrien Prost-Boucle; Frédéric Pétrot; Vincent Leroy; Hande Alemdar",
    "corresponding_authors": "",
    "abstract": "Stream processing has become extremely popular for analyzing huge volumes of data for a variety of applications, including IoT, social networks, retail, and software logs analysis. Streams of data are produced continuously and are mined to extract patterns characterizing the data. A class of data mining algorithm, called generate-and-test , produces a set of candidate patterns that are then evaluated over data. The main challenges of these algorithms are to achieve high throughput, low latency, and reduced power consumption. In this article, we present a novel power-efficient, fast, and versatile hardware architecture whose objective is to monitor a set of target patterns to maintain their frequency over a stream of data. This accelerator can be used to accelerate data-mining algorithms, including itemsets and sequences mining. The massive fine-grain reconfiguration capability of field-programmable gate array (FPGA) technologies is ideal to implement the high number of pattern-detection units needed for these intensive data-mining applications. We have thus designed and implemented an IP that features high-density FPGA occupation and high working frequency. We provide detailed description of the IP internal micro-architecture and its actual implementation and optimization for the targeted FPGA resources. We validate our architecture by developing a co-designed implementation of the Apriori Frequent Itemset Mining (FIM) algorithm, and perform numerous experiments against existing hardware and software solutions. We demonstrate that FIM hardware acceleration is particularly efficient for large and low-density datasets (i.e., long-tailed datasets). Our IP reaches a data throughput of 250 million items/s and monitors up to 11.6k patterns simultaneously, on a prototyping board that overall consumes 24W in the worst case. Furthermore, our hardware accelerator remains generic and can be integrated to other generate and test algorithms.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2592732369",
    "type": "article"
  },
  {
    "title": "Optimizing FPGA Performance, Power, and Dependability with Linear Programming",
    "doi": "https://doi.org/10.1145/3079756",
    "publication_date": "2017-06-29",
    "publication_year": 2017,
    "authors": "Nicholas Wulf; Alan D. George; Ross Gordon",
    "corresponding_authors": "",
    "abstract": "Field-programmable gate arrays (FPGA) are an increasingly attractive alternative to traditional microprocessor-based computing architectures in extreme-computing domains, such as aerospace and supercomputing. FPGAs offer several resource types that offer different tradeoffs between speed, power, and area, which make FPGAs highly flexible for varying application computational requirements. However, since an application’s computational operations can map to different resource types, a major challenge in leveraging resource-diverse FPGAs is determining the optimal distribution of these operations across the device’s available resources for varying FPGA devices, resulting in an extremely large design space. In order to facilitate fast design-space exploration, this article presents a method based on linear programming (LP) that determines the optimal operation distribution for a particular device and application with respect to performance, power, or dependability metrics. Our LP method is an effective tool for exploring early designs by quickly analyzing thousands of FPGAs to determine the best FPGA devices and operation distributions, which significantly reduces design time. We demonstrate our LP method’s effectiveness with two case studies involving dot-product and distance-calculation kernels on a range of Virtex-5 FPGAs. Results show that our LP method selects optimal distributions of operations to within an average of 4% of actual values.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2727617496",
    "type": "article"
  },
  {
    "title": "Exact and Practical Modulo Scheduling for High-Level Synthesis",
    "doi": "https://doi.org/10.1145/3317670",
    "publication_date": "2019-05-06",
    "publication_year": 2019,
    "authors": "Julian Oppermann; Melanie Reuter-Oppermann; Lukáš Sommer; Andreas Koch; Oliver Sinnen",
    "corresponding_authors": "",
    "abstract": "Loop pipelining is an essential technique in high-level synthesis to increase the throughput and resource utilisation of field-programmable gate array--based accelerators. It relies on modulo schedulers to compute an operator schedule that allows subsequent loop iterations to overlap partially when executed while still honouring all precedence and resource constraints. Modulo schedulers face a bi-criteria problem: minimise the initiation interval (II; i.e., the number of timesteps after which new iterations are started) and minimise the schedule length. We present Moovac, a novel exact formulation that models all aspects (including the II minimisation) of the modulo scheduling problem as a single integer linear program, and discuss simple measures to prevent excessive runtimes, to challenge the old preconception that exact modulo scheduling is impractical. We substantiate this claim by conducting an experimental study covering 188 loops from two established high-level synthesis benchmark suites, four different time limits, and three bounds for the schedule length, to compare our approach against a highly tuned exact formulation and a state-of-the-art heuristic algorithm. In the fastest configuration, an accumulated runtime of under 16 minutes is spent on scheduling all loops, and proven optimal IIs are found for 179 test instances.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2943915040",
    "type": "article"
  },
  {
    "title": "A Design Flow Engine for the Support of Customized Dynamic High Level Synthesis Flows",
    "doi": "https://doi.org/10.1145/3356475",
    "publication_date": "2019-10-31",
    "publication_year": 2019,
    "authors": "Marco Lattuada; Fabrizio Ferrandi",
    "corresponding_authors": "",
    "abstract": "High Level Synthesis is a set of methodologies aimed at generating hardware descriptions starting from specifications written in high-level languages. While these methodologies share different elements with traditional compilation flows, there are characteristics of the addressed problem which require ad hoc management. In particular, differently from most of the traditional compilation flows, the complexity and the execution time of the High Level Synthesis techniques are much less relevant than the quality of the produced results. For this reason, fixed-point analyses, as well as successive refinement optimizations, can be accepted, provided that they can improve the quality of the generated designs. This article presents a design flow engine for the description and the execution of complex and customized synthesis flows. It supports dynamic addition of passes and dependencies, cyclic dependencies, and selective pass invalidation. Experimental results show the benefits of such type of design flows with respect to static linear design flows when applied to High Level Synthesis.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2984033006",
    "type": "article"
  },
  {
    "title": "Optimizing OpenCL-Based CNN Design on FPGA with Comprehensive Design Space Exploration and Collaborative Performance Modeling",
    "doi": "https://doi.org/10.1145/3397514",
    "publication_date": "2020-06-23",
    "publication_year": 2020,
    "authors": "Jiandong Mu; Wei Zhang; Hao Liang; Sharad Sinha",
    "corresponding_authors": "",
    "abstract": "Recent success in applying convolutional neural networks (CNNs) to object detection and classification has sparked great interest in accelerating CNNs using hardware-like field-programmable gate arrays (FPGAs). However, finding an efficient FPGA design for a given CNN model and FPGA board is not trivial since a strong background in hardware design and detailed knowledge of the target board are required. In this work, we try to solve this problem by design space exploration with a collaborative framework. Our framework consists of three main parts: FPGA design generation, coarse-grained modeling, and fine-grained modeling. In the FPGA design generation, we propose a novel data structure, LoopTree, to capture the details of the FPGA design for CNN applications without writing down the source code. Different LoopTrees, which indicate different FPGA designs, are automatically generated in this process. A coarse-grained model will evaluate LoopTrees at the operation level, e.g., add, mult, and so on, so that the most efficient LoopTrees can be selected. A fine-grained model, which is based on the source code, will then refine the selected design in a cycle-accurate manner. A set of comprehensive OpenCL-based designs have been implemented on board to verify our framework. An average estimation error of 8.87% and 4.8% has been observed for our coarse-grained model and fine-grained model, respectively. This is much lower than the prevalent operation-statistics-based estimation, which is obtained according to a predefined formula for specific loop schedules.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3037897376",
    "type": "article"
  },
  {
    "title": "Enhancing the Security of FPGA-SoCs via the Usage of ARM TrustZone and a Hybrid-TPM",
    "doi": "https://doi.org/10.1145/3472959",
    "publication_date": "2021-11-30",
    "publication_year": 2021,
    "authors": "Mathieu Gross; Konrad Hohentanner; Stefan Wiehler; Georg Sigl",
    "corresponding_authors": "",
    "abstract": "Isolated execution is a concept commonly used for increasing the security of a computer system. In the embedded world, ARM TrustZone technology enables this goal and is currently used on mobile devices for applications such as secure payment or biometric authentication. In this work, we investigate the security benefits achievable through the usage of ARM TrustZone on FPGA-SoCs. We first adapt Microsoft’s implementation of a firmware Trusted Platform Module (fTPM) running inside ARM TrustZone for the Zynq UltraScale+ platform. This adaptation consists in integrating hardware accelerators available on the device to fTPM’s implementation and to enhance fTPM with an entropy source derived from on-chip SRAM start-up patterns. With our approach, we transform a software implementation of a TPM into a hybrid hardware/software design that could address some of the security drawbacks of the original implementation while keeping its flexibility. To demonstrate the security gains obtained via the usage of ARM TrustZone and our hybrid-TPM on FPGA-SoCs, we propose a framework that combines them for enabling a secure remote bitstream loading. The approach consists in preventing the insecure usages of a bitstream reconfiguration interface that are made possible by the manufacturer and to integrate the interface inside a Trusted Execution Environment.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3216159141",
    "type": "article"
  },
  {
    "title": "Intra-Masking Dual-Rail Memory on LUT Implementation for SCA-Resistant AES on FPGA",
    "doi": "https://doi.org/10.1145/2617595",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "Anh Tuan Hoang; Takeshi Fujino",
    "corresponding_authors": "",
    "abstract": "In current countermeasure design trends against differential power analysis (DPA), security at gate level is required in addition to the security algorithm. Several dual-rail pre-charge logics (DPL) have been proposed to achieve this goal. Designs using ASIC can attain this goal owing to its backend design restrictions on placement and routing. However, implementing these designs on field programmable gate arrays (FPGA) without information leakage is still a problem because of the difficulty involved in the restrictions on placement and routing on FPGA. This article describes our novel masked dual-rail pre-charged memory approach, called “intra-masking dual-rail memory (IMDRM) on LUT”, and its implementation on FPGA for Side-Channel Attack-resistant (SCA-resistant) AES. In the proposed design, all unsafe nodes, such as unmasking and masking, and parts of dual-rail memory with unsafe buses (buses that are not masked) are packed into a single LUT. This makes them balanced and independent of the placement and routing tools. Inputs and outputs of all LUTs are masked, and so can be considered safe signals. Several LUTs can be combined to create a safe SBox. The design is independent of the cryptographic algorithm, and hence, it can be applied to available cryptographic standards such as DES or AES as well as future standards. It requires no special placement or route constraints in its implementation. A correlation power analysis (CPA) attack on 1,000,000 traces of AES implementation on FPGA showed that the secret information is well protected against first-order side-channel attacks. Even though the number of LUTs used for memory in this implementation is seven times greater than that of the conventional unprotected single-rail memory table-lookup AES and three times greater than the implementation based on a composite field, it requires a smaller number of LUTs than all other advanced SCA-resistant implementations such as the wave dynamic differential logic, masked dual-rail pre-charge logic, and threshold.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2075417072",
    "type": "article"
  },
  {
    "title": "Benefits of Adding Hardware Support for Broadcast and Reduce Operations in MPSoC Applications",
    "doi": "https://doi.org/10.1145/2629470",
    "publication_date": "2014-08-01",
    "publication_year": 2014,
    "authors": "Yuanxi Peng; Manuel Saldaña; Christopher Madill; Xiaofeng Zou; Paul Chow",
    "corresponding_authors": "",
    "abstract": "MPI has been used as a parallel programming model for supercomputers and clusters and recently in MultiProcessor Systems-on-Chip (MPSoC). One component of MPI is collective communication and its performance is key for certain parallel applications to achieve good speedups. Previous work showed that, with synthetic communication-only benchmarks, communication improvements of up to 11.4-fold and 22-fold for broadcast and reduce operations, respectively, can be achieved by providing hardware support at the network level in a Network-on-Chip (NoC). However, these numbers do not provide a good estimation of the advantage for actual applications, as there are other factors that affect performance besides communications, such as computation. To this end, we extend our previous work by evaluating the impact of hardware support over a set of five parallel application kernels of varying computation-to-communication ratios. By introducing some useful computation to the performance evaluation, we obtain more representative results of the benefits of adding hardware support for broadcast and reduce operations. The experiments show that applications with lower computation-to-communication ratios benefit the most from hardware support as they highly depend on efficient collective communications to achieve better scalability. We also extend our work by doing more analysis on clock frequency, resource usage, power, and energy. The results show reasonable scalability for resource utilization and power in the network interfaces as the number of channels increases and that, even though more power is dissipated in the network interfaces due to the added hardware, the total energy used can still be less if the actual speedup is sufficient. The application kernels are executed in a 24-embedded-processor system distributed across four FPGAs.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2104942569",
    "type": "article"
  },
  {
    "title": "Mapping Adaptive Particle Filters to Heterogeneous Reconfigurable Systems",
    "doi": "https://doi.org/10.1145/2629469",
    "publication_date": "2014-12-29",
    "publication_year": 2014,
    "authors": "Thomas Chau; Xinyu Niu; Alison Eele; J.M. Maciejowski; Peter Y. K. Cheung; Wayne Luk",
    "corresponding_authors": "",
    "abstract": "This article presents an approach for mapping real-time applications based on particle filters (PFs) to heterogeneous reconfigurable systems, which typically consist of multiple FPGAs and CPUs. A method is proposed to adapt the number of particles dynamically and to utilise runtime reconfigurability of FPGAs for reduced power and energy consumption. A data compression scheme is employed to reduce communication overhead between FPGAs and CPUs. A mobile robot localisation and tracking application is developed to illustrate our approach. Experimental results show that the proposed adaptive PF can reduce up to 99% of computation time. Using runtime reconfiguration, we achieve a 25% to 34% reduction in idle power. A 1U system with four FPGAs is up to 169 times faster than a single-core CPU and 41 times faster than a 1U CPU server with 12 cores. It is also estimated to be 3 times faster than a system with four GPUs.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2143556363",
    "type": "article"
  },
  {
    "title": "ARC 2014",
    "doi": "https://doi.org/10.1145/2724722",
    "publication_date": "2015-11-04",
    "publication_year": 2015,
    "authors": "Neil Scicluna; Christos-Savvas Bouganis",
    "corresponding_authors": "",
    "abstract": "Clustering large numbers of data points is a very computationally demanding task that often needs to be accelerated in order to be useful in practical applications. This work focuses on the Density-Based Spatial Clustering of Applications with Noise (DBSCAN) algorithm, which is one of the state-of-the-art clustering algorithms, and targets its acceleration using an FPGA device. The article presents an optimized, scalable, and parameterizable architecture that takes advantage of the internal memory structure of modern FPGAs in order to deliver a high-performance clustering system. Post-synthesis simulation results show that the developed system can obtain mean speedups of 31× in real-world tests and 202× in synthetic tests when compared to state-of-the-art software counterparts running on a quad-core 3.4GHz Intel i7-2600k. Additionally, this implementation is also capable of clustering data with any number of dimensions without impacting the performance.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2295513314",
    "type": "article"
  },
  {
    "title": "A Parallel Sliding-Window Generator for High-Performance Digital-Signal Processing on FPGAs",
    "doi": "https://doi.org/10.1145/2800789",
    "publication_date": "2016-05-20",
    "publication_year": 2016,
    "authors": "Greg Stitt; Eric M. Schwartz; Patrick Cooke",
    "corresponding_authors": "",
    "abstract": "Sliding-window applications, an important class of the digital-signal processing domain, are highly amenable to pipeline parallelism on field-programmable gate arrays (FPGAs). Although memory bandwidth often restricts parallelism for many applications, sliding-window applications can leverage custom buffers, referred to as sliding-window generators, that provide massive input bandwidth that far exceeds the capabilities of external memory. Previous work has introduced a variety of sliding-window generators, but those approaches typically generate at most one window per cycle, which significantly restricts parallelism. In this article, we address this limitation with a parallel sliding-window generator that can generate a configurable number of windows every cycle. Although in practice the number of parallel windows is limited by memory bandwidth, we show that even with common bandwidth limitations, the presented generator enables near-linear speedups up to 16x faster than previous FPGA studies that generate a single window per cycle, which were already in some cases faster than graphics-processing units and microprocessors.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2397292334",
    "type": "article"
  },
  {
    "title": "Shared Memory Multicore MicroBlaze System with SMP Linux Support",
    "doi": "https://doi.org/10.1145/2870638",
    "publication_date": "2016-08-09",
    "publication_year": 2016,
    "authors": "Eric Matthews; Lesley Shannon; Alexandra Fedorova",
    "corresponding_authors": "",
    "abstract": "In this work, we present PolyBlaze, a scalable and configurable multicore platform for FPGA-based embedded systems and systems research. PolyBlaze is an extension of the MicroBlaze soft processor, leveraging the configurability of the MicroBlaze and bringing it into the multicore era with Linux Symmetric Multi-Processor (SMP) support. This work details the hardware modifications required for the MicroBlaze processor and its software stack to enable fully validated SMP operations, including atomic operation support, shared interrupts and timers, and exception handling. New in this work, we present a scalable and flexible memory hierarchy optimized for Field Programmable Gate Arrays (FPGAs), which manages atomic operations and provides support for future flexible memory hierarchies and heterogeneous systems. Also new is an in-depth analysis of key performance characteristics, including memory bandwidth, latency, and resource usage. For all system configurations, bandwidth is found to scale linearly with the addition of processor cores until the memory interface is saturated. Additionally, average memory latency remains constant until the memory interface is saturated; after which, it scales linearly with each additional processor core.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2505954124",
    "type": "article"
  },
  {
    "title": "A Microcoded Kernel Recursive Least Squares Processor Using FPGA Technology",
    "doi": "https://doi.org/10.1145/2950061",
    "publication_date": "2016-09-24",
    "publication_year": 2016,
    "authors": "Yeyong Pang; Shaojun Wang; Yu Peng; Xiyuan Peng; Nicholas J. Fraser; Philip H. W. Leong",
    "corresponding_authors": "",
    "abstract": "Kernel methods utilize linear methods in a nonlinear feature space and combine the advantages of both. Online kernel methods, such as kernel recursive least squares (KRLS) and kernel normalized least mean squares (KNLMS), perform nonlinear regression in a recursive manner, with similar computational requirements to linear techniques. In this article, an architecture for a microcoded kernel method accelerator is described, and high-performance implementations of sliding-window KRLS, fixed-budget KRLS, and KNLMS are presented. The architecture utilizes pipelining and vectorization for performance, and microcoding for reusability. The design can be scaled to allow tradeoffs between capacity, performance, and area. The design is compared with a central processing unit (CPU), digital signal processor (DSP), and Altera OpenCL implementations. In different configurations on an Altera Arria 10 device, our SW-KRLS implementation delivers floating-point throughput of approximately 16 GFLOPs, latency of 5.5μ S , and energy consumption of 10 − 4 J, these being improvements over a CPU by factors of 12, 17, and 24, respectively.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2524048463",
    "type": "article"
  },
  {
    "title": "Graph Minor Approach for Application Mapping on CGRAs",
    "doi": "https://doi.org/10.1145/2655242",
    "publication_date": "2014-08-01",
    "publication_year": 2014,
    "authors": "Liang Chen; Tulika Mitra",
    "corresponding_authors": "",
    "abstract": "Coarse-Grained Reconfigurable Arrays (CGRAs) exhibit high performance, improved flexibility, low cost, and power efficiency for various application domains. Compute-intensive loop kernels, which are perfect candidates to be executed on CGRAs, are usually mapped through modified modulo scheduling algorithms. These algorithms should be capable of performing both placement and routing. We formalize the CGRA mapping problem as a graph minor containment problem. We essentially test whether the dataflow graph representing the loop kernel is a minor of the modulo routing resource graph representing the CGRA resources and their interconnects. We design an exact graph minor testing approach that exploits the unique properties of both the dataflow graph and the routing resource graph to significantly prune the search space. We introduce additional heuristic strategies that drastically improve the compilation time while still generating optimal or near-optimal mapping solutions. Experimental evaluation confirms the efficiency of our approach.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4379390953",
    "type": "article"
  },
  {
    "title": "Detailed Placement for Dedicated LUT-Level FPGA Interconnect",
    "doi": "https://doi.org/10.1145/3501802",
    "publication_date": "2022-02-11",
    "publication_year": 2022,
    "authors": "Stefan Nikolić; Grace Zgheib; Paolo Ienne",
    "corresponding_authors": "",
    "abstract": "In this work, we develop timing-driven CAD support for FPGA architectures with direct connections between LUTs. We do so by proposing an efficient ILP-based detailed placer, which moves a carefully selected subset of LUTs from their original positions, so that connections of the user circuit can be appropriately aligned with the direct connections of the FPGA, reducing the circuit’s critical path delay. We discuss various aspects of making such an approach practicable, from efficient formulation of the integer programs themselves, to appropriate selection of the movable nodes. These careful considerations enable simultaneous movement of tens of LUTs with tens of candidate positions each, in a matter of minutes. In this manner, the impact of additional connections on the critical path delay more than doubles, compared to the previously reported results that relied solely on architecture-oblivious placement.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4211189781",
    "type": "article"
  },
  {
    "title": "FPGA Architecture Exploration for DNN Acceleration",
    "doi": "https://doi.org/10.1145/3503465",
    "publication_date": "2022-05-10",
    "publication_year": 2022,
    "authors": "Esther Roorda; S A Rasoulinezhad; Philip H. W. Leong; Steven J. E. Wilton",
    "corresponding_authors": "",
    "abstract": "Recent years have seen an explosion of machine learning applications implemented on Field-Programmable Gate Arrays (FPGAs) . FPGA vendors and researchers have responded by updating their fabrics to more efficiently implement machine learning accelerators, including innovations such as enhanced Digital Signal Processing (DSP) blocks and hardened systolic arrays. Evaluating architectural proposals is difficult, however, due to the lack of publicly available benchmark circuits. This paper addresses this problem by presenting an open-source benchmark circuit generator that creates realistic DNN-oriented circuits for use in FPGA architecture studies. Unlike previous generators, which create circuits that are agnostic of the underlying FPGA, our circuits explicitly instantiate embedded blocks, allowing for meaningful comparison of recent architectural proposals without the need for a complete inference computer-aided design (CAD) flow. Our circuits are compatible with the VTR CAD suite, allowing for architecture studies that investigate routing congestion and other low-level architectural implications. In addition to addressing the lack of machine learning benchmark circuits, the architecture exploration flow that we propose allows for a more comprehensive evaluation of FPGA architectures than traditional static benchmark suites. We demonstrate this through three case studies which illustrate how realistic benchmark circuits can be generated to target different heterogeneous FPGAs.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4229451071",
    "type": "article"
  },
  {
    "title": "FPGA-based Acceleration of Time Series Similarity Prediction: From Cloud to Edge",
    "doi": "https://doi.org/10.1145/3555810",
    "publication_date": "2022-08-12",
    "publication_year": 2022,
    "authors": "Amin Kalantar; Zachary Zimmerman; Philip Brisk",
    "corresponding_authors": "",
    "abstract": "With the proliferation of low-cost sensors and the Internet of Things, the rate of producing data far exceeds the compute and storage capabilities of today’s infrastructure. Much of this data takes the form of time series, and in response, there has been increasing interest in the creation of time series archives in the past decade, along with the development and deployment of novel analysis methods to process the data. The general strategy has been to apply a plurality of similarity search mechanisms to various subsets and subsequences of time series data to identify repeated patterns and anomalies; however, the computational demands of these approaches renders them incompatible with today’s power-constrained embedded CPUs. To address this challenge, we present FA-LAMP, an FPGA-accelerated implementation of the Learned Approximate Matrix Profile (LAMP) algorithm, which predicts the correlation between streaming data sampled in real-time and a representative time series dataset used for training. FA-LAMP lends itself as a real-time solution for time series analysis problems such as classification. We present the implementation of FA-LAMP on both edge- and cloud-based prototypes. On the edge devices, FA-LAMP integrates accelerated computation as close as possible to IoT sensors, thereby eliminating the need to transmit and store data in the cloud for posterior analysis. On the cloud-based accelerators, FA-LAMP can execute multiple LAMP models on the same board, allowing simultaneous processing of incoming data from multiple data sources across a network. LAMP employs a Convolutional Neural Network (CNN) for prediction. This work investigates the challenges and limitations of deploying CNNs on FPGAs using the Xilinx Deep Learning Processor Unit (DPU) and the Vitis AI development environment. We expose several technical limitations of the DPU, while providing a mechanism to overcome them by attaching custom IP block accelerators to the architecture. We evaluate FA-LAMP using a low-cost Xilinx Ultra96-V2 FPGA as well as a cloud-based Xilinx Alveo U280 accelerator card and measure their performance against a prototypical LAMP deployment running on a Raspberry Pi 3, an Edge TPU, a GPU, a desktop CPU, and a server-class CPU. In the edge scenario, the Ultra96-V2 FPGA improved performance and energy consumption compared to the Raspberry Pi; in the cloud scenario, the server CPU and GPU outperformed the Alveo U280 accelerator card, while the desktop CPU achieved comparable performance; however, the Alveo card offered an order of magnitude lower energy consumption compared to the other four platforms. Our implementation is publicly available at https://github.com/aminiok1/lamp-alveo.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4291142686",
    "type": "article"
  },
  {
    "title": "ADAS: <u>A</u> High Computational Utilization <u>D</u> ynamic Reconfigurable Hardware <u>A</u> ccelerator for <u>S</u> uper Resolution",
    "doi": "https://doi.org/10.1145/3570927",
    "publication_date": "2022-11-07",
    "publication_year": 2022,
    "authors": "Liang Chang; Xin Zhao; Jun Zhou",
    "corresponding_authors": "",
    "abstract": "Super-resolution (SR) based on deep learning has obtained superior performance in image reconstruction. Recently, various algorithm efforts have been committed to improving image reconstruction quality and speed. However, the inference of SR contains huge amounts of computation and data access, leading to low hardware implementation efficiency. For instance, the up-sampling with the deconvolution process requires considerable computation resources. In addition, the sizes of output feature maps of several middle layers are extraordinarily large, which is challenging to optimize, causing serious data access issues. In this work, we present an all-on-chip hardware architecture based on the deconvolution scheme and feature map segmentation strategy, namely ADAS, where all the generated data by the middle layers are buffered on-chip to avoid large data movements between on- and off-chip. In ADAS, we develop a hardware-friendly and efficient deconvolution scheme to accelerate the computation. Also, the dynamic reconfigurable process element (PE) combined with efficient mapping is proposed to enhance PE utilization up to nearly 100% and support multiple scaling factors. Based on our experimental results, ADAS demonstrates real-time image SR and better image reconstruction quality with PSNR (37.15 dB ) and SSIM (0.9587). Compared to baseline and validated with the FPGA platform, ADAS can support scaling factors of 2, 3, and 4, achieving 2.68 ×, 5.02 ×, and 8.28 × speedup.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4308391987",
    "type": "article"
  },
  {
    "title": "Implementation Approaches Trade-Offs for WiMax OFDM Functions on Reconfigurable Platforms",
    "doi": "https://doi.org/10.1145/1839480.1839482",
    "publication_date": "2010-09-01",
    "publication_year": 2010,
    "authors": "Ahmad Sghaier; Shawki Areibi; R.D. Dony",
    "corresponding_authors": "",
    "abstract": "This work investigates several approaches for implementing the OFDM functions of the fixed-WiMax standard on reconfigurable platforms. In the first phase, a custom RTL approach, using VHDL, is investigated. The approach shows the capability of a medium-size FPGA to accommodate the OFDM functions of a fixed-WiMax transceiver with only 50% occupation rate. In the second phase, a high-level approach based on the AccelDSP tool is used and compared to the custom RTL approach. The approach presents an easy flow to transfer MATLAB floating-point code into synthesizable cores. The AccelDSP approach shows an area overhead of 10%, while allowing early architectural exploration and accelerating the design time by a factor of two. However, the performance figure obtained is almost 1/4 of that obtained in the custom RTL approach. In the third phase, the Tensilica Xtensa configurable processor is targeted, which presents remarkable figures in terms of power, area, and design time. Comparing the three approaches indicates that the custom RTL approach has the lead in terms of performance. However, both the AccelDSP and the Tensilica Xtensa approaches show fast design time and early architectural exploration capability. In terms of power, the obtained estimation results show that the configurable Xtensa processor approach has the lead, where approximately the total power consumed is about 12--15 times less than those results obtained by the other two approaches.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2031152040",
    "type": "article"
  },
  {
    "title": "Merged Dictionary Code Compression for FPGA Implementation of Custom Microcoded PEs",
    "doi": "https://doi.org/10.1145/1371579.1371583",
    "publication_date": "2008-06-01",
    "publication_year": 2008,
    "authors": "Bita Gorjiara; Mehrdad Reshadi; Daniel D. Gajski",
    "corresponding_authors": "",
    "abstract": "Horizontal Microcoded Architecture (HMA) is a paradigm for designing programmable high-performance processing elements (PEs). However, it suffers from large code size, which can be addressed by compression. In this article, we study the code size of one of the new HMA-based technologies called No-Instruction-Set Computer (NISC). We show that NISC code size can be several times larger than a typical RISC processor, and we propose several low-overhead dictionary-based code compression techniques to reduce its code size. Our compression algorithm leverages the knowledge of “don't care” values in the control words and can reduce the code size by 3.3 times, on average. Despite such good results, as shown in this article, these compression techniques lead to poor FPGA implementations because they require many on-chip RAMs. To address this issue, we introduce an FPGA-aware dictionary-based technique that uses the dual-port feature of on-chip RAMs to reduce the number of utilized block RAMs by half. Additionally, we propose cascading two-levels of dictionaries for code size and block RAM reduction of large programs. For an MP3 application, a merged, cascaded, three-dictionary implementation reduces the number of utilized block RAMs by 4.3 times (76%) compared to a NISC without compression. This corresponds to 20% additional savings over the best single level dictionary-based compression.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2083324936",
    "type": "article"
  },
  {
    "title": "A Scalable and Programmable Modular Traffic Manager Architecture",
    "doi": "https://doi.org/10.1145/1968502.1968505",
    "publication_date": "2011-05-01",
    "publication_year": 2011,
    "authors": "Shane O’Neill; Roger Woods; Alan Marshall; Qi Zhang",
    "corresponding_authors": "",
    "abstract": "A key issue in the design of next-generation Internet routers and switches will be provision of Traffic Manager (TM) functionality in the datapaths of their high-speed switching fabrics. A new architecture that allows dynamic deployment of different TM functions is presented. By considering the processing requirements of operations such as policing and congestion, queuing, shaping, and scheduling, a solution has been derived that is scalable with a consistent programmable interface. Programmability is achieved using a function computation unit which determines the action (e.g., drop, queue, remark, forward) based on the packet attribute information and a memory storage part. Results of a Xilinx Virtex-5 FPGA reference design are presented.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W1971597759",
    "type": "article"
  },
  {
    "title": "Application-specific signatures for transactional memory in soft processors",
    "doi": "https://doi.org/10.1145/2000832.2000833",
    "publication_date": "2011-08-01",
    "publication_year": 2011,
    "authors": "Martin Labrecque; Mark C. Jeffrey; J. Gregory Steffan",
    "corresponding_authors": "",
    "abstract": "As reconfigurable computing hardware and in particular FPGA-based systems-on-chip comprise an increasing number of processor and accelerator cores, supporting sharing and synchronization in a way that is scalable and easy to program becomes a challenge. Transactional Memory (TM) is a potential solution to this problem, and an FPGA-based system provides the opportunity to support TM in hardware (HTM). Although there are many proposed approaches to HTM support for ASICs, these do not necessarily map well to FPGAs. In particular in this work we demonstrate that while signature -based conflict detection schemes (essentially bit-vectors) should intuitively be a good match to the bit parallelism of FPGAs, previous approaches result in unacceptable multicycle stalls, operating frequencies, or false-conflict rates. Capitalizing on the reconfigurable nature of FPGA-based systems, we propose an application-specific signature mechanism for HTM conflict detection. Our evaluation uses real and projected FPGA-based soft multiprocessor systems that support HTM and implement threaded, shared-memory network packet processing applications. We find that our application-specific approach: (i) maintains a reasonable operating frequency of 125 MHz, (ii) achieves a 9% to 71% increase in packet throughput relative to signatures with bit selection on a 2-thread architecture, and (iii) allows our HTM to achieve 6%, 54%, and 57% increases in packet throughput on an 8-thread architecture versus a baseline lock-based synchronization for three of four packet processing applications studied, due to reduced false synchronization.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2000625430",
    "type": "article"
  },
  {
    "title": "An FPGA-based accelerator for LambdaRank in Web search engines",
    "doi": "https://doi.org/10.1145/2000832.2000837",
    "publication_date": "2011-08-01",
    "publication_year": 2011,
    "authors": "Jing Yan; Ningyi Xu; Xiongfei Cai; Rui Gao; Yu Wang; Rong Luo; Feng-hsiung Hsu",
    "corresponding_authors": "",
    "abstract": "In modern Web search engines, Neural Network (NN)-based learning to rank algorithms is intensively used to increase the quality of search results. LambdaRank is one such algorithm. However, it is hard to be efficiently accelerated by computer clusters or GPUs, because: (i) the cost function for the ranking problem is much more complex than that of traditional Back-Propagation(BP) NNs, and (ii) no coarse-grained parallelism exists in the algorithm. This article presents an FPGA-based accelerator solution to provide high computing performance with low power consumption. A compact deep pipeline is proposed to handle the complex computing in the batch updating. The area scales linearly with the number of hidden nodes in the algorithm. We also carefully design a data format to enable streaming consumption of the training data from the host computer. The accelerator shows up to 15.3X (with PCIe x4) and 23.9X (with PCIe x8) speedup compared with the pure software implementation on datasets from a commercial search engine.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2081584581",
    "type": "article"
  },
  {
    "title": "Optimized System-on-Chip Integration of a Programmable ECC Coprocessor",
    "doi": "https://doi.org/10.1145/1857927.1857933",
    "publication_date": "2010-12-01",
    "publication_year": 2010,
    "authors": "Xu Guo; Patrick Schaumont",
    "corresponding_authors": "",
    "abstract": "Most hardware/software (HW/SW) codesigns of Elliptic Curve Cryptography have focused on the computational aspect of the ECC hardware, and not on the system integration into a System-on-Chip (SoC) architecture. We study the impact of the communication link between CPU and coprocessor hardware for a typical ECC design, and demonstrate that the SoC may become performance-limited due to coprocessor data- and instruction-transfers. A dual strategy is proposed to remove the bottleneck: introduction of control hierarchy as well as local storage. The performance of the ECC coprocessor can be almost independent of the selection of bus protocols. Besides performance, the proposed ECC coprocessor is also optimized for scalability. Using design space exploration of a large number of system configurations of different architectures, our proposed ECC coprocessor architecture enables trade-offs between area, speed, and security.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2093198099",
    "type": "article"
  },
  {
    "title": "JITPR",
    "doi": "https://doi.org/10.1145/2492185",
    "publication_date": "2013-07-01",
    "publication_year": 2013,
    "authors": "Harry Sidiropoulos; Kostas Siozios; Peter Figuli; Dimitrios Soudris; Michael Hübner; Jürgen Becker",
    "corresponding_authors": "",
    "abstract": "The execution runtime usually is a headache for designers performing application mapping onto reconfigurable architectures. In this article we propose a methodology, as well as the supporting toolset, targeting to provide fast application implementation onto reconfigurable architectures with the usage of a Just-In-Time (JIT) compilation framework. Experimental results prove the efficiency of the introduced framework, as we reduce the execution runtime compared to the state-of-the-art approach on average by 53.5×. Additionally, the derived solutions achieve higher operation frequencies by 1.17×, while they also exhibit significant lower fragmentation ratios of hardware resources.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2104223655",
    "type": "article"
  },
  {
    "title": "Framework for Rapid Performance Estimation of Embedded Soft Core Processors",
    "doi": "https://doi.org/10.1145/3195801",
    "publication_date": "2018-06-30",
    "publication_year": 2018,
    "authors": "Deshya Wijesundera; Alok Prakash; Thambipillai Srikanthan; Achintha Ihalage",
    "corresponding_authors": "",
    "abstract": "The large number of embedded soft core processors available today make it tedious and time consuming to select the best processor for a given application. This task is even more challenging due to the numerous configuration options available for a single soft core processor while optimizing for contradicting design requirements such as performance and area. In this article, we propose a generic framework for rapid performance estimation of applications on soft core processors. The proposed technique is scalable to the large number of configuration options available in modern soft core processors by relying on rapid and accurate estimation models instead of time-consuming FPGA synthesis and execution-based techniques. Experimental results on two leading commercial soft core processors executing applications from the widely used CHStone benchmark suite show an average error of less than 6% while running in the order of minutes when compared to hours taken by synthesis-based techniques.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2883414960",
    "type": "article"
  },
  {
    "title": "Novel Congestion-estimation and Routability-prediction Methods based on Machine Learning for Modern FPGAs",
    "doi": "https://doi.org/10.1145/3337930",
    "publication_date": "2019-08-13",
    "publication_year": 2019,
    "authors": "Abeer Al-Hyari; Ziad Abuowaimer; Timothy Martin; Gary Gréwal; Shawki Areibi; Anthony Vannelli",
    "corresponding_authors": "",
    "abstract": "Effectively estimating and managing congestion during placement can save substantial placement and routing runtime. In this article, we present a machine-learning model for accurately and efficiently estimating congestion during FPGA placement. Compared with the state-of-the-art machine-learning congestion-estimation model, our results show a 25% improvement in prediction accuracy. This makes our model competitive with congestion estimates produced using a global router. However, our model runs, on average, 291× faster than the global router. Overall, we are able to reduce placement runtimes by 17% and router runtimes by 19%. An additional machine-learning model is also presented that uses the output of the first congestion-estimation model to determine whether or not a placement is routable. This second model has an accuracy in the range of 93% to 98%, depending on the classification algorithm used to implement the learning model, and runtimes of a few milliseconds, thus making it suitable for inclusion in any placer with no worry of additional computational overhead.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2967974643",
    "type": "article"
  },
  {
    "title": "DSL-Based Hardware Generation with Scala",
    "doi": "https://doi.org/10.1145/3359754",
    "publication_date": "2019-12-19",
    "publication_year": 2019,
    "authors": "François Serre; Markus Püschel",
    "corresponding_authors": "",
    "abstract": "We present a hardware generator for computations with regular structure including the fast Fourier transform (FFT), sorting networks, and others. The input of the generator is a high-level description of the algorithm; the output is a token-based, synchronized design in the form of RTL-Verilog. Building on prior work, the generator uses several layers of domain-specific languages (DSLs) to represent and optimize at different levels of abstraction to produce a RAM- and area-efficient hardware implementation. Two of these layers and DSLs are novel. The first one allows the use and domain-specific optimization of state-of-the-art streaming permutations. The second DSL enables the automatic pipelining of a streaming hardware dataflow and the synchronization of its data-independent control signals. The generator including the DSLs are implemented in Scala, leveraging its type system, and uses concepts from lightweight modular staging (LMS) to handle the constraints of streaming hardware. Particularly, these concepts offer genericity over hardware number representation, including seamless switching between fixed-point arithmetic and FloPoCo generated IEEE floating-point operators, while ensuring type-safety. We show benchmarks of generated FFTs, sorting networks, and Walsh-Hadamard transforms that outperform prior generators.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2995522819",
    "type": "article"
  },
  {
    "title": "Partitioning and Scheduling with Module Merging on Dynamic Partial Reconfigurable FPGAs",
    "doi": "https://doi.org/10.1145/3403702",
    "publication_date": "2020-08-21",
    "publication_year": 2020,
    "authors": "Qi Tang; Zhe Wang; Biao Guo; Li Zhu; Jibo Wei",
    "corresponding_authors": "",
    "abstract": "Field programmable gate array (FPGA) is ubiquitous nowadays and is applied to many areas. Dynamic partial reconfiguration (DPR) is introduced to most modern FPGAs, enabling changing the function of a part of the FPGA by dynamically loading new bitstreams to the logic regions without affecting the function of other parts of the FPGA. However, delivering the powerful capacity of the DPR FPGA to the user depends on the efficient partitioning and scheduling technology. This article proposes the module merging technique for the partitioning and scheduling problem to reduce the reconfiguration overhead and improve the schedule performance. An exact approach based on the integer linear programming (ILP) for the partitioning and scheduling problem with module merging is proposed. The ILP-based approach is capable of solving the problem optimally, and can be used to further improve the performance of schedules produced by other non-optimal algorithms; however, it is time-consuming to solve large-scale problems. Therefore, a K-sliced-ILP algorithm based on the methodology of divide-and-conquer is proposed, which is able to reduce the time complexity significantly with the solution quality being degraded marginally. Experiments are carried out with a set of real-life applications, and the result demonstrates the effectiveness of the proposed methods.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3080137974",
    "type": "article"
  },
  {
    "title": "An OpenGL Compliant Hardware Implementation of a Graphic Processing Unit Using Field Programmable Gate Array–System on Chip Technology",
    "doi": "https://doi.org/10.1145/3410357",
    "publication_date": "2020-09-02",
    "publication_year": 2020,
    "authors": "Alexander E. Beasley; Christopher T. Clarke; Robert J. Watson",
    "corresponding_authors": "",
    "abstract": "FPGA-SoC technology provides a heterogeneous platform for advanced, high-performance systems. The System on Chip (SoC) architecture combines traditional single and multiple core processor topologies with flexible FPGA fabric. Dynamic reconfiguration allows the hardware accelerators to be changed at run-time. This article presents a novel OpenGL compliant GPU design implemented on an FPGA. The design uses an FPGA-SoC environment allowing the embedded processor to offload graphics operation onto a more suitable architecture. To the authors’ knowledge, this is a first. The graphics processor consists of GLSL compliant shaders, an efficient Barycentric Rasterizer, and a draw mode manager. Performance analysis shows the throughput of the shaders to be hundreds of millions of vertices per second. The design uses both pipelining and resource reuse to optimise throughput and resource use, allowing implementation on a low-cost, FPGA device. Pixel processing rates from this implementation are almost 80% higher than other FPGA implementations. Power consumption compared with comparative embedded devices shows the FPGA consuming as little as 2% of the power of a Mali device, and an up to 11.9-fold increase in efficiency compared to an Nvidia RTX 2060 - Turing architecture device.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3082314516",
    "type": "article"
  },
  {
    "title": "JITPR",
    "doi": "https://doi.org/10.1145/2499625.2492185",
    "publication_date": "2013-07-01",
    "publication_year": 2013,
    "authors": "Harry Sidiropoulos; Kostas Siozios; Peter Figuli; Dimitrios Soudris; Michael Hübner; Jürgen Becker",
    "corresponding_authors": "",
    "abstract": "The execution runtime usually is a headache for designers performing application mapping onto reconfigurable architectures. In this article we propose a methodology, as well as the supporting toolset, targeting to provide fast application implementation onto reconfigurable architectures with the usage of a Just-In-Time (JIT) compilation framework. Experimental results prove the efficiency of the introduced framework, as we reduce the execution runtime compared to the state-of-the-art approach on average by 53.5×. Additionally, the derived solutions achieve higher operation frequencies by 1.17×, while they also exhibit significant lower fragmentation ratios of hardware resources.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4255848313",
    "type": "article"
  },
  {
    "title": "A Self-Aware Tuning and Self-Aware Evaluation Method for Finite-Difference Applications in Reconfigurable Systems",
    "doi": "https://doi.org/10.1145/2617598",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "Xinyu Niu; Qiwei Jin; Wayne Luk; Stephen Weston",
    "corresponding_authors": "",
    "abstract": "Finite-difference methods are computationally intensive and required by many applications. Parameters of a finite-difference algorithm, such as grid size, can be varied to generate design space which contains algorithm instances with different constant coefficients. An algorithm instance with specific coefficients can either be mapped into general operators to construct static designs, or be implemented as constant-specific operators to form dynamic designs, which require runtime reconfiguration to update algorithm coefficients. This article proposes a tuning method to explore the design space to optimise both the static and the dynamic designs, and an evaluation method to select the design with maximum overall throughput, based on algorithm characteristics, design properties, available resources and runtime data size. For benchmark applications option pricing and Reverse-Time Migration (RTM), over 50% reduction in resource consumption has been achieved for both static designs and dynamic designs, while meeting precision requirements. For a single hardware implementation, the RTM design optimised with the proposed approach is expected to run 1.8 times faster than the best published design. The tuned static designs run thousands of times faster than the dynamic designs for algorithms with small data size, while the tuned dynamic designs achieve up to 5.9 times speedup over the corresponding static designs for large-scale finite-difference algorithms.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2165984413",
    "type": "article"
  },
  {
    "title": "Stream Aggregation with Compressed Sliding Windows",
    "doi": "https://doi.org/10.1145/3590774",
    "publication_date": "2023-04-05",
    "publication_year": 2023,
    "authors": "Prajith Ramakrishnan Geethakumari; Ioannis Sourdis",
    "corresponding_authors": "",
    "abstract": "High performance stream aggregation is critical for many emerging applications that analyze massive volumes of data. Incoming data needs to be stored in a sliding window during processing, in case the aggregation functions cannot be computed incrementally. Updating the window with new incoming values and reading it to feed the aggregation functions are the two primary steps in stream aggregation. Although window updates can be supported efficiently using multi-level queues, frequent window aggregations remain a performance bottleneck as they put tremendous pressure on the memory bandwidth and capacity. This article addresses this problem by enhancing StreamZip, a dataflow stream aggregation engine that is able to compress the sliding windows. StreamZip deals with a number of data and control dependency challenges to integrate a compressor in the stream aggregation pipeline and alleviate the memory pressure posed by frequent aggregations. In addition, StreamZip incorporates a caching mechanism for dealing with skewed-key distributions in the incoming data stream. In doing so, StreamZip offers higher throughput as well as larger effective window capacity to support larger problems. StreamZip supports diverse compression algorithms offering both lossless and lossy compression to integers as well as floating-point numbers. Compared to designs without compression, StreamZip lossless and lossy designs achieve up to 7.5× and 22× higher throughput, while improving the effective memory capacity by up to 5× and 23×, respectively.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4362610202",
    "type": "article"
  },
  {
    "title": "Parallelising Control Flow in Dynamic-scheduling High-level Synthesis",
    "doi": "https://doi.org/10.1145/3599973",
    "publication_date": "2023-05-31",
    "publication_year": 2023,
    "authors": "Jianyi Cheng; Lana Josipović; John Wickerson; George A. Constantinides",
    "corresponding_authors": "",
    "abstract": "Recently, there is a trend to use high-level synthesis (HLS) tools to generate dynamically scheduled hardware. The generated hardware is made up of components connected using handshake signals. These handshake signals schedule the components at runtime when inputs become available. Such approaches promise superior performance on “irregular” source programs, such as those whose control flow depends on input data. This is at the cost of additional area. Current dynamic scheduling techniques are well able to exploit parallelism among instructions within each basic block (BB) of the source program, but parallelism between BBs is under-explored, due to the complexity in runtime control flows and memory dependencies. Existing tools allow some of the operations of different BBs to overlap, but to simplify the analysis required at compile time they require the BBs to start in strict program order, thus limiting the achievable parallelism and overall performance. We formulate a general dependency model suitable for comparing the ability of different dynamic scheduling approaches to extract maximal parallelism at runtime. Using this model, we explore a variety of mechanisms for runtime scheduling, incorporating and generalising existing approaches. In particular, we precisely identify the restrictions in existing scheduling implementation and define possible optimisation solutions. We identify two particularly promising examples where the compile-time overhead is small and the area overhead is minimal and yet we are able to significantly speed up execution time: (1) parallelising consecutive independent loops; and (2) parallelising independent inner-loop instances in a nested loop as individual threads. Using benchmark sets from related works, we compare our proposed toolflow against a state-of-the-art dynamic-scheduling HLS tool called Dynamatic. Our results show that, on average, our toolflow yields a 4× speedup from (1) and a 2.9× speedup from (2), with a negligible area overhead. This increases to a 14.3× average speedup when combining (1) and (2).",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4378835952",
    "type": "article"
  },
  {
    "title": "Automation Schemes for FPGA Implementation of Wave-Pipelined Circuits",
    "doi": "https://doi.org/10.1145/1534916.1534921",
    "publication_date": "2009-06-01",
    "publication_year": 2009,
    "authors": "G. Seetharaman; B. Venkataramani",
    "corresponding_authors": "",
    "abstract": "Operating frequencies of combinational logic circuits can be increased using Wave-Pipelining (WP), by adjusting the clock periods and clock skews. In this article, Built-In Self-Test (BIST) and System-on-Chip (SOC) approaches are proposed for automating this adjustment and they are evaluated by implementation of filters using a Distributed Arithmetic Algorithm (DAA) and sinewave generator using the COordinate Rotation DIgital Computer (CORDIC). Both the circuits are studied by adopting three schemes: wave-pipelining, pipelining, and nonpipelining. Xilinx Spartan II and Altera Cyclone II FPGAs with Nios II soft-core processor are used for implementation of the circuits with the BIST and SOC approaches, respectively. The proposed schemes increase the speed of the WP circuits by a factor of 1.19--2.6 compared to nonpipelined circuits. The pipelined circuits achieve higher speed than the WP circuits by a factor of 1.13--3.27 at the cost of increase in area and power. When both pipelined and WP circuits are operated at the same frequency, the former dissipates more power for circuits with higher word sizes and for moderate logic depths. The observation regarding the dependence of the superiority of the WP circuits with regard to power dissipation on the logic depth is one of the major contributions of this article.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2002920847",
    "type": "article"
  },
  {
    "title": "From Silicon to Science",
    "doi": "https://doi.org/10.1145/1575779.1575786",
    "publication_date": "2009-09-01",
    "publication_year": 2009,
    "authors": "Keith D. Underwood; K. Scott Hemmert; Craig Ulmer",
    "corresponding_authors": "",
    "abstract": "The field of high performance computing (HPC) currently abounds with excitement about the potential of a broad class of things called accelerators . And, yet, few accelerator based systems are being deployed in general purpose HPC environments. Why is that? This article explores the challenges that accelerators face in the HPC world, with a specific focus on FPGA based systems. We begin with an overview of the characteristics and challenges of typical HPC systems and applications and discuss why FPGAs have the potential to have a significant impact. The bulk of the article is focused on twelve specific areas where FPGA researchers can make contributions to hasten the adoption of FPGAs in HPC environments.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2025089496",
    "type": "article"
  },
  {
    "title": "Static and Dynamic Memory Footprint Reduction for FPGA Routing Algorithms",
    "doi": "https://doi.org/10.1145/1462586.1462587",
    "publication_date": "2009-01-01",
    "publication_year": 2009,
    "authors": "Scott Y. L. Chin; Steven J. E. Wilton",
    "corresponding_authors": "",
    "abstract": "This article presents techniques to reduce the static and dynamic memory requirements of routing algorithms that target field-programmable gate arrays. During routing, memory is required to store both architectural data and temporary routing data. The architectural data is static, and provides a representation of the physical routing resources and programmable connections on the device. We show that by taking advantage of the regularity in FPGAs, we can reduce the amount of information that must be explicitly represented, leading to significant memory savings. The temporary routing data is dynamic, and contains scoring parameters and traceback information for each routing resource in the FPGA. By studying the lifespan of the temporary routing data objects, we develop several memory management schemes to reduce this component. To make our proposals concrete, we applied them to the routing algorithm in VPR and empirically quantified the impact on runtime memory footprint, and place and route time.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2102892811",
    "type": "article"
  },
  {
    "title": "Buffer Placement and Sizing for High-Performance Dataflow Circuits",
    "doi": "https://doi.org/10.1145/3477053",
    "publication_date": "2021-11-09",
    "publication_year": 2021,
    "authors": "Lana Josipović; Shabnam Sheikhha; Andrea Guerrieri; Paolo Ienne; Jordi Cortadella",
    "corresponding_authors": "",
    "abstract": "Commercial high-level synthesis tools typically produce statically scheduled circuits. Yet, effective C-to-circuit conversion of arbitrary software applications calls for dataflow circuits, as they can handle efficiently variable latencies (e.g., caches), unpredictable memory dependencies, and irregular control flow. Dataflow circuits exhibit an unconventional property: registers (usually referred to as “buffers”) can be placed anywhere in the circuit without changing its semantics, in strong contrast to what happens in traditional datapaths. Yet, although functionally irrelevant, this placement has a significant impact on the circuit’s timing and throughput. In this work, we show how to strategically place buffers into a dataflow circuit to optimize its performance. Our approach extracts a set of choice-free critical loops from arbitrary dataflow circuits and relies on the theory of marked graphs to optimize the buffer placement and sizing. Our performance optimization model supports important high-level synthesis features such as pipelined computational units, units with variable latency and throughput, and if-conversion. We demonstrate the performance benefits of our approach on a set of dataflow circuits obtained from imperative code.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3211800879",
    "type": "article"
  },
  {
    "title": "Scientific Application Demands on a Reconfigurable Functional Unit Interface",
    "doi": "https://doi.org/10.1145/1968502.1968510",
    "publication_date": "2011-05-01",
    "publication_year": 2011,
    "authors": "Kyle Rupnow; Keith D. Underwood; Katherine Compton",
    "corresponding_authors": "",
    "abstract": "Modern scientific applications are large, complex, and highly parallel they are commonly executed on supercomputers with tens of thousands of processors. Yet these applications still commonly require weeks or even months to execute. Thus, single-thread performance remains a concern for highly parallel scientific applications. Adding a reconfigurable accelerator to each CPU could improve system performance; however, scientific applications have design constraints that differ from most application domains commonly accelerated by reconfigurable logic. In this article, we discuss the constraints imposed by scientific applications on the computation model, the accelerator architecture, and the accelerator’s communication interface with the CPU. Based on these constraints and application analysis, we have previously proposed adding a Reconfigurable Functional Unit (RFU) to accelerate integer graphs that calculate complex memory addresses. In this work, we now propose a flexible multi-instruction interface technique that allows dataflow graphs implemented on the RFU to access a large number of inputs and outputs with minor CPU datapath modifications. We present an in-depth examination of the performance effects of different communication interfaces that use this technique, and select one that best matches the needs of Sandia’s scientific applications. Although RFU execution overall improves performance, we also isolate two key negative performance effects introduced by aggregating CPU instructions into dataflow graphs: delayed issue and graph serialization. Finally, to demonstrate the marketability of an RFU beyond scientific applications, we reanalyze the proposed interfaces using the SPEC-fp benchmark suite. We show that although choosing an interface based on SPEC-fp needs is detrimental to Sandia application performance, choosing an interface based on Sandia demands works well for more general-purpose applications.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W1999617969",
    "type": "article"
  },
  {
    "title": "Enabling Adaptive Techniques in Heterogeneous MPSoCs Based on Virtualization",
    "doi": "https://doi.org/10.1145/2362374.2362381",
    "publication_date": "2012-10-01",
    "publication_year": 2012,
    "authors": "Luciano Ost; Sameer Varyani; Leandro Soares Indrusiak; Marcelo Mandelli; Gabriel Marchesan Almeida; Eduardo Wächter; Fernando Moraes; Gilles Sassatelli",
    "corresponding_authors": "",
    "abstract": "This article explores the use of virtualization to enable mechanisms like task migration and dynamic mapping in heterogeneous MPSoCs, thereby targeting the design of systems capable of adapt their behavior to time-changing workloads. Because tasks may have to be mapped to target processors with different instruction set architectures, we propose the use of Low Level Virtual Machine (LLVM) to postcompile the tasks at runtime depending on their target processor. A novel dynamic mapping heuristic is also proposed, aiming to exploit the advantages of specialized processors while taking into account the overheads imposed by virtualization. Extensive experimental work at different levels of abstraction---FPGA prototype, RTL and system-level simulation---is presented to evaluate the proposed techniques.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2014877554",
    "type": "article"
  },
  {
    "title": "SHMEM+",
    "doi": "https://doi.org/10.1145/2000832.2000838",
    "publication_date": "2011-08-01",
    "publication_year": 2011,
    "authors": "Vikas A. Aggarwal; Alan D. George; Changil Yoon; Kishore K. Yalamanchili; Herman Lam",
    "corresponding_authors": "",
    "abstract": "Reconfigurable Computing (RC) systems based on FPGAs are becoming an increasingly attractive solution to building parallel systems of the future. Applications targeting such systems have demonstrated superior performance and reduced energy consumption versus their traditional counterparts based on microprocessors. However, most of such work has been limited to small system sizes. Unlike traditional HPC systems, lack of integrated, system-wide, parallel-programming models and languages presents a significant design challenge for creating applications targeting scalable, reconfigurable HPC systems. In this article, we extend the traditional Partitioned Global Address Space (PGAS) model to provide a multilevel integration of memory, which simplifies development of parallel applications for such systems and improves developer productivity. The new multilevel-PGAS programming model captures the unique characteristics of reconfigurable HPC systems, such as the existence of multiple levels of memory hierarchy and heterogeneous computation resources. Based on this model, we extend and adapt the SHMEM communication library to become what we call SHMEM+, the first known SHMEM library enabling coordination between FPGAs and CPUs in a reconfigurable, heterogeneous HPC system. Applications designed with SHMEM+ yield improved developer productivity compared to current methods of multidevice RC design and exhibit a high degree of portability. In addition, our design of SHMEM+ library itself is portable and provides peak communication bandwidth comparable to vendor-proprietary versions of SHMEM. Application case studies are presented to illustrate the advantages of SHMEM+.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2019957762",
    "type": "article"
  },
  {
    "title": "Towards development of an analytical model relating FPGA architecture parameters to routability",
    "doi": "https://doi.org/10.1145/2499625.2499627",
    "publication_date": "2013-07-01",
    "publication_year": 2013,
    "authors": "Joydip Das; Steven J. E. Wilton",
    "corresponding_authors": "",
    "abstract": "We present an analytical model relating FPGA architectural parameters to the routability of the FPGA. The inputs to the model include the channel width and the connection and the switch block flexibilities. The output is an estimate of the proportion of nets in a large circuit that can be expected to be successfully routed on the FPGA. We assume that the circuit is routed to the FPGA using a single-step combined global/detailed router. We show that the model correctly predicts routability trends. We also present an example application to demonstrate that this model may be a valuable tool for FPGA architects. When combined with the earlier works on analytical modeling, our model can be used to quickly predict the routability without going through any stage of an expensive CAD flow. We envisage that this model will benefit FPGA architecture designers and vendors to quickly evaluate FPGA routing fabrics.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2024174517",
    "type": "article"
  },
  {
    "title": "Self-Reconfigurable Constant Multiplier for FPGA",
    "doi": "https://doi.org/10.1145/2490830",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Javier Hormigo; Gabriel Caffarena; Juan Pablo Oliver; Eduardo Boemo",
    "corresponding_authors": "",
    "abstract": "Constant multipliers are widely used in signal processing applications to implement the multiplication of signals by a constant coefficient. However, in some applications, this coefficient remains invariable only during an interval of time, and then, its value changes to adapt to new circumstances. In this article, we present a self-reconfigurable constant multiplier suitable for LUT-based FPGAs able to reload the constant in runtime. The pipelined architecture presented is easily scalable to any multiplicand and constant sizes, for unsigned and signed representations. It can be reprogrammed in 16 clock cycles, equivalent to less than 100 ns in current FPGAs. This value is significantly smaller than FPGA partial configuration times. The presented approach is more efficient in terms of area and speed when compared to generic multipliers, achieving up to 91% area reduction and up to 102% speed improvement for the case-study circuits tested. The power consumption of the proposed multipliers are in the range of those of slice-based multipliers provided by the vendor.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2033372705",
    "type": "article"
  },
  {
    "title": "Exploiting data-level parallelism for energy-efficient implementation of LDPC decoders and DCT on an FPGA",
    "doi": "https://doi.org/10.1145/2068716.2068723",
    "publication_date": "2011-12-01",
    "publication_year": 2011,
    "authors": "Xiaoheng Chen; Venkatesh Akella",
    "corresponding_authors": "",
    "abstract": "We explore the use of Data-Level Parallelism (DLP) as a way of improving the energy efficiency and power consumption involved in running applications on an FPGA. We show that static power consumption is a significant fraction of the overall power consumption in an FPGA and that it does not change significantly even as the area required by an architecture increases, because of the dominance of interconnect in an FPGA. We show that the degree of DLP can be used in conjunction with frequency scaling to reduce the overall power consumption.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2084671567",
    "type": "article"
  },
  {
    "title": "Design and analysis of adaptive processor",
    "doi": "https://doi.org/10.1145/2133352.2133357",
    "publication_date": "2012-03-01",
    "publication_year": 2012,
    "authors": "Shigeyuki Takano",
    "corresponding_authors": "Shigeyuki Takano",
    "abstract": "A new computation model called CACHE (Cache Architecture for Configurable Hardware Engine) is proposed in this paper. This model does not require a dedicated host processor and its software to harness the reconfiguration. Autonomous reconfiguration is performed within a working-set of application datapaths. The CACHE model has lots of side effects; caching, resource allocation and assignment, placement and routing, and defragmentation, with a processing array itself and a special register called a working-set register file. The model aims to reduce three major workloads: (1) the processor and application design workload, (2) runtime resource management and scheduling workload, and (3) reconfiguration workload. In order to reduce these workloads, processor architecture is definitely different from traditional computing model and its microprocessor architecture. There are three major ideas to construct the computing system: (1) an on-chip working-set model mainly in order to control load and store of streams, namely to control traffics introducing overheads, (2) an on-chip deadlock properties model mainly in order to manage resources and to continuously configure datapaths corresponding to a working-set window, (3) a cache memory technique to work for these models, the mechanism is equivalent to the working-set window, and the cache memory's procedure is equivalent to resource request, acquirement, and release of deadlock properties. The first model focuses onto streaming applications, for example vector and matrix operations, filters, and so on, which takes coarser grained operations such as integer operations of C-language. Regarding performance compared with DSPs, that comes from constant throughput across different scale of the applications. In addition, extended model, we call Instant model that automatically generates instance of a datapath, outperforms the DSPs. This paper shows its computation model, architecture, low-level design, and analyses about basic characteristics of the execution.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2117005698",
    "type": "article"
  },
  {
    "title": "A SDM-TDM-Based Circuit-Switched Router for On-Chip Networks",
    "doi": "https://doi.org/10.1145/2362374.2362379",
    "publication_date": "2012-10-01",
    "publication_year": 2012,
    "authors": "Angelo Kuti Lusala; Jean-Didier Legat",
    "corresponding_authors": "",
    "abstract": "This article proposes a circuit-switched router that combines Spatial Division Multiplexing (SDM) and Time Division Multiplexing (TDM) in order to increase path diversity in the router while sharing channels among multiple connections. In this way, the probability of establishing paths through the network is increased, thereby significantly reducing contention in the network. Furthermore, Quality of Service (QoS) is easily guaranteed. The proposed router was synthesized on an Stratix III 3SL340F FPGA device. A 4 × 4 2D Mesh SDM-TDM Network-on-Chip (NoC) was built with the proposed router and synthesized on the 3SL340F FPGA device. The 4 × 4 2D Mesh SDM-TDM NoC was used to build on an FPGA device, a Multiprocessor System-on-Chip (MPSoC) platform consisted of 16 Nios II/f processors, 16 20-KB On-chip Memories, and 16 Network Interfaces. Synthesis results of the MPSoC platform show that the proposed router architecture can be used to built large practicable MPSoC platforms with the proposed NoC architecture with a reasonable hardware overhead and appreciable clock frequency. Simulation results show that combining SDM and TDM techniques in a router allows the highest probability of establishing paths through the network.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2166069619",
    "type": "article"
  },
  {
    "title": "Pipelined Parallel Join and Its FPGA-Based Acceleration",
    "doi": "https://doi.org/10.1145/3079759",
    "publication_date": "2017-12-27",
    "publication_year": 2017,
    "authors": "Masato Yoshimi; Yasin Oge; Tsutomu Yoshinaga",
    "corresponding_authors": "",
    "abstract": "A huge amount of data is being generated and accumulated in data centers, which leads to an important increase in the required energy consumption to analyze these data. Thus, we must consider the redesign of current computer systems architectures to be more friendly to applications based on distributed algorithms that require a high data transfer rate. Novel computer architectures that introduce dedicated accelerators to enable near-data processing have been discussed and developed for high-speed big-data analysis. In this work, we propose a computer system with an FPGA-based accelerator, namely, interconnected-FPGAs, which offers two advantages: (1) direct data transmission and (2) offloading computation into data-flow in the FPGA. In this article, we demonstrate the capability of the proposed interconnected-FPGAs system to accelerate join operations in a relational database. We developed a new parallel join algorithm, PPJoin, targeted to big-data analysis in a shared-nothing architecture. PPJoin is an extended version of the NUMA-based parallel join algorithm, created by overlapping computation by multicore processors and data communication. The data communication between computational nodes can be accelerated by direct data transmission without passing through the main memory of the hosts. To confirm the performance of the PPJoin algorithm and its acceleration process using an interconnected-FPGA platform, we evaluated a simple query for large tables. Additionally, to support availability, we also evaluated the actual benchmark query. Our evaluation results confirm that the PPJoin algorithm is faster than a software-based query engine by 1.5--5 times. Moreover, we experimentally confirmed that the direct data transmission by interconnected FPGAs reduces computational time around 20% for PPJoin.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2777015551",
    "type": "article"
  },
  {
    "title": "FPGA Implementations of Kernel Normalised Least Mean Squares Processors",
    "doi": "https://doi.org/10.1145/3106744",
    "publication_date": "2017-12-15",
    "publication_year": 2017,
    "authors": "Nicholas J. Fraser; Junkyu Lee; Duncan J. M. Moss; Julian Faraone; Stephen Tridgell; Craig Jin; Philip H. W. Leong",
    "corresponding_authors": "",
    "abstract": "Kernel adaptive filters (KAFs) are online machine learning algorithms which are amenable to highly efficient streaming implementations. They require only a single pass through the data and can act as universal approximators, i.e. approximate any continuous function with arbitrary accuracy. KAFs are members of a family of kernel methods which apply an implicit non-linear mapping of input data to a high dimensional feature space, permitting learning algorithms to be expressed entirely as inner products. Such an approach avoids explicit projection into the feature space, enabling computational efficiency. In this paper, we propose the first fully pipelined implementation of the kernel normalised least mean squares algorithm for regression. Independent training tasks necessary for hyperparameter optimisation fill pipeline stages, so no stall cycles to resolve dependencies are required. Together with other optimisations to reduce resource utilisation and latency, our core achieves 161 GFLOPS on a Virtex 7 XC7VX485T FPGA for a floating point implementation and 211 GOPS for fixed point. Our PCI Express based floating-point system implementation achieves 80% of the core’s speed, this being a speedup of 10× over an optimised implementation on a desktop processor and 2.66× over a GPU.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2779013372",
    "type": "article"
  },
  {
    "title": "A Reconfigurable Architecture for Binary Acceleration of Loops with Memory Accesses",
    "doi": "https://doi.org/10.1145/2629468",
    "publication_date": "2014-12-29",
    "publication_year": 2014,
    "authors": "Nuno Paulino; João Canas Ferreira; João M. P. Cardoso",
    "corresponding_authors": "",
    "abstract": "This article presents a reconfigurable hardware/software architecture for binary acceleration of embedded applications. A Reconfigurable Processing Unit (RPU) is used as a coprocessor of the General Purpose Processor (GPP) to accelerate the execution of repetitive instruction sequences called Megablocks . A toolchain detects Megablocks from instruction traces and generates customized RPU implementations. The implementation of Megablocks with memory accesses uses a memory-sharing mechanism to support concurrent accesses to the entire address space of the GPP’s data memory. The scheduling of load/store operations and memory access handling have been optimized to minimize the latency introduced by memory accesses. The system is able to dynamically switch the execution between the GPP and the RPU when executing the original binaries of the input application. Our proof-of-concept prototype achieved geometric mean speedups of 1.60× and 1.18× for, respectively, a set of 37 benchmarks and a subset considering the 9 most complex benchmarks. With respect to a previous version of our approach, we achieved geometric mean speedup improvements from 1.22 to 1.53 for the 10 benchmarks previously used.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2094835990",
    "type": "article"
  },
  {
    "title": "Coordination of Independent Loops in Self-Adaptive Systems",
    "doi": "https://doi.org/10.1145/2611563",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "Jacopo Panerati; Martina Maggio; Matteo Carminati; Filippo Sironi; Marco Triverio; Marco D. Santambrogio",
    "corresponding_authors": "",
    "abstract": "Nowadays, the same piece of code should run on different architectures, providing performance guarantees in a variety of environments and situations. To this end, designers often integrate existing systems with ad-hoc adaptive strategies able to tune specific parameters that impact performance or energy—for example, frequency scaling. However, these strategies interfere with one another and unpredictable performance degradation may occur due to the interaction between different entities. In this article, we propose a software approach to reconfiguration when different strategies, called loops , are encapsulated in the system and are available to be activated. Our solution to loop coordination is based on machine learning and it selects a policy for the activation of loops inside of a system without prior knowledge. We implemented our solution on top of GNU/Linux and evaluated it with a significant subset of the PARSEC benchmark suite.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2159176014",
    "type": "article"
  },
  {
    "title": "The Unified Accumulator Architecture",
    "doi": "https://doi.org/10.1145/2809432",
    "publication_date": "2016-05-20",
    "publication_year": 2016,
    "authors": "David Wilson; Greg Stitt",
    "corresponding_authors": "",
    "abstract": "Applications accelerated by field-programmable gate arrays (FPGAs) often require pipelined floating-point accumulators with a variety of different trade-offs. Although previous work has introduced numerous floating-point accumulation architectures, few cores are available for public use, which forces designers to use fixed-point implementations or vendor-provided cores that are not portable and are often not optimized for the desired set of trade-offs. In this article, we combine and extend previous floating-point accumulator architectures into a configurable, open-source core, referred to as the unified accumulator architecture (UAA), which enables designers to choose between different trade-offs for different applications. UAA is portable across FPGAs and allows designers to specialize the underlying adder core to take advantage of device-specific optimizations. By providing an extensible, open-source implementation, we hope for the research community to extend the provided core with new architectures and optimizations.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2395134749",
    "type": "article"
  },
  {
    "title": "Hardware Accelerated Alignment Algorithm for Optical Labeled Genomes",
    "doi": "https://doi.org/10.1145/2840811",
    "publication_date": "2016-09-03",
    "publication_year": 2016,
    "authors": "Pingfan Meng; Matthew Jacobsen; Motoki Kimura; Vladimir Dergachev; Thomas Anantharaman; Michael Requa; Ryan Kastner",
    "corresponding_authors": "",
    "abstract": "De novo assembly is a widely used methodology in bioinformatics. However, the conventional short-read-based de novo assembly is incapable of reliably reconstructing the large-scale structures of human genomes. Recently, a novel optical label-based technology has enabled reliable large-scale de novo assembly. Despite its advantage in large-scale genome analysis, this new technology requires a more computationally intensive alignment algorithm than its conventional counterpart. For example, the runtime of reconstructing a human genome is on the order of 10,000 hours on a sequential CPU. Therefore, in order to practically apply this new technology in genome research, accelerated approaches are desirable. In this article, we present three different accelerated approaches, multicore CPU, GPU, and FPGA. Against the sequential software baseline, our multicore CPU design achieved an 8.4× speedup, while the GPU and FPGA designs achieved 13.6× and 115× speedups, respectively. We also discuss the details of the design space exploration of this new assembly algorithm on these three different devices. Finally, we compare these devices in performance, optimization techniques, prices, and design efforts.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2520712063",
    "type": "article"
  },
  {
    "title": "Microarchitecture and Circuits for a 200 MHz Out-of-Order Soft Processor Memory System",
    "doi": "https://doi.org/10.1145/2974022",
    "publication_date": "2016-12-09",
    "publication_year": 2016,
    "authors": "Henry Wong; Vaughn Betz; Jonathan Rose",
    "corresponding_authors": "",
    "abstract": "Although FPGAs have grown in capacity, FPGA-based soft processors have grown very little because of the difficulty of achieving higher performance in exchange for area. Superscalar out-of-order processors promise large performance gains, and the memory subsystem is a key part of such a processor that must help supply increased performance. In this article, we describe and explore microarchitectural and circuit-level tradeoffs in the design of such a memory system. We show the significant instructions-per-cycle wins for providing various levels of out-of-order memory access and memory dependence speculation (1.32 × SPECint2000) and for the addition of a second-level cache (another 1.60 × ). With careful microarchitecture and circuit design, we also achieve a L1 translation lookaside buffers and cache lookup with 29% less logic delay than the simpler Nios II/f memory system.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2559901034",
    "type": "article"
  },
  {
    "title": "BurstZ+: Eliminating The Communication Bottleneck of Scientific Computing Accelerators via Accelerated Compression",
    "doi": "https://doi.org/10.1145/3476831",
    "publication_date": "2022-01-31",
    "publication_year": 2022,
    "authors": "Gongjin Sun; Seongyoung Kang; Sang-Woo Jun",
    "corresponding_authors": "",
    "abstract": "We present BurstZ+, an accelerator platform that eliminates the communication bottleneck between PCIe-attached scientific computing accelerators and their host servers, via hardware-optimized compression. While accelerators such as GPUs and FPGAs provide enormous computing capabilities, their effectiveness quickly deteriorates once data is larger than its on-board memory capacity, and performance becomes limited by the communication bandwidth of moving data between the host memory and accelerator. Compression has not been very useful in solving this issue due to performance and efficiency issues of compressing floating point numbers, which scientific data often consists of. BurstZ+ is an FPGA-based prototype accelerator platform which addresses the bandwidth issue via a class of novel hardware-optimized floating point compression algorithm called ZFP-V. We demonstrate that BurstZ+ can completely remove the host-side communication bottleneck for accelerators, using multiple stencil kernels with a wide range of operational intensities. Evaluated against hand-optimized implementations of kernel accelerators of the same architecture, our single-pipeline BurstZ+ prototype outperforms an accelerator without compression by almost 4×, and even an accelerator with enough memory for the entire dataset by over 2×. Furthermore, the projected performance of BurstZ+ on a future, faster FPGA scales to almost 7× that of the same accelerator without compression, whose performance is still limited by the PCIe bandwidth.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4210439397",
    "type": "article"
  },
  {
    "title": "Stratix 10 NX Architecture",
    "doi": "https://doi.org/10.1145/3520197",
    "publication_date": "2022-03-14",
    "publication_year": 2022,
    "authors": "Martin Langhammer; Eriko Nurvitadhi; Sergey Gribok; Bogdan Pasca",
    "corresponding_authors": "",
    "abstract": "The advent of AI has driven the exploration of high-density low-precision arithmetic on FPGAs. This has resulted in new methods in mapping both arithmetic functions as well as dataflows onto the fabric, as well as some changes to the embedded DSP Blocks. Technologies outside of the FPGA realm have also evolved, such as the addition of tensor structures for GPUs, as well as the introduction of numerous AI ASSPs, all of which have a higher claimed performance and efficiency than current FPGAs. In this article, we will introduce the Stratix 10 NX device, which is a variant of FPGA specifically optimized for the AI application space. In addition to the computational capabilities of the standard programmable soft-logic fabric, a new type of DSP Block provides the dense arrays of low-precision multipliers typically used in AI implementations. The architecture of the block is tuned for the common matrix-matrix or vector-matrix multiplications in AI, with capabilities designed to work efficiently for both small and large matrix sizes. The base precisions are INT8 and INT4, along with shared exponent support to support block FP16 and block FP12 numerics. All additions/accumulations can be done in INT32 or IEEE-754 single precision floating point (FP32), and multiple blocks can be cascaded together to support larger matrices. We will also describe methods by which the smaller precision multipliers can be aggregated to create larger multipliers that are more applicable to standard signal processing requirements. In the AI market, the FPGA must compete directly with other types of devices, rather than occupy a unique niche. Deterministic system performance is as important as the performance of individual FPGA elements, such as logic, memory, and DSP. We will show that the feed forward datapath structures that are needed to support the typical AI matrix-vector and matrix-matrix multiplication operations can consistently close timing at over 500 MHz on a mid-speed grade device, even if all of the Tensor Blocks on the device are used. We will also show a full-chip NPU processor implementation that out performs GPUs at the same process node for a variety of AI inferencing workloads, even though it has a lower operating frequency of 365 MHz. In terms of overall compute throughput, Stratix 10 NX is specified at 143 INT8/FP16 TOPs/FLOPs or 286 INT4/FP12 TOPS/FLOPs. Depending on the configuration, power efficiency is in the range of 1–4 TOPs or TFLOPs/W.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4220984504",
    "type": "article"
  },
  {
    "title": "Remarn: A Reconfigurable Multi-threaded Multi-core Accelerator for Recurrent Neural Networks",
    "doi": "https://doi.org/10.1145/3534969",
    "publication_date": "2022-05-17",
    "publication_year": 2022,
    "authors": "Zhiqiang Que; Hiroki Nakahara; Hongxiang Fan; He Li; Jiuxi Meng; Kuen Hung Tsoi; Xinyu Niu; Eriko Nurvitadhi; Wayne Luk",
    "corresponding_authors": "",
    "abstract": "This work introduces Remarn, a reconfigurable multi-threaded multi-core accelerator supporting both spatial and temporal co-execution of Recurrent Neural Network (RNN) inferences. It increases processing capabilities and quality of service of cloud-based neural processing units (NPUs) by improving their hardware utilization and by reducing design latency, with two innovations. First, a custom coarse-grained multi-threaded RNN/Long Short-Term Memory (LSTM) hardware architecture, switching tasks among threads when RNN computational engines meet data hazards. Second, the partitioning of this hardware architecture into multiple full-fledged sub-accelerator cores, enabling spatially co-execution of multiple RNN/LSTM inferences. These innovations improve the exploitation of the available parallelism to increase runtime hardware utilization and boost design throughput. Evaluation results show that a dual-threaded quad-core Remarn NPU achieves 2.91 times higher performance while only occupying 5.0% more area than a single-threaded one on a Stratix 10 FPGA. When compared with a Tesla V100 GPU implementation, our design achieves 6.5 times better performance and 15.6 times higher power efficiency, showing that our approach contributes to high performance and energy-efficient FPGA-based multi-RNN inference designs for datacenters.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4280577395",
    "type": "article"
  },
  {
    "title": "A Scalable Many-core Overlay Architecture on an HBM2-enabled Multi-Die FPGA",
    "doi": "https://doi.org/10.1145/3547657",
    "publication_date": "2022-07-20",
    "publication_year": 2022,
    "authors": "Riadh Ben Abdelhamid; Yoshiki Yamaguchi; Taisuke Boku",
    "corresponding_authors": "",
    "abstract": "The overlay architecture enables to raise the abstraction level of hardware design and enhances hardware-accelerated applications’ portability. In FPGAs, there is a growing awareness of the overlay structure as typified by many-core architecture. It works in theory; however, it is difficult in practice, because it is beset with serious design issues. For example, the size of FPGAs is bigger than before. It is exacerbating the issue of the place-and-route. Besides, a single FPGA is actually the sum of small-to-middle FPGAs by advancing packaging technology like silicon interposers. Thus, the tightly coupled many-core designs will face this covert issue that the wires among the regions are extremely restricted. This article proposes efficient essential processing elements, micro-architecture design, and the interconnect architecture toward a scalable many-core overlay design. In particular, our work proposes a novel compact buffering technique to reduce memory resource utilization in tightly connected overlays while preserving computational efficiency. This technique reduces the utilization of BlockRAM to nearly 50% while achieving a best-case computational efficiency of 91.93% in a three-dimensional Jacobi benchmark. Besides, the proposed enhancements led to around 2× and 3× improvement in performance and power efficiency, respectively. Moreover, the improved scalability allowed increasing compute resources and delivering around 4× better performance and power efficiency, as compared to the baseline Dynamically Re-programmable Architecture of Gather-scatter Overlay Nodes overlay.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4286001193",
    "type": "article"
  },
  {
    "title": "Design Space Exploration of Galois and Fibonacci Configuration Based on Espresso Stream Cipher",
    "doi": "https://doi.org/10.1145/3567428",
    "publication_date": "2022-10-08",
    "publication_year": 2022,
    "authors": "Zhengyuan Shi; Cheng Chen; Gangqiang Yang; Hailiang Xiong; Fudong Li; Honggang Hu; Zhiguo Wan",
    "corresponding_authors": "",
    "abstract": "Fibonacci and Galois are two different kinds of configurations in stream ciphers. Although many transformations between two configurations have been proposed, there is no sufficient analysis of their FPGA performance. Espresso stream cipher provides an ideal sample to explore such a problem. The 128-bit secret key Espresso is designed in Galois configuration, and there is a Fibonacci-configured Espresso variant proved with the equivalent security level. To fully leverage the efficiency of two configurations, we explore the hardware optimization approaches toward area and throughput, respectively. In short, the FPGA-implemented Fibonacci cipher is more suitable for extremely resource-constrained or high-throughput applications, while the Galois cipher compromises both area and speed. To the best of our knowledge, this is the first work to systematically compare the FPGA performance of cipher configurations under relatively fair cryptographic security. We hope this work can serve as a reference for the cryptography hardware architecture research community.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4303647666",
    "type": "article"
  },
  {
    "title": "Topgun: An ECC Accelerator for Private Set Intersection",
    "doi": "https://doi.org/10.1145/3603114",
    "publication_date": "2023-07-13",
    "publication_year": 2023,
    "authors": "Guiming Wu; Qianwen He; Jiali Jiang; Zhenxiang Zhang; Yuan Zhao; Yinchao Zou; Jie Zhang; Changzheng Wei; Ying Yan; Hui Zhang",
    "corresponding_authors": "",
    "abstract": "Elliptic Curve Cryptography (ECC), one of the most widely used asymmetric cryptographic algorithms, has been deployed in Transport Layer Security (TLS) protocol, blockchain, secure multiparty computation, and so on. As one of the most secure ECC curves, Curve25519 is employed by some secure protocols, such as TLS 1.3 and Diffie-Hellman Private Set Intersection (DH-PSI) protocol. High-performance implementation of ECC is required, especially for the DH-PSI protocol used in privacy-preserving platform. Point multiplication, the chief cryptographic primitive in ECC, is computationally expensive. To improve the performance of DH-PSI protocol, we propose Topgun, a novel and high-performance hardware architecture for point multiplication over Curve25519. The proposed architecture features a pipelined Finite-field Arithmetic Unit and a simple and highly efficient instruction set architecture. Compared to the best existing work on Xilinx Zynq 7000 series FPGA, our implementation with one Processing Element can achieve 3.14× speedup on the same device. To the best of our knowledge, our implementation appears to be the fastest among the state-of-the-art works. We also have implemented our architecture consisting of 4 Compute Groups, each with 16 PEs, on an Intel Agilex AGF027 FPGA. The measured performance of 4.48 Mops/s is achieved at the cost of 86 Watts power, which is the record-setting performance for point multiplication over Curve25519 on FPGAs.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4384201201",
    "type": "article"
  },
  {
    "title": "A Hardware Accelerator for the Semi-Global Matching Stereo Algorithm: An Efficient Implementation for the Stratix V and Zynq UltraScale+ FPGA Technology",
    "doi": "https://doi.org/10.1145/3615869",
    "publication_date": "2023-09-09",
    "publication_year": 2023,
    "authors": "John Kalomiros; John Vourvoulakis; Stavros Vologiannidis",
    "corresponding_authors": "",
    "abstract": "The semi-global matching stereo algorithm is a top performing algorithm in stereo vision. The recursive nature of the computations involved in this algorithm introduces an inherent data dependency problem, hindering the progressive computations of disparities at pixel clock. In this work, a novel hardware implementation of the semi-global matching algorithm is presented. A hardware structure of parallel comparators is proposed for the fast computation of the minima among large cost arrays in one clock cycle. Also, a hardware-friendly algorithm is proposed for the computation of the minima among far-indexed disparity costs, shortening the length of computations in the datapath. As a result, the recursive path cost computation is accelerated considerably. The system is implemented in a Stratix V device and in a Zynq UltraScale+ device. A throughput of 55,1 million disparities per second is achieved with maximum disparity 128 pixels and frame resolution 1280 × 720. The proposed architecture is less elaborate and more resource efficient than other systems in the literature and its performance compares favorably to them. An implementation on an actual FPGA board is also presented and serves as a real-world verification of the proposed system.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4386569262",
    "type": "article"
  },
  {
    "title": "GraphScale: Scalable Processing on FPGAs for HBM and Large Graphs",
    "doi": "https://doi.org/10.1145/3616497",
    "publication_date": "2023-09-13",
    "publication_year": 2023,
    "authors": "Jonas Dann; Daniel Ritter; Holger Fröning",
    "corresponding_authors": "",
    "abstract": "Recent advances in graph processing on FPGAs promise to alleviate performance bottlenecks with irregular memory access patterns. Such bottlenecks challenge performance for a growing number of important application areas like machine learning and data analytics. While FPGAs denote a promising solution through flexible memory hierarchies and massive parallelism, we argue that current graph processing accelerators either use the off-chip memory bandwidth inefficiently or do not scale well across memory channels. In this work, we propose GraphScale, a scalable graph processing framework for FPGAs. GraphScale combines multi-channel memory with asynchronous graph processing (i.e., for fast convergence on results) and a compressed graph representation (i.e., for efficient usage of memory bandwidth and reduced memory footprint). GraphScale solves common graph problems like breadth-first search, PageRank, and weakly connected components through modular user-defined functions, a novel two-dimensional partitioning scheme, and a high-performance two-level crossbar design. Additionally, we extend GraphScale to scale to modern high-bandwidth memory (HBM) and reduce partitioning overhead of large graphs with binary packing.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4386711145",
    "type": "article"
  },
  {
    "title": "Montgomery Multiplication Scalable Systolic Designs Optimized for DSP48E2",
    "doi": "https://doi.org/10.1145/3624571",
    "publication_date": "2023-09-15",
    "publication_year": 2023,
    "authors": "Louis Noyez; Nadia El Mrabet; Olivier Potin; Pascal Véron",
    "corresponding_authors": "",
    "abstract": "This article describes an extensive study of the use of DSP48E2 Slices in Ultrascale FPGAs to design hardware versions of the Montgomery Multiplication algorithm for the hardware acceleration of modular multiplications. Our fully scalable systolic architectures result in parallelized, DSP48E2-optimized scheduling of operations analogous to the FIOS block variant of the Montgomery Multiplication. We explore the impacts of different pipelining strategies within DSP blocks, scheduling of operations, processing element configurations, global design structures and their tradeoffs in terms of performance and resource costs. We discuss the application of our methodology to multiple types of DSP primitives. We provide ready-to-use fast, efficient, and fully parametrizable designs, which can adapt to a wide range of requirements and applications. Implementations are scalable to any operand width. Our most efficient designs can perform 128, 256, 512, 1024, 2048, and 4096 bits Montgomery modular multiplications in 0.0992 μs, 0.2032 μs, 0.3952 μs, 0.7792μs, 1.550 μs, and 3.099 μs using 4, 6, 11, 21, 41, and 82 DSP blocks, respectively.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4386773811",
    "type": "article"
  },
  {
    "title": "Perturb+mutate",
    "doi": "https://doi.org/10.1145/1391732.1391736",
    "publication_date": "2008-09-01",
    "publication_year": 2008,
    "authors": "David Grant; Guy Lemieux",
    "corresponding_authors": "",
    "abstract": "CAD tool designers are always searching for more benchmark circuits to stress their software. In this article we present a heuristic method to generate benchmark circuits specially suited for incremental place-and-route tools. The method removes part of a real circuit and replaces it with an altered version of the same circuit to mimic an incremental design change. The alteration consists of two steps: mutate followed by perturb . The perturb step exactly preserves as many circuit characteristics as possible. While perturbing, reproduction of interconnect locality, a characteristic that is difficult to measure reliably or reproduce exactly, is controlled using a new technique, ancestor depth control (ADC). Perturbing with ADC produces circuits with postrouting properties that match the best techniques known to-date. The mutate step produces targetted mutations resulting in controlled changes to specific circuit properties (while keeping other properties constant). We demonstrate one targetted mutation heuristic, scale, to significantly change circuit size with little change to other circuit characteristics. The method is simple enough for inclusion in a CAD tool directly, and fast enough for use in on-the-fly benchmark generation.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2137381762",
    "type": "article"
  },
  {
    "title": "Enhancing FPGAs with Magnetic Tunnel Junction-Based Block RAMs",
    "doi": "https://doi.org/10.1145/3154425",
    "publication_date": "2018-01-26",
    "publication_year": 2018,
    "authors": "Kosuke Tatsumura; Sadegh Yazdanshenas; Vaughn Betz",
    "corresponding_authors": "",
    "abstract": "While plentiful on-chip memory is necessary for many designs to fully utilize an FPGA’s computational capacity, SRAM scaling is becoming more difficult because of increasing device variation. An alternative is to build FPGA block RAM (BRAM) from magnetic tunnel junctions (MTJ), as this emerging embedded memory has a small cell size, low energy usage, and good scalability. We conduct a detailed comparison study of SRAM and MTJ BRAMs that includes cell designs that are robust with device variation, transistor-level design and optimization of all the required BRAM-specific circuits, and variation-aware simulation at the 22nm node. At a 256Kb block size, MTJ-BRAM is 3.06× denser and 55% more energy efficient and its F max is 274MHz, which is adequate for most FPGA system clock domains. We also detail further enhancements that allow these 256 Kb MTJ BRAMs to operate at a higher speed of 353MHz for the streaming FIFOs, which are very common in FPGA designs and describe how the non-volatility of MTJ BRAM enables novel on-chip configuration and power-down modes. For a RAM architecture similar to the latest commercial FPGAs, MTJ-BRAMs could expand FPGA memory capacity by 2.95× with no die size increase.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2793377509",
    "type": "article"
  },
  {
    "title": "FPGA-based Acceleration of FT Convolution for Pulsar Search Using OpenCL",
    "doi": "https://doi.org/10.1145/3268933",
    "publication_date": "2018-12-31",
    "publication_year": 2018,
    "authors": "Haomiao Wang; T. Prabu; Oliver Sinnen",
    "corresponding_authors": "",
    "abstract": "The Square Kilometre Array (SKA) project will be the world’s largest radio telescope array. With its large number of antennas, the number of signals that need to be processed is dramatic. One important element of the SKA’s Central Signal Processor package is pulsar search. This article focuses on the FPGA-based acceleration of the Frequency-Domain Acceleration Search module, which is a part of SKA pulsar search engine. In this module, the frequency-domain input signals have to be processed by 85 Finite Impulse response (FIR) filters within a short period of limitation and for thousands of input arrays. Because of the large scale of the input length and FIR filter size, even high-end FPGA devices cannot parallelise the task completely. We start by investigating both time-domain FIR filter (TDFIR) and frequency-domain FIR filter (FDFIR) to tackle this task. We applied the overlap-add algorithm to split the coefficient array of TDFIR and the overlap-save algorithm to split the input signals of FDFIR. To achieve fast prototyping design, we employed OpenCL, which is a high-level FPGA development technique. The performance and power consumption are evaluated using multiple FPGA devices simultaneously and compared with GPU results, which is achieved by porting FPGA-based OpenCL kernels. The experimental evaluation shows that the FDFIR solution is very competitive in terms of performance, with a clear energy consumption advantage over the GPU solution.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2963405438",
    "type": "article"
  },
  {
    "title": "Model-based Design of Hardware SC Polar Decoders for FPGAs",
    "doi": "https://doi.org/10.1145/3391431",
    "publication_date": "2020-05-30",
    "publication_year": 2020,
    "authors": "Yann Delomier; Bertrand Le Gal; Jérémie Crenne; Christophe Jégo",
    "corresponding_authors": "",
    "abstract": "Polar codes are a new error correction code family that should be benchmarked and evaluated in comparison to LDPC and turbo-codes. Indeed, recent advances in the 5G digital communication standard recommended the use of polar codes in EMBB control channels. However, in many cases, the implementation of efficient FEC hardware decoders is challenging. Specialised knowledge is required to enable and facilitate testing, rapid design iterations, and fast prototyping. In this article, a model-based design methodology to generate efficient hardware SC polar code decoders is presented. With HLS design process and tools, we demonstrate how FPGA system designers can quickly develop complex hardware systems with good performances. The favourable impact of design space exploration is underlined on achievable performances when a relevant computation model is used. The flexibility of the abstraction layers is evaluated. Hardware decoder generation efficiency is assessed and compared to competing approaches. It is shown that the fine-tuning of computation parallelism, bit length, pruning level, and working frequency help to design high-throughput decoders with moderate hardware complexities. Decoding throughputs higher than 300 Mbps are achieved on an Xilinx Virtex-7 device and on an Altera Stratix IV device.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3026871068",
    "type": "article"
  },
  {
    "title": "UNILOGIC",
    "doi": "https://doi.org/10.1145/3409115",
    "publication_date": "2020-09-09",
    "publication_year": 2020,
    "authors": "Άγγελος Ιωάννου; Konstantinos Georgopoulos; Pavlos Malakonakis; Dionisios Pnevmatikatos; Vassilis Papaefstathiou; Ioannis Papaefstathiou; Iakovos Mavroidis",
    "corresponding_authors": "",
    "abstract": "One of the main characteristics of High-performance Computing (HPC) applications is that they become increasingly performance and power demanding, pushing HPC systems to their limits. Existing HPC systems have not yet reached exascale performance mainly due to power limitations. Extrapolating from today’s top HPC systems, about 100–200 MWatts would be required to sustain an exaflop-level of performance. A promising solution for tackling power limitations is the deployment of energy-efficient reconfigurable resources (in the form of Field-programmable Gate Arrays (FPGAs)) tightly integrated with conventional CPUs. However, current FPGA tools and programming environments are optimized for accelerating a single application or even task on a single FPGA device. In this work, we present UNILOGIC (Unified Logic), a novel HPC-tailored parallel architecture that efficiently incorporates FPGAs. UNILOGIC adopts the Partitioned Global Address Space (PGAS) model and extends it to include hardware accelerators, i.e., tasks implemented on the reconfigurable resources. The main advantages of UNILOGIC are that (i) the hardware accelerators can be accessed directly by any processor in the system, and (ii) the hardware accelerators can access any memory location in the system. In this way, the proposed architecture offers a unified environment where all the reconfigurable resources can be seamlessly used by any processor/operating system. The UNILOGIC architecture also provides hardware virtualization of the reconfigurable logic so that the hardware accelerators can be shared among multiple applications or tasks. The FPGA layer of the architecture is implemented by splitting its reconfigurable resources into (i) a static partition, which provides the PGAS-related communication infrastructure, and (ii) fixed-size and dynamically reconfigurable slots that can be programmed and accessed independently or combined together to support both fine and coarse grain reconfiguration. 1 Finally, the UNILOGIC architecture has been evaluated on a custom prototype that consists of two 1U chassis, each of which includes eight interconnected daughter boards, called Quad-FPGA Daughter Boards (QFDBs); each QFDB supports four tightly coupled Xilinx Zynq Ultrascale+ MPSoCs as well as 64 Gigabytes of DDR4 memory, and thus, the prototype features a total of 64 Zynq MPSoCs and 1 Terabyte of memory. We tuned and evaluated the UNILOGIC prototype using both low-level (baremetal) performance tests, as well as two popular real-world HPC applications, one compute-intensive and one data-intensive. Our evaluation shows that UNILOGIC offers impressive performance that ranges from being 2.5 to 400 times faster and 46 to 300 times more energy efficient compared to conventional parallel systems utilizing only high-end CPUs, while it also outperforms GPUs by a factor ranging from 3 to 6 times in terms of time to solution, and from 10 to 20 times in terms of energy to solution.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3084890676",
    "type": "article"
  },
  {
    "title": "Parallel Unary Computing Based on Function Derivatives",
    "doi": "https://doi.org/10.1145/3418464",
    "publication_date": "2020-10-28",
    "publication_year": 2020,
    "authors": "Soheil Mohajer; Zhiheng Wang; Kia Bazargan; Yuyang Li",
    "corresponding_authors": "",
    "abstract": "The binary number representation has dominated digital logic for decades due to its compact storage requirements. An alternative representation is the unary number system: We use N bits, from which the first M are 1 and the rest are 0 to represent the value M/N . One-hot representation is a variation of the unary number system where it has one 1 in the N bits, where the 1’s position represents its value. We present a novel method that first converts binary numbers to unary using thermometer (one-hot) encoders and then uses a “scaling network” followed by voting gates that we call “alternator logic,” followed by a decoder to convert the numbers back to the binary format. For monotonically increasing functions, the scaling network is all we need, which essentially uses only the routing resources and flip-flops on a typical FPGA architecture. Our method is clearly superior to the conventional binary implementation: Our area×delay cost is on average only 0.4%, 4%, and 39% of the binary method for 8-, 10-, and 12-bit resolutions, respectively, in thermometer encoding scheme, and 0.5%, 15%, and 147% in the one-hot encoding scheme. In terms of power efficiency, our one-hot method is between about 69× and 114× better compared to conventional binary.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3097425375",
    "type": "article"
  },
  {
    "title": "Process Variability Analysis in Interconnect, Logic, and Arithmetic Blocks of 16-nm FinFET FPGAs",
    "doi": "https://doi.org/10.1145/3458843",
    "publication_date": "2021-08-12",
    "publication_year": 2021,
    "authors": "Endri Taka; Konstantinos Maragos; George Lentaris; Dimitrios Soudris",
    "corresponding_authors": "",
    "abstract": "In the current work, we study the process variability of logic, interconnect, and arithmetic/DSP resources in commercial 16-nm FPGAs. We create multiple, soft-macro sensors for each distinct resource under evaluation, and we deploy them across the FPGA fabric to measure intra-die variation, as well as across multiple FPGAs to measure inter-die variation. The derived results are used to create device-signature variability maps characterizing the distribution of variability across the die. Our study includes decoupling of variability to systematic and stochastic parts, exploration of variability under various voltage and temperature conditions and correlation analysis between the variability maps of the different resources. Furthermore, we scrutinize the impact of variability on the performance of actual test circuits and correlate the retrieved results with the sensor-based maps. Our experimental results on four Zynq XCZU7EV FPGAs showed significant intra- and inter-die variability, up to 7.8% and 8.9%, respectively, with a small increase under certain operating conditions. The correlation analysis demonstrated a strong correlation between the logic and arithmetic resources, whereas the interconnects showed a slightly weaker correlation in specific devices. Finally, a relatively moderate correlation was calculated between the variability maps and performance of test circuits due their dissimilar operating behavior versus our sensors.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3196238133",
    "type": "article"
  },
  {
    "title": "When Massive GPU Parallelism Ain’t Enough: A Novel Hardware Architecture of 2D-LSTM Neural Network",
    "doi": "https://doi.org/10.1145/3469661",
    "publication_date": "2021-11-09",
    "publication_year": 2021,
    "authors": "Vladimir Rybalkin; Jonas Ney; Menbere Kina Tekleyohannes; Norbert Wehn",
    "corresponding_authors": "",
    "abstract": "Multidimensional Long Short-Term Memory (MD-LSTM) neural network is an extension of one-dimensional LSTM for data with more than one dimension. MD-LSTM achieves state-of-the-art results in various applications, including handwritten text recognition, medical imaging, and many more. However, its implementation suffers from the inherently sequential execution that tremendously slows down both training and inference compared to other neural networks. The main goal of the current research is to provide acceleration for inference of MD-LSTM. We advocate that Field-Programmable Gate Array (FPGA) is an alternative platform for deep learning that can offer a solution when the massive parallelism of GPUs does not provide the necessary performance required by the application. In this article, we present the first hardware architecture for MD-LSTM. We conduct a systematic exploration to analyze a tradeoff between precision and accuracy. We use a challenging dataset for semantic segmentation, namely historical document image binarization from the DIBCO 2017 contest and a well-known MNIST dataset for handwritten digit recognition. Based on our new architecture, we implement FPGA-based accelerators that outperform Nvidia Geforce RTX 2080 Ti with respect to throughput by up to 9.9 and Nvidia Jetson AGX Xavier with respect to energy efficiency by up to 48 . Our accelerators achieve higher throughput, energy efficiency, and resource efficiency than FPGA-based implementations of convolutional neural networks (CNNs) for semantic segmentation tasks. For the handwritten digit recognition task, our FPGA implementations provide higher accuracy and can be considered as a solution when accuracy is a priority. Furthermore, they outperform earlier FPGA implementations of one-dimensional LSTMs with respect to throughput, energy efficiency, and resource efficiency.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3213445774",
    "type": "article"
  },
  {
    "title": "Hipernetch: High-Performance FPGA Network Switch",
    "doi": "https://doi.org/10.1145/3477054",
    "publication_date": "2021-11-30",
    "publication_year": 2021,
    "authors": "Philippos Papaphilippou; Jiuxi Meng; Nadeen Gebara; Wayne Luk",
    "corresponding_authors": "",
    "abstract": "We present Hipernetch, a novel FPGA-based design for performing high-bandwidth network switching. FPGAs have recently become more popular in data centers due to their promising capabilities for a wide range of applications. With the recent surge in transceiver bandwidth, they could further benefit the implementation and refinement of network switches used in data centers. Hipernetch replaces the crossbar with a “combined parallel round-robin arbiter”. Unlike a crossbar, the combined parallel round-robin arbiter is easy to pipeline, and does not require centralised iterative scheduling algorithms that try to fit too many steps in a single or a few FPGA cycles. The result is a network switch implementation on FPGAs operating at a high frequency and with a low port-to-port latency. Our proposed Hipernetch architecture additionally provides a competitive switching performance approaching output-queued crossbar switches. Our implemented Hipernetch designs exhibit a throughput that exceeds 100 Gbps per port for switches of up to 16 ports, reaching an aggregate throughput of around 1.7 Tbps.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3216789966",
    "type": "article"
  },
  {
    "title": "The Impact of Terrestrial Radiation on FPGAs in Data Centers",
    "doi": "https://doi.org/10.1145/3457198",
    "publication_date": "2021-12-06",
    "publication_year": 2021,
    "authors": "Andrew M. Keller; Michael Wirthlin",
    "corresponding_authors": "",
    "abstract": "Field programmable gate arrays (FPGAs) are used in large numbers in data centers around the world. They are used for cloud computing and computer networking. The most common type of FPGA used in data centers are re-programmable SRAM-based FPGAs. These devices offer potential performance and power consumption savings. A single device also carries a small susceptibility to radiation-induced soft errors, which can lead to unexpected behavior. This article examines the impact of terrestrial radiation on FPGAs in data centers. Results from artificial fault injection and accelerated radiation testing on several data-center-like FPGA applications are compared. A new fault injection scheme provides results that are more similar to radiation testing. Silent data corruption (SDC) is the most commonly observed failure mode followed by FPGA unavailable and host unresponsive. A hypothetical deployment of 100,000 FPGAs in Denver, Colorado, will experience upsets in configuration memory every half-hour on average and SDC failures every 0.5–11 days on average.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4200101119",
    "type": "article"
  },
  {
    "title": "Leveraging reconfigurability in the hardware/software codesign process",
    "doi": "https://doi.org/10.1145/2000832.2000840",
    "publication_date": "2011-08-01",
    "publication_year": 2011,
    "authors": "Lesley Shannon; Paul Chow",
    "corresponding_authors": "",
    "abstract": "Current technology allows designers to implement complete embedded computing systems on a single FPGA. Using an FPGA as the implementation platform introduces greater flexibility into the design process and allows a new approach to embedded system design. Since there is no cost to reprogramming an FPGA, system performance can be measured on-chip in the runtime environment and the system's architecture can be altered based on an evaluation of the data to meet design requirements. In this article, we discuss a new hardware/software codesign methodology tailored to reconfigurable platforms and a design infrastructure created to incorporate on-chip design tools. This methodology utilizes the FPGA's reconfigurability during the design process to profile and verify system performance, thereby reducing system design time. Our current design infrastructure includes: a system specification tool, two on-chip profiling tools, and an on-chip system verification tool.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W1996426321",
    "type": "article"
  },
  {
    "title": "Logarithmic-Time FPGA Bitstream Analysis",
    "doi": "https://doi.org/10.1145/1968502.1968503",
    "publication_date": "2011-05-01",
    "publication_year": 2011,
    "authors": "Étienne Bergeron; Louis-David Perron; Marc Feeley; Jean‐Pierre David",
    "corresponding_authors": "",
    "abstract": "Just-In-Time (JIT) compilation is frequently used in software engineering to accelerate program execution. Parts of the code are translated to machine code at runtime to speedup their execution by exploiting local and dynamic information of the computation. Modern FPGAs manufactured by Xilinx allow partial and dynamic configuration. Such features make them eligible platforms for JIT hardware compilation. Nevertheless, this has not been achieved until now because the mapping between a bitstream and the programmable points inside these FPGAs is not documented. In this article, we propose a methodology to retrieve the relevant information in logarithmic time per bit by methodically using the tools distributed by Xilinx. We give a practical case study which details the analysis of a Virtex-II Pro FPGA bitstream. The mapping of CLBs, BRAMs, and multipliers has been fully determined. Thanks to this information, we have been able to prototype tools in the fields of reverse mapping FPGA bitstreams, low-level simulation, and custom place-and-route. Finally preliminary results demonstrate that a processor embedded in an FPGA can compile, place, and route arithmetic and logic expressions inside the FPGA within a few milliseconds.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2009741039",
    "type": "article"
  },
  {
    "title": "Performance Scalability of Adaptive Processor Architecture",
    "doi": "https://doi.org/10.1145/3007902",
    "publication_date": "2017-04-11",
    "publication_year": 2017,
    "authors": "Shigeyuki Takano",
    "corresponding_authors": "Shigeyuki Takano",
    "abstract": "In this article, we evaluate the performance scalability of architectures called adaptive processors, which dynamically configure an application-specific pipelined datapath and perform a data-flow streaming execution. Previous works have examined the basics of the following: (1) a computational model that supports the swap-in/out of a partial datapath—namely, a virtual hardware is realized by hardware, without a host processor and its software; (2) an architecture that has shown a minimum pipeline requirement and a minimum component requirement; and (3) the characteristics of the execution phase and a stack shift that realizes the swap-in/out. However, these works did not explore the design space, particularly with respect to the following: (1) the clock cycle time on the adaptive processor, which must depend on a wire delay that is primarily used for the global communication of requests, acknowledgments, acquirements, releases, and so forth, and (2) a revised control system that can handle the out-of-order acknowledgment and in-order acquirement that guarantee the correct datapath configuration with a conditional branch for the configurations. This article explores the scaling of the ALU resources versus pipelining of the wires.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2605579187",
    "type": "article"
  },
  {
    "title": "Optimizing Soft Vector Processing in FPGA-Based Embedded Systems",
    "doi": "https://doi.org/10.1145/2912884",
    "publication_date": "2016-05-20",
    "publication_year": 2016,
    "authors": "Nachiket Kapre",
    "corresponding_authors": "Nachiket Kapre",
    "abstract": "Soft vector processors can augment and extend the capability of FPGA-based embedded systems-on-chip such as the Xilinx Zynq. However, configuring and optimizing the soft processor for best performance is hard. We must consider architectural parameters such as precision, vector lane count, vector length, chunk size, and DMA scheduling to ensure efficient execution of code on the soft vector processing platform. To simplify the design process, we develop a compiler framework and an autotuning runtime that splits the optimization into a combination of static and dynamic passes that map data-parallel computations to the soft processor. We compare and contrast implementations running on the scalar ARM processor, the embedded NEON hard vector engine, and low-level streaming Verilog designs with the VectorBlox MXP soft vector processor. Across a range of data-parallel benchmarks, we show that the MXP soft vector processor can outperform other organizations by up to 4 × while saving ≈ 10% dynamic power. Our compilation and runtime framework is also able to outperform the gcc NEON vectorizer under certain conditions by explicit generation of NEON intrinsics and performance tuning of the autogenerated data-parallel code. When constrained by IO bandwidth, soft vector processors are even competitive with spatial Verilog implementations of computation.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2407650548",
    "type": "article"
  },
  {
    "title": "An Optimized Hardware Architecture of a Multivariate Gaussian Random Number Generator",
    "doi": "https://doi.org/10.1145/1857927.1857929",
    "publication_date": "2010-12-01",
    "publication_year": 2010,
    "authors": "Chalermpol Saiprasert; Christos-Savvas Bouganis; George A. Constantinides",
    "corresponding_authors": "",
    "abstract": "Monte Carlo simulation is one of the most widely used techniques for computationally intensive simulations in mathematical analysis and modeling. A multivariate Gaussian random number generator is one of the main building blocks of such a system. Field Programmable Gate Arrays (FPGAs) are gaining increased popularity as an alternative means to the traditional general purpose processors targeting the acceleration of the computationally expensive random number generator block. This article presents a novel approach for mapping a multivariate Gaussian random number generator onto an FPGA by optimizing the computational path in terms of hardware resource usage subject to an acceptable error in the approximation of the distribution of interest. The proposed approach is based on the eigenvalue decomposition algorithm which leads to a design with different precision requirements in the computational paths. An analysis on the impact of the error due to truncation/rounding operation along the computational path is performed and an analytical expression of the error inserted into the system is presented. Based on the error analysis, three algorithms that optimize the resource utilization and at the same time minimize the error in the output of the system are presented and compared. Experimental results reveal that the hardware resource usage on an FPGA as well as the error in the approximation of the distribution of interest are significantly reduced by the use of the optimization techniques introduced in the proposed approach.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W1976797375",
    "type": "article"
  },
  {
    "title": "An Application Development Framework for ARISE Reconfigurable Processors",
    "doi": "https://doi.org/10.1145/1575779.1575784",
    "publication_date": "2009-09-01",
    "publication_year": 2009,
    "authors": "Nikolaos Vassiliadis; George Theodoridis; S. Nikolaidis",
    "corresponding_authors": "",
    "abstract": "Coupling reconfigurable hardware accelerators with processors is an effective way to meet the performance and flexibility required to cope with modern embedded applications. The ARISE framework provides a systematic approach to extend a processor once. It will thereafter support the coupling of arbitrary hardware accelerators. The accelerators can be coupled as coprocessors or functional units of the processor’s datapath, and therefore exploited as a hybrid, which includes both loose and tight computational models. This article presents a complete framework for developing applications on such hybrid reconfigurable ARISE machines. The framework integrates the automatic identification of custom instructions and the semiautomatic/profiling-driven identification of coprocessors supporting the hybrid computational model. Moreover, it supports a modular design approach where the software and the hardware modules are developed independently and later ported into any ARISE machine with reconfigurable technology. To evaluate efficiency, a set of benchmarks is implemented on an ARISE evaluation machine utilizing the proposed framework. In addition, the ARISE machine is compared against a well-established processor paradigm that utilizes reconfigurable accelerators following only the typical coprocessor approach. Experimental results prove that the framework can be used to exploit the hybrid computational model and achieve significant performance improvements over the typical coprocessor acceleration approach. Moreover, results demonstrate how the framework can be used to trade off performance, silicon area, and application development time.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2073354563",
    "type": "article"
  },
  {
    "title": "In-Circuit Debugging with Dynamic Reconfiguration of FPGA Interconnects",
    "doi": "https://doi.org/10.1145/3375459",
    "publication_date": "2020-01-30",
    "publication_year": 2020,
    "authors": "Alexandra Kourfali; Dirk Stroobandt",
    "corresponding_authors": "",
    "abstract": "In this work, a novel method for in-circuit debugging on FPGAs is introduced that allows the insertion of low-overhead debugging infrastructure by exploiting the technique of parameterized configurations. This allows the parameterization of the LUTs and the routing infrastructure to create a virtual network of debugging multiplexers. It aims to facilitate debugging, to increase the internal signal observability, and to reduce the debugging (area and reconfiguration) overhead. Signal ranking techniques are also introduced that classify signals that can be traced during debug. Finally, the results of the method are presented and compared with a commercial tool. The area and time results and the tradeoffs between internal signal observability and area and reconfiguration overhead are also explored.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3009122457",
    "type": "article"
  },
  {
    "title": "Net-length-based routability-driven power-aware clustering",
    "doi": "https://doi.org/10.1145/2068716.2068724",
    "publication_date": "2011-12-01",
    "publication_year": 2011,
    "authors": "Lakshmi Easwaran; Ali Akoglu",
    "corresponding_authors": "",
    "abstract": "The state-of-the-art power-aware clustering tool, P-T-VPack, achieves energy reduction by localizing nets with high switching activity at the expense of channel width and area. In this study, we employ predicted individual postplacement net length information during clustering and prioritize longer nets. This approach targets the capacitance factor for energy reduction, and prioritizes longer nets for channel width and area reduction. We first introduce a new clustering strategy, W-T-VPack, which replaces the switching activity in P-T-VPack with a net length factor. We obtain a 9.87% energy reduction over T-VPack (3.78% increase over P-T-VPack), while at the same time completely eliminating P-T-VPack's channel width and area overhead. We then introduce W-P-T-VPack, which combines switching activity and net length factors. W-P-T-VPack achieves 14.26% energy reduction (0.31% increase over P-T-VPack), while further improving channel width by up to 12.87% for different cluster sizes. We investigate the energy performance of routability (channel width)-driven clustering algorithms, and show that W-T-VPack consistently outperforms T-RPack and iRAC by at least 11.23% and 9.07%, respectively. We conclude that net-length-based clustering is an effective method to concurrently target energy and channel width.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W1967624322",
    "type": "article"
  },
  {
    "title": "High-Speed Hardware Partition Generation",
    "doi": "https://doi.org/10.1145/2629472",
    "publication_date": "2014-12-15",
    "publication_year": 2014,
    "authors": "Jon T. Butler; Tsutomu Sasao",
    "corresponding_authors": "",
    "abstract": "We demonstrate circuits that generate set and integer partitions on a set S of n objects at a rate of one per clock. Partitions are ways to group elements of a set together and have been extensively studied by researchers in algorithm design and theory. We offer two versions of a hardware set partition generator. In the first, partitions are produced in lexicographical order in response to successive clock pulses. In the second, an index input determines the set partition produced. Such circuits are useful in the hardware implementation of the optimum distribution of tasks to processors. We show circuits for integer partitions as well. Our circuits are combinational. For large n , they can have a large delay. However, one can easily pipeline them to produce one partition per clock period. We show (1) analytical and (2) experimental time/complexity results that quantify the efficiency of our designs. For example, our results show that a hardware set partition generator running on a 100MHz FPGA produces partitions at a rate that is approximately 10 times the rate of a software implementation on a processor running at 2.26GHz.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2066376256",
    "type": "article"
  },
  {
    "title": "Exploiting Task- and Data-Level Parallelism in Streaming Applications Implemented in FPGAs",
    "doi": "https://doi.org/10.1145/2535932",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Franjo Plavec; Z.G. Vranesic; Stephen D. Brown",
    "corresponding_authors": "",
    "abstract": "This article describes the design and implementation of a novel compilation flow that implements circuits in FPGAs from a streaming programming language. The streaming language supported is called FPGA Brook and is based on the existing Brook language. It allows system designers to express applications in a way that exposes parallelism, which can be exploited through hardware implementation. FPGA Brook supports replication, allowing parts of an application to be implemented as multiple hardware units operating in parallel. Hardware units are interconnected through FIFO buffers which use the small memory modules available in FPGAs. The FPGA Brook automated design flow uses a source-to-source compiler, developed as a part of this work, and combines it with a commercial behavioral synthesis tool to generate the hardware implementation. A suite of benchmark applications was developed in FPGA Brook and implemented using our design flow. Experimental results indicate that performance of many applications scales well with replication. Our benchmark applications also achieve significantly better results than corresponding implementations using a commercial behavioral synthesis tool. We conclude that using an automated design flow for implementation of streaming applications in FPGAs is a promising methodology.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2067618990",
    "type": "article"
  },
  {
    "title": "Memory Latency Hiding by Load Value Speculation for Reconfigurable Computers",
    "doi": "https://doi.org/10.1145/2362374.2362377",
    "publication_date": "2012-10-01",
    "publication_year": 2012,
    "authors": "Benjamin Thielmann; Jens Huthmann; Andreas Koch",
    "corresponding_authors": "",
    "abstract": "Load value speculation has long been proposed as a method to hide the latency of memory accesses. It has seen very limited use in actual processors, often due to the high overhead of reexecuting misspeculated computations. We present PreCoRe, a framework capable of generating application-specific microarchitectures supporting load value speculation on reconfigurable computers. The article examines the lightweight speculation and replay mechanisms, the architecture of the actual data value prediction units as well as the impact on the nonspeculative parts of the memory system. In experiments, using PreCoRe has achieved speedups of up to 2.48 times over nonspeculative implementations.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2126328430",
    "type": "article"
  },
  {
    "title": "CORDIC-Based Enhanced Systolic Array Architecture for QR Decomposition",
    "doi": "https://doi.org/10.1145/2827700",
    "publication_date": "2015-12-14",
    "publication_year": 2015,
    "authors": "Jianfeng Zhang; Paul Chow; Hengzhu Liu",
    "corresponding_authors": "",
    "abstract": "Multiple input multiple output (MIMO) with orthogonal frequency division multiplexing (OFDM) systems typically use orthogonal-triangular (QR) decomposition. In this article, we present an enhanced systolic array architecture to realize QR decomposition based on the Givens rotation (GR) method for a 4 × 4 real matrix. The coordinate rotation digital computer (CORDIC) algorithm is adopted and modified to speed up and simplify the process of GR. To verify the function and evaluate the performance, the proposed architectures are validated on a Virtex 5 FPGA development platform. Compared to a commercial implementation of vectoring CORDIC, the enhanced vectoring CORDIC is presented that uses 37.7% less hardware resources, dissipates 71.6% less power, and provides a 1.8 times speedup while maintaining the same computation accuracy. The enhanced QR systolic array architecture based on the enhanced vectoring CORDIC saves 24.5% in power dissipation, provides a factor of 1.5-fold improvement in throughput, and the hardware efficiency is improved 1.45-fold with no accuracy penalty when compared to our previously proposed QR systolic array architecture.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2301399139",
    "type": "article"
  },
  {
    "title": "Demystifying the Soft and Hardened Memory Systems of Modern FPGAs for Software Programmers through Microbenchmarking",
    "doi": "https://doi.org/10.1145/3517131",
    "publication_date": "2022-02-09",
    "publication_year": 2022,
    "authors": "Alec Lu; Zhenman Fang; Lesley Shannon",
    "corresponding_authors": "",
    "abstract": "Both modern datacenter and embedded Field Programmable Gate Arrays (FPGAs) provide great opportunities for high-performance and high-energy-efficiency computing. With the growing public availability of FPGAs from major cloud service providers such as AWS, Alibaba, and Nimbix, as well as uniform hardware accelerator development tools (such as Xilinx Vitis and Intel oneAPI) for software programmers, hardware and software developers can now easily access FPGA platforms. However, it is nontrivial to develop efficient FPGA accelerators, especially for software programmers who use high-level synthesis (HLS). The major goal of this article is to figure out how to efficiently access the memory system of modern datacenter and embedded FPGAs in HLS-based accelerator designs. This is especially important for memory-bound applications; for example, a naive accelerator design only utilizes less than 5% of the available off-chip memory bandwidth. To achieve our goal, we first identify a comprehensive set of factors that affect the memory bandwidth, including (1) the clock frequency of the accelerator design, (2) the number of concurrent memory access ports, (3) the data width of each port, (4) the maximum burst access length for each port, and (5) the size of consecutive data accesses. Then, we carefully design a set of HLS-based microbenchmarks to quantitatively evaluate the performance of the memory systems of datacenter FPGAs (Xilinx Alveo U200 and U280) and embedded FPGA (Xilinx ZCU104) when changing those affecting factors, and we provide insights into efficient memory access in HLS-based accelerator designs. Comparing between the typically used soft and hardened memory systems, respectively, found on datacenter and embedded FPGAs, we further summarize their unique features and discuss the effective approaches to leverage these systems. To demonstrate the usefulness of our insights, we also conduct two case studies to accelerate the widely used K-nearest neighbors (KNN) and sparse matrix-vector multiplication (SpMV) algorithms on datacenter FPGAs with a soft (and thus more flexible) memory system. Compared to the baseline designs, optimized designs leveraging our insights achieve about \\( 3.5\\times \\) and \\( 8.5\\times \\) speedups for the KNN and SpMV accelerators. Our final optimized KNN and SpMV designs on a Xilinx Alveo U200 FPGA fully utilize its off-chip memory bandwidth, and achieve about \\( 5.6\\times \\) and \\( 3.4\\times \\) speedups over the 24-core CPU implementations.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4210859512",
    "type": "article"
  },
  {
    "title": "RapidLayout: Fast Hard Block Placement of FPGA-optimized Systolic Arrays Using Evolutionary Algorithm",
    "doi": "https://doi.org/10.1145/3501803",
    "publication_date": "2022-02-14",
    "publication_year": 2022,
    "authors": "Niansong Zhang; Xiang Chen; Nachiket Kapre",
    "corresponding_authors": "",
    "abstract": "Evolutionary algorithms can outperform conventional placement algorithms such as simulated annealing, analytical placement, and manual placement on runtime, wirelength, pipelining cost, and clock frequency when mapping hard block intensive designs such as systolic arrays on Xilinx UltraScale+ FPGAs. For certain hard-block intensive designs, the commercial-grade Xilinx Vivado CAD tool cannot provide legal routing solutions without tedious manual placement constraints. Instead, we formulate hard block placement as a multi-objective optimization problem that targets wirelength squared and bounding box size. We build an end-to-end placement-and-routing flow called RapidLayout using the Xilinx RapidWright framework. RapidLayout runs 5–6 \\( \\times \\) faster than Vivado with manual constraints and eliminates the weeks-long effort to manually generate placement constraints. RapidLayout enables transfer learning from similar devices and bootstrapping from much smaller devices. Transfer learning in the UltraScale+ family achieves 11–14 \\( \\times \\) shorter runtime and bootstrapping from a 97% smaller device delivers 2.1–3.2 \\( \\times \\) faster optimizations. RapidLayout outperforms (1) a tuned simulated annealer by 2.7–30.8 \\( \\times \\) in runtime while achieving similar quality of results, (2) VPR by 1.5 \\( \\times \\) in runtime, 1.9–2.4 \\( \\times \\) in wirelength, and 3–4 \\( \\times \\) in bounding box size, while also (3) beating the analytical placer UTPlaceF by 9.3 \\( \\times \\) in runtime, 1.8–2.2 \\( \\times \\) in wirelength, and 2–2.7 \\( \\times \\) in bounding box size.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4213131631",
    "type": "article"
  },
  {
    "title": "Virtual networks -- distributed communication resource management",
    "doi": "https://doi.org/10.1145/2499625.2492186",
    "publication_date": "2013-07-01",
    "publication_year": 2013,
    "authors": "Jan Heißwolf; Aurang Zaib; Andreas Weichslgartner; Ralf König; Thomas Wild; Jürgen Teich; Andreas Herkersdorf; Jürgen Becker",
    "corresponding_authors": "",
    "abstract": "Networks-on-Chip (NoC) enable scalability for future manycore architectures, facilitating parallel communication between multiple cores. Applications running in parallel on a NoC-based architecture can affect each other due to overlapping communication. Quality-of-Service (QoS) must be supported by the communication infrastructure to execute communication-, real-time- and safety-critical applications on such an architecture. Different strategies have been proposed to provide QoS for point-to-point connections. These strategies allow each node to set up a limited number of connections to other nodes.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4242096974",
    "type": "article"
  },
  {
    "title": "Streaming Overlay Architecture for Lightweight LSTM Computation on FPGA SoCs",
    "doi": "https://doi.org/10.1145/3543069",
    "publication_date": "2022-06-21",
    "publication_year": 2022,
    "authors": "Lenos Ioannou; Suhaib A. Fahmy",
    "corresponding_authors": "",
    "abstract": "Long-Short Term Memory (LSTM) networks, and Recurrent Neural Networks (RNNs) in general, have demonstrated their suitability in many time series data applications, especially in Natural Language Processing (NLP) . Computationally, LSTMs introduce dependencies on previous outputs in each layer that complicate their computation and the design of custom computing architectures, compared to traditional feed-forward networks. Most neural network acceleration work has focused on optimising the core matrix-vector operations on highly capable FPGAs in server environments. Research that considers the embedded domain has often been unsuitable for streaming inference, relying heavily on batch processing to achieve high throughput. Moreover, many existing accelerator architectures have not focused on fully exploiting the underlying FPGA architecture, resulting in designs that achieve lower operating frequencies than the theoretical maximum. This paper presents a flexible overlay architecture for LSTMs on FPGA SoCs that is built around a streaming dataflow arrangement, uses DSP block capabilities directly, and is tailored to keep parameters within the architecture while moving input data serially to mitigate external memory access overheads. The architecture is designed as an overlay that can be configured to implement alternative models or update model parameters at runtime. It achieves higher operating frequency and demonstrates higher performance than other lightweight LSTM accelerators, as demonstrated in an FPGA SoC implementation.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4283219803",
    "type": "article"
  },
  {
    "title": "Data and Computation Reuse in CNNs Using Memristor TCAMs",
    "doi": "https://doi.org/10.1145/3549536",
    "publication_date": "2022-07-20",
    "publication_year": 2022,
    "authors": "Rafael Fão de Moura; João Paulo C. de Lima; Luigi Carro",
    "corresponding_authors": "",
    "abstract": "Exploiting computational and data reuse in CNNs is crucial for the successful design of resource-constrained platforms. In image recognition applications, high levels of input locality and redundancy present in CNNs have become the golden goose for skipping costly arithmetic operations. One promising technique for this consists in storing function responses of some input patterns into offline lookup tables and replacing online computation with search operations, which are highly efficient when implemented by emerging non-volatile memory technologies. In this work, we rethink both algorithm and architecture for exploiting locality and reuse opportunities by replacing entire convolutions with searches on Content-addressable Memories. By previously calculating convolution results and building compact lookup tables with our novel clustering algorithm, one can evaluate activations at constant time complexity, also requiring a single read operation of the current input tensor. Then, we devise a reconfigurable array of processing elements based on memristive Ternary Content-addressable Memories to efficiently implement the algorithmic solution and meet the flexibility requirements of several CNN architectures. Results show that our design reduces the number of multiplications and memory accesses proportionally to the number of convolutional layer channels. The average performance is 1,172 and 82 FPS for AlexNet and VGG-16 models, thus outperforming state-of-the-art works by 13×.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4286001379",
    "type": "article"
  },
  {
    "title": "Hardware Optimizations of Fruit-80 Stream Cipher: Smaller than Grain",
    "doi": "https://doi.org/10.1145/3569455",
    "publication_date": "2022-10-25",
    "publication_year": 2022,
    "authors": "Gangqiang Yang; Zhengyuan Shi; Cheng Chen; Hailiang Xiong; Fudong Li; Honggang Hu; Zhiguo Wan",
    "corresponding_authors": "",
    "abstract": "Fruit-80, which emerged as an ultra-lightweight stream cipher with 80-bit secret key, is oriented toward resource-constrained devices in the Internet of Things. In this article, we propose area and speed optimization architectures of Fruit-80 on FPGAs. Our implementations include both serial and parallel structure and optimize area, power, speed, and throughput, respectively. The area optimization architecture aims to achieve the most suitable ratio of look-up-tables and flip-flops to fully utilize the reconfigurable unit. It also reuses NFSR and LFSR feedback functions to save resources for high throughput. The speed optimization architecture adopts a hybrid approach for parallelization and reduces the latency of long data paths by pre-generating primary feedback and inserting flip-flops. Besides, we recommend using the round key function to optimize serial or parallel implementations for Fruit-80 and using indexing and shifting methods for different throughput. In conclusion, our results show that the area optimization architecture occupies up to 35 slices on Xilinx Spartan-3 FPGA and 18 slices on Xilinx 7 series FPGA, smaller than that of Grain and other common stream ciphers. The optimal throughput/area ratio of the speed optimization architecture is 7.74 Mbps/slice, better than that of Grain v1, which is 5.98 Mbps/slice. The serial implementation of Fruit-80 with round key function occupies only 75 slices on Spartan-3 FPGA. To the best of our knowledge, the result sets a new record of the minimum area in lightweight cipher implementation on FPGA.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4307203669",
    "type": "article"
  },
  {
    "title": "A High-Throughput, Resource-Efficient Implementation of the RoCEv2 Remote DMA Protocol and its Application",
    "doi": "https://doi.org/10.1145/3543176",
    "publication_date": "2022-12-22",
    "publication_year": 2022,
    "authors": "Niklas Schelten; Fritjof Steinert; Justin Knapheide; Anton Schulte; Benno Stabernack",
    "corresponding_authors": "",
    "abstract": "The use of application-specific accelerators in data centers has been the state of the art for at least a decade, starting with the availability of General Purpose GPUs achieving higher performance either overall or per watt. In most cases, these accelerators are coupled via PCIe interfaces to the corresponding hosts, which leads to disadvantages in interoperability, scalability and power consumption. As a viable alternative to PCIe-attached FPGA accelerators this paper proposes standalone FPGAs as Network-attached Accelerators (NAAs) . To enable reliable communication for decoupled FPGAs we present an RDMA over Converged Ethernet v2 (RoCEv2) communication stack for high-speed and low-latency data transfer integrated into a hardware framework. For NAAs to be used instead of PCIe coupled FPGAs the framework must provide similar throughput and latency with low resource usage. We show that our RoCEv2 stack is capable of achieving 100 Gb/s throughput with latencies of less than 4μs while using about 10% of the available resources on a mid-range FPGA. To evaluate the energy efficiency of our NAA architecture, we built a demonstrator with 8 NAAs for machine learning based image classification. Based on our measurements, network-attached FPGAs are a great alternative to the more energy-demanding PCIe-attached FPGA accelerators.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4312190975",
    "type": "article"
  },
  {
    "title": "Evaluating the Impact of Using Multiple-Metal Layers on the Layout Area of Switch Blocks for Tile-Based FPGAs in FinFET 7nm",
    "doi": "https://doi.org/10.1145/3639055",
    "publication_date": "2024-01-02",
    "publication_year": 2024,
    "authors": "Sajjad Rostami-Sani; Andy Ye",
    "corresponding_authors": "",
    "abstract": "A new area model for estimating the layout area of switch blocks is introduced in this work. The model is based on a realistic layout strategy. As a result, it not only takes into consideration the active area that is needed to construct a switch block but also the number of metal layers available and the actual dimensions of these metals. The model assigns metal layers to the routing tracks in a way that reduces the number of vias that are needed to connect different routing tracks together while maintaining the tile-based structure of FPGAs. It also takes into account the wiring area required for buffer insertion for long wire segments. The model is evaluated based on the layouts constructed in the ASAP7 FinFET 7nm Predictive Design Kit. We found that the new model, while specific to the layout strategy that it employs, improves upon the traditional active-based area estimation models by considering the growth of the metal area independently from the growth of the active area. As a result, the new model is able to more accurately estimate the layout area by predicting when the metal area will overtake the active area as the number of routing tracks is increased. This ability allows the more accurate estimation of the true layout cost of FPGA fabrics at the early floor planning and architectural exploration stage; and this increase in accuracy can encourage a wider use of custom FPGA fabrics that target specific sets of benchmarks in future SOC designs. Furthermore, our data indicate that the conclusions drawn from several significant prior architectural studies remain to be correct under FinFET geometries and wiring area considerations despite their exclusive use of active-only area models. This correctness is due to the small channel widths, around 30–60 tracks per channel, of the architectures that these studies investigate. For architectures that approach the channel width of modern commercial FPGAs with more than 100–200 tracks per channel, our data show that wiring area models justified by detailed layout considerations are an essential addition to active area models in the correct prediction of the implementation area of FPGAs.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4390490815",
    "type": "article"
  },
  {
    "title": "<i>AxOMaP</i> : Designing FPGA-based <u>A</u> ppro <u>x</u> imate Arithmetic <u>O</u> perators using <u>Ma</u> thematical <u>P</u> rogramming",
    "doi": "https://doi.org/10.1145/3648694",
    "publication_date": "2024-02-19",
    "publication_year": 2024,
    "authors": "Siva Satyendra Sahoo; Salim Ullah; Akash Kumar",
    "corresponding_authors": "",
    "abstract": "With the increasing application of machine learning (ML) algorithms in embedded systems, there is a rising necessity to design low-cost computer arithmetic for these resource-constrained systems. As a result, emerging models of computation, such as approximate and stochastic computing, that leverage the inherent error-resilience of such algorithms are being actively explored for implementing ML inference on resource-constrained systems. Approximate computing (AxC) aims to provide disproportionate gains in the power, performance, and area (PPA) of an application by allowing some level of reduction in its behavioral accuracy (BEHAV). Using approximate operators (AxOs) for computer arithmetic forms one of the more prevalent methods of implementing AxC. AxOs provide the additional scope for finer granularity of optimization, compared to only precision scaling of computer arithmetic. To this end, the design of platform-specific and cost-efficient approximate operators forms an important research goal. Recently, multiple works have reported the use of AI/ML-based approaches for synthesizing novel FPGA-based AxOs. However, most of such works limit the use of AI/ML to designing ML-based surrogate functions that are used during iterative optimization processes. To this end, we propose a novel data analysis-driven mathematical programming-based approach to synthesizing approximate operators for FPGAs. Specifically, we formulate mixed integer quadratically constrained programs based on the results of correlation analysis of the characterization data and use the solutions to enable a more directed search approach for evolutionary optimization algorithms. Compared to traditional evolutionary algorithms-based optimization, we report up to 21% improvement in the hypervolume, for joint optimization of PPA and BEHAV, in the design of signed 8-bit multipliers. Further, we report up to 27% better hypervolume than other state-of-the-art approaches to DSE for FPGA-based application-specific AxOs.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4391948018",
    "type": "article"
  },
  {
    "title": "L-FNNG: Accelerating Large-Scale KNN Graph Construction on CPU-FPGA Heterogeneous Platform",
    "doi": "https://doi.org/10.1145/3652609",
    "publication_date": "2024-03-14",
    "publication_year": 2024,
    "authors": "Chaoqiang Liu; Xiaofei Liao; Long Zheng; Yu Huang; Haifeng Liu; Yi Zhang; Haiheng He; Haoyan Huang; Jingyi Zhou; Hai Jin",
    "corresponding_authors": "",
    "abstract": "Due to the high complexity of constructing exact k -nearest neighbor graphs, approximate construction has become a popular research topic. The NN-Descent algorithm is one of the representative in-memory algorithms. To effectively handle large datasets, existing state-of-the-art solutions combine the divide-and-conquer approach and the NN-Descent algorithm, where large datasets are divided into multiple partitions, and a subgraph is constructed for each partition before all the subgraphs are merged, reducing the memory pressure significantly. However, such solutions fail to address inefficiencies in large-scale k -nearest neighbor graph construction. In this paper, we propose L-FNNG, a novel solution for accelerating large-scale k -nearest neighbor graph construction on CPU-FPGA heterogeneous platform. The CPU is responsible for dividing data and determining the order of partition processing, while the FPGA executes all construction tasks to utilize the acceleration capability fully. To accelerate the execution of construction tasks, we design an efficient FPGA accelerator, which includes the Block-based Scheduling (BS) and Useless Computation Aborting (UCA) techniques to address the problems of memory access and computation in the NN-Descent algorithm. We also propose an efficient scheduling strategy that includes a KD-tree-based data partitioning method and a hierarchical processing method to address scheduling inefficiency. We evaluate L-FNNG on a Xilinx Alveo U280 board hosted by a 64-core Xeon server. On multiple large-scale datasets, L-FNNG achieves, on average, 2.3× construction speedup over the state-of-the-art GPU-based solution.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4392806438",
    "type": "article"
  },
  {
    "title": "FADO: Floorplan-Aware Directive Optimization Based on Synthesis and Analytical Models for High-Level Synthesis Designs on Multi-Die FPGAs",
    "doi": "https://doi.org/10.1145/3653458",
    "publication_date": "2024-03-20",
    "publication_year": 2024,
    "authors": "Linfeng Du; Tingyuan Liang; Xiaofeng Zhou; Jinming Ge; Shangkun Li; Sharad Sinha; Jieru Zhao; Zhiyao Xie; Wei Zhang",
    "corresponding_authors": "",
    "abstract": "Multi-die FPGAs are widely adopted for large-scale accelerators, but optimizing high-level synthesis designs on these FPGAs faces two challenges. First, the delay caused by die-crossing nets creates an NP-hard floorplanning problem. Second, traditional directive optimization cannot consider resource constraints on each die or the timing issue incurred by the die-crossings. Furthermore, the high algorithmic complexity and the large scale lead to extended runtime for legalizing the floorplan of HLS designs under different directive configurations. To co-optimize the directives and floorplan of HLS designs on multi-die FPGAs, we formulate the co-search based on bin-packing variants and present two iterative optimization flows. The first (FADO 1.0) relies on a pre-built QoR library. It involves a greedy, latency-bottleneck-guided directive search, and an incremental floorplan legalization. Compared with a global floorplanning solution, it takes 693X~4925X shorter search time and achieves 1.16X~8.78X better design performance, measured in workload execution time. To remove the time-consuming QoR library generation, the second flow (FADO 2.0) integrates an analytical QoR model and redesigns the directive search to accelerate convergence. Through experiments on mixed dataflow and non-dataflow designs, compared with 1.0, FADO 2.0 further yields a 1.40X better design performance on average after implementation on the Alveo U250 FPGA.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4393002331",
    "type": "article"
  },
  {
    "title": "DANSEN: Database Acceleration on Native Computational Storage by Exploiting NDP",
    "doi": "https://doi.org/10.1145/3655625",
    "publication_date": "2024-04-04",
    "publication_year": 2024,
    "authors": "Sajjad Tamimi; Arthur Bernhardt; Florian Stock; Ilia Petrov; Andreas Koch",
    "corresponding_authors": "",
    "abstract": "This paper introduces DANSEN , the hardware accelerator component for neoDBMS, a full-stack computational storage system designed to manage on-device execution of database queries/transactions as a Near-Data Processing (NDP)-operation. The proposed system enables Database Management Systems (DBMS) to offload NDP-operations to the storage while maintaining control over data through a native storage interface . DANSEN provides an NDP-engine that enables DBMS to perform both low-level database tasks, such as performing database administration, as well as high-level tasks like executing SQL, on the smart storage device while observing the DBMS concurrency control. Furthermore, DANSEN enables the incorporation of custom accelerators as an NDP-operation, e.g., to perform hardware-accelerated ML inference directly on the stored data. We built the DANSEN storage prototype and interface on an Ultrascale+HBM FPGA and fully integrated it with PostgreSQL 12. Experimental results demonstrate that the proposed NDP approach outperforms software-only PostgreSQL using a fast off-the-shelf NVMe drive, and significantly improves the end-to-end execution time of an aggregation operation (similar to Q6 from CH-benCHmark, 150 million records) by ≈ 10.6 ×. The versatility of the proposed approach is also validated by integrating a compute-intensive data analytics application with multi-row results, outperforming PostgreSQL by ≈ 1.5 ×.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4393934463",
    "type": "article"
  },
  {
    "title": "DyRecMul: Fast and Low-Cost Approximate Multiplier for FPGAs using Dynamic Reconfiguration",
    "doi": "https://doi.org/10.1145/3663480",
    "publication_date": "2024-05-01",
    "publication_year": 2024,
    "authors": "Shervin Vakili; Mobin Vaziri; Amirhossein Zarei; J. M. Pierre Langlois",
    "corresponding_authors": "",
    "abstract": "Multipliers are widely-used arithmetic operators in digital signal processing and machine learning circuits. Due to their relatively high complexity, they can have high latency and be a significant source of power consumption. One strategy to alleviate these limitations is to use approximate computing. This paper thus introduces an original FPGA-based approximate multiplier specifically optimized for machine learning computations. It utilizes dynamically reconfigurable lookup table (LUT) primitives in AMD-Xilinx technology to realize the core part of the computations. The paper provides an in-depth analysis of the hardware architecture, implementation outcomes, and accuracy evaluations of the multiplier proposed in INT8 precision. The paper also facilitates the generalization of the proposed approximate multiplier idea to other datatypes, providing analysis and estimations for hardware cost and accuracy as a function of multiplier parameters. Implementation results on an AMD-Xilinx Kintex Ultrascale+ FPGA demonstrate remarkable savings of 64% and 67% in LUT utilization for signed multiplication and multiply-and-accumulation configurations, respectively when compared to the standard Xilinx multiplier core. Accuracy measurements on four popular deep learning (DL) benchmarks indicate a minimal average accuracy decrease of less than 0.29% during post-training deployment, with the maximum reduction staying less than 0.33%. The source code of this work is available on GitHub.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4396560422",
    "type": "article"
  },
  {
    "title": "A Computation of the Ninth Dedekind Number Using FPGA Supercomputing",
    "doi": "https://doi.org/10.1145/3674147",
    "publication_date": "2024-07-02",
    "publication_year": 2024,
    "authors": "Lennart Van Hirtum; Patrick De Causmaecker; Jens Goemaere; Tobias Kenter; Heinrich Riebler; Michael Lass; Christian Plessl",
    "corresponding_authors": "",
    "abstract": "This manuscript makes the claim of having computed the \\(9\\) th Dedekind number, D(9). This was done by accelerating the core operation of the process with an efficient FPGA design that outperforms an optimized 64-core CPU reference by 95 \\(\\times\\) . The FPGA execution was parallelized on the Noctua 2 supercomputer at Paderborn University. The resulting value for D(9) is 286386577668298411128469151667598498812366. This value can be verified in two steps. We have made the data file containing the 490 M results available, each of which can be verified separately on CPU, and the whole file sums to our proposed value. The paper explains the mathematical approach in the first part, before putting the focus on a deep dive into the FPGA accelerator implementation followed by a performance analysis. The FPGA implementation was done in Register-Transfer Level using a dual-clock architecture and shows how we achieved an impressive FMax of 450 MHz on the targeted Stratix 10 GX 2,800 FPGAs. The total compute time used was 47,000 FPGA hours.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4400227490",
    "type": "article"
  },
  {
    "title": "SPARTA: High-Level Synthesis of Parallel Multi-Threaded Accelerators",
    "doi": "https://doi.org/10.1145/3677035",
    "publication_date": "2024-07-12",
    "publication_year": 2024,
    "authors": "Giovanni Gozzi; Michele Fiorito; Serena Curzel; Claudio Barone; Vito Giovanni Castellana; Marco Minutoli; Antonino Tumeo; Fabrizio Ferrandi",
    "corresponding_authors": "",
    "abstract": "This paper presents a methodology for the Synthesis of PARallel multi-Threaded Accelerators (SPARTA) from OpenMP annotated C/C++ specifications. SPARTA extends an open-source HLS tool, enabling the generation of accelerators that provide latency tolerance for irregular memory accesses through multithreading, support fine-grained memory-level parallelism through a hot-potato deflection-based network-on-chip (NoC), support synchronization constructs, and can instantiate memory-side caches. Our approach is based on a custom runtime OpenMP library, providing flexibility and extensibility. Experimental results show high scalability when synthesizing irregular graph kernels. The accelerators generated with our approach are, on average, 2.29x faster than state-of-the-art HLS methodologies.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4400583821",
    "type": "article"
  },
  {
    "title": "CHARM 2.0: Composing Heterogeneous Accelerators for Deep Learning on Versal ACAP Architecture",
    "doi": "https://doi.org/10.1145/3686163",
    "publication_date": "2024-08-05",
    "publication_year": 2024,
    "authors": "Jinming Zhuang; Jason Lau; Hanchen Ye; Zhuoping Yang; Shixin Ji; Jack Lo; Kristof Denolf; Stephen Neuendorffer; Alex K. Jones; Jingtong Hu; Yiyu Shi; Deming Chen; Jason Cong; Peipei Zhou",
    "corresponding_authors": "",
    "abstract": "Dense matrix multiply (MM) serves as one of the most heavily used kernels in deep learning applications. To cope with the high computation demands of these applications, heterogeneous architectures featuring both FPGA and dedicated ASIC accelerators have emerged as promising platforms. For example, the AMD/Xilinx Versal ACAP architecture combines general-purpose CPU cores and programmable logic with AI Engine processors optimized for AI/ML. An array of 400 AI Engine processors executing at 1 GHz can provide up to 6.4 TFLOPS performance for 32-bit floating-point (FP32) data. However, machine learning models often contain both large and small MM operations. While large MM operations can be parallelized efficiently across many cores, small MM operations typically cannot. We observe that executing some small MM layers from the BERT natural language processing model on a large, monolithic MM accelerator in Versal ACAP achieved less than 5% of the theoretical peak performance. Therefore, one key question arises: How can we design accelerators to fully use the abundant computation resources under limited communication bandwidth for end-to-end applications with multiple MM layers of diverse sizes? We identify the biggest system throughput bottleneck resulting from the mismatch between the massive computation resources of one monolithic accelerator and the various MM layers of small sizes in the application. To resolve this problem, we propose the CHARM framework to compose multiple diverse MM accelerator architectures working concurrently on different layers within one application. CHARM includes analytical models that guide design space exploration to determine accelerator partitions and layer scheduling. To facilitate system designs, CHARM automatically generates code, enabling thorough onboard design verification. We deploy the CHARM framework on four different deep learning applications in FP32, INT16, and INT8 data types, including BERT, ViT, NCF, and MLP, on the AMD/Xilinx Versal ACAP VCK190 evaluation board. Our experiments show that we achieve 1.46 TFLOPS, 1.61 TFLOPS, 1.74 TFLOPS, and 2.94 TFLOPS inference throughput for BERT, ViT, NCF, and MLP in FP32 data type, respectively, which obtain 5.29 \\(\\times\\) , 32.51 \\(\\times\\) , 1.00 \\(\\times\\) , and 1.00 \\(\\times\\) throughput gains compared to one monolithic accelerator. CHARM achieves the maximum throughput of 1.91 TOPS, 1.18 TOPS, 4.06 TOPS, and 5.81 TOPS in the INT16 data type for the four applications. The maximum throughput achieved by CHARM in the INT8 data type is 3.65 TOPS, 1.28 TOPS, 10.19 TOPS, and 21.58 TOPS, respectively. We have open-sourced our tools, including detailed step-by-step guides to reproduce all the results presented in this article and to enable other users to learn and leverage CHARM framework and tools in their end-to-end systems: https://github.com/arc-research-lab/CHARM .",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4401331008",
    "type": "article"
  },
  {
    "title": "PASTA: Programming and Automation Support for Scalable Task-Parallel HLS Programs on Modern Multi-Die FPGAs",
    "doi": "https://doi.org/10.1145/3676849",
    "publication_date": "2024-08-05",
    "publication_year": 2024,
    "authors": "Moazin Khatti; Xingyu Tian; A. S. Baroughi; Akhil Raj Baranwal; Yuze Chi; Licheng Guo; Jason Cong; Zhenman Fang",
    "corresponding_authors": "",
    "abstract": "In recent years, the adoption of FPGAs in datacenters has increased, with a growing number of users choosing High-Level Synthesis (HLS) as their preferred programming method. While HLS simplifies FPGA programming, one notable challenge arises when scaling up designs for modern datacenter FPGAs that comprise multiple dies. The extra delays introduced due to die crossings and routing congestion can significantly degrade the frequency of large designs on these FPGA boards. Due to the gap between HLS design and physical design, it is challenging for HLS programmers to analyze and identify the root causes, and fix their HLS design to achieve better timing closure. Recent efforts have aimed to address these issues by employing coarse-grained floorplanning and pipelining strategies on task-parallel HLS designs where multiple tasks run concurrently and communicate through FIFO stream channels. However, many applications are not streaming friendly and many existing accelerator designs heavily rely on buffer channel based communication between tasks. In this work, we take a step further to support a task-parallel programming model where tasks can communicate via both FIFO stream channels and buffer channels. To achieve this goal, we design and implement the PASTA framework, which takes a large task-parallel HLS design as input and automatically generates a high-frequency FPGA accelerator via HLS and physical design co-optimization. Our framework introduces a latency-insensitive buffer channel design, which supports memory partitioning and ping-pong buffering while remaining compatible with vendor HLS tools. On the frontend, we provide an easy-to-use programming model for utilizing the proposed buffer channel; while on the backend, we implement efficient placement and pipelining strategies for the proposed buffer channel. To validate the effectiveness of our framework, we test it on four widely used Rodinia HLS benchmarks and two real-world accelerator designs and show an average frequency improvement of 25%, with peak improvements of up to 89% on AMD/Xilinx Alveo U280 boards compared to Vitis HLS baselines.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4401335869",
    "type": "article"
  },
  {
    "title": "ProgramGalois: A Programmable Generator of Radix-4 Discrete Galois Transformation Architecture for Lattice-Based Cryptography",
    "doi": "https://doi.org/10.1145/3689437",
    "publication_date": "2024-08-24",
    "publication_year": 2024,
    "authors": "Guangyan Li; Zewen Ye; Donglong Chen; Wangchen Dai; Gaoyu Mao; Kejie Huang; Ray C. C. Cheung",
    "corresponding_authors": "",
    "abstract": "Lattice-based cryptography (LBC) has been established as a prominent research field, with particular attention on post-quantum cryptography (PQC) and fully homomorphic encryption (FHE). As the implementing bottleneck of PQC and FHE, number theoretic transform (NTT) has been extensively studied. However, current works struggled with scalability, hindering their adaptation to various parameters, such as bit width and polynomial length. In this article, we proposed a novel Discrete Galois Transformation (DGT) algorithm utilizing the radix-4 variant to achieve a higher level of parallelism to the existing NTT. Furthermore, to implement the efficient radix-4 DGT adapting more LBCs, we proposed a set of scalable building blocks, including a modified Barrett modular multiplier accepting arbitrary modulus with only one integer multiplier, a radix-4 DGT butterfly unit, and a stream permutation network. The proposed modules are implemented on the Xilinx Virtex-7 and U250 FPGA to evaluate resource utilization and performance. Lastly, a design space exploration framework is proposed to generate optimized radix-4 DGT hardware constrained by polynomial and platform parameters. The sensitivity analysis showcases the generated hardware’s performance and scalability. The implementation results on the Xilinx Virtex-7 and U250 FPGA show significant performance improvements over the state-of-the-art works, which reached at least 35%, 192%, and 68% area-time product improvements in terms of LUTs, BRAMs, and DSPs, respectively.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4401845226",
    "type": "article"
  },
  {
    "title": "FiberFlex: Real-time FPGA-based Intelligent &amp; Distributed Fiber Sensor System for Pedestrian Recognition",
    "doi": "https://doi.org/10.1145/3690389",
    "publication_date": "2024-08-28",
    "publication_year": 2024,
    "authors": "Yuqi Li; Kehao Zhao; Jieru Zhao; Qirui Wang; Shuda Zhong; Nageswara Lalam; Ruishu Wright; Peipei Zhou; Kevin P. Chen",
    "corresponding_authors": "",
    "abstract": "In recent years, security monitoring of public places and critical infrastructure has heavily relied on the widespread use of cameras, raising concerns about personal privacy violations. To balance the need for effective security monitoring with the protection of personal privacy, we explore the potential of optical fiber sensors for this application. This article proposes FiberFlex, an intelligent and distributed fiber sensor system. Ultizing Field Programmable Gate Arrays (FPGA) high-level synthesis (HLS) acceleration, FiberFlex offers real-time pedestrian detection by co-designing the entire pipeline of optical signal acquisition, processing, and recognition networks based on the principles of optical fiber sensing. As a promising alternative to traditional camera-based monitoring systems, FiberFlex achieves pedestrian detection by analyzing the vibration patterns caused by pedestrian footsteps, enabling security monitoring while preserving individual privacy. FiberFlex comprises three modules: First , fiber-optic sensing system: A fiber-optic distributed acoustic sensing (DAS) system is built and used to measure the ground vibration waves generated by people walking. Second , algorithms: We first collect the training data by measuring the ground vibration waves, label the data, and use the data to train the neural network models to perform pedestrian recognition. Third , hardware accelerators: We use HLS tools to design hardware modules on FPGA for data collection and pre-processing and integrate them with the downstream neural network accelerators to perform in-line real-time pedestrian detection. The final detection results are sent back from FPGA to the host CPU. We implement our system FiberFlex with the in-house built DAS system and AMD/Xilinx Kintex7 FPGA KC705 board and verify the whole system using the real-world collected data. We conduct recognition tests on five test subjects of varying ages, heights, and weights in a fixed sensing area. Each subject experienced 20 real-time recognition tests using their daily walking habits, and the subjects were given adequate rest between tests. After 100 tests on five test subjects, the overall real-time recognition accuracy exceeded \\(88.0\\%\\) . The whole system uses 55 W of power, 33 W in the optical DAS system and 22 W in the FPGA. Relying on its end-to-end interdisciplinary design, FiberFlex seamlessly combines fiber-optic sensors with FPGA accelerators to enable low-power real-time security monitoring without compromising privacy, making it a valuable addition to the existing security monitoring network. According to FiberFlex, more valuable research can be conducted in the future, such as fall monitoring for the elderly, migration of identification networks between different application scenarios, and improvement of anti-interference performance in more complex environments. In future perception networks, where the “eyes” are not feasible, let’s use fiber optic touch instead.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4401945473",
    "type": "article"
  },
  {
    "title": "CollectiveHLS: A Collaborative Approach to High-Level Synthesis Design Optimization",
    "doi": "https://doi.org/10.1145/3702005",
    "publication_date": "2024-10-26",
    "publication_year": 2024,
    "authors": "Aggelos Ferikoglou; Andreas Kosmas Kakolyris; Dimosthenis Masouros; Dimitrios Soudris; Sotirios Xydis",
    "corresponding_authors": "",
    "abstract": "High-Level Synthesis (HLS) has played a pivotal role in making FPGAs accessible to a broader audience by facilitating high-level device programming and rapid microarchitecture customization through the use of directives. However, manually selecting the right directives can be a formidable challenge for programmers lacking a hardware background. This paper presents CollectiveHLS, an ultra-fast, knowledge-driven approach to optimizing HLS designs. It automates the identification and application of optimal directive configurations from the original source code, focusing on minimizing design latency and ensuring synthesizability. This optimization approach is entirely data-driven, offering a generalized HLS tuning solution without reliance on Quality of Result (QoR) models or meta-heuristics. CollectiveHLS is designed, implemented, and evaluated using around 60 applications sourced from well-established benchmark suites and GitHub repositories, all running on a Xilinx UltraScale + MPSoC ZCU104. It achieves an average geometric mean speedup of up to \\(23.1\\times\\) compared to the official source code without directives, while maintaining synthesizability and feasibility rates of 100% and 96.6%, respectively, matching those of Vitis, the industry-standard framework for FPGA acceleration. Comparisons with resource over-provisioning, traditional genetic algorithm-based Design Space Exploration (DSE), and State-of-the-Art (SotA) approaches demonstrate that CollectiveHLS produces designs of comparable quality \\(14.6\\times\\) faster on average. These results underscore the potential of our approach as an ultra-fast and automated solution for HLS optimization.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4403782727",
    "type": "article"
  },
  {
    "title": "Compressing Neural Networks Using Learnable 1-D Non-Linear Functions",
    "doi": "https://doi.org/10.1145/3705926",
    "publication_date": "2024-12-03",
    "publication_year": 2024,
    "authors": "Gaurav Singh; Kia Bazargan",
    "corresponding_authors": "",
    "abstract": "As deep learning models grow in size to achieve state-of-the-art accuracy, there is a pressing need for compact models. To address this challenge, we introduce a novel operation called Personal Self-Attention (PSA). It is specifically designed to learn non-linear 1-D functions, enhancing existing spline-based methods while remaining compatible with gradient backpropagation. By integrating these non-linear functions with linear transformations, we can achieve the accuracy of larger models but with significantly smaller hidden dimensions, which is crucial for FPGA implementations. We evaluate PSA by implementing it in a Multi-Layer Perceptron (MLP)-based vision model, ResMLP, and testing it on the CIFAR-10 classification task. MLP is gaining increasing popularity due to its widespread use in large-language models. Our results confirm that PSA achieves equivalent accuracy with a 2x smaller hidden size compared to conventional MLPs. Furthermore, by quantizing our non-linear function into a simple lookup table, we reduce the number of operations required by 45%-28%, which offers significant benefits for hardware accelerators. To showcase this, we design an end-to-end unrolled streaming accelerator for ResMLP, demonstrating that our compressed model maintains an 88% accuracy while reducing LUT + DSP resource requirements by 25%, and doubling throughput to 32kFPS. Additionally, we implement a fixed-size SIMD accelerator for the same compressed model that achieves a 62.1% improvement in throughput while only consuming 3.5% extra LUTs.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4404961763",
    "type": "article"
  },
  {
    "title": "Accelerating In-memory Database Functionality with FPGAs",
    "doi": "https://doi.org/10.1145/3706113",
    "publication_date": "2024-12-18",
    "publication_year": 2024,
    "authors": "John J. Leggett; John McGlone; S.S. Demirsoy; Christian Faerber; Vadim Pelyushenko",
    "corresponding_authors": "",
    "abstract": "In this article, we present a hardware offload of part of the delta merge process used in In-Memory Databases (IMDBs). The delta merge process is fundamental in maintaining high transactional throughput for IMDBs. Improving the efficiency of the delta merge process allows running it more frequently, which will improve the performance for transactional throughout for an IMDB. Our FPGA design supports more use cases than existing research, and was demonstrated to be faster than the existing implementation in an enterprise database, offering speedups of between 4 \\(\\times\\) and 100 \\(\\times\\) compared to the CPU optimised implementation, depending on the properties of the database columns.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4405529260",
    "type": "article"
  },
  {
    "title": "A Synthesizable Datapath-Oriented Embedded FPGA Fabric for Silicon Debug Applications",
    "doi": "https://doi.org/10.1145/1331897.1331903",
    "publication_date": "2008-03-01",
    "publication_year": 2008,
    "authors": "Steven J. E. Wilton; Chun Hok Ho; B.R. Quinton; Philip H. W. Leong; Wayne Luk",
    "corresponding_authors": "",
    "abstract": "We present an architecture for a synthesizable datapath-oriented FPGA core that can be used to provide post-fabrication flexibility to an SoC. Our architecture is optimized for bus-based operations and employs a directional routing architecture, which allows it to be synthesized using standard ASIC design tools and flows. The primary motivation for this architecture is to provide an efficient mechanism to support on-chip debugging. The fabric can also be used to implement other datapath-oriented circuits such as those needed in signal processing and computation-intensive applications. We evaluate our architecture using a set of benchmark circuits and compare it to previous fabrics in terms of area, speed, and power.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2114181278",
    "type": "article"
  },
  {
    "title": "ACE-GCN: A Fast Data-driven FPGA Accelerator for GCN Embedding",
    "doi": "https://doi.org/10.1145/3470536",
    "publication_date": "2021-09-13",
    "publication_year": 2021,
    "authors": "José Romero Hung; Chao Li; Pengyu Wang; Chuanming Shao; Jinyang Guo; Jing Wang; Guoyong Shi",
    "corresponding_authors": "",
    "abstract": "ACE-GCN is a fast and resource/energy-efficient FPGA accelerator for graph convolutional embedding under data-driven and in-place processing conditions. Our accelerator exploits the inherent power law distribution and high sparsity commonly exhibited by real-world graphs datasets. Contrary to other hardware implementations of GCN, on which traditional optimization techniques are employed to bypass the problem of dataset sparsity, our architecture is designed to take advantage of this very same situation. We propose and implement an innovative acceleration approach supported by our “implicit-processing-by-association” concept, in conjunction with a dataset-customized convolutional operator. The computational relief and consequential acceleration effect arise from the possibility of replacing rather complex convolutional operations for a faster embedding result estimation. Based on a computationally inexpensive and super-expedited similarity calculation, our accelerator is able to decide from the automatic embedding estimation or the unavoidable direct convolution operation. Evaluations demonstrate that our approach presents excellent applicability and competitive acceleration value. Depending on the dataset and efficiency level at the target, between 23× and 4,930× PyG baseline, coming close to AWB-GCN by 46% to 81% on smaller datasets and noticeable surpassing AWB-GCN for larger datasets and with controllable accuracy loss levels. We further demonstrate the unique hardware optimization characteristics of our approach and discuss its multi-processing potentiality.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3201267481",
    "type": "article"
  },
  {
    "title": "A Real-Time Deep Learning OFDM Receiver",
    "doi": "https://doi.org/10.1145/3494049",
    "publication_date": "2021-12-27",
    "publication_year": 2021,
    "authors": "Stefan Brennsteiner; Tughrul Arslan; John Thompson; A.C. McCormick",
    "corresponding_authors": "",
    "abstract": "Machine learning in the physical layer of communication systems holds the potential to improve performance and simplify design methodology. Many algorithms have been proposed; however, the model complexity is often unfeasible for real-time deployment. The real-time processing capability of these systems has not been proven yet. In this work, we propose a novel, less complex, fully connected neural network to perform channel estimation and signal detection in an orthogonal frequency division multiplexing system. The memory requirement, which is often the bottleneck for fully connected neural networks, is reduced by ≈ 27 times by applying known compression techniques in a three-step training process. Extensive experiments were performed for pruning and quantizing the weights of the neural network detector. Additionally, Huffman encoding was used on the weights to further reduce memory requirements. Based on this approach, we propose the first field-programmable gate array based, real-time capable neural network accelerator, specifically designed to accelerate the orthogonal frequency division multiplexing detector workload. The accelerator is synthesized for a Xilinx RFSoC field-programmable gate array, uses small-batch processing to increase throughput, efficiently supports branching neural networks, and implements superscalar Huffman decoders.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4200348909",
    "type": "article"
  },
  {
    "title": "Approximate Constant-Coefficient Multiplication Using Hybrid Binary-Unary Computing for FPGAs",
    "doi": "https://doi.org/10.1145/3494570",
    "publication_date": "2021-12-27",
    "publication_year": 2021,
    "authors": "S. Rasoul Faraji; Pierre Abillama; Kia Bazargan",
    "corresponding_authors": "",
    "abstract": "Multipliers are used in virtually all Digital Signal Processing (DSP) applications such as image and video processing. Multiplier efficiency has a direct impact on the overall performance of such applications, especially when real-time processing is needed, as in 4K video processing, or where hardware resources are limited, as in mobile and IoT devices. We propose a novel, low-cost, low energy, and high-speed approximate constant coefficient multiplier (CCM) using a hybrid binary-unary encoding method. The proposed method implements a CCM using simple routing networks with no logic gates in the unary domain, which results in more efficient multipliers compared to Xilinx LogiCORE IP CCMs and table-based KCM CCMs (Flopoco) on average. We evaluate the proposed multipliers on 2-D discrete cosine transform algorithm as a common DSP module. Post-routing FPGA results show that the proposed multipliers can improve the {area, area × delay, power consumption, and energy-delay product} of a 2-D discrete cosine transform on average by {30%, 33%, 30%, 31%}. Moreover, the throughput of the proposed 2-D discrete cosine transform is on average 5% more than that of the binary architecture implemented using table-based KCM CCMs. We will show that our method has fewer routability issues compared to binary implementations when implementing a DCT core.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4200415201",
    "type": "article"
  },
  {
    "title": "A Unified FPGA Virtualization Framework for General-Purpose Deep Neural Networks in the Cloud",
    "doi": "https://doi.org/10.1145/3480170",
    "publication_date": "2021-12-27",
    "publication_year": 2021,
    "authors": "Shulin Zeng; Guohao Dai; Hanbo Sun; Jun Liu; Shiyao Li; Guangjun Ge; Kai Zhong; Kaiyuan Guo; Yu Wang; Huazhong Yang",
    "corresponding_authors": "",
    "abstract": "INFerence-as-a-Service (INFaaS) has become a primary workload in the cloud. However, existing FPGA-based Deep Neural Network (DNN) accelerators are mainly optimized for the fastest speed of a single task, while the multi-tenancy of INFaaS has not been explored yet. As the demand for INFaaS keeps growing, simply increasing the number of FPGA-based DNN accelerators is not cost-effective, while merely sharing these single-task optimized DNN accelerators in a time-division multiplexing way could lead to poor isolation and high-performance loss for INFaaS. On the other hand, current cloud-based DNN accelerators have excessive compilation overhead, especially when scaling out to multi-FPGA systems for multi-tenant sharing, leading to unacceptable compilation costs for both offline deployment and online reconfiguration. Therefore, it is far from providing efficient and flexible FPGA virtualization for public and private cloud scenarios. Aiming to solve these problems, we propose a unified virtualization framework for general-purpose deep neural networks in the cloud, enabling multi-tenant sharing for both the Convolution Neural Network (CNN), and the Recurrent Neural Network (RNN) accelerators on a single FPGA. The isolation is enabled by introducing a two-level instruction dispatch module and a multi-core based hardware resources pool. Such designs provide isolated and runtime-programmable hardware resources, which further leads to performance isolation for multi-tenant sharing. On the other hand, to overcome the heavy re-compilation overheads, a tiling-based instruction frame package design and a two-stage static-dynamic compilation, are proposed. Only the lightweight runtime information is re-compiled with ∼1 ms overhead, thus guaranteeing the private cloud’s performance. Finally, the extensive experimental results show that the proposed virtualized solutions achieve up to 3.12× and 6.18× higher throughput in the private cloud compared with the static CNN and RNN baseline designs, respectively.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4200421900",
    "type": "article"
  },
  {
    "title": "SuperDragon",
    "doi": "https://doi.org/10.1145/2740966",
    "publication_date": "2015-09-13",
    "publication_year": 2015,
    "authors": "Guangming Tan; Chunming Zhang; Wendi Wang; Peiheng Zhang",
    "corresponding_authors": "",
    "abstract": "The data deluge in medical imaging processing requires faster and more efficient systems. Due to the advance in recent heterogeneous architecture, there has been a resurgence in research aimed at domain-specific accelerators. In this article, we develop an experimental system SuperDragon for evaluating acceleration of a single-particle Cryo-electron microscopy (Cryo-EM) 3D reconstruction package EMAN through a hybrid of CPU, GPU, and FPGA parallel architecture. Based on a comprehensive workload characterization, we exploit multigrained parallelism in the Cryo-EM 3D reconstruction algorithm and investigate a proper computational mapping to the underlying heterogeneous architecture. The package is restructured with task-level (MPI), thread-level (OpenMP), and data-level (GPU and FPGA) parallelism. Especially, the proposed FPGA accelerator is a stream architecture that emphasizes the importance of optimizing computing dominated data access patterns. Besides, the configurable computing streams are constructed by arranging the hardware modules and bypassing channels to form a linear deep pipeline. Compared to the multicore (six-core) program, the GPU and FPGA implementations achieve speedups of 8.4 and 2.25 times in execution time while improving power efficiency by factors of 7.2 and 14.2, respectively.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W1980934908",
    "type": "article"
  },
  {
    "title": "Automating Elimination of Idle Functions by Runtime Reconfiguration",
    "doi": "https://doi.org/10.1145/2700415",
    "publication_date": "2015-05-11",
    "publication_year": 2015,
    "authors": "Xinyu Niu; Thomas Chau; Qiwei Jin; Wayne Luk; Qiang Liu; Oliver Pell",
    "corresponding_authors": "",
    "abstract": "A design approach is proposed to automatically identify and exploit runtime reconfiguration opportunities with optimised resource utilisation by eliminating idle functions. We introduce Reconfiguration Data Flow Graph, a hierarchical graph structure enabling reconfigurable designs to be synthesised in three steps: function analysis, configuration organisation, and runtime solution generation. The synthesised reconfigurable designs are dynamically evaluated and selected under various runtime conditions. Three applications—barrier option pricing, particle filter, and reverse time migration—are used in evaluating the proposed approach. The runtime solutions approximate their theoretical performance by eliminating idle functions and are 1.31 to 2.19 times faster than optimised static designs. FPGA designs developed with the proposed approach are up to 43.8 times faster than optimised CPU reference designs and 1.55 times faster than optimised GPU designs.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2248318413",
    "type": "article"
  },
  {
    "title": "Value State Flow Graph",
    "doi": "https://doi.org/10.1145/2807702",
    "publication_date": "2015-12-04",
    "publication_year": 2015,
    "authors": "Ali Mustafa Zaidi; David Greaves",
    "corresponding_authors": "",
    "abstract": "Although custom (and reconfigurable) computing can provide orders-of-magnitude improvements in energy efficiency and performance for many numeric, data-parallel applications, performance on nonnumeric, sequential code is often worse than conventional superscalar processors. This work attempts to improve sequential performance in custom hardware by (a) switching from a statically scheduled to a dynamically scheduled (dataflow) execution model and (b) developing a new compiler IR for high-level synthesis—the value state flow graph (VSFG)—that enables aggressive exposition of ILP even in the presence of complex control flow. Compared to existing control-data flow graph (CDFG)-based IRs, the VSFG exposes more instruction-level parallelism from control-intensive sequential code by exploiting aggressive speculation, enabling control dependence analysis, as well as execution along multiple flows of control. This new IR is directly implemented as a static-dataflow graph in hardware by our prototype high-level synthesis tool chain and shows an average speedup of 1.13× over equivalent hardware generated using LegUp, an existing CDFG-based HLS tool. Furthermore, the VSFG allows us to further trade area and energy for performance through loop unrolling, increasing the average speedup to 1.55×, with a peak speedup of 4.05×. Our VSFG-based hardware approaches the sequential cycle counts of an Intel Nehalem Core i7 processor while consuming only 0.25× the energy of an in-order Altera Nios II f processor.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2293133527",
    "type": "article"
  },
  {
    "title": "A Reconfigurable Architecture for the Detection of Strongly Connected Components",
    "doi": "https://doi.org/10.1145/2807700",
    "publication_date": "2015-12-04",
    "publication_year": 2015,
    "authors": "Osama G. Attia; Kevin R. Townsend; Phillip H. Jones; Joseph Zambreno",
    "corresponding_authors": "",
    "abstract": "The Strongly Connected Components (SCCs) detection algorithm serves as a keystone for many graph analysis applications. The SCC execution time for large-scale graphs, as with many other graph algorithms, is dominated by memory latency. In this article, we investigate the design of a parallel hardware architecture for the detection of SCCs in directed graphs. We propose a design methodology that alleviates memory latency and problems with irregular memory access. The design is composed of 16 processing elements dedicated to parallel Breadth-First Search (BFS) and eight processing elements dedicated to finding intersection in parallel. Processing elements are organized to reuse resources and utilize memory bandwidth efficiently. We demonstrate a prototype of our design using the Convey HC-2 system, a commercial high-performance reconfigurable computing coprocessor. Our experimental results show a speedup of as much as 17× for detecting SCCs in large-scale graphs when compared to a conventional sequential software implementation.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2293864981",
    "type": "article"
  },
  {
    "title": "ARC 2014 Over-Clocking KLT Designs on FPGAs under Process, Voltage, and Temperature Variation",
    "doi": "https://doi.org/10.1145/2818380",
    "publication_date": "2015-11-04",
    "publication_year": 2015,
    "authors": "Rui Policarpo Duarte; Christos-Savvas Bouganis",
    "corresponding_authors": "",
    "abstract": "Karhunen-Loeve Transformation is a widely used algorithm in signal processing that often implemented with high-throughput requisites. This work presents a novel methodology to optimise KLT designs on FPGAs that outperform typical design methodologies, through a prior characterisation of the arithmetic units in the datapath of the circuit under various operating conditions. Limited by the ever-increasing process variation, the delay models available in synthesis tools are no longer suitable for extreme performance optimisation of designs, and as they are generic, they need to consider the worst-case performance for a given fabrication process. Hence, they heavily penalise the maximum possible achieved performance of a design by leaving safety margin. This work presents a novel unified optimisation framework which contemplates a prior characterisation of the embedded multipliers on the target FPGA device under process, voltage, and temperature variation. The proposed framework allows a design space exploration leading to designs without any latency overheads that achieve high throughput while producing less errors than typical methodologies, operating with the same throughput. Experimental results demonstrate that the proposed methodology outperforms the typical implementation in three real-life design strategies: high performance, low power, and temperature variation; and it produced circuit designs that performed up to 18dB better when over-clocked.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2293975354",
    "type": "article"
  },
  {
    "title": "Program-Invariant Checking for Soft-Error Detection using Reconfigurable Hardware",
    "doi": "https://doi.org/10.1145/2751563",
    "publication_date": "2015-11-05",
    "publication_year": 2015,
    "authors": "Joon-Seok Park; Pedro C. Diniz",
    "corresponding_authors": "",
    "abstract": "There is an increasing concern about transient errors in deep submicron processor architectures. Software-only error detection approaches that exploit program invariants for silent error detection incur large execution overheads and are unreliable as state can be corrupted after invariant checkpoints. In this article, we explore the use of configurable hardware structures for the continuous evaluation of high-level program invariants at the assembly level. We evaluate the resource requirements and performance of the proposed predicate-evaluation hardware structures when integrated with a 32-bit MIPS soft core on a contemporary reconfigurable hardware device. The results, for a small set of kernel codes, reveal that these hardware structures require a very small number of hardware resources with negligible impact on the processor core that they are integrated in. Moreover, the amount of resources is fairly insensitive to the complexity of the invariants, thus making the proposed structures an attractive alternative to software-only predicate checking.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2295476163",
    "type": "article"
  },
  {
    "title": "ODoST",
    "doi": "https://doi.org/10.1145/2870639",
    "publication_date": "2016-08-11",
    "publication_year": 2016,
    "authors": "Ting Yu; Chris P. Bradley; Oliver Sinnen",
    "corresponding_authors": "",
    "abstract": "Dynamic biomedical systems are mathematically described by Ordinary Differential Equations (ODEs) and their solution is often one of the most computationally intensive parts in biomedical simulations. With high inherent parallelism, hardware acceleration based on Field-Programmable Gate Arrays (FPGAs) has great potential to increase the computational performance of the model simulations, while being very power-efficient. However, the manual hardware implementation is complex and time consuming. The advantages of FPGA designs can only be realised if there is a general solution to automate the process. In this article, we propose a domain-specific high-level synthesis tool called ODoST that automatically generates an FPGA-based Hardware Accelerator Module (HAM) from a high-level description. In this direct approach, ODE equations are directly mapped to processing pipelines without any intermediate architecture layer of processing elements. We evaluate the generated HAMs on real hardware based on their resource usage, processing speed, and power consumption, and compare them with CPUs and a GPU. The results show that FPGA implementations can achieve 15.3 times more speedup compared to a single core CPU solution and perform similarly to an auto-generated GPU solution, while the FPGA implementations can achieve 14.5 times more power efficiency than the CPU and 3.1 times compared to the optimised GPU solution. Improved speedups are foreseeable based on further optimisations.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2513140321",
    "type": "article"
  },
  {
    "title": "Logic Shrinkage: Learned Connectivity Sparsification for LUT-Based Neural Networks",
    "doi": "https://doi.org/10.1145/3583075",
    "publication_date": "2023-02-10",
    "publication_year": 2023,
    "authors": "Erwei Wang; Marie Auffret; Georgios-Ilias Stavrou; Peter Y. K. Cheung; George A. Constantinides; Mohamed S. Abdelfattah; James J. Davis",
    "corresponding_authors": "",
    "abstract": "Field-programmable gate array (FPGA)–specific deep neural network (DNN) architectures using native lookup tables (LUTs) as independently trainable inference operators have been shown to achieve favorable area-accuracy and energy-accuracy trade-offs. The first work in this area, LUTNet, exhibited state-of-the-art performance for standard DNN benchmarks. In this article, we propose the learned optimization of such LUT-based topologies, resulting in higher-efficiency designs than via the direct use of off-the-shelf, hand-designed networks. Existing implementations of this class of architecture require the manual specification of the number of inputs per LUT, K . Choosing appropriate K a priori is challenging. Doing so at even high granularity, for example, per layer, is a time-consuming and error-prone process that leaves FPGAs’ spatial flexibility underexploited. Furthermore, prior works see LUT inputs connected randomly, which does not guarantee a good choice of network topology. To address these issues, we propose logic shrinkage , a fine-grained netlist pruning methodology enabling K to be automatically learned for every LUT in a neural network targeted for FPGA inference. By removing LUT inputs determined to be of low importance, our method increases the efficiency of the resultant accelerators. Our GPU-friendly solution to LUT input removal is capable of processing large topologies during their training with negligible slowdown. With logic shrinkage, we improve the area and energy efficiency of the best-performing LUTNet implementation of the CNV network classifying CIFAR-10 by 1.54× and 1.31×, respectively, while matching its accuracy. This implementation also reaches 2.71× the area efficiency of an equally accurate, heavily pruned binary neural network (BNN). On ImageNet, with the Bi-Real Net architecture, employment of logic shrinkage results in a post-synthesis area reduction of 2.67× vs. LUTNet, allowing for implementation that was previously impossible on today’s largest FPGAs. We validate the benefits of logic shrinkage in the context of real application deployment by implementing a face mask detection DNN using a BNN, LUTNet, and logic-shrunk layers. Our results show that logic shrinkage results in area gains versus LUTNet (up to 1.20×) and equally pruned BNNs (up to 1.08×), along with accuracy improvements.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4319965883",
    "type": "article"
  },
  {
    "title": "Resource Sharing in Dataflow Circuits",
    "doi": "https://doi.org/10.1145/3597614",
    "publication_date": "2023-06-02",
    "publication_year": 2023,
    "authors": "Lana Josipović; Axel Marmet; Andrea Guerrieri; Paolo Ienne",
    "corresponding_authors": "",
    "abstract": "To achieve resource-efficient hardware designs, high-level synthesis (HLS) tools share (i.e., time-multiplex) functional units among operations of the same type. This optimization is typically performed in conjunction with operation scheduling to ensure the best possible unit usage at each point in time. Dataflow circuits have emerged as an alternative HLS approach to efficiently handle irregular and control-dominated code. However, these circuits do not have a predetermined schedule—in its absence, it is challenging to determine which operations can share a functional unit without a performance penalty. More critically, although sharing seems to imply only some trivial circuitry, time-multiplexing units in dataflow circuits may cause deadlock by blocking certain data transfers and preventing operations from executing. In this paper, we present a technique to automatically identify performance-acceptable resource sharing opportunities in dataflow circuits. More importantly, we describe a sharing mechanism which achieves functionally correct and deadlock-free dataflow designs. On a set of benchmarks obtained from C code, we show that our approach effectively implements resource sharing. It results in significant area savings at a minor performance penalty compared to dataflow circuits which do not support this feature (i.e., it achieves a 64%, 2%, and 18% average reduction in DSPs, LUTs, and FFs, respectively, with an average increase in total execution time of only 2%) and matches the sharing capabilities of a state-of-the-art HLS tool.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4379160785",
    "type": "article"
  },
  {
    "title": "BLOOP: Boolean Satisfiability-based Optimized Loop Pipelining",
    "doi": "https://doi.org/10.1145/3599972",
    "publication_date": "2023-05-30",
    "publication_year": 2023,
    "authors": "Nicolai Fiege; Peter Zipf",
    "corresponding_authors": "",
    "abstract": "Modulo scheduling is the premier technique for throughput maximization of loops in high-level synthesis by interleaving consecutive loop iterations. The number of clock cycles between data insertions is called the initiation interval (II). For throughput maximization, this value should be as low as possible; therefore, its minimization is the main optimization goal. Despite its long historical existence, modulo scheduling always remained a relevant research topic over the years with many exact and heuristic algorithms available in the literature. Nevertheless, we are able to leverage the scalability of modern Boolean Satisfiability (SAT) solvers to outperform state-of-the-art ILP-based algorithms for latency-optimal modulo scheduling for both integer and rational IIs. Our algorithm is able to compute valid modulo schedules for the whole CHStone and MachSuite benchmark suites, with 99% of the solutions being proven to be throughput optimal for a timeout of only 10 minutes per candidate II. For various time limits, not a single tested scheduler from the state of the art is able to compute more verified optimal solutions or even a single schedule with a higher throughput than our proposed approach. Using an HLS toolflow, we show that our algorithm can be effectively used to generate Pareto-optimal FPGA implementations regarding throughput and resource usage.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4381281730",
    "type": "article"
  },
  {
    "title": "HyBNN: Quantifying and Optimizing Hardware Efficiency of Binary Neural Networks",
    "doi": "https://doi.org/10.1145/3631610",
    "publication_date": "2023-11-07",
    "publication_year": 2023,
    "authors": "Geng Yang; Jie Lei; Zhenman Fang; Yunsong Li; Jiaqing Zhang; Weiying Xie",
    "corresponding_authors": "",
    "abstract": "Binary neural network (BNN), where both the weight and the activation values are represented with one bit, provides an attractive alternative to deploy highly efficient deep learning inference on resource-constrained edge devices. However, our investigation reveals that, to achieve satisfactory accuracy gains, state-of-the-art (SOTA) BNNs, such as FracBNN and ReActNet, usually have to incorporate various auxiliary floating-point components and increase the model size, which in turn degrades the hardware performance efficiency. In this article, we aim to quantify such hardware inefficiency in SOTA BNNs and further mitigate it with negligible accuracy loss. First, we observe that the auxiliary floating-point (AFP) components consume an average of 93% DSPs, 46% LUTs, and 62% FFs, among the entire BNN accelerator resource utilization. To mitigate such overhead, we propose a novel algorithm-hardware co-design, called FuseBNN , to fuse those AFP operators without hurting the accuracy. On average, FuseBNN reduces AFP resource utilization to 59% DSPs, 13% LUTs, and 16% FFs. Second, SOTA BNNs often use the compact MobileNetV1 as the backbone network but have to replace the lightweight 3 × 3 depth-wise convolution (DWC) with the 3 × 3 standard convolution (SC, e.g., in ReActNet and our ReActNet-adapted BaseBNN) or even more complex fractional 3 × 3 SC (e.g., in FracBNN) to bridge the accuracy gap. As a result, the model parameter size is significantly increased and becomes 2.25× larger than that of the 4-bit direct quantization with the original DWC (4-Bit-Net); the number of multiply-accumulate operations is also significantly increased so that the overall LUT resource usage of BaseBNN is almost the same as that of 4-Bit-Net. To address this issue, we propose HyBNN , where we binarize depth-wise separation convolution (DSC) blocks for the first time to decrease the model size and incorporate 4-bit DSC blocks to compensate for the accuracy loss. For the ship detection task in synthetic aperture radar imagery on the AMD-Xilinx ZCU102 FPGA, HyBNN achieves a detection accuracy of 94.8% and a detection speed of 615 frames per second (FPS), which is 6.8× faster than FuseBNN+ (94.9% accuracy) and 2.7× faster than 4-Bit-Net (95.9% accuracy). For image classification on the CIFAR-10 dataset on the AMD-Xilinx Ultra96-V2 FPGA, HyBNN achieves 1.5× speedup and 0.7% better accuracy over SOTA FracBNN.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4388455116",
    "type": "article"
  },
  {
    "title": "Across Time and Space: <scp>Senju</scp> ’s Approach for Scaling Iterative Stencil Loop Accelerators on Single and Multiple FPGAs",
    "doi": "https://doi.org/10.1145/3634920",
    "publication_date": "2023-11-29",
    "publication_year": 2023,
    "authors": "Emanuele Del Sozzo; Davide Conficconi; Kentaro Sano",
    "corresponding_authors": "",
    "abstract": "Stencil-based applications play an essential role in high-performance systems as they occur in numerous computational areas, such as partial differential equation solving. In this context, Iterative Stencil Loops (ISLs) represent a prominent and well-known algorithmic class within the stencil domain. Specifically, ISL-based calculations iteratively apply the same stencil to a multi-dimensional point grid multiple times or until convergence. However, due to their iterative and intensive nature, ISLs are highly performance-hungry, demanding specialized solutions. Here, Field Programmable Gate Arrays (FPGAs) represent a valid architectural choice as they enable the design of custom, parallel, and scalable ISL accelerators. Besides, the regular structure of ISLs makes them an ideal candidate for automatic optimization and generation flows. For these reasons, this article introduces Senju , an automation framework for the design of highly parallel ISL accelerators targeting single-/multi-FPGA systems. Given an input description, Senju automates the entire design process and provides accurate performance estimations. The experimental evaluation shows remarkable and scalable results, outperforming single- and multi-FPGA literature approaches under different metrics. Finally, we present a new analysis of temporal and spatial parallelism trade-offs in a real-case scenario and discuss our performance through a single- and novel specialized multi-FPGA formulation of the Roofline Model.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4389140831",
    "type": "article"
  },
  {
    "title": "High-Performance Instruction Scheduling Circuits for Superscalar Out-of-Order Soft Processors",
    "doi": "https://doi.org/10.1145/3093741",
    "publication_date": "2018-01-09",
    "publication_year": 2018,
    "authors": "Henry Wong; Vaughn Betz; Jonathan Rose",
    "corresponding_authors": "",
    "abstract": "Soft processors have a role to play in simplifying field-programmable gate array (FPGA) application design as they can be deployed only when needed, and it is easier to write and debug single-threaded software code than create hardware. The breadth of this second role increases when the performance of the soft processor increases, yet the sophisticated out-of-order superscalar approaches that arrived in the mid-1990s are not employed, despite their area cost now being easily tolerable. In this article, we take an important step toward out-of-order execution in soft processors by exploring instruction scheduling in an FPGA substrate. This differs from the hard-processor design problem because the logic substrate is restricted to LUTs, whereas hard processor scheduling circuits employ CAM and wired-OR structures to great benefit. We discuss both circuit and microarchitectural trade-offs and compare three circuit structures for the scheduler, including a new structure called a fused-logic matrix scheduler . Using our optimized circuits, we show that four-issue distributed schedulers with up to 54 entries can be built with the same cycle time as the commercial Nios II/f soft processor (240MHz). This careful design has the potential to significantly increase both the IPC and raw compute performance of a soft processor, compared to current commercial soft processors.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2782868422",
    "type": "article"
  },
  {
    "title": "Wotan",
    "doi": "https://doi.org/10.1145/3195800",
    "publication_date": "2018-06-30",
    "publication_year": 2018,
    "authors": "Oleg Petelin; Vaughn Betz",
    "corresponding_authors": "",
    "abstract": "FPGA routing architectures consist of routing wires and programmable switches that together account for the majority of the fabric delay and area, making evaluation and optimization of an FPGA’s routing architecture very important. Routing architectures have traditionally been evaluated using a full synthesize, pack, place and route CAD flow over a suite of benchmark circuits. While the results are accurate, a full CAD flow has a long runtime and is often tuned to a specific FPGA architecture type, which limits exploration of different architecture options early in the design process. In this article, we present Wotan, a tool to quickly estimate routability for a wide range of architectures without the use of benchmark circuits. At its core, our routability predictor efficiently counts paths through the FPGA routing graph to (1) estimate the probability of node congestion and (2) estimate the probabilities to successfully route a randomized subset of (source, sink) pairs, which are then combined into an overall routability metric. We describe our predictor and present routability estimates for a range of 6-LUT and 4-LUT architectures using mixes of wire types connected in complex ways, showing a rank correlation of 0.91 with routability results from the full VPR CAD flow while requiring 18× less CPU effort.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2883647869",
    "type": "article"
  },
  {
    "title": "Automated Synthesis of Streaming Transfer Level Hardware Designs",
    "doi": "https://doi.org/10.1145/3243930",
    "publication_date": "2018-06-30",
    "publication_year": 2018,
    "authors": "Marc-André Daigneault; Jean‐Pierre David",
    "corresponding_authors": "",
    "abstract": "As modern field-programmable gate arrays (FPGA) enable high computing performance and efficiency, their programming with low-level hardware description languages is time-consuming and remains a major obstacle to their adoption. High-level synthesis compilers are able to produce register-transfer-level (RTL) designs from C/C++ algorithmic descriptions, but despite allowing significant design-time improvements, these tools are not always able to generate hardware designs that compare to handmade RTL designs. In this article, we consider synthesis from an intermediate-level (IL) language that allows the description of algorithmic state machines handling connections between streaming sources and sinks. However, the interconnection of streaming sources and sinks can lead to cyclic combinational relations, resulting in undesirable behaviors or un-synthesizable designs. We propose a functional-level methodology to automate the resolution of such cyclic relations into acyclic combinational functions. The proposed IL synthesis methodology has been applied to the design of pipelined floating-point cores. The results obtained show how the proposed IL methodology can simplify the description of pipelined architectures while enabling performances that are close to those achievable through an RTL design methodology.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2901251593",
    "type": "article"
  },
  {
    "title": "Fast Adjustable NPN Classification Using Generalized Symmetries",
    "doi": "https://doi.org/10.1145/3313917",
    "publication_date": "2019-04-12",
    "publication_year": 2019,
    "authors": "Xuegong Zhou; Lingli Wang; Alan Mishchenko",
    "corresponding_authors": "",
    "abstract": "NPN classification of Boolean functions is a powerful technique used in many logic synthesis and technology mapping tools in both standard cell and FPGA design flows. Computing the canonical form is the most common approach of Boolean function classification. This article proposes two different hybrid NPN canonical forms and a new algorithm to compute them. By exploiting symmetries under different phase assignment as well as higher-order symmetries, the search space of NPN canonical form computation is pruned and the runtime is dramatically reduced. Nevertheless, the runtime for some difficult functions remains high. Fast heuristic method can be used for such functions to compute semi-canonical forms in a reasonable time. The proposed algorithm can be adjusted to be a slow exact algorithm or a fast heuristic algorithm with lower quality. For exact NPN classification, the proposed algorithm is 40× faster than state-of-the-art. For heuristic classification, the proposed algorithm has similar performance as state-of-the-art with a possibility to trade runtime for quality.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2935888545",
    "type": "article"
  },
  {
    "title": "A Novel FPGA Implementation of a Time-to-Digital Converter Supporting Run-Time Estimation and Compensation",
    "doi": "https://doi.org/10.1145/3322482",
    "publication_date": "2019-05-30",
    "publication_year": 2019,
    "authors": "Van Luan Dinh; Xuan Truong Nguyen; Hyuk‐Jae Lee",
    "corresponding_authors": "",
    "abstract": "Time-to-digital converters (TDCs) are widely used in applications that require the measurement of the time interval between events. In previous designs using a feedback loop and an extended delay line, process-voltage-temperature (PVT) variation often decreases the accuracy of measurements. To overcome the loss of accuracy caused by PVT variation, this study proposes a novel design of a synthesizable TDC that employs run-time estimation and compensation of PVT variation. A delay line consisting of a series of buffers is used to detect the period of a ring oscillator designed to measure the time interval between two events. By comparing the detected period and the system clock, the variation of the oscillation period is compensated at run-time. The proposed TDC is successfully implemented by using a low-cost Xilinx Spartan-6 LX9 FPGA with a 50-MHz oscillator. Experimental results show that the proposed TDC is robust to PVT variation with a resolution of 19.1 ps. In comparison with previous design, the proposed TDC achieves about five times better tradeoff in the area, resolution, and frequency of the reference clock.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2946839483",
    "type": "article"
  },
  {
    "title": "Fast Turnaround HLS Debugging Using Dependency Analysis and Debug Overlays",
    "doi": "https://doi.org/10.1145/3372490",
    "publication_date": "2020-01-28",
    "publication_year": 2020,
    "authors": "Al-Shahna Jamal; Eli Cahill; Jeffrey Goeders; Steven J. E. Wilton",
    "corresponding_authors": "",
    "abstract": "High-level synthesis (HLS) has gained considerable traction over recent years, as it allows for faster development and verification of hardware accelerators than traditional RTL design. While HLS allows for most bugs to be caught during software verification, certain non-deterministic or data-dependent bugs still require debugging the actual hardware system during execution. Recent work has focused on techniques to allow designers to perform in-system debug of HLS circuits in the context of the original software code; however, like RTL debug, the user must still determine the root cause of a bug using small execution traces, with lengthy debug turns. In this work, we demonstrate techniques aimed at reducing the time HLS designers spend performing in-system debug. Our approaches consist of performing data dependency analysis to guide the user in selecting which variables are observed by the debug instrumentation, as well as an associated debug overlay that allows for rapid reconfiguration of the debug logic, enabling rapid switching of variable observation between debug iterations. In addition, our overlay provides additional debug capability, such as selective function tracing and conditional buffer freeze points. We explore the area overhead of these different overlay features, showing a basic overlay with only a 1.7% increase in area overhead from the baseline debug instrumentation, while a deluxe variant offers 2×--7× improvement in trace buffer memory utilization with conditional buffer freeze support.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2999408974",
    "type": "article"
  },
  {
    "title": "HopliteBuf",
    "doi": "https://doi.org/10.1145/3375899",
    "publication_date": "2020-02-13",
    "publication_year": 2020,
    "authors": "Tushar Garg; Saud Wasly; Rodolfo Pellizzoni; Nachiket Kapre",
    "corresponding_authors": "",
    "abstract": "HopliteBuf is a deflection-free, low-cost, and high-speed FPGA overlay Network-on-chip (NoC) with stall-free buffers. It is an FPGA-friendly 2D unidirectional torus topology built on top of HopliteRT overlay NoC. The stall-free buffers in HopliteBuf are supported by static analysis tools based on network calculus that help determine worst-case FIFO occupancy bounds for a prescribed workload. We implement these FIFOs using cheap LUT SRAMs (Xilinx SRL32s and Intel MLABs) to reduce cost. HopliteBuf is a hybrid microarchitecture that combines the performance benefits of conventional buffered NoCs by using stall-free buffers with the cost advantages of deflection-routed NoCs by retaining the lightweight unidirectional torus topology structure. We present two design variants of the HopliteBuf NoC: (1) single corner-turn FIFO ( W → S ) and (2) dual corner-turn FIFO ( W → S + N ). The single corner-turn ( W → S ) design is simpler and only introduces a buffering requirement for packets changing dimension from the X ring to the downhill Y ring (or West to South). The dual corner-turn variant requires two FIFOs for turning packets going downhill ( W → S ) as well as uphill ( W → N ). The dual corner-turn design overcomes the mathematical analysis challenges associated with single corner-turn designs for communication workloads with cyclic dependencies between flow traversal paths at the expense of a small increase in resource cost. Our static analysis delivers bounds that are not only better (in latency) than HopliteRT but also tighter by 2−3×. Across 100 randomly generated flowsets mapped to a 5×5 system size, HopliteBuf is able to route a larger fraction of these flowsets with &lt;128-deep FIFOs, boost worst-case routing latency by ≈ 2× for mutually feasible flowsets, and support a 10% higher injection rate than HopliteRT. At 20% injection rates, HopliteRT is only able to route 1--2% of the flowsets, while HopliteBuf can deliver 40--50% sustainability. When compared to the W → S bkp backpressure-based router, we observe that our HopliteBuf solution offers 25--30% better feasibility at 30--40% lower LUT cost.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3015099477",
    "type": "article"
  },
  {
    "title": "Processing Grid-format Real-world Graphs on DRAM-based FPGA Accelerators with Application-specific Caching Mechanisms",
    "doi": "https://doi.org/10.1145/3391920",
    "publication_date": "2020-06-03",
    "publication_year": 2020,
    "authors": "Zhiyuan Shao; Chenhao Liu; Ruoshi Li; Xiaofei Liao; Hai Jin",
    "corresponding_authors": "",
    "abstract": "Graph processing is one of the important research topics in the big-data era. To build a general framework for graph processing by using a DRAM-based FPGA board with deep memory hierarchy, one of the reasonable methods is to partition a given big graph into multiple small subgraphs, represent the graph with a two-dimensional grid, and then process the subgraphs one after another to divide and conquer the whole problem. Such a method (grid-graph processing) stores the graph data in the off-chip memory devices (e.g., on-board or host DRAM) that have large storage capacities but relatively small bandwidths, and processes individual small subgraphs one after another by using the on-chip memory devices (e.g., FFs, BRAM, and URAM) that have small storage capacities but superior random access performances. However, directly exchanging graph (vertex and edge) data between the processing units in FPGA chip with slow off-chip DRAMs during grid-graph processing leads to limited performances and excessive data transmission amounts between the FPGA chip and off-chip memory devices. In this article, we show that it is effective in improving the performance of grid-graph processing on DRAM-based FPGA hardware accelerators by leveraging the flexibility and programmability of FPGAs to build application-specific caching mechanisms, which bridge the performance gaps between on-chip and off-chip memory devices, and reduce the data transmission amounts by exploiting the localities on data accessing. We design two application-specific caching mechanisms (i.e., vertex caching and edge caching ) to exploit two types of localities (i.e., vertex locality and subgraph locality ) that exist in grid-graph processing, respectively. Experimental results show that with the vertex caching mechanism, our system (named as FabGraph) achieves up to 3.1× and 2.5× speedups for BFS and PageRank, respectively, over ForeGraph when processing medium graphs stored in the on-board DRAM. With the edge caching mechanism, the extension of FabGraph (named as FabGraph+) achieves up to 9.96× speedups for BFS over FPGP when processing large graphs stored in the host DRAM.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3034163943",
    "type": "article"
  },
  {
    "title": "A Protection and Pay-per-use Licensing Scheme for On-cloud FPGA Circuit IPs",
    "doi": "https://doi.org/10.1145/3329861",
    "publication_date": "2019-08-13",
    "publication_year": 2019,
    "authors": "Muhammad E. S. Elrabaa; Mohamed A. Al-Asli; Marwan Abu‐Amara",
    "corresponding_authors": "",
    "abstract": "Using security primitives, a novel scheme for licensing hardware intellectual properties (HWIPs) on Field Programmable Gate Arrays (FPGAs) in public clouds is proposed. The proposed scheme enforces a pay-per-use model, allows HWIP's installation only on specific on-cloud FPGAs, and efficiently protects the HWIPs from being cloned, reverse engineered, or used without the owner's authorization by any party, including a cloud insider. It also provides protection for the users’ designs integrated with the HWIP on the same FPGA. This enables cloud tenants to license HWIPs in the cloud from the HWIP vendors at a relatively low price based on usage instead of paying the expensive unlimited HWIP license fee. The scheme includes a protocol for FPGA authentication, HWIP secure decryption, and usage by the clients without the need for the HWIP vendor to be involved or divulge their secret keys. A complete prototype test-bed implementation showed that the proposed scheme is very feasible with relatively low resource utilization. Experiments also showed that a HWIP could be licensed and set up in the on-cloud FPGA in 0.9s. This is 15 times faster than setting up the same HWIP from outside the cloud, which takes about 14s based on the average global Internet speed.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3043889988",
    "type": "article"
  },
  {
    "title": "A Novel Multicontext Coarse-Grained Reconfigurable Architecture (CGRA) For Accelerating Column-Oriented Databases",
    "doi": "https://doi.org/10.1145/1968502.1968504",
    "publication_date": "2011-05-01",
    "publication_year": 2011,
    "authors": "Pranav Vaidya; John J. Lee",
    "corresponding_authors": "",
    "abstract": "The storage model of column-oriented databases is similar in structure to densely packed matrices/vectors found in many high-performance computing applications. Hence, hardware-accelerated vectorized matrix operations using Reconfigurable Logic (RL) coprocessors may find parallels in hardware acceleration of databases. In this article, we explore this hypothesis by proposing a multicontext, coarse-grained Reconfigurable coprocessor Unit (RU) model that is used to accelerate some of the database operations in hardware for column-oriented databases. We then describe the implementation of hardware algorithms for the equi-join, nonequi-join, and inverse-lookup database operations. Finally, we evaluate these algorithms using a microbenchmark query. Our results indicate that the query execution on the proposed RU model is one to two orders of magnitude faster than the software-only query execution.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W1970076885",
    "type": "article"
  },
  {
    "title": "POWER-MODES",
    "doi": "https://doi.org/10.1145/2392616.2392617",
    "publication_date": "2012-12-01",
    "publication_year": 2012,
    "authors": "Armin Krieg; Johannes Grinschgl; Christian Steger; Reinhold Weiß; Holger Bock; Josef Haid",
    "corresponding_authors": "",
    "abstract": "Innovation cycles have been shortening significantly during the last years. This process puts tremendous pressure on designers of embedded systems for security-or reliability-critical applications. Eventual design problems not detected during design time can lead to lost money, confidentiality, or even loss of life in extreme cases. Therefore it is of vital importance to evaluate a new system for its robustness against intentionally and random induced operational faults. Currently this is generally done using extensive simulation runs using gate-level models or direct measurements on the finished silicon product. These approaches either need a significant amount of time and computational power for these simulations or rely on existing product samples. This article presents a novel system evaluation platform using power emulation and fault injection techniques to provide an additional tool for developers of embedded systems in security-and reliability-critical fields. Faults are emulated using state-of-the-art fault injection methods and a flexible pattern representation approach. The resulting effects of these faults on the power consumption profile are estimated using state-of-the-art power emulation hardware. A modular system augmentation approach provides emulation flexibility similar to fault simulation implementations. The platform enables the efficient evaluation of new hardware or software implementations of critical security or reliability solutions at an early development phase.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2062140121",
    "type": "article"
  },
  {
    "title": "SCF",
    "doi": "https://doi.org/10.1145/2209285.2209286",
    "publication_date": "2012-06-01",
    "publication_year": 2012,
    "authors": "Vikas A. Aggarwal; Greg Stitt; Alan D. George; Changil Yoon",
    "corresponding_authors": "",
    "abstract": "Heterogeneous computing systems comprised of accelerators such as FPGAs, GPUs, and manycore processors coupled with standard microprocessors are becoming an increasingly popular solution for future computing systems due to their higher performance and energy efficiency. Although programming languages and tools are evolving to simplify device-level design, programming such systems is still difficult and time-consuming largely due to system-wide challenges involving communication between heterogeneous devices, which currently require ad hoc solutions. Most communication frameworks and APIs which have dominated parallel application development for decades were developed for homogeneous systems, and hence cannot be directly employed for hybrid systems. To solve this problem, this article presents the System Coordination Framework (SCF), which employs message passing to transparently enable communication between tasks described using different programming tools (and languages), and running on heterogeneous processing devices of systems from domains ranging from embedded systems to High-Performance Computing (HPC) systems. By hiding low-level architectural details of the underlying communication from an application designer, SCF can improve application development productivity, provide higher levels of application portability, and offer rapid design-space exploration of different task/device mappings. In addition, SCF enables custom communication synthesis that exploits mechanisms specific to different devices and platforms, which can provide performance improvements over generic solutions employed previously. Our results indicate a performance improvement of 28× and 682× by employing FPGA devices for two applications presented in this article, while simultaneously improving the developer productivity by approximately 2.5 to 5 times by using SCF.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2067153009",
    "type": "article"
  },
  {
    "title": "Specializing FGPU for Persistent Deep Learning",
    "doi": "https://doi.org/10.1145/3457886",
    "publication_date": "2021-06-30",
    "publication_year": 2021,
    "authors": "Rui Ma; Jia-Ching Hsu; Tian Tan; Eriko Nurvitadhi; David Sheffield; Rob Pelt; Martin Langhammer; Jaewoong Sim; Aravind Dasu; Derek Chiou",
    "corresponding_authors": "",
    "abstract": "Overlay architectures are a good way to enable fast development and debug on FPGAs at the expense of potentially limited performance compared to fully customized FPGA designs. When used in concert with hand-tuned FPGA solutions, performant overlay architectures can improve time-to-solution and thus overall productivity of FPGA solutions. This work tunes and specializes FGPU, an open source OpenCL-programmable GPU overlay for FPGAs. We demonstrate that our persistent deep learning (PDL )-FGPU architecture maintains the ease-of-programming and generality of GPU programming while achieving high performance from specialization for the persistent deep learning domain. We also propose an easy method to specialize for other domains. PDL-FGPU includes new instructions, along with micro-architecture and compiler enhancements. We evaluate both the FGPU baseline and the proposed PDL-FGPU on a modern high-end Intel Stratix 10 2800 FPGA in simulation running persistent DL applications (RNN, GRU, LSTM), and non-DL applications to demonstrate generality. PDL-FGPU requires 1.4–3× more ALMs, 4.4–6.4× more M20ks, and 1–9.5× more DSPs than baseline, but improves performance by 56–693× for PDL applications with an average 23.1% degradation on non-PDL applications. We integrated the PDL-FGPU overlay into Intel OPAE to measure real-world performance/power and demonstrate that PDL-FGPU is only 4.0–10.4× slower than the Nvidia V100.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3180683307",
    "type": "article"
  },
  {
    "title": "Reconfigurable Framework for Resilient Semantic Segmentation for Space Applications",
    "doi": "https://doi.org/10.1145/3472770",
    "publication_date": "2021-09-13",
    "publication_year": 2021,
    "authors": "Sebastian Sabogal; Alan D. George; Gary Crum",
    "corresponding_authors": "",
    "abstract": "Deep learning (DL) presents new opportunities for enabling spacecraft autonomy, onboard analysis, and intelligent applications for space missions. However, DL applications are computationally intensive and often infeasible to deploy on radiation-hardened (rad-hard) processors, which traditionally harness a fraction of the computational capability of their commercial-off-the-shelf counterparts. Commercial FPGAs and system-on-chips present numerous architectural advantages and provide the computation capabilities to enable onboard DL applications; however, these devices are highly susceptible to radiation-induced single-event effects (SEEs) that can degrade the dependability of DL applications. In this article, we propose Reconfigurable ConvNet (RECON), a reconfigurable acceleration framework for dependable, high-performance semantic segmentation for space applications. In RECON, we propose both selective and adaptive approaches to enable efficient SEE mitigation. In our selective approach, control-flow parts are selectively protected by triple-modular redundancy to minimize SEE-induced hangs, and in our adaptive approach, partial reconfiguration is used to adapt the mitigation of dataflow parts in response to a dynamic radiation environment. Combined, both approaches enable RECON to maximize system performability subject to mission availability constraints. We perform fault injection and neutron irradiation to observe the susceptibility of RECON and use dependability modeling to evaluate RECON in various orbital case studies to demonstrate a 1.5–3.0× performability improvement in both performance and energy efficiency compared to static approaches.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3199457779",
    "type": "article"
  },
  {
    "title": "Approaches for FPGA Design Assurance",
    "doi": "https://doi.org/10.1145/3491233",
    "publication_date": "2021-12-27",
    "publication_year": 2021,
    "authors": "Eli Cahill; Brad Hutchings; Jeffrey Goeders",
    "corresponding_authors": "",
    "abstract": "Field-Programmable Gate Arrays (FPGAs) are widely used for custom hardware implementations, including in many security-sensitive industries, such as defense, communications, transportation, medical, and more. Compiling source hardware descriptions to FPGA bitstreams requires the use of complex computer-aided design (CAD) tools. These tools are typically proprietary and closed-source, and it is not possible to easily determine that the produced bitstream is equivalent to the source design. In this work, we present various FPGA design flows that leverage pre-synthesizing or pre-implementing parts of the design, combined with open-source synthesis tools, bitstream-to-netlist tools, and commercial equivalence-checking tools, to verify that a produced hardware design is equivalent to the designer’s source design. We evaluate these different design flows on several benchmark circuits and demonstrate that they are effective at detecting malicious modifications made to the design during compilation. We compare our proposed design flows with baseline commercial design flows and measure the overheads to area and runtime.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4200122249",
    "type": "article"
  },
  {
    "title": "Space Optimization on Counters for FPGA-Based Perl Compatible Regular Expressions",
    "doi": "https://doi.org/10.1145/1575779.1575783",
    "publication_date": "2009-09-01",
    "publication_year": 2009,
    "authors": "Chia‐Tien Dan Lo; Yi-Gang Tai",
    "corresponding_authors": "",
    "abstract": "With their expressiveness and simplicity, Perl compatible regular expressions (PCREs) have been adopted in mainstream signature based network intrusion detection systems (NIDSs) to describe known attack signatures, especially for polymorphic worms. NIDSs rely on an underlying string matching engine that simulates PCREs to inspect each network packet. PCRE is a superset of traditional regular expressions, and provides advanced features. However, this pattern matching becomes a performance bottleneck of software-based NIDSs, causing a big portion of their execution time to be dedicated to payload inspection, which results in an unacceptable packet drop rate. The penetration of these unexamined packets creates a security hole in such systems. Over the past decade, hardware acceleration for the pattern matching has been studied extensively and a marginal performance has been achieved. Among hardware approaches, FPGA-based acceleration engines provide great flexibility because new signatures can be compiled and programmed into their reconfigurable architecture. As more and more malicious signatures are discovered, it becomes harder to map a complete set of malicious signatures specified in PCREs to an FPGA chip. One of the space consuming components is the counter used in the constrained repetitions for PCREs. Therefore, we propose a space efficient SelectRAM counter for PCREs that use counting. The design takes advantage of the basic components contained in a configurable logic block, and thus optimizes space usage. A set of basic PCRE blocks has been built in hardware to implement PCREs. Experimental results show that the proposed scheme outperforms existing designs by at least fivefold.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W1976739977",
    "type": "article"
  },
  {
    "title": "Efficient Heterogeneous Architecture Floorplan Optimization using Analytical Methods",
    "doi": "https://doi.org/10.1145/1857927.1857930",
    "publication_date": "2010-12-01",
    "publication_year": 2010,
    "authors": "Asma Kahoul; Alastair Smith; George A. Constantinides; Peter Y. K. Cheung",
    "corresponding_authors": "",
    "abstract": "This paper argues the case for the use of analytical models in FPGA architecture exploration. We show that the problem, when simplified, is amenable to formal optimization techniques such as integer linear programming. However, the simplification process may lead to inaccurate models. To test the overall methodology, we feed the resulting architectures to VPR 5.0 and quantify their performance in comparison with traditional design methodologies. Our results show that the resulting architectures are better than those found using parameter sweep techniques. In addition, we show that these architectures can be further improved by combining the accuracy of VPR 5.0 with the efficiency of analytical techniques. This is achieved using a closed loop framework which iteratively refines the analytical model using the place and route outputs from VPR.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W1986707531",
    "type": "article"
  },
  {
    "title": "Runtime Reconfiguration of Multiprocessors Based on Compile-Time Analysis",
    "doi": "https://doi.org/10.1145/1839480.1839487",
    "publication_date": "2010-09-01",
    "publication_year": 2010,
    "authors": "Madhura Purnaprajna; Mario Porrmann; Ulrich Rueckert; Michael Hussmann; Michael Thies; Uwe Kastens",
    "corresponding_authors": "",
    "abstract": "In multiprocessors, performance improvement is typically achieved by exploring parallelism with fixed granularities, such as instruction-level, task-level, or data-level parallelism. We introduce a new reconfiguration mechanism that facilitates variations in these granularities in order to optimize resource utilization in addition to performance improvements. Our reconfigurable multiprocessor QuadroCore combines the advantages of reconfigurability and parallel processing. In this article, a unified hardware-software approach for the design of our QuadroCore is presented. This design flow is enabled via compiler-driven reconfiguration which matches application-specific characteristics to a fixed set of architectural variations. A special reconfiguration mechanism has been developed that alters the architecture within a single clock cycle. The QuadroCore has been implemented on Xilinx XC2V6000 for functional validation and on UMC’s 90nm standard cell technology for performance estimation. A diverse set of applications have been mapped onto the reconfigurable multiprocessor to meet orthogonal performance characteristics in terms of time and power. Speedup measurements show a 2--11 times performance increase in comparison to a single processor. Additionally, the reconfiguration scheme has been applied to save power in data-parallel applications. Gate-level simulations have been performed to measure the power-performance trade-offs for two computationally complex applications. The power reports confirm that introducing this scheme of reconfiguration results in power savings in the range of 15--24%.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2019654652",
    "type": "article"
  },
  {
    "title": "An FPGA Logic Cell and Carry Chain Configurable as a 6:2 or 7:2 Compressor",
    "doi": "https://doi.org/10.1145/1575774.1575778",
    "publication_date": "2009-09-01",
    "publication_year": 2009,
    "authors": "Hadi Parandeh-Afshar; Philip Brisk; Paolo Ienne",
    "corresponding_authors": "",
    "abstract": "To improve FPGA performance for arithmetic circuits that are dominated by multi-input addition operations, an FPGA logic block is proposed that can be configured as a 6:2 or 7:2 compressor. Compressors have been used successfully in the past to realize parallel multipliers in VLSI technology; however, the peculiar structure of FPGA logic blocks, coupled with the high cost of the routing network relative to ASIC technology, renders compressors ineffective when mapped onto the general logic of an FPGA. On the other hand, current FPGA logic cells have already been enhanced with carry chains to improve arithmetic functionality, for example, to realize fast ternary carry-propagate addition. The contribution of this article is a new FPGA logic cell that is specialized to help realize efficient compressor trees on FPGAs. The new FPGA logic cell has two variants that can respectively be configured as a 6:2 or a 7:2 compressor using additional carry chains that, coupled with lookup tables, provide the necessary functionality. Experiments show that the use of these modified logic cells significantly reduces the delay of compressor trees synthesized on FPGAs compared to state-of-the-art synthesis techniques, with a moderate increase in area and power consumption.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2160331356",
    "type": "article"
  },
  {
    "title": "Efficient Branch and Bound on FPGAs Using Work Stealing and Instance-Specific Designs",
    "doi": "https://doi.org/10.1145/3053687",
    "publication_date": "2017-06-27",
    "publication_year": 2017,
    "authors": "Heinrich Riebler; Michael Lass; Robert Mittendorf; Thomas Löcke; Christian Plessl",
    "corresponding_authors": "",
    "abstract": "Branch and bound (B8B) algorithms structure the search space as a tree and eliminate infeasible solutions early by pruning subtrees that cannot lead to a valid or optimal solution. Custom hardware designs significantly accelerate the execution of these algorithms. In this article, we demonstrate a high-performance B8B implementation on FPGAs. First, we identify general elements of B8B algorithms and describe their implementation as a finite state machine. Then, we introduce workers that autonomously cooperate using work stealing to allow parallel execution and full utilization of the target FPGA. Finally, we explore advantages of instance-specific designs that target a specific problem instance to improve performance. We evaluate our concepts by applying them to a branch and bound problem, the reconstruction of corrupted AES keys obtained from cold-boot attacks. The evaluation shows that our work stealing approach is scalable with the available resources and provides speedups proportional to the number of workers. Instance-specific designs allow us to achieve an overall speedup of 47 × compared to the fastest implementation of AES key reconstruction so far. Finally, we demonstrate how instance-specific designs can be generated just-in-time such that the provided speedups outweigh the additional time required for design synthesis.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2725670766",
    "type": "article"
  },
  {
    "title": "Reducing the Performance Gap between Soft Scalar CPUs and Custom Hardware with TILT",
    "doi": "https://doi.org/10.1145/3079757",
    "publication_date": "2017-06-27",
    "publication_year": 2017,
    "authors": "Ilian Tili; Kalin Ovtcharov; J. Gregory Steffan",
    "corresponding_authors": "",
    "abstract": "By using resource sharing field-programmable gate array (FPGA) compute engines, we can reduce the performance gap between soft scalar CPUs and resource-intensive custom datapath designs. This article demonstrates that Thread- and Instruction-Level parallel Template architecture (TILT), a programmable FPGA-based horizontally microcoded compute engine designed to highly utilize floating point (FP) functional units (FUs), can improve significantly the average throughput of eight FP-intensive applications compared to a soft scalar CPU (similar to a FP-extended Nios). For eight benchmark applications, we show that: (i) a base TILT configuration having a single instance for each FU type can improve the performance over a soft scalar CPU by 15.8 × , while requiring on average 26% of the custom datapaths’ area; (ii) selectively increasing the number of FUs can more than double TILT’s average throughput, reducing the custom-datapath-throughput-gap from 576 × to 14 × ; and (iii) replicated instances of the most computationally dense TILT configuration that fit within the area of each custom datapath design can reduce the gap to 8.27 × , while replicated instances of application-tuned configurations of TILT can reduce the custom-datapath-throughput-gap to an average of 5.22 × , and up to 3.41 × for the Matrix Multiply benchmark. Last, we present methods for design space reduction, and we correctly predict the computationally densest design for seven out of eight benchmarks.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2731391612",
    "type": "article"
  },
  {
    "title": "Network on Chip Architecture for Multi-Agent Systems in FPGA",
    "doi": "https://doi.org/10.1145/3121112",
    "publication_date": "2017-11-22",
    "publication_year": 2017,
    "authors": "Eduardo A. Gerlein; T.M. McGinnity; Ammar Belatreche; Sonya Coleman",
    "corresponding_authors": "",
    "abstract": "A system of interacting agents is, by definition, very demanding in terms of computational resources. Although multi-agent systems have been used to solve complex problems in many areas, it is usually very difficult to perform large-scale simulations in their targeted serial computing platforms. Reconfigurable hardware, in particular Field Programmable Gate Arrays devices, have been successfully used in High Performance Computing applications due to their inherent flexibility, data parallelism, and algorithm acceleration capabilities. Indeed, reconfigurable hardware seems to be the next logical step in the agency paradigm, but only a few attempts have been successful in implementing multi-agent systems in these platforms. This article discusses the problem of inter-agent communications in Field Programmable Gate Arrays. It proposes a Network-on-Chip in a hierarchical star topology to enable agents’ transactions through message broadcasting using the Open Core Protocol as an interface between hardware modules. A customizable router microarchitecture is described and a multi-agent system is created to simulate and analyse message exchanges in a generic heavy traffic load agent-based application. Experiments have shown a throughput of 1.6Gbps per port at 100MHz without packet loss and seamless scalability characteristics.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2761999636",
    "type": "article"
  },
  {
    "title": "Efficient Reconfigurable Architecture for Pricing Exotic Options",
    "doi": "https://doi.org/10.1145/3158228",
    "publication_date": "2017-12-22",
    "publication_year": 2017,
    "authors": "Pieter Fabry; David B. Thomas",
    "corresponding_authors": "",
    "abstract": "This article presents a new method for Monte Carlo (MC) option pricing using field-programmable gate arrays (FPGAs), which use a discrete-space random walk over a binomial lattice, rather than the continuous space-walks used by existing approaches. The underlying hypothesis is that the discrete-space walk will significantly reduce the area needed for each MC engine, and the resulting increase in parallelisation and raw performance outweighs any accuracy losses introduced by the discretisation. Experimental results support this hypothesis, showing that for a given MC simulation size, there is no significant loss in accuracy by using a discrete space model for the path-dependent exotic financial options. Analysis of the binomial simulation model shows that only limited-precision fixed-point arithmetic is needed, and also shows that pairs of MC kernels are able to share RAM resources. When using realistic constraints on pricing problems, it was found that the size of a discrete-space MC engine can be kept to 370 Flip-Flops and 233 Lookup Tables, allowing up to 3,000 variance-reduced MC cores in one FPGA. The combination of a highly parallelisable architecture and model-specific optimisations means that the binomial pricing technique allows for a 50× improvement in throughput compared to existing FPGA approaches, without any reduction in accuracy.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2779169358",
    "type": "article"
  },
  {
    "title": "Improving Energy Efficiency of CGRAs with Low-Overhead Fine-Grained Power Domains",
    "doi": "https://doi.org/10.1145/3558394",
    "publication_date": "2022-08-27",
    "publication_year": 2022,
    "authors": "Ankita Nayak; Keyi Zhang; Rajsekhar Setaluri; Alex Carsello; Makai Mann; Christopher Torng; Stephen Richardson; Rick Bahr; Pat Hanrahan; Mark Horowitz; Priyanka Raina",
    "corresponding_authors": "",
    "abstract": "To effectively minimize static power for a wide range of applications, power domains for coarse-grained reconfigurable array (CGRA) architectures need to be more fine-grained than those found in a typical application-specific integrated circuit. However, the special isolation logic needed to ensure electrical protection between off and on domains makes fine-grained power domains area- and timing-inefficient. We propose a novel design of the CGRA routing fabric that reduces the area overhead of power domain boundary protection from around 9% to less than 1% without incurring any extra timing delay from the isolation cells. Conventional Unified Power Format based flow for power domain boundary protection does not support this design choice. Therefore, we create our own compiler-like passes that iteratively introduce the needed design changes, and formally verify the transformations using methods based on satisfiability modulo theories. These passes also let us optimize how we handle test and debug signals through the off tiles in the CGRA. Using our framework, we add power domains to a CGRA that we designed and taped out. The CGRA has 32 × 16 processing element and memory tiles and 4-MB secondary memory. We address the implementation challenges encountered due to the introduction of fine-grained power domains, including the addressing of the CGRA tiles, the power grid design, well substrate connections, and distribution of global signals. Our CGRA achieves up to 83% reduction in leakage power and 26% reduction in total power versus an identical CGRA without multiple power domains, for a range of image processing and machine learning applications.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4293367110",
    "type": "article"
  },
  {
    "title": "FPGA Acceleration of Probabilistic Sentential Decision Diagrams with High-level Synthesis",
    "doi": "https://doi.org/10.1145/3561514",
    "publication_date": "2022-09-06",
    "publication_year": 2022,
    "authors": "Young Choi; Carlos Santillana; Yujia Shen; Adnan Darwiche; Jason Cong",
    "corresponding_authors": "",
    "abstract": "Probabilistic Sentential Decision Diagrams (PSDDs) provide efficient methods for modeling and reasoning with probability distributions in the presence of massive logical constraints. PSDDs can also be synthesized from graphical models such as Bayesian networks (BNs) therefore offering a new set of tools for performing inference on these models (in time linear in the PSDD size). Despite these favorable characteristics of PSDDs, we have found multiple challenges in PSDD’s FPGA acceleration. Problems include limited parallelism, data dependency, and small pipeline iterations. In this article, we propose several optimization techniques to solve these issues with novel pipeline scheduling and parallelization schemes. We designed the PSDD kernel with a high-level synthesis (HLS) tool for ease of implementation and verified it on the Xilinx Alveo U250 board. Experimental results show that our methods improve the baseline FPGA HLS implementation performance by 2,200X and the multicore CPU implementation by 20X. The proposed design also outperforms state-of-the-art BN and Sum Product Network (SPN) accelerators that store the graph information in memory.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4294733180",
    "type": "article"
  },
  {
    "title": "Fixed-point FPGA Implementation of the FFT Accumulation Method for Real-time Cyclostationary Analysis",
    "doi": "https://doi.org/10.1145/3567429",
    "publication_date": "2022-10-10",
    "publication_year": 2022,
    "authors": "Carol Jingyi Li; Xiangwei Li; Binglei Lou; Craig Jin; David Boland; Philip H. W. Leong",
    "corresponding_authors": "",
    "abstract": "The spectral correlation density (SCD) is an important tool in cyclostationary signal detection and classification. Even using efficient techniques based on the fast Fourier transform (FFT), real-time implementations are challenging because of the high computational complexity. A key dimension for computational optimization lies in minimizing the wordlength employed. In this article, we analyze the relationship between wordlength and signal-to-quantization noise in fixed-point implementations of the SCD function. A canonical SCD estimation algorithm, the FFT accumulation method (FAM) using fixed-point arithmetic, is studied. We derive closed-form expressions for SQNR and compare them at wordlengths ranging from 14 to 26 bits. The differences between the calculated SQNR and bit-exact simulations are less than 1 dB. Furthermore, an HLS-based FPGA design is implemented on a Xilinx Zynq UltraScale+ XCZU28DR-2FFVG1517E RFSoC. Using less than 25% of the logic fabric on the device, it consumes 7.7 W total on-chip power and has a power efficiency of 12.4 GOPS/W, which is an order of magnitude improvement over an Nvidia Tesla K40 graphics processing unit (GPU) implementation. In terms of throughput, it achieves 50 MS/sec, which is a speedup of 1.6 over a recent optimized FPGA implementation.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4303984403",
    "type": "article"
  },
  {
    "title": "Particle graphics on reconfigurable hardware",
    "doi": "https://doi.org/10.1145/1391732.1391735",
    "publication_date": "2008-09-01",
    "publication_year": 2008,
    "authors": "John Sachs Beeckler; Warren J. Gross",
    "corresponding_authors": "",
    "abstract": "Particle graphics simulations are well suited for modeling complex phenomena such as water, cloth, explosions, fire, smoke, and clouds. They are normally realized in software as part of an interactive graphics application. The computational complexity of particle graphics simulations restricts the number of particles that can be updated in software at interactive frame rates. This article presents the design and implementation of a hardware particle graphics engine for accelerating real-time particle graphics simulations. We explore the design process, implementation issues, and limitations of using field-programmable gate arrays (FPGAs) for the acceleration of particle graphics. The FPGA particle engine processes million-particle systems at a rate from 47 to 112 million particles per second, which represents one to two orders of magnitude speedup over a 2.8 GHz CPU. Using three FPGAs, a maximum sustained performance of 112 million particles per second was achieved.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2020433498",
    "type": "article"
  },
  {
    "title": "FRoC 2.0",
    "doi": "https://doi.org/10.1145/3354188",
    "publication_date": "2019-09-09",
    "publication_year": 2019,
    "authors": "Ibrahim Ahmed; Shuze Zhao; James Meijers; Olivier Trescases; Vaughn Betz",
    "corresponding_authors": "",
    "abstract": "In earlier technology nodes, FPGAs had low power consumption compared to other compute chips such as CPUs and GPUs. However, in the 14nm technology node, FPGAs are consuming unprecedented power in the 100+W range, making power consumption a pressing concern. To reduce FPGA power consumption, several researchers have proposed deploying dynamic voltage scaling. While the previously proposed solutions show promising results, they have difficulty guaranteeing safe operation at reduced voltages for applications that use the FPGA hard blocks. In this work, we present the first DVS solution that is able to fully handle FPGA applications that use BRAMs. Our solution not only robustly tests the soft logic component of the application but also tests all components connected to the BRAMs. We extend a previously proposed CAD tool, FRoC, to automatically generate calibration bitstreams that are used to measure the application’s critical path delays on silicon. The calibration bitstreams also include testers that ensure all used SRAM cells operate safely while scaling V dd . We experimentally show that using our DVS solution we can save 32% of the total power consumed by a discrete Fourier transform application running with the fixed nominal supply voltage and clocked at the F max reported by static timing analysis.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2972436488",
    "type": "article"
  },
  {
    "title": "CoNFV",
    "doi": "https://doi.org/10.1145/3409113",
    "publication_date": "2020-08-18",
    "publication_year": 2020,
    "authors": "Xuzhi Zhang; Xiaozhe Shao; George Provelengios; Naveen Kumar Dumpala; Lixin Gao; Russell Tessier",
    "corresponding_authors": "",
    "abstract": "Network function virtualization (NFV) is a powerful networking approach that leverages computing resources to perform a time-varying set of network processing functions. Although microprocessors can be used for this purpose, their performance limitations and lack of specialization present implementation challenges. In this article, we describe a new heterogeneous hardware-software NFV platform called CoNFV that provides scalability and programmability while supporting significant hardware-level parallelism and reconfiguration. Our computing platform takes advantage of both field-programmable gate arrays (FPGAs) and microprocessors to implement numerous virtual network functions (VNF) that can be dynamically customized to specific network flow needs. The most distinctive feature of our system is the use of global network state to coordinate NFV operations. Traffic management and hardware reconfiguration functions are performed by a global coordinator that allows for the rapid sharing of network function states and continuous evaluation of network function needs. With the help of state sharing mechanism offered by the coordinator, customer-defined VNF instances can be easily migrated between heterogeneous middleboxes as the network environment changes. A resource allocation and scheduling algorithm dynamically assesses resource deployments as network flows and conditions are updated. We show that our deployment algorithm can successfully reallocate FPGA and microprocessor resources in a fraction of a second in response to changes in network flow capacity and network security threats including intrusion.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3060398970",
    "type": "article"
  },
  {
    "title": "Graph-Based Approaches to Placement of Processing Element Networks on FPGAs for Physical Model Simulation",
    "doi": "https://doi.org/10.1145/2629521",
    "publication_date": "2014-12-15",
    "publication_year": 2014,
    "authors": "Bailey Miller; Frank Vahid; Tony Givargis; Philip Brisk",
    "corresponding_authors": "",
    "abstract": "Physical models utilize mathematical equations to characterize physical systems like airway mechanics, neuron networks, or chemical reactions. Previous work has shown that field programmable gate arrays (FPGAs) execute physical models efficiently. To improve the implementation of physical models on FPGAs, this article leverages graph theoretic techniques to synthesize physical models onto FPGAs. The first phase maps physical model equations onto a structured virtual processing element (PE) graph using graph theoretic folding techniques. The second phase maps the structured virtual PE graph onto physical PE regions on an FPGA using graph embedding theory. A simulated annealing algorithm is introduced that can map any physical model onto an FPGA regardless of the model's underlying topology. We further extend the simulated annealing approach by leveraging existing graph drawing algorithms to generate the initial placement. Compared to previous work on physical model implementation on FPGAs, embedding increases clock frequency by 25% on average (for applicable topologies), whereas simulated annealing increases frequency by 13% on average. The embedding approach typically produces a circuit whose frequency is limited by the FPGA clock instead of routing. Additionally, complex models that could not previously be routed due to complexity were made routable when using placement constraints.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2033648414",
    "type": "article"
  },
  {
    "title": "Analyzing System-Level Information’s Correlation to FPGA Placement",
    "doi": "https://doi.org/10.1145/2501985",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Farnaz Gharibian; Lesley Shannon; Peter Jamieson; Kevin Chung",
    "corresponding_authors": "",
    "abstract": "One popular placement algorithms for Field-Programmable Gate Arrays (FPGAs) is called Simulated Annealing (SA). This algorithm tries to create a good quality placement from a flattened design that no longer contains any high-level information related to the original design hierarchy. Placement is an NP-hard problem, and as the size and complexity of designs implemented on FPGAs increases, SA does not scale well to find good solutions in a timely fashion. In this article, we investigate if system-level information can be reconstructed from a flattened netlist and evaluate how that information is realized in terms of its locality in the final placement. If there is a strong relationship between good quality placements and system-level information, then it may be possible to divide a large design into smaller components and improve the time needed to create a good quality placement. Our preliminary results suggest that the locality property of the information embedded in the system-level HDL structure (i.e. “module”, “always”, and “if” statements) is greatly affected by designer HDL coding style. Therefore, a reconstructive algorithm, called Affinity Propagation, is also considered as a possible method of generating a meaningful coarse-grain picture of the design.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2039194939",
    "type": "article"
  },
  {
    "title": "On the Impact of Replacing Low-Speed Configuration Buses on FPGAs with the Chip’s Internal Configuration Infrastructure",
    "doi": "https://doi.org/10.1145/2700835",
    "publication_date": "2015-10-27",
    "publication_year": 2015,
    "authors": "Karel Heyse; Jente Basteleus; Brahim Al Farisi; Dirk Stroobandt; Oliver Kadlcek; Oliver Pell",
    "corresponding_authors": "",
    "abstract": "It is common for large hardware designs to have a number of registers or memories whose contents have to be changed very seldom (e.g., only at startup). The conventional way of accessing these memories is through a low-speed memory bus. This bus uses valuable hardware resources, introduces long global connections, and contributes to routing congestion. Hence, it has an impact on the overall design even though it is only rarely used. A Field-Programmable Gate Array (FPGA) already contains a global communication mechanism in the form of its configuration infrastructure. In this article, we evaluate the use of the configuration infrastructure as a replacement for a low-speed memory bus on the Maxeler HPC platform. We find that by removing the conventional low-speed memory bus, the maximum clock frequency of some applications can be improved by 8%. Improvements by 25% and more are also attainable, but constraints of the Xilinx reconfiguration infrastructure prevent fully exploiting these benefits at the moment. We present a number of possible changes to the Xilinx reconfiguration infrastructure and tools that would solve this and make these results more widely applicable.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2075557986",
    "type": "article"
  },
  {
    "title": "FPGA technology mapping with encoded libraries and staged priority cuts",
    "doi": "https://doi.org/10.1145/2068716.2068721",
    "publication_date": "2011-12-01",
    "publication_year": 2011,
    "authors": "Andrew Kennings; Kristofer Vorwerk; Arun Kundu; Val Pevzner; Andy Fox",
    "corresponding_authors": "",
    "abstract": "Technology mapping is an important step in the FPGA CAD flow in which a network of simple gates is converted into a network of logic blocks. This article considers enhancements to a traditional LUT-based mapping algorithm for an FPGA comprised of logic blocks which implement only a subset of functions of up to k variables; specifically, the logic block is a partial LUT, but it possesses more inputs than a typical LUT. An analysis of the logic block is presented, and techniques for postmapping area recovery and timing-driven buffer insertion are also described. Numerical results are put forth which substantiate the efficacy of the proposed methods using real circuits mapped to a commercial FPGA architecture.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2083030004",
    "type": "article"
  },
  {
    "title": "An Analytical Model for Evaluating Static Power of Homogeneous FPGA Architectures",
    "doi": "https://doi.org/10.1145/2535935",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Yoon Kah Leow; Ali Akoglu; Susan Lysecky",
    "corresponding_authors": "",
    "abstract": "As capacity of the field-programmable gate arrays (FPGAs) continues to increase, power dissipated in the logic and routing resources has become a critical concern for FPGA architects. Recent studies have shown that static power is fast approaching the dynamic power in submicron devices. In this article, we propose an analytical model for relating homogeneous island-style-based FPGA architecture to static power. Current FPGA power models are tightly coupled with CAD tools. Our CAD-independent model captures the static power for a given FPGA architecture based on estimates of routing and logic resource utilizations from a pre-technology mapped netlist. We observe an average correlation ratio (C-Ratio) of 95% and a minimum absolute percentage error (MAPE) rate of 15% with respect to the experimental results generated by the Versatile Placement Routing (VPR) tool over the MCNC benchmarks. Our model offers application engineers and FPGA architects the capability to evaluate the impact of their design choices on static power without having to go through CAD-intensive investigations.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2109841315",
    "type": "article"
  },
  {
    "title": "A Runtime FPGA Placement and Routing Using Low-Complexity Graph Traversal",
    "doi": "https://doi.org/10.1145/2660775",
    "publication_date": "2015-03-17",
    "publication_year": 2015,
    "authors": "Ricardo Ferreira; Luiza Caroline Oliveira Rocha; André Gustavo dos Santos; José Augusto M. Nacif; Stephan Wong; Luigi Carro",
    "corresponding_authors": "",
    "abstract": "Dynamic Partial Reconfiguration (DPaR) enables efficient allocation of logic resources by adding new functionalities or by sharing and/or multiplexing resources over time. Placement and routing (P&amp;R) is one of the most time-consuming steps in the DPaR flow. P&amp;R are two independent NP-complete problems, and, even for medium size circuits, traditional P&amp;R algorithms are not capable of placing and routing hardware modules at runtime. We propose a novel runtime P&amp;R algorithm for Field-Programmable Gate Array (FPGA)-based designs. Our algorithm models the FPGA as an implicit graph with a direct correspondence to the target FPGA. The P&amp;R is performed as a graph mapping problem by exploring the node locality during a depth-first traversal. We perform the P&amp;R using a greedy heuristic that executes in polynomial time. Unlike state-of-the-art algorithms, our approach does not try similar solutions, thus allowing the P&amp;R to execute in milliseconds. Our algorithm is also suitable for P&amp;R in fragmented regions. We generate results for a manufacturer-independent virtual FPGA. Compared with the most popular P&amp;R tool running the same benchmark suite, our algorithm is up to three orders of magnitude faster.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2112872614",
    "type": "article"
  },
  {
    "title": "Memory Interface Design for 3D Stencil Kernels on a Massively Parallel Memory System",
    "doi": "https://doi.org/10.1145/2800788",
    "publication_date": "2015-09-11",
    "publication_year": 2015,
    "authors": "Zheming Jin; Jason D. Bakos",
    "corresponding_authors": "",
    "abstract": "Massively parallel memory systems are designed to deliver high bandwidth at relatively low clock speed for memory-intensive applications implemented on programmable logic. For example, the Convey HC-1 provides 1,024 DRAM banks to each of four FPGAs through a full crossbar, presenting a peak bandwidth of 76.8GB/s to the user logic. Such highly parallel memory systems suffer from high latency, and their effective bandwidth is highly sensitive to access ordering. To achieve high performance, the user must use a customized memory interface that combines scheduling, latency hiding, and data reuse. In this article, we describe the design of a custom memory interface for 3D stencil kernels on the Convey HC-1 that incorporates these features. Experimental results show that the proposed memory interface achieves a speedup in runtime of 2.2 for 6-point stencil and 9.5 for 27-point stencil when compared to a naive memory interface.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2146141543",
    "type": "article"
  },
  {
    "title": "An Enhanced Adaptive Recoding Rotation CORDIC",
    "doi": "https://doi.org/10.1145/2812813",
    "publication_date": "2015-11-02",
    "publication_year": 2015,
    "authors": "Jianfeng Zhang; Paul Chow; Hengzhu Liu",
    "corresponding_authors": "",
    "abstract": "The Conventional Coordinate Rotation Digital Computer (CORDIC) algorithm has been widely used in many applications, particularly in Direct Digital Frequency Synthesizers (DDS) and Fast Fourier Transforms (FFT). However, CORDIC is constrained by the excessive number of iterations, angle data path, and scaling factor compensation. In this article, an enhanced adaptive recoding CORDIC (EARC) is proposed. It uses the enhanced adaptive recoding method to reduce the required iterations and adopts the trigonometric transformation scheme to scale up the rotation angles. Computing sine and cosine is used first to compare the core functionality of EARC with basic CORDIC; then a 16-bit DDS and a 1,024-point FFT based on EARC are evaluated to demonstrate the benefits of EARC in larger applications. All the proposed architectures are validated on a Virtex 5 FPGA development platform. Compared with a commercial implementation of CORDIC, EARC requires 33.3% less hardware resources, provides a twofold speedup, dissipates 70.4% less power, and improves accuracy in terms of the Bit Error Position (BEP). Compared to the state-of-the-art Hybrid CORDIC, EARC reduces latency by 11.1% and consumes 17% less power. Compared with a commercial implementation of DDS, the dissipated power of the proposed DDS is reduced by 27.2%. The proposed DDS improves Spurious-Free Dynamic Range (SFDR) by nearly 7 dBc and dissipates 21.8% less power when compared with a recently published DDS circuit. The FFT based on EARC dissipates a factor of 2.05 less power than the commercial FFT even when choosing the 100% toggle rate for the FFT based on EARC and the 12.5% toggle rate for the commercial FFT. Compared with a recently published FFT, the FFT based on EARC improves Signal-to-Noise Ratio (SNR) by 8.9 dB and consumes 7.78% less power.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2296058057",
    "type": "article"
  },
  {
    "title": "Coarse-Grained Architecture for Fingerprint Matching",
    "doi": "https://doi.org/10.1145/2791296",
    "publication_date": "2015-12-17",
    "publication_year": 2015,
    "authors": "Jinwei Xu; Jingfei Jiang; Yong Dou; Xiaolong Shen; Zhiqiang Liu",
    "corresponding_authors": "",
    "abstract": "Fingerprint matching is a key procedure in fingerprint identification applications. The minutiae-based fingerprint matching algorithm is one of the most typical algorithms achieving a reasonably correct recognition rate. This study proposes a coarse-grained parallel architecture called fingerprint matching core (FMC) to accelerate fingerprint matching. The proposed architecture has a two-level parallel structure (i.e., parallel among groups (PAG) and parallel in group (PIG)). A multirequest controller is added to the PAG structure to obtain a concurrent operation of the multiple processing element group (PEG). The DDR3 controller is used in the PIG structure to read eight minutiae from eight different fingerprints and realize the simultaneous computation of the eight PEs. The whole system is implemented on a Xilinx FPGA board with a Virtex VII XC7VX485T chip. The 16-PEG FMC achieves a throughput of about 9.63 million fingerprint pairs per second, which is larger than that achieved on a Tesla K20c platform. The software execution times are also measured on the 2.93GHz Intel Xeon 5670, 2.3GHz AMD Opteron(tm) Processor 6376, and Tesla K20c platforms. The Intel Xeon 5670 has two processors with 12 cores, and the AMD Opteron(tm) Processor 6376 has two processors with 16 cores. Moreover, the throughput is about 31 times that achieved on a 2.93GHz Intel Xeon 5670 single core.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2297638248",
    "type": "article"
  },
  {
    "title": "Subliminal Channels in the Private Information Retrieval Protocols",
    "doi": null,
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "Meredith L. Patterson; Len Sassaman",
    "corresponding_authors": "",
    "abstract": "Information-theoretic private information retrieval (PIR) protocols, such as those described by Chor et al. [5], provide a mechanism by which users can retrieve information from a database distributed across multiple servers in such a way that neither the servers nor an outside observer can determine the contents of the data being retrieved. More recent PIR protocols also provide protection against Byzantine servers, such that a user can detect when one or more servers have attempted to tamper with the data he has requested. In some cases (as in the protocols presented by Beimel and Stahl [1]), the user can still recover his data and protect the contents of his query if the number of Byzantine servers is below a certain threshold; this property is referred to as Byzantine-recovery. However, tampering with a user’s data is not the only goal a Byzantine server might have. We present a scenario in which an arbitrarily sized coalition of Byzantine servers transforms the userbase of a PIR network into a signaling framework with varying levels of detectability by means of a subliminal channel [11]. We describe several such subliminal channel techniques, illustrate several use-cases for this subliminal channel, and demonstrate its applicability to a wide variety of PIR protocols.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2523888567",
    "type": "article"
  },
  {
    "title": "Fine-Grained Interconnect Synthesis",
    "doi": "https://doi.org/10.1145/2892641",
    "publication_date": "2016-08-11",
    "publication_year": 2016,
    "authors": "Alex Rodionov; David Biancolin; Jonathan Rose",
    "corresponding_authors": "",
    "abstract": "One of the key challenges for the FPGA industry going forward is to make the task of designing hardware easier. A significant portion of that design task is the creation of the interconnect pathways between functional structures. We present a synthesis tool that automates this process and focuses on the interconnect needs in the fine-grained (sub-IP-block) design space. Here there are several issues that prior research and tools do not address well: the need to have fixed, deterministic latency between communicating units (to enable high-performance local communication without the area overheads of latency insensitivity), and the ability to avoid generating unnecessary arbitration hardware when the application design can avoid it. Using a design example, our tool generates interconnect that requires 69% fewer lines of specification code than a handwritten Verilog implementation, which is a 32% overall reduction for the entire application. The resulting system, while requiring 6% more total functional and interconnect area, achieves the same performance. We also show a quantitative and qualitative advantages against an existing commercial interconnect synthesis tool, over which we achieve a 25% performance advantage and 15%/57% logic/memory area savings.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3004351771",
    "type": "article"
  },
  {
    "title": "Statistical Timing and Power Optimization of Architecture and Device for FPGAs",
    "doi": "https://doi.org/10.1145/2209285.2209288",
    "publication_date": "2012-06-01",
    "publication_year": 2012,
    "authors": "Lerong Cheng; Wenyao Xu; Fang Gong; Yan Lin; Ho-Yan Wong; Lei He",
    "corresponding_authors": "",
    "abstract": "Process variation in nanometer technology is becoming an important issue for cutting-edge FPGAs with a multimillion gate capacity. Considering both die-to-die and within-die variations in effective channel length, threshold voltage, and gate oxide thickness, we first develop closed-form models of chip-level FPGA leakage and timing variations. Experiments show that the mean and standard deviation computed by our models are within 3% from those computed by Monte Carlo simulation. We also observe that the leakage and timing variations can be up to 3X and 1.9X, respectively. We then derive analytical yield models considering both leakage and timing variations, and use such models to evaluate the performance of FPGA device and architecture considering process variations. Compared to the baseline, which uses the VPR architecture and device setup based on the ITRS roadmap, device and architecture tuning improves leakage yield by 10.4%, timing yield by 5.7%, and leakage and timing combined yield by 9.4%. We also observe that LUT size of 4 gives the highest leakage yield, LUT size of 7 gives the highest timing yield, but LUT size of 5 achieves the maximum leakage and timing combined yield. To the best of our knowledge, this is the first in-depth study on FPGA architecture and device coevaluation considering process variation.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2125036433",
    "type": "article"
  },
  {
    "title": "Design and Analysis of Configurable Ring Oscillators for True Random Number Generation Based on Coherent Sampling",
    "doi": "https://doi.org/10.1145/3433166",
    "publication_date": "2021-06-05",
    "publication_year": 2021,
    "authors": "Adriaan Peetermans; Vladimir Rožić; Ingrid Verbauwhede",
    "corresponding_authors": "",
    "abstract": "True Random Number Generators (TRNGs) are indispensable in modern cryptosystems. Unfortunately, to guarantee high entropy of the generated numbers, many TRNG designs require a complex implementation procedure, often involving manual placement and routing. In this work, we introduce, analyse, and compare three dynamic calibration mechanisms for the COherent Sampling ring Oscillator based TRNG: GateVar , WireVar , and LUTVar , enabling easy integration of the entropy source into complex systems. The TRNG setup procedure automatically selects a configuration that guarantees the security requirements. In the experiments, we show that two out of the three proposed mechanisms are capable of assuring correct TRNG operation even when an automatic placement is carried out and when the design is ported to another Field-Programmable Gate Array (FPGA) family. We generated random bits on both a Xilinx Spartan 7 and a Microsemi SmartFusion2 implementation that, without post processing, passed the AIS-31 statistical tests at a throughput of 4.65 Mbit/s and 1.47 Mbit/s, respectively.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3168229806",
    "type": "article"
  },
  {
    "title": "Continuous Online Self-Monitoring Introspection Circuitry for Timing Repair by Incremental Partial-Reconfiguration (COSMIC TRIP)",
    "doi": "https://doi.org/10.1145/3158229",
    "publication_date": "2018-01-26",
    "publication_year": 2018,
    "authors": "Hans Giesen; Benjamin Gojman; Raphael Rubin; Ji Kim; André DeHon",
    "corresponding_authors": "",
    "abstract": "We show that continuously monitoring on-chip delays at the LUT-to-LUT link level during operation allows a field-programmable gate array to detect and self-adapt to aging and environmental timing effects. Using a lightweight (&lt;4% added area) mechanism for monitoring transition timing, a Difference Detector with First-Fail Latch, we can estimate the timing margin on circuits and identify the individual links that have degraded and whose delay is determining the worst-case circuit delay. Combined with Choose-Your-own-Adventure precomputed, fine-grained repair alternatives, we introduce a strategy for rapid, in-system incremental repair of links with degraded timing. We show that these techniques allow us to respond to a single aging event in less than 190ms for the toronto20 benchmarks. The result is a step toward systems where adaptive reconfiguration on the time-scale of seconds is viable and beneficial.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2794057135",
    "type": "article"
  },
  {
    "title": "Loop Unrolling for Energy Efficiency in Low-Cost Field-Programmable Gate Arrays",
    "doi": "https://doi.org/10.1145/3289186",
    "publication_date": "2018-12-31",
    "publication_year": 2018,
    "authors": "Naveen Kumar Dumpala; Shivukumar B. Patil; Daniel Holcomb; Russell Tessier",
    "corresponding_authors": "",
    "abstract": "Field-programmable gate arrays (FPGAs) are used for a wide variety of computations in low-cost embedded systems. Although these systems often have modest performance constraints, their energy consumption must typically be limited. Many FPGA applications employ repetitive loops that cannot be straightforwardly split into parallel computations. Performing a loop sequentially generally requires high-speed clocks that consume considerable clock power and sometimes require clock generation using a phase-locked loop (PLL). Loop unrolling addresses the high-speed clock issue, but its use often leads to significant combinational glitch power. In this work, a computer-aided design (CAD) approach that unrolls loops for designs targeted to low-cost FPGAs is described. Our approach considers latency constraints in an effort to minimize energy consumption for loop-based computation. To reduce glitch power, a glitch-filtering approach is introduced that provides a balance between glitch reduction and design performance. Glitch-filter enable signals are generated and routed to the filters using resources best suited to the target FPGA. Our approach automatically inserts glitch filters and associated control logic into a design prior to processing with FPGA synthesis, place, and route tools. Our energy-saving loop-unrolling approach has been evaluated using five benchmarks often used in low-cost FPGAs. The energy-saving capabilities of the approach have been evaluated for an Intel Cyclone IV and a Xilinx Artix-7 FPGA using board-level power measurement. The use of unrolling and glitch filtering is shown to reduce energy by at least 65% for an Artix-7 device and 50% for a Cyclone IV device while meeting design latency constraints.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2912887622",
    "type": "article"
  },
  {
    "title": "Large-scale Cellular Automata on FPGAs",
    "doi": "https://doi.org/10.1145/3423185",
    "publication_date": "2020-12-14",
    "publication_year": 2020,
    "authors": "Nikolaos Kyparissas; Apostolos Dollas",
    "corresponding_authors": "",
    "abstract": "Cellular automata (CA) are discrete mathematical models discovered in the 1940s by John von Neumann and Stanislaw Ulam and have been used extensively in many scientific disciplines ever since. The present work evolved from a Field Programmable Gate Array– (FPGA) based design to simulate urban growth into a generic architecture that is automatically generated by a framework to efficiently compute complex cellular automata with large 29 × 29 neighborhoods in Cartesian or toroidal grids, with 16 or 256 states per cell. The new architecture and the framework are presented in detail, including results in terms of modeling capabilities and performance. Large neighborhoods greatly enhance CA modeling capabilities, such as the implementation of anisotropic rules. Performance-wise, the proposed architecture runs on a medium-size FPGA up to 51 times faster vs. a CPU running highly optimized C code. Compared to GPUs the speedup is harder to quantify, because CA results have been reported on GPU implementations with neighborhoods up to 11 × 11, in which case FPGA performance is roughly on par with GPU; however, based on published GPU trends, for 29 × 29 neighborhoods the proposed architecture is expected to have better performance vs. a GPU, at one-10th the energy requirements. The architecture and sample designs are open source available under the creative commons license.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3113031744",
    "type": "article"
  },
  {
    "title": "Improving Loop Parallelization by a Combination of Static and Dynamic Analyses in HLS",
    "doi": "https://doi.org/10.1145/3501801",
    "publication_date": "2022-02-04",
    "publication_year": 2022,
    "authors": "Florian Dewald; Johanna Rohde; Christian Hochberger; Heiko Mantel",
    "corresponding_authors": "",
    "abstract": "High-level synthesis (HLS) can be used to create hardware accelerators for compute-intense software parts such as loop structures. Usually, this process requires significant amount of user interaction to steer kernel selection and optimizations. This can be tedious and time-consuming. In this article, we present an approach that fully autonomously finds independent loop iterations and reductions to create parallelized accelerators. We combine static analysis with information available only at runtime to maximize the parallelism exploited by the created accelerators. For loops where we see potential for parallelism, we create fully parallelized kernel implementations. If static information does not suffice to deduce independence, then we assume independence at compile time. We verify this assumption by statically created checks that are dynamically evaluated at runtime, before using the optimized kernel. Evaluating our approach, we can generate speedups for five out of seven benchmarks. With four loop iterations running in parallel, we achieve ideal speedups of up to 4× and on average speedups of 2.27×, both in comparison to an unoptimized accelerator.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4210545793",
    "type": "article"
  },
  {
    "title": "Quick-Div: Rethinking Integer Divider Design for FPGA-based Soft-processors",
    "doi": "https://doi.org/10.1145/3502492",
    "publication_date": "2022-02-04",
    "publication_year": 2022,
    "authors": "Eric Matthews; Alec Lu; Zhenman Fang; Lesley Shannon",
    "corresponding_authors": "",
    "abstract": "In today’s FPGA-based soft-processors, one of the slowest instructions is integer division. Compared to the low single-digit latency of other arithmetic operations, the fixed 32-cycle latency of radix-2 division is substantially longer. Given that today’s soft-processors typically only implement radix-2 division—if they support hardware division at all—there is significant potential to improve the performance of integer dividers. In this work, we present a set of high-performance, data-dependent, variable-latency integer dividers for FPGA-based soft-processors that we call Quick-Div . We compare them to various radix-N dividers and provide a thorough analysis in terms of latency and resource usage. In addition, we analyze the frequency scaling for such divider designs when (1) treated as a stand-alone unit and (2) integrated as part of a high-performance soft-processor. Moreover, we provide additional theoretical analysis of different dividers’ behaviour and develop a new better-performing Quick-Div variant, called Quick-radix-4 . Experimental results show that our Quick-radix-4 design can achieve up to 6.8× better performance and 6.1× better performance-per-LUT over the radix-2 divider for applications such as random number generation. Even in cases where division operations constitute as little as 1% of all executed instructions, Quick-radix-4 provides a performance uplift of 16% compared to the radix-2 divider.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4210720283",
    "type": "article"
  },
  {
    "title": "Inducing Non-uniform FPGA Aging Using Configuration-based Short Circuits",
    "doi": "https://doi.org/10.1145/3517042",
    "publication_date": "2022-02-11",
    "publication_year": 2022,
    "authors": "Hayden Cook; Jacob Arscott; Brent George; Tanner Gaskin; Jeffrey Goeders; Brad Hutchings",
    "corresponding_authors": "",
    "abstract": "This work demonstrates a novel method of accelerating FPGA aging by configuring FPGAs to implement thousands of short circuits, resulting in high on-chip currents and temperatures. Patterns of ring oscillators are placed across the chip and are used to characterize the operating frequency of the FPGA fabric. Over the course of several months of running the short circuits on two-thirds of the reconfigurable fabric, with daily characterization of the FPGA 6 performance, we demonstrate a decrease in FPGA frequency of 8.5%. We demonstrate that this aging is induced in a non-uniform manner. The maximum slowdown outside of the shorted regions is 2.1%, or about a fourth of the maximum slowdown that is experienced inside the shorted region. In addition, we demonstrate that the slowdown is linear after the first two weeks of the experiment and is unaffected by a recovery period. Additional experiments involving short circuits are also performed to demonstrate the results of our initial experiments are repeatable. These experiments also use a more fine-grained characterization method that provides further insight into the non-uniformed nature of the aging caused by short circuits.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4211053878",
    "type": "article"
  },
  {
    "title": "Highly Parallel Multi-FPGA System Compilation from Sequential C/C++ Code in the AWS Cloud",
    "doi": "https://doi.org/10.1145/3507698",
    "publication_date": "2022-05-16",
    "publication_year": 2022,
    "authors": "Kemal Ebci̇oğlu; İsmail San",
    "corresponding_authors": "",
    "abstract": "We present a High Level Synthesis compiler that automatically obtains a multi-chip accelerator system from a single-threaded sequential C/C++ application. Invoking the multi-chip accelerator is functionally identical to invoking the single-threaded sequential code the multi-chip accelerator is compiled from. Therefore, software development for using the multi-chip accelerator hardware is simplified, but the multi-chip accelerator can exhibit extremely high parallelism. We have implemented, tested, and verified our push-button system design model on multiple field-programmable gate arrays (FPGAs) of the Amazon Web Services EC2 F1 instances platform, using, as an example, a sequential-natured DES key search application that does not have any DOALL loops and that tries each candidate key in order and stops as soon as a correct key is found. An 8- FPGA accelerator produced by our compiler achieves 44,600 times better performance than an x86 Xeon CPU executing the sequential single-threaded C program the accelerator was compiled from. New features of our compiler system include: an ability to parallelize outer loops with loop-carried control dependences, an ability to pipeline an outer loop without fully unrolling its inner loops, and fully automated deployment, execution and termination of multi-FPGA application-specific accelerators in the AWS cloud, without requiring any manual steps.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4280537328",
    "type": "article"
  },
  {
    "title": "A Scalable Systolic Accelerator for Estimation of the Spectral Correlation Density Function and Its FPGA Implementation",
    "doi": "https://doi.org/10.1145/3546181",
    "publication_date": "2022-07-04",
    "publication_year": 2022,
    "authors": "Xiangwei Li; Douglas L. Maskell; Carol Jingyi Li; Philip H. W. Leong; David Boland",
    "corresponding_authors": "",
    "abstract": "The spectral correlation density (SCD) function is the time-averaged correlation of two spectral components used for analyzing periodic signals with time-varying spectral content. Although the analysis is extremely powerful, it has not been widely adopted in real-time applications due to its high computational complexity. In this article, we present an efficient FPGA implementation of the FFT accumulation method (FAM) for estimating the SCD function and its alpha profile. The implementation uses a linear systolic array with a bi-directional datapath consisting of DSP-based processing elements (PEs) with a dedicated instruction schedule, achieving a PE utilization of 88.2%. The 128-PE implementation achieves a clock frequency in excess of 530 MHz and consumes 151K LUTs, 151K FFs, 264 BRAMs, 4 URAMs, and 1,054 DSPs, which is less than 36% of the logic fabric on a Zynq UltraScale+ XCZU28DR-2FFVG1517E RFSoC device. It has a modest 12.5W power consumption and an energy efficiency of 4,832 MOPS/W, which is 20.6× better than the published state-of-the-art GPU implementation. In terms of throughput, it achieves 15,340 windows/s (15,340 windows/s × 2,048 samples/window = 31.4 MS/s), which is a 4.65× improvement compared to the above-mentioned GPU implementation and 807× compared to an existing hybrid FPGA-GPU implementation.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4283798585",
    "type": "article"
  },
  {
    "title": "Efficient Design of Low Bitwidth Convolutional Neural Networks on FPGA with Optimized Dot Product Units",
    "doi": "https://doi.org/10.1145/3546182",
    "publication_date": "2022-07-04",
    "publication_year": 2022,
    "authors": "Mário Véstias; Rui Policarpo Duarte; José T. de Sousa; Horácio C. Neto",
    "corresponding_authors": "",
    "abstract": "Designing hardware accelerators to run the inference of convolutional neural networks (CNN) is under intensive research. Several different architectures have been proposed along with hardware-oriented optimizations of the neural network models. One of the most used optimizations is quantization since it reduces the memory requirements to store weights and layer maps, the memory bandwidth requirements and the hardware complexity. As a consequence, the inference throughput has improved and the computing cost has been reduced, allowing inference to be executed on embedded devices. In this work, we propose highly efficient dot-product arithmetic units for ternary and non-ternary convolutional neural networks on FPGA. The non-ternary dot-product unit uses a fused multiply-add that avoids expensive adder trees, while the ternary dot-product unit uses a dual product unit followed by an optimized conditional adder tree structure. In both cases, designs with and without embedded DSP are considered. The solution is configurable and can be adapted to the available number of resources of the FPGA to achieve the best efficiency. A CNN architecture was developed and characterized using the proposed dot product units. The results show a performance improvement of 1.8 × with a 2× more area efficiency for low bit-width quantizations when compared to previous works running large CNNs in FPGA.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4283801046",
    "type": "article"
  },
  {
    "title": "Toward Software-like Debugging for FPGAs via Checkpointing and Transaction-based Co-Simulation",
    "doi": "https://doi.org/10.1145/3552521",
    "publication_date": "2022-08-01",
    "publication_year": 2022,
    "authors": "Sameh Attia; Vaughn Betz",
    "corresponding_authors": "",
    "abstract": "Checkpoint-based debugging flows have recently been developed that allow the user to move the design state back and forth between an FPGA and a simulator. They provide a softwarelike debugging experience by combining the speed of hardware execution and the full visibility of simulation. However, they assume the entire system state can be moved to a simulator, limiting them to self-contained systems. In this article, we present StateLink, a transaction-based co-simulation framework that allows part of the system (the task) to run in a simulator and still interact with other system components that reside in hardware. StateLink allows tasks to remain connected to and active in the overall hardware system after their state is moved to a simulator. This extends the functionality of checkpoint-based debugging frameworks to designs with external I/Os and significantly speeds up the simulation of tasks that are part of a large system. StateLink typically adds no timing overhead and a modest hardware area overhead. The total area overhead of using the proposed flow on a Memcached system is only 13%. This flow allows the user to benefit from both the hardware speedup of ∼1M× and the StateLink speedup of up to 44× versus full system simulation.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4289315837",
    "type": "article"
  },
  {
    "title": "RIVER",
    "doi": "https://doi.org/10.1145/2655238",
    "publication_date": "2014-08-01",
    "publication_year": 2014,
    "authors": "Christian Brugger; Dominic Hillenbrand; M. Balzer",
    "corresponding_authors": "",
    "abstract": "For high-performance embedded hard-real-time systems, ASICs and FPGAs hold advantages over general-purpose processors and graphics accelerators (GPUs). However, developing signal processing architectures from scratch requires significant resources. Our design methodology is based on sets of configurable building blocks that provide storage, dataflow, computation, and control. Based on our building blocks, we generate hundreds of thousands of our dynamic streaming engine processors that we call DSEs. We store our DSEs in a repository that can be queried for (online) design space exploration. From this repository, DSEs can be downloaded and instantiated within milliseconds on FPGAs. If a loss of flexibility can be tolerated then ASIC implementations are feasible as well. In this article we focus on FPGA implementations. Our DSEs vary in cores, computational lanes, bitwidths, power consumption, and frequency. To the best of our knowledge we are the first to propose online design space exploration based on repositories of precompiled cores that are assembled of common building blocks. For demonstration purposes we map algorithms for image processing and financial mathematics to DSEs and compare the performance to existing highly optimized signal and graphics accelerators.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W1990393863",
    "type": "article"
  },
  {
    "title": "Imprecise Datapath Design",
    "doi": "https://doi.org/10.1145/2629527",
    "publication_date": "2015-03-17",
    "publication_year": 2015,
    "authors": "Kan Shi; David Boland; George A. Constantinides",
    "corresponding_authors": "",
    "abstract": "In this article, we describe an alternative circuit design methodology when considering trade-offs between accuracy, performance, and silicon area. We compare two different approaches that could trade accuracy for performance. One is the traditional approach where the precision used in the datapath is limited to meet a target latency. The other is a proposed new approach which simply allows the datapath to operate without timing closure. We demonstrate analytically and experimentally that on average our approach obtains either smaller errors or equivalent faster operating frequencies in comparison to the traditional approach. This is because the worst case caused by timing violations only happens rarely, while precision loss results in errors to most data. We also show that for basic arithmetic operations such as addition, applying our approach to the simple building block of ripple carry adders can achieve better accuracy or performance than using faster adder designs to achieve similar latency.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W1994365239",
    "type": "article"
  },
  {
    "title": "Adaptive Parallelism Exploitation under Physical and Real-Time Constraints for Resilient Systems",
    "doi": "https://doi.org/10.1145/2556943",
    "publication_date": "2014-08-01",
    "publication_year": 2014,
    "authors": "Fábio Itturiet; Gabriel L. Nazar; Ronaldo Rodrigues Ferreira; Álvaro Moreira; Luigi Carro",
    "corresponding_authors": "",
    "abstract": "This article introduces the resilient adaptive algebraic architecture that aims at adapting parallelism exploitation of a matrix multiplication algorithm in a time-deterministic fashion to reduce power consumption while meeting real-time deadlines present in most DSP-like applications. The proposed architecture provides low-overhead error correction capabilities relying on the hardware implementation of the algorithm-based fault-tolerance method that is executed concurrently with matrix multiplication, providing efficient occupation of memory and power resources. The Resilient Adaptive Algebraic Architecture (RA 3 ) is evaluated using three real-time industrial case studies from the telecom and multimedia application domains to present the design space exploration and the adaptation possibilities the architecture offers to hardware designers. RA 3 is compared in its performance and energy efficiency with standard high-performance architectures, namely a GPU and an out-of-order general-purpose processor. Finally, we present the results of fault injection campaigns in order to measure the architecture resilience to soft errors.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2139382710",
    "type": "article"
  },
  {
    "title": "Exploiting FPGA-Aware Merging of Custom Instructions for Runtime Reconfiguration",
    "doi": "https://doi.org/10.1145/2655240",
    "publication_date": "2014-08-01",
    "publication_year": 2014,
    "authors": "Siew-Kei Lam; Christopher T. Clarke; Thambipillai Srikanthan",
    "corresponding_authors": "",
    "abstract": "Runtime reconfiguration is a promising solution for reducing hardware cost in embedded systems, without compromising on performance. We present a framework that aims to increase the performance benefits of reconfigurable processors that support full or partial runtime reconfiguration. The proposed framework achieves this by: (1) providing a means for choosing suitable custom instruction selection heuristics, (2) leveraging FPGA-aware merging of custom instructions to maximize the reconfigurable logic block utilization in each configuration, and (3) incorporating a hierarchical loop partitioning strategy to reduce runtime reconfiguration overhead. We show that the performance gain can be improved by employing suitable custom instruction selection heuristics that, in turn, depend on the reconfigurable resource constraints and the merging factor (extent to which the selected custom instructions can be merged). The hierarchical loop partitioning strategy leads to an average performance gain of over 31% and 46% for full and partial runtime reconfiguration, respectively. Performance gain can be further increased to over 52% and 70% for full and partial runtime reconfiguration, respectively, by exploiting FPGA-aware merging of custom instructions.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2144176771",
    "type": "article"
  },
  {
    "title": "ARC 2014",
    "doi": "https://doi.org/10.1145/2766454",
    "publication_date": "2015-11-06",
    "publication_year": 2015,
    "authors": "Yuhui Bai; Syed Zahid Ahmed; Bertrand Granado",
    "corresponding_authors": "",
    "abstract": "The embedded image processing systems like smartphones and digital cameras have tight limits on storage, computation power, network connectivity, and battery usage. These limitations make it important to ensure efficient image coding. In the article, we present a novel heap-based priority queue structure employed by an Adaptive Scanning of Wavelet Data scheme (ASWD) targeting an embedded platform. ASWD is a context modeling block implemented via priority queues in a wavelet-based image coder to reorganize the wavelet coefficients into locally stationary sequences. The architecture we propose exploits efficient use of FPGA’s on-chip dual-port memories in an adaptive manner. Innovations of index-aware system linked to each element in the queue makes the location of queue element traceable in the heap as per the requirements of the ASWD algorithm. Moreover, use of 4-port memories along with intelligent data concatenation of queue elements yielded in a cost effective enhanced memory access. The memory ports are adaptively assigned to different units during different processing phases in a manner to optimally take advantage of memory access required by that phase. The architectural innovations can also be exploited in other applications that require efficient hardware implementations of generic priority queue or classical sorting applications which sort into the index. We designed and validated the hardware on an Altera’s Stratix IV FPGA as an IP accelerator in a Nios II processor based System on Chip. We show that our architecture at 150MHz can provide 45X speedup compared to an embedded ARM Cortex-A9 processor at 666MHz targeting the throughput of 10MB/s.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2196080855",
    "type": "article"
  },
  {
    "title": "Analysis of Fixed, Reconfigurable, and Hybrid Devices with Computational, Memory, I/O, &amp; Realizable-Utilization Metrics",
    "doi": "https://doi.org/10.1145/2888401",
    "publication_date": "2016-09-24",
    "publication_year": 2016,
    "authors": "Justin Richardson; Alan D. George; Kevin Cheng; Herman Lam",
    "corresponding_authors": "",
    "abstract": "The modern processor landscape is a varied and diverse community. As such, developers need a way to quickly and fairly compare various devices for use with particular applications. This article expands the authors’ previously published computational-density metrics and presents an analysis of a new generation of various device architectures, including CPU, DSP, FPGA, GPU, and hybrid architectures. Also, new memory metrics are added to expand the existing suite of metrics to characterize the memory resources on various processing devices. Finally, a new relational metric, realizable utilization (RU) , is introduced, which quantifies the fraction of the computational density metric that an application achieves within an individual implementation. The RU metric can be used to provide valuable feedback to application developers and architecture designers by highlighting the upper bound on specific application optimization and providing a quantifiable measure of theoretical and realizable performance. Overall, the analysis in this article quantifies the performance tradeoffs among the architectures studied, the memory characteristics of different device types, and the efficiency of device architectures.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2527614115",
    "type": "article"
  },
  {
    "title": "Deterministic Approach for Range-enhanced Reconfigurable Packet Classification Engine",
    "doi": "https://doi.org/10.1145/3586577",
    "publication_date": "2023-03-04",
    "publication_year": 2023,
    "authors": "M Dhayalakumar; Noor Mahammad Sk",
    "corresponding_authors": "",
    "abstract": "Reconfigurable hardware is a promising technology for implementing firewalls, routing mechanisms, and new protocols for evolving high-performance network systems. This work presents a novel deterministic approach for a Range-enhanced Reconfigurable Packet Classification Engine based on the number of rules on FPGAs. The proposed framework uses a RAM-established Ternary Match to represent the prefix and the range prefix and efficient rule-reordering for priority selection to get both best-match and multi-match in the same architecture. The recommended framework exhibits 3.2 Mbits of LUT-RAM-based ternary content addressable memory (TCAM) to hold a maximum of 31.3 K of 104- bit rules with 520 MPPS . LUT-RAM, along with BRAM, shows 4 Mbits of TCAM space to implement 38.5 K of 104- bit rules to sustain a throughput of 400 MPPS on Virtex-7 FPGA. The complete architecture offers scalability, better resource utilization (minimum of 50% ), representation of inverse prefix with single entry, range expansion with a single rule, getting best- and multi-match, and determination of the required number of FPGA resources for a particular dataset.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4323074779",
    "type": "article"
  },
  {
    "title": "A Survey of Processing Systems for Phylogenetics and Population Genetics",
    "doi": "https://doi.org/10.1145/3588033",
    "publication_date": "2023-03-16",
    "publication_year": 2023,
    "authors": "Reinout Corts; Nikolaos Alachiotis",
    "corresponding_authors": "",
    "abstract": "The COVID-19 pandemic brought Bioinformatics into the spotlight, revealing that several existing methods, algorithms, and tools were not well prepared to handle large amounts of genomic data efficiently. This led to prohibitively long execution times and the need to reduce the extent of analyses to obtain results in a reasonable amount of time. In this survey, we review available high-performance computing and hardware-accelerated systems based on FPGA and GPU technology. Optimized and hardware-accelerated systems can conduct more thorough analyses considerably faster than pure software implementations, allowing to reach important conclusions in a timely manner to drive scientific discoveries. We discuss the reasons that are currently hindering high-performance solutions from being widely deployed in real-world biological analyses and describe a research direction that can pave the way to enable this.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4327590773",
    "type": "article"
  },
  {
    "title": "NeuroHSMD: Neuromorphic Hybrid Spiking Motion Detector",
    "doi": "https://doi.org/10.1145/3588318",
    "publication_date": "2023-03-17",
    "publication_year": 2023,
    "authors": "Pedro Machado; João Filipe Ferreira; Andreas Oikonomou; T.M. McGinnity",
    "corresponding_authors": "",
    "abstract": "Vertebrate retinas are highly-efficient in processing trivial visual tasks such as detecting moving objects, which still represent complex challenges for modern computers. In vertebrates, the detection of object motion is performed by specialised retinal cells named Object Motion Sensitive Ganglion Cells (OMS-GC). OMS-GC process continuous visual signals and generate spike patterns that are post-processed by the Visual Cortex. Our previous Hybrid Sensitive Motion Detector (HSMD) algorithm was the first hybrid algorithm to enhance Background subtraction (BS) algorithms with a customised 3-layer Spiking Neural Network (SNN) that generates OMS-GC spiking-like responses. In this work, we present a Neuromorphic Hybrid Sensitive Motion Detector (NeuroHSMD) algorithm that accelerates our HSMD algorithm using Field-Programmable Gate Arrays (FPGAs). The NeuroHSMD was compared against the HSMD algorithm, using the same 2012 Change Detection (CDnet2012) and 2014 Change Detection (CDnet2014) benchmark datasets. When tested against the CDnet2012 and CDnet2014 datasets, NeuroHSMD performs object motion detection at 720 × 480 at 28.06 Frames Per Second (fps) and 720 × 480 at 28.71 fps, respectively, with no degradation of quality. Moreover, the NeuroHSMD proposed in this article was completely implemented in Open Computer Language (OpenCL) and therefore is easily replicated in other devices such as Graphical Processing Units (GPUs) and clusters of Central Processing Units (CPUs).",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4327742760",
    "type": "article"
  },
  {
    "title": "NAPOLY: A Non-deterministic Automata Processor OverLaY",
    "doi": "https://doi.org/10.1145/3593586",
    "publication_date": "2023-04-24",
    "publication_year": 2023,
    "authors": "Rasha Karakchi; Jason D. Bakos",
    "corresponding_authors": "",
    "abstract": "Deterministic and Non-deterministic Finite Automata (DFA and NFA) comprise the core of many big data applications. Recent efforts to develop Domain-Specific Architectures (DSAs) for DFA/NFA have taken divergent approaches, but achieving consistent throughput for arbitrarily-large pattern sets, state activation rates, and pattern match rates remains a challenge. In this article, we present NAPOLY (Non-Deterministic Automata Processor OverLaY), an FPGA overlay and associated compiler. A common limitation of prior efforts is a limit on NFA size for achieving the advertised throughput. NAPOLY is optimized for fast re-programming to permit practical time-division multiplexing of the hardware and permit high asymptotic throughput for NFAs of unlimited size, unlimited state activation rate, and high pattern reporting rate. NAPOLY also allows for offline generation of configurations having tradeoffs between state capacity and transition capacity. In this article, we (1) evaluate NAPOLY using benchmarks packaged in the ANMLZoo benchmark suite, (2) evaluate the use of an SAT solver for allocating physical resources, and (3) compare NAPOLY’s performance against existing solutions. NAPOLY performs most favorably on larger benchmarks, benchmarks with higher state activation frequency, and benchmarks with higher reporting frequency. NAPOLY outperforms the fastest of the CPU and GPU implementations in 10 out of 12 benchmarks.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4366825874",
    "type": "article"
  },
  {
    "title": "Exploring FPGA Switch-Blocks without Explicitly Listing Connectivity Patterns",
    "doi": "https://doi.org/10.1145/3597417",
    "publication_date": "2023-05-17",
    "publication_year": 2023,
    "authors": "Stefan Nikolić; Paolo Ienne",
    "corresponding_authors": "",
    "abstract": "Increased lower metal resistance makes physical aspects of Field-Programmable Gate Array (FPGA) switch-blocks more relevant than before. The need to navigate a design space where each individual switch can have significant impact on the FPGA’s performance in turn makes automated switch-pattern exploration techniques increasingly appealing. However, most existing exploration techniques have a fundamental limitation—they use the CAD tools as a black box to evaluate the performance of explicitly listed switch-patterns. Given the time needed to route a modern circuit on a single architecture, the number of switch-patterns that can be explicitly tested quickly becomes negligible compared to the size of the design space. This article presents a technique that removes this fundamental limitation by making the entire design space visible to the router and letting it choose the switches to be added to the pattern, based on the requirements of the circuits being routed. The key to preventing the router from selecting arbitrary switches that would render the final pattern excessively large is to apply the same negotiation principle used by the router to remove congestion, just in the opposite direction, to make the signals reach a consensus on which switches are worthy of being included in the final switch-pattern.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4376877228",
    "type": "article"
  },
  {
    "title": "An FPGA Accelerator for Genome Variant Calling",
    "doi": "https://doi.org/10.1145/3595297",
    "publication_date": "2023-05-22",
    "publication_year": 2023,
    "authors": "Tiancheng Xu; Scott Rixner; Alan L. Cox",
    "corresponding_authors": "",
    "abstract": "In genome analysis, it is often important to identify variants from a reference genome. However, identifying variants that occur with low frequency can be challenging, as it is computationally intensive to do so accurately. LoFreq is a widely used program that is adept at identifying low-frequency variants. This article presents a design framework for an FPGA-based accelerator for LoFreq. In particular, this accelerator is targeted at virus analysis, which is particularly challenging, compared to human genome analysis, as the characteristics of the data to be analyzed are fundamentally different. Across the design space, this accelerator can achieve up to 120× speedups on the core computation of LoFreq and speedups of up to 51.7× across the entire program.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4377245027",
    "type": "article"
  },
  {
    "title": "Constraint-Aware Multi-Technique Approximate High-Level Synthesis for FPGAs",
    "doi": "https://doi.org/10.1145/3624481",
    "publication_date": "2023-09-13",
    "publication_year": 2023,
    "authors": "Marcos T. Leipnitz; Gabriel L. Nazar",
    "corresponding_authors": "",
    "abstract": "Numerous approximate computing (AC) techniques have been developed to reduce the design costs in error-resilient application domains, such as signal and multimedia processing, data mining, machine learning, and computer vision, to trade-off computation accuracy with area and power savings or performance improvements. Selecting adequate techniques for each application and optimization target is complex but crucial for high-quality results. In this context, Approximate High-Level Synthesis (AHLS) tools have been proposed to alleviate the burden of hand-crafting approximate circuits by automating the exploitation of AC techniques. However, such tools are typically tied to a specific approximation technique or a difficult-to-extend set of techniques whose exploitation is not fully automated or steered by optimization targets. Therefore, available AHLS tools overlook the benefits of expanding the design space by mixing diverse approximation techniques toward meeting specific design objectives with minimum error. In this work, we propose an AHLS design methodology for FPGAs that automatically identifies efficient combinations of multiple approximation techniques for different applications and design constraints. Compared to single-technique approaches, decreases of up to 30% in mean squared error and absolute increases of up to 6.5% in percentage accuracy were obtained for a set of image, video, signal processing and machine learning benchmarks.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4386710186",
    "type": "article"
  },
  {
    "title": "The Open-source DeLiBA2 Hardware/Software Framework for Distributed Storage Accelerators",
    "doi": "https://doi.org/10.1145/3624482",
    "publication_date": "2023-09-14",
    "publication_year": 2023,
    "authors": "Babar Khan; Carsten Heinz; Andreas Koch",
    "corresponding_authors": "",
    "abstract": "With the trend towards ever larger “big data” applications, many of the gains achievable by using specialized compute accelerators become diminished due to the growing I/O overheads. While there have been several research efforts into computational storage and FPGA implementations of the NVMe interface, to our knowledge, there have been only very limited efforts to move larger parts of the Linux block I/O stack into FPGA-based hardware accelerators. Our hardware/software framework DeLiBA initially addressed this deficiency by allowing high-productivity development of software components of the I/O stack in user instead of kernel space and leverages a proven FPGA SoC framework to quickly compose and deploy the actual FPGA-based I/O accelerators. In its initial form, it achieves 10% higher throughput and up to 2.3× the I/Os per second (IOPS) for a proof-of-concept Ceph accelerator running in a real multi-node Ceph cluster. In DeLiBA2, we have extended the framework further to better support distributed storage systems, specifically by directly integrating the block I/O accelerators with a hardware-accelerated network stack, as well as by accelerating more storage functions. With these improvements, performance grows significantly: The cluster-level speedups now reach up to 2.8× for both throughput and IOPS relative to Ceph in software in synthetic benchmarks and achieve end-to-end wall-clock speedups of 20% for the real workload of building a large software package.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4386745882",
    "type": "article"
  },
  {
    "title": "ExHiPR: Extended High-Level Partial Reconfiguration for Fast Incremental FPGA Compilation",
    "doi": "https://doi.org/10.1145/3617837",
    "publication_date": "2023-09-14",
    "publication_year": 2023,
    "authors": "Yuanlong Xiao; Dongjoon Park; Zeyu Jason Niu; Aditya Hota; André DeHon",
    "corresponding_authors": "",
    "abstract": "Partial Reconfiguration (PR) is a key technique in the application design on modern FPGAs. However, current PR tools heavily rely on the developer to manually conduct PR module definition, floorplanning, and flow control at a low level. The existing PR tools do not consider High-Level-Synthesis languages either, which are of great interest to software developers. We propose HiPR, an open-source framework, to bridge the gap between HLS and PR. HiPR allows the developer to define partially reconfigurable C/C++ functions, instead of Verilog modules, to accelerate the FPGA incremental compilation and automate the flow from C/C++ to bitstreams. We use a lightweight Simulated Annealing floorplanner and show that it can produce high-quality PR floorplans an order of magnitude faster than analytic methods. By mapping Rosetta HLS benchmarks, we demonstrate that the incremental compilation can be accelerated by 3–10× compared with state-of-the-art Xilinx Vitis flow without performance loss, at the cost of 15–67% one-time overlay set-up time.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4386746087",
    "type": "article"
  },
  {
    "title": "Automated Buffer Sizing of Dataflow Applications in a High-level Synthesis Workflow",
    "doi": "https://doi.org/10.1145/3626103",
    "publication_date": "2023-09-29",
    "publication_year": 2023,
    "authors": "Alexandre Honorat; Mickaël Dardaillon; Hugo Miomandre; Jean-François Nezan",
    "corresponding_authors": "",
    "abstract": "High-Level Synthesis (HLS) tools are mature enough to provide efficient code generation for computation kernels on FPGA hardware. For more complex applications, multiple kernels may be connected by a dataflow graph. Although some tools, such as Xilinx Vitis HLS, support dataflow directives, they lack efficient analysis methods to compute the buffer sizes between kernels in a dataflow graph. This article proposes an original method to safely approximate such buffer sizes. The first contribution computes an initial overestimation of buffer sizes without knowing the memory access patterns of kernels. The second contribution iteratively refines those buffer sizes, thanks to cosimulation. Moreover, the article introduces an open source framework using these methods to facilitate dataflow programming on FPGA using HLS. The proposed methods and framework have been tested on seven dataflow applications and outperform Vitis HLS cosimulation in five benchmarks, either in terms of BRAM and LUT usage, or in terms of exploration time. In the two other benchmarks, our best method gets results similar to Vitis HLS. Last but not least, our method admits directed cycles in the application graphs.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4387187832",
    "type": "article"
  },
  {
    "title": "Reprogrammable Non-Linear Circuits Using ReRAM for NN Accelerators",
    "doi": "https://doi.org/10.1145/3617894",
    "publication_date": "2023-10-10",
    "publication_year": 2023,
    "authors": "Rafael Fão de Moura; Luigi Carro",
    "corresponding_authors": "",
    "abstract": "As the massive usage of artificial intelligence techniques spreads in the economy, researchers are exploring new techniques to reduce the energy consumption of Neural Network (NN) applications, especially as the complexity of NNs continues to increase. Using analog Resistive RAM devices to compute matrix-vector multiplication in O (1) time complexity is a promising approach, but it is true that these implementations often fail to cover the diversity of non-linearities required for modern NN applications. In this work, we propose a novel approach where Resistive RAMs themselves can be reprogrammed to compute not only the required matrix multiplications but also the activation functions, Softmax, and pooling layers, reducing energy in complex NNs. This approach offers more versatility for researching novel NN layouts compared to custom logic. Results show that our device outperforms analog and digital field-programmable approaches by up to 8.5× in experiments on real-world human activity recognition and language modeling datasets with convolutional neural network, generative pre-trained Transformer, and long short-term memory models.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4387495993",
    "type": "article"
  },
  {
    "title": "<scp>Strega</scp> : An HTTP Server for FPGAs",
    "doi": "https://doi.org/10.1145/3611312",
    "publication_date": "2023-10-10",
    "publication_year": 2023,
    "authors": "Fabio Maschi; Gustavo Alonso",
    "corresponding_authors": "",
    "abstract": "The computer architecture landscape is being reshaped by the new opportunities, challenges, and constraints brought by the cloud. On the one hand, high-level applications profit from specialised hardware to boost their performance and reduce deployment costs. On the other hand, cloud providers maximise the CPU time allocated to client applications by offloading infrastructure tasks to hardware accelerators. While it is well understood how to do this for, e.g., network function virtualisation and protocols such as TCP/IP, support for higher networking layers is still largely missing, limiting the potential of accelerators. In this article, we present Strega , an open source 1 light-weight Hypertext Transfer Protocol (HTTP) server that enables crucial functionality such as FPGA-accelerated functions being called through a RESTful protocol (FPGA-as-a-Function). Our experimental analysis shows that a single Strega node sustains a throughput of 1.7 M HTTP requests per second with an end-to-end latency as low as 16, μs, outperforming nginx running on 32 vCPUs in both metrics, and can even be an alternative to the traditional OpenCL flow over the PCIe bus. Through this work, we pave the way for running microservices directly on FPGAs, bypassing CPU overhead and realising the full potential of FPGA acceleration in distributed cloud applications.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4387496205",
    "type": "article"
  },
  {
    "title": "Designing Deep Learning Models on FPGA with Multiple Heterogeneous Engines",
    "doi": "https://doi.org/10.1145/3615870",
    "publication_date": "2023-10-10",
    "publication_year": 2023,
    "authors": "Miguel Reis; Mário Véstias; Horácio C. Neto",
    "corresponding_authors": "",
    "abstract": "Deep learning models are becoming more complex and heterogeneous with new layer types to improve their accuracy. This brings a considerable challenge to the designers of accelerators of deep neural networks. There have been several architectures and design flows to map deep learning models on hardware, but they are limited to a particular model and/or layer types. Also, the architectures generated by these tools target, in general, high-performance devices, not appropriate for embedded computing. This article proposes a multi-engine architecture and a design flow to implement deep learning models on FPGA. The hardware design uses high-level synthesis to allow design space exploration. The architecture is scalable and therefore applicable to any density FPGAs. The architecture and design flow were applied to the development of a hardware/software system for image classification with ResNet50, object detection with YOLOv3-Tiny, and image segmentation with DeepLabV3+. The system was tested in a low-density Zynq UltraScale+ ZU3EG FPGA to show its scalability. The results show that the proposed multi-engine architecture generates efficient accelerators. An accelerator of ResNet50 with a 4-bit quantization achieves 67 FPS, and the object detector with YOLOv3-Tiny with a throughput of 36 FPS and the image segmentation application achieves 1.4 FPS.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4387502592",
    "type": "article"
  },
  {
    "title": "On the Malicious Potential of Xilinx’s Internal Configuration Access Port (ICAP)",
    "doi": "https://doi.org/10.1145/3633204",
    "publication_date": "2023-11-17",
    "publication_year": 2023,
    "authors": "Nils Albartus; Maik Ender; Jan-Niklas Möller; Marc Fyrbiak; Christof Paar; Russell Tessier",
    "corresponding_authors": "",
    "abstract": "Field Programmable Gate Arrays (FPGAs) have become increasingly popular in computing platforms. With recent advances in bitstream format reverse engineering, the scientific community has widely explored static FPGA security threats. For example, it is now possible to convert a bitstream to a netlist, revealing design information, and apply modifications to the static bitstream based on this knowledge. However, a systematic study of the influence of the bitstream format understanding in regards to the security aspects of the dynamic configuration process, particularly for Xilinx’s Internal Configuration Access Port (ICAP), is lacking. This article fills this gap by comprehensively analyzing the security implications of ICAP interfaces, which primarily support dynamic partial reconfiguration. We delve into the Xilinx bitstream file format, identify misconceptions in official documentation, and propose novel configuration (attack) primitives based on dynamic reconfiguration, i.e., create/read/update/delete circuits in the FPGA, without requiring pre-definition during the design phase. Our primitives are consolidated in a novel Stealthy Reconfigurable Adaptive Trojan framework to conceal Trojans and evade state-of-the-art netlist reverse engineering methods. As FPGAs become integral to modern cloud computing, this research presents crucial insights on potential security risks, including the possibility of a malicious tenant or provider altering or spying on another tenant’s configuration undetected.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4388763411",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Section on FCCM 2022",
    "doi": "https://doi.org/10.1145/3632092",
    "publication_date": "2023-12-05",
    "publication_year": 2023,
    "authors": "Jing Li; Martin Herbordt",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4389347391",
    "type": "article"
  },
  {
    "title": "AEKA: FPGA Implementation of Area-Efficient Karatsuba Accelerator for Ring-Binary-LWE-Based Lightweight PQC",
    "doi": "https://doi.org/10.1145/3637215",
    "publication_date": "2023-12-11",
    "publication_year": 2023,
    "authors": "Tianyou Bao; Pengzhou He; Jiafeng Xie; H S. Jacinto",
    "corresponding_authors": "",
    "abstract": "Lightweight PQC-related research and development have gradually gained attention from the research community recently. Ring-Binary-Learning-with-Errors (RBLWE)-based encryption scheme (RBLWE-ENC), a promising lightweight PQC based on small parameter sets to fit related applications (but not in favor of deploying popular fast algorithms like number theoretic transform). To solve this problem, in this article, we present a novel implementation of hardware acceleration for RBLWE-ENC based on Karatsuba algorithm, particularly on the field-programmable gate array (FPGA) platform. In detail, we have proposed an area-efficient Karatsuba Accelerator (AEKA) for RBLWE-ENC, based on three layers of innovative efforts. First of all, we reformulate the signal processing sequence within the major arithmetic component of the KA-based polynomial multiplication for RBLWE-ENC to obtain a new algorithm. Then, we have designed the proposed algorithm into a new hardware accelerator with several novel algorithm-to-architecture mapping techniques. Finally, we have conducted thorough complexity analysis and comparison to demonstrate the efficiency of the proposed accelerator, e.g., it involves 62.5% higher throughput and 60.2% less area-delay product (ADP) than the state-of-the-art design for n =512 (Virtex-7 device, similar setup). The proposed AEKA design strategy is highly efficient on the FPGA devices, i.e., small resource usage with superior timing, which can be integrated with other necessary systems for lightweight-oriented high-performance applications (e.g., servers). The outcome of this work is also expected to generate impacts for lightweight PQC advancement.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4389572099",
    "type": "article"
  },
  {
    "title": "Reconfigurable architecture for VBSME with variable pixel precision",
    "doi": "https://doi.org/10.1145/2133352.2133355",
    "publication_date": "2012-03-01",
    "publication_year": 2012,
    "authors": "Joaquín Olivares",
    "corresponding_authors": "Joaquín Olivares",
    "abstract": "Current video coding standards, e.g. MPEG-4 H.264/AVC, include Variable Block Size Motion Estimation, in this paper, this process is implemented by a reconfigurable architecture based on Signed Digit arithmetic. Bit serial computation is applied to reconfigure pixel precision. The reconfigurable architectural model is extremely simple to reconfigure. Pixel truncation is used to speed up computation saving up 23.5% of clock cycles for 4-bit precision. This design allows to process all motion vectors of a block in just one iteration. This system has been implemented in FPGA, and HDTVp results are presented. Main characteristics, of this architecture are: very reduced cost, high performance, and reconfigurable pixel precision, these features could be useful in mobile devices.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W1991390056",
    "type": "article"
  },
  {
    "title": "Asymmetric Cache Coherency",
    "doi": "https://doi.org/10.1145/2362374.2362376",
    "publication_date": "2012-10-01",
    "publication_year": 2012,
    "authors": "John Shield; Jean-Philippe Diguet; Guy Gogniat",
    "corresponding_authors": "",
    "abstract": "Asymmetric coherency is a new optimization method for coherency policies to support nonuniform workloads in multicore processors. Asymmetric coherency assists in load balancing a workload and this is applicable to SoC multicores where the applications are not evenly spread among the processors and customization of the coherency is possible. Asymmetric coherency is a policy change, and consequently our designs require little or no additional hardware over an existing system. We explore two different types of asymmetric coherency policies. Our bus-based asymmetric coherency policy, generated a 60% coherency cost reduction (reduction of latencies due to coherency messages) for nonshared data. Our directory-based asymmetric coherency policy, showed up to a 5.8% execution time improvement and up to a 22% improvement in average memory latency for the parallel benchmarks Sha, using a statically allocated asymmetry. Dynamically allocated asymmetry was found to generate further improvements in access latency, increasing the effectiveness of asymmetric coherency by up to 73.8% when compared to the static asymmetric solution.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2003017330",
    "type": "article"
  },
  {
    "title": "A comprehensive performance analysis of virtual routers on FPGA",
    "doi": "https://doi.org/10.1145/2492187",
    "publication_date": "2013-07-01",
    "publication_year": 2013,
    "authors": "Thilan Ganegedara; Viktor K. Prasanna",
    "corresponding_authors": "",
    "abstract": "Network virtualization has gained much popularity with the advent of datacenter networking. The hardware aspect of network virtualization, router virtualization , allows network service providers to consolidate network hardware, reducing equipment cost and management overhead. Several approaches have been proposed to achieve router virtualization to support several virtual networks on a single hardware platform. However, their performance has not been analyzed quantitatively to understand the benefits of each approach. In this work, we perform a comprehensive analysis of performance of these approaches on Field Programmable Gate Array (FPGA) with respect to memory consumption, throughput, and power consumption. Generalized versions of virtualization approaches are evaluated based on post place-and-route results on a state-of-the-art FPGA. Grouping of routing tables is proposed as a novel approach to improve scalability (i.e., the number of virtual networks hosted on a single chip) of virtual routers on FPGA with respect to memory requirement. Further, we employ floor-planning techniques to efficiently utilize chip resources and achieve high performance for virtualized, pipelined architectures, resulting in 1.6× speedup on the average compared with the non-floor-planned approach. The results indicate that the proposed solution is able to support 100+ and 50 virtual routers per chip in the near-best and near-worst case scenarios, while operating at 20+ Gbps rates.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2011986793",
    "type": "article"
  },
  {
    "title": "A Reconfigurable Parallel Hardware Implementation of the Self-Tuning Regulator",
    "doi": "https://doi.org/10.1145/2535934",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "T Ananthan; Varghese Mathew Vaidyan",
    "corresponding_authors": "",
    "abstract": "The self-tuning regulator (STR) is a popular adaptive control algorithm. A high-performance computer is required for its implementation due to the heavy online computational burden. To extend STR for more real-time applications, a parallel hardware implementation on a low-cost reconfigurable computer is presented. The hardware was incorporated with multistage matrix multiplication (MMM) and trace technique to enhance the processing speed. This design was deeply pipelined to achieve high throughput. The algorithm was prototyped on a Xilinx field-programmable gate array (FPGA) device with a maximum operating frequency of 210.436 MHz. Application-specific integrated circuit (ASIC) implementation of STR was reported.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2064519112",
    "type": "article"
  },
  {
    "title": "A comprehensive performance analysis of virtual routers on FPGA",
    "doi": "https://doi.org/10.1145/2499625.2492187",
    "publication_date": "2013-07-01",
    "publication_year": 2013,
    "authors": "Thilan Ganegedara; Viktor K. Prasanna",
    "corresponding_authors": "",
    "abstract": "Network virtualization has gained much popularity with the advent of datacenter networking. The hardware aspect of network virtualization, router virtualization, allows network service providers to consolidate network hardware, reducing equipment cost and management overhead. Several approaches have been proposed to achieve router virtualization to support several virtual networks on a single hardware platform. However, their performance has not been analyzed quantitatively to understand the benefits of each approach. In this work, we perform a comprehensive analysis of performance of these approaches on Field Programmable Gate Array (FPGA) with respect to memory consumption, throughput, and power consumption. Generalized versions of virtualization approaches are evaluated based on post place-and-route results on a state-of-the-art FPGA. Grouping of routing tables is proposed as a novel approach to improve scalability (i.e., the number of virtual networks hosted on a single chip) of virtual routers on FPGA with respect to memory requirement. Further, we employ floor-planning techniques to efficiently utilize chip resources and achieve high performance for virtualized, pipelined architectures, resulting in 1.6× speedup on the average compared with the non-floor-planned approach. The results indicate that the proposed solution is able to support 100+ and 50 virtual routers per chip in the near-best and near-worst case scenarios, while operating at 20+ Gbps rates.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4234614825",
    "type": "article"
  },
  {
    "title": "Scalable Phylogeny Reconstruction with Disaggregated Near-memory Processing",
    "doi": "https://doi.org/10.1145/3484983",
    "publication_date": "2021-12-27",
    "publication_year": 2021,
    "authors": "Nikolaos Alachiotis; Panagiotis Skrimponis; Manolis Pissadakis; Dionisios Pnevmatikatos",
    "corresponding_authors": "",
    "abstract": "Disaggregated computer architectures eliminate resource fragmentation in next-generation datacenters by enabling virtual machines to employ resources such as CPUs, memory, and accelerators that are physically located on different servers. While this paves the way for highly compute- and/or memory-intensive applications to potentially deploy all CPUs and/or memory resources in a datacenter, it poses a major challenge to the efficient deployment of hardware accelerators: input/output data can reside on different servers than the ones hosting accelerator resources, thereby requiring time- and energy-consuming remote data transfers that diminish the gains of hardware acceleration. Targeting a disaggregated datacenter architecture similar to the IBM dReDBox disaggregated datacenter prototype, the present work explores the potential of deploying custom acceleration units adjacently to the disaggregated-memory controller on memory bricks (in dReDBox terminology), which is implemented on FPGA technology, to reduce data movement and improve performance and energy efficiency when reconstructing large phylogenies (evolutionary relationships among organisms). A fundamental computational kernel is the Phylogenetic Likelihood Function (PLF), which dominates the total execution time (up to 95%) of widely used maximum-likelihood methods. Numerous efforts to boost PLF performance over the years focused on accelerating computation; since the PLF is a data-intensive, memory-bound operation, performance remains limited by data movement, and memory disaggregation only exacerbates the problem. We describe two near-memory processing models, one that addresses the problem of workload distribution to memory bricks, which is particularly tailored toward larger genomes (e.g., plants and mammals), and one that reduces overall memory requirements through memory-side data interpolation transparently to the application, thereby allowing the phylogeny size to scale to a larger number of organisms without requiring additional memory.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4200039529",
    "type": "article"
  },
  {
    "title": "Titan-R",
    "doi": "https://doi.org/10.1145/1754386.1754388",
    "publication_date": "2010-05-01",
    "publication_year": 2010,
    "authors": "Konstantinos Papadopoulos; Ioannis Papaefstathiou",
    "corresponding_authors": "",
    "abstract": "Data compression techniques can alleviate bandwidth problems in even multigigabit networks and are especially useful when combined with encryption. This article demonstrates a reconfigurable hardware compressor/decompressor core, the Titan-R, which can compress/decompress data streams at 8.5 Gb/sec, making it the fastest reconfigurable such device ever proposed; the presented full-duplex implementation allows for fully symmetric compression and decompression rates at 8.5 Gbps each. Its compression algorithm is a variation of the most widely used and efficient such scheme, the Lempel-Ziv (LZ) algorithm that uses part of the previous input stream as the dictionary. In order to support this high network throughput, the Titan-R utilizes a very fine-grained pipeline and takes advantage of the high bandwidth provided by the distributed on-chip RAMs of state-of-the-art FPGAs.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2005477835",
    "type": "article"
  },
  {
    "title": "The First 25 Years of the FPL Conference",
    "doi": "https://doi.org/10.1145/2996468",
    "publication_date": "2017-03-22",
    "publication_year": 2017,
    "authors": "Philip H. W. Leong; Hideharu Amano; Jason H. Anderson; Koen Bertels; João M. P. Cardoso; Oliver Diessel; Guy Gogniat; Mike Hutton; Junkyu Lee; Wayne Luk; Patrick Lysaght; Marco Platzner; Viktor K. Prasanna; Tero Rissa; Cristina Silvano; Hayden Kwok‐Hay So; Yu Wang",
    "corresponding_authors": "",
    "abstract": "A summary of contributions made by significant papers from the first 25 years of the Field-Programmable Logic and Applications conference (FPL) is presented. The 27 papers chosen represent those which have most strongly influenced theory and practice in the field.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2570060992",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1661438",
    "publication_date": "2010-01-01",
    "publication_year": 2010,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4239966922",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1462586",
    "publication_date": "2009-01-01",
    "publication_year": 2009,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4240798459",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1575779",
    "publication_date": "2009-09-01",
    "publication_year": 2009,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "This article explores the application of reconfigurable hardware to the acceleration of financial computation using tree-based pricing models. Two parallel pipelined architectures have been developed for option valuation using binomial trees and ...",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4243281591",
    "type": "paratext"
  },
  {
    "title": "BISWSRBS: A Winograd-based CNN Accelerator with a Fine-grained Regular Sparsity Pattern and Mixed Precision Quantization",
    "doi": "https://doi.org/10.1145/3467476",
    "publication_date": "2021-09-13",
    "publication_year": 2021,
    "authors": "Tao Yang; Zhezhi He; Tengchuan Kou; Qingzheng Li; Qi Han; Haibao Yu; Fangxin Liu; Yun Liang; Li Jiang",
    "corresponding_authors": "",
    "abstract": "Field-programmable Gate Array (FPGA) is a high-performance computing platform for Convolution Neural Networks (CNNs) inference. Winograd algorithm, weight pruning, and quantization are widely adopted to reduce the storage and arithmetic overhead of CNNs on FPGAs. Recent studies strive to prune the weights in the Winograd domain, however, resulting in irregular sparse patterns and leading to low parallelism and reduced utilization of resources. Besides, there are few works to discuss a suitable quantization scheme for Winograd. In this article, we propose a regular sparse pruning pattern in the Winograd-based CNN, namely, Sub-row-balanced Sparsity (SRBS) pattern, to overcome the challenge of the irregular sparse pattern. Then, we develop a two-step hardware co-optimization approach to improve the model accuracy using the SRBS pattern. Based on the pruned model, we implement a mixed precision quantization to further reduce the computational complexity of bit operations. Finally, we design an FPGA accelerator that takes both the advantage of the SRBS pattern to eliminate low-parallelism computation and the irregular memory accesses, as well as the mixed precision quantization to get a layer-wise bit width. Experimental results on VGG16/VGG-nagadomi with CIFAR-10 and ResNet-18/34/50 with ImageNet show up to 11.8×/8.67× and 8.17×/8.31×/10.6× speedup, 12.74×/9.19× and 8.75×/8.81×/11.1× energy efficiency improvement, respectively, compared with the state-of-the-art dense Winograd accelerator [20] with negligible loss of model accuracy. We also show that our design has 4.11× speedup compared with the state-of-the-art sparse Winograd accelerator [19] on VGG16.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3199348509",
    "type": "article"
  },
  {
    "title": "Request, Coalesce, Serve, and Forget: Miss-Optimized Memory Systems for Bandwidth-Bound Cache-Unfriendly Applications on FPGAs",
    "doi": "https://doi.org/10.1145/3466823",
    "publication_date": "2021-12-01",
    "publication_year": 2021,
    "authors": "Mikhail Asiatici; Paolo Ienne",
    "corresponding_authors": "",
    "abstract": "Applications such as large-scale sparse linear algebra and graph analytics are challenging to accelerate on FPGAs due to the short irregular memory accesses, resulting in low cache hit rates. Nonblocking caches reduce the bandwidth required by misses by requesting each cache line only once, even when there are multiple misses corresponding to it. However, such reuse mechanism is traditionally implemented using an associative lookup. This limits the number of misses that are considered for reuse to a few tens, at most. In this article, we present an efficient pipeline that can process and store thousands of outstanding misses in cuckoo hash tables in on-chip SRAM with minimal stalls. This brings the same bandwidth advantage as a larger cache for a fraction of the area budget, because outstanding misses do not need a data array, which can significantly speed up irregular memory-bound latency-insensitive applications. In addition, we extend nonblocking caches to generate variable-length bursts to memory, which increases the bandwidth delivered by DRAMs and their controllers. The resulting miss-optimized memory system provides up to 25% speedup with 24× area reduction on 15 large sparse matrix-vector multiplication benchmarks evaluated on an embedded and a datacenter FPGA system.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3216286241",
    "type": "article"
  },
  {
    "title": "Design and Evaluation of a Tunable PUF Architecture for FPGAs",
    "doi": "https://doi.org/10.1145/3491237",
    "publication_date": "2021-11-29",
    "publication_year": 2021,
    "authors": "Franz-Josef Streit; P. Kruger; Andreas Becher; Stefan Wildermann; Jürgen Teich",
    "corresponding_authors": "",
    "abstract": "FPGA-based Physical Unclonable Functions (PUF) have emerged as a viable alternative to permanent key storage by turning effects of inaccuracies during the manufacturing process of a chip into a unique, FPGA-intrinsic secret. However, many fixed PUF designs may suffer from unsatisfactory statistical properties in terms of uniqueness, uniformity, and robustness. Moreover, a PUF signature may alter over time due to aging or changing operating conditions, rendering a PUF insecure in the worst case. As a remedy, we propose CHOICE , a novel class of FPGA-based PUF designs with tunable uniqueness and reliability characteristics. By the use of addressable shift registers available on an FPGA, we show that a wide configuration space for adjusting a device-specific PUF response is obtained without any sacrifice of randomness. In particular, we demonstrate the concept of address-tunable propagation delays, whereby we are able to increase or decrease the probability of obtaining “ 1 ”s in the PUF response. Experimental evaluations on a group of six 28 nm Xilinx Artix-7 FPGAs show that CHOICE PUFs provide a large range of configurations to allow a fine-tuning to an average uniqueness between 49% and 51%, while simultaneously achieving bit error rates below 1.5%, thus outperforming state-of-the-art PUF designs. Moreover, with only a single FPGA slice per PUF bit, CHOICE is one of the smallest PUF designs currently available for FPGAs. It is well-known that signal propagation delays are affected by temperature, as the operating temperature impacts the internal currents of transistors that ultimately make up the circuit. We therefore comprehensively investigate how temperature variations affect the PUF response and demonstrate how the tunability of CHOICE enables us to determine configurations that show a high robustness to such variations. As a case study, we present a cryptographic key generation scheme based on CHOICE PUF responses as device-intrinsic secret and investigate the design objectives resource costs, performance, and temperature robustness to show the practicability of our approach.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3216491377",
    "type": "article"
  },
  {
    "title": "Cloud Building Block Chip for Creating FPGA and ASIC Clouds",
    "doi": "https://doi.org/10.1145/3466822",
    "publication_date": "2021-12-01",
    "publication_year": 2021,
    "authors": "Atakan Doğan; Kemal Ebci̇oğlu",
    "corresponding_authors": "",
    "abstract": "Hardware-accelerated cloud computing systems based on FPGA chips (FPGA cloud) or ASIC chips (ASIC cloud) have emerged as a new technology trend for power-efficient acceleration of various software applications. However, the operating systems and hypervisors currently used in cloud computing will lead to power, performance, and scalability problems in an exascale cloud computing environment. Consequently, the present study proposes a parallel hardware hypervisor system that is implemented entirely in special-purpose hardware, and that virtualizes application-specific multi-chip supercomputers, to enable virtual supercomputers to share available FPGA and ASIC resources in a cloud system. In addition to the virtualization of multi-chip supercomputers, the system’s other unique features include simultaneous migration of multiple communicating hardware tasks, and on-demand increase or decrease of hardware resources allocated to a virtual supercomputer. Partitioning the flat hardware design of the proposed hypervisor system into multiple partitions and applying the chip unioning technique to its partitions, the present study introduces a cloud building block chip that can be used to create FPGA or ASIC clouds as well. Single-chip and multi-chip verification studies have been done to verify the functional correctness of the hypervisor system, which consumes only a fraction of (10%) hardware resources.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3217260874",
    "type": "article"
  },
  {
    "title": "Introduction",
    "doi": "https://doi.org/10.1145/1331897.1331898",
    "publication_date": "2008-03-01",
    "publication_year": 2008,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4248522845",
    "type": "article"
  },
  {
    "title": "Efficient Fine-grained Processor-logic Interactions on the Cache-coherent Zynq Platform",
    "doi": "https://doi.org/10.1145/3277506",
    "publication_date": "2018-12-31",
    "publication_year": 2018,
    "authors": "Alexander Kroh; Oliver Diessel",
    "corresponding_authors": "",
    "abstract": "The introduction of cache-coherent processor-logic interconnects in CPU-FPGA platforms promises low-latency communication between CPU and FPGA fabrics. This reduced latency improves the performance of heterogeneous systems implemented on such devices and gives rise to new software architectures that can better use the available hardware. Via an extended study accelerating the software task scheduler of a microkernel operating system, this article reports on the potential for accelerating applications that exhibit fine-grained interactions. In doing so, we evaluate the performance of direct and cache-coherent communication methods for applications that involve frequent, low-bandwidth transactions between CPU and programmable logic. In the specific case we studied, we found that replacing a highly optimised software implementation of the task scheduler with an FPGA-based scheduler reduces the cost of communication between two software threads by 5.5%. We also found that, while hardware acceleration reduces cache footprint, we still observe execution time variability because of other non-deterministic features of the CPU.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2911076136",
    "type": "article"
  },
  {
    "title": "An Efficient Memory Partitioning Approach for Multi-Pattern Data Access via Data Reuse",
    "doi": "https://doi.org/10.1145/3301296",
    "publication_date": "2019-02-05",
    "publication_year": 2019,
    "authors": "Wensong Li; Fan Yang; Hengliang Zhu; Xuan Zeng; Dian Zhou",
    "corresponding_authors": "",
    "abstract": "Memory bandwidth has become a bottleneck that impedes performance improvement during the parallelism optimization of the datapath. Memory partitioning is a practical approach to reduce bank-level conflicts and increase the bandwidth on a field-programmable gate array. In this work, we propose a memory partitioning approach for multi-pattern data access. First, we propose to combine multiple patterns into a single pattern to reduce the complexity of multi-pattern. Then, we propose to perform data reuse analysis on the combined pattern to find data reuse opportunities and the non-reusable data pattern. Finally, an efficient bank mapping algorithm with low complexity and low overhead is proposed to find the optimal memory partitioning solution. Experimental results demonstrated that compared to the state-of-the-art method, our proposed approach can reduce the number of block RAMS by 58.9% on average, with 79.6% reduction in SLICEs, 85.3% reduction in LUTs, 67.9% in reduction Flip-Flops, 54.6% reduction in DSP48Es, 83.9% reduction in SRLs, 50.0% reduction in storage overhead, 95.0% reduction in execution time, and 77.3% reduction in dynamic power consumption on average. Meanwhile, the performance can be improved by 14.0% on average.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2913102945",
    "type": "article"
  },
  {
    "title": "RAiSD-X",
    "doi": "https://doi.org/10.1145/3364225",
    "publication_date": "2019-12-19",
    "publication_year": 2019,
    "authors": "Nikolaos Alachiotis; Charalampos Vatsolakis; Grigorios G. Chrysos; Dionisios Pnevmatikatos",
    "corresponding_authors": "",
    "abstract": "Detecting traces of positive selection in genomes carries theoretical significance and has practical applications from shedding light on the forces that drive adaptive evolution to the design of more effective drug treatments. The size of genomic datasets currently grows at an unprecedented pace, fueled by continuous advances in DNA sequencing technologies, leading to ever-increasing compute and memory requirements for meaningful genomic analyses. The majority of existing methods for positive selection detection either are not designed to handle whole genomes or scale poorly with the sample size; they inevitably resort to a runtime versus accuracy tradeoff, raising an alarming concern for the feasibility of future large-scale scans. To this end, we present RAiSD-X, a high-performance system that relies on a decoupled access-execute processing paradigm for efficient FPGA acceleration and couples a novel, to our knowledge, sliding-window algorithm for the recently introduced μ statistic with a mutation-driven hashing technique to rapidly detect patterns in the data. RAiSD-X achieves up to three orders of magnitude faster processing than widely used software implementations, and more importantly, it can exhaustively scan thousands of human chromosomes in minutes, yielding a scalable full-system solution for future studies of positive selection in species of flora and fauna.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2994915271",
    "type": "article"
  },
  {
    "title": "An All-digital Compute-in-memory FPGA Architecture for Deep Learning Acceleration",
    "doi": "https://doi.org/10.1145/3640469",
    "publication_date": "2024-01-15",
    "publication_year": 2024,
    "authors": "Yonggen Li; Xin Li; Haibin Shen; Jicong Fan; Yanfeng Xu; Kejie Huang",
    "corresponding_authors": "",
    "abstract": "Field Programmable Gate Array (FPGA) is a versatile and programmable hardware platform, which makes it a promising candidate for accelerating Deep Neural Networks (DNNs). However, FPGA’s computing energy efficiency is low due to the domination of energy consumption by interconnect data movement. In this article, we propose an all-digital Compute-in-memory FPGA architecture for deep learning acceleration. Furthermore, we present a bit-serial computing circuit of the Digital CIM core for accelerating vector-matrix multiplication (VMM) operations. A Network-CIM-deployer ( NCIMD ) is also developed to support automatic deployment and mapping of DNN networks. NCIMD provides a user-friendly API of DNN models in Caffe format. Meanwhile, we introduce a Weight-stationary dataflow and describe the method of mapping a single layer of the network to the CIM array in the architecture. We conduct experimental tests on the proposed FPGA architecture in the field of Deep Learning (DL), as well as in non-DL fields, using different architectural layouts and mapping strategies. We also compare the results with the conventional FPGA architecture. The experimental results show that compared to the conventional FPGA architecture, the energy efficiency can achieve a maximum speedup of 16.1×, while the latency can decrease up to 40% in our proposed CIM FPGA architecture.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4390880248",
    "type": "article"
  },
  {
    "title": "DONGLE 2.0: Direct FPGA-Orchestrated NVMe Storage for HLS",
    "doi": "https://doi.org/10.1145/3650038",
    "publication_date": "2024-03-05",
    "publication_year": 2024,
    "authors": "Yuk Wong; Jialiang Zhang; Jing Li",
    "corresponding_authors": "",
    "abstract": "Rapid growth in data size poses significant computational and memory challenges to data processing. FPGA accelerators and near-storage processing have emerged as compelling solutions for tackling the growing computational and memory requirements. Many FPGA-based accelerators have shown to be effective in processing large data sets by leveraging the storage capability of either host-attached or FPGA-attached storage devices. However, the current HLS development environment does not allow direct access to host-or FPGA-attached NVMe storage from the HLS code. As such, users must frequently hand off between HLS and host code to access data in storage, and such a process requires tedious programming to ensure functional correctness. Moreover, since the HLS code uses radically different methods to access storage compared to DRAM, the HLS codebase targeting DRAM-based platforms cannot be easily ported to NVMe-based platforms, resulting in limited code portability and reusability. Furthermore, frequent suspension of HLS kernel and synchronization between CPU and FPGA introduce significant latency overhead and require sophisticated scheduling mechanisms to hide latency. To address these challenges, we propose a new HLS storage interface named DONGLE 2.0 that enables direct FPGA-orchestrated NVMe storage access. By providing a unified interface for storage and memory access, DONGLE 2.0 allows a single-source HLS program to target multiple memory/storage devices, thus making the codebase cleaner, portable, and more efficient. DONGLE 2.0 is an extension to DONGLE 1.0 [ 1 ] but adds support for host-attached storage. While its primary focus is still on FPGA NVMe access in near-storage configurations, the added host storage support ensures its compatibility with platforms that lack native support for FPGA-attached NVMe storage. We implemented a prototype of DONGLE 2.0 using an AMD/Xilinx Alveo U200 FPGA and Solidigm DC-P4610 SSD. Our evaluation on various workloads showed a geometric mean speed-up of 2.3× and a reduction in lines of code (LoC) by 2.4× compared to the state-of-the-art commercial platform when using FPGA-attached NVMe storage. Moreover, DONGLE 2.0 demonstrated a geometric mean speed-up of 1.5× and a reduction in LoC by 2.4× compared to the state-of-the-art commercial platform when using host-attached NVMe storage.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4392472718",
    "type": "article"
  },
  {
    "title": "Designing an IEEE-Compliant FPU that Supports Configurable Precision for Soft Processors",
    "doi": "https://doi.org/10.1145/3650036",
    "publication_date": "2024-03-15",
    "publication_year": 2024,
    "authors": "Chris Keilbart; Yuhui Gao; Martin Chua; Eric Matthews; Steven J. E. Wilton; Lesley Shannon",
    "corresponding_authors": "",
    "abstract": "Field Programmable Gate Arrays (FPGAs) are commonly used to accelerate floating-point (FP) applications. Although researchers have extensively studied FPGA FP implementations, existing work has largely focused on standalone operators and frequency-optimized designs. These works are not suitable for FPGA soft processors which are more sensitive to latency, impose a lower frequency ceiling, and require IEEE FP standard compliance. We present an open-source floating-point unit (FPU) for FPGA RISC-V soft processors that is fully IEEE compliant with configurable levels of FP precision. Our design emphasizes runtime performance with 25% lower latency in the most common instructions compared to previous works while maintaining efficient resource utilization. Our FPU also allows users to explore various mantissa widths without having to rewrite or recompile their algorithms. We use this to investigate the scalability of our reduced-precision FPU across numerous microbenchmark functions as well as more complex case studies. Our experiments show that applications like the discrete cosine transformation and the Black-Scholes model can realize a speedup of more than 1.35x in conjunction with a 43% and 35% reduction in lookup table and flip-flop resources while experiencing less than a 0.025% average loss in numerical accuracy with a 16-bit mantissa width.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4392849806",
    "type": "article"
  },
  {
    "title": "NC-Library: Expanding SystemC Capabilities for Nested reConfigurable Hardware Modelling",
    "doi": "https://doi.org/10.1145/3662001",
    "publication_date": "2024-04-27",
    "publication_year": 2024,
    "authors": "Julian Haase; Najdet Charaf; Alexander Groß; Diana Göhringer",
    "corresponding_authors": "",
    "abstract": "As runtime reconfiguration is used in an increasing number of hardware architectures, new simulation and modeling tools are needed to support the developer during the design phases. In this article, a language extension for SystemC is presented, together with a design methodology for the description and simulation of dynamically reconfigurable hardware at different levels of abstraction. The library presented offers a high degree of flexibility in the description of reconfiguration features and their management, while allowing runtime reconfiguration simulation, removal, and replacement of custom modules as well as third-party components throughout the architecture development process. In addition, our approach supports the emerging concept of nested reconfiguration and split regions with a minimal simulation overhead of a maximum of three delta cycles for signal and transaction forwarding, and four delta cycles for the reconfiguration process.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4395703441",
    "type": "article"
  },
  {
    "title": "Dynamic-ACTS - A Dynamic Graph Analytics Accelerator For HBM-Enabled FPGAs",
    "doi": "https://doi.org/10.1145/3662002",
    "publication_date": "2024-04-30",
    "publication_year": 2024,
    "authors": "Oluwole Jaiyeoba; Kevin Skadron",
    "corresponding_authors": "",
    "abstract": "Graph processing frameworks suffer performance degradation from under-utilization of available memory bandwidth, because graph traversal often exhibits poor locality. A prior work, ACTS [ 24 ], accelerates graph processing with FPGAs and High Bandwidth Memory (HBM). ACTS achieves locality by partitioning vertex-update messages (based on destination vertex IDs) generated online after active edges have been processed. This work introduces Dynamic-ACTS which builds on ideas in ACTS to support dynamic graphs. The key innovation is to use a hash table to find the edges to be updated. Compared to Gunrock, a GPU graph engine, Dynamic-ACTS achieves a geometric mean speedup of 1.5X, with a maximum speedup of 4.6X. Compared to GraphLily, an FPGA-HBM graph engine, Dynamic-ACTS achieves a geometric speedup of 3.6X, with a maximum speedup of 16.5X. Our results also showed a geometric mean power reduction of 50% and a mean reduction of energy-delay product of 88% over Gunrock. Compared to GraSU, an FPGA graph updating engine, Dynamic-ACTS achieves an average speedup of 15X.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4396528607",
    "type": "article"
  },
  {
    "title": "Efficient SpMM Accelerator for Deep Learning: Sparkle and Its Automated Generator",
    "doi": "https://doi.org/10.1145/3665896",
    "publication_date": "2024-06-07",
    "publication_year": 2024,
    "authors": "Shiyao Xu; Jingfei Jiang; Jinwei Xu; Xifu Qian",
    "corresponding_authors": "",
    "abstract": "Deep learning (DL) technology has made breakthroughs in a wide range of intelligent tasks such as vision, language, recommendation systems, etc. Sparse matrix multiplication (SpMM) is the key computation kernel of most sparse models. Conventional computing platforms such as CPUs, GPUs, and AI chips with regular processing units are unable to effectively support sparse computation due to their fixed structure and instruction sets. This work extends Sparkle, an accelerator architecture, which is developed specifically for processing SpMM in DL. During the balanced data loading process, some modifications are implemented to enhance the flexibility of the Sparkle architecture. Additionally, a Sparkle generator is proposed to accommodate diverse resource constraints and facilitate adaptable deployment. Leveraging Sparkle’s structural parameters and template-based design methods, the generator enables automatic Sparkle circuit generation under varying parameters. An instantiated Sparkle accelerator is implemented on the Xilinx xqvu11p FPGA platform with a specific configuration. Compared to the state-of-the-art SpMM accelerator SIGMA, the Sparkle accelerator instance improves the sparse computing efficiency by about 10 to 20 \\(\\%\\) . Furthermore, the Sparkle instance achieved 7.76 \\(\\times\\) higher performance over the Nvidia Orin NX GPU. More instances of accelerators with different parameters were evaluated, demonstrating that the Sparkle architecture can effectively accelerate SpMM.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4399424818",
    "type": "article"
  },
  {
    "title": "Improving Fault Tolerance for FPGA SoCs through Post-Radiation Design Analysis",
    "doi": "https://doi.org/10.1145/3674841",
    "publication_date": "2024-07-19",
    "publication_year": 2024,
    "authors": "Andrew Elbert Wilson; Nathan Baker; Ethan Campbell; Michael Wirthlin",
    "corresponding_authors": "",
    "abstract": "FPGAs have been shown to operate reliably within harsh radiation environments by employing single-event upset (SEU) mitigation techniques, such as configuration scrubbing, triple-modular redundancy, error correction coding, and radiation aware implementation techniques. The effectiveness of these techniques, however, is limited when using complex system-level designs that employ complex I/O interfaces with single-point failures. In previous work, a complex SoC system running Linux applied several of these techniques only to obtain an improvement of 14 \\(\\times\\) in mean time to failure (MTTF). A detailed post-radiation fault analysis found that the limitations in reliability were due to the DDR interface, the global clock network, and interconnect. This article applied a number of design-specific SEU mitigation techniques to address the limitations in reliability of this design. These changes include triplicating the global clock, optimizing the placement of the reduction output voters and input flip-flops, and employing a mapping technique called “striping.” The application of these techniques improved MTTF of the mitigated design by a factor of 1.54 \\(\\times\\) and thus provides a 22.8X \\(\\times\\) MTTF improvement over the unmitigated design. A post-radiation fault analysis using BFAT was also performed to find the remaining design vulnerabilities.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4400811724",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Section on FPGA 2023",
    "doi": "https://doi.org/10.1145/3695841",
    "publication_date": "2024-09-12",
    "publication_year": 2024,
    "authors": "Suhaib A. Fahmy; Jason D. Bakos",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4402481458",
    "type": "article"
  },
  {
    "title": "Canalis: A Throughput-Optimized Framework for Real-Time Stream Processing of Wireless Communication",
    "doi": "https://doi.org/10.1145/3695880",
    "publication_date": "2024-09-18",
    "publication_year": 2024,
    "authors": "Kuan-Yu Chen; Thomas Nelson; Alireza Khadem; Morteza Fayazi; Sanjay Sri Vallabh Singapuram; Ronald Dreslinski; Nishil Talati; Hun-Seok Kim; David Blaauw",
    "corresponding_authors": "",
    "abstract": "Stream processing, which involves real-time computation of data as it is created or received, is vital for various applications, specifically wireless communication. The evolving protocols, the requirement for high-throughput, and the challenges of handling diverse processing patterns make it demanding. Traditional platforms grapple with meeting real-time throughput and latency requirements due to large data volume, sequential and indeterministic data arrival, and variable data rates, leading to inefficiencies in memory access and parallel processing. We present Canalis, a throughput-optimized framework designed to address these challenges, ensuring high-performance while achieving low energy consumption. Canalis is a hardware-software co-designed system. It includes a programmable spatial architecture, Flux Stream Processing Unit (FluxSPU), proposed by this work to enhance data throughput and energy efficiency. FluxSPU is accompanied by a software stack that eases the programming process. We evaluated Canalis with eight distinct benchmarks. When compared to CPU and GPU in mobile SoC to demonstrate the effectiveness of domain specialization, Canalis achieves an average speedup of 13.4 \\(\\times\\) and 6.6 \\(\\times\\) , and energy savings of 189.8 \\(\\times\\) and 283.9 \\(\\times\\) , respectively. In contrast to equivalent ASICs of the benchmarks, the average energy overhead of Canalis is within 2.4 \\(\\times\\) , successfully maintaining generalizations without incurring significant overhead.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4402597882",
    "type": "article"
  },
  {
    "title": "SILVIA: Automated Superword-Level Parallelism Exploitation via HLS-Specific LLVM Passes for Compute-Intensive FPGA Accelerators",
    "doi": "https://doi.org/10.1145/3705324",
    "publication_date": "2024-11-21",
    "publication_year": 2024,
    "authors": "Giovanni Brignone; Roberto Bosio; Fabrizio Ottati; Claudio Sansoè; Luciano Lavagno",
    "corresponding_authors": "",
    "abstract": "High-level synthesis (HLS) aims at democratizing custom hardware acceleration with highly abstracted software-like descriptions. However, efficient accelerators still require substantial low-level hardware optimizations, defeating the HLS intent. In the context of field-programmable gate arrays, digital signal processors (DSPs) are a crucial resource that typically requires a significant optimization effort for its efficient utilization, especially when used for sub-word vectorization. This work proposes SILVIA, an open-source LLVM transformation pass that automatically identifies superword-level parallelism within an HLS design and exploits it by packing multiple operations, such as additions, multiplications, and multiply-and-adds, into a single DSP. SILVIA is integrated in the flow of the commercial AMD Vitis HLS tool and proves its effectiveness by packing multiple operations on the DSPs without any manual source-code modifications on several diverse state-of-the-art HLS designs such as convolutional neural networks and basic linear algebra subprograms accelerators, reducing the DSP utilization for additions by 70 % and for multiplications and multiply-and-adds by 50 % on average.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404590421",
    "type": "article"
  },
  {
    "title": "Fantastic Circuits and Where to Find Them - A Holistic ILP Formulation for Model-Based Hardware Design",
    "doi": "https://doi.org/10.1145/3705325",
    "publication_date": "2024-11-22",
    "publication_year": 2024,
    "authors": "Nicolai Fiege; Peter Zipf",
    "corresponding_authors": "",
    "abstract": "The end of Moore’s law and Dennard scaling emphasizes the need for application-specific computing architectures to achieve high resource and energy efficiency and real-time performance. The concept of a silicon compiler remains an enduring aspiration for design time reduction. In order to generate hardware implementations at register transfer level from behavioral descriptions, design automation tools must address challenging and interdependent problems, including allocation, scheduling, and binding. Additionally, manual intervention by the user is necessary to balance the resources vs. performance trade-off via, for example, function inlining or loop unrolling/pipelining. Existing approaches typically solve these problems sequentially, compromising optimality in favor of simplicity and run-time. Here we show how to model the whole model-based design flow as one holistic integer linear programming (ILP) formulation aiming at consistently deriving the optimal microarchitecture for any given application. Incorporating clock gating minimizes the number of useless operations with negligible resource overhead (if any), while always guaranteeing optimal throughput. The unified nature of the proposed ILP model enables implementations unmatched by state-of-the-art approaches in terms of resource efficiency and measured power consumption. These results facilitate a streamlined design flow for highly optimized embedded systems in the context of model-based design.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404637342",
    "type": "article"
  },
  {
    "title": "DF-BETA: An FPGA-based Memory Locality Aware Decision Forest Accelerator via Bit-Level Early Termination",
    "doi": "https://doi.org/10.1145/3706114",
    "publication_date": "2024-12-02",
    "publication_year": 2024,
    "authors": "Daichi Tokuda; Shinya Takamaeda-Yamazaki",
    "corresponding_authors": "",
    "abstract": "Decision forests, particularly Gradient Boosted Decision Trees (GBDT), are popular due to their high prediction performance and computational efficiency, making them suitable for embedded systems with circuit size and available energy constraints. In this study, we propose a new lightweight GBDT inference acceleration mechanism through the hardware and algorithm co-design. First, we present LoADPack, a hardware-friendly GBDT algorithm that enhances memory access locality. LoADPack obtains trees where the features and thresholds used across the entire ensemble are regular regardless of a branching direction by unifying some nodes and aligning the memory access patterns. Furthermore, we present DF-BETA, a resource-efficient accelerator for the LoADPack algorithm. DF-BETA utilizes MSB-first bit-serial computation to enable early determination of comparison calculations of 32-bit floating-point numbers, optimizing the operation for determining a branch direction. The hardware complexity and computation termination speed vary with the granularity of bit-serial computation. Therefore, we conduct design space exploration of DF-BETA to identify the optimal configuration. Our findings reveal that using 4-bit-serial comparators minimizes circuit size while achieving the leading throughput. Compared to running unconstrained GBDT on a typical accelerator with 32-bit bit-parallel comparators, our accelerator achieves 1.6 times higher throughput on average while maintaining comparable accuracy.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404919948",
    "type": "article"
  },
  {
    "title": "Introduction to Special Issue on FPGAs in Data Centers, Part II",
    "doi": "https://doi.org/10.1145/3495231",
    "publication_date": "2022-05-10",
    "publication_year": 2022,
    "authors": "Ken Eguro; Stephen Neuendorffer; Viktor K. Prasanna; Hongbo Rong",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4229441099",
    "type": "article"
  },
  {
    "title": "Efficient Compilation and Mapping of Fixed Function Combinational Logic onto Digital Signal Processors Targeting Neural Network Inference and Utilizing High-level Synthesis",
    "doi": "https://doi.org/10.1145/3559543",
    "publication_date": "2022-08-25",
    "publication_year": 2022,
    "authors": "Soheil Nazar Shahsavani; Arash Fayyazi; Mahdi Nazemi; Massoud Pedram",
    "corresponding_authors": "",
    "abstract": "Recent efforts for improving the performance of neural network (NN) accelerators that meet today’s application requirements have given rise to a new trend of logic-based NN inference relying on fixed function combinational logic. Mapping such large Boolean functions with many input variables and product terms to digital signal processors (DSPs) on Field-programmable gate arrays (FPGAs) needs a novel framework considering the structure and reconfigurability of DSP blocks during this process. The proposed methodology in this article maps the fixed function combinational logic blocks to a set of Boolean functions where Boolean operations corresponding to each function are mapped to DSP devices rather than look-up tables on the FPGAs to take advantage of the high performance, low latency, and parallelism of DSP blocks. This article also presents an innovative design and optimization methodology for compilation and mapping of NNs, utilizing fixed function combinational logic to DSPs on FPGAs employing high-level synthesis flow. Our experimental evaluations across several datasets and selected NNs demonstrate the comparable performance of our framework in terms of the inference latency and output accuracy compared to prior art FPGA-based NN accelerators employing DSPs.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4293089346",
    "type": "article"
  },
  {
    "title": "Adaptive Selection and Clustering of Partial Reconfiguration Modules for Modern FPGA Design Flow",
    "doi": "https://doi.org/10.1145/3567427",
    "publication_date": "2022-10-10",
    "publication_year": 2022,
    "authors": "Kang Zhao; Yuchun Ma; Ruining He; J.G. Zhang; Ning Xu; Jinian Bian",
    "corresponding_authors": "",
    "abstract": "Dynamic Partially Reconfiguration (DPR) on FPGA has attracted significant research interest in recent years since it provides benefits such as reduced area and flexible functionality. However, due to the lack of supporting synthesis tools in the current DPR design flow, leveraging benefits from DPR requires specific design expertise with laborious manual design effort. Considering the complicated concurrency relations among various functions, it is challenging to select appropriate Partial Reconfiguration Modules (PR Modules) and cluster them into proper groups with a proper reconfiguration schedule so that the hardware modules can be swapped in and out correctly during the run time. Furthermore, the design of PR Modules also impacts reconfiguration latency and resource utilization greatly. In this paper, we propose a Maximum-Weight Independent Set model to formulate the PR Module selection and clustering problem so that the original manual exploration can be solved efficiently and automatically. We also propose a step-wise adjustment configuration prefetching strategy incorporated in our model to generate optimized reconfiguration schedules. Our proposed approach not only supports various design constraints but also can consider multiple objectives such as area and reconfiguration delay. Experimental results show that our approach can optimize resource utilization and reduce reconfiguration delay with good scalability. Especially, the implementation of the real design case shows that our approach can be embedded in Xilinx's DPR design flow successfully.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4303986854",
    "type": "article"
  },
  {
    "title": "fSEAD: A Composable FPGA-based Streaming Ensemble Anomaly Detection Library",
    "doi": "https://doi.org/10.1145/3568992",
    "publication_date": "2022-10-20",
    "publication_year": 2022,
    "authors": "Binglei Lou; David Boland; Philip H. W. Leong",
    "corresponding_authors": "",
    "abstract": "Machine learning ensembles combine multiple base models to produce a more accurate output. They can be applied to a range of machine learning problems, including anomaly detection. In this paper, we investigate how to maximize the composability and scalability of an FPGA-based streaming ensemble anomaly detector (fSEAD). To achieve this, we propose a flexible computing architecture consisting of multiple partially reconfigurable regions, pblocks, which each implement anomaly detectors. Our proof-of-concept design supports three state-of-the-art anomaly detection algorithms: Loda, RS-Hash and xStream. Each algorithm is scalable, meaning multiple instances can be placed within a pblock to improve performance. Moreover, fSEAD is implemented using High-level synthesis (HLS), meaning further custom anomaly detectors can be supported. Pblocks are interconnected via an AXI-switch, enabling them to be composed in an arbitrary fashion before combining and merging results at run-time to create an ensemble that maximizes the use of FPGA resources and accuracy. Through utilizing reconfigurable Dynamic Function eXchange (DFX), the detector can be modified at run-time to adapt to changing environmental conditions. We compare fSEAD to an equivalent central processing unit (CPU) implementation using four standard datasets, with speed-ups ranging from $3\\times$ to $8\\times$.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4306873730",
    "type": "article"
  },
  {
    "title": "QiCells: A Modular RFSoC-based Approach to Interface Superconducting Quantum Bits",
    "doi": "https://doi.org/10.1145/3571820",
    "publication_date": "2022-12-26",
    "publication_year": 2022,
    "authors": "Richard Gebauer; Nick Karcher; Mehmed Güler; Oliver Sander",
    "corresponding_authors": "",
    "abstract": "Quantum computers will be a revolutionary extension of the heterogeneous computing world. They consist of many quantum bits (qubits) and require a careful design of the interface between the classical computer architecture and the quantum processor. For example, even single nanosecond variations of the interaction may have an influence on the quantum state. Designing a tailored interface electronics is therefore a major challenge, both in terms of signal integrity with respect to single channels, as well as the scaling of the signal count. We developed such an interface electronics, an RFSoC-based qubit control system called QiController. In this article, we present the modular FPGA firmware design of our system. It features so-called digital unit cells, or QiCells. Each cell contains all the logic necessary to interact with a single superconducting qubit, including a custom-built RISC-V-based sequencer. Synchronization and data exchange between the cells is facilitated using a special star-point structure. Versatile routing and frequency-division multiplexing of generated signals between QiCells and converters are also supported. High-level programmability is provided using a custom Python-based description language and an associated compiler. We furthermore provide the resource utilization of our design and demonstrate its correct operation using an actual superconducting five-qubit chip.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4312218763",
    "type": "article"
  },
  {
    "title": "High-efficiency Compressor Trees for Latest AMD FPGAs",
    "doi": "https://doi.org/10.1145/3645097",
    "publication_date": "2024-02-10",
    "publication_year": 2024,
    "authors": "Konstantin J. Hoßfeld; Hans Jakob Damsgaard; Jari Nurmi; Michaela Blott; Thomas B. Preußer",
    "corresponding_authors": "",
    "abstract": "High-fan-in dot product computations are ubiquitous in highly relevant application domains, such as signal processing and machine learning. Particularly, the diverse set of data formats used in machine learning poses a challenge for flexible efficient design solutions. Ideally, a dot product summation is composed from a carry-free compressor tree followed by a terminal carry-propagate addition. On FPGA, these compressor trees are constructed from generalized parallel counters whose architecture is closely tied to the underlying reconfigurable fabric. This work reviews known counter designs and proposes new ones in the context of the new AMD Versal™ fabric. On this basis, we develop a compressor generator featuring variable-sized counters, novel counter composition heuristics, explicit clustering strategies, and case-specific optimizations like logic gate absorption. In comparison to the Vivado™ default implementation, the combination of such a compressor with a novel, highly efficient quaternary adder reduces the LUT footprint across different bit matrix input shapes by 45% for a plain summation and by 46% for a terminal accumulation at a slight cost in critical path delay still allowing an operation well above 500 MHz. We demonstrate the aptness of our solution at examples of low-precision integer dot product accumulation units.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4391718800",
    "type": "article"
  },
  {
    "title": "Introduction to the FPL 2021 Special Section",
    "doi": "https://doi.org/10.1145/3635115",
    "publication_date": "2024-02-12",
    "publication_year": 2024,
    "authors": "Diana Göhringer; Γεώργιος Κεραμίδας; Akash Kumar",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4391746499",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on FPL 2022",
    "doi": "https://doi.org/10.1145/3643474",
    "publication_date": "2024-03-13",
    "publication_year": 2024,
    "authors": "Andreas Koch; Kentaro Sano",
    "corresponding_authors": "",
    "abstract": "This paper presents SAccO (Scalable Accelerator platform Osnabrück), a novel framework for implementing data-intensive applications using scalable and portable reconfigurable hardware accelerators. Instead of using expensive \"reconfigurable ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4392751008",
    "type": "article"
  },
  {
    "title": "Toward FPGA Intellectual Property (IP) Encryption from Netlist to Bitstream",
    "doi": "https://doi.org/10.1145/3656644",
    "publication_date": "2024-04-12",
    "publication_year": 2024,
    "authors": "Daniel Hutchings; Adam Taylor; Jeffrey Goeders",
    "corresponding_authors": "",
    "abstract": "Current IP encryption methods offered by FPGA vendors use an approach where the IP is decrypted during the CAD flow, and remains unencrypted in the bitstream. Given the ease of accessing modern bitstream-to-netlist tools, encrypted IP is vulnerable to inspection and theft from the IP user. While the entire bitstream can be encrypted, this is done by the user, and is not a mechanism to protect confidentiality of 3rd party IP. In this work we present a design methodology, along with a proof-of-concept tool, that demonstrates how IP can remain partially encrypted through the CAD flow and into the bitstream. We show how this approach can support multiple encryption keys from different vendors, and can be deployed using existing CAD tools and FPGA families. Our results document the benefits and costs of using such an approach to provide much greater protection for 3rd party IP.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4394766506",
    "type": "article"
  },
  {
    "title": "A Scalable Accelerator for Local Score Computation of Structure Learning in Bayesian Networks",
    "doi": "https://doi.org/10.1145/3674842",
    "publication_date": "2024-07-02",
    "publication_year": 2024,
    "authors": "Ryota Miyagi; Ryota Yasudo; Kentaro Sano; Hideki Takase",
    "corresponding_authors": "",
    "abstract": "A Bayesian network is a powerful tool for representing uncertainty in data, offering transparent and interpretable inference, unlike neural networks’ black-box mechanisms. To fully harness the potential of Bayesian networks, it is essential to learn the graph structure that appropriately represents variable interrelations within data. Score-based structure learning, which involves constructing collections of potentially optimal parent sets for each variable, is computationally intensive, especially when dealing with high-dimensional data in discrete random variables. Our proposed novel acceleration algorithm extracts high levels of parallelism, offering significant advantages even with reduced reusability of computational results. In addition, it employs an elastic data representation tailored for parallel computation, making it FPGA-friendly and optimizing module occupancy while ensuring uniform handling of diverse problem scenarios. Demonstrated on a Xilinx Alveo U50 FPGA, our implementation significantly outperforms optimal CPU algorithms and is several times faster than GPU implementations on an NVIDIA TITAN RTX. Furthermore, the results of performance modeling for the accelerator indicate that, for sufficiently large problem instances, it is weakly scalable, meaning that it effectively utilizes increased computational resources for parallelization. To our knowledge, this is the first study to propose a comprehensive methodology for accelerating score-based structure learning, blending algorithmic and architectural considerations.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4400227478",
    "type": "article"
  },
  {
    "title": "FPGA Accelerated Implementation of 3D Mesh Secret Sharing Based on Symmetric Similarity of Model",
    "doi": "https://doi.org/10.1145/3689049",
    "publication_date": "2024-08-19",
    "publication_year": 2024,
    "authors": "Zi-Ming Wu; Meng-Yuan Zhao; Bin Yan; Jeng‐Shyang Pan; Hong-Mei Yang",
    "corresponding_authors": "",
    "abstract": "Secret sharing is particularly important in the field of information security, which allows for the reconstruction of secret information from secure shares. However, due to the large amount of data and non-integer data type of 3D (three-dimensional) models, it is less efficient to perform sharing compared to the 2D(two-dimensional) digital images. In order to enable efficient sharing of 3D models as well, this article designs different circuit modules to parallelize the process of sharing 3D models. The proposed structure exploits the vertex extension to perform a complete reconstruction of the 3D model. In the sharing phase, the symmetric similarity of 3D model is exploited to realize the sharing in parallel, which improves the efficiency of sharing. In the reconstruction phase, the Strassen algorithm is used to optimize matrix multiplication, which saves circuit resources. Simulation results show that the structure performs more than 100 times faster than software. The utilization of circuit resources and the size of the share are both optimized. The validity of the designed hardware architecture is confirmed after analyzing the data from experiments performed on several 3D models.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4401687156",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on FPGA-based Embedded Systems for Industrial and IoT Applications",
    "doi": "https://doi.org/10.1145/3698202",
    "publication_date": "2024-10-05",
    "publication_year": 2024,
    "authors": "Satwant Singh; Carlos Montenegro; Yun Liang; Yao Chen; Nele Mentens; Raymond Nijssen",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4403157172",
    "type": "article"
  },
  {
    "title": "A Speculative Loop Pipeline Framework with Accurate Path Modeling for High-Level Synthesis",
    "doi": "https://doi.org/10.1145/3705732",
    "publication_date": "2024-11-26",
    "publication_year": 2024,
    "authors": "Yuhan She; Jierui Liu; Yanlong Huang; Ray C. C. Cheung; Hong Yan",
    "corresponding_authors": "",
    "abstract": "Loop pipelining is a key optimization in High-Level Synthesis (HLS), aimed at overlapping the execution of iterations. Static scheduling, dominant in commercial HLS tools, configures the pipeline based on compile-time analysis, proving conservative for designs with irregular control flow and memory access due to imbalanced recurrences. Speculative Loop pipeline (SLP) is a novel concept that addresses the problem by introducing the speculation and recovery mechanism at the source level to improve the throughput. Although proven promising, it has a significant gap from practical application: It requires accurate early-stage modeling of the pipeline configuration for each path, which is unable to obtain with classic HLS scheduling methods because the SLP process itself interferes with the path length. In this work, we made a step forward by proposing a practical SLP framework with accurate path modeling ability through iterative tuning. We further optimize the SLP technology by combining automatic dataflow extraction with speculative source-level transformation to further boost the performance in specific design patterns. Our framework works on the source level and is easy to be plugged into existing downstream HLS tools. Experiment results demonstrate significant performance improvements over commercial HLS tools and better resource trade-offs compared to the state-of-the-art dynamic-scheduling-based solutions.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404722456",
    "type": "article"
  },
  {
    "title": "FPGA-based Block Minifloat Training Accelerator for a Time Series Prediction Network",
    "doi": "https://doi.org/10.1145/3707209",
    "publication_date": "2024-12-06",
    "publication_year": 2024,
    "authors": "Wenjie Zhou; Haoyan Qi; David Boland; Philip H. W. Leong",
    "corresponding_authors": "",
    "abstract": "Time series forecasting is the problem of predicting future data samples from historical information and recent deep neural networks (DNNs) based techniques have achieved excellent results compared with conventional statistical approaches. Many applications at the edge can utilise this technology and most implementations have focused on inference, an ability to train at the edge would enable the deep neural network (DNN) to adapt to changing conditions. Unfortunately, training requires approximately three times more memory and computation than inference. Moreover, edge applications are often constrained by energy efficiency. In this work, we implement a block minifloat (BM) training accelerator for a time series prediction network, N-BEATS. Our architecture involves a mixed precision GEMM accelerator that utilizes block minifloat (BM) arithmetic. We use a 4-bit DSP packing scheme to optimize the implementation further, achieving a throughput of 779 Gops. The resulting power efficiency is 42.4 Gops/W, 3.1x better than a graphics processing unit in a similar technology.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4405112145",
    "type": "article"
  },
  {
    "title": "Identification of Dynamic Circuit Specialization Opportunities in RTL Code",
    "doi": "https://doi.org/10.1145/2629640",
    "publication_date": "2015-03-06",
    "publication_year": 2015,
    "authors": "Tom Davidson; Elias Vansteenkiste; Karel Heyse; Karel Bruneel; Dirk Stroobandt",
    "corresponding_authors": "",
    "abstract": "Dynamic Circuit Specialization (DCS) optimizes a Field-Programmable Gate Array (FPGA) design by assuming a set of its input signals are constant for a reasonable amount of time, leading to a smaller and faster FPGA circuit. When the signals actually change, a new circuit is loaded into the FPGA through runtime reconfiguration. The signals the design is specialized for are called parameters. For certain designs, parameters can be selected so the DCS implementation is both smaller and faster than the original implementation. However, DCS also introduces an overhead that is difficult for the designer to take into account, making it hard to determine whether a design is improved by DCS or not. This article presents extensive results on a profiling methodology that analyses Register-Transfer Level (RTL) implementations of applications to check if DCS would be beneficial. It proposes to use the functional density as a measure for the area efficiency of an implementation, as this measure contains both the overhead and the gains of a DCS implementation. The first step of the methodology is to analyse the dynamic behaviour of signals in the design, to find good parameter candidates. The overhead of DCS is highly dependent on this dynamic behaviour. A second stage calculates the functional density for each candidate and compares it to the functional density of the original design. The profiling methodology resulted in three implementations of a profiling tool, the DCS-RTL profiler. The execution time, accuracy, and the quality of each implementation is assessed based on data from 10 RTL designs. All designs, except for the two 16-bit adaptable Finite Impulse Response (FIR) filters, are analysed in 1 hour or less.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W1978323085",
    "type": "article"
  },
  {
    "title": "Autonomous Soft-Error Tolerance of FPGA Configuration Bits",
    "doi": "https://doi.org/10.1145/2629580",
    "publication_date": "2015-03-24",
    "publication_year": 2015,
    "authors": "Anup Das; Shyamsundar Venkataraman; Akash Kumar",
    "corresponding_authors": "",
    "abstract": "Field-programmable gate arrays (FPGAs) are increasingly susceptible to radiation-induced single event upsets (SEUs). These upsets are predominant in a space environment; however, with increasing use of static RAM (SRAM) in modern FPGAs, these SEUs are gaining prominence even in a terrestrial environment. SEUs can flip SRAM bits of FPGA, potentially altering the functionality of the implemented design. This has motivated FPGA designers to investigate techniques to protect the FPGA configuration bits against such inadvertent bit flips (soft error). Traditionally, triple modular redundancy (TMR) is used to protect the FPGA bit flips. Increasing design complexity and limited battery life motivate for alternative approaches for soft-error tolerance. In this article, we propose a technique to improve autonomous fault-masking capabilities of a design by maximizing the number of zeros or ones in lookup tables (LUTs). The technique analyzes critical configuration bits and utilizes spare resources (XOR gates and carry chains) of FPGAs to selectively manipulate the logic implemented in LUTs using two operations: LUT restructuring and LUT decomposition. We implemented the proposed approach for Xilinx Virtex-6 FPGAs and validated the same with a wide set of designs from the MCNC, IWLS 2005, and ITC99 benchmark suites. Results demonstrate that the proposed logic restructuring maximizes logic 0 (or 1) of LUTs by an average of 20%, achieving 80% fault masking with no area overhead. The fault rate of the entire design is reduced by 60% on average as compared to the existing techniques. Furthermore, the logic decomposition algorithm provides incremental fault-tolerance capabilities and achieves an additional 5% fault masking with an average 7% increase in slice usage. The complete methodology is implemented into a tool for Xilinx FPGA and is made available online for the benefit of the research community. The algorithms are lightweight, and the whole design flow (including Xilinx Place and Route) was completed in 75 minutes for the largest benchmark in the set.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2007876688",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on the 11 <sup>th</sup> International Conference on Field-Programmable Technology (FPT'12)",
    "doi": "https://doi.org/10.1145/2655712",
    "publication_date": "2014-08-01",
    "publication_year": 2014,
    "authors": "Jason H. Anderson; Ki‐Young Choi",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2049941566",
    "type": "article"
  },
  {
    "title": "Guest Editorial FPL 2013",
    "doi": "https://doi.org/10.1145/2737805",
    "publication_date": "2015-03-24",
    "publication_year": 2015,
    "authors": "João M. P. Cardoso; Pedro C. Diniz; Katherine Morrow",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2069632730",
    "type": "editorial"
  },
  {
    "title": "FPGA Implementation of a Special-Purpose VLIW Structure for Double-Precision Elementary Function",
    "doi": "https://doi.org/10.1145/2617594",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "Yuanwu Lei; Leifeng Guo; Yong Dou; Sheng Ma; Jinbo Xu",
    "corresponding_authors": "",
    "abstract": "In the current article, the capability and flexibility of field programmable gate-arrays (FPGAs) to implement IEEE-754 double-precision floating-point elementary functions are explored. To perform various elementary functions on the unified hardware efficiently, we propose a special-purpose very long instruction word (VLIW) processor, called DP_VELP. This processor is equipped with multiple basic units, and its performance is improved through an explicitly parallel technique. Pipelined evaluation of polynomial approximation with Estrin's scheme is proposed, by scheduling basic components in an optimal order to avoid data hazard stalls and achieve minimal latency. The custom VLIW processor can achieve high scalability. Under the control of specific VLIW instructions, the basic units are combined into special-purpose hardware for elementary functions. Common elementary functions are presented as examples to illustrate the design of elementary function in DP_VELP in detail. Minimax approximation scheme is used to reduce degree of polynomial. Compromise between the size of lookup table and the latency is discussed, and the internal precision is carefully planned to guarantee accuracy of the result. Finally, we create a prototype of the DP_VELP unit and an FPGA accelerator based on the DP_VELP unit on a Xilinx XC6VLX760 FPGA chip to implement the SGP4/SDP4 application. Compared with previous researches, the proposed design can achieve low latency with a reasonable amount of resources and evaluate a variety of elementary functions with the unified hardware to satisfy the demands in scientific applications. Experimental results show that the proposed design guarantees more than 99% of correct rounding. Moreover, the SGP4/SDP4 accelerator, which is equipped with 39 DP_VELP units and runs at 200 MHz, outperforms the parallel software approach with hyper-thread technology on an Intel Xeon Quad E5620 CPU at 2.40 GHz by a factor of 7X.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2104393432",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on the 7th International Workshop on Reconfigurable Communication-centric Systems-on-Chip (ReCoSoC'12)",
    "doi": "https://doi.org/10.1145/2655710",
    "publication_date": "2014-08-01",
    "publication_year": 2014,
    "authors": "Nikolaos Voros; Guy Gogniat",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2113999957",
    "type": "article"
  },
  {
    "title": "Introduction to the TRETS Special Section on the Workshop on Self-Awareness in Reconfigurable Computing Systems (SRCS'12)",
    "doi": "https://doi.org/10.1145/2611564",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "Tobias Becker",
    "corresponding_authors": "Tobias Becker",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2159002928",
    "type": "article"
  },
  {
    "title": "Guest Editorial ARC 2014",
    "doi": "https://doi.org/10.1145/2831431",
    "publication_date": "2015-11-02",
    "publication_year": 2015,
    "authors": "Diana Goehringer; Marco D. Santambrogio; João M. P. Cardoso; Koen Bertels",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2198262283",
    "type": "editorial"
  },
  {
    "title": "Guest Editorial RAW 2014",
    "doi": "https://doi.org/10.1145/2841314",
    "publication_date": "2016-02-03",
    "publication_year": 2016,
    "authors": "Marco D. Santambrogio; Ramachandran Vaidyanathan",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2343584709",
    "type": "editorial"
  },
  {
    "title": "Introduction to Special Issue on Reconfigurable Components with Source Code",
    "doi": "https://doi.org/10.1145/2907949",
    "publication_date": "2016-05-20",
    "publication_year": 2016,
    "authors": "André DeHon; Derek Chiou",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2406788532",
    "type": "article"
  },
  {
    "title": "A Retargetable Compilation Framework for Heterogeneous Reconfigurable Computing",
    "doi": "https://doi.org/10.1145/2843946",
    "publication_date": "2016-08-11",
    "publication_year": 2016,
    "authors": "Zain Ul-Abdin; Bertil Svensson",
    "corresponding_authors": "",
    "abstract": "The future trend in microprocessors for the more advanced embedded systems is focusing on massively parallel reconfigurable architectures, consisting of heterogeneous ensembles of hundreds of processing elements communicating over a reconfigurable interconnection network. However, the mastering of low-level microarchitectural details involved in the programming of such massively parallel platforms becomes too cumbersome, which limits their adoption in many applications. Thus, there is a dire need for an approach to produce high-performance scalable implementations that harness the computational resources of the emerging reconfigurable platforms. This article addresses the grand challenge of accessibility of these diverse reconfigurable platforms by suggesting the use of a high-level language, occam-pi, and developing a complete design flow for building, compiling, and generating machine code for heterogeneous coarse-grained hardware. We have evaluated the approach by implementing complex industrial case studies and three common signal processing algorithms. The results of the implemented case studies suggest that the occam-pi language-based approach, because of its well-defined semantics for expressing concurrency and reconfigurability, simplifies the development of applications employing runtime reconfigurable devices. The associated compiler framework ensures portability as well as the performance benefits across heterogeneous platforms.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2511035207",
    "type": "article"
  },
  {
    "title": "A Flexible SoC and Its Methodology for Parser-Based Applications",
    "doi": "https://doi.org/10.1145/2939379",
    "publication_date": "2016-09-24",
    "publication_year": 2016,
    "authors": "Bertrand Le Gal; Yérom-David Bromberg; Laurent Réveillère; Jigar Solanki",
    "corresponding_authors": "",
    "abstract": "Embedded systems are being increasingly network interconnected. They are required to interact with their environment through text-based protocol messages. Parsing such messages is control dominated. The work presented in this article attempts to accelerate message parsers using a codesign-based approach. We propose a generic architecture associated with an automated design methodology that enables SoC/SoPC system generation from high-level specifications of message protocols. Experimental results obtained on a Xilinx ML605 board show acceleration factors ranging from four to 11. Both static and dynamic reconfigurations of coprocessors are discussed and then evaluated so as to reduce the system hardware complexity.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2527089770",
    "type": "article"
  },
  {
    "title": "Introduction",
    "doi": "https://doi.org/10.1145/2955103",
    "publication_date": "2016-09-02",
    "publication_year": 2016,
    "authors": "Deming Chen",
    "corresponding_authors": "Deming Chen",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2605737239",
    "type": "article"
  },
  {
    "title": "Application of Specific Delay Window Routing for Timing Optimization in FPGA Designs",
    "doi": "https://doi.org/10.1145/2892640",
    "publication_date": "2016-08-09",
    "publication_year": 2016,
    "authors": "Evan Wegley; Yanhua Yi; Qinhai Zhang",
    "corresponding_authors": "",
    "abstract": "In addition to optimizing for long-path timing and routability, commercial FPGA routing engines must also optimize for various timing constraints, enabling users to fine tune their designs. These timing constraints involve both long- and short-path timing requirements. The intricacies of commercial FPGA architectures add difficulty to the problem of supporting such constraints. In this work, we introduce specific delay window routing as a general method for optimization during the routing stage of the FPGA design flow, which can be applied to various timing constraints constituting both long- and short-path requirements. Furthermore, we propose a key adjustment to standard FPGA routing technology for the purposes of specific delay window routing. By using dual-wave expansion instead of traditional single-wave expansion, we solve the critical issue of inaccurate delay estimation in our wave search, which would otherwise make routing according to a specific delay window difficult. Our results show that this dual-wave method can support stricter timing constraints than the standard single-wave method. For a suite of designs with constraints requiring connections to meet a target delay within 250ps, our dual-wave method could satisfy the requirement for all designs, whereas the single-wave method failed for more than two thirds of the designs.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3086650877",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2746532",
    "publication_date": "2015-04-17",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "In this article, we describe an alternative circuit design methodology when considering trade-offs between accuracy, performance, and silicon area. We compare two different approaches that could trade accuracy for performance. One is the traditional ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4237416831",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2940351",
    "publication_date": "2016-09-12",
    "publication_year": 2016,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Soft vector processors can augment and extend the capability of FPGA-based embedded systems-on-chip such as the Xilinx Zynq. However, configuring and optimizing the soft processor for best performance is hard. We must consider architectural parameters ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4241611893",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2854101",
    "publication_date": "2016-02-03",
    "publication_year": 2016,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Multiple input multiple output (MIMO) with orthogonal frequency division multiplexing (OFDM) systems typically use orthogonal-triangular (QR) decomposition. In this article, we present an enhanced systolic array architecture to realize QR decomposition ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4242082768",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2744082",
    "publication_date": "2015-03-06",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Dependability issues due to nonfunctional properties are emerging as a major cause of faults in modern digital systems. Effective countermeasures have to be developed to properly manage their critical timing effects. This article presents a methodology ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4246825722",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2664590",
    "publication_date": "2014-08-01",
    "publication_year": 2014,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Multi-ported memories are challenging to implement on FPGAs since the block RAMs included in the fabric typically have only two ports. Hence we must construct memories requiring more than two ports, either out of logic elements or by combining multiple ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4247048208",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3002131",
    "publication_date": "2016-12-28",
    "publication_year": 2016,
    "authors": "Nicholas Wulf; Alan D. George; Ross Gordon",
    "corresponding_authors": "",
    "abstract": "On-board processing systems are often deployed in harsh aerospace environments and must therefore adhere to stringent constraints such as low power, small size, and high dependability in the presence of faults. Field-programmable gate arrays (FPGAs) are ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4247790078",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2984740",
    "publication_date": "2016-09-20",
    "publication_year": 2016,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The future trend in microprocessors for the more advanced embedded systems is focusing on massively parallel reconfigurable architectures, consisting of heterogeneous ensembles of hundreds of processing elements communicating over a reconfigurable ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4248770280",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2638850",
    "publication_date": "2014-06-01",
    "publication_year": 2014,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Exploring architectures for large, modern FPGAs requires sophisticated software that can model and target hypothetical devices. Furthermore, research into new CAD algorithms often requires a complete and open source baseline CAD flow. This article ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4248998059",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2839314",
    "publication_date": "2015-11-24",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "There is an increasing concern about transient errors in deep submicron processor architectures. Software-only error detection approaches that exploit program invariants for silent error detection incur large execution overheads and are unreliable as ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4250033051",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2822909",
    "publication_date": "2015-10-01",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Reconfigurable computing devices such as field-programmable gate arrays (FPGAs) offer advantages over fixed-logic CPU and GPU architectures, including improved performance, superior power efficiency, and reconfigurability. The challenge of FPGA ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4251361808",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2589584",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Advances in silicon process technology have made it possible to include multiple processor cores on a single die. Billion transistor architectures usually in the form of networks-on-chip present a wide range of challenges in design, microarchitecture, ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4252232887",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2699137",
    "publication_date": "2015-01-23",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We demonstrate circuits that generate set and integer partitions on a set S of n objects at a rate of one per clock. Partitions are ways to group elements of a set together and have been extensively studied by researchers in algorithm design and theory. ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4253299244",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2770880",
    "publication_date": "2015-05-19",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We consider the impact of compiler optimizations on the quality of high-level synthesis (HLS)-generated field-programmable gate array (FPGA) hardware. Using an HLS tool implemented within the state-of-the-art LLVM compiler, we study the effect of ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4253690438",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2740885",
    "publication_date": "2015-02-01",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4256576782",
    "type": "paratext"
  },
  {
    "title": "Analytical Performance Estimation for Large-Scale Reconfigurable Dataflow Platforms",
    "doi": "https://doi.org/10.1145/3452742",
    "publication_date": "2021-08-12",
    "publication_year": 2021,
    "authors": "Ryota Yasudo; José G. F. Coutinho; Ana-Lucia Varbanescu; Wayne Luk; Hideharu Amano; Tobias Becker; Ce Guo",
    "corresponding_authors": "",
    "abstract": "Next-generation high-performance computing platforms will handle extreme data- and compute-intensive problems that are intractable with today’s technology. A promising path in achieving the next leap in high-performance computing is to embrace heterogeneity and specialised computing in the form of reconfigurable accelerators such as FPGAs, which have been shown to speed up compute-intensive tasks with reduced power consumption. However, assessing the feasibility of large-scale heterogeneous systems requires fast and accurate performance prediction. This article proposes Performance Estimation for Reconfigurable Kernels and Systems (PERKS), a novel performance estimation framework for reconfigurable dataflow platforms. PERKS makes use of an analytical model with machine and application parameters for predicting the performance of multi-accelerator systems and detecting their bottlenecks. Model calibration is automatic, making the model flexible and usable for different machine configurations and applications, including hypothetical ones. Our experimental results show that PERKS can predict the performance of current workloads on reconfigurable dataflow platforms with an accuracy above 91%. The results also illustrate how the modelling scales to large workloads, and how performance impact of architectural features can be estimated in seconds.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3193665245",
    "type": "article"
  },
  {
    "title": "Hardware Context Switch-based Cryptographic Accelerator for Handling Multiple Streams",
    "doi": "https://doi.org/10.1145/3460941",
    "publication_date": "2021-08-12",
    "publication_year": 2021,
    "authors": "Arif Sasongko; I. M. Narendra Kumara; Arief Wicaksana; Frédéric Rousseau; Olivier Muller",
    "corresponding_authors": "",
    "abstract": "The confidentiality and integrity of a stream has become one of the biggest issues in telecommunication. The best available algorithm handling the confidentiality of a data stream is the symmetric key block cipher combined with a chaining mode of operation such as cipher block chaining (CBC) or counter mode (CTR). This scheme is difficult to accelerate using hardware when multiple streams coexist. This is caused by the computation time requirement and mainly by management of the streams. In most accelerators, computation is treated at the block-level rather than as a stream, making the management of multiple streams complex. This article presents a solution combining CBC and CTR modes of operation with a hardware context switching. The hardware context switching allows the accelerator to treat the data as a stream. Each stream can have different parameters: key, initialization value, state of counter. Stream switching was managed by the hardware context switching mechanism. A high-level synthesis tool was used to generate the context switching circuit. The scheme was tested on three cryptographic algorithms: AES, DES, and BC3. The hardware context switching allowed the software to manage multiple streams easily, efficiently, and rapidly. The software was freed of the task of managing the stream state. Compared to the original algorithm, about 18%–38% additional logic elements were required to implement the CBC or CTR mode and the additional circuits to support context switching. Using this method, the performance overhead when treating multiple streams was low, and the performance was comparable to that of existing hardware accelerators not supporting multiple streams.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3196070409",
    "type": "article"
  },
  {
    "title": "Dependency Graph-based High-level Synthesis for Maximum Instruction Parallelism",
    "doi": "https://doi.org/10.1145/3468875",
    "publication_date": "2021-09-13",
    "publication_year": 2021,
    "authors": "Zhenghua Gu; Wenqing Wan; Jundong Xie; Chang Wu",
    "corresponding_authors": "",
    "abstract": "Performance optimization is an important goal for High-level Synthesis (HLS). Existing HLS scheduling algorithms are all based on Control and Data Flow Graph (CDFG) and will schedule basic blocks in sequential order. Our study shows that the sequential scheduling order of basic blocks is a big limiting factor for achievable circuit performance. In this article, we propose a Dependency Graph (DG) with two important properties for scheduling. First, DG is a directed acyclic graph. Thus, no loop breaking heuristic is needed for scheduling. Second, DG can be used to identify the exact instruction parallelism. Our experiment shows that DG can lead to 76% instruction parallelism increase over CDFG. Based on DG, we propose a bottom-up scheduling algorithm to achieve much higher instruction parallelism than existing algorithms. Hierarchical state transition graph with guard conditions is proposed for efficient implementation of such high parallelism scheduling. Our experimental results show that our DG-based HLS algorithm can outperform the CDFG-based LegUp and the state-of-the-art industrial tool Vivado HLS by 2.88× and 1.29× on circuit latency, respectively.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3199987448",
    "type": "article"
  },
  {
    "title": "Note from the TRETS EiC about the new Journal-first track in FPT’21",
    "doi": "https://doi.org/10.1145/3501280",
    "publication_date": "2021-11-29",
    "publication_year": 2021,
    "authors": "Deming Chen",
    "corresponding_authors": "Deming Chen",
    "abstract": "No abstract available.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3216207579",
    "type": "article"
  },
  {
    "title": "Hardware Acceleration of High-Performance Computational Flow Dynamics Using High-Bandwidth Memory-Enabled Field-Programmable Gate Arrays",
    "doi": "https://doi.org/10.1145/3476229",
    "publication_date": "2021-12-06",
    "publication_year": 2021,
    "authors": "Tom Hogervorst; Răzvan Nane; Giacomo Marchiori; Tong Qiu; Markus Blatt; Alf Birger Rustad",
    "corresponding_authors": "",
    "abstract": "Scientific computing is at the core of many High-Performance Computing applications, including computational flow dynamics. Because of the uttermost importance to simulate increasingly larger computational models, hardware acceleration is receiving increased attention due to its potential to maximize the performance of scientific computing. A Field-Programmable Gate Array is a reconfigurable hardware accelerator that is fully customizable in terms of computational resources and memory storage requirements of an application during its lifetime. Therefore, it is an ideal candidate to accelerate scientific computing applications because of the possibility to fully customize the memory hierarchy important in irregular applications such as iterative linear solvers found in scientific libraries. In this paper, we study the potential of using FPGA in HPC because of the rapid advances in reconfigurable hardware, such as the increase in on-chip memory size, increasing number of logic cells, and the integration of High-Bandwidth Memories on board. To perform this study, we first propose a novel ILU0 preconditioner tightly integrated with a BiCGStab solver kernel designed using a mixture of High-Level Synthesis and Register-Transfer Level hand-coded design. Second, we integrate the developed preconditioned iterative solver in Flow from the Open Porous Media (OPM) project, a state-of-the-art open-source reservoir simulator. Finally, we perform a thorough evaluation of the FPGA solver kernel in both standalone mode and integrated into the reservoir simulator that includes all the on-chip URAM and BRAM, on-board High-Bandwidth Memory, and off-chip CPU memory data transfers required in a complex simulator software such as OPM's Flow. We evaluate the performance on the Norne field, a real-world case reservoir model using a grid with more than 10^5 cells and using 3 unknowns per cell.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4200174914",
    "type": "article"
  },
  {
    "title": "Introduction to special section FPGA 2009",
    "doi": "https://doi.org/10.1145/2068716.2068717",
    "publication_date": "2011-12-01",
    "publication_year": 2011,
    "authors": "Peter Y. K. Cheung",
    "corresponding_authors": "Peter Y. K. Cheung",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W1976447725",
    "type": "article"
  },
  {
    "title": "Introduction to the special section on 19th reconfigurable architectures workshop (RAW 2012)",
    "doi": "https://doi.org/10.1145/2499625.2499626",
    "publication_date": "2013-07-01",
    "publication_year": 2013,
    "authors": "Diana Goehringer; René Cumplido",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W1977260701",
    "type": "article"
  },
  {
    "title": "A Performance-Oriented Algorithm with Consideration on Communication Cost for Dynamically Reconfigurable FPGA Partitioning",
    "doi": "https://doi.org/10.1145/1968502.1968507",
    "publication_date": "2011-05-01",
    "publication_year": 2011,
    "authors": "Tzu-Chiang Tai; Yen‐Tai Lai",
    "corresponding_authors": "",
    "abstract": "Dynamically reconfigurable FPGAs (DRFPGAs) have high logic utilization because of time-multiplexed interconnects and logic. In this article, we propose a performance-oriented algorithm for the DRFPGA partitioning problem. This algorithm partitions a given circuit system into stages such that the upper bound of the execution times of subcircuits is minimized. The communication cost is taken into consideration in the process of searching for the optimal solution. A graph is first constructed to represent the precedence constraints and calculate the number of buffers needed in a partitioning. This algorithm includes three phases. The first phase reduces the problem size by clustering the gates into subsystems that have only one output. Such a subsystem has a large number of intraconnections because the fan-outs of all vertices except for the one output are fed to the vertices inside the subsystem. This phase significantly reduces the computational complexity of partitioning. The second phase finds a partition with optimal performance. Finally, the third phase minimizes the communication cost by using an iterative improvement approach. Experimental results based on the Xilinx architecture show that our algorithm yields better partitioning solutions than traditional approaches.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2105811127",
    "type": "article"
  },
  {
    "title": "Domain-Specific Optimization of Signal Recognition Targeting FPGAs",
    "doi": "https://doi.org/10.1145/1968502.1968508",
    "publication_date": "2011-05-01",
    "publication_year": 2011,
    "authors": "Melina Demertzi; Pedro C. Diniz; Mary Hall; Anna C. Gilbert; Yi Wang",
    "corresponding_authors": "",
    "abstract": "Domain-specific optimizations on matrix computations exploiting specific arithmetic and matrix representation formats have achieved significant performance/area gains in Field-Programmable Gate Array (FPGA) hardware designs. In this article, we explore the application of data-driven optimizations to reduce both storage and computation requirements to the problem of signal recognition from a known dictionary. By starting with a high-level mathematical representation of a signal recognition problem, we perform optimizations across the layers of the system, exploiting mathematical structure to improve implementation efficiency. Specifically, we use Walsh wavelet packets in conjunction with a BestBasis algorithm to distinguish between spoken digits. The resulting transform matrices are quite sparse, and exhibit a rich algebraic structure that contains significant overlap across rows. As a consequence, dot-product computations of the transform matrix and signal vectors exhibit significant computation reuse, or repeated identical computations. We present an algorithm for identifying this computation reuse and scheduling of the row computations. We exploit this reuse to derive FPGA hardware implementations that reduce the amount of computation for an individual matrix by as much as 6.35× and an average of 2× for a single dot-product unit. The implementation that exploits reuse achieves a 2× computation reduction compared to three concurrently-executing simpler accumulator units with the same aggregate design area and outperforms software implementations on high-end desktop personal computers.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2128025471",
    "type": "article"
  },
  {
    "title": "Integration of Net-Length Factor with Timing- and Routability-Driven Clustering Algorithms",
    "doi": "https://doi.org/10.1145/2517324",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Hanyu Liu; Senthilkumar T. Rajavel; Ali Akoglu",
    "corresponding_authors": "",
    "abstract": "In FPGA CAD flow, the clustering stage builds the foundation for placement and routing stages and affects performance parameters, such as routability, delay, and channel width significantly. Net sharing and criticality are the two most commonly used factors in clustering cost functions. With this study, we first derive a third term, net-length factor, and then design a generic method for integrating net length into the clustering algorithms. Net-length factor enables characterizing the nets based on the routing stress they might cause during later stages of the CAD flow and is essential for enhancing the routability of the design. We evaluate the effectiveness of integrating net length as a factor into the well-known timing (T-VPack)-, depopulation (T-NDPack)-, and routability (iRAC and T-RPack)-driven clustering algorithms. Through exhaustive experimental studies, we show that net-length factor consistently helps improve the channel-width performance of routability-, depopulation-, and timing-driven clustering algorithms that do not explicitly target low fan-out nets in their cost functions. Particularly, net-length factor leads to average reduction in channel width for T-VPack, T-RPack, and T-NDPack by 11.6%, 10.8%, and 14.2%, respectively, and in a majority of the cases, improves the critical-path delay without increasing the array size.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2143305738",
    "type": "article"
  },
  {
    "title": "Test compression for dynamically reconfigurable processors",
    "doi": "https://doi.org/10.1145/2068716.2068726",
    "publication_date": "2011-12-01",
    "publication_year": 2011,
    "authors": "Hiroaki Inoue; Junya Yamada; Hideyuki Yoneda; Katsumi Togawa; Masato Motomura; Koichiro Furuta",
    "corresponding_authors": "",
    "abstract": "We present the world's first test compression technique that features automation of compression rules for test time reduction on dynamically reconfigurable processors. Evaluations on an actual 40-nm product show that our technique achieves a 2.7 times compression ratio for original configuration information (better than does GZIP), the peak decompression bandwidth of 1.6 GB/s, and 2.7 times shorter test times.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2146164522",
    "type": "article"
  },
  {
    "title": "Optimizing Wait States in the Synthesis of Memory References with Unpredictable Latencies",
    "doi": "https://doi.org/10.1145/2535936",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "Yosi Ben-Asher; Ron Meldiner; Nadav Rotem",
    "corresponding_authors": "",
    "abstract": "We consider the problem of synthesizing circuits (from C to Verilog) that are optimized to handle unpredictable latencies of memory operations. Unpredictable memory latencies can occur due to the use of on chip caches, DRAM memory modules, buffers/queues, or multiport memories. Typically, high-level synthesis compilers assume fixed and known memory latencies, and thus are able to schedule the code’s operations efficiently. The operations in the source code are scheduled into states of a state machine whose states will be synthesized to Verilog. The goal is to minimize scheduling length by maximizing the number of operations (and in particular memory operations) that are executed in parallel at the same state. However, with unpredictable latencies, there can be an exponential number of possible orders in which these parallel memory operations can terminate. Thus, in order to minimize the scheduling, we need a different schedule for any such order. This is not practical, and we show a technique of synthesizing a compact state machine that schedules only a small subset of these possible termination orders. Our results show that this compact state machine can improve the execution time compared to a regular scheduling that waits for the termination of all the active memory references in every state.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2163903259",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2000832",
    "publication_date": "2011-08-01",
    "publication_year": 2011,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "As reconfigurable computing hardware and in particular FPGA-based systems-on-chip comprise an increasing number of processor and accelerator cores, supporting sharing and synchronization in a way that is scalable and easy to program becomes a challenge. ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4232048190",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2457443",
    "publication_date": "2013-05-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Advances in semiconductor technology brings to the market incredibly dense devices, capable of handling tens to hundreds floating-point operators on a single chip; so do the latest field programmable gate arrays (FPGAs). In order to alleviate the ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4233093307",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2499625",
    "publication_date": "2013-07-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The execution runtime usually is a headache for designers performing application mapping onto reconfigurable architectures. In this article we propose a methodology, as well as the supporting toolset, targeting to provide fast application implementation ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4234610190",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1968502",
    "publication_date": "2011-05-01",
    "publication_year": 2011,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Just-In-Time (JIT) compilation is frequently used in software engineering to accelerate program execution. Parts of the code are translated to machine code at runtime to speedup their execution by exploiting local and dynamic information of the ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4237994566",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2558905",
    "publication_date": "2013-12-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "This article describes the design and implementation of a novel compilation flow that implements circuits in FPGAs from a streaming programming language. The streaming language supported is called FPGA Brook and is based on the existing Brook language. ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4241949092",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2068716",
    "publication_date": "2011-12-01",
    "publication_year": 2011,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4246939235",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2535556",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "In FPGA CAD flow, the clustering stage builds the foundation for placement and routing stages and affects performance parameters, such as routability, delay, and channel width significantly. Net sharing and criticality are the two most commonly used ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4247893879",
    "type": "paratext"
  },
  {
    "title": "Bandwidth Management in Application Mapping for Dynamically Reconfigurable Architectures",
    "doi": "https://doi.org/10.1145/1839480.1839488",
    "publication_date": "2010-09-01",
    "publication_year": 2010,
    "authors": "Sudarshan Banerjee; Elaheh Bozorgzadeh; Juanjo Noguera; Nikil Dutt",
    "corresponding_authors": "",
    "abstract": "Partial dynamic reconfiguration (often referred to as partial RTR) enables true on-demand computing. In an on-demand computing environment, a dynamically invoked application is assigned resources such as data bandwidth, configurable logic. The limited logic resources are customized during application execution by exploiting partial RTR. In this article, we propose an approach that maximizes application performance when available bandwidth and logic resources are limited. Our proposed approach is based on theoretical principles of minimizing application schedule length under bandwidth and logic resource constraints. It includes detailed microarchitectural considerations on a commercially popular reconfigurable device, and it exploits partial RTR very effectively by utilizing data-parallelism property of common image-processing applications. We present extensive application case studies on a cycle-accurate simulation platform that includes detailed resource considerations of the Xilinx Virtex XC2V3000. Our experimental results demonstrate that applying our proposed approach to common image-filtering applications leads to 15--20% performance gain in scenarios with limited bandwidth, when compared to prior work that also exploits data-parallelism with RTR but includes simpler bandwidth considerations. Last but not the least, we also demonstrate how our proposed theoretical principles can be directly applied to solve related problems such as minimizing schedule length under logic resource and power constraints.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W1990231588",
    "type": "article"
  },
  {
    "title": "GUEST EDITORS’ INTRODUCTION ICFPT 2007",
    "doi": "https://doi.org/10.1145/1534916.1534917",
    "publication_date": "2009-06-01",
    "publication_year": 2009,
    "authors": "Hideharu Amano; Tadao Nakamura",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2020233214",
    "type": "article"
  },
  {
    "title": "Configuration Merging in Point-to-Point Networks for Module-Based FPGA Reconfiguration",
    "doi": "https://doi.org/10.1145/1661438.1661442",
    "publication_date": "2010-01-01",
    "publication_year": 2010,
    "authors": "Shannon Koh; Oliver Diessel",
    "corresponding_authors": "",
    "abstract": "Partial runtime reconfiguration allows some circuit components to be reconfigured while the remaining circuitry continues to operate. Applications partitioned into modules have the potential to exploit this capability to virtualize hardware by swapping modules as required. One of the challenges in doing so is to provide a communication infrastructure that supports the interfaces and communication needs of a sequence of dynamic module swaps. In contrast to previous approaches which have examined the use of buses and networks-on-chip for this purpose, we examine the use of customized point-to-point wiring harnesses to provide the dynamic connections required for dynamic modular reconfiguration in an efficient manner. The COMMA methodology implements applications on tile-reconfigurable FPGAs, such as the Virtex-4, and its design flow is integrated with the early access partial reconfiguration tools from Xilinx. This article outlines the methodology and describes greedy and dynamic programming approaches to merging the communication graphs of successive configurations in order to generate effective wiring harnesses within the methodology. Our evaluation indicates merging can markedly reduce total reconfiguration delays at the cost of increased critical path delays. Application of the technique is likely to be limited to scenarios in which the execution time between reconfigurations is short.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2043268824",
    "type": "article"
  },
  {
    "title": "A Variable-Grain Logic Cell and Routing Architecture for a Reconfigurable IP Core",
    "doi": "https://doi.org/10.1145/1857927.1857932",
    "publication_date": "2010-12-01",
    "publication_year": 2010,
    "authors": "Kazuki Inoue; Qian Zhao; Yasuhiro Okamoto; Hiroki Yosho; Motoki Amagasaki; Masahiro Iida; Toshinori Sueyoshi",
    "corresponding_authors": "",
    "abstract": "In the present study, we investigate the use of reconfigurable logic devices (RLDs) as intellectual properties (IPs) for system on a chip (SoC). Using RLDs, SoCs can achieve both high performance and high flexibility. However, conventional RLDs have problems related to performance, area, and power consumption. In order to resolve these problems, we investigated the features of RLD architecture. RLDs are classified into fine-grained and coarse-grained devices based on their architecture. Generally, the granularity of an RLD is limited to either type, which means that a device can only achieve high performance in applications that are suited to its architecture. Therefore, we propose a variable-grain logic cell (VGLC) architecture that can overcome the trade-off between fine-grained and coarse-grained architectures, which are required for the implementation of random and arithmetic logics, respectively. The VGLC is based on a 4-bit adder including configuration bits, which can perform arithmetic and random logic operations unlike the LUT. In the present paper, a local interconnection architecture for the VGLC is proposed. Several types of local interconnections composed of different crossbars are compared, and the trade-off between hardware resources and flexibility is discussed. Using local interconnection, the routing area is reduced by a maximum of 49%.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2067228559",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue ARC’08",
    "doi": "https://doi.org/10.1145/1575779.1575780",
    "publication_date": "2009-09-01",
    "publication_year": 2009,
    "authors": "Katherine Compton; Roger Woods; Christos-Savvas Bouganis; Pedro C. Diniz",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2072105565",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on ReCoSoC 2011",
    "doi": "https://doi.org/10.1145/2362374.2362375",
    "publication_date": "2012-10-01",
    "publication_year": 2012,
    "authors": "Michael Hübner",
    "corresponding_authors": "Michael Hübner",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2084278566",
    "type": "article"
  },
  {
    "title": "An Automated Flow for Arithmetic Component Generation in Field-Programmable Gate Arrays",
    "doi": "https://doi.org/10.1145/1839480.1839483",
    "publication_date": "2010-09-01",
    "publication_year": 2010,
    "authors": "Alastair Smith; George A. Constantinides; Peter Y. K. Cheung",
    "corresponding_authors": "",
    "abstract": "State-of-the-art configurable logic platforms, such as Field-Programmable Gate Arrays (FPGAs), consist of a heterogeneous mixture of different component types. Compared to traditional homogeneous configurable platforms, heterogeneity provides speed and density advantages. This is due to the replacement of inefficient programmable logic and routing with specialized logic and fixed interconnect in components such as memories, embedded processor units, and fused arithmetic units. Given the increasing complexity of these components, this article introduces a method to automatically propose and explore the benefits of different types of fused arithmetic units. The methods are based on common subgraph extraction techniques, meaning that it is possible to explore different subcircuits that occur frequently across a set of benchmarks. A quantitative analysis is performed of the various fused arithmetic circuits identified by our tool, which are then automatically synthesized to an ASIC process, providing a study of the speed and area benefits of the components. The results of this study provide bounds on the performance of heterogeneous FPGAs: by incorporating coarse-grain components which match the specific needs of a set of benchmarks we show that significant improvements in circuit speed and area can be made.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2141915750",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2362374",
    "publication_date": "2012-10-01",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Asymmetric coherency is a new optimization method for coherency policies to support nonuniform workloads in multicore processors. Asymmetric coherency assists in load balancing a workload and this is applicable to SoC multicores where the applications ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4237024048",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1534916",
    "publication_date": "2009-06-01",
    "publication_year": 2009,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "As one of the most promising Spintronics applications, MRAM combines the advantages of high writing and reading speed, limitless endurance, and nonvolatility. The integration of MRAM in FPGAs allows the logic circuit to rapidly configure the algorithm, ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4237992798",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1502781",
    "publication_date": "2009-03-01",
    "publication_year": 2009,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "This special issue on Security in Reconfigurable Systems Design reports on recent research results in the design and implementation of trustworthy reconfigurable systems. Five articles cover topics including power-efficient implementation of public-key ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4238849974",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1839480",
    "publication_date": "2010-09-01",
    "publication_year": 2010,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Floating-point applications are a growing trend in the FPGA community. As such, it has become critical to create floating-point units optimized for standard FPGA technology. Unfortunately, the FPGA design space is very different from the VLSI design ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4239549039",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2133352",
    "publication_date": "2012-03-01",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Electronic systems on a chip increasingly suffer from component variation, voltage noise, thermal hotspots, and other subtle physical phenomena. Systems with reconfigurability have unique opportunities for adapting to such effects. Required, however, ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4243297774",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1862648",
    "publication_date": "2010-11-01",
    "publication_year": 2010,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4247317663",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1857927",
    "publication_date": "2010-12-01",
    "publication_year": 2010,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4251066134",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2392616",
    "publication_date": "2012-12-01",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Innovation cycles have been shortening significantly during the last years. This process puts tremendous pressure on designers of embedded systems for security-or reliability-critical applications. Eventual design problems not detected during design ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4254471502",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1575774",
    "publication_date": "2009-09-01",
    "publication_year": 2009,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4254717408",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1754386",
    "publication_date": "2010-05-01",
    "publication_year": 2010,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "FPGA devices have often found use as higher-performance alternatives to programmable processors for implementing computations. Applications successfully implemented on FPGAs typically contain high levels of parallelism and often use simple statically ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4255673542",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2209285",
    "publication_date": "2012-06-01",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Heterogeneous computing systems comprised of accelerators such as FPGAs, GPUs, and manycore processors coupled with standard microprocessors are becoming an increasingly popular solution for future computing systems due to their higher performance and ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4256108269",
    "type": "paratext"
  },
  {
    "title": "Guest Editorial",
    "doi": "https://doi.org/10.1145/1331897.1341292",
    "publication_date": "2008-03-01",
    "publication_year": 2008,
    "authors": "André DeHon; Mike Hutton",
    "corresponding_authors": "",
    "abstract": "editorial Free Access Share on Guest Editorial: TRETS Special Edition on the 15th International Symposium on FPGAs Guest Editors: André DeHon View Profile , Mike Hutton View Profile Authors Info & Claims ACM Transactions on Reconfigurable Technology and SystemsVolume 1Issue 1Article No.: 2pp 1–3https://doi.org/10.1145/1331897.1341292Published:17 March 2008Publication History 0citation417DownloadsMetricsTotal Citations0Total Downloads417Last 12 Months14Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Publisher SiteeReaderPDF",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W1965199411",
    "type": "editorial"
  },
  {
    "title": "Efficient Assembly for High-Order Unstructured FEM Meshes (FPL 2015)",
    "doi": "https://doi.org/10.1145/3024064",
    "publication_date": "2017-04-06",
    "publication_year": 2017,
    "authors": "Pavel Burovskiy; Paul Grigoras; Spencer J. Sherwin; Wayne Luk",
    "corresponding_authors": "",
    "abstract": "The Finite Element Method (FEM) is a common numerical technique used for solving Partial Differential Equations on large and unstructured domain geometries. Numerical methods for FEM typically use algorithms and data structures which exhibit an unstructured memory access pattern. This makes acceleration of FEM on Field-Programmable Gate Arrays using an efficient, deeply pipelined architecture particularly challenging. In this work, we focus on implementing and optimising a vector assembly operation which, in the context of FEM, induces the unstructured memory access. We propose a dataflow architecture, graph-based theoretical model, and design flow for optimising the assembly operation for spectral/hp finite element method on reconfigurable accelerators. We evaluate the proposed approach on two benchmark meshes and show that the graph-theoretic method of generating a static data access schedule results in a significant improvement in resource utilisation compared to prior work. This enables supporting larger FEM meshes on FPGA than previously possible.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2604766758",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Section on FPL 2015",
    "doi": "https://doi.org/10.1145/3041224",
    "publication_date": "2017-04-06",
    "publication_year": 2017,
    "authors": "João M. P. Cardoso; Cristina Silvano",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2604854979",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3166118",
    "publication_date": "2017-12-27",
    "publication_year": 2017,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "A system of interacting agents is, by definition, very demanding in terms of computational resources. Although multi-agent systems have been used to solve complex problems in many areas, it is usually very difficult to perform large-scale simulations in ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4229510727",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1391732",
    "publication_date": "2008-09-01",
    "publication_year": 2008,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "FPGA clock networks consume a significant amount of power, since they toggle every clock cycle and must be flexible enough to implement the clocks for a wide range of different applications. The efficiency of FPGA clock networks can be improved by ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4231365898",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1371579",
    "publication_date": "2008-06-01",
    "publication_year": 2008,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The resistance against powerful index-calculus attacks makes Elliptic Curve Cryptosystems (ECC) an interesting alternative to conventional asymmetric cryptosystems, like RSA. Operands in ECC require significantly less bits at the same level of security, ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4232728353",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3102109",
    "publication_date": "2017-07-21",
    "publication_year": 2017,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Deep convolutional neural networks (CNNs) have gained great success in various computer vision applications. State-of-the-art CNN models for large-scale applications are computation intensive and memory expensive and, hence, are mainly processed on high-...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4235383298",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3068424",
    "publication_date": "2017-04-11",
    "publication_year": 2017,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "In this article, we consider implementing field-programmable gate arrays (FPGAs) using a standard cell design methodology and present a framework for the automated generation of synthesizable FPGA fabrics. The open-source Verilog-to-Routing (VTR) FPGA ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4242259506",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1331897",
    "publication_date": "2008-03-01",
    "publication_year": 2008,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "A new method for improving the timing yield of field-programmable gate array (FPGA) devices affected by intrinsic within-die variation is proposed. The timing variation is reduced by selecting an appropriate configuration for each chip from a set of ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4256245254",
    "type": "paratext"
  },
  {
    "title": "Introduction to Special Section on FPT’20",
    "doi": "https://doi.org/10.1145/3579850",
    "publication_date": "2023-02-15",
    "publication_year": 2023,
    "authors": "Oliver Sinnen; Qiang Liu; Azadeh Davoodi",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4320891709",
    "type": "article"
  },
  {
    "title": "Increasing the Robustness of TERO-TRNGs Against Process Variation",
    "doi": "https://doi.org/10.1145/3597418",
    "publication_date": "2023-05-23",
    "publication_year": 2023,
    "authors": "Christian Skubich; Peter Reichel; Marc Reichenbach",
    "corresponding_authors": "",
    "abstract": "The transition effect ring oscillator is a popular design for building entropy sources because it is compact, built from digital elements only, and is very well suited for FPGAs. However, it is known to be quite sensitive to process variation. Although the latter is useful for building physical unclonable functions, it is interfering with the application as an entropy source. In this article, we investigate an approach to increase reliability. We show that adding a third stage eliminates much of the susceptibility to process variation and how a resulting gigahertz oscillation can be evaluated on an FPGA. The design is supported by physical and stochastic modeling. The physical model is validated using an experiment with dynamically reconfigurable look-up tables.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4377821031",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on FPT 2021",
    "doi": "https://doi.org/10.1145/3603701",
    "publication_date": "2023-06-13",
    "publication_year": 2023,
    "authors": "Andreas Koch; Wei Zhang",
    "corresponding_authors": "",
    "abstract": "An impressive hardness theory which can prove compression lower bounds for a large number of FPT problems has been established under the assumption that NP ⊈ coNP/poly. However, there are no problems in FPT for which the existence of ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4380442551",
    "type": "article"
  },
  {
    "title": "Artifact Evaluation for ACM TRETS Papers Submitted from the FPT Journal Track",
    "doi": "https://doi.org/10.1145/3596513",
    "publication_date": "2023-06-21",
    "publication_year": 2023,
    "authors": "Miriam Leeser",
    "corresponding_authors": "Miriam Leeser",
    "abstract": "Authors of papers that were accepted to ACM TRETS via the FPT 2022 journal track had the option of participating in Artifact Evaluation (AE). Four papers from this track volunteered to participate in the AE process. All of these papers have been awarded badges from ACM as described below.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4381510233",
    "type": "article"
  },
  {
    "title": "A Partitioned CAM Architecture with FPGA Acceleration for Binary Descriptor Matching",
    "doi": "https://doi.org/10.1145/3624749",
    "publication_date": "2023-10-05",
    "publication_year": 2023,
    "authors": "Parastoo Soleimani; David W. Capson; Kin Fun Li",
    "corresponding_authors": "",
    "abstract": "An efficient architecture for image descriptor matching that uses a partitioned content-addressable memory (CAM)-based approach is proposed. CAM is frequently used in high-speed content-matching applications. However, due to its lack of functionality to support approximate matching, conventional CAM is not directly useful for image descriptor matching. Our modifications improve the CAM architecture to support approximate content matching for selecting image matches with local binary descriptors. Matches are based on Hamming distances computed for all possible pairs of binary descriptors extracted from two images. We demonstrate an FPGA-based implementation of our CAM-based descriptor-matching unit to illustrate the high matching speed of our design. The time complexity of our modified CAM method for binary descriptor matching is O(n). Our method performs binary descriptor matching at a rate of one descriptor per clock cycle at a frequency of 102 MHz. The resource utilization and timing metrics of several experiments are reported to demonstrate the efficacy and scalability of our design.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4387377482",
    "type": "article"
  },
  {
    "title": "Covert-channels in FPGA-enabled SmartSSDs",
    "doi": "https://doi.org/10.1145/3635312",
    "publication_date": "2023-12-04",
    "publication_year": 2023,
    "authors": "Theodoros Trochatos; Anthony Etim; Jakub Szefer",
    "corresponding_authors": "",
    "abstract": "Cloud computing providers today offer access to a variety of devices, which users can rent and access remotely in a shared setting. Among these devices are SmartSSDs, which are solid-state disks (SSD) augmented with an FPGA, enabling users to instantiate custom circuits within the FPGA, including potentially malicious circuits for power and temperature measurement. Normally, cloud users have no remote access to power and temperature data, but with SmartSSDs they could abuse the FPGA component to instantiate circuits to learn this information. Additionally, custom power waster circuits can be instantiated within the FPGA. This paper shows for the first time that by leveraging ring oscillator sensors and power wasters, numerous covert-channels in FPGA-enabled SmartSSDs could be used to transmit information. This work presents two channels in single-tenant setting (SmartSSD is used by one user at a time) and two channels in multi-tenant setting (FPGA and SSD inside SmartSSD is shared by different users). The presented covert channels can reach close to 100% accuracy. Meanwhile, bandwidth of the channels can be easily scaled by cloud users renting more SmartSSDs as the bandwidth of the covert channels is proportional to number of SmartSSD used.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4389307124",
    "type": "article"
  },
  {
    "title": "Design, Calibration, and Evaluation of Real-time Waveform Matching on an FPGA-based Digitizer at 10 GS/s",
    "doi": "https://doi.org/10.1145/3635719",
    "publication_date": "2023-12-05",
    "publication_year": 2023,
    "authors": "Jens Trautmann; P. Kruger; Andreas Becher; Stefan Wildermann; Jürgen Teich",
    "corresponding_authors": "",
    "abstract": "Digitizing side-channel signals at high sampling rates produces huge amounts of data, while side-channel analysis techniques only need those specific trace segments containing Cryptographic Operations (COs). For detecting these segments, waveform-matching techniques have been established comparing the signal with a template of the CO’s characteristic pattern. Real-time waveform matching requires highly parallel implementations as achieved by hardware design but also reconfigurability as provided by Field-Programmable Gate Arrays (FPGAs) to adapt the matching hardware to a specific CO pattern. However, currently proposed designs process the samples from analog-to-digital converters sequentially and can only process low sampling rates due to the limited clock speed of FPGAs. In this article, we present a parallel waveform-matching architecture capable of performing high-speed waveform matching on a high-end FPGA-based digitizer. We also present a workflow for calibrating the waveform-matching system to the specific pattern of the CO in the presence of hardware restrictions provided by the FPGA hardware. Our implementation enables waveform matching at 10 GS/s, offering a speedup of 50× compared to the fastest state-of-the-art implementation known to us. We demonstrate how to apply the technique for attacking the widespread XTS-AES algorithm using waveform matching to recover the encrypted tweak even in the presence of so-called systemic noise.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4389337731",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Section on FPGA 2022",
    "doi": "https://doi.org/10.1145/3618114",
    "publication_date": "2023-12-13",
    "publication_year": 2023,
    "authors": "Paolo Ienne",
    "corresponding_authors": "Paolo Ienne",
    "abstract": "introduction Share on Introduction to the Special Section on FPGA 2022 Author: Paolo Ienne École Polytechnique Fédérale de Lausanne (EPFL), Switzerland École Polytechnique Fédérale de Lausanne (EPFL), Switzerland 0000-0002-6142-7345View Profile Authors Info & Claims ACM Transactions on Reconfigurable Technology and SystemsVolume 16Issue 4Article No.: 56pp 1–2https://doi.org/10.1145/3618114Published:13 December 2023Publication History 0citation29DownloadsMetricsTotal Citations0Total Downloads29Last 12 Months29Last 6 weeks15 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4389675774",
    "type": "article"
  },
  {
    "title": "CSAIL2019 Crypto-Puzzle Solver Architecture",
    "doi": "https://doi.org/10.1145/3639056",
    "publication_date": "2023-12-29",
    "publication_year": 2023,
    "authors": "Sergey Gribok; Bogdan Pasca; Martin Langhammer",
    "corresponding_authors": "",
    "abstract": "tThe CSAIL2019 time-lock puzzle is an unsolved cryptographic challenge introduced by Ron Rivest in 2019, replacing the solved LCS35 puzzle. Solving these types of puzzles requires large amounts of intrinsically sequential computations, with each iteration performing a very large (3,072-bit for CSAIL2019) modular multiplication operation. The complexity of each iteration is several times greater than known field-programmable gate array (FPGA) implementations, and the number of iterations has been increased by about 1,000x compared with LCS35. Because of the high complexity of this new puzzle, a number of intermediate, or milestone, versions of the puzzle have been specified. In this article, we present several FPGA architectures for the CSAIL2019 solver, which we implement on a medium-sized Intel Agilex device. We develop a new multi-cycle modular multiplication method, which is flexible and can fit on a wide variety of sizes of current FPGAs. We introduce a class of multi-cycle squarer-based architectures that allow for better resource and area trade-offs. We also demonstrate a new approach for improving the fitting and timing closure of large, chip-filling arithmetic designs. We used the solver to compute the first 23 out of 28 milestone solutions of the puzzle, which are the first reported results for this problem.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4390400552",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Section on FCCM’16",
    "doi": "https://doi.org/10.1145/3183572",
    "publication_date": "2018-03-07",
    "publication_year": 2018,
    "authors": "Jason D. Bakos",
    "corresponding_authors": "Jason D. Bakos",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2798651889",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Section on Deep Learning in FPGAs",
    "doi": "https://doi.org/10.1145/3294768",
    "publication_date": "2018-09-30",
    "publication_year": 2018,
    "authors": "Adrien Prost-Boucle; Alban Bourge; Frédéric Pétrot",
    "corresponding_authors": "",
    "abstract": "editorial Free Access Share on Introduction to the Special Section on Deep Learning in FPGAs Editors: Deming Chen University of Illinois at Urbana-Champaign University of Illinois at Urbana-ChampaignView Profile , Andrew Putnam Microsoft Research Microsoft ResearchView Profile , Steve Wilton University of British Columbia University of British ColumbiaView Profile Authors Info & Claims ACM Transactions on Reconfigurable Technology and SystemsVolume 11Issue 3September 2018 Article No.: 14pp 1–3https://doi.org/10.1145/3294768Published:22 December 2018Publication History 1citation261DownloadsMetricsTotal Citations1Total Downloads261Last 12 Months53Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2914846372",
    "type": "article"
  },
  {
    "title": "FlexSaaS",
    "doi": "https://doi.org/10.1145/3301409",
    "publication_date": "2019-02-17",
    "publication_year": 2019,
    "authors": "Shi‐Jie Cao; Lanshun Nie; Dechen Zhan; Wenqiang Wang; Ningyi Xu; Ramashis Das; Ming Wu; Lintao Zhang; Derek Chiou",
    "corresponding_authors": "",
    "abstract": "Web search engines deploy large-scale selection services on CPUs to identify a set of web pages that match user queries. An FPGA-based accelerator can exploit various levels of parallelism and provide a lower latency, higher throughput, more energy-efficient solution than commodity CPUs. However, maintaining such a customized accelerator in a commercial search engine is challenging because selection services are changed often. This article presents our design for FlexSaaS (Flexible Selection as a Service), an FPGA-based accelerator for web search selection. To address efficiency and flexibility challenges, FlexSaaS abstracts computing models and separates memory access from computation. Specifically, FlexSaaS (i) contains a reconfigurable number of matching processors that can handle various possible query plans, (ii) decouples index stream reading from matching computation to fetch and decode index files, and (iii) includes a universal memory accessor that hides the complex memory hierarchy and reduces host data access latency. Evaluated on FPGAs in the selection service of a commercial web search--the Bing web search engine—FlexSaaS can be evolved quickly to adapt to new updates. Compared to the software baseline, FlexSaaS on Arria 10 reduces average latency by 30% and increases throughput by 1.5×.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2915527401",
    "type": "article"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/3326451",
    "publication_date": "2019-06-17",
    "publication_year": 2019,
    "authors": "Chen Deming",
    "corresponding_authors": "Chen Deming",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2953328090",
    "type": "editorial"
  },
  {
    "title": "The FPOA, a Medium-grained Reconfigurable Architecture for High-level Synthesis",
    "doi": "https://doi.org/10.1145/3340556",
    "publication_date": "2019-11-11",
    "publication_year": 2019,
    "authors": "Jason Gorski; Darrin M. Hanna",
    "corresponding_authors": "",
    "abstract": "In this article, we present a novel type of medium-grained reconfigurable architecture that we term the Field Programmable Operation Array (FPOA). This device has been designed specifically for the implementation of HLS-generated circuitry. At the core of the FPOA is the OP-block. Unlike a standard LUT, an OP-block performs multi-bit operations through gate-based logic structures, translating into greater speed and efficiency in digital circuit implementation. &lt;?tight?&gt;Our device is not optimized for a specific application domain. Rather, we have created a device that is optimized for a specific circuit structure, namely those generated by HLS. This gives the FPOA a significant advantage as it can be used across all application domains. In this work, we add support for both distributed and block memory to the FPOA architecture. Experimental results show up to a 13.5× reduction in logic area and a 9.5× reduction in critical path delay for circuit implementation using the FPOA compared to a standard FPGA.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2988354469",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Section on Security in FPGA-accelerated Cloud and Datacenters",
    "doi": "https://doi.org/10.1145/3352060",
    "publication_date": "2019-09-13",
    "publication_year": 2019,
    "authors": "Chistophe Bobda; Ken Eguro",
    "corresponding_authors": "",
    "abstract": "editorial Free Access Share on Introduction to the Special Section on Security in FPGA-accelerated Cloud and Datacenters Editors: Chistophe Bobda University of Florida Russell Tessier, University of Massachusetts Amherst University of Florida Russell Tessier, University of Massachusetts AmherstView Profile , Ken Eguro Microsoft Research Ryan Kastner, University of California, San Diego Microsoft Research Ryan Kastner, University of California, San DiegoView Profile Authors Info & Claims ACM Transactions on Reconfigurable Technology and SystemsVolume 12Issue 3September 2019 Article No.: 11epp 1–3https://doi.org/10.1145/3352060Published:13 September 2019Publication History 0citation190DownloadsMetricsTotal Citations0Total Downloads190Last 12 Months44Last 6 weeks5 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteView all FormatsPDF",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3046160745",
    "type": "article"
  },
  {
    "title": "PIMAP 並列化反復最適化によるLUTベース技術マッピング改善のための柔軟なフレームワーク【JST・京大機械翻訳】",
    "doi": null,
    "publication_date": "2019-01-01",
    "publication_year": 2019,
    "authors": "Gai Liu; Zhiru Zhang",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3191410357",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3310278",
    "publication_date": "2019-04-05",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Memory bandwidth has become a bottleneck that impedes performance improvement during the parallelism optimization of the datapath. Memory partitioning is a practical approach to reduce bank-level conflicts and increase the bandwidth on a field-...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4231434349",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3242893",
    "publication_date": "2018-11-12",
    "publication_year": 2018,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The large number of embedded soft core processors available today make it tedious and time consuming to select the best processor for a given application. This task is even more challenging due to the numerous configuration options available for a ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4238273068",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3322884",
    "publication_date": "2019-06-18",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Convolutional Neural Network (ConvNet or CNN) algorithms are characterized by a large number of model parameters and high computational complexity. These two requirements have made it challenging for implementations on resource-limited FPGAs. The ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4239182263",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3299999",
    "publication_date": "2018-12-22",
    "publication_year": 2018,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Although performing inference with artificial neural networks (ANN) was until quite recently considered as essentially compute intensive, the emergence of deep neural networks coupled with the evolution of the integration technology transformed ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4240318478",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3357092",
    "publication_date": "2019-09-25",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "In complex FPGA designs, implementations of algorithms and protocols from third-party sources are common. However, the monolithic nature of FPGAs means that all sub-circuits share common on-chip infrastructure, such as routing resources. This presents ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4242875322",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3303942",
    "publication_date": "2019-01-29",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Modern FPGA synthesis tools typically apply a predetermined sequence of logic optimizations on the input logic network before carrying out technology mapping. While the “known recipes” of logic transformations often lead to improved mapping results, ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4244751265",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3178391",
    "publication_date": "2018-03-22",
    "publication_year": 2018,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Soft processors have a role to play in simplifying field-programmable gate array (FPGA) application design as they can be deployed only when needed, and it is easier to write and debug single-threaded software code than create hardware. The breadth of ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4250034470",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3377289",
    "publication_date": "2019-12-19",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4250242378",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3361265",
    "publication_date": "2019-11-27",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Given the growth in data inputs and application complexity, it is often the case that a single hardware accelerator is not enough to solve a given problem. In particular, the computational demands and I/O of many tasks in machine learning often require ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4250419357",
    "type": "paratext"
  },
  {
    "title": "Kernel Normalised Least Mean Squares with Delayed Model Adaptation",
    "doi": "https://doi.org/10.1145/3376924",
    "publication_date": "2020-02-13",
    "publication_year": 2020,
    "authors": "Nicholas J. Fraser; Philip H. W. Leong",
    "corresponding_authors": "",
    "abstract": "Kernel adaptive filters (KAFs) are non-linear filters which can adapt temporally and have the additional benefit of being computationally efficient through use of the “kernel trick”. In a number of real-world applications, such as channel equalisation, the non-linear mapping provides significant improvements over conventional linear techniques such as the least mean squares (LMS) and recursive least squares (RLS) algorithms. Prior works have focused mainly on the theory and accuracy of KAFs, with little research on their implementations. This article proposes several variants of algorithms based on the kernel normalised least mean squares (KNLMS) algorithm which utilise a delayed model update to minimise dependencies. Subsequently, this work proposes corresponding hardware architectures which utilise this delayed model update to achieve high sample rates and low latency while also providing high modelling accuracy. The resultant delayed KNLMS (DKNLMS) algorithms can achieve clock rates up to 12× higher than the standard KNLMS algorithm, with minimal impact on accuracy and stability. A system implementation achieves 250 GOps/s and a throughput of 187.4 MHz on an Ultra96 board with 1.8× higher throughput than previous state of the art.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3014091620",
    "type": "article"
  },
  {
    "title": "Introduction to Special Section on FCCM 2019",
    "doi": "https://doi.org/10.1145/3410373",
    "publication_date": "2020-08-25",
    "publication_year": 2020,
    "authors": "André DeHon",
    "corresponding_authors": "André DeHon",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3081440313",
    "type": "article"
  },
  {
    "title": "PipeArch",
    "doi": "https://doi.org/10.1145/3418465",
    "publication_date": "2020-11-05",
    "publication_year": 2020,
    "authors": "Kaan Kara; Gustavo Alonso",
    "corresponding_authors": "",
    "abstract": "Data processing systems based on FPGAs offer high performance and energy efficiency for a variety of applications. However, these advantages are achieved through highly specialized designs. The high degree of specialization leads to accelerators with narrow functionality and designs adhering to a rigid execution flow. For multi-tenant systems this limits the scope of applicability of FPGA-based accelerators, because, first, supporting a single operation is unlikely to have any significant impact on the overall performance of the system, and, second, serving multiple users satisfactorily is difficult due to simplistic scheduling policies enforced when using the accelerator. Standard operating system and database management system features that would help address these limitations, such as context-switching, preemptive scheduling, and thread migration are practically non-existent in current FPGA accelerator efforts. In this work, we propose PipeArch, an open-source project 1 for developing FPGA-based accelerators that combine the high efficiency of specialized hardware designs with the generality and functionality known from conventional CPU threads. PipeArch provides programmability and extensibility in the accelerator without losing the advantages of SIMD-parallelism and deep pipelining. PipeArch supports context-switching and thread migration, thereby enabling for the first time new capabilities such as preemptive scheduling in FPGA accelerators within a high-performance data processing setting. We have used PipeArch to implement a variety of machine learning methods for generalized linear model training and recommender systems showing empirically their advantages over a high-end CPU and even over fully specialized FPGA designs.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3097469347",
    "type": "article"
  },
  {
    "title": "遅延モデル適応によるカーネル正規化最小平均二乗【JST・京大機械翻訳】",
    "doi": null,
    "publication_date": "2020-01-01",
    "publication_year": 2020,
    "authors": "Jane Nicholas; H W Leong Philip",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3193148781",
    "type": "article"
  },
  {
    "title": "Introduction to Special Issue on FPGAs in Data Centers",
    "doi": "https://doi.org/10.1145/3493607",
    "publication_date": "2022-01-31",
    "publication_year": 2022,
    "authors": "Ken Eguro; Stephen Neuendorffer; Viktor K. Prasanna; Hongbo Rong",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4210567646",
    "type": "article"
  },
  {
    "title": "HopliteML: Evolving Application Customized FPGA NoCs with Adaptable Routers and Regulators",
    "doi": "https://doi.org/10.1145/3507699",
    "publication_date": "2022-02-14",
    "publication_year": 2022,
    "authors": "Gurshaant Malik; Ian Elmore Lang; Rodolfo Pellizzoni; Nachiket Kapre",
    "corresponding_authors": "",
    "abstract": "We can overcome the pessimism in worst-case routing latency analysis of timing-predictable Network-on-Chip (NoC) workloads by single-digit factors through the use of a hybrid field-programmable gate array (FPGA)–optimized NoC and workload-adapted regulation. Timing-predictable FPGA-optimized NoCs such as HopliteBuf integrate stall-free FIFOs that are sized using offline static analysis of a user-supplied flow pattern and rates. For certain bursty traffic and flow configurations, static analysis delivers very large, sometimes infeasible, FIFO size bounds and large worst-case latency bounds. Alternatively, backpressure-based NoCs such as HopliteBP can operate with lower latencies for certain bursty flows. However, they suffer from severe pessimism in the analysis due to the effect of pipelining of packets and interleaving of flows at switch ports. As we show in this article, a hybrid FPGA NoC that seamlessly composes both design styles on a per-switch basis delivers the best of both worlds, with improved feasibility (bounded operation) and tighter latency bounds. We select the NoC switch configuration through a novel evolutionary algorithm based on Maximum Likelihood Estimation (MLE). For synthetic ( RANDOM , LOCAL ) and real-world ( SpMV , Graph ) workloads, we demonstrate ≈2–3× improvements in feasibility and ≈1–6.8× in worst-case latency while requiring an LUT cost only ≈1–1.5× larger than the cheapest HopliteBuf solution. We also deploy and verify our NoC (PL) and MLE framework (PS) on a Pynq-Z1 to adapt and reconfigure NoC switches dynamically. We can further improve a workload’s routability by learning to surgically tune regulation rates for each traffic trace to maximize available routing bandwidth. We capture critical dependency between traces by modelling the regulation space as a multivariate Gaussian distribution and learn the distribution’s parameters using Covariance Matrix Adaptation Evolution Strategy (CMA-ES). We also propose nested learning, which learns switch configurations and regulation rates in tandem. Compared with stand-alone switch learning, this symbiotic nested learning helps achieve ≈ 1.5× lower cost constrained latency, ≈ 3.1× faster individual rates, and ≈ 1.4× faster mean rates. We also evaluate improvements to vanilla NoCs’ routing using only stand-alone rate learning (no switch learning), with ≈ 1.6× lower latency across synthetic and real-world benchmarks.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4213381076",
    "type": "article"
  },
  {
    "title": "Advantages of a Statistical Estimation Approach for Clock Frequency Estimation of Heterogeneous and Irregular CGRAs",
    "doi": "https://doi.org/10.1145/3531062",
    "publication_date": "2022-04-25",
    "publication_year": 2022,
    "authors": "Dennis Leander Wolf; Christoph Spang; Daniel Diener; Christian Hochberger",
    "corresponding_authors": "",
    "abstract": "Estimating the maximum clock frequency of homogeneous Coarse Grained Reconfigurable Arrays/Architectures (CGRAs) with an arbitrary number of Processing Elements (PE) is difficult. Clock frequency estimation of highly heterogeneous CGRAs takes additional factors into account, thus is even more difficult. Main challenges are the heterogeneous set of operators for each Processing Element (PE) and the irregular interconnect (connecting a CGRA’s PEs). Multiple estimation approaches could be reasonable. We propose an optimized statistical estimator, which is based on our prior work. We demonstrate its superiority to state-of-the-art neural networks in terms of accuracy and robustness, especially in situations with a sparse set of training data.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4224731124",
    "type": "article"
  },
  {
    "title": "Median Filters on FPGAs for Infinite Data and Large, Rectangular Windows",
    "doi": "https://doi.org/10.1145/3530273",
    "publication_date": "2022-05-05",
    "publication_year": 2022,
    "authors": "Krystine Dawn Sherwin; Kevin I‐Kai Wang; T. Prabu; B. W. Stappers; Oliver Sinnen",
    "corresponding_authors": "",
    "abstract": "Efficient architectures and implementations of median filters have been well investigated in the past. In this article, we focus on median filters for very big scientific applications with very large windows and an infinite stream of data, inspired by big data needs in the Square Kilometre Array (SKA) pulsar search engine, but transferable to other big data domains. We propose a novel approach for very large rectangular windows on an FPGA accelerator device able to support the processing of infinite streams of data. OpenCL is used for rapid parameter sweeping and design space exploration based on a pipelined model of the system. Evaluation on a host/accelerator system with an Arria 10 device surpassed 64 million values processed per second considered for the SKA real time requirement, achieving 83.4M value/s while reading from/writing to disk. These results are compared with a state-of-the-art software implementation only achieving 41M value/s for over twice the total system energy cost.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4229010545",
    "type": "article"
  },
  {
    "title": "Adaptive Clock Management of HLS-generated Circuits on FPGAs",
    "doi": "https://doi.org/10.1145/3520140",
    "publication_date": "2022-05-30",
    "publication_year": 2022,
    "authors": "Kahlan Gibson; Esther Roorda; Daniel Holanda Noronha; Steven J. E. Wilton",
    "corresponding_authors": "",
    "abstract": "In this article, we present Syncopation , a performance-boosting fine-grained timing analysis and adaptive clock management technique for High-Level Synthesis-generated circuits implemented on Field-Programmable Gate Arrays. The key idea is to use the HLS scheduling information along with the placement and routing results to determine the worst-case timing path for individual clock cycles. By adjusting the clock period on a cycle-by-cycle basis, we can increase performance of an HLS-generated circuit. Our experiments show that Syncopation improves performance by 3.2% (geomean) across all benchmarks (up to 47%). In addition, by employing targeted synthesis techniques along with Syncopation, we can achieve 10.3% performance improvement (geomean) across all benchmarks (up to 50%). Syncopation instrumentation is implemented entirely in soft logic without requiring alterations to the HLS-synthesis toolchain or changes to the FPGA, and has been validated on real hardware.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4281682568",
    "type": "article"
  },
  {
    "title": "Hardware-accelerated Real-time Drift-awareness for Robust Deep Learning on Wireless RF Data",
    "doi": "https://doi.org/10.1145/3563394",
    "publication_date": "2022-09-12",
    "publication_year": 2022,
    "authors": "Chanaka Ganewattha; Zaheer Khan; Janne Lehtomäki; Matti Latva‐aho",
    "corresponding_authors": "",
    "abstract": "Proactive and intelligent management of network resource utilization (RU) using deep learning (DL) can significantly improve the efficiency and performance of the next generation of wireless networks. However, variations in wireless RU are often affected by uncertain events and change points due to the deviations of real data distribution from that of the original training data. Such deviations, which are known as dataset drifts, can subsequently lead to a shift in the corresponding decision boundary degrading the DL model prediction performance. To address these challenges, we present hardware-accelerated real-time radio frequency (RF) analytics and drift-awareness modules for robust DL predictions. We have prototyped the proposed design on a Zynq-7000 System-on-Chip that contains an FPGA and an embedded ARM processor. We have used Xilinx Vivado design suite for synthesis and analysis of the HDL design for the proposed solution. To detect dataset drifts, the proposed solution adopts a distance-based technique on FPGA to quantify in real-time the change between the prediction distribution obtained from DL predictions and data distribution of input streaming samples. Using various performance metrics, we have extensively evaluated the performance of the proposed solution and shown that it can significantly improve the DL model robustness in the presence of dataset drifts.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4295292882",
    "type": "article"
  },
  {
    "title": "Introduction to Special Section on FPGA 2021",
    "doi": "https://doi.org/10.1145/3536335",
    "publication_date": "2022-12-09",
    "publication_year": 2022,
    "authors": "Wai Yie Leong",
    "corresponding_authors": "Wai Yie Leong",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4311261140",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Section on FPL 2020",
    "doi": "https://doi.org/10.1145/3536336",
    "publication_date": "2022-12-14",
    "publication_year": 2022,
    "authors": "Nele Mentens; Lionel Sousa; Pedro Trancoso",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4311390781",
    "type": "article"
  },
  {
    "title": "Practical Model Checking on FPGAs",
    "doi": "https://doi.org/10.1145/3448272",
    "publication_date": "2021-06-30",
    "publication_year": 2021,
    "authors": "Shenghsun Cho; Mrunal Patel; Michael Ferdman; Peter Milder",
    "corresponding_authors": "",
    "abstract": "Software verification is an important stage of the software development process, particularly for mission-critical systems. As the traditional methodology of using unit tests falls short of verifying complex software, developers are increasingly relying on formal verification methods, such as explicit state model checking, to automatically verify that the software functions properly. However, due to the ever-increasing complexity of software designs, model checking cannot be performed in a reasonable amount of time when running on general-purpose cores, leading to the exploration of hardware-accelerated model checking. FPGAs have been demonstrated to be promising verification accelerators, exhibiting nearly three orders of magnitude speedup over software. Unfortunately, the “FPGA programmability wall,” particularly the long synthesis and place-and-route times, block the general adoption of FPGAs for model checking. To address this problem, we designed a runtime-programmable pipeline specifically for model checkers on FPGAs to minimize the “preparation time” before a model can be checked. Our design of the successor state generator and the state validator modules enables FPGA-acceleration of model checking without incurring the time-consuming FPGA implementation stages, reducing the preparation time before checking a model from hours to less than a minute, while incurring only a 26% execution time overhead compared to model-specific implementations.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3182395188",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Section on FPL 2019",
    "doi": "https://doi.org/10.1145/3459587",
    "publication_date": "2021-06-30",
    "publication_year": 2021,
    "authors": "Xavier Martorell; Carlos Álvarez; Christos-Savvas Bouganis; Ioannis Sourdis",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3189606950",
    "type": "article"
  },
  {
    "title": "Rethinking Embedded Blocks for Machine Learning Applications",
    "doi": "https://doi.org/10.1145/3491234",
    "publication_date": "2021-11-30",
    "publication_year": 2021,
    "authors": "S A Rasoulinezhad; Esther Roorda; Steven J. E. Wilton; Philip H. W. Leong; David Boland",
    "corresponding_authors": "",
    "abstract": "The underlying goal of FPGA architecture research is to devise flexible substrates that implement a wide variety of circuits efficiently. Contemporary FPGA architectures have been optimized to support networking, signal processing, and image processing applications through high-precision digital signal processing (DSP) blocks. The recent emergence of machine learning has created a new set of demands characterized by: (1) higher computational density and (2) low precision arithmetic requirements. With the goal of exploring this new design space in a methodical manner, we first propose a problem formulation involving computing nested loops over multiply-accumulate (MAC) operations, which covers many basic linear algebra primitives and standard deep neural network (DNN) kernels. A quantitative methodology for deriving efficient coarse-grained compute block architectures from benchmarks is then proposed together with a family of new embedded blocks, called MLBlocks. An MLBlock instance includes several multiply-accumulate units connected via a flexible routing, where each configuration performs a few parallel dot-products in a systolic array fashion. This architecture is parameterized with support for different data movements, reuse, and precisions, utilizing a columnar arrangement that is compatible with existing FPGA architectures. On synthetic benchmarks, we demonstrate that for 8-bit arithmetic, MLBlocks offer 6× improved performance over the commercial Xilinx DSP48E2 architecture with smaller area and delay; and for time-multiplexed 16-bit arithmetic, achieves 2× higher performance per area with the same area and frequency. All source codes and data, along with documents to reproduce all the results in this article, are available at http://github.com/raminrasoulinezhad/MLBlocks .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3217045338",
    "type": "article"
  },
  {
    "title": "Introduction to Special Section on FPGA 2020",
    "doi": "https://doi.org/10.1145/3485586",
    "publication_date": "2021-11-30",
    "publication_year": 2021,
    "authors": "Lesley Shannon",
    "corresponding_authors": "Lesley Shannon",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3217766295",
    "type": "article"
  }
]