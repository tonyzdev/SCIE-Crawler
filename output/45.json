[
  {
    "title": "Mersenne twister",
    "doi": "https://doi.org/10.1145/272991.272995",
    "publication_date": "1998-01-01",
    "publication_year": 1998,
    "authors": "Makoto Matsumoto; Takuji Nishimura",
    "corresponding_authors": "",
    "abstract": "A new algorithm called Mersenne Twister (MT) is proposed for generating uniform pseudorandom numbers. For a particular choice of parameters, the algorithm provides a super astronomical period of 2 19937 −1 and 623-dimensional equidistribution up to 32-bit accuracy, while using a working area of only 624 words. This is a new variant of the previously proposed generators, TGFSR, modified so as to admit a Mersenne-prime period. The characteristic polynomial has many terms. The distribution up to v bits accuracy for 1 ≤ v ≤ 32 is also shown to be good. An algorithm is also given that checks the primitivity of the characteristic polynomial of MT with computational complexity O(p 2 ) where p is the degree of the polynomial. We implemented this generator in portable C-code. It passed several stringent statistical tests, including diehard. Its speed is comparable to other modern generators. Its merits are due to the efficient algorithms that are unique to polynomial calculations over the two-element field.",
    "cited_by_count": 5641,
    "openalex_id": "https://openalex.org/W2095595785",
    "type": "article"
  },
  {
    "title": "Simulation modeling for analysis",
    "doi": "https://doi.org/10.1145/1667072.1667074",
    "publication_date": "2010-01-01",
    "publication_year": 2010,
    "authors": "Lee W. Schruben",
    "corresponding_authors": "Lee W. Schruben",
    "abstract": "This article explores possibilities for designing and executing simulation models with specific analysis goals in mind, and shows that a tight coupling of the modeling and analysis phases in a simulation project can lead to dramatic improvements in the study results. Suggestions are made for how simulation analysis, considered in the explicit context of discrete-event simulation models, can create new opportunities for meaningful research and more efficient modeling. Modeling decisions can play a significant role in the performance of analytical procedures. How a simulation model is designed can enable, inhibit, or even invalidate analytical procedures and methodology research results.",
    "cited_by_count": 913,
    "openalex_id": "https://openalex.org/W1992870153",
    "type": "article"
  },
  {
    "title": "Experiences creating three implementations of the repast agent modeling toolkit",
    "doi": "https://doi.org/10.1145/1122012.1122013",
    "publication_date": "2006-01-01",
    "publication_year": 2006,
    "authors": "Michael North; Nicholson Collier; J. R. Vos",
    "corresponding_authors": "",
    "abstract": "Many agent-based modeling and simulation researchers and practitioners have called for varying levels of simulation interoperability ranging from shared software architectures to common agent communications languages. These calls have been at least partially answered by several specifications and technologies. In fact, Tanenbaum [1988] has remarked that the “nice thing about standards is that there are so many to choose from.” Tanenbaum goes on to say that “if you do not like any of them, you can just wait for next year's model.” This article does not seek to introduce next year's model. Rather, the goal is to contribute to the larger simulation community the authors' accumulated experiences from developing several implementations of an agent-based simulation toolkit. As such, this article focuses on the implementation of simulation architectures rather than agent communications languages. It is hoped that ongoing architecture standards efforts will benefit from this new knowledge and use it to produce architecture standards with increased robustness.",
    "cited_by_count": 698,
    "openalex_id": "https://openalex.org/W2077563974",
    "type": "article"
  },
  {
    "title": "Fast simulation of rare events in queueing and reliability models",
    "doi": "https://doi.org/10.1145/203091.203094",
    "publication_date": "1995-01-01",
    "publication_year": 1995,
    "authors": "Philip Heidelberger",
    "corresponding_authors": "Philip Heidelberger",
    "abstract": "This paper surveys efficient techniques for estimating, via simulation, the probabilities of certain rare events in queueing and reliability models. The rare events of interest are long waiting times or buffer overflows in queueing systems, and system failure events in reliability models of highly dependable computing systems. The general approach to speeding up such simulations is to accelerate the occurrence of the rare events by using importance sampling. In importance sampling, the system is simulated using a new set of input probability distributions, and unbiased estimates are recovered by multiplying the simulation output by a likelihood ratio. Our focus is on describing asymptotically optimal importance sampling techniques. Using asymptotically optimal importance sampling, the number of samples required to get accurate estimates grows slowly compared to the rate at which the probability of the rare event approaches zero. In practice, this means that run lengths can be reduced by many orders of magnitude, compared to standard simulation. In certain cases, asymptotically optimal importance sampling results in estimates having bounded relative error. With bounded relative error, only a fixed number of samples are required to get accurate estimates, no matter how rare the event of interest is. The queueing systems studied include simple queues (e.g., GI/GI/1), Jackson networks, discrete time queues with multiple autocorrelated arrival processes that arise in the analysis of Asynchronous Transfer Mode communications switches, and tree structured networks of such switches. Both Markovian and non-Markovian reliability models are treated.",
    "cited_by_count": 571,
    "openalex_id": "https://openalex.org/W2146416787",
    "type": "article"
  },
  {
    "title": "A fully sequential procedure for indifference-zone selection in simulation",
    "doi": "https://doi.org/10.1145/502109.502111",
    "publication_date": "2001-07-01",
    "publication_year": 2001,
    "authors": "Seong‐Hee Kim; Barry L. Nelson",
    "corresponding_authors": "",
    "abstract": "We present procedures for selecting the best or near-best of a finite number of simulated systems when best is defined by maximum or minimum expected performance. The procedures are appropriate when it is possible to repeatedly obtain small, incremental samples from each simulated system. The goal of such a sequential procedure is to eliminate, at an early stage of experimentation, those simulated systems that are apparently inferior, and thereby reduce the overall computational effort required to find the best. The procedures we present accommodate unequal variances across systems and the use of common random numbers. However, they are based on the assumption of normally distributed data, so we analyze the impact of batching (to achieve approximate normality or independence) on the performance of the procedures. Comparisons with some existing indifference-zone procedures are also provided.",
    "cited_by_count": 461,
    "openalex_id": "https://openalex.org/W2037728958",
    "type": "article"
  },
  {
    "title": "A Survey of Statistical Model Checking",
    "doi": "https://doi.org/10.1145/3158668",
    "publication_date": "2018-01-31",
    "publication_year": 2018,
    "authors": "Gul Agha; Karl Palmskog",
    "corresponding_authors": "",
    "abstract": "Interactive, distributed, and embedded systems often behave stochastically, for example, when inputs, message delays, or failures conform to a probability distribution. However, reasoning analytically about the behavior of complex stochastic systems is generally infeasible. While simulations of systems are commonly used in engineering practice, they have not traditionally been used to reason about formal specifications. Statistical model checking (SMC) addresses this weakness by using a simulation-based approach to reason about precise properties specified in a stochastic temporal logic. A specification for a communication system may state that within some time bound, the probability that the number of messages in a queue will be greater than 5 must be less than 0.01. Using SMC, executions of a stochastic system are first sampled, after which statistical techniques are applied to determine whether such a property holds. While the output of sample-based methods are not always correct, statistical inference can quantify the confidence in the result produced. In effect, SMC provides a more widely applicable and scalable alternative to analysis of properties of stochastic systems using numerical and symbolic methods. SMC techniques have been successfully applied to analyze systems with large state spaces in areas such as computer networking, security, and systems biology. In this article, we survey SMC algorithms, techniques, and tools, while emphasizing current limitations and tradeoffs between precision and scalability.",
    "cited_by_count": 275,
    "openalex_id": "https://openalex.org/W2787596386",
    "type": "article"
  },
  {
    "title": "Cycle-Accurate Network on Chip Simulation with Noxim",
    "doi": "https://doi.org/10.1145/2953878",
    "publication_date": "2016-08-16",
    "publication_year": 2016,
    "authors": "Vincenzo Catania; Andrea Mineo; Salvatore Monteleone; Maurizio Palesi; Davide Patti",
    "corresponding_authors": "",
    "abstract": "The on-chip communication in current Chip-MultiProcessors (CMP) and MultiProcessor-SoC (MPSoC) is mainly based on the Network-on-Chip (NoC) design paradigm. Unfortunately, it is foreseen that conventional NoC architectures cannot sustain the performance, power, and reliability requirements demanded by the next generation of manycore architectures. Recently, emerging on-chip communication technologies, like wireless Networks-on-Chip (WiNoCs), have been proposed as candidate solutions for addressing the scalability limitations of conventional multi-hop NoC architectures. In a WiNoC, a subset of network nodes are equipped with a wireless interface which allows them long-range communication in a single hop. Assessing the performance and power figures of NoC and WiNoC architectures requires the availability of simulation tools that are often limited on modeling specific network configurations. This article presents Noxim, an open, configurable, extendible, cycle-accurate NoC simulator developed in SystemC, which allows to analyze the performance and power figures of both conventional wired NoC and emerging WiNoC architectures.",
    "cited_by_count": 208,
    "openalex_id": "https://openalex.org/W2509453741",
    "type": "article"
  },
  {
    "title": "Using the SimOS machine simulator to study complex computer systems",
    "doi": "https://doi.org/10.1145/244804.244807",
    "publication_date": "1997-01-01",
    "publication_year": 1997,
    "authors": "Mendel Rosenblum; Edouard Bugnion; Scott Devine; Stephen Alan Herrod",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Using the SimOS machine simulator to study complex computer systems Authors: Mendel Rosenblum Stanford Univ., Stanford, CA Stanford Univ., Stanford, CAView Profile , Edouard Bugnion Stanford Univ., Stanford, CA Stanford Univ., Stanford, CAView Profile , Scott Devine Stanford Univ., Stanford, CA Stanford Univ., Stanford, CAView Profile , Stephen A. Herrod Stanford Univ., Stanford, CA Stanford Univ., Stanford, CAView Profile Authors Info & Claims ACM Transactions on Modeling and Computer SimulationVolume 7Issue 1Jan. 1997 pp 78–103https://doi.org/10.1145/244804.244807Online:01 January 1997Publication History 257citation1,415DownloadsMetricsTotal Citations257Total Downloads1,415Last 12 Months46Last 6 weeks15 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 356,
    "openalex_id": "https://openalex.org/W2011992756",
    "type": "article"
  },
  {
    "title": "Implementation and tests of low-discrepancy sequences",
    "doi": "https://doi.org/10.1145/146382.146385",
    "publication_date": "1992-07-01",
    "publication_year": 1992,
    "authors": "Paul Bratley; Bennett L. Fox; Harald Niederreiter",
    "corresponding_authors": "",
    "abstract": "Low-discrepancy sequences are used for numerical integration, in simulation, and in related applications. Techniques for producing such sequences have been proposed by, among others, Halton, Sobol´, Faure, and Niederreiter. Niederreiter's sequences have the best theoretical asymptotic properties. The paper describes two ways to implement the latter sequences on a computer and discusses the results obtained in various practical tests on particular integrals.",
    "cited_by_count": 268,
    "openalex_id": "https://openalex.org/W2163417850",
    "type": "article"
  },
  {
    "title": "Latin supercube sampling for very high-dimensional simulations",
    "doi": "https://doi.org/10.1145/272991.273010",
    "publication_date": "1998-01-01",
    "publication_year": 1998,
    "authors": "Art B. Owen",
    "corresponding_authors": "Art B. Owen",
    "abstract": "This article introduces Latin supercube sampling (LSS) for very high-dimensional simulations such as arise in particle transport, finance, and queueing. LSS is developed as a combination of two widely used methods: Latin hypercube sampling (LHS) and quasi-Monte Carlo (QMC). In LSS, the input variables are grouped into subsets, and a lower-dimensional QMC method is used within each subset. The QMC points are presented in random order within subsets. QMC methods have been observed to lose effectiveness in high-dimensional problems. This article shows that LSS can extend the benefits of QMC to much higher dimensions, when one can make a good grouping of input variables. Some suggestions for grouping variables are given for the motivating examples. Even a poor grouping can still be expected to do as well as LHS. The article also extends LHS and LSS to infinite-dimensional problems. The paper includes a survey of QMC methods, randomized versions of them (RQMC), and previous methods for extending QMC to higher dimensions. Furthermore it shows that LSS applied with RQMC is more reliable than LSS with QMC.",
    "cited_by_count": 254,
    "openalex_id": "https://openalex.org/W2030999551",
    "type": "article"
  },
  {
    "title": "Efficient optimistic parallel simulations using reverse computation",
    "doi": "https://doi.org/10.1145/347823.347828",
    "publication_date": "1999-07-01",
    "publication_year": 1999,
    "authors": "Christopher D. Carothers; Kalyan S. Perumalla; Richard M. Fujimoto",
    "corresponding_authors": "",
    "abstract": "In optimistic parallel simulations, state-saving techniques have traditionally been used to realize rollback. In this article, we propose reverse computation as an alternative approach, and compare its execution performance against that of state-saving. Using compiler techniques, we describe an approach to automatically generate reversible computations, and to optimize them to reap the performance benefits of reverse computation transparently. For certain fine-grain models, such as queuing network models, we show that reverse computation can yield significant improvement in execution speed coupled with significant reduction in memory utilization, as compared to traditional state-saving. On sample models using reverse computation, we observe as much as a six-fold improvement in execution speed over traditional state-saving.",
    "cited_by_count": 221,
    "openalex_id": "https://openalex.org/W1976533090",
    "type": "article"
  },
  {
    "title": "Rearchitecting the UML infrastructure",
    "doi": "https://doi.org/10.1145/643120.643123",
    "publication_date": "2002-10-01",
    "publication_year": 2002,
    "authors": "Colin Atkinson; Thomas Kühne",
    "corresponding_authors": "",
    "abstract": "Metamodeling is one of the core foundations of computer-automated multiparadigm modeling. However, there is currently little agreement about what form the required metamodeling approach should take and precisely what role metamodels should play. This article addresses the problem by first describing some fundamental problems in the industry's leading metamodeling technology, the UML framework, and then explaining how this framework could be rearchitected to overcome these problems. Three main issues are identified in the current framework: the dual classification problem arising from the need to capture both the logical and physical classification of model elements, the class/object duality problem arising from the need to capture both the classlike and objectlike facets of some model elements, and the replication of concepts problem arising from the need to define certain concepts multiple times. Three main proposals for rearchitecting the UML framework to overcome these problems are then presented: the separation of logical and physical classification dimensions, the unification of the class and object facets of model elements, and the enhancement of the instantiation mechanism to allow definitions to transcend multiple levels. The article concludes with a discussion of other practical issues involved in rearchitecting the UML modeling framework in the proposed way.",
    "cited_by_count": 203,
    "openalex_id": "https://openalex.org/W1967791174",
    "type": "article"
  },
  {
    "title": "Modeling formalisms for dynamic structure systems",
    "doi": "https://doi.org/10.1145/268403.268423",
    "publication_date": "1997-10-01",
    "publication_year": 1997,
    "authors": "Fernando J. Barros",
    "corresponding_authors": "Fernando J. Barros",
    "abstract": "We present a new concept for a system network to represent systems that are able to undergo structural change. Change in structure is defined in general terms, and includes the addition and deletion of systems and the modification of the relations among components. The structure of a system network is stored in the network executive. Any change in structure-related information is mapped into modifications in the network structure. Based on these concepts, we derive three new system specifications that provide a shorthand notation to specify classes of dynamic structure systems. These new formalisms are: dynamic structure discrete time system, dynamic structure differential equation specified systems, and dynamic structure discrete event system specification. We demonstrate that these formalisms are closed under coupling, making hierarchical model construction possible. formalisms are described using set theoretic notation and general systems theory concepts.",
    "cited_by_count": 189,
    "openalex_id": "https://openalex.org/W2003404630",
    "type": "article"
  },
  {
    "title": "Discrete-event simulation optimization using ranking, selection, and multiple comparison procedures",
    "doi": "https://doi.org/10.1145/858481.858484",
    "publication_date": "2003-04-01",
    "publication_year": 2003,
    "authors": "James R. Swisher; Sheldon H. Jacobson; Enver Yücesan",
    "corresponding_authors": "",
    "abstract": "An important use for discrete-event simulation models lies in comparing and contrasting competing design alternatives without incurring any physical costs. This article presents a survey of the literature for two widely used classes of statistical methods for selecting the best design from among a finite set of k alternatives: ranking and selection (R&amp;S) and multiple comparison procedures (MCPs). A comprehensive survey of each topic is presented along with a summary of recent unified R&amp;S-MCP approaches. Procedures are recommended based on their statistical efficiency and ease of application; guidelines for procedure application are offered.",
    "cited_by_count": 187,
    "openalex_id": "https://openalex.org/W1964187450",
    "type": "article"
  },
  {
    "title": "Twisted GFSR generators II",
    "doi": "https://doi.org/10.1145/189443.189445",
    "publication_date": "1994-07-01",
    "publication_year": 1994,
    "authors": "Makoto Matsumoto; Yoshiharu Kurita",
    "corresponding_authors": "",
    "abstract": "The twisted GFSR generators proposed in a previous article have a defect in k -distribution for k larger than the order of recurrence. In this follow up article, we introduce and analyze a new TGFSR variant having better k -distribution property. We provide an efficient algorithm to obtain the order of equidistribution, together with a tight upper bound on the order. We discuss a method to search for generators attaining this bound, and we list some of these such generators. The upper bound turns out to be (sometimes far) less than the maximum order of equidistribution for a generator of that period length, but far more than that for a GFSR with a working are of the same size.",
    "cited_by_count": 179,
    "openalex_id": "https://openalex.org/W2111433785",
    "type": "article"
  },
  {
    "title": "Crowd modeling and simulation technologies",
    "doi": "https://doi.org/10.1145/1842722.1842725",
    "publication_date": "2010-10-01",
    "publication_year": 2010,
    "authors": "Suiping Zhou; Dan Chen; Wentong Cai; Linbo Luo; Malcolm Yoke Hean Low; Feng Tian; Victor Tay; Darren Wee Sze Ong; Benjamin D. Hamilton",
    "corresponding_authors": "",
    "abstract": "As a collective and highly dynamic social group, the human crowd is a fascinating phenomenon that has been frequently studied by experts from various areas. Recently, computer-based modeling and simulation technologies have emerged to support investigation of the dynamics of crowds, such as a crowd's behaviors under normal and emergent situations. This article assesses the major existing technologies for crowd modeling and simulation. We first propose a two-dimensional categorization mechanism to classify existing work depending on the size of crowds and the time-scale of the crowd phenomena of interest. Four evaluation criteria have also been introduced to evaluate existing crowd simulation systems from the point of view of both a modeler and an end-user. We have discussed some influential existing work in crowd modeling and simulation regarding their major features, performance as well as the technologies used in this work. We have also discussed some open problems in the area. This article will provide the researchers with useful information and insights on the state of the art of the technologies in crowd modeling and simulation as well as future research directions.",
    "cited_by_count": 178,
    "openalex_id": "https://openalex.org/W2028545909",
    "type": "article"
  },
  {
    "title": "Twisted GFSR generators",
    "doi": "https://doi.org/10.1145/146382.146383",
    "publication_date": "1992-07-01",
    "publication_year": 1992,
    "authors": "Makoto Matsumoto; Yoshiharu Kurita",
    "corresponding_authors": "",
    "abstract": "The generalized feed back shift register (GFSR) algorithm suggested by Lewis and Payne is a widely used pseudorandom number generator, but has the following serious drawbacks: (1) an initialization scheme to assure higher order equidistribution is involved and is time consuming; (2) each bit of the generated words constitutes an m -sequence based on a primitive trinomials, which shows poor randomness with respect to weight distribution; (3) a large working area is necessary; (4) the period of sequence is far shorter than the theoretical upper bound. This paper presents the twisted GFSR (TGFSR) algorithm, a slightly but essentially modified version of the GFSR, which solves all the above problems without loss of merit. Some practical TGFSR generators were implemented and passed strict empirical tests. These new generators are most suitable for simulation of a large distributive system, which requires a number of mutually independent pseudorandom number generators with compact size.",
    "cited_by_count": 178,
    "openalex_id": "https://openalex.org/W2133988913",
    "type": "article"
  },
  {
    "title": "Industrial strength COMPASS",
    "doi": "https://doi.org/10.1145/1667072.1667075",
    "publication_date": "2010-01-01",
    "publication_year": 2010,
    "authors": "Jie Xu; Barry L. Nelson; L. Jeff Hong",
    "corresponding_authors": "",
    "abstract": "Industrial Strength COMPASS (ISC) is a particular implementation of a general framework for optimizing the expected value of a performance measure of a stochastic simulation with respect to integer-ordered decision variables in a finite (but typically large) feasible region defined by linear-integer constraints. The framework consists of a global-search phase, followed by a local-search phase, and ending with a “clean-up” (selection of the best) phase. Each phase provides a probability 1 convergence guarantee as the simulation effort increases without bound: Convergence to a globally optimal solution in the global-search phase; convergence to a locally optimal solution in the local-search phase; and convergence to the best of a small number of good solutions in the clean-up phase. In practice, ISC stops short of such convergence by applying an improvement-based transition rule from the global phase to the local phase; a statistical test of convergence from the local phase to the clean-up phase; and a ranking-and-selection procedure to terminate the clean-up phase. Small-sample validity of the statistical test and ranking-and-selection procedure is proven for normally distributed data. ISC is compared to the commercial optimization via simulation package OptQuest on five test problems that range from 2 to 20 decision variables and on the order of 10 4 to 10 20 feasible solutions. These test cases represent response-surface models with known properties and realistic system simulation problems.",
    "cited_by_count": 172,
    "openalex_id": "https://openalex.org/W2148121522",
    "type": "article"
  },
  {
    "title": "A Distributed Platform for Global-Scale Agent-Based Models of Disease Transmission",
    "doi": "https://doi.org/10.1145/2043635.2043637",
    "publication_date": "2011-12-01",
    "publication_year": 2011,
    "authors": "Jon Parker; Joshua M. Epstein",
    "corresponding_authors": "",
    "abstract": "The Global-Scale Agent Model (GSAM) is presented. The GSAM is a high-performance distributed platform for agent-based epidemic modeling capable of simulating a disease outbreak in a population of several billion agents. It is unprecedented in its scale, its speed, and its use of Java. Solutions to multiple challenges inherent in distributing massive agent-based models are presented. Communication, synchronization, and memory usage are among the topics covered in detail. The memory usage discussion is Java specific. However, the communication and synchronization discussions apply broadly. We provide benchmarks illustrating the GSAM’s speed and scalability.",
    "cited_by_count": 164,
    "openalex_id": "https://openalex.org/W2092092574",
    "type": "article"
  },
  {
    "title": "Massive Parallelization of Serial Inference Algorithms for a Complex Generalized Linear Model",
    "doi": "https://doi.org/10.1145/2414416.2414791",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Marc A. Suchard; Shawn Simpson; Ivan Zorych; Patrick Ryan; David Madigan",
    "corresponding_authors": "",
    "abstract": "Following a series of high-profile drug safety disasters in recent years, many countries are redoubling their efforts to ensure the safety of licensed medical products. Large-scale observational databases such as claims databases or electronic health record systems are attracting particular attention in this regard, but present significant methodological and computational concerns. In this article we show how high-performance statistical computation, including graphics processing units, relatively inexpensive highly parallel computing devices, can enable complex methods in large databases. We focus on optimization and massive parallelization of cyclic coordinate descent approaches to fit a conditioned generalized linear model involving tens of millions of observations and thousands of predictors in a Bayesian context. We find orders-of-magnitude improvement in overall run-time. Coordinate descent approaches are ubiquitous in high-dimensional statistics and the algorithms we propose open up exciting new methodological possibilities with the potential to significantly improve drug safety.",
    "cited_by_count": 130,
    "openalex_id": "https://openalex.org/W2104640308",
    "type": "article"
  },
  {
    "title": "Better estimation of small sobol' sensitivity indices",
    "doi": "https://doi.org/10.1145/2457459.2457460",
    "publication_date": "2013-05-01",
    "publication_year": 2013,
    "authors": "Art B. Owen",
    "corresponding_authors": "Art B. Owen",
    "abstract": "A new method for estimating Sobol' indices is proposed. The new method makes use of 3 independent input vectors rather than the usual 2. It attains much greater accuracy on problems where the target Sobol' index is small, even outperforming some oracles that adjust using the true but unknown mean of the function. The new estimator attains a better rate of convergence than the old one in a small effects limit. When the target Sobol' index is quite large, the oracles do better than the new method.",
    "cited_by_count": 122,
    "openalex_id": "https://openalex.org/W1980609382",
    "type": "article"
  },
  {
    "title": "Stochastically Constrained Ranking and Selection via SCORE",
    "doi": "https://doi.org/10.1145/2630066",
    "publication_date": "2014-08-13",
    "publication_year": 2014,
    "authors": "Raghu Pasupathy; Susan R. Hunter; Nugroho A. Pujowidianto; Loo Hay Lee; Chun‐Hung Chen",
    "corresponding_authors": "",
    "abstract": "Consider the context of constrained Simulation Optimization (SO); that is, optimization problems where the objective and constraint functions are known through dependent Monte Carlo estimators. For solving such problems on large finite spaces , we provide an easily implemented sampling framework called SCORE (Sampling Criteria for Optimization using Rate Estimators) that approximates the optimal simulation budget allocation. We develop a general theory, but, like much of the existing literature on ranking and selection, our focus is on SO problems where the distribution of the simulation observations is Gaussian. We first characterize the nature of the optimal simulation budget as a bi-level optimization problem. We then show that under a certain asymptotic limit, the solution to the bi-level optimization problem becomes surprisingly tractable and is expressed through a single intuitive measure, the score . We provide an iterative SO algorithm that repeatedly estimates the score and determines how the available simulation budget should be expended across contending systems. Numerical experience with the algorithm resulting from the proposed sampling approximation is very encouraging—in numerous examples of constrained SO problems having 1,000 to 10,000 systems, the optimal allocation is identified to negligible error within a few seconds to 1 minute on a typical laptop computer. Corresponding times to solve the full bi-level optimization problem range from tens of minutes to several hours.",
    "cited_by_count": 112,
    "openalex_id": "https://openalex.org/W2033413738",
    "type": "article"
  },
  {
    "title": "Research Challenges in Parallel and Distributed Simulation",
    "doi": "https://doi.org/10.1145/2866577",
    "publication_date": "2016-05-02",
    "publication_year": 2016,
    "authors": "Richard M. Fujimoto",
    "corresponding_authors": "Richard M. Fujimoto",
    "abstract": "The parallel and distributed simulation field has evolved and grown from its origins in the 1970s and 1980s and remains an active field of research to this day. A brief overview of research in the field is presented. Future research topics are explored including areas such as problem-driven simulation of large-scale systems and complex networks, exploitation of graphical processing unit hardware and cloud computing environments, predictive online simulation for system management and optimization, power and energy consumption in mobile platforms and data centers, and composition of heterogeneous simulations.",
    "cited_by_count": 98,
    "openalex_id": "https://openalex.org/W2345709169",
    "type": "article"
  },
  {
    "title": "Engineering Resilient Collective Adaptive Systems by Self-Stabilisation",
    "doi": "https://doi.org/10.1145/3177774",
    "publication_date": "2018-03-09",
    "publication_year": 2018,
    "authors": "Mirko Viroli; Giorgio Audrito; Jacob Beal; Ferruccio Damiani; Danilo Pianini",
    "corresponding_authors": "",
    "abstract": "Collective adaptive systems are an emerging class of networked computational systems particularly suited for application domains such as smart cities, complex sensor networks, and the Internet of Things. These systems tend to feature large-scale, heterogeneity of communication model (including opportunistic peer-to-peer wireless interaction) and require inherent self-adaptiveness properties to address unforeseen changes in operating conditions. In this context, it is extremely difficult (if not seemingly intractable) to engineer reusable pieces of distributed behaviour to make them provably correct and smoothly composable. Building on the field calculus, a computational model (and associated toolchain) capturing the notion of aggregate network-level computation, we address this problem with an engineering methodology coupling formal theory and computer simulation. On the one hand, functional properties are addressed by identifying the largest-to-date field calculus fragment generating self-stabilising behaviour, guaranteed to eventually attain a correct and stable final state despite any transient perturbation in state or topology and including highly reusable building blocks for information spreading, aggregation, and time evolution. On the other hand, dynamical properties are addressed by simulation, empirically evaluating the different performances that can be obtained by switching between implementations of building blocks with provably equivalent functional properties. Overall, our methodology sheds light on how to identify core building blocks of collective behaviour and how to select implementations that improve system performance while leaving overall system function and resiliency properties unchanged.",
    "cited_by_count": 97,
    "openalex_id": "https://openalex.org/W2962760239",
    "type": "article"
  },
  {
    "title": "Guest editorial",
    "doi": "https://doi.org/10.1145/858481.858482",
    "publication_date": "2003-04-01",
    "publication_year": 2003,
    "authors": "Michael C. Fu; Barry L. Nelson",
    "corresponding_authors": "",
    "abstract": "editorial Free Access Share on Guest editorial Editors: Michael Fu University of Maryland, College Park, MD University of Maryland, College Park, MDView Profile , Barry Nelson Northwestern University, Evanston, IL Northwestern University, Evanston, ILView Profile Authors Info & Claims ACM Transactions on Modeling and Computer SimulationVolume 13Issue 2April 2003 pp 105–107https://doi.org/10.1145/858481.858482Published:01 April 2003Publication History 6citation586DownloadsMetricsTotal Citations6Total Downloads586Last 12 Months18Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 180,
    "openalex_id": "https://openalex.org/W2732186277",
    "type": "editorial"
  },
  {
    "title": "Modeling, simulation, sensitivity analysis, and optimization of hybrid systems",
    "doi": "https://doi.org/10.1145/643120.643122",
    "publication_date": "2002-10-01",
    "publication_year": 2002,
    "authors": "Paul I. Barton; Cha Kun Lee",
    "corresponding_authors": "",
    "abstract": "Hybrid (discrete/continuous) systems exhibit both discrete state and continuous state dynamics which interact to such a significant extent that they cannot be decoupled and must be analyzed simultaneously. We present an overview of the work that has been done in the modeling, simulation, sensitivity analysis, and optimization of hybrid systems, paying particular attention to the interaction between discrete and continuous dynamics. A concise intuitive framework for hybrid system modeling is presented, together with discussions on robust state event location, transfer functions of the continuous state at discontinuities, parametric sensitivity analysis of hybrid systems, and challenges in optimization.",
    "cited_by_count": 166,
    "openalex_id": "https://openalex.org/W2094731495",
    "type": "article"
  },
  {
    "title": "Analysis of an importance sampling estimator for tandem queues",
    "doi": "https://doi.org/10.1145/203091.203093",
    "publication_date": "1995-01-01",
    "publication_year": 1995,
    "authors": "Paul Glasserman; Shing-Gang Kou",
    "corresponding_authors": "",
    "abstract": "We analyze the performance of an importance sampling estimator for a rare-event probability in tandem Jackson networks. The rare event we consider corresponds to the network population reaching K before returning to ø, starting from ø, with K large. The estimator we study is based on interchanging the arrival rate and the smallest service rate and is therefore a generalization of the asymptotically optimal estimator for an M/M/1 queue. We examine its asymptotic performance for large K , showing that in certain parameter regions the estimator has an asymptotic efficiency property, but that in other regions it does not. The setting we consider is perhaps the simplest case of a rare-event simulation problem in which boundaries on the state space play a significant role.",
    "cited_by_count": 150,
    "openalex_id": "https://openalex.org/W2162407085",
    "type": "article"
  },
  {
    "title": "State event location in differential-algebraic models",
    "doi": "https://doi.org/10.1145/232807.232809",
    "publication_date": "1996-04-01",
    "publication_year": 1996,
    "authors": "Taeshin Park; Paul I. Barton",
    "corresponding_authors": "",
    "abstract": "An efficient discontinuity handling algorithm for initial value problems in differential-algebraic equations is presented. The algorithm supports flexible representation of state conditions in propositional logic, and guarantees the location of all state events in strict time order. The algorithm consists of two phases:(1) event detection and(2) consistent event location. In the event detection phase, the entire integration step is searched for the state event by solving the interpolation polynomials for the discontinuity functions generated by the BDF method. An efficient hierarchical polynomial root-finding procedure based upon interval arithmetic guarantees detection of the state event even if multiple state condition transitions exist in an integration step, in which case many existing algorithms may fail. As a second phase of the algorithm, a consistent even location calculation is developed that accurately locates the state event detected earlier while completely eliminating incorrect reactivation of the same state event immediately after the consistent initialization calculation that may follow. This numerical phenomenon has not been explained before and is termed discontinuity sticking . Results from various test problems are presented to demonstrate the correctness and efficiency of the algorithm.",
    "cited_by_count": 147,
    "openalex_id": "https://openalex.org/W2031130394",
    "type": "article"
  },
  {
    "title": "A multimodel methodology for qualitative model engineering",
    "doi": "https://doi.org/10.1145/132277.132280",
    "publication_date": "1992-01-02",
    "publication_year": 1992,
    "authors": "Paul A. Fishwick; Bernard P. Zeigler",
    "corresponding_authors": "",
    "abstract": "Qualitative models arising in artificial intelligence domain often concern real systems that are difficult to represent with traditional means. However, some promise for dealing with such systems is offered by research in simulation methodology. Such research produces models that combine both continuous and discrete-event formalisms. Nevertheless, the aims and approaches of the AI and the simulation communities remain rather mutually ill understood. Consequently, there is a need to bridge theory and methodology in order to have a uniform language when either analyzing or reasoning about physical systems. This article introduces a methodology and formalism for developing multiple, cooperative models of physical systems of the type studied in qualitative physics. The formalism combines discrete-event and continuous models and offers an approach to building intelligent machines capable of physical modeling and reasoning.",
    "cited_by_count": 140,
    "openalex_id": "https://openalex.org/W1974332708",
    "type": "article"
  },
  {
    "title": "Modeling and generating multivariate time-series input processes using a vector autoregressive technique",
    "doi": "https://doi.org/10.1145/937332.937333",
    "publication_date": "2003-07-01",
    "publication_year": 2003,
    "authors": "Bahar Biller; Barry L. Nelson",
    "corresponding_authors": "",
    "abstract": "We present a model for representing stationary multivariate time-series input processes with marginal distributions from the Johnson translation system and an autocorrelation structure specified through some finite lag. We then describe how to generate data accurately to drive computer simulations. The central idea is to transform a Gaussian vector autoregressive process into the desired multivariate time-series input process that we presume as having a VARTA (Vector-Autoregressive-To-Anything) distribution. We manipulate the autocorrelation structure of the Gaussian vector autoregressive process so that we achieve the desired autocorrelation structure for the simulation input process. We call this the correlation-matching problem and solve it by an algorithm that incorporates a numerical-search procedure and a numerical-integration technique. An illustrative example is included.",
    "cited_by_count": 137,
    "openalex_id": "https://openalex.org/W1993614146",
    "type": "article"
  },
  {
    "title": "A combined procedure for optimization via simulation",
    "doi": "https://doi.org/10.1145/858481.858485",
    "publication_date": "2003-04-01",
    "publication_year": 2003,
    "authors": "Juta Pichitlamken; Barry L. Nelson",
    "corresponding_authors": "",
    "abstract": "We propose an optimization-via-simulation algorithm for use when the performance measure is estimated via a stochastic, discrete-event simulation, and the decision variables may be subject to deterministic linear integer constraints. Our approach---which consists of a global guidance system, a selection-of-the-best procedure, and local improvement---is globally convergent under very mild conditions.",
    "cited_by_count": 131,
    "openalex_id": "https://openalex.org/W2070093042",
    "type": "article"
  },
  {
    "title": "Variable-sample methods for stochastic optimization",
    "doi": "https://doi.org/10.1145/858481.858483",
    "publication_date": "2003-04-01",
    "publication_year": 2003,
    "authors": "Tito Homem‐de‐Mello",
    "corresponding_authors": "Tito Homem‐de‐Mello",
    "abstract": "In this article we discuss the application of a certain class of Monte Carlo methods to stochastic optimization problems. Particularly, we study variable-sample techniques, in which the objective function is replaced, at each iteration , by a sample average approximation. We first provide general results on the schedule of sample sizes, under which variable-sample methods yield consistent estimators as well as bounds on the estimation error. Because the convergence analysis is performed pathwisely, we are able to obtain our results in a flexible setting, which requires mild assumptions on the distributions and which includes the possibility of using different sampling distributions along the algorithm. We illustrate these ideas by studying a modification of the well-known pure random search method, adapting it to the variable-sample scheme, and show conditions for convergence of the algorithm. Implementation issues are discussed and numerical results are presented to illustrate the ideas.",
    "cited_by_count": 131,
    "openalex_id": "https://openalex.org/W2137385155",
    "type": "article"
  },
  {
    "title": "A search for good multiple recursive random number generators",
    "doi": "https://doi.org/10.1145/169702.169698",
    "publication_date": "1993-04-01",
    "publication_year": 1993,
    "authors": "Pierre L’Ecuyer; F. Blouin; Raymond Couture",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on A search for good multiple recursive random number generators Authors: Pierre L'Ecuyer Université de Montréal Université de MontréalView Profile , François Blouin Université Laval Université LavalView Profile , Raymond Couture Université Laval Université LavalView Profile Authors Info & Claims ACM Transactions on Modeling and Computer SimulationVolume 3Issue 2April 1993 pp 87–98https://doi.org/10.1145/169702.169698Published:01 April 1993Publication History 90citation998DownloadsMetricsTotal Citations90Total Downloads998Last 12 Months35Last 6 weeks4 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 128,
    "openalex_id": "https://openalex.org/W2015605031",
    "type": "article"
  },
  {
    "title": "Stationarity detection in the initial transient problem",
    "doi": "https://doi.org/10.1145/137926.137932",
    "publication_date": "1992-04-01",
    "publication_year": 1992,
    "authors": "Søren Asmussen; Peter W. Glynn; Hermann Þórisson",
    "corresponding_authors": "",
    "abstract": "Let X = {X(t)} t ≥ 0 be a stochastic process with a stationary version X * . It is investigated when it is possible to generate by simulation a version X˜ of X with lower initial bias than X itself, in the sense that either X˜ is strictly stationary (has the same distribution as X * ) or the distribution of X˜ is close to the distribution of X * . Particular attention is given to regenerative processes and Markov processes with a finite, countable, or general state space. The results are both positive and negative, and indicate that the tail of the distribution of the cycle length τ plays a critical role. The negative results essentially state that without some information on this tail, no a priori computable bias reduction is possible; in particular, this is the case for the class of all Markov processes with a countably infinite state space. On the contrary, the positive results give algorithms for simulating X˜ for various classes of processes with some special structure on τ . In particular, one can generate X˜ as strictly stationary for finite state Markov chains, Markov chains satisfying a Doeblin-type minorization, and regenerative processes with the cycle length τ bounded or having a stationary age distribution that can be generated by simulation.",
    "cited_by_count": 127,
    "openalex_id": "https://openalex.org/W2089897750",
    "type": "article"
  },
  {
    "title": "Random variate generation for exponentially and polynomially tilted stable distributions",
    "doi": "https://doi.org/10.1145/1596519.1596523",
    "publication_date": "2009-10-01",
    "publication_year": 2009,
    "authors": "Luc Devroye",
    "corresponding_authors": "Luc Devroye",
    "abstract": "We develop exact random variate generators for the polynomially and exponentially tilted unilateral stable distributions. The algorithms, which generalize Kanter's method, are uniformly fast over all choices of the tilting and stable parameters. The key to the solution is a new distribution which we call Zolotarev's distribution. We also present a novel double rejection method that is useful whenever densities have an integral representation involving an auxiliary variable.",
    "cited_by_count": 113,
    "openalex_id": "https://openalex.org/W2025952763",
    "type": "article"
  },
  {
    "title": "Asymptotic robustness of estimators in rare-event simulation",
    "doi": "https://doi.org/10.1145/1667072.1667078",
    "publication_date": "2010-01-01",
    "publication_year": 2010,
    "authors": "Pierre L’Ecuyer; José Blanchet; Bruno Tuffin; Peter W. Glynn",
    "corresponding_authors": "",
    "abstract": "The asymptotic robustness of estimators as a function of a rarity parameter, in the context of rare-event simulation, is often qualified by properties such as bounded relative error (BRE) and logarithmic efficiency (LE), also called asymptotic optimality. However, these properties do not suffice to ensure that moments of order higher than one are well estimated. For example, they do not guarantee that the variance of the empirical variance remains under control as a function of the rarity parameter. We study generalizations of the BRE and LE properties that take care of this limitation. They are named bounded relative moment of order k (BRM- k ) and logarithmic efficiency of order k (LE- k ), where k ≥ 1 is an arbitrary real number. We also introduce and examine a stronger notion called vanishing relative centered moment of order k , and exhibit examples where it holds. These properties are of interest for various estimators, including the empirical mean and the empirical variance. We develop (sufficient) Lyapunov-type conditions for these properties in a setting where state-dependent importance sampling (IS) is used to estimate first-passage time probabilities. We show how these conditions can guide us in the design of good IS schemes, that enjoy convenient asymptotic robustness properties, in the context of random walks with light-tailed and heavy-tailed increments. As another illustration, we study the hierarchy between these robustness properties (and a few others) for a model of highly reliable Markovian system (HRMS) where the goal is to estimate the failure probability of the system. In this setting, for a popular class of IS schemes, we show that BRM- k and LE- k are equivalent and that these properties become strictly stronger when k increases. We also obtain a necessary and sufficient condition for BRM- k in terms of quantities that can be readily computed from the parameters of the model.",
    "cited_by_count": 108,
    "openalex_id": "https://openalex.org/W2118479715",
    "type": "article"
  },
  {
    "title": "Simulation-based models of emergency departments:",
    "doi": "https://doi.org/10.1145/2000494.2000497",
    "publication_date": "2011-08-01",
    "publication_year": 2011,
    "authors": "Sergey Zeltyn; Yariv N. Marmor; Avishai Mandelbaum; Boaz Carmeli; Ohad Greenshpan; Yossi Mesika; Sergev Wasserkrug; Pnina Vortman; Avraham Shtub; Tirza Lauterman; Dagan Schwartz; Kobi Moskovitch; Sara Tzafrir; Fuad Basis",
    "corresponding_authors": "",
    "abstract": "The Emergency Department (ED) of a modern hospital is a highly complex system that gives rise to numerous managerial challenges. It spans the full spectrum of operational, clinical, and financial perspectives, over varying horizons: operational—a few hours or days ahead; tactical—weeks or a few months ahead; and strategic, which involves planning on monthly and yearly scales. Simulation offers a natural framework within which to address these challenges, as realistic ED models are typically intractable analytically. We apply a general and flexible ED simulator to address several significant problems that arose in a large Israeli hospital. The article focuses mainly, but not exclusively, on workforce staffing problems over these time horizons. First, we demonstrate that our simulation model can support real-time control, which enables short-term prediction and operational planning (physician and nurse staffing) for several hours or days ahead. To this end, we present a novel simulation-based technique that implements the concept of offered-load and discover that it performs better than a common alternative. Then we evaluate ED staff scheduling that adjusts for midterm changes (tactical horizon, several weeks or months ahead). Finally, we analyze the design and staffing problems that arose from physical relocation of the ED (strategic yearly horizon). Application of the simulation-based approach led to the implementation of our design and staffing recommendations.",
    "cited_by_count": 99,
    "openalex_id": "https://openalex.org/W2029195235",
    "type": "article"
  },
  {
    "title": "Monte Carlo Methods for Value-at-Risk and Conditional Value-at-Risk",
    "doi": "https://doi.org/10.1145/2661631",
    "publication_date": "2014-08-13",
    "publication_year": 2014,
    "authors": "L. Jeff Hong; Zhaolin Hu; Guangwu Liu",
    "corresponding_authors": "",
    "abstract": "Value-at-risk (VaR) and conditional value-at-risk (CVaR) are two widely used risk measures of large losses and are employed in the financial industry for risk management purposes. In practice, loss distributions typically do not have closed-form expressions, but they can often be simulated (i.e., random observations of the loss distribution may be obtained by running a computer program). Therefore, Monte Carlo methods that design simulation experiments and utilize simulated observations are often employed in estimation, sensitivity analysis, and optimization of VaRs and CVaRs. In this article, we review some of the recent developments in these methods, provide a unified framework to understand them, and discuss their applications in financial risk management.",
    "cited_by_count": 93,
    "openalex_id": "https://openalex.org/W2083545691",
    "type": "article"
  },
  {
    "title": "Data assimilation using sequential monte carlo methods in wildfire spread simulation",
    "doi": "https://doi.org/10.1145/2379810.2379816",
    "publication_date": "2012-11-01",
    "publication_year": 2012,
    "authors": "Haidong Xue; Feng Gu; Xiaolin Hu",
    "corresponding_authors": "",
    "abstract": "Assimilating real-time sensor data into large-scale spatial-temporal simulations, such as simulations of wildfires, is a promising technique for improving simulation results. This asks for advanced data assimilation methods that can work with the complex structures and nonlinear behaviors associated with the simulation models. This article presents a data assimilation framework using Sequential Monte Carlo (SMC) methods for wildfire spread simulations. The models and algorithms of the framework are described, and experimental results are provided. This work demonstrates the feasibility of applying SMC methods to data assimilation of wildfire spread simulations. The developed framework can potentially be generalized to other application areas where sophisticated simulation models are used.",
    "cited_by_count": 92,
    "openalex_id": "https://openalex.org/W2093805255",
    "type": "article"
  },
  {
    "title": "The effects of common random numbers on stochastic kriging metamodels",
    "doi": "https://doi.org/10.1145/2133390.2133391",
    "publication_date": "2012-03-01",
    "publication_year": 2012,
    "authors": "Xi Chen; Bruce E. Ankenman; Barry L. Nelson",
    "corresponding_authors": "",
    "abstract": "Ankenman et al. introduced stochastic kriging as a metamodeling tool for representing stochastic simulation response surfaces, and employed a very simple example to suggest that the use of Common Random Numbers (CRN) degrades the capability of stochastic kriging to predict the true response surface. In this article we undertake an in-depth analysis of the interaction between CRN and stochastic kriging by analyzing a richer collection of models; in particular, we consider stochastic kriging models with a linear trend term. We also perform an empirical study of the effect of CRN on stochastic kriging. We also consider the effect of CRN on metamodel parameter estimation and response-surface gradient estimation, as well as response-surface prediction. In brief, we confirm that CRN is detrimental to prediction, but show that it leads to better estimation of slope parameters and superior gradient estimation compared to independent simulation.",
    "cited_by_count": 91,
    "openalex_id": "https://openalex.org/W1982539084",
    "type": "article"
  },
  {
    "title": "SESSL",
    "doi": "https://doi.org/10.1145/2567895",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "Roland Ewald; Adelinde M. Uhrmacher",
    "corresponding_authors": "",
    "abstract": "This article introduces SESSL ( S imulation E xperiment S pecification via a S cala L ayer ), an embedded domain-specific language for simulation experiments. It serves as an additional software layer between users and simulation systems and is implemented in Scala. SESSL supports multiple simulation systems and offers various features (e.g., for experiment design, performance analysis, result reporting, and simulation-based optimization). It supports “cutting-edge” experiments by allowing to add custom code, enables a reuse of functionality across simulation systems, and improves the reproducibility of simulation experiments.",
    "cited_by_count": 90,
    "openalex_id": "https://openalex.org/W1974143589",
    "type": "article"
  },
  {
    "title": "Modeling Infectious Disease Epidemics in Mass Religious Gatherings: A Systematic Review",
    "doi": "https://doi.org/10.1145/3716869",
    "publication_date": "2025-02-11",
    "publication_year": 2025,
    "authors": "Sultanah M. Alshammari; Mohammed Ba-Aoum; Nofe Alganmi; Ahmad Showail",
    "corresponding_authors": "",
    "abstract": "Like other global mass gatherings, religious pilgrimages, such as Hajj, Arba’een, and the Hindu festival Kumbh Mela, attract millions of pilgrims to gather at specific holy sites on specific dates. During disease pandemics, mass gatherings can become super spreader events, causing exponential growth of infections in multiple regions. Epidemic modeling approaches can be valuable tools for studying the impact of mass gatherings on global health during disease outbreaks. To assess the use of epidemic models at religious pilgrimages, we compile published studies that proposed epidemic models at mass religious gatherings. A review of existing epidemic models at various religious gatherings highlights the role of epidemic modeling approaches in assessing the implications of religious pilgrimages on disease pandemics. All the articles surveyed showed a link between hosting religious gatherings and an increase in the number of cases of the simulated epidemic. In addition, we found that the SEIR mathematical model was the most common type developed with variations in some of the retrieved papers. The results reported in these studies motivate further investigation of the role of epidemic modeling and simulation in estimating the size and geographic scale of infections while hosting religious gatherings. Finally, we believe that this survey paper draws attention to the application of epidemic models in the advanced planning of recurrent religious pilgrimages, as it is not feasible to cancel, suspend, or reallocate these pilgrimages. These epidemic models can provide a baseline for policymakers to determine which control measures should be implemented and when.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4407374207",
    "type": "review"
  },
  {
    "title": "Accelerating the convergence of random search methods for discrete stochastic optimization",
    "doi": "https://doi.org/10.1145/352222.352225",
    "publication_date": "1999-10-01",
    "publication_year": 1999,
    "authors": "Sigrún Andradóttir",
    "corresponding_authors": "Sigrún Andradóttir",
    "abstract": "We discuss the choice of the estimation of the optimal solution when random search methods are applied to solve discrete stochastic optimization problems. At the present time, such optimization methods usually estimate the optimal solution using either the feasible solution the method is currently exploring or the feasible solution visited most often so far by the method. We propose using all the observed objective function values generated as the random search method moves around the feasible region seeking an optimal solution to obtain increasingly more precise estimates of the objective function values at the different points in the feasible region. At any given time, the feasible solution that has the best estimated objective function value (largest one for maximization problems; the smallest one for minimization problems) is used as the estimate of the optimal solution. We discuss the advantages of using this approach for estimating the optimal solution and present numerical results showing that modifying an existing random search method to use tnhis approach for estimating the optimal soluation appears to yield improved performance. We also present sereval rate of convergence results for random search methods using our approach for estimating the optimal solution. One these random search methods is a new variant of the stochastic comparison method; in addition to specifying the rate of convergence of this method, we prove that it is guaranteed to converge almost surely to the set of global optimal solutions and present a result that demonstrates that this method is likely to perform well in practice.",
    "cited_by_count": 132,
    "openalex_id": "https://openalex.org/W2125969801",
    "type": "article"
  },
  {
    "title": "Dynamic structures in modeling and simulation",
    "doi": "https://doi.org/10.1145/384169.384173",
    "publication_date": "2001-04-01",
    "publication_year": 2001,
    "authors": "Adelinde M. Uhrmacher",
    "corresponding_authors": "Adelinde M. Uhrmacher",
    "abstract": "As the number of flexible, adaptable systems grows so does the need for specification and analysis tools that support adaptable system structures. The increasing number of simulation tools that equip models with the capability of changing their behavior patterns, composition, and interactions raises the desire for a theoretical and methodological approach. A formalism is introduced based on DEVS which emphasizes the reflective nature of variable structure models. The proposed formalism and DEVS are shown to be bisimilar, which emphasizes the role of variable structure models as an agency of modularization. The formalism is used to reveal general problems and solutions in implementing variable structure models.",
    "cited_by_count": 130,
    "openalex_id": "https://openalex.org/W2089507474",
    "type": "article"
  },
  {
    "title": "A methodology for certification of modeling and simulation applications",
    "doi": "https://doi.org/10.1145/508366.508369",
    "publication_date": "2001-10-01",
    "publication_year": 2001,
    "authors": "Osman Balcı",
    "corresponding_authors": "Osman Balcı",
    "abstract": "Certification of modeling and simulation (M&amp;S) applications poses significant technical challenges for M&amp;S program managers, engineers, and practitioners. Certification is becoming increasingly more important as M&amp;S applications are used more and more for military training, complex system design evaluation, M&amp;S-based acquisition, problem solving, and critical decision making. Certification, a very complex process, involves the measurement and evaluation of hundreds of qualitative and quantitative elements, mandates subject matter expert evaluation, and requires the integration of different evaluations. Planning and managing such measurements and evaluations requires a unifying methodology and should not be performed in an ad hoc manner. This paper presents such a methodology. The methodology consists of the following body of methods, rules, and postulates: (a) employment of subject matter experts, (b) construction of a hierarchy of indicators, (c) relative criticality weighting of indicators using the analytic hierarchy process, (d) using a rule-based expert knowledge base with an object-oriented specification language, (e) assignment of crisp, fuzzy, and nominal scores for the indicators, (f) aggregation of indicator scores, (g) graphical representation of the indicator scores and weights, (h) hypertext certification report, and (i) interpretation of the results. The methodology can be used for certification of any kind of M&amp;S application either throughout the M&amp;S development life cycle or after the development is completed.",
    "cited_by_count": 126,
    "openalex_id": "https://openalex.org/W2054514273",
    "type": "article"
  },
  {
    "title": "A comparative study of parallel and sequential priority queue algorithms",
    "doi": "https://doi.org/10.1145/249204.249205",
    "publication_date": "1997-04-01",
    "publication_year": 1997,
    "authors": "Robert Rönngren; Rassul Ayani",
    "corresponding_authors": "",
    "abstract": "Priority queues are used in many applications including real-time systems, operating systems, and simulations. Their implementation may have a profound effect on the performance of such applications. In this article, we study the performance of well-known sequential priority queue implementations and the recently proposed parallel access priority queues. To accurately assess the performance of a priority queue, the performance measurement methodology must be appropriate. We use the Classic Hold, the Markov Model, and an Up/Down access pattern to measure performance and look at both the average access time and the worst-case time that are of vital interest to real-tiem applicatons. Our results suggest that the best choice for priority queue algorithms depends heavily on the application. For queue sizes smaller than 1,000 elements, the Splay Tree, the Skew Heap, and Henriksen's algorithm show good average access times. For large queue sized of 5,000 elements or more, the Calendar Queue and the Lazy Queue offer good average access times but have very long worst-case access times. The Skew Heap and the splay Tree exhibit the best worst-case access times. Among the parallel access priority queues tested, the Parallel Access Skew Heap provides the best performance on small shares memory multiprocessors.",
    "cited_by_count": 121,
    "openalex_id": "https://openalex.org/W2055676969",
    "type": "article"
  },
  {
    "title": "Behavior of the NORTA method for correlated random vector generation as the dimension increases",
    "doi": "https://doi.org/10.1145/937332.937336",
    "publication_date": "2003-07-01",
    "publication_year": 2003,
    "authors": "Soumyadip Ghosh; Shane G. Henderson",
    "corresponding_authors": "",
    "abstract": "The NORTA method is a fast general-purpose method for generating samples of a random vector with given marginal distributions and given correlation matrix. It is known that there exist marginal distributions and correlation matrices that the NORTA method cannot match, even though a random vector with the prescribed qualities exists. We investigate this problem as the dimension of the random vector increases. Simulation results show that the problem rapidly becomes acute, in the sense that NORTA fails to work with an increasingly large proportion of correlation matrices. Simulation results also show that if one is willing to settle for a correlation matrix that is \"close\" to the desired one, then NORTA performs well with increasing dimension. As part of our analysis, we develop a method for sampling correlation matrices uniformly (in a certain precise sense) from the set of all such matrices. This procedure can be used more generally for sampling uniformly from the space of all symmetric positive definite matrices with diagonal elements fixed at positive values.",
    "cited_by_count": 121,
    "openalex_id": "https://openalex.org/W2057237348",
    "type": "article"
  },
  {
    "title": "Time-space consistency in large-scale distributed virtual environments",
    "doi": "https://doi.org/10.1145/974734.974736",
    "publication_date": "2004-01-01",
    "publication_year": 2004,
    "authors": "Suiping Zhou; Wentong Cai; Bu‐Sung Lee; Stephen John Turner",
    "corresponding_authors": "",
    "abstract": "Maintaining a consistent view of the simulated world among different simulation nodes is a fundamental problem in large-scale distributed virtual environments (DVEs). In this paper, we characterize this problem by quantifying the time-space inconsistency in a DVE. To this end, a metric is defined to measure the time-space inconsistency in a DVE. One major advantage of the metric is that it may be estimated based on some characteristic parameters of a DVE, such as clock asynchrony, message transmission delay, the accuracy of the dead reckoning algorithm, the kinetics of the moving entity, and human factors. Thus the metric can be used to evaluate the time-space consistency property of a DVE without the actual execution of the DVE application, which is especially useful in the design stage of a DVE. Our work also clearly shows how the characteristic parameters of a DVE are interrelated in deciding the time-space inconsistency, so that we may fine-tune the DVE to make it as consistent as possible. To verify the effectiveness of the metric, a Ping-Pong game is developed. Experimental results show that the metric is effective in evaluating the time-space consistency property of the game.",
    "cited_by_count": 114,
    "openalex_id": "https://openalex.org/W2094962446",
    "type": "article"
  },
  {
    "title": "On the xorshift random number generators",
    "doi": "https://doi.org/10.1145/1113316.1113319",
    "publication_date": "2005-10-01",
    "publication_year": 2005,
    "authors": "François Panneton; Pierre L’Ecuyer",
    "corresponding_authors": "",
    "abstract": "G. Marsaglia recently introduced a class of very fast xorshift random number generators, whose implementation uses three “xorshift” operations. They belong to a large family of generators based on linear recurrences modulo 2, which also includes shift-register generators, the Mersenne twister, and several others. In this article, we analyze the theoretical properties of xorshift generators, search for the best ones with respect to the equidistribution criterion, and test them empirically. We find that the vast majority of xorshift generators with only three xorshift operations, including those having good equidistribution, fail several simple statistical tests. We also discuss generators with more than three xorshifts.",
    "cited_by_count": 110,
    "openalex_id": "https://openalex.org/W2010523529",
    "type": "article"
  },
  {
    "title": "Cloning parallel simulations",
    "doi": "https://doi.org/10.1145/508366.508370",
    "publication_date": "2001-10-01",
    "publication_year": 2001,
    "authors": "Maria Hybinette; Richard M. Fujimoto",
    "corresponding_authors": "",
    "abstract": "We present a cloning mechanism that enables the evaluation of multiple simulated futures. Performance of the mechanism is analyzed and evaluated experimentally on a shared memory multiprocessor. A running parallel discrete event simulation is dynamically cloned at decision points to explore different execution paths concurrently. In this way, what-if and alternative scenario analysis can be performed in applications such as gaming or tactical and strategic battle management. A construct called virtual logical processes avoids repeating common computations among clones and improves efficiency. The advantages of cloning are preserved regardless of the number of clones (or execution paths). Our performance results with a commercial air traffic control simulation demonstrate that cloning can significantly reduce the time required to compute multiple alternate futures.",
    "cited_by_count": 109,
    "openalex_id": "https://openalex.org/W2009634341",
    "type": "article"
  },
  {
    "title": "Performance bounds on parallel self-initiating discrete-event simulations",
    "doi": "https://doi.org/10.1145/102810.102812",
    "publication_date": "1991-01-03",
    "publication_year": 1991,
    "authors": "David M. Nicol",
    "corresponding_authors": "David M. Nicol",
    "abstract": "This paper considers the use of massively parallel architectures to execute discrete-event simulations of what we term “self-initiating” models. A logical process in a self-initiating model schedules its own state reevaluation times, independently of any other logical process, and sends its new state to other logical processes following the reevaluation. Our interest is in the effects of that communication on synchronization. Using a model that idealizes the communication topology of a simulation, we consider the performance of various synchronization protocols by deriving upper and lower bounds on optimal performance, upper bounds on Time Warp's performance, and lower bounds on the performance of a new consevative protocol. Our analysis of Time Warp includes some of the overhead costs of state saving and rollback; the effects of propogating rollbacks are ignored. The analysis points out sufficient conditions for the conservitive protocol to outperform Time Warp. The analysis also quantifies the sensitivity of performance to message fanout, lookahead ability, and the probability distributions underlying the simulation.",
    "cited_by_count": 108,
    "openalex_id": "https://openalex.org/W1974020230",
    "type": "article"
  },
  {
    "title": "Simulating heavy tailed processes using delayed hazard rate twisting",
    "doi": "https://doi.org/10.1145/566392.566394",
    "publication_date": "2002-04-01",
    "publication_year": 2002,
    "authors": "Sandeep Juneja; Perwez Shahabuddin",
    "corresponding_authors": "",
    "abstract": "Consider the problem of estimating the small probability that the maximum of a random walk exceeds a large threshold, when the process has a negative drift and the underlying random variables may have heavy tailed distributions, that is, their tail distribution decays at a subexponential rate. We consider one class of such problems that has applications in estimating the ruin probability associated with insurance claim processes with subexponentially distributed claim sizes, and in estimating the probability of large delays in an M / G /1 queue with subexponentially distributed service times. Significant work has been done on analogous problems for the light tailed case (when the tail distribution decreases at an exponential rate or faster) that involve importance sampling methods using appropriate exponential twisting. However, for the subexponential case, such exponential twisting is infeasible and alternative techniques are needed. In this paper we introduce importance sampling techniques where the new probability measure is obtained by twisting the hazard rate of the original distribution. For subexponential distributions this amounts to subexponential twisting---twisting at a subexponential rate. In addition, we introduce the technique of \"delaying\" the twisting and show that the combination of the two techniques produces asymptotically optimal estimates of the small probability mentioned above.",
    "cited_by_count": 106,
    "openalex_id": "https://openalex.org/W2077846544",
    "type": "article"
  },
  {
    "title": "Variance with alternative scramblings of digital nets",
    "doi": "https://doi.org/10.1145/945511.945518",
    "publication_date": "2003-10-01",
    "publication_year": 2003,
    "authors": "Art B. Owen",
    "corresponding_authors": "Art B. Owen",
    "abstract": "There have been many proposals for randomizations of digital nets. Some of those proposals greatly reduce the computational burden of random scrambling. This article compares the sampling variance under different scrambling methods. Some scrambling methods adversely affect the variance, even to the extent of deteriorating the rate at which variance converges to zero. Surprisingly, a new scramble proposed here, has the effect of improving the rate at which the variance converges to zero, but so far, only for one dimensional integrands. The mean squared L 2 discrepancy is commonly used to study scrambling schemes. In this case, it does not distinguish among some scrambles with different convergence rates for the variance.",
    "cited_by_count": 105,
    "openalex_id": "https://openalex.org/W1986595758",
    "type": "article"
  },
  {
    "title": "On the equivalent bandwidth of self-similar sources",
    "doi": "https://doi.org/10.1145/364996.365003",
    "publication_date": "2000-04-01",
    "publication_year": 2000,
    "authors": "Nelson L. S. da Fonseca; G.S. Mayor; Cesar A. V. Neto",
    "corresponding_authors": "",
    "abstract": "This article presents a method for the computation of the equivalent bandwidth of an aggregate of heterogeneous self-similar sources, as well as the time scales of interest for queueing systems fed by a fractal Brownian motion (fBm) process. Moreover, the fractal leaky bucket, a novel policing mechanism capable of accurately monitoring self-similar sources, is introduced.",
    "cited_by_count": 104,
    "openalex_id": "https://openalex.org/W1970562937",
    "type": "article"
  },
  {
    "title": "Consistency maintenance in multiresolution simulation",
    "doi": "https://doi.org/10.1145/259207.259235",
    "publication_date": "1997-07-01",
    "publication_year": 1997,
    "authors": "Paul F. Reynolds; Anand Natrajan; Sudhir Srinivasan",
    "corresponding_authors": "",
    "abstract": "Simulations that run at multiple levels of resolution often encounter consistency problems because of insufficient correlation between the attributes at multiple levels of the same entity. Inconsistency may occur despite the existence of valid models at each resolution level. Cross-Resolution Modeling (CRM) attempts to build effective multiresolution simulations. The traditional approach to CRM—aggregation-disaggregation—causes chain disaggregation and puts an unacceptable burden on resources. We present four fundamental observations that would help guide future approaches to CRM. These observations form the basis of an approach we propose that involves the design of Multiple Resolution Entities (MREs). MREs are the foundation of a design that incorporates maintaining internal consistency. We also propose maintenance of core attributes as an approach to maintaining internal consistency within an MRE..",
    "cited_by_count": 104,
    "openalex_id": "https://openalex.org/W2027494374",
    "type": "article"
  },
  {
    "title": "Optimal memory management for time warp parallel simulation",
    "doi": "https://doi.org/10.1145/130611.130612",
    "publication_date": "1991-10-01",
    "publication_year": 1991,
    "authors": "Yi‐Bing Lin; Bruno R. Preiss",
    "corresponding_authors": "",
    "abstract": "Recently there has been a great deal of interest in performance evalution of parallel simulation. Most work is devoted to the time complexity and assumes that the amount of memory available for parallel simulation is unlimited. This paper studies the space complexity of parallel simulation. Our goal is to design an efficient memory management protocol which guarantees that the memory consumption of parallel simulation is of the same order as sequential simulation. (Such an algorithm is referred to as a optimal .) First, we derive the relationships among the space complexities of sequential simulation, Chandy-Misra simulation [2], and Time Warp simulation [7]. We show that Chandy-Misra may consume more storage than sequential simulation, or vice versa. Then we show that Time Warp never consumes less memory than sequential simulation. Then we describe cancelback , an optimal Time Warp memory management protocol proposed by Jefferson. Although cancelback is considered to be complete solution for the storage management problem in Time Warp, some efficiency issues in implementing this algorithm must be considered. We propose an optimal algorithm called artifical rollback . We show that this algorithm is easy to implement and analyze. An implementation of artificial rollback is given, which is integrated with processor scheduling to adjust the memory consumption rate based on the amount of free storage available in the system.",
    "cited_by_count": 103,
    "openalex_id": "https://openalex.org/W2004367824",
    "type": "article"
  },
  {
    "title": "A federated approach to distributed network simulation",
    "doi": "https://doi.org/10.1145/985793.985795",
    "publication_date": "2004-04-01",
    "publication_year": 2004,
    "authors": "George F. Riley; Mostafa Ammar; Richard M. Fujimoto; Alfred Park; Kalyan S. Perumalla; Donghua Xu",
    "corresponding_authors": "",
    "abstract": "We describe an approach and our experiences in applying federated simulation techniques to create large-scale parallel simulations of computer networks. Using the federated approach, the topology and the protocol stack of the simulated network is partitioned into a number of submodels, and a simulation process is instantiated for each one. Runtime infrastructure software provides services for interprocess communication and synchronization (time management). We first describe issues that arise in homogeneous federations where a sequential simulator is federated with itself to realize a parallel implementation. We then describe additional issues that must be addressed in heterogeneous federations composed of different network simulation packages, and describe a dynamic simulation backplane mechanism that facilitates interoperability among different network simulators. Specifically, the dynamic simulation backplane provides a means of addressing key issues that arise in federating different network simulators: differing packet representations, incomplete implementations of network protocol models, and differing levels of detail among the simulation processes. We discuss two different methods for using the backplane for interactions between heterogeneous simulators: the cross-protocol stack method and the split-protocol stack method. Finally, results from an experimental study are presented for both the homogeneous and heterogeneous cases that provide evidence of the scalability of our federated approach on two moderately sized computing clusters. Two different homogeneous implementations are described: Parallel/Distributed ns ( pdns ) and the Georgia Tech Network Simulator ( GTNetS ). Results of a heterogeneous implementation federating ns with GloMoSim are described. This research demonstrates that federated simulations are a viable approach to realizing efficient parallel network simulation tools.",
    "cited_by_count": 103,
    "openalex_id": "https://openalex.org/W2076563609",
    "type": "article"
  },
  {
    "title": "An analysis of rollback-based simulation",
    "doi": "https://doi.org/10.1145/116890.116912",
    "publication_date": "1991-04-01",
    "publication_year": 1991,
    "authors": "Boris D. Lubachevsky; Adam Schwartz; Alan Weiss",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on An analysis of rollback-based simulation Authors: Boris Lubachevsky AT&T Bell Labs, Murray Hill, NJ AT&T Bell Labs, Murray Hill, NJView Profile , Adam Schwartz Technion, IIT, Haifa, Israel Technion, IIT, Haifa, IsraelView Profile , Alan Weiss AT&T Bell Labs, Murray Hill, NJ AT&T Bell Labs, Murray Hill, NJView Profile Authors Info & Claims ACM Transactions on Modeling and Computer SimulationVolume 1Issue 2pp 154–193https://doi.org/10.1145/116890.116912Published:01 April 1991Publication History 98citation559DownloadsMetricsTotal Citations98Total Downloads559Last 12 Months70Last 6 weeks8 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 103,
    "openalex_id": "https://openalex.org/W2087985902",
    "type": "article"
  },
  {
    "title": "Computing global virtual time in shared-memory multiprocessors",
    "doi": "https://doi.org/10.1145/268403.268404",
    "publication_date": "1997-10-01",
    "publication_year": 1997,
    "authors": "Richard M. Fujimoto; Maria Hybinette",
    "corresponding_authors": "",
    "abstract": "Global virtual time (GVT) is used in the Time Warp synchronization mechanism to perform irrevocable operations such as I/O and to reclaim storage. Most existing algorithms for computing GVT assume a message-passing programming model. Here, GVT computation is examined in the context of a shared-memory model. We observe that computation of GVT is much simpler in shared-memory multiprocessors because these machines normally guarantee that no two processors will observe a set of memory operations as occurring in different orders. Exploiting this fact, we propose an efficient, asynchronous, shared-memory GVT algorithm and prove its correctness. This algorithm does not require message acknowledgments, special GVT messages, or FIFO delivery of messages, and requires only a minimal number of shared variables and data structures. The algorithm only requires one round of interprocessor communication to compute GVT, in contrast to many message-based algorithms that require two. An efficient implementatin is described that eliminates the need for a processor to explicitly compute a local minimum for time warp systems using a lowest-timestamp-first scheduling policy in each processor. In addition, we propose a new mechanism called on-the-fly fossil collection that enables efficient storage reclamation for simulations containing large numbers, e.g., hundreds of thousand or even millions of simulator objects. On-the-fly fossil collection can be used in time warp systems executing on either shared-memory or message-based machines. Performance measurements of the GVT algorithm and the on-the-fly fossil collection mechanism on a Kendall Square Research KSR-2 machine demonstrate that these techniques enable frequent GVT and fossil collections, e.g., every millisecond, without incurring a significant performance penalty",
    "cited_by_count": 102,
    "openalex_id": "https://openalex.org/W2053132205",
    "type": "article"
  },
  {
    "title": "Effects of the checkpoint interval on time and space in time warp",
    "doi": "https://doi.org/10.1145/189443.189444",
    "publication_date": "1994-07-01",
    "publication_year": 1994,
    "authors": "Bruno R. Preiss; Wayne M. Loucks; Ian D. Macintyre",
    "corresponding_authors": "",
    "abstract": "Optimistically synchronized parallel discrete-event simulation is based on the use of communicating sequential processes. Optimistic synchronization means that the processes proceed under the assumption that a synchronized execution schedule is fortuitous. Periodic checkpointing of the state of a process allows the process to roll back to an earlier state when synchronization errors are detected. This article examines the effects of varying the checkpoint interval on the execution time and memory space needed to perform a parallel simulation. The empirical results presented in this article were obtained from the simulation of closed stochastic queuing networks with several different topologies. Various intraprocessor process-scheduling algorithms and both lazy and aggressive cancellation strategies are considered. The empirical results are compared with analytical formulae predicting time-optimal checkpoint intervals. Two modes of operation, throttling and thrashing , have been noted and their effect examined. As the checkpoint interval is increased from one, there is a throttling effect among processes on the same processor, which improves performance. When the checkpoint interval is made too large, there is a thrashing effect caused by interaction between processes on different processors. It is shown that the time-optimal and space-optimal checkpoint intervals are not the same. Furthermore, a checkpoint interval that is too small affects space adversely more than time, whereas, a checkpoint interval that is too large affects time adversely more than space.",
    "cited_by_count": 102,
    "openalex_id": "https://openalex.org/W2073792478",
    "type": "article"
  },
  {
    "title": "ASAP3: a batch means procedure for steady-state simulation analysis",
    "doi": "https://doi.org/10.1145/1044322.1044325",
    "publication_date": "2005-01-01",
    "publication_year": 2005,
    "authors": "Natalie M. Steiger; Emily K. Lada; James R. Wilson; Jeffrey A. Joines; Christos Alexopoulos; David Goldsman",
    "corresponding_authors": "",
    "abstract": "We introduce ASAP3, a refinement of the batch means algorithms ASAP and ASAP2, that delivers point and confidence-interval estimators for the expected response of a steady-state simulation. ASAP3 is a sequential procedure designed to produce a confidence-interval estimator that satisfies user-specified requirements on absolute or relative precision as well as coverage probability. ASAP3 operates as follows: the batch size is progressively increased until the batch means pass the Shapiro-Wilk test for multivariate normality; and then ASAP3 fits a first-order autoregressive (AR(1)) time series model to the batch means. If necessary, the batch size is further increased until the autoregressive parameter in the AR(1) model does not significantly exceed 0.8. Next, ASAP3 computes the terms of an inverse Cornish-Fisher expansion for the classical batch means t -ratio based on the AR(1) parameter estimates; and finally ASAP3 delivers a correlation-adjusted confidence interval based on this expansion. Regarding not only conformance to the precision and coverage-probability requirements but also the mean and variance of the half-length of the delivered confidence interval, ASAP3 compared favorably to other batch means procedures (namely, ABATCH, ASAP, ASAP2, and LBATCH) in an extensive experimental performance evaluation.",
    "cited_by_count": 97,
    "openalex_id": "https://openalex.org/W2039039263",
    "type": "article"
  },
  {
    "title": "Rare events, splitting, and quasi-Monte Carlo",
    "doi": "https://doi.org/10.1145/1225275.1225280",
    "publication_date": "2007-04-01",
    "publication_year": 2007,
    "authors": "Pierre L’Ecuyer; Valérie Demers; Bruno Tuffin",
    "corresponding_authors": "",
    "abstract": "In the context of rare-event simulation, splitting and importance sampling (IS) are the primary approaches to make important rare events happen more frequently in a simulation and yet recover an unbiased estimator of the target performance measure, with much smaller variance than a straightforward Monte Carlo (MC) estimator. Randomized quasi-Monte Carlo (RQMC) is another class of methods for reducing the noise of simulation estimators, by sampling more evenly than with standard MC. It typically works well for simulations that depend mostly on very few random numbers. In splitting and IS, on the other hand, we often simulate Markov chains whose sample paths are a function of a long sequence of independent random numbers generated during the simulation. In this article, we show that RQMC can be used jointly with splitting and/or IS to construct better estimators than those obtained by either of these methods alone. We do that in a setting where the goal is to estimate the probability of reaching B before reaching (or returning to) A when starting A from a distinguished state not in B , where A and B are two disjoint subsets of the state space, and B is very rarely reached. This problem has several practical applications. The article is in fact a two-in-one: the first part provides a guided tour of splitting techniques, introducing along the way some improvements in the implementation of multilevel splitting. At the end of the article, we also give examples of situations where splitting is not effective. For these examples, we compare different ways of applying IS and combining it with RQMC.",
    "cited_by_count": 94,
    "openalex_id": "https://openalex.org/W1990674830",
    "type": "article"
  },
  {
    "title": "Very large fractional factorial and central composite designs",
    "doi": "https://doi.org/10.1145/1113316.1113320",
    "publication_date": "2005-10-01",
    "publication_year": 2005,
    "authors": "Susan M. Sanchez; Paul J. Sánchez",
    "corresponding_authors": "",
    "abstract": "We present a concise representation of fractional factorials and an algorithm to quickly generate resolution V designs. The description is based on properties of a complete, orthogonal discrete-valued basis set called Walsh functions. We tabulate two-level resolution V fractional factorial designs, as well as central composite designs allowing estimation of full second-order models, for experiments involving up to 120 factors. The simple algorithm provided can be used to characterize even larger designs, and a fast Walsh transform method quickly generates design matrices from our representation.",
    "cited_by_count": 93,
    "openalex_id": "https://openalex.org/W1993934821",
    "type": "article"
  },
  {
    "title": "Efficient and portable combined Tausworthe random number generators",
    "doi": "https://doi.org/10.1145/116890.116892",
    "publication_date": "1991-04-01",
    "publication_year": 1991,
    "authors": "Shu Tezuka; Pierre L’Ecuyer",
    "corresponding_authors": "",
    "abstract": "article Efficient and portable combined Tausworthe random number generators Share on Authors: Shu Tezuka IBM Research, Tokyo, Japan IBM Research, Tokyo, JapanView Profile , Pierre L'Ecuyer Univ. de Montreal, Montreal, P.Q., Canada Univ. de Montreal, Montreal, P.Q., CanadaView Profile Authors Info & Claims ACM Transactions on Modeling and Computer SimulationVolume 1Issue 2April 1991 pp 99–112https://doi.org/10.1145/116890.116892Online:01 April 1991Publication History 69citation545DownloadsMetricsTotal Citations69Total Downloads545Last 12 Months24Last 6 weeks5 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",
    "cited_by_count": 93,
    "openalex_id": "https://openalex.org/W2057062020",
    "type": "article"
  },
  {
    "title": "Importance sampling for sums of random variables with regularly varying tails",
    "doi": "https://doi.org/10.1145/1243991.1243995",
    "publication_date": "2007-07-01",
    "publication_year": 2007,
    "authors": "Paul Dupuis; Kevin Leder; Hui Wang",
    "corresponding_authors": "",
    "abstract": "Importance sampling is a variance reduction technique for efficient estimation of rare-event probabilities by Monte Carlo. For random variables with heavy tails there is little consensus on how to choose the change of measure used in importance sampling. In this article we study dynamic importance sampling schemes for sums of independent and identically distributed random variables with regularly varying tails. The number of summands can be random but must be independent of the summands. For estimating the probability that the sum exceeds a given threshold, we explicitly identify a class of dynamic importance sampling algorithms with bounded relative errors. In fact, these schemes are nearly asymptotically optimal in the sense that the second moment of the corresponding importance sampling estimator can be made as close as desired to the minimal possible value.",
    "cited_by_count": 87,
    "openalex_id": "https://openalex.org/W2031575583",
    "type": "article"
  },
  {
    "title": "Finding feasible systems in the presence of constraints on multiple performance measures",
    "doi": "https://doi.org/10.1145/1842713.1842716",
    "publication_date": "2010-09-01",
    "publication_year": 2010,
    "authors": "Demet Batur; Seong‐Hee Kim",
    "corresponding_authors": "",
    "abstract": "We consider the problem of finding a set of feasible or near-feasible systems among a finite number of simulated systems in the presence of constraints on secondary performance measures. We first present a generic procedure that detects the feasibility of one system in the presence of one constraint and extend it to the case of two or more systems and constraints. To accelerate the elimination of infeasible systems, a method that reuses collected observations and its variance-updating version are discussed. Experimental results are presented to compare the performance of the proposed procedures.",
    "cited_by_count": 85,
    "openalex_id": "https://openalex.org/W2069209488",
    "type": "article"
  },
  {
    "title": "Stochastic formulation of SPICE-type electronic circuit simulation with polynomial chaos",
    "doi": "https://doi.org/10.1145/1391978.1391981",
    "publication_date": "2008-09-01",
    "publication_year": 2008,
    "authors": "Kai Strunz; Qianli Su",
    "corresponding_authors": "",
    "abstract": "A methodology for efficient tolerance analysis of electronic circuits based on nonsampling stochastic simulation of transients is formulated, implemented, and validated. We model the stochastic behavior of all quantities that are subject to tolerance spectrally with polynomial chaos. A library of stochastic models of linear and nonlinear circuit elements is created. In analogy to the deterministic implementation of the SPICE electronic circuit simulator, the overall stochastic circuit model is obtained using nodal analysis. In the proposed case studies, we analyze the influence of device tolerance on the response of a lowpass filter, the impact of temperature variability on the output of an amplifier, and the effect of changes of the load of a diode bridge on the probability density function of the output voltage. The case studies demonstrate that the novel methodology is computationally faster than the Monte Carlo method and more accurate and flexible than the root-sum-square method. This makes the stochastic circuit simulator, referred to as PolySPICE, a compelling candidate for the tolerance study of reliability-critical electronic circuits.",
    "cited_by_count": 83,
    "openalex_id": "https://openalex.org/W2053719548",
    "type": "article"
  },
  {
    "title": "On the validity of flow-level tcp network models for grid and cloud simulations",
    "doi": "https://doi.org/10.1145/2517448",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Pedro Isaacsson Velho; Lucas Mello Schnorr; Henri Casanova; Arnaud Legrand",
    "corresponding_authors": "",
    "abstract": "Researchers in the area of grid/cloud computing perform many of their experiments using simulations that must capture network behavior. In this context, packet-level simulations, which are widely used to study network protocols, are too costly given the typical large scales of simulated systems and applications. An alternative is to implement network simulations with less costly flow-level models. Several flow-level models have been proposed and implemented in grid/cloud simulators. Surprisingly, published validations of these models, if any, consist of verifications for only a few simple cases. Consequently, even when they have been used to obtain published results, the ability of these simulators to produce scientifically meaningful results is in doubt. This work evaluates these state-of-the-art flow-level network models of TCP communication via comparison to packet-level simulation. While it is straightforward to show cases in which previously proposed models lead to good results, instead we follow the critical method , which places model refutation at the center of the scientific activity, and we systematically seek cases that lead to invalid results. Careful analysis of these cases reveals fundamental flaws and also suggests improvements. One contribution of this work is that these improvements lead to a new model that, while far from being perfect, improves upon all previously proposed models in the context of simulation of grids or clouds. A more important contribution, perhaps, is provided by the pitfalls and unexpected behaviors encountered in this work, leading to a number of enlightening lessons. In particular, this work shows that model validation cannot be achieved solely by exhibiting (possibly many) “good cases.” Confidence in the quality of a model can only be strengthened through an invalidation approach that attempts to prove the model wrong.",
    "cited_by_count": 72,
    "openalex_id": "https://openalex.org/W2033587324",
    "type": "article"
  },
  {
    "title": "Generalized Halton sequences in 2008",
    "doi": "https://doi.org/10.1145/1596519.1596520",
    "publication_date": "2009-10-01",
    "publication_year": 2009,
    "authors": "Henri Faure; Christiane Lemieux",
    "corresponding_authors": "",
    "abstract": "Halton sequences have always been quite popular with practitioners, in part because of their intuitive definition and ease of implementation. However, in their original form, these sequences have also been known for their inadequacy to integrate functions in moderate to large dimensions, in which case ( t , s )-sequences such as the Sobol' sequence are usually preferred. To overcome this problem, one possible approach is to include permutations in the definition of Halton sequences—thereby obtaining generalized Halton sequences —an idea that goes back to almost thirty years ago, and that has been studied by many researchers in the last few years. In parallel to these efforts, an important improvement in the upper bounds for the discrepancy of Halton sequences has been made by Atanassov in 2004. Together, these two lines of research have revived the interest in Halton sequences. In this article, we review different generalized Halton sequences that have been proposed recently, and compare them by means of numerical experiments. We also propose a new generalized Halton sequence which, we believe, offers a practical advantage over the surveyed constructions, and that should be of interest to practitioners.",
    "cited_by_count": 71,
    "openalex_id": "https://openalex.org/W2103255584",
    "type": "article"
  },
  {
    "title": "The stochastic root-finding problem",
    "doi": "https://doi.org/10.1145/1921598.1921603",
    "publication_date": "2011-02-04",
    "publication_year": 2011,
    "authors": "Raghu Pasupathy; Sujin Kim",
    "corresponding_authors": "",
    "abstract": "The stochastic root-finding problem (SRFP) is that of finding the zero(s) of a vector function, that is, solving a nonlinear system of equations when the function is expressed implicitly through a stochastic simulation. SRFPs are equivalently expressed as stochastic fixed-point problems, where the underlying function is expressed implicitly via a noisy simulation. After motivating SRFPs using a few examples, we review available methods to solve such problems on constrained Euclidean spaces. We present the current literature as three broad categories, and detail the basic theoretical results that are currently known in each of the categories. With a view towards helping the practitioner, we discuss specific variations in their implementable form, and provide references to computer code when easily available. Finally, we list a few questions that are worthwhile research pursuits from the standpoint of advancing our knowledge of the theoretical underpinnings and the implementation aspects of solutions to SRFPs.",
    "cited_by_count": 66,
    "openalex_id": "https://openalex.org/W2103446957",
    "type": "article"
  },
  {
    "title": "Confidence intervals for quantiles when applying variance-reduction techniques",
    "doi": "https://doi.org/10.1145/2133390.2133394",
    "publication_date": "2012-03-01",
    "publication_year": 2012,
    "authors": "Fang Chu; Marvin K. Nakayama",
    "corresponding_authors": "",
    "abstract": "Quantiles, which are also known as values-at-risk in finance, frequently arise in practice as measures of risk. This article develops asymptotically valid confidence intervals for quantiles estimated via simulation using variance-reduction techniques (VRTs). We establish our results within a general framework for VRTs, which we show includes importance sampling, stratified sampling, antithetic variates, and control variates. Our method for verifying asymptotic validity is to first demonstrate that a quantile estimator obtained via a VRT within our framework satisfies a Bahadur-Ghosh representation. We then exploit this to show that the quantile estimator obeys a central limit theorem (CLT) and to develop a consistent estimator for the variance constant appearing in the CLT, which enables us to construct a confidence interval. We provide explicit formulae for the estimators for each of the VRTs considered.",
    "cited_by_count": 62,
    "openalex_id": "https://openalex.org/W2154513807",
    "type": "article"
  },
  {
    "title": "Integer-Ordered Simulation Optimization using R-SPLINE",
    "doi": "https://doi.org/10.1145/2499913.2499916",
    "publication_date": "2013-07-01",
    "publication_year": 2013,
    "authors": "Wang Hong-gang; Raghu Pasupathy; Bruce W. Schmeiser",
    "corresponding_authors": "",
    "abstract": "We consider simulation-optimization (SO) models where the decision variables are integer ordered and the objective function is defined implicitly via a simulation oracle, which for any feasible solution can be called to compute a point estimate of the objective-function value. We develop R-SPLINE---a Retrospective-search algorithm that alternates between a continuous Search using Piecewise-Linear Interpolation and a discrete Neighborhood Enumeration, to asymptotically identify a local minimum. R-SPLINE appears to be among the first few gradient-based search algorithms tailored for solving integer-ordered local SO problems. In addition to proving the almost-sure convergence of R-SPLINE’s iterates to the set of local minima, we demonstrate that the probability of R-SPLINE returning a solution outside the set of true local minima decays exponentially in a certain precise sense. R-SPLINE, with no parameter tuning, compares favorably with popular existing algorithms.",
    "cited_by_count": 58,
    "openalex_id": "https://openalex.org/W2044732262",
    "type": "article"
  },
  {
    "title": "An Introduction to Multiobjective Simulation Optimization",
    "doi": "https://doi.org/10.1145/3299872",
    "publication_date": "2019-01-24",
    "publication_year": 2019,
    "authors": "Susan R. Hunter; Eric A. Applegate; Viplove Arora; Bryan Chong; Kyle Cooper; Oscar Rincón‐Guevara; Carolina Vivas-Valencia",
    "corresponding_authors": "",
    "abstract": "The multiobjective simulation optimization (MOSO) problem is a nonlinear multiobjective optimization problem in which multiple simultaneous and conflicting objective functions can only be observed with stochastic error. We provide an introduction to MOSO at the advanced tutorial level, aimed at researchers and practitioners who wish to begin working in this emerging area. Our focus is exclusively on MOSO methods that characterize the entire efficient or Pareto-optimal set as the solution to the MOSO problem; later, this set may be used as input to the broader multicriteria decision-making process. Our introduction to MOSO includes an overview of existing theory, methods, and provably convergent algorithms that explicitly control sampling error for (1) MOSO on finite sets, called multiobjective ranking and selection; (2) MOSO with integer-ordered decision variables; and (3) MOSO with continuous decision variables. In the context of integer-ordered and continuous decision variables, we focus on methods that provably converge to a local efficient set under the natural ordering. We also discuss key open questions that remain in this emerging field.",
    "cited_by_count": 57,
    "openalex_id": "https://openalex.org/W2914811027",
    "type": "article"
  },
  {
    "title": "Parametrized Adomian Decomposition Method with Optimum Convergence",
    "doi": "https://doi.org/10.1145/3106373",
    "publication_date": "2017-10-27",
    "publication_year": 2017,
    "authors": "Mustafa Türkyılmazoğlu",
    "corresponding_authors": "Mustafa Türkyılmazoğlu",
    "abstract": "The classical Adomian decomposition method frequently used to solve linear and nonlinear algebraic or integro-differential equations of ordinary and partial type is revisited. Rewriting the technique in an elegant form, a parameter so-called as the convergence control parameter, is embedded into the method to control the convergence and the rate of convergence of the method. Besides the constant level curves for determining suitable values, an effective approach for obtaining the best possible convergence control parameter is later devised based on the squared residual error of the studied problem. The optimum Adomian decomposition method is proved to converge to the true solution where the classical Adomian decomposition method fails to converge. When both methods are convergent, the present algorithm is observed to accelerate the rate of convergence. Moreover, the restricted domain of convergent physical solution obtained by the classical Adomian method is shown to be greatly extended to a finer interval by the optimum Adomian decomposition method. The justification of the new scheme is made clear on several mathematical/physical examples selected from the open literature. Finally, an example is provided to demonstrate the better accuracy of the optimum Adomian decomposition method over the recently popular homotopy analysis method.",
    "cited_by_count": 52,
    "openalex_id": "https://openalex.org/W2765605098",
    "type": "article"
  },
  {
    "title": "MFNetSim: A Multi-Fidelity Network Simulation Framework for Multi-Traffic Modeling of Dragonfly Systems",
    "doi": "https://doi.org/10.1145/3729424",
    "publication_date": "2025-04-23",
    "publication_year": 2025,
    "authors": "Xin Wang; Kevin A. Brown; Robert Ross; Christopher D. Carothers; Zhiling Lan",
    "corresponding_authors": "",
    "abstract": "In high-performance computing (HPC), modern supercomputers typically provide exclusive computing resources to user applications. Nevertheless, the interconnect network is a shared resource for both inter-node communication and across-node I/O access, among co-running workloads, leading to inevitable network interference. In this study, we develop MFNetSim, a multi-fidelity modeling framework that enables simulation of multi-traffic simultaneously over the interconnect network, including inter-process communication and I/O traffic. By combining different levels of abstraction, MFNetSim can efficiently co-model the communication and I/O traffic occurring on HPC systems equipped with flash-based storage. We conduct simulation studies of hybrid workloads composed of traditional HPC applications and emerging ML applications on a 1,056-node Dragonfly system with various configurations. Our analysis provides various observations regarding how network interference affects communication and I/O traffic.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4409701007",
    "type": "article"
  },
  {
    "title": "Using (Not-so) Large Language Models to Generate Simulation Models in a Formal DSL: A Study on Reaction Networks",
    "doi": "https://doi.org/10.1145/3733719",
    "publication_date": "2025-05-28",
    "publication_year": 2025,
    "authors": "Justin N. Kreikemeyer; Miłosz Jankowski; Pia Wilsdorf; Adelinde M. Uhrmacher",
    "corresponding_authors": "",
    "abstract": "Formal languages are an integral part of modeling and simulation. They allow the distillation of knowledge into concise simulation models amenable to automatic execution, interpretation, and analysis. However, the arguably most humanly accessible means of expressing models is through natural language, which is not easily interpretable by computers. Here, we evaluate how a Large Language Model (LLM) might be used for formalizing natural language into simulation models. Existing studies only explored using very large LLMs, like the commercial GPT models, without fine-tuning model weights. To close this gap, we show how an open-weights, 7B-parameter Mistral model can be fine-tuned to translate natural language descriptions to reaction network models in a domain-specific language, offering a self-hostable, compute-, and memory efficient alternative. To this end, we develop a synthetic data generator to serve as the basis for fine-tuning and evaluation. Our quantitative evaluation shows that our fine-tuned Mistral model can recover the ground truth simulation model in up to 84.5% of cases. In addition, our small-scale user study demonstrates the model's practical potential for one-time generation as well as interactive modeling in various domains. While promising, in its current form, the fine-tuned small LLM cannot catch up with large LLMs. We conclude that higher-quality training data are required, and expect future small and open-source LLMs to offer new opportunities.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4410830192",
    "type": "article"
  },
  {
    "title": "Interactive geospatial stories for flood management",
    "doi": "https://doi.org/10.1145/3733718",
    "publication_date": "2025-05-28",
    "publication_year": 2025,
    "authors": "Artem Konev; Bernhard Sadransky; Silvana Rauer‐Zechmeister; Daniel Cornel; Ingo Schwerdorf; Jürgen Waser; Milena Vuckovic",
    "corresponding_authors": "",
    "abstract": "With the increasing frequency and severity of flood events worldwide, the need for flood simulations has become more critical than ever, together with the communication of predicted hazards and possible mitigation measures. Since this communication frequently relies on static cartographic representations, it falls short in communicating complex spatial and temporal information and the intricate dynamics of flooding in an engaging and accessible manner. This limitation becomes particularly apparent when addressing diverse audiences with varying levels of expertise. This paper presents an approach that uses interactive geospatial stories to better communicate various aspects of flood management strategies. Our method integrates geospatial visualization with interactive storytelling techniques, creating a compelling platform for engaging stakeholders, fostering understanding and supporting participatory decision-making in flood-prone regions. We discuss the conceptual model, the principles of guided information visualization, and interaction modalities of our web-based, scenario-driven storytelling application. We also evaluate its effectiveness through two exemplary interactive geospatial stories designed to help domain experts explain different aspects of water-sensitive design practices and the use of simulation models for predicting, visualizing, and managing water flows.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4410830373",
    "type": "article"
  },
  {
    "title": "Two-timescale simultaneous perturbation stochastic approximation using deterministic perturbation sequences",
    "doi": "https://doi.org/10.1145/858481.858486",
    "publication_date": "2003-04-01",
    "publication_year": 2003,
    "authors": "Shalabh Bhatnagar; Michael C. Fu; Steven I. Marcus; I-Jeng Wang",
    "corresponding_authors": "",
    "abstract": "Simultaneous perturbation stochastic approximation (SPSA) algorithms have been found to be very effective for high-dimensional simulation optimization problems. The main idea is to estimate the gradient using simulation output performance measures at only two settings of the N -dimensional parameter vector being optimized rather than at the N + 1 or 2 N settings required by the usual one-sided or symmetric difference estimates, respectively. The two settings of the parameter vector are obtained by simultaneously changing the parameter vector in each component direction using random perturbations. In this article, in order to enhance the convergence of these algorithms, we consider deterministic sequences of perturbations for two-timescale SPSA algorithms. Two constructions for the perturbation sequences are considered: complete lexicographical cycles and much shorter sequences based on normalized Hadamard matrices. Recently, one-simulation versions of SPSA have been proposed, and we also investigate these algorithms using deterministic sequences. Rigorous convergence analyses for all proposed algorithms are presented in detail. Extensive numerical experiments on a network of M / G /1 queues with feedback indicate that the deterministic sequence SPSA algorithms perform significantly better than the corresponding randomized algorithms.",
    "cited_by_count": 94,
    "openalex_id": "https://openalex.org/W2125852847",
    "type": "article"
  },
  {
    "title": "Parallel execution for serial simulators",
    "doi": "https://doi.org/10.1145/235025.235031",
    "publication_date": "1996-07-01",
    "publication_year": 1996,
    "authors": "David M. Nicol; Philip Heidelberger",
    "corresponding_authors": "",
    "abstract": "This article describes an approach to discrete event simulation modeling that appears to be effective for developing portable and efficient parallel execution of models of large distributed systems and communication networks. In this approach, the modeler develops submodels with an existing sequential simulation modeling tool, using the full expressive power of the tool. A set of modeling language extensions permits automatically sychronized communication between submodels; however, the automation requires that any such communication must take a nonzero amount of simulation time. Within this modeling paradigm, a variety of conservative synchronization protocols can transparently support conservative execution of submodels on potentially different processors. A specific implementation of this approach, U.P.S. (Utilitarian Parallel Simulator), is described, along with performance results on the Intel Paragon and on the IBM SP2.",
    "cited_by_count": 93,
    "openalex_id": "https://openalex.org/W2048714664",
    "type": "article"
  },
  {
    "title": "HCSM",
    "doi": "https://doi.org/10.1145/217853.217857",
    "publication_date": "1995-07-01",
    "publication_year": 1995,
    "authors": "James F. Cremer; Joseph K. Kearney; Yiannis Papelis",
    "corresponding_authors": "",
    "abstract": "This paper presents HCSM, a framework for behavior and scenario control based on communicating hierarchical, concurrent state machines. We specify the structure and an operational execution model of HCSM's state machines. Without providing formal semantics, we provide enough detail to implement the state machines and an execution engine to run them. HCSM explicitly marries the reactive (or logical) portion of system behavior with the control activities that produce the behavior. HCSM state machines contain activity functions that produce outputs each time a machine is executed. An activity function's output value is computed as a function of accessible external data and the outputs of lower-level state machines. We show how this enables HCSM to model behaviors that involve attending to multiple concurrent concerns and arbitrating between conflicting demands for limited resources. The execution algorithm is free of order dependencies that cause robustness and stability problems in behavior modeling. In addition, we examine the problems of populating virtual environments with autonomous agents exhibiting interesting behavior and of authoring scenarios involving such agents. We argue that HCSM is well suited for modeling the reactive behavior of autonomous agents and for directing such agents to produce desired situations. We demonstrate use of HCSM for modeling vehicle behavior and orchestrating scenarios in the Iowa Driving Simulator, an immersive real-time virtual driving environment.",
    "cited_by_count": 91,
    "openalex_id": "https://openalex.org/W1994043045",
    "type": "article"
  },
  {
    "title": "Eliminating the boundary effect of a large-scale personal communication service network simulation",
    "doi": "https://doi.org/10.1145/175007.175012",
    "publication_date": "1994-04-01",
    "publication_year": 1994,
    "authors": "Yi‐Bing Lin; Victor Mak",
    "corresponding_authors": "",
    "abstract": "Eliminating the boundary effects is an important issue for a large-scale personal communication service (PCS) network simulation. A PCS network is often modeled by a network of hexagonal cells. The boundary may significantly bias the ouput statistics if the number of hexagonal cells is small in a PCS network simulation. On the other hand, if the simulation is to be completed within a reasonable time on the available computing resources, the number of cells in the simulation cannot be too large. To avoid the inaccuracy caused by the boundary effect for a PCS network simulation with limited computing resources, we propose wrapping the hexagonal mesh into a homogeneous graph (i.e., all nodes in the graph are topologically identical). We show that by using the wrapped hexagonal mesh, the inaccuracy of the output measures can be limited even though the number of cells in the simulation is small. We can thus obtain the same statistical accuracy while using significantly less computation power than required for a simulation without cell wrapping.",
    "cited_by_count": 90,
    "openalex_id": "https://openalex.org/W2072941602",
    "type": "article"
  },
  {
    "title": "Cross-entropy and rare events for maximal cut and partition problems",
    "doi": "https://doi.org/10.1145/511442.511444",
    "publication_date": "2002-01-01",
    "publication_year": 2002,
    "authors": "Reuven Y. Rubinstein",
    "corresponding_authors": "Reuven Y. Rubinstein",
    "abstract": "We show how to solve the maximal cut and partition problems using a randomized algorithm based on the cross-entropy method. For the maximal cut problem, the proposed algorithm employs an auxiliary Bernoulli distribution, which transforms the original deterministic network into an associated stochastic one, called the associated stochastic network (ASN). Each iteration of the randomized algorithm for the ASN involves the following two phases:(1) Generation of random cuts using a multidimensional Ber ( p ) distribution and calculation of the associated cut lengths (objective functions) and some related quantities, such as rare-event probabilities.(2) Updating the parameter vector p on the basis of the data collected in the first phase.We show that the Ber ( p ) distribution converges in distribution to a degenerated one, Ber ( p d * ), p d * = ( pd ,1,..., pd,n ) in the sense that someelements of p d * , will be unities and the rest zeros. The unity elements of p d * uniquely define a cut which will be taken as the estimate of the maximal cut. A similar approach is used for the partition problem. Supporting numerical results are given as well. Our numerical studies suggest that for the maximal cut and partition problems the proposed algorithm typically has polynomial complexity in the size of the network.",
    "cited_by_count": 90,
    "openalex_id": "https://openalex.org/W2089739565",
    "type": "article"
  },
  {
    "title": "Web-based simulation",
    "doi": "https://doi.org/10.1145/353735.353736",
    "publication_date": "2000-01-01",
    "publication_year": 2000,
    "authors": "Ernest H. Page; Arnold H. Buss; Paul A. Fishwick; Kevin J. Healy; Richard E. Nance; Ray J. Paul",
    "corresponding_authors": "",
    "abstract": "The nature of the emerging field of web-based simulation is examined in terms of its relationship to the fundamental aspects of simulation research and practice. The presentation, assuming a form of debate, is based on a panel session held at the first International Conference on Web-Based Modeling and Simulation, which was sponsored by the Society for Computer Simulation during 11-14 January 1998 in San Diego, California. While no clear “winner” is evident in this debate, the issues raised here certainly merit ongoing attention and contemplation.",
    "cited_by_count": 90,
    "openalex_id": "https://openalex.org/W2097747860",
    "type": "article"
  },
  {
    "title": "Visualizing real-time multivariate data using preattentive processing",
    "doi": "https://doi.org/10.1145/217853.217855",
    "publication_date": "1995-07-01",
    "publication_year": 1995,
    "authors": "Christopher G. Healey; Kellogg S. Booth; James T. Enns",
    "corresponding_authors": "",
    "abstract": "A new method is presented for visualizing data as they are generated from real-time applications. These techniques allow viewers to perform simple data analysis tasks such as detection of data groups and boundaries, target detection, and estimation. The goal is to do this rapidly and accurately on a dynamic sequence of data frames. Our techniques take advantage of an ability of the human visual system called preattentive processing. Preattentive processing refers to an initial organization of the visual system based on operations believed to be rapid, automatic, and spatially parallel. Examples of visual features that can be detected in this way include hue, orientation, intensity, size, curvature, and line length. We believe that studies from preattentive processing should be used to assist in the design of visualization tools, especially those for which high speed target, boundary, and region detection are important. Previous work has shown that results from research in preattentive processing can be used to build visualization tools that allow rapid and accurate analysis of individual, static data frames. We extend these techniques to a dynamic real-time environment. This allows users to perform similar tasks on dynamic sequences of frames, exactly like those generated by real-time systems such as visual interactive simulation. We studied two known preattentive features, hue and curvature. The primary question investigated was whether rapid and accurate target and boundary detection in dynamic sequences is possible using these features. Behavioral experiments were run that simulated displays from our preattentive visualization tools. Analysis of the results of the experiments showed that rapid and accurate target and boundary detection is possible with both hue and curvature. A second question, whether interactions occur between the two features in a real-time environment, was answered positively.",
    "cited_by_count": 88,
    "openalex_id": "https://openalex.org/W1978322172",
    "type": "article"
  },
  {
    "title": "Modeling train movements through complex rail networks",
    "doi": "https://doi.org/10.1145/974734.974737",
    "publication_date": "2004-01-01",
    "publication_year": 2004,
    "authors": "Quan Lu; Maged Dessouky; Robert C. Leachman",
    "corresponding_authors": "",
    "abstract": "Trains operating in densely populated metropolitan areas typically encounter complex trackage configurations. To make optimal use of the available rail capacity, some portions of the rail network may consist of single-track lines while other locations may consist of double- or triple-track lines. Because of varying local conditions, different points in the rail network may have different speed limits. We formulate a graphical technique for modeling such complex rail networks; and we use this technique to develop a deadlock-free algorithm for dispatching each train to its destination with nearly minimal travel time while (a) abiding by the speed limits at each point on each train's route, and (b) maintaining adequate headways between trains. We implemented this train-dispatching algorithm in a simulation model of the movements of passenger and freight trains in Los Angeles County, and we validated the simulation as yielding an adequate approximation to the current system performance.",
    "cited_by_count": 86,
    "openalex_id": "https://openalex.org/W1987074583",
    "type": "article"
  },
  {
    "title": "Source-oriented topology aggregation with multiple QoS parameters in hierarchical networks",
    "doi": "https://doi.org/10.1145/369534.369542",
    "publication_date": "2000-10-01",
    "publication_year": 2000,
    "authors": "Turgay Korkmaz; Marwan Krunz",
    "corresponding_authors": "",
    "abstract": "In this paper, we investigate the problem of topology aggregation (TA) for scalable, QoS-based routing in hierarchical networks. TA is the process of summarizing the topological information of a subset of network elements. This summary is flooded throughout the network and used by various nodes to determine appropriate routes for connection requests. A key issue in the design of a TA scheme is the appropriate balance between compaction and the corresponding routing performance. The contributions of this paper are twofold. First, we introduce a source-oriented approach to TA, hich provides better performance than existing approaches. The intuition behind this approach is that the advertised topology-state information is used by source nodes to determine tentative routes for connection requests. Accordingly, only information relevant to source nodes needs to be advertised. We integrate the source-oriented approach into three new TA schemes that provide different trade-offs between compaction and accuracy. Second, we extend our source-oriented approach to multi-QoS-based TA. A key issue here is the determination of appropriate values for the multiple QoS parameters associated with a logical link. Two new approaches to computing these values are introduced. Extensive simulations are used to evaluate the performance of out proposed schemes.",
    "cited_by_count": 84,
    "openalex_id": "https://openalex.org/W2049996330",
    "type": "article"
  },
  {
    "title": "Polynomial arithmetic analogue of Halton sequences",
    "doi": "https://doi.org/10.1145/169702.169694",
    "publication_date": "1993-04-01",
    "publication_year": 1993,
    "authors": "Shu Tezuka",
    "corresponding_authors": "Shu Tezuka",
    "abstract": "article Polynomial arithmetic analogue of Halton sequences Share on Author: Shu Tezuka IBM Research, Tokyo, Research Laboratory IBM Research, Tokyo, Research LaboratoryView Profile Authors Info & Claims ACM Transactions on Modeling and Computer SimulationVolume 3Issue 2April 1993 pp 99–107https://doi.org/10.1145/169702.169694Online:01 April 1993Publication History 54citation657DownloadsMetricsTotal Citations54Total Downloads657Last 12 Months6Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",
    "cited_by_count": 84,
    "openalex_id": "https://openalex.org/W2071735635",
    "type": "article"
  },
  {
    "title": "Calculation of confidence intervals for simulation output",
    "doi": "https://doi.org/10.1145/1029174.1029176",
    "publication_date": "2004-10-01",
    "publication_year": 2004,
    "authors": "Russell Cheng; W. S. Holland",
    "corresponding_authors": "",
    "abstract": "This article is concerned with the calculation of confidence intervals for simulation output that is dependent on two sources of variability. One, referred to as &lt;i&gt;simulation variability&lt;/i&gt;, arises from the use of random numbers in the simulation itself; and the other, referred to as &lt;i&gt;parameter variability&lt;/i&gt;, arises when the input parameters are unknown and have to be estimated from observed data. Three approaches to the calculation of confidence intervals are presented--the traditional asymptotic normality theory approach, a bootstrap approach and a new method which produces a conservative approximation based on performing just two simulation runs at carefully selected parameter settings. It is demonstrated that the traditional and bootstrap approaches provide similar degrees of accuracy and that whilst the new method may sometimes be very conservative, it can be calculated in a small fraction of the computational time of the exact methods.",
    "cited_by_count": 83,
    "openalex_id": "https://openalex.org/W1983701869",
    "type": "article"
  },
  {
    "title": "To batch or not to batch?",
    "doi": "https://doi.org/10.1145/974734.974738",
    "publication_date": "2004-01-01",
    "publication_year": 2004,
    "authors": "Christos Alexopoulos; David Goldsman",
    "corresponding_authors": "",
    "abstract": "When designing steady-state computer simulation experiments, one may be faced with the choice of batching observations in one long run or replicating a number of smaller runs. Both methods are potentially useful in the course of undertaking simulation output analysis. The tradeoffs between the two alternatives are well known: batching ameliorates the effects of initialization bias, but produces batch means that might be correlated; replication yields independent sample means, but may suffer from initialization bias at the beginning of each of the runs. We present several new results and specific examples to lend insight as to when one method might be preferred over the other. In steady-state, batching and replication perform similarly in terms of estimating the mean and variance parameter, but replication tends to do better than batching with regard to the performance of confidence intervals for the mean. Such a victory for replication may be hollow, for in the presence of an initial transient, batching often performs better than replication when it comes to point and confidence-interval estimation of the steady-state mean. We conclude---like other classic references---that in the context of estimation of the steady-state mean, batching is typically the wiser approach.",
    "cited_by_count": 83,
    "openalex_id": "https://openalex.org/W2006343645",
    "type": "article"
  },
  {
    "title": "Continuous random variate generation by fast numerical inversion",
    "doi": "https://doi.org/10.1145/945511.945517",
    "publication_date": "2003-10-01",
    "publication_year": 2003,
    "authors": "Wolfgang Hörmann; Josef Leydold",
    "corresponding_authors": "",
    "abstract": "The inversion method for generating nonuniform random variates has some advantages compared to other generation methods, since it monotonically transforms uniform random numbers into non-uniform random variates. Hence, it is the method of choice in the simulation literature. However, except for some simple cases where the inverse of the cumulative distribution function is a simple function we need numerical methods. Often inversion by \"brute force\" is used, applying either very slow iterative methods or linear interpolation of the CDF and huge tables. But then the user has to accept unnecessarily large errors or excessive memory requirements, that slow down the algorithm. In this article, we demonstrate that with Hermite interpolation of the inverse CDF we can obtain very small error bounds close to machine precision. Using our adaptive interval splitting method, this accuracy is reached with moderately sized tables that allow for a fast and simple generation procedure.",
    "cited_by_count": 82,
    "openalex_id": "https://openalex.org/W2010377171",
    "type": "article"
  },
  {
    "title": "A study of time warp rollback mechanisms",
    "doi": "https://doi.org/10.1145/102810.102813",
    "publication_date": "1991-01-03",
    "publication_year": 1991,
    "authors": "Yi‐Bing Lin; Edward D. Lazowska",
    "corresponding_authors": "",
    "abstract": "The Time Warp “optimistic” approach is one of the most important parallel simulation protocols. Time Warp synchronizes processes via rollback . The original rollback mechanism called lazy cancellation has aroused great interest. This paper studies these rollback mechanisms. The general tradeoffs between aggressive and lazy cancellation are discussed, and by a conservitive-optimal simulation is defined for comparitive purposes. Within the framework of aggressive cancellation, we offer some observations and analyze the rollback behavior of tandom systems. The lazy cancellation mechanism iss examined using a metric called the sensitivity of output message . Both aggressive and lazy cancellation are shown to work well for a process with a small simulated load intensity. Finally, an analytical model is given to analyze message preemtion , an important factor that affects the performance of rollback mechanisms. Results indicate that message preemtion has a significant effect on performance when (1) the processor is highly utilized, (2) the execution times of messages have high varience, and (3) rollbacks occur frequently.",
    "cited_by_count": 80,
    "openalex_id": "https://openalex.org/W1988078107",
    "type": "article"
  },
  {
    "title": "Adaptive multivariate three-timescale stochastic approximation algorithms for simulation based optimization",
    "doi": "https://doi.org/10.1145/1044322.1044326",
    "publication_date": "2005-01-01",
    "publication_year": 2005,
    "authors": "Shalabh Bhatnagar",
    "corresponding_authors": "Shalabh Bhatnagar",
    "abstract": "We develop in this article, four adaptive three-timescale stochastic approximation algorithms for simulation optimization that estimate both the gradient and Hessian of average cost at each update epoch. These algorithms use four, three, two, and one simulation(s), respectively, and update the values of the decision variable and Hessian matrix components simultaneously, with estimates based on the simultaneous perturbation methodology. Our algorithms use coupled stochastic recursions that proceed using three different timescales or step-size schedules. We present a detailed convergence analysis of the algorithms and show numerical experiments using all the developed algorithms on a two-node network of M/G/1 queues with feedback for a 50-dimensional parameter vector. We provide comparisons of the performance of these algorithms with two recently developed two-timescale steepest descent simultaneous perturbation analogs that use randomized and deterministic perturbation sequences, respectively. We also present experiments to explore the sensitivity of the algorithms to their associated parameters. The algorithms that use four and three simulations, respectively, perform significantly better than the rest of the algorithms.",
    "cited_by_count": 78,
    "openalex_id": "https://openalex.org/W2150152677",
    "type": "article"
  },
  {
    "title": "Analysis of parallel replicated simulations under a completion time constraint",
    "doi": "https://doi.org/10.1145/102810.102811",
    "publication_date": "1991-01-03",
    "publication_year": 1991,
    "authors": "Peter W. Glynn; Philip Heidelberger",
    "corresponding_authors": "",
    "abstract": "We analyze properties associated with a simple yet effective way to exploit parallel processors in discrete event simulations: averaging the results of multiple, independent replications that are run, in parallel, on multiple processors. We focus on estimating expectations from terminating simulations, or steady state parameters from regenerative simulations. We assume that there is a CPU time constraint, t , on each of P processors. Unless the replication lengths are bounded, one must be willing to simulate beyond any fixed, finite time t on at least some processors in order to always obtain a strongly consistent estimator (as the number of processors increases). We therefore consider simulation experiments in which t is viewed as either being a strict constraint, or a guideline, in which case simulation beyond time t is permitted. The statistical properties, including strong laws, central limit theorems, bias expansions, and completion time distributions of a variety of estimators obtainable from such an experiment are derived. We propose an unbiased estimator for a simple mean value. This estimator requires preselecting a fraction of the processors. Simulation beyond time t may be required on a preselected processor, but only if no replications have yet been completed on that processor. being a strict constraint, or a guideline, in which case simulation beyond time t is permitted. The statistical properties, including strong laws, central limit theorems, bias expansions, and completion time distributions of a variety of estimators obtainable from such an experiment are derived. We propose an unbiased estimator for a simple mean value. This estimator requires preselecting a fraction of the processors. Simulation beyond time t may be required on a preselected processor, but only if no replications have yet been completed on that processor.",
    "cited_by_count": 77,
    "openalex_id": "https://openalex.org/W1987801042",
    "type": "article"
  },
  {
    "title": "A behavioral theory of insider-threat risks",
    "doi": "https://doi.org/10.1145/1346325.1346328",
    "publication_date": "2008-04-01",
    "publication_year": 2008,
    "authors": "Ignacio J. Martinez‐Moyano; Eliot Rich; Stephen Conrad; David F. Andersen; Thomas R. Stewart",
    "corresponding_authors": "",
    "abstract": "The authors describe a behavioral theory of the dynamics of insider-threat risks. Drawing on data related to information technology security violations and on a case study created to explain the dynamics observed in that data, the authors constructed a system dynamics model of a theory of the development of insider-threat risks and conducted numerical simulations to explore the parameter and response spaces of the model. By examining several scenarios in which attention to events, increased judging capabilities, better information, and training activities are simulated, the authors theorize about why information technology security effectiveness changes over time. The simulation results argue against the common presumption that increased security comes at the cost of reduced production.",
    "cited_by_count": 75,
    "openalex_id": "https://openalex.org/W2060821979",
    "type": "article"
  },
  {
    "title": "A time-division algorithm for parallel simulation",
    "doi": "https://doi.org/10.1145/102810.214307",
    "publication_date": "1991-01-03",
    "publication_year": 1991,
    "authors": "Yi‐Bing Lin; Edward D. Lazowska",
    "corresponding_authors": "",
    "abstract": "article Free AccessA time-division algorithm for parallel simulation Share on Authors: Yi-Bing Lin Department of Computer Science and Engineering, University of Washington, Seattle, WA Department of Computer Science and Engineering, University of Washington, Seattle, WAView Profile , Edward D. Lazowska Department of Computer Science and Engineering, University of Washington, Seattle, WA Department of Computer Science and Engineering, University of Washington, Seattle, WAView Profile Authors Info & Claims ACM Transactions on Modeling and Computer SimulationVolume 1Issue 1Jan. 1991 pp 73–83https://doi.org/10.1145/102810.214307Online:03 January 1991Publication History 59citation377DownloadsMetricsTotal Citations59Total Downloads377Last 12 Months9Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 73,
    "openalex_id": "https://openalex.org/W2024487458",
    "type": "article"
  },
  {
    "title": "Distributed simulation of agent-based systems with HLA",
    "doi": "https://doi.org/10.1145/1243991.1243992",
    "publication_date": "2007-07-01",
    "publication_year": 2007,
    "authors": "Michael Lees; Brian Logan; Georgios Theodoropoulos",
    "corresponding_authors": "",
    "abstract": "In this article we describe HLA_AGENT, a tool for the distributed simulation of agent-based systems, which integrates the SIM_AGENT agent toolkit and the High Level Architecture (HLA) simulator interoperability framework. HLA_AGENT offers enhanced simulation scalability and allows interoperation with other HLA-compliant simulators, promoting simulation reuse. Using a simple Tileworld example, we show how HLA_AGENT can be used to flexibly distribute a SIM_AGENT simulation so as to exploit available computing resources. We present experimental results that illustrate the performance of HLA_AGENT on a Linux cluster running a distributed version of Tileworld and compare this with the original nondistributed SIM_AGENT version.",
    "cited_by_count": 65,
    "openalex_id": "https://openalex.org/W1974804570",
    "type": "article"
  },
  {
    "title": "Simulation optimization using the cross-entropy method with optimal computing budget allocation",
    "doi": "https://doi.org/10.1145/1667072.1667076",
    "publication_date": "2010-01-01",
    "publication_year": 2010,
    "authors": "Donghai He; Loo Hay Lee; Chun‐Hung Chen; Michael C. Fu; Segev Wasserkrug",
    "corresponding_authors": "",
    "abstract": "We propose to improve the efficiency of simulation optimization by integrating the notion of optimal computing budget allocation into the Cross-Entropy (CE) method, which is a global optimization search approach that iteratively updates a parameterized distribution from which candidate solutions are generated. This article focuses on continuous optimization problems. In the stochastic simulation setting where replications are expensive but noise in the objective function estimate could mislead the search process, the allocation of simulation replications can make a significant difference in the performance of such global optimization search algorithms. A new allocation scheme is developed based on the notion of optimal computing budget allocation. The proposed approach improves the updating of the sampling distribution by carrying out this computing budget allocation in an efficient manner, by minimizing the expected mean-squared error of the CE weight function. Numerical experiments indicate that the computational efficiency of the CE method can be substantially improved if the ideas of computing budget allocation are applied.",
    "cited_by_count": 63,
    "openalex_id": "https://openalex.org/W2078133463",
    "type": "article"
  },
  {
    "title": "Integrated simulation and optimization for wildfire containment",
    "doi": "https://doi.org/10.1145/1596519.1596524",
    "publication_date": "2009-10-01",
    "publication_year": 2009,
    "authors": "Xiaolin Hu; Lewis Ntaimo",
    "corresponding_authors": "",
    "abstract": "Wildfire containment is an important but challenging task. The ability to predict fire spread behavior, optimize a plan for firefighting resource dispatch and evaluate such a plan using several firefighting tactics is essential for supporting decision making for containing wildfires. In this article, we present an integrated framework for wildfire spread simulation, firefighting resource optimization and wildfire suppression simulation. We present a stochastic mixed-integer programming model for initial attack to generate firefighting resource dispatch plans using as input fire spread scenario results from a standard wildfire behavior simulator. A new agent-based discrete event simulation model for fire suppression is used to simulate fire suppression based on dispatch plans from the stochastic optimization model, and in turn provides feedback to the optimization model for revising the dispatch plans if necessary. We report on several experimental results, which demonstrate that different firefighting tactics can lead to significantly different fire suppression results for a given dispatch plan, and simulation of these tactics can provide valuable information for fire managers in selecting dispatch plans from optimization models before actual implementation in the field.",
    "cited_by_count": 62,
    "openalex_id": "https://openalex.org/W1990887640",
    "type": "article"
  },
  {
    "title": "A New Algorithm for Simulating Wildfire Spread through Cellular Automata",
    "doi": "https://doi.org/10.1145/2043635.2043641",
    "publication_date": "2011-12-01",
    "publication_year": 2011,
    "authors": "Giuseppe A. Trunfio; Donato D’Ambrosio; Rocco Rongo; William Spataro; Salvatore Di Gregorio",
    "corresponding_authors": "",
    "abstract": "Cell-based methods for simulating wildfires can be computationally more efficient than techniques based on the fire perimeter expansion. In spite of this, their success has been limited by the distortions that plague the simulated shapes. This article presents a novel algorithm for wildfire simulation through Cellular Automata (CA), which is able to effectively mitigate the problem of distorted fire shapes. Such a result is obtained allowing spread directions that are not constrained to the few angles imposed by the lattice of cells and the neighborhood size. The characteristics of the proposed algorithm are empirically investigated under homogeneous conditions through some comparisons with the outcomes of a typical CA-based simulator. Also, using two significant heterogeneous landscapes, a comparison with the vector-based simulator FARSITE is discussed. According to the results of this study, the proposed approach performs significantly better, in terms of accuracy, than the CA taken as reference. In addition, at a far less computational cost, it provides burned regions that are equivalent, for practical purposes, to those given by FARSITE.",
    "cited_by_count": 61,
    "openalex_id": "https://openalex.org/W2034167956",
    "type": "article"
  },
  {
    "title": "An integrated human decision making model for evacuation scenarios under a BDI framework",
    "doi": "https://doi.org/10.1145/1842722.1842728",
    "publication_date": "2010-10-01",
    "publication_year": 2010,
    "authors": "Seungho Lee; Young‐Jun Son; Judy Jin",
    "corresponding_authors": "",
    "abstract": "An integrated Belief-Desire-Intention (BDI) modeling framework is proposed for human decision making and planning for evacuation scenarios, whose submodules are based on a Bayesian Belief Network (BBN), Decision-Field-Theory (DFT), and a Probabilistic Depth-First Search (PDFS) technique. A key novelty of the proposed model is its ability to represent both the human decision-making and decision-planning functions in a unified framework. To mimic realistic human behaviors, attributes of the BDI framework are reverse-engineered from human-in-the-loop experiments conducted in the Cave Automatic Virtual Environment (CAVE). The proposed modeling framework is demonstrated for a human's evacuation behaviors in response to a terrorist bomb attack. The simulated environment and agents (models of humans) conforming to the proposed BDI framework are implemented in AnyLogic® agent-based simulation software, where each agent calls external Netica BBN software to perform its perceptual processing function and Soar software to perform its real-time planning and decision-execution functions. The constructed simulation has been used to test the impact of several factors (e.g., demographics, number of police officers, information sharing via speakers) on evacuation performance (e.g., average evacuation time, percentage of casualties).",
    "cited_by_count": 61,
    "openalex_id": "https://openalex.org/W2062467034",
    "type": "article"
  },
  {
    "title": "CDNsim",
    "doi": "https://doi.org/10.1145/1734222.1734226",
    "publication_date": "2010-04-01",
    "publication_year": 2010,
    "authors": "Kostas Stamos; George Pallis; Athena Vakali; Dimitrios Katsaros; Antonis Sidiropoulos; Yannis Manolopoulos",
    "corresponding_authors": "",
    "abstract": "Content distribution networks (CDNs) have gained considerable attention in the past few years. Hence there is need for developing frameworks for carrying out CDN simulations. In this article we present a modeling and simulation framework for CDNs, called CDNsim. CDNsim has been designated to provide a realistic simulation for CDNs, simulating the surrogate servers, the TCP/IP protocol, and the main CDN functions. The main advantages of this tool are its high performance, its extensibility, and its user interface, which is used to configure its parameters. CDNsim provides an automated environment for conducting experiments and extracting client, server, and network statistics. The purpose of CDNsim is to be used as a testbed for CDN evaluation and experimentation. This is quite useful to both the research community (to experiment with new CDN data management techniques), and for CDN developers (to evaluate profits on prior certain CDN installations).",
    "cited_by_count": 59,
    "openalex_id": "https://openalex.org/W2054152361",
    "type": "article"
  },
  {
    "title": "A modeling framework that combines markov models and discrete-event simulation for stroke patient care",
    "doi": "https://doi.org/10.1145/2000494.2000498",
    "publication_date": "2011-08-01",
    "publication_year": 2011,
    "authors": "Sally McClean; Maria Barton; Lalit Garg; Ken Fullerton",
    "corresponding_authors": "",
    "abstract": "Stroke disease places a heavy burden on society, incurring long periods of hospital and community care. Also stroke is a highly complex disease with heterogeneous outcomes and multiple strategies for therapy and care. In this article we develop a modeling framework that clusters patients with respect to their length of stay (LOS); phase-type models are then used to describe patient flows for each cluster. In most cases, there are multiple outcomes, such as discharge to normal residence, nursing home, or death. We therefore derive a novel analytical model for the distribution of LOS in such situations. A model of the whole care system is developed, based on Poisson admissions to hospital, and results obtained for expected numbers in different states of the system at any time. We can thus describe the whole integrated system of stroke patient care, which will facilitate planning of services. We also use the basic model to build a discrete-event simulation, which incorporates back-up queues to model delayed discharge. Based on stroke patients' data from the Belfast City Hospital, various scenarios are explored with a focus on the potential efficiency gains if LOS, prior to discharge to a private nursing home, can be reduced. Predictions for bed occupancy are also provided. The overall modeling framework characterizes the behavior of stroke patient populations, with a focus on integrated system-wide planning, encompassing hospital and community services. Within this general framework we can develop either analytic or simulation models that take account of patient heterogeneity and multiple care options.",
    "cited_by_count": 58,
    "openalex_id": "https://openalex.org/W1972065863",
    "type": "article"
  },
  {
    "title": "Guest editors' introduction to special issue on the first INFORMS simulation society research workshop",
    "doi": "https://doi.org/10.1145/1667072.1667073",
    "publication_date": "2010-01-01",
    "publication_year": 2010,
    "authors": "Stephen E. Chick; Enver Yücesan",
    "corresponding_authors": "",
    "abstract": "introduction Share on Guest editors' introduction to special issue on the first INFORMS simulation society research workshop Authors: Stephen E. Chick INSEAD, Technology and Operations Management Area, Fontainebleau Cedex, France INSEAD, Technology and Operations Management Area, Fontainebleau Cedex, FranceView Profile , Enver Yücesan INSEAD, Technology and Operations Management Area, Fontainebleau Cedex, France INSEAD, Technology and Operations Management Area, Fontainebleau Cedex, FranceView Profile Authors Info & Claims ACM Transactions on Modeling and Computer SimulationVolume 20Issue 1Article No.: 1pp 1–3https://doi.org/10.1145/1667072.1667073Published:08 February 2010Publication History 0citation172DownloadsMetricsTotal Citations0Total Downloads172Last 12 Months1Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",
    "cited_by_count": 58,
    "openalex_id": "https://openalex.org/W2091172490",
    "type": "article"
  },
  {
    "title": "The Effect of Robust Decisions on the Cost of Uncertainty in Military Airlift Operations",
    "doi": "https://doi.org/10.1145/2043635.2043636",
    "publication_date": "2011-12-01",
    "publication_year": 2011,
    "authors": "Warren B. Powell; Belgacem Bouzaïene-Ayari; Jean Berger; Abdeslem Boukhtouta; Abraham George",
    "corresponding_authors": "",
    "abstract": "There are a number of sources of randomness that arise in military airlift operations. However, the cost of uncertainty can be difficult to estimate, and is easy to overestimate if we use simplistic decision rules. Using data from Canadian military airlift operations, we study the effect of uncertainty in customer demands as well as aircraft failures, on the overall cost. The system is first analyzed using the types of myopic decision rules widely used in the research literature. The performance of the myopic policy is then compared to the results obtained using robust decisions that account for the uncertainty of future events. These are obtained by modeling the problem as a dynamic program, and solving Bellman’s equations using approximate dynamic programming. The experiments show that even approximate solutions to Bellman’s equations produce decisions that reduce the cost of uncertainty.",
    "cited_by_count": 55,
    "openalex_id": "https://openalex.org/W2131579983",
    "type": "article"
  },
  {
    "title": "Stochastic kriging with biased sample estimates",
    "doi": "https://doi.org/10.1145/2567893",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "Xi Chen; Kyoung-Kuk Kim",
    "corresponding_authors": "",
    "abstract": "Stochastic kriging has been studied as an effective metamodeling technique for approximating response surfaces in the context of stochastic simulation. In a simulation experiment, an analyst typically needs to estimate relevant metamodel parameters and further do prediction; therefore, the impact of parameter estimation on the performance of the metamodel-based predictor has drawn some attention in the literature. However, how the standard stochastic kriging predictor is affected by the presence of bias in finite-sample estimates has not yet been fully investigated. In this article, we study the predictive performance and investigate optimal budget allocation rules subject to a fixed computational budget constraint. Furthermore, we extend the analysis to two-level or nested simulation, which has been recently documented in the risk management literature, with biased estimators.",
    "cited_by_count": 50,
    "openalex_id": "https://openalex.org/W2138103271",
    "type": "article"
  },
  {
    "title": "Confidence Intervals for Quantiles Using Sectioning When Applying Variance-Reduction Techniques",
    "doi": "https://doi.org/10.1145/2558328",
    "publication_date": "2014-05-07",
    "publication_year": 2014,
    "authors": "Marvin K. Nakayama",
    "corresponding_authors": "Marvin K. Nakayama",
    "abstract": "We develop confidence intervals (CIs) for quantiles when applying variance-reduction techniques (VRTs) and sectioning. Similar to batching, sectioning partitions the independent and identically distributed (i.i.d.) outputs into nonoverlapping batches and computes a quantile estimator from each batch. But rather than centering the CI at the average of the quantile estimators across the batches, as in batching, sectioning centers the CI at the overall quantile estimator based on all the outputs. A similar modification is made to the sample variance, which is used to determine the width of the CI. We establish the asymptotic validity of the sectioning CI for importance sampling and control variates, and the proofs rely on first showing that the corresponding quantile estimators satisfy a Bahadur representation, which we have done in prior work. Here, we present some numerical results.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W2113115437",
    "type": "article"
  },
  {
    "title": "Rare-event Simulation for Neural Network and Random Forest Predictors",
    "doi": "https://doi.org/10.1145/3519385",
    "publication_date": "2022-03-07",
    "publication_year": 2022,
    "authors": "Yuanlu Bai; Zhiyuan Huang; Henry Lam; Ding Zhao",
    "corresponding_authors": "",
    "abstract": "We study rare-event simulation for a class of problems where the target hitting sets of interest are defined via modern machine learning tools such as neural networks and random forests. This problem is motivated from fast emerging studies on the safety evaluation of intelligent systems, robustness quantification of learning models, and other potential applications to large-scale simulation in which machine learning tools can be used to approximate complex rare-event set boundaries. We investigate an importance sampling scheme that integrates the dominating point machinery in large deviations and sequential mixed integer programming to locate the underlying dominating points. Our approach works for a range of neural network architectures including fully connected layers, rectified linear units, normalization, pooling and convolutional layers, and random forests built from standard decision trees. We provide efficiency guarantees and numerical demonstration of our approach using a classification model in the UCI Machine Learning Repository.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W3092003702",
    "type": "article"
  },
  {
    "title": "Tables of 64-bit Mersenne twisters",
    "doi": "https://doi.org/10.1145/369534.369540",
    "publication_date": "2000-10-01",
    "publication_year": 2000,
    "authors": "Takuji Nishimura",
    "corresponding_authors": "Takuji Nishimura",
    "abstract": "We give new parameters for a Mersenne Twister pseudorandom number gene rator for 64-bit word machines.",
    "cited_by_count": 79,
    "openalex_id": "https://openalex.org/W2000397086",
    "type": "article"
  },
  {
    "title": "Confidence intervals using orthonormally weighted standardized time series",
    "doi": "https://doi.org/10.1145/352222.352223",
    "publication_date": "1999-10-01",
    "publication_year": 1999,
    "authors": "Robert D. Foley; David Goldsman",
    "corresponding_authors": "",
    "abstract": "We extend the standardized time series area method for constructing confidence intervals for the mean of a stationary stochastic process. The proposed intervals are based on orthonormally weighted standardized time series area variance estimators. The underlying area estimators possess two important properties: they are first-order unbiased, and they are asymptotically independent of each other. These properties are largely the result of a careful choice of weighting functions, which we explicitly describe. The asymptotic independence of the area estimators yields more degrees of freedom than various predecessors; this, in turn, produces smaller mean and variance of the length of the resulting confidence intervals. We illustrate the efficacy of the new procedure via exact and Monte Carlo examples. We also provide suggestions for efficient implementation of the method.",
    "cited_by_count": 75,
    "openalex_id": "https://openalex.org/W2033739720",
    "type": "article"
  },
  {
    "title": "Empirical evidence concerning AES",
    "doi": "https://doi.org/10.1145/945511.945515",
    "publication_date": "2003-10-01",
    "publication_year": 2003,
    "authors": "Peter Hellekalek; Stefan Wegenkittl",
    "corresponding_authors": "",
    "abstract": "AES, the Advanced Encryption Standard, is one of the most important algorithms in modern cryptography. Certain randomness properties of AES are of vital importance for its security. At the same time, these properties make AES an interesting candidate for a fast nonlinear random number generator for stochastic simulation. In this article, we address both of these two aspects of AES. We study the performance of AES in a series of statistical tests that are related to cryptographic notions like confusion and diffusion. At the same time, these tests provide empirical evidence for the suitability of AES in stochastic simulation. A substantial part of this article is devoted to the strategy behind our tests and to their relation to other important test statistics like Maurer's Universal Test.",
    "cited_by_count": 75,
    "openalex_id": "https://openalex.org/W2033803521",
    "type": "article"
  },
  {
    "title": "A qualitative simulation approach for fuzzy dynamical models",
    "doi": "https://doi.org/10.1145/200883.200884",
    "publication_date": "1994-10-01",
    "publication_year": 1994,
    "authors": "Andrea Bonarini; Gianluca Bontempi",
    "corresponding_authors": "",
    "abstract": "This article deals with simulation of approximate models of dynamic systems. We propose an approach that is appropriate when the uncertainty intrinsic in some models cannot be reduced by traditional identification techniques, due to the impossibility of gathering experimental data about the system itself. The article presents a methodology for qualitative modeling and simulation of approximately known systems. The proposed solution is based on the Fuzzy Sets theory, extending the power of traditional numerical-logical methods. We have implemented a fuzzy simulator that integrates a fuzzy, qualitative approach and traditional, quantitative methods.",
    "cited_by_count": 73,
    "openalex_id": "https://openalex.org/W2049071798",
    "type": "article"
  },
  {
    "title": "Performance and reliability analysis of relevance filtering for scalable distributed interactive simulation",
    "doi": "https://doi.org/10.1145/259207.259209",
    "publication_date": "1997-07-01",
    "publication_year": 1997,
    "authors": "Mostafa Bassiouni; Ming-Hsing Chiu; Margaret L. Loper; Michael Garnsey; Jim Williams",
    "corresponding_authors": "",
    "abstract": "Achieving the real-time linkage among multiple, geographically-distant, local area networks that support distributed interactive simulation (DIS) requires tremendous bandwidth and communication resources. Today, meeting the bandwidth and communication requirements of DIS is one of the major challenges facing the design and implementation of large scale DIS training exercises. In this article, we discuss the DIS scalability problem, briefly overview the major bandwidth reduction techniques currently being investigated and implemented in contemporary DIS systems, and present a detailed analysis on the performance and reliability of relevance filtering—a promising technique to improve the scalability of distributed simulation. The idea of relevance filtering is to analyze the semantic contents of the state update messages of a simulated entity (vehicle) and transmit only the ones found to be relevant to other entities. We present our entity-based model for relevance filtering and discuss the implementation of filtering-at-transmission and filtering-at-reception. We introduce the concept of filtering reliability and present different methods to eliminate or reduce filtering errors. Methods that can ensure complete filtering reliability while providing significant bandwidth reduction are developed. Performance evaluation results of relevance filtering and of the filtering reliability methods are presented. The insight gained from our work and the challenges still facing the design of large scale DIS training exercises are discussed.",
    "cited_by_count": 73,
    "openalex_id": "https://openalex.org/W2055899756",
    "type": "article"
  },
  {
    "title": "The mean square discrepancy of randomized nets",
    "doi": "https://doi.org/10.1145/240896.240909",
    "publication_date": "1996-10-01",
    "publication_year": 1996,
    "authors": "Fred J. Hickernell",
    "corresponding_authors": "Fred J. Hickernell",
    "abstract": "One popular family of low dicrepancy sets is the ( t, m, s )-nets. Recently a randomization of these nets that preserves their net property has been introduced. In this article a formula for the mean square L 2 -discrepancy of ( 0, m, s )-nets in base b is derived. This formula has a computational complexity of only O(s log( N ) + s 2 ) for large N or s, where N = b m is the number of points. Moreover, the root mean square L 2 -discrepancy of ( 0, m, s )-nets is show to be O( N -1 [log(N)] (s-1)/2 ) as N tends to infinity, the same asymptotic order as the known lower bound for the L 2 -discrepancy of an arbitrary set.",
    "cited_by_count": 73,
    "openalex_id": "https://openalex.org/W2083847167",
    "type": "article"
  },
  {
    "title": "A unifying framework for distributed simulation",
    "doi": "https://doi.org/10.1145/130611.130614",
    "publication_date": "1991-10-01",
    "publication_year": 1991,
    "authors": "Rajive Bagrodia; K. Mani Chandy; Wen Toh Liao",
    "corresponding_authors": "",
    "abstract": "A theory of distributed simulation applicable to both discrete-event and continuous simulation is presented. It derives many existing simulation algorithms from the theory and describes an implementation of a new algorithm derived from the theory. A high-level discrete-event simulation language has been implemented, using the new algorithm, on parallel computers; performance results of the implementation are also presented.",
    "cited_by_count": 72,
    "openalex_id": "https://openalex.org/W2028897999",
    "type": "article"
  },
  {
    "title": "A modification of the process interaction world view",
    "doi": "https://doi.org/10.1145/137926.137927",
    "publication_date": "1992-04-01",
    "publication_year": 1992,
    "authors": "Bruce A. Cota; Robert G. Sargent",
    "corresponding_authors": "",
    "abstract": "A formal definition of the process interaction world view is reviewed. A shortcoming of this world view is identified with respect to modularity and encapsulation. A modification of the world view that supports modularity and encapsulation is presented, and its advantages are discussed. It is shown how some recent process-oriented simulation languages support this modified world view. The impact of this modification on parallel simulation is discussed, and the modified world view is briefly compared to the use of object-oriented simulation modeling.",
    "cited_by_count": 70,
    "openalex_id": "https://openalex.org/W2086729363",
    "type": "article"
  },
  {
    "title": "MAYA",
    "doi": "https://doi.org/10.1145/985793.985796",
    "publication_date": "2004-04-01",
    "publication_year": 2004,
    "authors": "Junlan Zhou; Zhengrong Ji; Mineo Takai; Rajive Bagrodia",
    "corresponding_authors": "",
    "abstract": "The flourish of large-scale network applications across the Internet and or MANET has raised a challenge to network modeling environments that support experimentation and analysis of close interactions between real applications and network dynamics. To facilitate such experimentations, this paper presents MAYA , a multiparadigm network modeling framework including discrete event models, analytical models and physical network interfaces, together with its illustrative implementation using QualNet, fluid flow TCP model and physical network interface. MAYA framework allows users to interface simulated networks directly with physical networks, while attaining real-time constraints even for large-scale networks by incorporating above multiparadigm network modeling techniques. It also gives user the flexibility to emulate applications on nodes in both real and simulated networks. Experiments are conducted to validate the interoperation of QualNet and fluid flow model, to examine the performance of MAYA as well as to evaluate the optimization techniques, namely interleaved execution of fluid flow model and causality-preserve realtime synchronization relaxation. Experimental results indicate that MAYA is a scalable and extensible solution to modeling of close interactions between real application and network dynamics.",
    "cited_by_count": 69,
    "openalex_id": "https://openalex.org/W1985332874",
    "type": "article"
  },
  {
    "title": "A Bernoulli factory",
    "doi": "https://doi.org/10.1145/175007.175019",
    "publication_date": "1994-04-01",
    "publication_year": 1994,
    "authors": "Michaël Keane; G. L. O’Brien",
    "corresponding_authors": "",
    "abstract": "Necessary and sufficient conditions on a function f(p) are given for the existence of a simulation procedure to simulate a Bernoulli variable with success probability f(p) from independent Bernoulli variables with success probability p, with p being constrained to lie in a subset P of [0,1] but otherwise unknown —Authors' Abstract .",
    "cited_by_count": 64,
    "openalex_id": "https://openalex.org/W1985617186",
    "type": "article"
  },
  {
    "title": "Reducing parameter uncertainty for stochastic systems",
    "doi": "https://doi.org/10.1145/1122012.1122014",
    "publication_date": "2006-01-01",
    "publication_year": 2006,
    "authors": "Szu Hui Ng; Stephen E. Chick",
    "corresponding_authors": "",
    "abstract": "The design of many production and service systems is informed by stochastic model analysis. But the parameters of statistical distributions of stochastic models are rarely known with certainty, and are often estimated from field data. Even if the mean system performance is a known function of the model's parameters, there may still be uncertainty about the mean performance because the parameters are not known precisely. Several methods have been proposed to quantify this uncertainty, but data sampling plans have not yet been provided to reduce parameter uncertainty in a way that effectively reduces uncertainty about mean performance. The optimal solution is challenging, so we use asymptotic approximations to obtain closed-form results for sampling plans. The results apply to a wide class of stochastic models, including situations where the mean performance is unknown but estimated with simulation. Analytical and empirical results for the M/M/1 queue, a quadratic response-surface model, and a simulated critical care facility illustrate the ideas.",
    "cited_by_count": 63,
    "openalex_id": "https://openalex.org/W2047466230",
    "type": "article"
  },
  {
    "title": "A sort-based DDM matching algorithm for HLA",
    "doi": "https://doi.org/10.1145/1044322.1044324",
    "publication_date": "2005-01-01",
    "publication_year": 2005,
    "authors": "Côme Raczy; Gary Tan; Jiangbo Yu",
    "corresponding_authors": "",
    "abstract": "The High Level Architecture (HLA) is an architecture for reuse and interoperation of simulations. It provides several Data Distribution Management (DDM) services to reduce the transmission and reception of irrelevant data. These services rely on the computation of the intersection between \"update\" and \"subscription\" regions. Currently, there are several main DDM filtering algorithms. Since each approach still has some shortcomings, we have focused our research on the design and the evaluation of intersection algorithms for the DDM. In this article, we introduce a new algorithm in which extents are sorted before computing the intersections. Our experiments show that usually the sort-based algorithm has the best performance among all approaches. The improvement of its performance ranges between 30% and 99% over the brute force and hybrid approaches.",
    "cited_by_count": 63,
    "openalex_id": "https://openalex.org/W2054316261",
    "type": "article"
  },
  {
    "title": "Adaptive Newton-based multivariate smoothed functional algorithms for simulation optimization",
    "doi": "https://doi.org/10.1145/1315575.1315577",
    "publication_date": "2007-12-01",
    "publication_year": 2007,
    "authors": "Shalabh Bhatnagar",
    "corresponding_authors": "Shalabh Bhatnagar",
    "abstract": "In this article, we present three smoothed functional (SF) algorithms for simulation optimization. While one of these estimates only the gradient by using a finite difference approximation with two parallel simulations, the other two are adaptive Newton-based stochastic approximation algorithms that estimate both the gradient and Hessian. One of the Newton-based algorithms uses only one simulation and has a one-sided estimate in both the gradient and Hessian, while the other uses two-sided estimates in both quantities and requires two simulations. For obtaining gradient and Hessian estimates, we perturb each parameter component randomly using independent and identically distributed (i.i.d) Gaussian random variates. The earlier SF algorithms in the literature only estimate the gradient of the objective function. Using similar techniques, we derive two unbiased SF-based estimators for the Hessian and develop suitable three-timescale stochastic approximation procedures for simulation optimization. We present a detailed convergence analysis of our algorithms and show numerical experiments with parameters of dimension 50 on a setting involving a network of M / G /1 queues with feedback. We compare the performance of our algorithms with related algorithms in the literature. While our two-simulation Newton-based algorithm shows the best results overall, our one-simulation algorithm shows better performance compared to other one-simulation algorithms.",
    "cited_by_count": 60,
    "openalex_id": "https://openalex.org/W2115020145",
    "type": "article"
  },
  {
    "title": "On constructing optimistic simulation algorithms for the discrete event system specification",
    "doi": "https://doi.org/10.1145/1456645.1456646",
    "publication_date": "2008-12-01",
    "publication_year": 2008,
    "authors": "James Nutaro",
    "corresponding_authors": "James Nutaro",
    "abstract": "This article describes a Time Warp simulation algorithm for discrete event models that are described in terms of the Discrete Event System Specification (DEVS). The article shows how the total state transition and total output function of a DEVS atomic model can be transformed into an event processing procedure for a logical process. A specific Time Warp algorithm is constructed around this logical process, and it is shown that the algorithm correctly simulates a DEVS coupled model that consists entirely of interacting atomic models. The simulation algorithm is presented abstractly; it is intended to provide a basis for implementing efficient and scalable parallel algorithms that correctly simulate DEVS models.",
    "cited_by_count": 56,
    "openalex_id": "https://openalex.org/W1971833118",
    "type": "article"
  },
  {
    "title": "A state event detection algorithm for numerically simulating hybrid systems with model singularities",
    "doi": "https://doi.org/10.1145/1189756.1189757",
    "publication_date": "2007-01-01",
    "publication_year": 2007,
    "authors": "Joel M. Esposito; Vijay Kumar",
    "corresponding_authors": "",
    "abstract": "This article describes an algorithm for detecting the occurrence of events, which signify discontinuities in the first derivative of the state variables, while simulating a set of nonsmooth differential equations. Such combined-discrete continuous systems arise in many contexts and are often referred to as hybrid systems, switched systems, or nonsmooth systems. In all cases, the state events are triggered at simulated times which generate states corresponding to the zeros of some algebraic “event” function. It has been noted that all existing simulators are prone to failure when these events occur in the neighborhood of model singularities---regions of the state space where the right-hand side of the differential equation is undefined. Such model singularities are often the impetus for using nonsmooth models in the first place. This failure occurs because existing algorithms blindly attempt to interpolate across singular regions, checking for possible events after the fact. The event detection algorithm described here overcomes this limitation using an approach inspired by feedback control theory. A carefully constructed extrapolation polynomial is used to select the integration step size by checking for potential future events, avoiding the need to evaluate the differential equation in potentially singular regions. It is shown that this alternate approach gives added functionality with little impact on the simulation efficiency.",
    "cited_by_count": 54,
    "openalex_id": "https://openalex.org/W2071982890",
    "type": "article"
  },
  {
    "title": "A framework for locally convergent random-search algorithms for discrete optimization via simulation",
    "doi": "https://doi.org/10.1145/1276927.1276932",
    "publication_date": "2007-09-01",
    "publication_year": 2007,
    "authors": "L. Jeff Hong; Barry L. Nelson",
    "corresponding_authors": "",
    "abstract": "The goal of this article is to provide a general framework for locally convergent random-search algorithms for stochastic optimization problems when the objective function is embedded in a stochastic simulation and the decision variables are integer ordered. The framework guarantees desirable asymptotic properties, including almost-sure convergence and known rate of convergence, for any algorithms that conform to its mild conditions. Within this framework, algorithm designers can incorporate sophisticated search schemes and complicated statistical procedures to design new algorithms.",
    "cited_by_count": 53,
    "openalex_id": "https://openalex.org/W1965055282",
    "type": "article"
  },
  {
    "title": "Random variate generation by numerical inversion when only the density is known",
    "doi": "https://doi.org/10.1145/1842722.1842723",
    "publication_date": "2010-10-01",
    "publication_year": 2010,
    "authors": "Gerhard Derflinger; Wolfgang Hörmann; Josef Leydold",
    "corresponding_authors": "",
    "abstract": "We present a numerical inversion method for generating random variates from continuous distributions when only the density function is given. The algorithm is based on polynomial interpolation of the inverse CDF and Gauss-Lobatto integration. The user can select the required precision, which may be close to machine precision for smooth, bounded densities; the necessary tables have moderate size. Our computational experiments with the classical standard distributions (normal, beta, gamma, t-distributions) and with the noncentral chi-square, hyperbolic, generalized hyperbolic, and stable distributions showed that our algorithm always reaches the required precision. The setup time is moderate and the marginal execution time is very fast and nearly the same for all distributions. Thus for the case that large samples with fixed parameters are required the proposed algorithm is the fastest inversion method known. Speed-up factors up to 1000 are obtained when compared to inversion algorithms developed for the specific distributions. This makes our algorithm especially attractive for the simulation of copulas and for quasi--Monte Carlo applications.",
    "cited_by_count": 49,
    "openalex_id": "https://openalex.org/W2007307276",
    "type": "article"
  },
  {
    "title": "Graph annotations in modeling complex network topologies",
    "doi": "https://doi.org/10.1145/1596519.1596522",
    "publication_date": "2009-10-01",
    "publication_year": 2009,
    "authors": "Xenofontas Dimitropoulos; Dmitri Krioukov; Amin Vahdat; George Riley",
    "corresponding_authors": "",
    "abstract": "The coarsest approximation of the structure of a complex network, such as the Internet, is a simple undirected unweighted graph. This approximation, however, loses too much detail. In reality, objects represented by vertices and edges in such a graph possess some nontrivial internal structure that varies across and differentiates among distinct types of links or nodes. In this work, we abstract such additional information as network annotations . We introduce a network topology modeling framework that treats annotations as an extended correlation profile of a network. Assuming we have this profile measured for a given network, we present an algorithm to rescale it in order to construct networks of varying size that still reproduce the original measured annotation profile. Using this methodology, we accurately capture the network properties essential for realistic simulations of network applications and protocols, or any other simulations involving complex network topologies, including modeling and simulation of network evolution. We apply our approach to the Autonomous System (AS) topology of the Internet annotated with business relationships between ASs. This topology captures the large-scale structure of the Internet. In depth understanding of this structure and tools to model it are cornerstones of research on future Internet architectures and designs. We find that our techniques are able to accurately capture the structure of annotation correlations within this topology, thus reproducing a number of its important properties in synthetically-generated random graphs.",
    "cited_by_count": 48,
    "openalex_id": "https://openalex.org/W2145102203",
    "type": "article"
  },
  {
    "title": "A variant of importance splitting for rare event estimation",
    "doi": "https://doi.org/10.1145/1899396.1899401",
    "publication_date": "2011-02-01",
    "publication_year": 2011,
    "authors": "Michael Amrein; Hans R. Künsch",
    "corresponding_authors": "",
    "abstract": "Importance splitting is a simulation technique to estimate very small entrance probabilities for Markov processes by splitting sample paths at various stages before reaching the set of interest. This can be done in many ways, yielding different variants of the method. In this context, we propose a new one, called fixed number of successes. We prove unbiasedness for the new and some known variants, because in many papers, the proof is based on an incorrect argument. Further, we analyze its behavior in a simplified setting in terms of efficiency and asymptotics in comparison to the standard variant. The main difference is that it controls the imprecision of the estimator rather than the computational effort. Our analysis and simulation examples show that it is rather robust in terms of parameter choice and we present a two-stage procedure which also yields confidence intervals.",
    "cited_by_count": 47,
    "openalex_id": "https://openalex.org/W1967621367",
    "type": "article"
  },
  {
    "title": "Sampling Exponentially Tilted Stable Distributions",
    "doi": "https://doi.org/10.1145/2043635.2043638",
    "publication_date": "2011-12-01",
    "publication_year": 2011,
    "authors": "Marius Hofert",
    "corresponding_authors": "Marius Hofert",
    "abstract": "Several algorithms for sampling exponentially tilted positive stable distributions have recently been suggested. Three of them are known as exact methods, that is, neither do they rely on approximations nor on numerically critical procedures. One of these algorithms is outperformed by another one uniformly over all parameters. The remaining two algorithms are based on different ideas and both have their advantages. After a brief overview of sampling algorithms for exponentially tilted positive stable distributions, the two algorithms are compared. A rule is derived when to apply which for sampling these distributions.",
    "cited_by_count": 43,
    "openalex_id": "https://openalex.org/W2018857587",
    "type": "article"
  },
  {
    "title": "Evolutionary optimization of low-discrepancy sequences",
    "doi": "https://doi.org/10.1145/2133390.2133393",
    "publication_date": "2012-03-01",
    "publication_year": 2012,
    "authors": "François-Michel De Rainville; Christian Gagné; Olivier Teytaud; Denis Laurendeau",
    "corresponding_authors": "",
    "abstract": "Low-discrepancy sequences provide a way to generate quasi-random numbers of high dimensionality with a very high level of uniformity. The nearly orthogonal Latin hypercube and the generalized Halton sequence are two popular methods when it comes to generate low-discrepancy sequences. In this article, we propose to use evolutionary algorithms in order to find optimized solutions to the combinatorial problem of configuring generators of these sequences. Experimental results show that the optimized sequence generators behave at least as well as generators from the literature for the Halton sequence and significantly better for the nearly orthogonal Latin hypercube.",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W2100184918",
    "type": "article"
  },
  {
    "title": "Constructing nearly orthogonal latin hypercubes for any nonsaturated run-variable combination",
    "doi": "https://doi.org/10.1145/2379810.2379813",
    "publication_date": "2012-11-01",
    "publication_year": 2012,
    "authors": "Alejandro Hernández; Thomas W. Lucas; Matthew Carlyle",
    "corresponding_authors": "",
    "abstract": "We present a new method for constructing nearly orthogonal Latin hypercubes that greatly expands their availability to experimenters. Latin hypercube designs have proven useful for exploring complex, high-dimensional computational models, but can be plagued with unacceptable correlations among input variables. To improve upon their effectiveness, many researchers have developed algorithms that generate orthogonal and nearly orthogonal Latin hypercubes. Unfortunately, these methodologies can have strict limitations on the feasible number of experimental runs and variables. To overcome these restrictions, we develop a mixed integer programming algorithm that generates Latin hypercubes with little or no correlation among their columns for most any determinate run-variable combination—including fully saturated designs. Moreover, many designs can be constructed for a specified number of runs and factors—thereby providing experimenters with a choice of several designs. In addition, our algorithm can be used to quickly adapt to changing experimental conditions by augmenting existing designs by adding new variables or generating new designs to accommodate a change in runs.",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W2171083234",
    "type": "article"
  },
  {
    "title": "DGHPSIM:",
    "doi": "https://doi.org/10.1145/2000494.2000496",
    "publication_date": "2011-08-01",
    "publication_year": 2011,
    "authors": "Murat M. Günal; Michael Pidd",
    "corresponding_authors": "",
    "abstract": "The British National Health Service (NHS) has a performance management framework that aims to guarantee short waiting times for patients by including mandatory targets for hospitals. DGHPSim is a suite of four components that simulates the activities of an NHS general hospital to show the effect of different policies on waiting times in these hospitals. DGHPSim has a generic structure that is used to simulate a particular hospital by employing data appropriate to that hospital from available data sets. Two of the components of DGHPSim, the accident and emergency simulator and the outpatient simulator, may be used independently as stand-alone simulators of these hospital functions. The DGHPSim suite incorporates a novel way of simulating the multitasking behavior of clinicians and uses transition matrices, extracted from standard datasets, to represent the states through which patients pass and the wards in which they may be treated. As a whole, the DGHPSim suite may be used to investigate improvement options before their implementation or to investigate how a hospital has improved its performance. We show how DGHPSim is used to investigate reported performance improvements in an English general hospital.",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W1995677689",
    "type": "article"
  },
  {
    "title": "Green Simulation",
    "doi": "https://doi.org/10.1145/3129130",
    "publication_date": "2017-10-27",
    "publication_year": 2017,
    "authors": "Mingbin Feng; Jeremy Staum",
    "corresponding_authors": "",
    "abstract": "We introduce a new paradigm in simulation experiment design and analysis, called “green simulation,” for the setting in which experiments are performed repeatedly with the same simulation model. Green simulation means reusing outputs from previous experiments to answer the question currently being asked of the simulation model. As one method for green simulation, we propose estimators that reuse outputs from previous experiments by weighting them with likelihood ratios, when parameters of distributions in the simulation model differ across experiments. We analyze convergence of these estimators as more experiments are repeated, while a stochastic process changes the parameters used in each experiment. As another method for green simulation, we propose an estimator based on stochastic kriging. We find that green simulation can reduce mean squared error by more than an order of magnitude in examples involving catastrophe bond pricing and credit risk evaluation.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W2600394393",
    "type": "article"
  },
  {
    "title": "Selection Procedures for Simulations with Multiple Constraints under Independent and Correlated Sampling",
    "doi": "https://doi.org/10.1145/2567921",
    "publication_date": "2014-05-01",
    "publication_year": 2014,
    "authors": "Christopher M. Healey; Sigrún Andradóttir; Seong‐Hee Kim",
    "corresponding_authors": "",
    "abstract": "We consider the problem of selecting the best feasible system with constraints on multiple secondary performance measures. We develop fully sequential indifference-zone procedures to solve this problem that guarantee a nominal probability of correct selection. In addition, we address two issues critical to the efficiency of these procedures: namely, the allocation of error between feasibility determination and selection of the best system, and the use of Common Random Numbers. We provide a recommended error allocation as a function of the number of constraints, supported by an experimental study and an approximate asymptotic analysis. The validity and efficiency of the new procedures with independent and CRN are demonstrated through both analytical and experimental results.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W2026507776",
    "type": "article"
  },
  {
    "title": "I<scp>ndemics</scp>",
    "doi": "https://doi.org/10.1145/2501602",
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "Keith Bisset; Jiangzhuo Chen; Suruchi Deodhar; Xizhou Feng; Yifei Ma; Madhav Marathe",
    "corresponding_authors": "",
    "abstract": "We describe the design and prototype implementation of I ndemics (_Interactive; Epi_demic; _Simulation;)—a modeling environment utilizing high-performance computing technologies for supporting complex epidemic simulations. I ndemics can support policy analysts and epidemiologists interested in planning and control of pandemics. I ndemics goes beyond traditional epidemic simulations by providing a simple and powerful way to represent and analyze policy-based as well as individual-based adaptive interventions . Users can also stop the simulation at any point, assess the state of the simulated system, and add additional interventions. I ndemics is available to end-users via a web-based interface. Detailed performance analysis shows that I ndemics greatly enhances the capability and productivity of simulating complex intervention strategies with a marginal decrease in performance. We also demonstrate how I ndemics was applied in some real case studies where complex interventions were implemented.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W2123951972",
    "type": "article"
  },
  {
    "title": "Two-stage stochastic optimization for optimal power flow under renewable generation uncertainty",
    "doi": "https://doi.org/10.1145/2553084",
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "Dzung T. Phan; Soumyadip Ghosh",
    "corresponding_authors": "",
    "abstract": "We propose a two-stage stochastic version of the classical economic dispatch problem with alternating-current power flow constraints, a nonconvex optimization formulation that is central to power transmission and distribution over an electricity grid. Certain generation decisions made in the first stage cannot further be changed in the second stage, where the uncertainty due to various factors such as renewable generation is realized. Any supply-demand mismatch in the second stage must be alleviated using high marginal cost power sources that can be tapped in short order. We solve a Sample-Average Approximation (SAA) of this formulation by capturing the uncertainty using a finite number of scenario samples. We propose two outer approximation algorithms to solve this nonconvex program to global optimality. We use recently discovered structural properties for the classical deterministic problem to show that when these properties hold the sequence of approximate solutions obtained under both alternatives has a limit point that is a globally optimal solution to the two-stage nonconvex SAA program. We also present an alternate local optimization approach to solving the SAA problem based on the Alternating Direction Method of Multipliers (ADMM). Numerical experiments for a variety of parameter settings were carried out to demonstrate the efficiency and usability of our method over ADMM for large practical instances.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W2089580348",
    "type": "article"
  },
  {
    "title": "Exact and Approximate Moment Derivation for Probabilistic Loops With Non-Polynomial Assignments",
    "doi": "https://doi.org/10.1145/3641545",
    "publication_date": "2024-01-23",
    "publication_year": 2024,
    "authors": "Andrey Kofnov; Marcel Moosbrugger; Miroslav Stankovič; Ezio Bartocci; Efstathia Bura",
    "corresponding_authors": "",
    "abstract": "Many stochastic continuous-state dynamical systems can be modeled as probabilistic programs with nonlinear non-polynomial updates in non-nested loops. We present two methods, one approximate and one exact, to automatically compute, without sampling, moment-based invariants for such probabilistic programs as closed-form solutions parameterized by the loop iteration. The exact method applies to probabilistic programs with trigonometric and exponential updates and is embedded in the Polar tool. The approximate method for moment computation applies to any nonlinear random function as it exploits the theory of polynomial chaos expansion to approximate non-polynomial updates as the sum of orthogonal polynomials. This translates the dynamical system to a non-nested loop with polynomial updates, and thus renders it conformable with the Polar tool that computes the moments of any order of the state variables. We evaluate our methods on an extensive number of examples ranging from modeling monetary policy to several physical motion systems in uncertain environments. The experimental results demonstrate the advantages of our approach with respect to the current state-of-the-art.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4391145581",
    "type": "article"
  },
  {
    "title": "Context, Composition, Automation, and Communication - The C <sup>2</sup> AC Roadmap for Modeling and Simulation",
    "doi": "https://doi.org/10.1145/3673226",
    "publication_date": "2024-08-13",
    "publication_year": 2024,
    "authors": "Adelinde M. Uhrmacher; Peter I. Frazier; Reiner Hähnle; Franziska Klügl; Fabian Lorig; Bertram Ludäscher; Laura Nenzi; Cristina Ruiz-Martín; Bernhard Rumpe⋆; Claudia Szabo; Gabriel Wainer; Pia Wilsdorf",
    "corresponding_authors": "",
    "abstract": "Simulation has become, in many application areas, a sine qua non . Most recently, COVID-19 has underlined the importance of simulation studies and limitations in current practices and methods. We identify four goals of methodological work for addressing these limitations. The first is to provide better support for capturing, representing, and evaluating the context of simulation studies, including research questions, assumptions, requirements, and activities contributing to a simulation study. In addition, the composition of simulation models and other simulation studies’ products must be supported beyond syntactical coherence, including aspects of semantics and purpose, enabling their effective reuse. A higher degree of automating simulation studies will contribute to more systematic, standardized simulation studies and their efficiency. Finally, it is essential to invest increased effort into effectively communicating results and the processes involved in simulation studies to enable their use in research and decision making. These goals are not pursued independently of each other, but they will benefit from and sometimes even rely on advances in other sub-fields. In this article, we explore the basis and interdependencies evident in current research and practice and delineate future research directions based on these considerations.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4399809040",
    "type": "article"
  },
  {
    "title": "Enhancing P4-Based Network Emulation Fidelity Through a Lightweight Virtual Time System and Application Evaluation",
    "doi": "https://doi.org/10.1145/3725530",
    "publication_date": "2025-03-21",
    "publication_year": 2025,
    "authors": "Gong Chen; Zheng Hu; Yanfeng Qu; Dong Jin",
    "corresponding_authors": "",
    "abstract": "P4 serves as a programming language for configuring flexible and programmable network data planes, facilitating the development of custom protocols and programmable switches, and driving innovation in software-defined networking and network function virtualization. While the Linux container based network emulator, Mininet, coupled with the BMv2 software P4 switch, is widely used for rapid prototyping of P4-based applications, BMv2’s diminished performance raises fidelity concerns under high traffic and large network scenarios. In this paper, we introduce a lightweight virtual time system integrated into Mininet with BMv2 to enhance fidelity and scalability. By applying a time dilation factor (TDF) to interactions between containers and the physical machine, we optimize the emulated P4 network’s perceived speed from the application processes’ perspective. System evaluation demonstrates accurate emulation of significantly larger networks under high loads with minimal system overhead. We showcase our system’s utility through two network applications: an emulation of a TCP SYN flood attack and an ECMP load balancer. Evaluating against a production-grade software switch, Open vSwitch, and a physical testbed, we highlight the virtual time system’s improvement in temporal fidelity despite the observed performance degradation in BMv2 software switches.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4408688842",
    "type": "article"
  },
  {
    "title": "Privacy Meets Performance: Enhancing Distributed Simulation-based Federated Multi-agent Learning with Privacy-preserving Surrogate Model",
    "doi": "https://doi.org/10.1145/3728466",
    "publication_date": "2025-05-15",
    "publication_year": 2025,
    "authors": "Bo Zhang; Wen Jun Tan; Wentong Cai; Allan N. Zhang",
    "corresponding_authors": "",
    "abstract": "In recent years, model-free multi-agent reinforcement learning (MaRL) has become a powerful tool for learning effective policies to solve optimization problems. However, individual agents may raise concerns about sharing their internal data, simulation models, and decision models in collaborative optimization. Distributed simulation (DS) and federated learning have been widely used as privacy-preserving methods to hide simulation details and maintain data and model privacy. Despite their benefits, these methods often require large amounts of interaction and data to converge, which leads to a high communication time, especially if the agents are distributed around the world. To address this issue, we propose a distributed surrogate model for DS-based federated MaRL to utilize the surrogate model instead of DS during the training. This can enhance data efficiency and effectiveness to accelerate agent learning while maintaining data and model privacy. An aerospace supply chain (SC) is used as the experimental scenario to evaluate the performance of our proposed approach, in terms of SC profits, training convergence, and execution time. Experimental results show that our proposed approach can achieve higher SC profits with the same number of simulation runs, converge faster, and reduce execution time to gain the same level of SC profits.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4410406935",
    "type": "article"
  },
  {
    "title": "Parallel simulation of chip-multiprocessor architectures",
    "doi": "https://doi.org/10.1145/643114.643116",
    "publication_date": "2002-07-01",
    "publication_year": 2002,
    "authors": "Matthew C. Chidester; Alan D. George",
    "corresponding_authors": "",
    "abstract": "Chip-multiprocessor (CMP) architectures present a challenge for efficient simulation, combining the requirements of a detailed microprocessor simulator with that of a tightly-coupled parallel system. In this paper, a distributed simulator for target CMPs is presented based on the Message Passing Interface (MPI) designed to run on a host cluster of workstations. Microbenchmark-based evaluation is used to narrow the parallelization design space concerning the performance impact of distributed vs. centralized target L2 simulation, blocking vs. non-blocking remote cache accesses, null-message vs. barrier techniques for clock synchronization, and network interconnect selection. The best combination is shown to yield speedups of up to 16 on a 9-node cluster of dual-CPU workstations, partially due to cache effects.",
    "cited_by_count": 66,
    "openalex_id": "https://openalex.org/W2033708281",
    "type": "article"
  },
  {
    "title": "A system of high-dimensional, efficient, long-cycle and portable uniform random number generators",
    "doi": "https://doi.org/10.1145/945511.945513",
    "publication_date": "2003-10-01",
    "publication_year": 2003,
    "authors": "Lih‐Yuan Deng; Hongquan Xu",
    "corresponding_authors": "",
    "abstract": "We propose a system of multiple recursive generators of modulus p and order k where all nonzero coefficients of the recurrence are equal. The advantage of this property is that a single multiplication is needed to compute the recurrence, so the generator would run faster than the general case. For p = 2 31 − 1, the most popular modulus used, we provide tables of specific parameter values yielding maximum period for recurrence of order k = 102 and 120. For p = 2 31 − 55719 and k = 1511, we have found generators with a period length approximately 10 14100.5 .",
    "cited_by_count": 66,
    "openalex_id": "https://openalex.org/W2145218017",
    "type": "article"
  },
  {
    "title": "Structural and behavioral equivalence of simulation models",
    "doi": "https://doi.org/10.1145/132277.132281",
    "publication_date": "1992-01-02",
    "publication_year": 1992,
    "authors": "Enver Yücesan; Lee W. Schruben",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Structural and behavioral equivalence of simulation models Authors: Enver Yücesan European Institute of Business Administration European Institute of Business AdministrationView Profile , Lee Schruben Cornell University Cornell UniversityView Profile Authors Info & Claims ACM Transactions on Modeling and Computer SimulationVolume 2Issue 1Jan. 1992 pp 82–103https://doi.org/10.1145/132277.132281Online:02 January 1992Publication History 46citation0DownloadsMetricsTotal Citations46Total Downloads0Last 12 Months0Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 61,
    "openalex_id": "https://openalex.org/W2010843738",
    "type": "article"
  },
  {
    "title": "Adaptive memory management and optimism control in time warp",
    "doi": "https://doi.org/10.1145/249204.249207",
    "publication_date": "1997-04-01",
    "publication_year": 1997,
    "authors": "Samir R. Das; Richard M. Fujimoto",
    "corresponding_authors": "",
    "abstract": "It is widely believed that the Time Warp protocol for parallel discrete event simulation is prone to two potential problems: an excessive amount of wasted, rolled back computation resulting from “rollback thrashing” behaviors, and inefficient use of memory, leading to poor performance of virtual memory and/or multiprocessor cache systems. An adaptive mechanism is proposed based on the Cancelback memory management protocol for shared-memory multiprocessors that dynamically controls the amount of memory used in the simulation in order to maximize performance. The proposed mechanism is adaptive in the sense that it monitors the execution of the Time Warp program, and using simple models, automatically adjusts the amount of memory used to reduce Time Warp overheads (fossil collection, Cancelback, the amount of rolled back computation, etc.) to a manageable level. We describe an implementation of this mechanism on a shared memory, Kendall Square Research KSR-1, multiprocessor and demonstrate its effectiveness in automatically maximizing performance while minimizing memory utilzation, for several synthetic and benchmark discrete event simulation applications. We also demonstrate the adaptive ability of the mechanism by showing that it “tracks” the time-varying nature of a communication network simulation.",
    "cited_by_count": 61,
    "openalex_id": "https://openalex.org/W2027842155",
    "type": "article"
  },
  {
    "title": "Elastic time",
    "doi": "https://doi.org/10.1145/280265.280267",
    "publication_date": "1998-04-01",
    "publication_year": 1998,
    "authors": "Sudhir Srinivasan; Paul F. Reynolds",
    "corresponding_authors": "",
    "abstract": "We introduce a new class of synchronization protocols for parallel discrete event simulation, those based on near-perfect state information (NPSI). NPSI protocols are adaptive dynamically controlling the rate at which processes constituting a parallel simulation proceed with the goal of completing a simulation efficiently. We show by analysis that a class of adaptive protocols (that includes NPSI and several others) can both arbitrarily outperform and be arbitrarily outperformed by the Time Warp synchronization protocol. This mixed result both substantiates the promising results we and other adaptive protocol designers have observed, and cautions those who might assume that any adaptive protocol will always be better than any nonadaptive one. We establish in an experimental study that a particular NPSI protocol, the Elastic Time Algorithm , outperforms Time Warp, both temporally and spatially on every workload tested. Although significant options remain with respect to the design of ETA, the work presented here establishes the class of NPSI protocols as a very promising approach.",
    "cited_by_count": 59,
    "openalex_id": "https://openalex.org/W2296199700",
    "type": "article"
  },
  {
    "title": "Bounded relative error in estimating transient measures of highly dependable non-Markovian systems",
    "doi": "https://doi.org/10.1145/175007.175008",
    "publication_date": "1994-04-01",
    "publication_year": 1994,
    "authors": "Philip Heidelberger; Perwez Shahabuddin; Victor F. Nicola",
    "corresponding_authors": "",
    "abstract": "This article deals with fast simulation techniques for estimating transient measures in highly dependable systems. The systems we consider of components with generally distributed lifetimes and repair times, with complex interaction among components. As is well known, standard simulation of highly dependable systems is very inefficient, and importance-sampling is widely used to improve efficiency. We present two new techniques, one of which is based on the uniformization approach to simulation, and the other is a natural extension of the uniformization approach which we call exponential transformation. We show that under certain assumptions, these techniques have the bounded relative error property, i.e., the relative error of the simulation estimate remains bounded as components become more and more reliable, unlike standard simulation in which it tends to infinity. This implies that only a fixed number of observations are required to achieve a given relative error, no matter how rare the failure events are.",
    "cited_by_count": 58,
    "openalex_id": "https://openalex.org/W2164523847",
    "type": "article"
  },
  {
    "title": "Scalable fluid models and simulations for large-scale IP networks",
    "doi": "https://doi.org/10.1145/1010621.1010625",
    "publication_date": "2004-07-01",
    "publication_year": 2004,
    "authors": "Yong Liu; Francesco Lo Presti; Vishal Misra; Don Towsley; Yu Gu",
    "corresponding_authors": "",
    "abstract": "In this article we present a scalable model of a network of Active Queue Management (AQM) routers serving a large population of Transport Control Protocol (TCP) flows. We present efficient solution techniques that allow one to obtain the transient behavior of the average queue lengths and packet loss/mark probabilities of AQM routers, and average end-to-end throughput and latencies of TCP users. We model different versions of TCP as well as different implementations of RED Random Early Detection (RED), the most popular AQM scheme currently in use. Comparisons between the models and ns simulation show our models to be quite accurate while at the same time requiring substantially less time to solve than packet level simulations, especially when workloads and bandwidths are high.",
    "cited_by_count": 57,
    "openalex_id": "https://openalex.org/W2000951918",
    "type": "article"
  },
  {
    "title": "Parallelism analyzers for parallel discrete event simulation",
    "doi": "https://doi.org/10.1145/146382.146401",
    "publication_date": "1992-07-01",
    "publication_year": 1992,
    "authors": "Yi‐Bing Lin",
    "corresponding_authors": "Yi‐Bing Lin",
    "abstract": "Discrete event simulation is usually time consuming. Recently, there has been a great deal of interest in using parallel computers to speed up the simulation process. Before the parallel simulation approach is applied, it is important to understand the inherent parallelism of simulation applications. A simple technique called critical path analysis was proposed to study paralllelism of simulation applications. This paper describes three critical path analysis algorithms based on different event-scheduling (process scheduling) policies. These algorithms are much simpler than a previous approach, where the events must be recorded in a trace and an extra pass is required to process the event trace. In our approach, the critical path analysis algorithms are integrated with the sequential simulation. At the end of the sequential simulation, the optimal parallel execution time is also computed. Livny proposed an algorithm similar to our approach (His, however, was designed for a specific language). Our algorithms can be integrated with sequential simulation programs written by users or be integrated with simulation languages. Another advantage of our algorithms over previous approaches is that ours can be used to study load balancing under different event-scheduling policies. Since our algorithms can be easily inserted in sequential simulation programs, critical path analysis can be applied to existing sequential programs without difficulty. The results can then be used to predict the performance of parallel simulation on similar applications. An example is given to show how useful information can be obtained from our algorithms.",
    "cited_by_count": 57,
    "openalex_id": "https://openalex.org/W2070292444",
    "type": "article"
  },
  {
    "title": "An evaluation of the Chandy-Misra-Bryant algorithm for digital logic simulation",
    "doi": "https://doi.org/10.1145/130611.130613",
    "publication_date": "1991-10-01",
    "publication_year": 1991,
    "authors": "L. Soule; Anoop Gupta",
    "corresponding_authors": "",
    "abstract": "We explore the suitability of the Chandy-Misra-Bryant (CMB) algorithm for the domain of digital logic simulation. Our evaluation is based on results for six realistic benchmark circuits, one of them being the R6000 microprocessor form MIPS. A quantitative evaluation of the concurrency exhibited by the CMB algorithm shows that an average of 42-196 element activations can be evaluated in parallel if arbitrarily many processors are available. One major factor limiting the parallel performance is the large number of deadlocks that occur. We present a classification of the types of deadlocks and describe them in terms of circuit structure. Using domain-specific knowledge, we propose and evaluate several methods for both reducing the number of deadlock occurences and for reducing the time spent on each occurence. Running on a 16-processor Encore Multimax we observe speedups of 6-9. While these self-relative speedups are larger than a parallel version of the traditional centralized-time event-driven algorithm, they come at the price of large overheads: significantly more complex element evaluations, extra element evaluations, and deadlock resolution time. These overheads overwhelm the advantages of using distributed time and consistently make the parallel performance of the CMB algorithm about three times slower than that of the traditional parallel event-driven algorithm. Our experience leads us to conclude that the distributed-time CMB algorithm does not present a viable alternative to the centralized-time event-driven algorithm in the domain of parallel digital logic simulation.",
    "cited_by_count": 56,
    "openalex_id": "https://openalex.org/W1966644024",
    "type": "article"
  },
  {
    "title": "A knowledge-based approach for the validation of simulation models",
    "doi": "https://doi.org/10.1145/229493.229511",
    "publication_date": "1996-01-01",
    "publication_year": 1996,
    "authors": "Louis G. Birta; F. Nur Özmizrak",
    "corresponding_authors": "",
    "abstract": "A new perspective of the validation problemfor simulation models is formulated in this article. The approach is knowledge-based and focuses on behavioral validation. It has the important feature of providing a basis for the development of a software environment that can automate the validation activity. Discrete, continuous and combined simulation models can be treated in a uniform manner. The key element of the approach is a validation knowledge base (VKB). This is developed as three disjoint sets of relationships among the input and output variables of the simulation model. These relationships serve to capture all aspects of expected behavior of the simulation model. A simple characterization of model behavior is presented which provides the basis for specifying the relationships from which the VKB is constructed The utilization of all the information in the VKB in an efficient way is an important subgoal of our system architecture. This requirement gives rise to an experiment design problem. This problem is carefully formulated and examined within the framework of the behavior characterizations that exist in the VKB. In particular, a basis for its solution is established in a constraint set framework by carrying out a transformation on the relationships within the VKB. The constraint set context for the problem has the advantage of providing an environment which not only facilitates analysis but also enables the application of a variety of solution techniques.",
    "cited_by_count": 56,
    "openalex_id": "https://openalex.org/W2022830354",
    "type": "article"
  },
  {
    "title": "Discrete event fluid modeling of background TCP traffic",
    "doi": "https://doi.org/10.1145/1010621.1010622",
    "publication_date": "2004-07-01",
    "publication_year": 2004,
    "authors": "David M. Nicol; Guanhua Yan",
    "corresponding_authors": "",
    "abstract": "TCP is the most widely used transport layer protocol used in the Internet today. A TCP session adapts the demands it places on the network to observations of bandwidth availability on the network. Because TCP is adaptive, any model of its behavior that aspires to be accurate must be influenced by other network traffic. This point is especially important in the context of using simulation to evaluate some new network algorithm of interest (e.g., reliable multicast) in an environment where the background traffic affects---and is affected by---its behavior. We need to generate background traffic efficiently in a way that captures the salient features of TCP, while the reference and background traffic representations interact with each other. This article describes a fluid model of TCP and a switching model that has flows represented by fluids interacting with packet-oriented flows. We describe conditions under which a fluid model produces exactly the same behavior as a packet-oriented model, and we quantify the performance advantages of the approach both analytically and empirically. We observe that very significant speedups may be attained while keeping high accuracy.",
    "cited_by_count": 56,
    "openalex_id": "https://openalex.org/W2152701897",
    "type": "article"
  },
  {
    "title": "Comparison with a standard via fully sequential procedures",
    "doi": "https://doi.org/10.1145/1060576.1060579",
    "publication_date": "2005-04-01",
    "publication_year": 2005,
    "authors": "Seong‐Hee Kim",
    "corresponding_authors": "Seong‐Hee Kim",
    "abstract": "We develop fully sequential procedures for comparison with a standard. The goal is to find systems whose expected performance measures are larger or smaller than a single system referred to as a standard and, if there is any, to find the one with the largest or smallest performance. The general formulation of comparison with a standard gives the standard a special status and tries to protect it when its performance is better than or even equal to performance measures of all the other alternatives. Therefore, the problem cannot be formulated as the selection of the best and a specialized procedure is required. Our procedures allow for unequal variances across systems, the use of common random numbers, and known or unknown expected performance of the standard. Experimental results are provided to compare the efficiency of the procedure with other existing procedures.",
    "cited_by_count": 54,
    "openalex_id": "https://openalex.org/W2100987884",
    "type": "article"
  },
  {
    "title": "Efficient and portable multiple recursive generators of large order",
    "doi": "https://doi.org/10.1145/1044322.1044323",
    "publication_date": "2005-01-01",
    "publication_year": 2005,
    "authors": "Lih‐Yuan Deng",
    "corresponding_authors": "Lih‐Yuan Deng",
    "abstract": "Deng and Xu [2003] proposed a system of multiple recursive generators of prime modulus p and order k , where all nonzero coefficients of the recurrence are equal. This type of generator is efficient because only a single multiplication is required. It is common to choose p = 2 31 −1 and some multipliers to further improve the speed of the generator. In this case, some fast implementations are available without using explicit division or multiplication. For such a p , Deng and Xu [2003] provided specific parameters, yielding the maximum period for recurrence of order k , up to 120. One problem of extending it to a larger k is the difficulty of finding a complete factorization of p k −1. In this article, we apply an efficient technique to find k such that it is easy to factor p k −1, with p = 2 31 −1. The largest one found is k = 1597. To find multiple recursive generators of large order k , we introduce an efficient search algorithm with an early exit strategy in case of a failed search. For k = 1597, we constructed several efficient and portable generators with the period length approximately 10 14903.1 .",
    "cited_by_count": 53,
    "openalex_id": "https://openalex.org/W2037703035",
    "type": "article"
  },
  {
    "title": "Massively parallel and distributed simulation of a class of discrete event systems",
    "doi": "https://doi.org/10.1145/146382.146389",
    "publication_date": "1992-07-01",
    "publication_year": 1992,
    "authors": "Pirooz Vakili",
    "corresponding_authors": "Pirooz Vakili",
    "abstract": "In this paper we propose a new approach to parallel and distributed simulation of discrete event systems. Most parallel and distributed discrete event simulation algorithms are concerned with the simulation of one “large” discrete event system. In this case the computational intensity is due to the size and complexity of the simulated system. In contrast, we are interested in simulating a “large” number of “medium sized” systems. These are variants of a “nominal system” with different system parameter values or operation policies. The computational intensity in our case is due to the “large” number of simulated variants. Many simulation projects such as factor screening, performance modeling, and optimization require system performance evaluations at many parameter values; and others, we believe, could significantly benefit from them. There is considerable work in the literature on stochastic coupling of trajectories of parametric families of stochastic processes. Our approach can be viewed as the simulation of the coupled trajectories. We use a single clock mechanism that drives all trajectories simultaneously, hence the approach is called Single Clock Multiple System (SCMS) simulation. The single clock synchronizes all trajectories such that the “same” event occurs at the “same” time at all systems. This synchronization is the basis of our parallel and distributed algorithms. We focus on a particular implementation of the SCMS simulation using the so-called Standard Clock (SC) technique and also on the massively parallel implementation of the SC algorithms on the SIMD Connection Machine. Orders of magnitude of speedup is possible. Furthermore, the possibility of concurrent performance evaluation and comparison at many system parameter values offers new and significant opportunities for performance optimization.",
    "cited_by_count": 53,
    "openalex_id": "https://openalex.org/W2134718337",
    "type": "article"
  },
  {
    "title": "On the efficiency of RESTART for multidimensional state systems",
    "doi": "https://doi.org/10.1145/1147224.1147227",
    "publication_date": "2006-07-01",
    "publication_year": 2006,
    "authors": "Manuel Villén‐Altamirano; José Villén‐Altamirano",
    "corresponding_authors": "",
    "abstract": "RESTART (Repetitive Simulation Trials After Reaching Thresholds) is a widely applicable accelerated simulation technique that allows the evaluation of extremely low probabilities. The focus of this article is on providing guidelines for achieving a high efficiency in a simulation with RESTART. Emphasis is placed on the choice of the importance function, that is, the function of the system state for determining when retrials are made. A heuristic approach which is shown to be effective for some systems is proposed for this choice. A two-queue tandem network is used to illustrate the efficiency achieved following these guidelines. The importance function chosen in this example shows that an appropriate choice of the importance function leads to an efficient simulation of a system with multidimensional state space. Also presented are sufficient conditions for achieving asymptotic efficiency, and it is shown that they are not very restrictive in RESTART simulation.",
    "cited_by_count": 50,
    "openalex_id": "https://openalex.org/W2008228386",
    "type": "article"
  },
  {
    "title": "Analysis of state-independent importance-sampling measures for the two-node tandem queue",
    "doi": "https://doi.org/10.1145/1147224.1147226",
    "publication_date": "2006-07-01",
    "publication_year": 2006,
    "authors": "Pieter-Tjerk de Boer",
    "corresponding_authors": "Pieter-Tjerk de Boer",
    "abstract": "We investigate the simulation of overflow of the total population of a Markovian two-node tandem queue model during a busy cycle, using importance sampling with a state-independent change of measure. We show that the only such change of measure that may possibly result in asymptotically efficient simulation for large overflow levels is exchanging the arrival rate with the smallest service rate. For this change of measure, we classify the model's parameter space into regions of asymptotic efficiency, exponential growth of the relative error, and infinite variance, using both analytical and numerical techniques.",
    "cited_by_count": 50,
    "openalex_id": "https://openalex.org/W2155440248",
    "type": "article"
  },
  {
    "title": "A metamodel for federation architectures",
    "doi": "https://doi.org/10.1145/1371574.1371576",
    "publication_date": "2008-07-01",
    "publication_year": 2008,
    "authors": "Okan Topçu; Mehmet Adak; Halit Oğuztüzün",
    "corresponding_authors": "",
    "abstract": "This article proposes a metamodel for describing the architecture of a High Level Architecture (HLA) compliant federation. A salient feature of the Federation Architecture Metamodel (FAMM) is the behavioral description of federates based on live sequence charts. FAMM formalizes the standard HLA Object Model and Federate Interface Specification. FAMM supports processing through automated tools, and in particular through code generation. It is formulated in metaGME, the metamodel for the Generic Modeling Environment.",
    "cited_by_count": 48,
    "openalex_id": "https://openalex.org/W2079440720",
    "type": "article"
  },
  {
    "title": "Gradient estimation for discrete-event systems by measure-valued differentiation",
    "doi": "https://doi.org/10.1145/1667072.1667077",
    "publication_date": "2010-01-01",
    "publication_year": 2010,
    "authors": "Bernd Heidergott; Felisa J. Vázquez-Abad; Georg Ch. Pflug; Taoying Farenhorst-Yuan",
    "corresponding_authors": "",
    "abstract": "In simulation of complex stochastic systems, such as Discrete-Event Systems (DES), statistical distributions are used to model the underlying randomness in the system. A sensitivity analysis of the simulation output with respect to parameters of the input distributions, such as the mean and the variance, is therefore of great value. The focus of this article is to provide a practical guide for robust sensitivity, respectively, gradient estimation that can be easily implemented along the simulation of a DES. We study the Measure-Valued Differentiation (MVD) approach to sensitivity estimation. Specifically, we will exploit the “modular” structure of the MVD approach, by firstly providing measure-valued derivatives for input distributions that are of importance in practice, and subsequently, by showing that if an input distribution possesses a measure-valued derivative, then so does the overall Markov kernel modeling the system transitions. This simplifies the complexity of applying MVD drastically: one only has to study the measure-valued derivative of the input distribution, a measure-valued derivative of the associated Markov kernel is then given through a simple formula in canonical form. The derivative representations of the underlying simple distributions derived in this article can be stored in a computer library. Combined with the generic MVD estimator, this yields an automated gradient estimation procedure. The challenge in automating MVD so that it can be included into a simulation package is the verification of the integrability condition to guarantee that the estimators are unbiased. The key contribution of the article is that we establish a general condition for unbiasedness which is easily checked in applications. Gradient estimators obtained by MVD are typically phantom estimators and we discuss the numerical efficiency of phantom estimators with the example of waiting times in the G/G/1 queue.",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W1975928280",
    "type": "article"
  },
  {
    "title": "Two-phase screening procedure for simulation experiments",
    "doi": "https://doi.org/10.1145/1502787.1502790",
    "publication_date": "2009-03-01",
    "publication_year": 2009,
    "authors": "Susan M. Sanchez; Hong Wan; Thomas W. Lucas",
    "corresponding_authors": "",
    "abstract": "Analysts examining complex simulation models often conduct screening experiments to identify important factors. The controlled sequential bifurcation screening procedures CSB and CSB-X use a sequence of tests to classify factors as important or unimportant, while controlling Type I error and power. These procedures require analysts to identify the directions of the effects prior to experimentation, which can be problematic. We propose hybrid two-phase approaches, FFCSB and FFCSBX, as alternatives. Phase 1 uses an efficient fractional factorial to estimate factor effect directions; phase 2 uses CSB or CSB-X. Empirical investigations show these outperform CSB(X) in efficiency and effectiveness for many situations of practical interest.",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W2621378850",
    "type": "article"
  },
  {
    "title": "An analysis of a variation of hit-and-run for uniform sampling from general regions",
    "doi": "https://doi.org/10.1145/1921598.1921600",
    "publication_date": "2011-02-04",
    "publication_year": 2011,
    "authors": "Seksan Kiatsupaibul; Robert L. Smith; Zelda B. Zabinsky",
    "corresponding_authors": "",
    "abstract": "Hit-and-run, a class of MCMC samplers that converges to general multivariate distributions, is known to be unique in its ability to mix fast for uniform distributions over convex bodies. In particular, its rate of convergence to a uniform distribution is of a low order polynomial in the dimension. However, when the body of interest is difficult to sample from, typically a hyperrectangle is introduced that encloses the original body, and a one-dimensional acceptance/rejection is performed. The fast mixing analysis of hit-and-run does not account for this one-dimensional sampling that is often needed for implementation of the algorithm. Here we show that the effect of the size of the hyperrectangle on the efficiency of the algorithm is only a linear scaling effect. We also introduce a variation of hit-and-run that accelerates the sampler and demonstrate its capability through a computational study.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W2033491515",
    "type": "article"
  },
  {
    "title": "Bayesian Kriging Analysis and Design for Stochastic Simulations",
    "doi": "https://doi.org/10.1145/2331140.2331145",
    "publication_date": "2012-08-01",
    "publication_year": 2012,
    "authors": "Szu Hui Ng; Jun Yin",
    "corresponding_authors": "",
    "abstract": "Kriging is an increasingly popular metamodeling tool in simulation due to its flexibility in global fitting and prediction. In the fitting of this metamodel, the parameters are often estimated from the simulation data, which introduces parameter estimation uncertainties into the overall prediction error. Traditional plug-in estimators usually ignore these uncertainties, which can be substantial in stochastic simulations. This typically leads to an underestimation of the total variability and an overconfidence in the results. In this article, a Bayesian metamodeling approach for kriging prediction is proposed for stochastic simulations to more appropriately account for the parameter uncertainties. We derive the predictive distribution under certain assumptions and also provide a general Markov Chain Monte Carlo analysis approach to handle more general assumptions on the parameters and design. Numerical results indicate that the Bayesian approach has better coverage and better predictive variance than a previously proposed modified nugget effect kriging model, especially in cases where the stochastic variability is high. In addition, we further consider the important problem of planning the experimental design. We propose a two-stage design approach that systematically balances the allocation of computing resources to new design points and replication numbers in order to reduce the uncertainties and improve the accuracy of the predictions.",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W2161433505",
    "type": "article"
  },
  {
    "title": "Gradient Extrapolated Stochastic Kriging",
    "doi": "https://doi.org/10.1145/2658995",
    "publication_date": "2014-08-13",
    "publication_year": 2014,
    "authors": "Huashuai Qu; Michael C. Fu",
    "corresponding_authors": "",
    "abstract": "We introduce an approach for enhancing stochastic kriging in the setting where additional direct gradient information is available (e.g., provided by techniques such as perturbation analysis or the likelihood ratio method). The new approach, called gradient extrapolated stochastic kriging (GESK) , incorporates direct gradient estimates by extrapolating additional responses. For two simplified settings, we show that GESK reduces mean squared error (MSE) compared to stochastic kriging under certain conditions on step sizes. Since extrapolation step sizes are crucial to the performance of the GESK model, we propose two different approaches to determine the step sizes: maximizing penalized likelihood and minimizing integrated mean squared error. Numerical experiments are conducted to illustrate the performance of the GESK model and to compare it with alternative approaches.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W2065233986",
    "type": "article"
  },
  {
    "title": "Moment-Based Methods for Parameter Inference and Experiment Design for Stochastic Biochemical Reaction Networks",
    "doi": "https://doi.org/10.1145/2688906",
    "publication_date": "2015-02-17",
    "publication_year": 2015,
    "authors": "Jakob Ruess; John Lygeros",
    "corresponding_authors": "",
    "abstract": "Continuous-time Markov chains are commonly used in practice for modeling biochemical reaction networks in which the inherent randomness of the molecular interactions cannot be ignored. This has motivated recent research effort into methods for parameter inference and experiment design for such models. The major difficulty is that such methods usually require one to iteratively solve the chemical master equation that governs the time evolution of the probability distribution of the system. This, however, is rarely possible, and even approximation techniques remain limited to relatively small and simple systems. An alternative explored in this article is to base methods on only some low-order moments of the entire probability distribution. We summarize the theory behind such moment-based methods for parameter inference and experiment design and provide new case studies where we investigate their performance.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W1997377511",
    "type": "article"
  },
  {
    "title": "Semantics and Efficient Simulation Algorithms of an Expressive Multilevel Modeling Language",
    "doi": "https://doi.org/10.1145/2998499",
    "publication_date": "2017-04-30",
    "publication_year": 2017,
    "authors": "Tobias Helms; Tom Warnke; Carsten Maus; Adelinde M. Uhrmacher",
    "corresponding_authors": "",
    "abstract": "The domain-specific modeling and simulation language ML-Rules is aimed at facilitating the description of cell biological systems at different levels of organization. Model states are chemical solutions that consist of dynamically nested, attributed entities. The model dynamics are described by rules that are constrained by arbitrary functions, which can operate on the entities’ attributes, (nested) solutions, and the reaction kinetics. Thus, ML-Rules supports an expressive hierarchical, variable structure modeling of cell biological systems. The formal syntax and semantics of ML-Rules show that it is firmly rooted in continuous-time Markov chains. In addition to a generic stochastic simulation algorithm for ML-Rules, we introduce several specialized algorithms that are able to handle subclasses of ML-Rules more efficiently. The algorithms are compared in a performance study, leading to conclusions on the relation between expressive power and computational complexity of rule-based modeling languages.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2615981627",
    "type": "article"
  },
  {
    "title": "Formalization of Weak Emergence in Multiagent Systems",
    "doi": "https://doi.org/10.1145/2815502",
    "publication_date": "2015-09-23",
    "publication_year": 2015,
    "authors": "Claudia Szabo; Yong Meng Teo",
    "corresponding_authors": "",
    "abstract": "Emergence becomes a distinguishing system feature as system complexity grows with the number of components, interactions, and connectivities. Examples of emergent behaviors include the flocking of birds, traffic jams, and hubs in social networks, among others. Despite significant research interest in recent years, there is a lack of formal methods to understand, identify, and predict emergent behavior in multiagent systems. Existing approaches either require detailed prior knowledge about emergent behavior or are computationally infeasible. This article introduces a grammar-based approach to formalize and identify the existence and extent of emergence without the need for prior knowledge of emergent properties. Our approach is based on weak (basic) emergence that is both generated and autonomous from the underlying agents. We employ formal grammars to capture agent interactions in the forms of words written on a common tape. Our formalism captures agents of diverse types and open systems. We propose an automated approach for the identification of emergent behavior and show its benefits through theoretical and experimental analysis. We also propose a significant reduction of state-space explosion through the use of our proposed degree of interaction metrics. Our experiments using the boids model show the feasibility of our approach but also highlight future avenues of improvement.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W2025291914",
    "type": "article"
  },
  {
    "title": "Risk Quantification in Stochastic Simulation under Input Uncertainty",
    "doi": "https://doi.org/10.1145/3329117",
    "publication_date": "2020-01-31",
    "publication_year": 2020,
    "authors": "Helin Zhu; Tianyi Liu; Enlu Zhou",
    "corresponding_authors": "",
    "abstract": "When simulating a complex stochastic system, the behavior of output response depends on input parameters estimated from finite real-world data, and the finiteness of data brings input uncertainty into the system. The quantification of the impact of input uncertainty on output response has been extensively studied. Most of the existing literature focuses on providing inferences on the mean response at the true but unknown input parameter, including point estimation and confidence interval construction. Risk quantification of mean response under input uncertainty often plays an important role in system evaluation and control, because it provides inferences on extreme scenarios of mean response in all possible input models. To the best of our knowledge, it has rarely been systematically studied in the literature. In this article, first we introduce risk measures of mean response under input uncertainty and propose a nested Monte Carlo simulation approach to estimate them. Then we develop asymptotical properties such as consistency and asymptotic normality for the proposed nested risk estimators. We further study the associated budget allocation problem for efficient nested risk simulation and finally use a sharing economy example to illustrate the importance of accessing and controlling risk due to input uncertainty.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W3122517437",
    "type": "article"
  },
  {
    "title": "Data-driven Crowd Modeling Techniques: A Survey",
    "doi": "https://doi.org/10.1145/3481299",
    "publication_date": "2022-01-07",
    "publication_year": 2022,
    "authors": "Jinghui Zhong; Dongrui Li; Zhixing Huang; Chengyu Lu; Wentong Cai",
    "corresponding_authors": "",
    "abstract": "Data-driven crowd modeling has now become a popular and effective approach for generating realistic crowd simulation and has been applied to a range of applications, such as anomaly detection and game design. In the past decades, a number of data-driven crowd modeling techniques have been proposed, providing many options for people to generate virtual crowd simulation. This article provides a comprehensive survey of these state-of-the-art data-driven modeling techniques. We first describe the commonly used datasets for crowd modeling. Then, we categorize and discuss the state-of-the-art data-driven crowd modeling methods. After that, data-driven crowd model validation techniques are discussed. Finally, six promising future research topics of data-driven crowd modeling are discussed.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W4206507298",
    "type": "article"
  },
  {
    "title": "On the use of self-similar processes in network simulation",
    "doi": "https://doi.org/10.1145/364996.365004",
    "publication_date": "2000-04-01",
    "publication_year": 2000,
    "authors": "J.C. López-Ardao; Cándido López-Garcı́a; Andrés Suárez-González; Manuel Fernández‐Veiga; Raúl F. Rodríguez‐Rubio",
    "corresponding_authors": "",
    "abstract": "Several recent traffic measurement studies have convincingly shown the presence of self-similarity in modern high-speed networks, involving a very important revolution in the stochastic modeling of traffic. Thus the use of self-similar processes has opened new problems and research fields in network performance analysis, mainly in simulation studies, where the efficient synthetic generation of sample paths (traces) corresponding to self-similar traffic is one of the main topics. In this article, we justify the selection of interarrival time instead of counting processes for modeling arrivals. Also, we discuss the advantages and drawbacks of the most important self-similar processes when applied to traffic modeling in simulation studies, proposing the use of models based in F-ARIMA, mainly due to their flexibility to capture both long- and short-range correlations. However, F-ARIMA processes have been little used in simulation studies, mainly because the synthetic generation methods available in the literature are very inefficient compared with those for FGN. In order to solve this problem, we propose a new method that can generate high-quality traces corresponding to a F-ARIMA(p, d, q) process. A comparison with existing methods shows that the new method is significantly more efficient, and even slightly better than the best method for FGN.",
    "cited_by_count": 56,
    "openalex_id": "https://openalex.org/W2134435105",
    "type": "article"
  },
  {
    "title": "Simulation run lengths to estimate blocking probabilities",
    "doi": "https://doi.org/10.1145/229493.229496",
    "publication_date": "1996-01-01",
    "publication_year": 1996,
    "authors": "R. Srikant; Ward Whitt",
    "corresponding_authors": "",
    "abstract": "We derive formulas approximating the asymptotic variance of four estimators for steady-state blocking probability in a multiserver loss system, exploiting diffusion process limits. These formulas can be used to predict simulation run lengths required to obtain desired statistical precision before the simulation has been run, which can aid in the design of simulation experiments. They also indicate that one estimator can be much better than another, depending on the loading. An indirect estimator based on estimating the mean occupancy is significantly more (less) efficient than a direct estimator for heavy (light) loads. A major concern is the way computational effort scales with system size. For all the estimators, the asymptotic variance tends to be inversely proportional to the system size, so that the computational effort (regarded as proportional to the product of the asymptotic variance and the arrival rate) does not grow as system size increases. Indeed, holding the blocking probability fixed, the computational effort with a good estimator decreases to zero as the system size increases. The asymptotic variance formulas also reveal the impact of the arrival-process and service-time variability on the statistical precision. We validate these formulas by comparing them to exact numerical results for the special case of the classical Erlang M/M/s/0 model and simulation estimates for more general G/GI/s/0 models. It is natural to delete an initial portion of the simulation run to allow the system to approach steady state when it starts out empty. For small to moderately size systems, the time to approach steady state tends to be negligible compared to the time required to obtain good estimates in steady state. However, as the system size increases, the time to approach steady state remains approximately unchanged, or even increases slightly, so that the computational effort associated with letting the system approach steady state becomes a greater portion of the overall computational effort as system size increases.",
    "cited_by_count": 55,
    "openalex_id": "https://openalex.org/W1976267292",
    "type": "article"
  },
  {
    "title": "On the lattice structure of the add-with-carry and subtract-with-borrow random number generators",
    "doi": "https://doi.org/10.1145/159737.159749",
    "publication_date": "1993-10-01",
    "publication_year": 1993,
    "authors": "Shu Tezuka; Pierre L’Ecuyer; Raymond Couture",
    "corresponding_authors": "",
    "abstract": "Marsaglia and Zaman recently proposed new classes of random number generators, called add-with-carry (AWC) and subtract-with-borrow (SWB), which are capable of quickly generating very long-period (pseudo)-random number sequences using very little memory. We show that these sequences are essentially equivalent to linear congruential sequences with very large prime moduli. So, the AWC/SWB generators can be viewed as efficient ways of implementing such large linear congruential generators. As a consequence, the theoretical properties of such generators can be studied in the same way as for linear congruential generators, namely, via the spectral and lattice tests. We also show how the equivalence can be exploited to implement efficient jumping-ahead facilities for the AWC and SWB sequences. Our numerical examples illustrate the fact that AWC/SWB generators have extremely bad lattice structure in high dimensions.",
    "cited_by_count": 54,
    "openalex_id": "https://openalex.org/W2057897685",
    "type": "article"
  },
  {
    "title": "Efficient simulation of a tandem Jackson network",
    "doi": "https://doi.org/10.1145/566392.566395",
    "publication_date": "2002-04-01",
    "publication_year": 2002,
    "authors": "Dirk P. Kroese; Victor F. Nicola",
    "corresponding_authors": "",
    "abstract": "The two-node tandem Jackson network serves as a convenient reference model for the analysis and testing of different methodologies and techniques in rare event simulation. In this paper we consider a new approach to efficiently estimate the probability that the content of the second buffer exceeds some high level L before it becomes empty, starting from a given state. The approach is based on a Markov additive process representation of the buffer processes, leading to an exponential change of measure to be used in an importance sampling procedure. Unlike changes of measures proposed and studied in recent literature, the one derived here is a function of the content of the first buffer. We prove that when the first buffer is finite, this method yields asymptotically efficient simulation for any set of arrival and service rates. In fact, the relative error is bounded independent of the level L ; a new result which is not established for any other known method. When the first buffer is infinite, we propose a natural extension of the exponential change of measure for the finite buffer case. In this case, the relative error is shown to be bounded (independent of L ) only when the second server is the bottleneck; a result which is known to hold for some other methods derived through large deviations analysis. When the first server is the bottleneck, experimental results using our method seem to suggest that the relative error is bounded linearly in L .",
    "cited_by_count": 54,
    "openalex_id": "https://openalex.org/W2090951154",
    "type": "article"
  },
  {
    "title": "Bad subsequences of well-known linear congruential pseudorandom number generators",
    "doi": "https://doi.org/10.1145/272991.273009",
    "publication_date": "1998-01-01",
    "publication_year": 1998,
    "authors": "Karl Entacher",
    "corresponding_authors": "Karl Entacher",
    "abstract": "We present a spectral test analysis of full-period subsequences with small step sizes generated by well-known linear congruential pseudorandom number generators. Subsequences may occur in certain simulation problems or as a method to get parallel streams of pseudorandom numbers. Applying the spectral test, it is possible to find bad subsequences with small step sizes for almost all linear pseudorandom number generators currently in use.",
    "cited_by_count": 53,
    "openalex_id": "https://openalex.org/W1976376596",
    "type": "article"
  },
  {
    "title": "Evaluating models of memory allocation",
    "doi": "https://doi.org/10.1145/174619.174624",
    "publication_date": "1994-01-01",
    "publication_year": 1994,
    "authors": "Benjamin G. Zorn; Dirk Grunwald",
    "corresponding_authors": "",
    "abstract": "Because dynamic memory management is an important part of a large class of computer programs, high-performance algorithms for dynamic memory management have been and will continue to be of considerable interest. The goal of this research is to explore the size and accuracy of synthetic models of program allocation behavior. These models, if accurate enough, proved an attractive alternative to algorithm evaluation based on trace-driven simulation using actual traces. Based on our analysis, we conclude that even relatively simple synthetic models can effectively emulate the allocation behavior of well-behaved programs. However, even the most complex models we investigate can only roughly approximate the behavior of more complex programs and/or allocation policies. While synthetic models have been used to evaluate the performance of dynamic memory management algorithms, our results show that these models can be inaccurate and must be used with care. Given current trends toward more complex applications and allocation algorithms, the synthetic models we investigate are likely to be even less accurate in the future.",
    "cited_by_count": 53,
    "openalex_id": "https://openalex.org/W2039222964",
    "type": "article"
  },
  {
    "title": "Modeling methodology for integrated simulation of embedded systems",
    "doi": "https://doi.org/10.1145/778553.778557",
    "publication_date": "2003-01-01",
    "publication_year": 2003,
    "authors": "Ákos Lédeczi; James P. Davis; Sandeep Neema; Aditya Agrawal",
    "corresponding_authors": "",
    "abstract": "Developing a single embedded application involves a multitude of different development tools including several different simulators. Most tools use different abstractions, have their own formalisms to represent the system under development, utilize different input and output data formats, and have their own semantics. A unified environment that allows capturing the system in one place and one that drives all necessary simulators and analysis tools from this shared representation needs a common representation technology that must support several different abstractions and formalisms seamlessly. Describing the individual formalisms by metamodels and carefully composing them is the underlying technology behind MILAN, a Model-based Integrated Simulation Framework. MILAN is an extensible framework that supports multigranular simulation of embedded systems by seamlessly integrating existing simulators into a unified environment. Formal metamodels and explicit constraints define the domain-specific modeling language developed for MILAN that combines hierarchical, heterogeneous, parametric dataflow representation with strong data typing. Multiple modeling aspects separate orthogonal concepts. The language also allows the representation of the design space of the application, not just a point solution. Nonfunctional requirements are captured as formal, application-specific constraints. MILAN has integrated tool support for design-space exploration and pruning. The models are used to automatically configure the integrated functional simulators, high-level performance and power estimators, cycle-accurate performance simulators, and power-aware simulators. Simulation results are used to automatically update the system models. The article focuses on the modeling methodology and briefly describes how the integrated models are utilized in the framework.",
    "cited_by_count": 53,
    "openalex_id": "https://openalex.org/W2054657305",
    "type": "article"
  },
  {
    "title": "Dynamic structure multiparadigm modeling and simulation",
    "doi": "https://doi.org/10.1145/937332.937335",
    "publication_date": "2003-07-01",
    "publication_year": 2003,
    "authors": "Fernando J. Barros",
    "corresponding_authors": "Fernando J. Barros",
    "abstract": "This article presents the Heterogeneous Flow System Specification (HFSS), a formalism aimed to represent hierarchical and modular hybrid flow systems with dynamic structure. The concept of hybrid flow systems provides a generalization of the conventional concept of hybrid system and it can represent a whole plethora of systems, namely: discrete event systems, multicomponent and multirate numerical methods, multirate and multicomponent sampling systems, event locators and time-varying systems. The ability to join all these types of models makes HFSS an excellent framework for merging components built in different paradigms. We present several examples of model definition in the HFSS formalism and we also exploit the ability of the HFSS formalism to represent mutirate numerical integrators.",
    "cited_by_count": 52,
    "openalex_id": "https://openalex.org/W1988002669",
    "type": "article"
  },
  {
    "title": "Inversive and linear congruential pseudorandom number generators in empirical tests",
    "doi": "https://doi.org/10.1145/249204.249208",
    "publication_date": "1997-04-01",
    "publication_year": 1997,
    "authors": "Hannes Leeb; Stefan Wegenkittl",
    "corresponding_authors": "",
    "abstract": "We present results from a series of empirical tests of pseudorandom number generators. The tests cover a broad range of designs due to bit-oriented, efficient test statistics and a testing procedure inwhich we vary the sample size, dimension, and the statistics' resolution within vast bounds. Inversive generation methods pass the tests for a broader range of thest parameters thatn linear generators with equal period legth. The results exemplify how the lattice structure of linear generatorsf can affect a stochastic simulation and suggest the use of inversive generators for cross-checking the results.",
    "cited_by_count": 52,
    "openalex_id": "https://openalex.org/W2027302670",
    "type": "article"
  },
  {
    "title": "An integrated approach to system modeling using a synthesis of artificial intelligence, software engineering and simulation methodologies",
    "doi": "https://doi.org/10.1145/149516.149530",
    "publication_date": "1992-10-01",
    "publication_year": 1992,
    "authors": "Paul A. Fishwick",
    "corresponding_authors": "Paul A. Fishwick",
    "abstract": "article An integrated approach to system modeling using a synthesis of artificial intelligence, software engineering and simulation methodologies Share on Author: Paul A. Fishwick Univ. of Florida, Gainesville Univ. of Florida, GainesvilleView Profile Authors Info & Claims ACM Transactions on Modeling and Computer SimulationVolume 2Issue 4Oct. 1992 pp 307–330https://doi.org/10.1145/149516.149530Online:01 October 1992Publication History 37citation1,309DownloadsMetricsTotal Citations37Total Downloads1,309Last 12 Months24Last 6 weeks5 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",
    "cited_by_count": 51,
    "openalex_id": "https://openalex.org/W2001198375",
    "type": "article"
  },
  {
    "title": "Synchronous relaxation for parallel simulations with applications to circuit-switched networks",
    "doi": "https://doi.org/10.1145/159737.159744",
    "publication_date": "1993-10-01",
    "publication_year": 1993,
    "authors": "Stephen G. Eick; Albert Greenberg; Boris D. Lubachevsky; Alan Weiss",
    "corresponding_authors": "",
    "abstract": "Synchronous relaxation , a new, general-purpose, efficient method for parallel simulation, is proposed. The method is applied to obtain a new parallel algorithm for simulating large circuit-switched communication networks. To show that synchronous-relaxation method is efficient, we present the results of circuit-switched network simulation experiments, and analytic approximations derived from a mathematical model of the simulation method.",
    "cited_by_count": 51,
    "openalex_id": "https://openalex.org/W2014950525",
    "type": "article"
  },
  {
    "title": "Computing the distribution function of a conditional expectation via monte carlo",
    "doi": "https://doi.org/10.1145/937332.937334",
    "publication_date": "2003-07-01",
    "publication_year": 2003,
    "authors": "Shinghoi Lee; Peter W. Glynn",
    "corresponding_authors": "",
    "abstract": "We examine different ways of numerically computing the distribution function of conditional expectations where the conditioning element takes values in a finite or countably infinite outcome space. Both the conditional expectation and the distribution function itself are computed via Monte Carlo simulation. Given a limited (and fixed) computer budget, the quality of the estimator is gauged by the inverse of its mean square error. It is a function of the fraction of the budget allocated to estimating the conditional expectation versus the amount of sampling done relative to the \"conditioning variable.\" We will present the asymptotically optimal rates of convergence for different estimators and resolve the trade-off between the bias and variance of the estimators. Moreover, central limit theorems are established for some of the estimators proposed. We will also provide algorithms for the practical implementation of two of the estimators and illustrate how confidence intervals can be formed in each case. Major potential application areas include calculation of Value at Risk (VaR) in the field of mathematical finance and Bayesian performance analysis.",
    "cited_by_count": 51,
    "openalex_id": "https://openalex.org/W2069311504",
    "type": "article"
  },
  {
    "title": "Parallel simulation of stochastic Petri nets using recurrence equations",
    "doi": "https://doi.org/10.1145/151527.151545",
    "publication_date": "1993-01-02",
    "publication_year": 1993,
    "authors": "François Baccelli; Miguel Canales",
    "corresponding_authors": "",
    "abstract": "article Parallel simulation of stochastic Petri nets using recurrence equations Share on Authors: François Baccelli View Profile , Miguel Canales View Profile Authors Info & Claims ACM Transactions on Modeling and Computer SimulationVolume 3Issue 1Jan. 1993 pp 20–41https://doi.org/10.1145/151527.151545Online:02 January 1993Publication History 40citation375DownloadsMetricsTotal Citations40Total Downloads375Last 12 Months6Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",
    "cited_by_count": 50,
    "openalex_id": "https://openalex.org/W2040209790",
    "type": "article"
  },
  {
    "title": "Ladder queue",
    "doi": "https://doi.org/10.1145/1103323.1103324",
    "publication_date": "2005-07-01",
    "publication_year": 2005,
    "authors": "Wai Teng Tang; Rick Siow Mong Goh; Ian Li-Jin Thng",
    "corresponding_authors": "",
    "abstract": "This article describes a new priority queue implementation for managing the pending event set in discrete event simulation. Extensive empirical results demonstrate that it consistently outperforms other current popular candidates. This new implementation, called Ladder Queue, is also theoretically justified to have O (1) amortized access time complexity, as long as the mean jump parameter of the priority increment distribution is finite and greater than zero, regardless of its variance. Many practical priority increment distributions satisfy this condition including unbounded variance distributions like the Pareto distribution. This renders the LadderQ the ideal discrete event queue structure for stable O (1) performance even under practical queue distributions with infinite variance. Numerical simulations ranging from 100 to 10 million events affirm the O (1) property of LadderQ and that it is a superior structure for large-scale discrete event simulation.",
    "cited_by_count": 49,
    "openalex_id": "https://openalex.org/W2028075868",
    "type": "article"
  },
  {
    "title": "Statistical sampling of microarchitecture simulation",
    "doi": "https://doi.org/10.1145/1147224.1147225",
    "publication_date": "2006-07-01",
    "publication_year": 2006,
    "authors": "Roland E. Wunderlich; Thomas F. Wenisch; Babak Falsafi; James C. Hoe",
    "corresponding_authors": "",
    "abstract": "Current software-based microarchitecture simulators are many orders of magnitude slower than the hardware they simulate. Hence, most microarchitecture design studies draw their conclusions from drastically truncated benchmark simulations that are often inaccurate and misleading. This article presents the Sampling Microarchitecture Simulation (SMARTS) framework as an approach to enable fast and accurate performance measurements of full-length benchmarks. SMARTS accelerates simulation by selectively measuring in detail only an appropriate benchmark subset. SMARTS prescribes a statistically sound procedure for configuring a systematic sampling simulation run to achieve a desired quantifiable confidence in estimates.Analysis of the SPEC CPU2000 benchmark suite shows that CPI and energy per instruction (EPI) can be estimated to within ±3% with 99.7% confidence by measuring fewer than 50 million instructions per benchmark. In practice, inaccuracy in microarchitectural state initialization introduces an additional uncertainty which we empirically bound to ∼2% for the tested benchmarks. Our implementation of SMARTS achieves an actual average error of only 0.64% on CPI and 0.59% on EPI for the tested benchmarks, running with average speedups of 35 and 60 over detailed simulation of 8-way and 16-way out-of-order processors, respectively.",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W2124917173",
    "type": "article"
  },
  {
    "title": "Common defects in initialization of pseudorandom number generators",
    "doi": "https://doi.org/10.1145/1276927.1276928",
    "publication_date": "2007-09-01",
    "publication_year": 2007,
    "authors": "Makoto Matsumoto; Isaku Wada; Ai Kuramoto; Hyo Ashihara",
    "corresponding_authors": "",
    "abstract": "We demonstrate that a majority of modern random number generators, such as the newest version of rand.c, ranlux, and combined multiple recursive generators, have some manifest correlations in their outputs if the initial state is filled up using another linear recurrence with similar modulus. Among 58 available generators in the GNU scientific library, 40 show such defects. This is not because of the recursion, but because of carelessly chosen initialization schemes in the implementations. A good initialization scheme eliminates this phenomenon.",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W1990752200",
    "type": "article"
  },
  {
    "title": "An adaptive approach to accelerated evaluation of highly available services",
    "doi": "https://doi.org/10.1145/1315575.1315576",
    "publication_date": "2007-12-01",
    "publication_year": 2007,
    "authors": "Chih-Chieh Hsu; Michael Devetsikiotis",
    "corresponding_authors": "",
    "abstract": "We motivate and describe improved fast simulation techniques for the accelerated performance evaluation of highly available services. In systems that provide such services, service unavailability events are rare due to a low component failure rate or high resource capacity. Using traditional Monte Carlo simulation to evaluate such services requires a large amount of runtime. Importance sampling (IS) has been applied to certain instances of such systems, focusing on single-class and/or homogeneous resource demands. In this article, we formulate highly available services as multiresource losstype systems, and we present two IS methods for fast simulation, extending to multiple classes and nonhomogeneous resource demands. First, for the cases in which component failure rates are small, we prove that static IS using the Standard Clock (S-ISSC) method exhibits the bounded relative error (BRE) property. Second, for estimating failure probabilities due to large capacity or fast service in systems that have nonrare component failure rates, we propose adaptive ISSC (A-ISSC), which estimates the relative probability of reaching each possible state of system failure in every step of the simulation. Using A-ISSC, IS methods which are proven to be efficient can be extended to multidimensional cases, while still retaining a very favorable performance, as supported by our validation experiments.",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W2041780793",
    "type": "article"
  },
  {
    "title": "Sample-based estimation of correlation ratio with polynomial approximation",
    "doi": "https://doi.org/10.1145/1315575.1315578",
    "publication_date": "2007-12-01",
    "publication_year": 2007,
    "authors": "Daniel Lewandowski; Roger Cooke; Radboud J. Duintjer Tebbens",
    "corresponding_authors": "",
    "abstract": "Sensitivity analysis has become a natural step in the uncertainty analysis framework. As there is no general sensitivity measure that would capture all information on impact of input factors on model output, analysts tend to combine various measures to obtain a broader image of interactions between different modes. This article concentrates on the correlation ratio, demonstrates methods for calculating this quantity efficiently and accurately, and compares the results. A new method inspired by artificial intelligence techniques emerges as outperforming the familiar methods.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W1970599243",
    "type": "article"
  },
  {
    "title": "Symbiotic adaptive multisimulation",
    "doi": "https://doi.org/10.1145/1456645.1456647",
    "publication_date": "2008-12-01",
    "publication_year": 2008,
    "authors": "Bradley Mitchell; Levent Yılmaz",
    "corresponding_authors": "",
    "abstract": "Inspired by the compound arthropod eye, Symbiotic Adaptive Multisimulation (SAMS) introduces an autonomic decision support capability for systems in shifting, ill-defined, uncertain environments. Rather than rely on a single authoritative model, SAMS explores an ensemble of plausible models, which are individually flawed but collectively provide more insight than would be possible otherwise. A case study based on a UAV team search and attack model is presented to illustrate the potential of SAMS. Results demonstrate the capability of SAMS to produce a large degree of exploratory behavior, followed by increased exploitative search behavior as the physical system unfolds.",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W1999368090",
    "type": "article"
  },
  {
    "title": "Multistep-ahead neural-network predictors for network traffic reduction in distributed interactive applications",
    "doi": "https://doi.org/10.1145/1276927.1276929",
    "publication_date": "2007-09-01",
    "publication_year": 2007,
    "authors": "Aaron McCoy; Tomás Ward; Séamus McLoone; Declan Delaney",
    "corresponding_authors": "",
    "abstract": "Predictive contract mechanisms such as dead reckoning are widely employed to support scalable remote entity modeling in distributed interactive applications (DIAs). By employing a form of controlled inconsistency, a reduction in network traffic is achieved. However, by relying on the distribution of instantaneous derivative information, dead reckoning trades remote extrapolation accuracy for low computational complexity and ease-of-implementation. In this article, we present a novel extension of dead reckoning, termed neuro-reckoning, that seeks to replace the use of instantaneous velocity information with predictive velocity information in order to improve the accuracy of entity position extrapolation at remote hosts. Under our proposed neuro-reckoning approach, each controlling host employs a bank of neural network predictors trained to estimate future changes in entity velocity up to and including some maximum prediction horizon. The effect of each estimated change in velocity on the current entity position is simulated to produce an estimate for the likely position of the entity over some short time-span. Upon detecting an error threshold violation, the controlling host transmits a predictive velocity vector that extrapolates through the estimated position, as opposed to transmitting the instantaneous velocity vector. Such an approach succeeds in reducing the spatial error associated with remote extrapolation of entity state. Consequently, a further reduction in network traffic can be achieved. Simulation results conducted using several human users in a highly interactive DIA indicate significant potential for improved scalability when compared to the use of IEEE DIS standard dead reckoning. Our proposed neuro-reckoning framework exhibits low computational resource overhead for real-time use and can be seamlessly integrated into many existing dead reckoning mechanisms.",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W2006751479",
    "type": "article"
  },
  {
    "title": "An efficient single-pass trace compression technique utilizing instruction streams",
    "doi": "https://doi.org/10.1145/1189756.1189758",
    "publication_date": "2007-01-01",
    "publication_year": 2007,
    "authors": "Aleksandar Milenković; Milena Milenković",
    "corresponding_authors": "",
    "abstract": "Trace-driven simulations have been widely used in computer architecture for quantitative evaluations of new ideas and design prototypes. Efficient trace compression and fast decompression are crucial for contemporary workloads, as representative benchmarks grow in size and number. This article presents Stream-Based Compression (SBC), a novel technique for single-pass compression of address traces. The SBC technique compresses both instruction and data addresses by associating them with a particular instruction stream, that is, a block of consecutively executing instructions. The compressed instruction trace is a trace of instruction stream identifiers. The compressed data address trace encompasses the data address stride and the number of repetitions for each memory-referencing instruction in a stream, ordered by the corresponding stream appearances in the trace. SBC reduces the size of SPEC CPU2000 Dinero instruction and data address traces from 18 to 309 times, outperforming the best trace compression techniques presented in the open literature. SBC can be successfully combined with general-purpose compression techniques. The combined SBC-gzip compression ratio is from 80 to 35,595, and the SBC-bzip2 compression ratio is from 75 to 191,257. Moreover, SBC outperforms other trace compression techniques when both decompression time and compression time are considered. This article also shows how the SBC algorithm can be modified for hardware implementation with very modest resources and only a minor loss in compression ratio.",
    "cited_by_count": 37,
    "openalex_id": "https://openalex.org/W2090590416",
    "type": "article"
  },
  {
    "title": "An analysis of queuing network simulation using GPU-based hardware acceleration",
    "doi": "https://doi.org/10.1145/1921598.1921602",
    "publication_date": "2011-02-04",
    "publication_year": 2011,
    "authors": "Hyungwook Park; Paul A. Fishwick",
    "corresponding_authors": "",
    "abstract": "Queuing networks are used widely in computer simulation studies. Examples of queuing networks can be found in areas such as the supply chains, manufacturing work flow, and internet routing. If the networks are fairly small in size and complexity, it is possible to create discrete event simulations of the networks without incurring significant delays in analyzing the system. However, as the networks grow in size, such analysis can be time consuming, and thus require more expensive parallel processing computers or clusters. We have constructed a set of tools that allow the analyst to simulate queuing networks in parallel, using the fairly inexpensive and commonly available graphics processing units (GPUs) found in most recent computing platforms. We present an analysis of a GPU-based algorithm, describing benefits and issues with the GPU approach. The algorithm clusters events, achieving speedup at the expense of an approximation error which grows as the cluster size increases. We were able to achieve 10-x speedup using our approach with a small error in a specific implementation of a synthetic closed queuing network simulation. This error can be mitigated, based on error analysis trends, obtaining reasonably accurate output statistics. The experimental results of the mobile ad hoc network simulation show that errors occur only in the time-dependent output statistics.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W2035693340",
    "type": "article"
  },
  {
    "title": "Simulating Lévy Processes from Their Characteristic Functions and Financial Applications",
    "doi": "https://doi.org/10.1145/2331140.2331142",
    "publication_date": "2012-08-01",
    "publication_year": 2012,
    "authors": "Zisheng Chen; Liming Feng; Lin Xiong",
    "corresponding_authors": "",
    "abstract": "The simulation of a discrete sample path of a Lévy process reduces to simulating from the distribution of a Lévy increment. For a general Lévy process with exponential moments, the inverse transform method proposed in Glasserman and Liu [2010] is reliable and efficient. The values of the cumulative distribution function (cdf) are computed by inverting the characteristic function and tabulated on a uniform grid. The inverse of the cumulative distribution function is obtained by linear interpolation. In this article, we apply a Hilbert transform method for the characteristic function inversion. The Hilbert transform representation for the cdf can be discretized using a simple rule highly accurately. Most importantly, the error estimates admit explicit and computable expressions, which allow us to compute the cdf to any desired accuracy. We present an explicit bound for the estimation bias in terms of the range of the grid where probabilities are tabulated, the step size of the grid, and the approximation error for the probabilities. The bound can be computed from the characteristic function directly and allows one to determine the size and fineness of the grid and numerical parameters for evaluating the Hilbert transforms for any given bias tolerance level in one-dimensional problems. For multidimensional problems, we present a procedure for selecting the grid and the numerical parameters that is shown to converge theoretically and works well practically. The inverse transform method is attractive not only for Lévy processes that are otherwise not easy to simulate, but also for processes with special structures that could be simulated in different ways. The method is very fast and accurate when combined with quasi-Monte Carlo schemes and variance reduction techniques. The main results we derived are not limited to Lévy processes and can be applied to simulating from tabulated cumulative distribution functions in general and characteristic functions in our analytic class in particular.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W3123817978",
    "type": "article"
  },
  {
    "title": "Introduction to Special Issue on Monte Carlo Methods in Statistics",
    "doi": "https://doi.org/10.1145/2414416.2414417",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Arnaud Doucet; Christian P. Robert",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W2167666241",
    "type": "article"
  },
  {
    "title": "Stochastic approximation algorithms for constrained optimization via simulation",
    "doi": "https://doi.org/10.1145/1921598.1921599",
    "publication_date": "2011-02-04",
    "publication_year": 2011,
    "authors": "Shalabh Bhatnagar; N. Hemachandra; Vivek Mishra",
    "corresponding_authors": "",
    "abstract": "We develop four algorithms for simulation-based optimization under multiple inequality constraints. Both the cost and the constraint functions are considered to be long-run averages of certain state-dependent single-stage functions. We pose the problem in the simulation optimization framework by using the Lagrange multiplier method. Two of our algorithms estimate only the gradient of the Lagrangian, while the other two estimate both the gradient and the Hessian of it. In the process, we also develop various new estimators for the gradient and Hessian. All our algorithms use two simulations each. Two of these algorithms are based on the smoothed functional (SF) technique, while the other two are based on the simultaneous perturbation stochastic approximation (SPSA) method. We prove the convergence of our algorithms and show numerical experiments on a setting involving an open Jackson network. The Newton-based SF algorithm is seen to show the best overall performance.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2059984770",
    "type": "article"
  },
  {
    "title": "Model Reconstruction for Moment-Based Stochastic Chemical Kinetics",
    "doi": "https://doi.org/10.1145/2699712",
    "publication_date": "2015-04-06",
    "publication_year": 2015,
    "authors": "Alexander Andreychenko; Linar Mikeev; Verena Wolf",
    "corresponding_authors": "",
    "abstract": "Based on the theory of stochastic chemical kinetics, the inherent randomness of biochemical reaction networks can be described by discrete-state continuous-time Markov chains. However, the analysis of such processes is computationally expensive and sophisticated numerical methods are required. Here, we propose an analysis framework in which we integrate a number of moments of the process instead of the state probabilities. This results in a very efficient simulation of the time evolution of the process. To regain the state probabilities from the moment representation, we combine the fast moment-based simulation with a maximum entropy approach for the reconstruction of the underlying probability distribution. We investigate the usefulness of this combined approach in the setting of stochastic chemical kinetics and present numerical results for three reaction networks showing its efficiency and accuracy. Besides a simple dimerization system, we study a bistable switch system and a multiattractor network with complex dynamics.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2294445361",
    "type": "article"
  },
  {
    "title": "Design and Verification of Trusted Collective Adaptive Systems",
    "doi": "https://doi.org/10.1145/3155337",
    "publication_date": "2018-02-22",
    "publication_year": 2018,
    "authors": "Alessandro Aldini",
    "corresponding_authors": "Alessandro Aldini",
    "abstract": "Collective adaptive systems (CAS) often adopt cooperative operating strategies to run distributed decision-making mechanisms. Sometimes, their effectiveness massively relies on the collaborative nature of individuals’ behavior. Stimulating cooperation while preventing selfish and malicious behaviors is the main objective of trust and reputation models. These models are largely used in distributed, peer-to-peer environments and, therefore, represent an ideal framework for improving the robustness, as well as security, of CAS. In this article, we propose a formal framework for modeling and verifying trusted CAS. From the modeling perspective, mobility, adaptiveness, and trust-based interaction represent the main ingredients used to define a flexible and easy-to-use paradigm. Concerning analysis, formal automated techniques based on equivalence and model checking support the prediction of the CAS behavior and the verification of the underlying trust and reputation models, with the specific aim of estimating robustness with respect to the typical attacks conducted against webs of trust.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2790861688",
    "type": "article"
  },
  {
    "title": "Transparently Mixing Undo Logs and Software Reversibility for State Recovery in Optimistic PDES",
    "doi": "https://doi.org/10.1145/3077583",
    "publication_date": "2017-04-30",
    "publication_year": 2017,
    "authors": "Davide Cingolani; Alessandro Pellegrini; Francesco Quaglia",
    "corresponding_authors": "",
    "abstract": "The Time Warp synchronization protocol for Parallel Discrete Event Simulation (PDES) is universally considered a viable solution to exploit the intrinsic simulation model parallelism and to provide model execution speedup. Yet it leads the PDES system to execute events in an order that may generate causal inconsistencies that need to be recovered via rollback , which requires restoration of a previous (consistent) simulation state whenever a causality violation is detected. The rollback operation is so critical for the performance of a Time Warp system that it has been extensively studied in the literature for decades to find approaches suitable to optimize it. The proposed solutions can be roughly classified as based on either checkpointing or reverse computing . In this article, we explore the practical design and implementation of a fully new approach based on the runtime generation of so-called undo code blocks , which are blocks of instructions implementing the reverse memory side effects generated by the forward execution of the events. However, this is not done by recomputing the original values to be restored, as instead it occurs in reverse computing schemes. Hence, the philosophy undo code blocks rely on is similar in spirit to that of undo-logs (as a form of checkpointing). Nevertheless, they are not data logs (as instead checkpoints are); rather, they are logs of instructions. Our proposal is fully transparent, thanks to the reliance on static software instrumentation (targeting the x86 architecture and Linux systems). Also, as we show, it can be combined with classical checkpointing to further improve the runtime behavior of the state recoverability support as a function of the workload. We also present experimental results related to our implementation, which is released as free software and fully integrated into the open source ROOT-Sim package. Experimental data support the viability and effectiveness of our proposal.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2618211684",
    "type": "article"
  },
  {
    "title": "GDCSim",
    "doi": "https://doi.org/10.1145/2553083",
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "Sandeep K. S. Gupta; Ayan Banerjee; Zahra Abbasi; Georgios Varsamopoulos; Michael Jonas; Joshua Ferguson; Rose Robin Gilbert; Tridib Mukherjee",
    "corresponding_authors": "",
    "abstract": "Energy-efficient data center design and management has been a challenge of increasing importance in the past decade due to its potential to save billions of dollars in energy costs. However, the state of the art in design and evaluation of data centers require designers to be expertly familiar with a prohibitively large number of domain-specific design tools that necessitate user intervention in each step of the design process. This is due to the lack of a holistic data center design tool. To fill this gap, this article presents an iterative green data center design framework, the Green Data Center Simulator (GDCSim), for the design and development of energy-efficient data centers.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2085448152",
    "type": "article"
  },
  {
    "title": "Statistical Debugging for Simulations",
    "doi": "https://doi.org/10.1145/2699722",
    "publication_date": "2015-04-16",
    "publication_year": 2015,
    "authors": "Ross Gore; Paul F. Reynolds; David Kamensky; Saikou Y. Diallo; José J. Padilla",
    "corresponding_authors": "",
    "abstract": "Predictions from simulations have entered the mainstream of public policy and decision-making practices. Unfortunately, methods for gaining insight into faulty simulations outputs have not kept pace. Ideally, an insight gathering method would automatically identify the cause of a faulty output and explain to the simulation developer how to correct it. In the field of software engineering, this challenge has been addressed for general-purpose software through statistical debuggers. We present two research contributions, elastic predicates and many-valued labeling functions , that enable debuggers designed for general-purpose software to become more effective for simulations employing random variates and continuous numbers. Elastic predicates address deficiencies of existing debuggers related to continuous numbers, whereas many-valued labeling functions support the use of random variates. When used in combinations, these contributions allow a simulation developer tasked with localizing the program statement causing the faulty simulation output to examine 40% fewer statements than the leading alternatives. Our evaluation shows that elastic predicates and many-valued labeling functions maintain their ability to reduce the number of program statements that need to be examined under the imperfect conditions that developers experience in practice.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2008807403",
    "type": "article"
  },
  {
    "title": "Efficient Parallel Discrete Event Simulation on Cloud/Virtual Machine Platforms",
    "doi": "https://doi.org/10.1145/2746232",
    "publication_date": "2015-07-01",
    "publication_year": 2015,
    "authors": "Srikanth B. Yoginath; Kalyan S. Perumalla",
    "corresponding_authors": "",
    "abstract": "Cloud and Virtual Machine (VM) technologies present new challenges with respect to performance and monetary cost in executing parallel discrete event simulation (PDES) applications. Due to the introduction of overall cost as a metric, the traditional use of the highest-end computing configuration is no longer the most obvious choice. Moreover, the unique runtime dynamics and configuration choices of Cloud and VM platforms introduce new design considerations and runtime characteristics specific to PDES over Cloud/VMs. Here, an empirical study is presented to help understand the dynamics, trends, and trade-offs in executing PDES on Cloud/VM platforms. Performance and cost measures obtained from multiple PDES applications executed on the Amazon EC2 Cloud and on a high-end VM host machine reveal new, counterintuitive VM--PDES dynamics and guidelines. One of the critical aspects uncovered is the fundamental mismatch in hypervisor scheduler policies designed for general Cloud workloads versus the virtual time ordering needed for PDES workloads. This insight is supported by experimental data revealing the gross deterioration in PDES performance traceable to VM scheduling policy. To overcome this fundamental problem, the design and implementation of a new deadlock-free scheduler algorithm are presented, optimized specifically for PDES applications on VMs. The scalability of our scheduler has been tested in up to 128 VMs multiplexed on 32 cores, showing significant improvement in the runtime relative to the default Cloud/VM scheduler. The observations, algorithmic design, and results are timely for emerging Cloud/VM-based installations, highlighting the need for PDES-specific support in high-performance discrete event simulations on Cloud/VM platforms.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2147694014",
    "type": "article"
  },
  {
    "title": "Distributed Approaches to Supply Chain Simulation",
    "doi": "https://doi.org/10.1145/3466170",
    "publication_date": "2021-08-16",
    "publication_year": 2021,
    "authors": "Navonil Mustafee; Korina Katsaliaki; Simon J. E. Taylor",
    "corresponding_authors": "",
    "abstract": "The field of Supply Chain Management (SCM ) is experiencing rapid strides in the use of Industry 4.0 technologies and the conceptualization of new supply chain configurations for online retail, sustainable and green supply chains, and the Circular Economy. Thus, there is an increasing impetus to use simulation techniques such as discrete-event simulation, agent-based simulation, and hybrid simulation in the context of SCM. In conventional supply chain simulation, the underlying constituents of the system like manufacturing, distribution, retail, and logistics processes are often modelled and executed as a single model. Unlike this conventional approach, a distributed supply chain simulation (DSCS) enables the coordinated execution of simulation models using specialist software. To understand the current state-of-the-art of DSCS, this paper presents a methodological review and categorization of literature in DSCS using a framework-based approach. Through a study of over 130 articles, we report on the motivation for using DSCS, the modelling techniques, the underlying distributed computing technologies and middleware, its advantages and a future agenda, and also limitations and trade-offs that may be associated with this approach. The increasing adoption of technologies like Internet-of-Things and Cloud Computing will ensure the availability of both data and models for distributed decision-making, which is likely to enable data-driven DSCS of the future. This review aims to inform organizational stakeholders, simulation researchers and practitioners, distributed systems developers and software vendors, as to the current state-of-the art of DSCS, and which will inform the development of future DSCS using new applied computing approaches.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W3193311371",
    "type": "article"
  },
  {
    "title": "Automatic Reuse, Adaption, and Execution of Simulation Experiments via Provenance Patterns",
    "doi": "https://doi.org/10.1145/3564928",
    "publication_date": "2022-09-27",
    "publication_year": 2022,
    "authors": "Pia Wilsdorf; Anja Wolpers; Jason Hilton; Fiete Haack; Adelinde M. Uhrmacher",
    "corresponding_authors": "",
    "abstract": "Simulation experiments are typically conducted repeatedly during the model development process, for example, to revalidate if a behavioral property still holds after several model changes. Approaches for automatically reusing and generating simulation experiments can support modelers in conducting simulation studies in a more systematic and effective manner. They rely on explicit experiment specifications and, so far, on user interaction for initiating the reuse. Thereby, they are constrained to support the reuse of simulation experiments in a specific setting. Our approach now goes one step further by automatically identifying and adapting the experiments to be reused for a variety of scenarios. To achieve this, we exploit provenance graphs of simulation studies, which provide valuable information about the previous modeling and experimenting activities, and contain meta-information about the different entities that were used or produced during the simulation study. We define provenance patterns and associate them with a semantics, which allows us to interpret the different activities and construct transformation rules for provenance graphs. Our approach is implemented in a Reuse and Adapt framework for Simulation Experiments (RASE), which can interface with various modeling and simulation tools. In the case studies, we demonstrate the utility of our framework for (1) the repeated sensitivity analysis of an agent-based model of migration routes and (2) the cross-validation of two models of a cell signaling pathway.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W3200472368",
    "type": "article"
  },
  {
    "title": "A Language for Agent-based Discrete-event Modeling and Simulation of Linked Lives",
    "doi": "https://doi.org/10.1145/3486634",
    "publication_date": "2022-01-07",
    "publication_year": 2022,
    "authors": "Oliver Reinhardt; Tom Warnke; Adelinde M. Uhrmacher",
    "corresponding_authors": "",
    "abstract": "In agent-based modeling and simulation, discrete-time methods prevail. While there is a need to cover the agents’ dynamics in continuous time, commonly used agent-based modeling frameworks offer little support for discrete-event simulation. Here, we present a formal syntax and semantics of the language ML3 (Modeling Language for Linked Lives) for modeling and simulating multi-agent systems as discrete-event systems. The language focuses on applications in demography, such as migration processes, and considers this discipline’s specific requirements. These include the importance of life courses being linked and the age-dependency of activities and events. The developed abstract syntax of the language combines the metaphor of agents with guarded commands. Its semantics is defined in terms of Generalized Semi-Markov Processes. The concrete language has been realized as an external domain-specific language. We discuss implications for efficient simulation algorithms and elucidate benefits of formally defining domain-specific languages for modeling and simulation.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W3204383674",
    "type": "article"
  },
  {
    "title": "Optimizing Reachability Probabilities for a Restricted Class of Stochastic Hybrid Automata via Flowpipe Construction",
    "doi": "https://doi.org/10.1145/3607197",
    "publication_date": "2023-07-11",
    "publication_year": 2023,
    "authors": "Carina da Silva; Stefan Schupp; Anne Remke",
    "corresponding_authors": "",
    "abstract": "Stochastic hybrid automata (SHA) are a powerful tool to evaluate the dependability and safety of critical infrastructures. However, the resolution of nondeterminism, which is present in many purely hybrid models, is often only implicitly considered in SHA. This article instead proposes algorithms for computing maximum and minimum reachability probabilities for singular automata with urgent transitions and random clocks that follow arbitrary continuous probability distributions. We borrow a well-known approach from hybrid systems reachability analysis, namely flowpipe construction, which is then extended to optimize nondeterminism in the presence of random variables. First, valuations of random clocks that ensure reachability of specific goal states are extracted from the computed flowpipes, and second, reachability probabilities are computed by integrating over these valuations. We compute maximum and minimum probabilities for history-dependent prophetic and non-prophetic schedulers using set-based methods. The implementation featuring the library HyPro and the complexity of the approach are discussed in detail. Two case studies featuring nondeterministic choices show the feasibility of the approach.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4383982168",
    "type": "article"
  },
  {
    "title": "Spatial/Temporal Locality-based Load-sharing in Speculative Discrete Event Simulation on Multi-core Machines",
    "doi": "https://doi.org/10.1145/3639703",
    "publication_date": "2024-01-08",
    "publication_year": 2024,
    "authors": "Federica Montesano; Romolo Marotta; Francesco Quaglia",
    "corresponding_authors": "",
    "abstract": "Shared-memory multi-processor/multi-core machines have become a reference for many application contexts. In particular, the recent literature on speculative parallel discrete event simulation has reshuffled the architectural organization of simulation systems in order to deeply exploit the main features of this type of machine. A core aspect dealt with has been the full sharing of the workload at the level of individual simulation events, which enables keeping the rollback incidence minimal. However, making each worker thread continuously switch its execution between events destined to different simulation objects does not favor locality. This problem appears even more evident in the case of Non-Uniform-Memory-Access (NUMA) machines, in which memory accesses generating a cache miss to be served by a far NUMA node give rise to both higher latency and higher traffic at the level of the NUMA interconnection. In this article, we propose a workload-sharing algorithm in which the worker threads can have short-term binding with specific simulation objects to favor spatial locality. The new bindings—carried out when a thread decides to switch its execution to other simulation objects—are based on both (a) the timeline according to which the object states have passed through the caching hierarchy and (b) the (dynamic) placement of objects within the NUMA architecture. At the same time, our solution still enables the worker threads to focus their activities on the events to be processed whose timestamps are closer to the simulation commit horizon—hence, we exploit temporal locality along virtual time and keep the rollback incidence minimal. In our design, we exploit lock-free constructs to support scalable thread synchronization while accessing the shared event pool. Furthermore, we exploit a multi-view approach of the event pool content, which additionally favors local accesses to the parts of the event pool that are currently relevant for the thread activity. Our solution has been released as an integration within the USE (Ultimate-Share-Everything) open-source speculative simulation platform available to the community. Furthermore, in this article we report the results of an experimental study that shows the effectiveness of our proposal.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4390658931",
    "type": "article"
  },
  {
    "title": "Overlapping Batch Confidence Intervals on Statistical Functionals Constructed from Time Series: Application to Quantiles, Optimization, and Estimation",
    "doi": "https://doi.org/10.1145/3649437",
    "publication_date": "2024-03-14",
    "publication_year": 2024,
    "authors": "Ziwei Su; Raghu Pasupathy; Yingchieh Yeh; Peter W. Glynn",
    "corresponding_authors": "",
    "abstract": "We propose a general purpose confidence interval procedure (CIP) for statistical functionals constructed using data from a stationary time series. The procedures we propose are based on derived distribution-free analogues of the χ 2 and Student’s t random variables for the statistical functional context and hence apply in a wide variety of settings including quantile estimation, gradient estimation, M-estimation, Conditional Value at Risk (CVaR) estimation, and arrival process rate estimation, apart from more traditional statistical settings. Like the method of subsampling, we use overlapping batches (OB) of time-series data to estimate the underlying variance parameter; unlike subsampling and the bootstrap, however, we assume that the implied point estimator of the statistical functional obeys a central limit theorem (CLT) to help identify the weak asymptotics (called OB-x limits, x = I, II, III) of batched Studentized statistics. The OB-x limits, certain functionals of the Wiener process parameterized by the size of the batches and the extent of their overlap, form the essential machinery for characterizing dependence and, consequently, the correctness of the proposed CIPs. The message from extensive numerical experimentation is that in settings where a functional CLT on the point estimator is in effect, using large overlapping batches alongside OB-x critical values yields confidence intervals that are often of significantly higher quality than those obtained from more generic methods like subsampling or the bootstrap. We illustrate using examples from CVaR estimation, ARMA parameter estimation, and non-homogeneous Poisson process rate estimation; R and MATLAB code for OB-x critical values is available at web.ics.purdue.edu/∼pasupath .",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4392806501",
    "type": "article"
  },
  {
    "title": "ENHANCE: Multilevel Heterogeneous Performance-Aware Re-Partitioning Algorithm For Microscopic Vehicle Traffic Simulation",
    "doi": "https://doi.org/10.1145/3670401",
    "publication_date": "2024-06-04",
    "publication_year": 2024,
    "authors": "Anibal Siguenza-Torres; Alexander Wieder; Zhuoxiao Meng; Santiago Narvaez Rivas; Mingyue Gao; Margherita Grossi; Xiaorui Du; Stefano Bortoli; Wentong Cai; Alois Knoll",
    "corresponding_authors": "",
    "abstract": "Driven by our work on a large-scale distributed microscopic road traffic simulator, we present ENHANCE, a novel re-partitioning approach that allows incorporating fine-grained simulator-specific cost models into the partitioning process to account for the actual performance characteristics of the simulator. The use of explicit cost models enables partitioning for heterogeneous resources, which are a common occurrence in cloud deployments. Importantly, ENHANCE can be used in conjunction with other partitioning approaches by further enhancing partitions according to provided cost models. We demonstrate the benefits of our approach in an experimental evaluation showing performance improvements of up to 29% against METIS under heterogeneous conditions. Taking a different perspective, the partitioning produced by ENHANCE can provide similar performance as METIS, but using up to 20% fewer resources.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4399320957",
    "type": "article"
  },
  {
    "title": "A case study of verification, validation, and accreditation for advanced distributed simulation",
    "doi": "https://doi.org/10.1145/259207.259375",
    "publication_date": "1997-07-01",
    "publication_year": 1997,
    "authors": "Ernest H. Page; Bradford S. Canova; John A. Tufarolo",
    "corresponding_authors": "",
    "abstract": "The techniques and methodologies for verification and validation of software-based systems have arguably realized their greatest utility within the context of simulation. Advanced Distributed Simulation (ADS), a major initiative within the defense modeling and simulation community, presents a variety of challenges to the classical approaches. A case study of the development process and concomitant verification and validation activities for the Joint Training Confederation (JTC) is presented. The JTC is one of the largest current ADS efforts, and the primary application of the Aggregate Level Simulation Protocol. A dichotomy between classical verification and validation approaches and the requirements of a prototypical ADS environment is illustrated. Mechanisms and research directions to resolve these differences are briefly discussed.",
    "cited_by_count": 49,
    "openalex_id": "https://openalex.org/W2019030560",
    "type": "article"
  },
  {
    "title": "Two-stage multiple-comparison procedures for steady-state simulations",
    "doi": "https://doi.org/10.1145/301677.301679",
    "publication_date": "1999-01-01",
    "publication_year": 1999,
    "authors": "Halim Damerdji; Marvin K. Nakayama",
    "corresponding_authors": "",
    "abstract": "Procedures for multiple comparisons with the best are investigated in the context of steady-state simulation, whereby a number k of different systems (stochastic processes) are compared based upon their (asymptotic) means μ i ( i = 1,2,…, k ). The variances of these (asymptotically stationary) processes are assumed to be unknown and possibly unequal. We consider the problem of constructing simultaneous confidence intervals for μ i -max j≠i μ j ( i=1,2,…,k) which is known as multiple comparisons with the best (MCB). Our intervals are constrained to contain 0, and so are called constrained MCB intervals. In particular, two-stage procedures for construction of absolute- and relative-width confidence intervals are presented. Their validity is addressed by showing that the confidence intervals cover the parameters with probability of at least some user-specified threshold value, as the confidence intervals' width parameter shrinks to 0. The general assumption about the processes is that they satisfy a functional central limit theorem. The simulation output analysis procedures are based on the method of standardized time series (the batch means method is a special case). The techniques developed here extend to other multiple-comparison procedures such as unconstrained MCB, multiple comparisons with a control, and all-pairwise comparisons. Although simulation is the context in this paper, the results naturally apply to (asymptotically) stationary time series.",
    "cited_by_count": 49,
    "openalex_id": "https://openalex.org/W2078722281",
    "type": "article"
  },
  {
    "title": "Technical note",
    "doi": "https://doi.org/10.1145/295251.295259",
    "publication_date": "1998-10-01",
    "publication_year": 1998,
    "authors": "P. S. Coe; F. Howell; Roland N. Ibbett; L. M. Williams",
    "corresponding_authors": "",
    "abstract": "A hierarchical computer architecture design and simulation environment (HASE) has been developed at the University of Edinburgh. HASE allows rapid development and exploration of computer architectures at multiple levels of abstraction, encompassing both hardware and software. It has five modes of operation (Design, Model Validation, Build Simulation, Simulate System, and Experiment) which formalize the design cycle and allow a proper separation of concerns among the different phases of simulation activity. The software of HASE itself includes a project data storage facility, a discrete-event simulation engine, graphical display/ editing mechanisms, a visualization mechanism, and tools for setting up experiments and gathering results. HASE has been used in a number of research and student projects and these exemplify many of the interesting features of HASE and their relation to designing, simulating and evaluating scalable systems. They include the modeling of scalable implementations of the hierarchical PRAM model of parallel computation on a 2-D mesh, the evaluation of the performance of multiprocessor interconnection networks, and a model of the Stanford DASH architecture.",
    "cited_by_count": 49,
    "openalex_id": "https://openalex.org/W2089507843",
    "type": "article"
  },
  {
    "title": "Metamodeling in EIA/CDIF---meta-metamodel and metamodels",
    "doi": "https://doi.org/10.1145/643120.643124",
    "publication_date": "2002-10-01",
    "publication_year": 2002,
    "authors": "Rony G. Flatscher",
    "corresponding_authors": "Rony G. Flatscher",
    "abstract": "This article introduces the EIA/CDIF set of standards for the modeling of information systems and its exchange among computer-aided software tools of different vendors. It lays out the meta-metamodel and the standardized metamodels which are fully depicted in a hierarchical layout and annotated with the unique identifiers of all the standardized modeling concepts. The article also stresses the fact that EIA/CDIF has been used as the baseline in the creation of an international standard, the ISO/CDIF set of models, an ongoing project.",
    "cited_by_count": 47,
    "openalex_id": "https://openalex.org/W2038096666",
    "type": "article"
  },
  {
    "title": "Guest editorial",
    "doi": "https://doi.org/10.1145/643120.643121",
    "publication_date": "2002-10-01",
    "publication_year": 2002,
    "authors": "Pieter J. Mosterman; Hans Vangheluwe",
    "corresponding_authors": "",
    "abstract": "editorial Free Access Share on Guest editorial: Special issue on computer automated multi-paradigm modeling Authors: Pieter J. Mosterman The MathWorks, Inc. Natick, Massachusetts The MathWorks, Inc. Natick, MassachusettsView Profile , Hans Vangheluwe McGill University Montreal, Canada McGill University Montreal, CanadaView Profile Authors Info & Claims ACM Transactions on Modeling and Computer SimulationVolume 12Issue 4October 2002 pp 249–255https://doi.org/10.1145/643120.643121Online:01 October 2002Publication History 34citation701DownloadsMetricsTotal Citations34Total Downloads701Last 12 Months13Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 45,
    "openalex_id": "https://openalex.org/W2085495298",
    "type": "editorial"
  },
  {
    "title": "A component-based approach to modeling and simulating mixed-signal and hybrid systems",
    "doi": "https://doi.org/10.1145/643120.643125",
    "publication_date": "2002-10-01",
    "publication_year": 2002,
    "authors": "Jie Liu; Edward A. Lee",
    "corresponding_authors": "",
    "abstract": "Systems with both continuous and discrete behaviors can be modeled using a mixed-signal style or a hybrid systems style. This article presents a component-based modeling and simulation framework that supports both modeling styles. The component framework, based on an actor metamodel, takes a hierarchical approach to manage heterogeneity in modeling complex systems. We describe how ordinary differential equations, discrete event systems, and finite-state machines can be built under this metamodel. A mixed-signal system is a hierarchical composition of continuous-time and discrete event models, and a hybrid system is a hierarchical composition of continuous-time and finite-state-machine models. Hierarchical composition and information hiding help build clean models and efficient execution engines. Simulation technologies, in particular, the interaction between a continuous-time ODE solving engine and various discrete simulation engines are discussed. A signal type system is introduced to schedule hybrid components inside a continuous-time environment. Breakpoints are used to control the numerical integration step sizes so that discrete events are handled properly. A \"refiring\" mechanism and a \"rollback\" mechanism are designed to manage continuous components inside a discrete event environment. The technologies are implemented in the Ptolemy II software environment. Examples are given to show the applications of this framework in mixed-signal and hybrid systems.",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W2016124853",
    "type": "article"
  },
  {
    "title": "Variance reduction through smoothing and control variates for Markov chain simulations",
    "doi": "https://doi.org/10.1145/174153.174154",
    "publication_date": "1993-07-01",
    "publication_year": 1993,
    "authors": "Sigrún Andradóttir; Daniel P. Heyman; Teunis J. Ott",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Variance reduction through smoothing and control variates for Markov chain simulations Authors: Sigrún Andradóttir Univ. of Wisconsin–Madison, Madison Univ. of Wisconsin–Madison, MadisonView Profile , Daniel P. Heyman Bellcore, Morristown, NJ Bellcore, Morristown, NJView Profile , Teunis J. Ott Bellcore, Morristown, NJ Bellcore, Morristown, NJView Profile Authors Info & Claims ACM Transactions on Modeling and Computer SimulationVolume 3Issue 3July 1993 pp 167–189https://doi.org/10.1145/174153.174154Published:01 July 1993Publication History 25citation420DownloadsMetricsTotal Citations25Total Downloads420Last 12 Months11Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 42,
    "openalex_id": "https://openalex.org/W2003423964",
    "type": "article"
  },
  {
    "title": "A framework for simulation of surrounding vehicles in driving simulators",
    "doi": "https://doi.org/10.1145/1371574.1371575",
    "publication_date": "2008-07-01",
    "publication_year": 2008,
    "authors": "Johan Olstam; Jan T. Lundgren; Mikael Adlers; Pontus Matstoms",
    "corresponding_authors": "",
    "abstract": "This article describes a framework for generation and simulation of surrounding vehicles in a driving simulator. The proposed framework generates a traffic stream, corresponding to a given target flow and simulates realistic interactions between vehicles. The framework is based on an approach in which only a limited area around the driving simulator vehicle is simulated. This closest neighborhood is divided into one inner area and two outer areas. Vehicles in the inner area are simulated according to a microscopic simulation model including advanced submodels for driving behavior while vehicles in the outer areas are updated according to a less time-consuming mesoscopic simulation model. The presented work includes a new framework for generating and simulating vehicles within a moving area. It also includes the development of an enhanced model for overtakings and a simple mesoscopic traffic model. The framework has been validated on the number of vehicles that catch up with the driving simulator vehicle and vice versa. The agreement is good for active and passive catch-ups on rural roads and for passive catch-ups on freeways, but less good for active catch-ups on freeways. The reason for this seems to be deficiencies in the utilized lane-changing model. It has been verified that the framework is able to achieve the target flow and that there is a gain in computational time of using the outer areas. The framework has also been tested within the VTI Driving simulator III.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W2068283841",
    "type": "article"
  },
  {
    "title": "Joint congestion control and distributed scheduling for throughput guarantees in wireless networks",
    "doi": "https://doi.org/10.1145/1870085.1870090",
    "publication_date": "2010-12-01",
    "publication_year": 2010,
    "authors": "Gaurav Sharma; Changhee Joo; Ness B. Shroff; Ravi R. Mazumdar",
    "corresponding_authors": "",
    "abstract": "We consider the problem of throughput-optimal cross-layer design of wireless networks. We propose a joint congestion control and scheduling algorithm that achieves a fraction 1/ d I ( G ) of the capacity region, where d I ( G ) depends on certain structural properties of the underlying connectivity graph G of the wireless network, and also on the type of interference constraints. For a wide range of wireless networks, d I ( G ) can be upper bounded by a constant, independent of the number of nodes in the network. The scheduling element of our algorithm is the maximal scheduling policy. Although this scheduling policy has been considered in several previous works, the challenges underlying its practical implementation in a fully distributed manner while accounting for necessary message exchanges have not been addressed in the literature. In this article, we propose two algorithms for the distributed implementation of the maximal scheduling policy accounting for message exchanges, and analytically show that they still can achieve the performance guarantee under the 1-hop and 2-hop interference models. We also evaluate the performance of our cross-layer solutions in more realistic network settings with imperfect synchronization under the Signal-to-Interference-Plus-Noise Ratio (SINR) interference model, and compare with the standard layered approaches such as TCP over IEEE 802.11b DCF networks.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2000167291",
    "type": "article"
  },
  {
    "title": "Probabilistic analysis of simulation-based games",
    "doi": "https://doi.org/10.1145/1842713.1842719",
    "publication_date": "2010-09-01",
    "publication_year": 2010,
    "authors": "Yevgeniy Vorobeychik",
    "corresponding_authors": "Yevgeniy Vorobeychik",
    "abstract": "The field of game theory has proved to be of great importance in modeling interactions between self-interested parties in a variety of settings. Traditionally, game-theoretic analysis relied on highly stylized models to provide interesting insights about problems at hand. The shortcoming of such models is that they often do not capture vital detail. On the other hand, many real strategic settings, such as sponsored search auctions and supply-chains, can be modeled in high resolution using simulations. Recently, a number of approaches have been introduced to perform analysis of game-theoretic scenarios via simulation-based models. The first contribution of this work is the asymptotic analysis of Nash equilibria obtained from simulation-based models. The second contribution is to derive expressions for probabilistic bounds on the quality of Nash equilibrium solutions obtained using simulation data. In this vein, we derive very general distribution-free bounds, as well as bounds which rely on the standard normality assumptions, and extend the bounds to infinite games via Lipschitz continuity. Finally, we introduce a new maximum-a-posteriori estimator of Nash equilibria based on game-theoretic simulation data and show that it is consistent and almost surely unique.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2023986907",
    "type": "article"
  },
  {
    "title": "Guest editors' introduction",
    "doi": "https://doi.org/10.1145/1870085.1870086",
    "publication_date": "2010-12-01",
    "publication_year": 2010,
    "authors": "Michael Devetsikiotis; Fabrizio Granelli",
    "corresponding_authors": "",
    "abstract": "introduction Share on Guest editors' introduction: Special issue on modeling and simulation of cross-layer interactions in communication networks Authors: Michael Devetsikiotis North carolina state university North carolina state universityView Profile , Fabrizio Granelli University of trento University of trentoView Profile Authors Info & Claims ACM Transactions on Modeling and Computer SimulationVolume 21Issue 1December 2010 Article No.: 1pp 1–4https://doi.org/10.1145/1870085.1870086Published:17 December 2010Publication History 0citation178DownloadsMetricsTotal Citations0Total Downloads178Last 12 Months2Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2041373939",
    "type": "article"
  },
  {
    "title": "Retrospective-approximation algorithms for the multidimensional stochastic root-finding problem",
    "doi": "https://doi.org/10.1145/1502787.1502788",
    "publication_date": "2009-03-01",
    "publication_year": 2009,
    "authors": "Raghu Pasupathy; Bruce W. Schmeiser",
    "corresponding_authors": "",
    "abstract": "The stochastic root-finding problem (SRFP) is that of solving a nonlinear system of equations using only a simulation that provides estimates of the functions at requested points. Equivalently, SRFPs seek locations where an unknown vector function attains a given target using only a simulation capable of providing estimates of the function. SRFPs find application in a wide variety of physical settings. We develop a family of retrospective-approximation (RA) algorithms called Bounding RA that efficiently solves a certain class of multidimensional SRFPs. During each iteration, Bounding RA generates and solves a sample-path problem by identifying a polytope of stipulated diameter, with an image that bounds the given target to within stipulated tolerance. Across iterations, the stipulations become increasingly stringent, resulting in a sequence of shrinking polytopes that approach the correct solution. Efficiency results from: (i) the RA structure, (ii) the idea of using bounding polytopes to exploit problem structure, and (iii) careful step-size and direction choice during algorithm evolution. Bounding RA has good finite-time performance that is robust with respect to the location of the initial solution, and algorithm parameter values. Empirical tests suggest that Bounding RA outperforms Simultaneous Perturbation Stochastic Approximation (SPSA), which is arguably the best-known algorithm for solving SRFPs.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2621667117",
    "type": "article"
  },
  {
    "title": "Modeling and simulation of pedestrian behaviors in crowded places",
    "doi": "https://doi.org/10.1145/1921598.1921604",
    "publication_date": "2011-02-04",
    "publication_year": 2011,
    "authors": "Wee Lit Koh; Suiping Zhou",
    "corresponding_authors": "",
    "abstract": "Pedestrian simulation has many applications in computer games, military simulations, and animation systems. A realistic pedestrian simulation requires a realistic pedestrian behavioral model that takes into account the various behavioral aspects of a real pedestrian. In this article, we describe our work on such a model, which aims to generate human-like pedestrian behaviors. To this end, various important factors in a real-pedestrian's decision-making process are considered in our model. These factors include a pedestrian's sensory attention, memory, and navigational behaviors. In particular, a two-level navigation model is proposed to generate realistic navigational behavior. As a result, our pedestrian model is able to generate various realistic behaviors such as overtaking, waiting, side-stepping and lane-forming in a crowded area. The simulated pedestrians are also able to navigate through complex environment, given an abstract map of the environment.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2053795387",
    "type": "article"
  },
  {
    "title": "Spatial-Temporal Modelling and Analysis of Bacterial Colonies with Phase Variable Genes",
    "doi": "https://doi.org/10.1145/2742546",
    "publication_date": "2015-04-06",
    "publication_year": 2015,
    "authors": "Ovidiu Pârvu; David Gilbert; Monika Heiner; Fei Liu; Nigel J. Saunders; Simon Shaw",
    "corresponding_authors": "",
    "abstract": "This article defines a novel spatial-temporal modelling and analysis methodology applied to a systems biology case study, namely phase variation patterning in bacterial colony growth. We employ coloured stochastic Petri nets to construct the model and run stochastic simulations to record the development of the circular colonies over time and space. The simulation output is visualised in 2D, and sector-like patterns are automatically detected and analysed. Space is modelled using 2.5 dimensions considering both a rectangular and circular geometry, and the effects of imposing different geometries on space are measured. We close by outlining an interpretation of the Petri net model in terms of finite difference approximations of partial differential equations (PDEs). One result is the derivation of the “best” nine-point diffusion model. Our multidimensional modelling and analysis approach is a precursor to potential future work on more complex multiscale modelling.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2206783966",
    "type": "article"
  },
  {
    "title": "Multilevel Sequential Monte Carlo Samplers for Normalizing Constants",
    "doi": "https://doi.org/10.1145/3092841",
    "publication_date": "2017-07-31",
    "publication_year": 2017,
    "authors": "Pierre Del Moral; Ajay Jasra; Kody J. H. Law; Yan Zhou",
    "corresponding_authors": "",
    "abstract": "This article considers the Sequential Monte Carlo (SMC) approximation of ratios of normalizing constants associated to posterior distributions which in principle rely on continuum models. Therefore, the Monte Carlo estimation error and the discrete approximation error must be balanced. A multilevel strategy is utilized to substantially reduce the cost to obtain a given error level in the approximation as compared to standard estimators. Two estimators are considered and relative variance bounds are given. The theoretical results are numerically illustrated for two Bayesian inverse problems arising from elliptic Partial Differential Equations (PDEs). The examples involve the inversion of observations of the solution of (i) a one-dimensional Poisson equation to infer the diffusion coefficient, and (ii) a two-dimensional Poisson equation to infer the external forcing.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2963868836",
    "type": "article"
  },
  {
    "title": "Interval Markov Decision Processes with Multiple Objectives",
    "doi": "https://doi.org/10.1145/3309683",
    "publication_date": "2019-10-31",
    "publication_year": 2019,
    "authors": "Ernst Moritz Hahn; Vahid Hashemi; Holger Hermanns; Morteza Lahijanian; Andrea Turrini",
    "corresponding_authors": "",
    "abstract": "Accurate Modelling of a real-world system with probabilistic behaviour is a difficult task. Sensor noise and statistical estimations, among other imprecisions, make the exact probability values impossible to obtain. In this article, we consider Interval Markov decision processes ( IMDP s), which generalise classical MDP s by having interval-valued transition probabilities. They provide a powerful modelling tool for probabilistic systems with an additional variation or uncertainty that prevents the knowledge of the exact transition probabilities. We investigate the problem of robust multi-objective synthesis for IMDP s and Pareto curve analysis of multi-objective queries on IMDP s. We study how to find a robust (randomised) strategy that satisfies multiple objectives involving rewards, reachability, and more general ω-regular properties against all possible resolutions of the transition probability uncertainties, as well as to generate an approximate Pareto curve providing an explicit view of the trade-offs between multiple objectives. We show that the multi-objective synthesis problem is PSPACE -hard and provide a value iteration-based decision algorithm to approximate the Pareto set of achievable points. We finally demonstrate the practical effectiveness of our proposed approaches by applying them on several case studies using a prototype tool.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2913279531",
    "type": "article"
  },
  {
    "title": "Model Continuity in Discrete Event Simulation",
    "doi": "https://doi.org/10.1145/2699714",
    "publication_date": "2015-04-16",
    "publication_year": 2015,
    "authors": "Deniz Çetinkaya; Alexander Verbraeck; Mamadou Seck",
    "corresponding_authors": "",
    "abstract": "Most of the well-known modeling and simulation (M&amp;S) methodologies state the importance of conceptual modeling in simulation studies, and they suggest the use of conceptual models during the simulation model development process. However, only a limited number of methodologies refers to how to move from a conceptual model to an executable simulation model. Besides, existing M&amp;S methodologies do not typically provide a formal method for model transformations between the models in different stages of the development process. Hence, in the current M&amp;S practice, model continuity is usually not fulfilled. In this article, a model-driven development framework for M&amp;S is presented to bridge the gap between different stages of a simulation study and to obtain model continuity. The applicability of the framework is illustrated with a prototype modeling environment and a case study in the discrete event simulation domain.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2058116005",
    "type": "article"
  },
  {
    "title": "Automatic Moment-Closure Approximation of Spatially Distributed Collective Adaptive Systems",
    "doi": "https://doi.org/10.1145/2883608",
    "publication_date": "2016-03-07",
    "publication_year": 2016,
    "authors": "Cheng Feng; Jane Hillston; Vashti Galpin",
    "corresponding_authors": "",
    "abstract": "Spatially distributed collective adaptive systems are an important class of systems that pose significant challenges to modeling due to the size and complexity of their state spaces. This problem is acute when the dynamic behavior of the system must be captured, such as to predict system performance. In this article, we present an abstraction technique that automatically derives a moment-closure approximation of the dynamic behavior of a spatially distributed collective adaptive system from a discrete representation of the entities involved. The moment-closure technique is demonstrated to give accurate estimates of dynamic behavior, although the number of ordinary differential equations generated for the second-order joint moments can grow large in some cases. For these cases, we propose a rigorous model reduction technique and demonstrate its use to substantially reduce the computational effort with only limited impact on the accuracy if the reduction threshold is set appropriately. All techniques reported in this article are implemented in a tool that is freely available for download.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2294591174",
    "type": "article"
  },
  {
    "title": "Real-Time Crowd Simulation Integrating Potential Fields and Agent Method",
    "doi": "https://doi.org/10.1145/2885496",
    "publication_date": "2016-03-07",
    "publication_year": 2016,
    "authors": "Guanghui Lü; Leiting Chen; Weiping Luo",
    "corresponding_authors": "",
    "abstract": "Crowd simulation is studied extensively in computer graphics, animation, and safety. A real-time crowd simulator has been developed based on potential fields and agent approach in this article. This simulator produces realistic complex heterogeneous motion and improves the simulation rates by at least 32% in comparison with the potential field results. The model of this simulator can efficiently tackle the problems in global optimal navigation, collision avoidance, and dynamic interaction; furthermore, it allows an agent to make independent decisions.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2299838070",
    "type": "article"
  },
  {
    "title": "On the Marginal Standard Error Rule and the Testing of Initial Transient Deletion Methods",
    "doi": "https://doi.org/10.1145/2961052",
    "publication_date": "2016-08-02",
    "publication_year": 2016,
    "authors": "Rob J. Wang; Peter W. Glynn",
    "corresponding_authors": "",
    "abstract": "In the planning of steady-state simulations, a central issue is the initial transient problem, in which an initial segment of the simulation output is adversely contaminated by initialization bias. Our article makes several contributions toward the analysis of this computational challenge. To begin, we introduce useful ways for measuring the magnitude of the initial transient effect in the single replication setting. We then analyze the marginal standard error rule (MSER) and prove that MSER’s deletion point is determined, as the simulation time horizon tends to infinity, by the minimizer of a certain random walk. We use this insight, together with fluid limit intuition associated with queueing models, to generate two nonpathological examples in which at least one variant of MSER fails to accurately predict the duration of the initial transient. Our results suggest that the efficacy of a deletion procedure is sensitive to the choice of performance measure, and that the set of standard test problems on which initial transient procedures are tested should be significantly broadened.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2478641305",
    "type": "article"
  },
  {
    "title": "Multivariate Input Uncertainty in Output Analysis for Stochastic Simulation",
    "doi": "https://doi.org/10.1145/2990190",
    "publication_date": "2016-10-23",
    "publication_year": 2016,
    "authors": "Wei Xie; Barry L. Nelson; Russell R. Barton",
    "corresponding_authors": "",
    "abstract": "When we use simulations to estimate the performance of stochastic systems, the simulation is often driven by input models estimated from finite real-world data. A complete statistical characterization of system performance estimates requires quantifying both input model and simulation estimation errors. The components of input models in many complex systems could be dependent. In this paper, we represent the distribution of a random vector by its marginal distributions and a dependence measure: either product-moment or Spearman rank correlations. To quantify the impact from dependent input model and simulation estimation errors on system performance estimates, we propose a metamodel-assisted bootstrap framework that is applicable to cases when the parametric family of multivariate input distributions is known or unknown. In either case, we first characterize the input models by their moments that are estimated using real-world data. Then, we employ the bootstrap to quantify the input estimation error, and an equation-based stochastic kriging metamodel to propagate the input uncertainty to the output mean, which can also reduce the influence of simulation estimation error due to output variability. Asymptotic analysis provides theoretical support for our approach, while an empirical study demonstrates that it has good finite-sample performance.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2538445363",
    "type": "article"
  },
  {
    "title": "Knowledge Discovery in Simulation Data",
    "doi": "https://doi.org/10.1145/3391299",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "Niclas Feldkamp; Sören Bergmann; Steffen Straßburger",
    "corresponding_authors": "",
    "abstract": "This article provides a comprehensive and in-depth overview of our work on knowledge discovery in simulations. Application-wise, we focus on manufacturing simulations. Specifically, we propose and discuss a methodology for designing, executing, and analyzing large-scale simulation experiments with a broad coverage of possible system behavior targeted at generating knowledge about the system. Based on the concept of data farming, we suggest a two-phase process which starts with a data generation phase, in which a smart experiment design is used to set up and efficiently execute a large number of simulation experiments. In the second phase, the knowledge discovery phase, data mining and visually aided analysis methods are applied on the gathered simulation input and output data. This article gives insights into this knowledge discovery phase by discussing different machine learning approaches and their suitability for different manufacturing simulation problems. With this, we provide guidelines on how to conduct knowledge discovery studies within the manufacturing simulation context. We also introduce different case studies, both academic and applied, and use them to validate our methodology.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W3098112554",
    "type": "article"
  },
  {
    "title": "Data Farming",
    "doi": "https://doi.org/10.1145/3425398",
    "publication_date": "2020-10-31",
    "publication_year": 2020,
    "authors": "Susan M. Sanchez",
    "corresponding_authors": "Susan M. Sanchez",
    "abstract": "Data farming is a descriptive metaphor that captures the notion of generating data purposefully to maximize the information “yield” from simulation models. Large-scale designed experiments let us grow the simulation output efficiently and effectively. We can explore massive input spaces, uncover interesting features of complex simulation response surfaces, and explicitly identify cause-and-effect relationships. Data farming has been used in the defense community over the past two decades, and has resulted in quantum leaps in the breadth, depth, and timeliness of the insights yielded by simulation models. In this article, we provide an overview of current data farming capabilities and their relationship to emerging techniques in data science and analytics. We use graphics to motivate insight into some of the benefits of a data farming approach. Finally, we share some thoughts about opportunities and challenges for further improving the state of the art, and transforming the state of the practice, in the data farming domain.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W3108410348",
    "type": "article"
  },
  {
    "title": "Falsification of Hybrid Systems Using Adaptive Probabilistic Search",
    "doi": "https://doi.org/10.1145/3459605",
    "publication_date": "2021-07-18",
    "publication_year": 2021,
    "authors": "Gidon Ernst; Sean Sedwards; Zhenya Zhang; Ichiro Hasuo",
    "corresponding_authors": "",
    "abstract": "We present and analyse an algorithm that quickly finds falsifying inputs for hybrid systems. Our method is based on a probabilistically directed tree search, whose distribution adapts to consider an increasingly fine-grained discretization of the input space. In experiments with standard benchmarks, our algorithm shows comparable or better performance to existing techniques, yet it does not build an explicit model of a system. Instead, at each decision point within a single trial, it makes an uninformed probabilistic choice between simple strategies to extend the input signal by means of exploration or exploitation. Key to our approach is the way input signal space is decomposed into levels, such that coarse segments are more probable than fine segments. We perform experiments to demonstrate how and why our approach works, finding that a fully randomized exploration strategy performs as well as our original algorithm that exploits robustness. We propose this strategy as a new baseline for falsification and conclude that more discriminative benchmarks are required.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W3185643228",
    "type": "article"
  },
  {
    "title": "Simulating the Impact of Dynamic Rerouting on Metropolitan-scale Traffic Systems",
    "doi": "https://doi.org/10.1145/3579842",
    "publication_date": "2023-01-10",
    "publication_year": 2023,
    "authors": "Cy Chan; Anu Kuncheria; Jane Macfarlane",
    "corresponding_authors": "",
    "abstract": "The rapid introduction of mobile navigation aides that use real-time road network information to suggest alternate routes to drivers is making it more difficult for researchers and government transportation agencies to understand and predict the dynamics of congested transportation systems. Computer simulation is a key capability for these organizations to analyze hypothetical scenarios; however, the complexity of transportation systems makes it challenging for them to simulate very large geographical regions, such as multi-city metropolitan areas. In this article, we describe enhancements to the Mobiliti parallel traffic simulator to model dynamic rerouting behavior with the addition of vehicle controller actors and vehicle-to-controller reroute requests. The simulator is designed to support distributed-memory parallel execution using discrete event simulation and be scalable on high-performance computing platforms. We demonstrate the potential of the simulator by analyzing the impact of varying the population penetration rate of dynamic rerouting on the San Francisco Bay Area road network. Using high-performance parallel computing, we can simulate a day in the San Francisco Bay Area with 19 million vehicle trips with 50 percent dynamic rerouting penetration over a road network with 0.5 million nodes and 1 million links in less than three minutes. We present a sensitivity study on the dynamic rerouting parameters, discuss the simulator’s parallel scalability, and analyze system-level impacts of changing the dynamic rerouting penetration. Furthermore, we examine the varying effects on different functional classes and geographical regions and present a validation of the simulation results compared to real-world data.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4315481752",
    "type": "article"
  },
  {
    "title": "Parallel Simulation of Quantum Networks with Distributed Quantum State Management",
    "doi": "https://doi.org/10.1145/3634701",
    "publication_date": "2024-01-31",
    "publication_year": 2024,
    "authors": "Xiaoliang Wu; Alexander Kolar; Joaquín Chung; Dong Jin; Martin Suchara; Rajkumar Kettimuthu",
    "corresponding_authors": "",
    "abstract": "Quantum network simulators offer the opportunity to cost-efficiently investigate potential avenues for building networks that scale with the number of users, communication distance, and application demands by simulating alternative hardware designs and control protocols. Several quantum network simulators have been recently developed with these goals in mind. As the size of the simulated networks increases, however, sequential execution becomes time-consuming. Parallel execution presents a suitable method for scalable simulations of large-scale quantum networks, but the unique attributes of quantum information create unexpected challenges. In this work, we identify requirements for parallel simulation of quantum networks and develop the first parallel discrete-event quantum network simulator by modifying the existing serial simulator SeQUeNCe. Our contributions include the design and development of a quantum state manager (QSM) that maintains shared quantum information distributed across multiple processes. We also optimize our parallel code by minimizing the overhead of the QSM and decreasing the amount of synchronization needed among processes. Using these techniques, we observe a speedup of 2 to 25 times when simulating a 1,024-node linear network topology using 2 to 128 processes. We also observe an efficiency greater than 0.5 for up to 32 processes in a linear network topology of the same size and with the same workload. We repeat this evaluation with a randomized workload on a caveman network. We also introduce several methods for partitioning networks by mapping them to different parallel simulation processes. We have released the parallel SeQUeNCe simulator as an open source tool alongside the existing sequential version.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3211604738",
    "type": "article"
  },
  {
    "title": "Bayesian Optimisation for Constrained Problems",
    "doi": "https://doi.org/10.1145/3641544",
    "publication_date": "2024-01-22",
    "publication_year": 2024,
    "authors": "Juan Ungredda; Juergen Branke",
    "corresponding_authors": "",
    "abstract": "Many real-world optimisation problems such as hyperparameter tuning in machine learning or simulation-based optimisation can be formulated as expensive-to-evaluate black-box functions. A popular approach to tackle such problems is Bayesian optimisation, which builds a response surface model based on the data collected so far, and uses the mean and uncertainty predicted by the model to decide what information to collect next. In this article, we propose a generalisation of the well-known Knowledge Gradient acquisition function that allows it to handle constraints. We empirically compare the new algorithm with four other state-of-the-art constrained Bayesian optimisation algorithms and demonstrate its superior performance. We also prove theoretical convergence in the infinite budget limit.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4391111026",
    "type": "article"
  },
  {
    "title": "Comparing Statistical, Analytical, and Learning-Based Routing Approaches for Delay-Tolerant Networks",
    "doi": "https://doi.org/10.1145/3665927",
    "publication_date": "2024-05-25",
    "publication_year": 2024,
    "authors": "Pedro R. D’Argenio; Juan A. Fraire; Arnd Hartmanns; Fernando D. Raverta",
    "corresponding_authors": "",
    "abstract": "In delay-tolerant networks (DTNs) with uncertain contact plans, the communication episodes and their reliabilities are known a priori. To maximise the end-to-end delivery probability, a bounded network-wide number of message copies are allowed. The resulting multi-copy routing optimization problem is naturally modelled as a Markov decision process with distributed information. In this paper, we provide an in-depth comparison of three solution approaches: statistical model checking with scheduler sampling, the analytical RUCoP algorithm based on probabilistic model checking, and an implementation of concurrent Q-learning. We use an extensive benchmark set comprising random networks, scalable binomial topologies, and realistic ring-road low Earth orbit satellite networks. We evaluate the obtained message delivery probabilities as well as the computational effort. Our results show that all three approaches are suitable tools for obtaining reliable routes in DTN, and expose a trade-off between scalability and solution quality.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4399017885",
    "type": "article"
  },
  {
    "title": "An empirical evaluation of several methods to select the best system",
    "doi": "https://doi.org/10.1145/352222.352226",
    "publication_date": "1999-10-01",
    "publication_year": 1999,
    "authors": "K. Inoue; Stephen E. Chick; Chun‐Hung Chen",
    "corresponding_authors": "",
    "abstract": "Simulation is an important tool for comparing the performance of several alternative systems. There is therefore significant interest in procedures that efficiently select the best system, where best is defined by the maximum or minimum expected simulation output. In this paper, we examine both two-stage and sequential procedures that represent three structurally different modeling methodologies for allocating simulation replications to identify the best system, and we evaluate them empirically with respect to several measures of effectiveness. Empirical evidence suggests that sequential procedures perform better than their two-stage counterparts, including a heuristic sequential variation on Rinott's procedure. Further, there appears to be significant benefit to using procedures based on a Bayesian, average-case analysis as opposed to the statistically-conservative indifference-zone formulation.",
    "cited_by_count": 44,
    "openalex_id": "https://openalex.org/W2049740020",
    "type": "article"
  },
  {
    "title": "HAVEGE",
    "doi": "https://doi.org/10.1145/945511.945516",
    "publication_date": "2003-10-01",
    "publication_year": 2003,
    "authors": "André Seznec; Nicolas Sendrier",
    "corresponding_authors": "",
    "abstract": "Random numbers with high cryptographic quality are needed to enhance the security of cryptography applications. Software heuristics for generating empirically strong random number sequences rely on entropy gathering by measuring unpredictable external events. These generators only deliver a few bits per event. This limits them to being used as seeds for pseudorandom generators.General-purpose processors feature a large number of hardware mechanisms that aim to improve performance: caches, branch predictors, …. The state of these components is not architectural (i.e., the result of an ordinary application does not depend on it). It is also volatile and cannot be directly monitored by the user. On the other hand, every operating system interrupt modifies thousands of these binary volatile states.In this article, we present and analyze HAVEGE (HArdware Volatile Entropy Gathering and Expansion), a new user-level software heuristic to generate practically strong random numbers on general-purpose computers. The hardware clock cycle counter of the processor can be used to gather part of the entropy/uncertainty introduced by operating system interrupts in the internal states of the processor. Then, we show how this entropy gathering technique can be combined with pseudorandom number generation in HAVEGE. Since the internal state of HAVEGE includes thousands of internal volatile hardware states, it seems impossible even for the user itself to reproduce the generated sequences.",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W1985402296",
    "type": "article"
  },
  {
    "title": "Modeling cost/performance of a parallel computer simulator",
    "doi": "https://doi.org/10.1145/244804.244808",
    "publication_date": "1997-01-01",
    "publication_year": 1997,
    "authors": "Babak Falsafi; David A. Wood",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Modeling cost/performance of a parallel computer simulator Authors: Babak Falsafi Univ. of Wisconsin, Madison Univ. of Wisconsin, MadisonView Profile , David A. Wood Univ. of Wisconsin, Madison Univ. of Wisconsin, MadisonView Profile Authors Info & Claims ACM Transactions on Modeling and Computer SimulationVolume 7Issue 1Jan. 1997 pp 104–130https://doi.org/10.1145/244804.244808Published:01 January 1997Publication History 30citation601DownloadsMetricsTotal Citations30Total Downloads601Last 12 Months17Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W1991626794",
    "type": "article"
  },
  {
    "title": "Regenerative steady-state simulation of discrete-event systems",
    "doi": "https://doi.org/10.1145/508366.508367",
    "publication_date": "2001-10-01",
    "publication_year": 2001,
    "authors": "Shane G. Henderson; Peter W. Glynn",
    "corresponding_authors": "",
    "abstract": "The regenerative method possesses certain asymptotic properties that dominate those of other steady-state simulation output analysis methods, such as batch means. Therefore, applying the regenerative method to steady-state discrete-event system simulations is of great interest. In this paper, we survey the state of the art in this area. The main difficulty in applying the regenerative method in our context is perhaps in identifying regenerative cycle boundaries. We examine this issue through the use of the \"smoothness index.\" Regenerative cycles are easily identified in systems with unit smoothness index, but this is typically not the case for systems with nonunit smoothness index. We show that \"most\" (in a certain precise sense) discrete-event simulations will have nonunit smoothness index, and extend the asymptotic theory of regenerative simulation estimators to this context.",
    "cited_by_count": 41,
    "openalex_id": "https://openalex.org/W2013935096",
    "type": "article"
  },
  {
    "title": "Analysis of bounded time warp and comparison with YAWNS",
    "doi": "https://doi.org/10.1145/240896.240913",
    "publication_date": "1996-10-01",
    "publication_year": 1996,
    "authors": "Phillip M. Dickens; David M. Nicol; Paul F. Reynolds; J. M. Duva",
    "corresponding_authors": "",
    "abstract": "This article studies an analytic model of parallel discrete-event simulation, comparing the YAWNS conservative synchronization protocol with Bounded Time Warp. The assumed simulation problem is a heavily loaded queuing network where the probability of an idle server is closed to zero. We model workload and job routing in standard ways, then develop and validate methods for computing approximated performance measures as a function of the degree of optimism allowed, overhead costs of state-saving, rollback, and barrier synchronization, and workload aggregation. We find that Bounded Time Warp is superior when the number of servers per physical processor is low (i.e., sparse load), but that aggregating workload improves YAWNS relative performance.",
    "cited_by_count": 40,
    "openalex_id": "https://openalex.org/W2041375474",
    "type": "article"
  },
  {
    "title": "Random variate generation for multivariate unimodal densities",
    "doi": "https://doi.org/10.1145/268403.268413",
    "publication_date": "1997-10-01",
    "publication_year": 1997,
    "authors": "Luc Devroye",
    "corresponding_authors": "Luc Devroye",
    "abstract": "A probability density on a finite-dimensional Euclidean space is orthounimodal with a given mode if within each orthant (quadrant) defined by the mode, the density is a monotone function of each of its arguments individually. Up to a linear transformation, most of the commonly used random vectors possess orthounimodal densities. To generate a random vector from a given orthounimodal density, several general-purpose algorithms are presented; and an experimental performance evaluation illustrates the potential efficiency increases that can be achieved by these algorithms versus naive rejection.",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W2048568723",
    "type": "article"
  },
  {
    "title": "Simultaneous events and lookahead in simulation protocols",
    "doi": "https://doi.org/10.1145/361026.361032",
    "publication_date": "2000-07-01",
    "publication_year": 2000,
    "authors": "Vikas Jha; Rajive Bagrodia",
    "corresponding_authors": "",
    "abstract": "A discrete event simulation model may contain several events that have the same timestamp, referred to as simultaneous events. In general, the results of a simulation depend on the order in which simultaneous events are executed. Simulation languages and protocols use different, sometimes ad hoc, tie-breaking mechanisms to order simulataneous events. As a result, it may be impossible to reproduce the results of a simulation model across different simulators. This article presents a systematic analysis of the lookahead requirements for sequential and parallel simulation protocols, utilizing the process-oriented world view, with respect to their abililty to execute models with simultaneous events in a deterministic order. In particular, the article shows that most protocols, including the global event list protocol and commonly used parallel conservative and optimistic protocols, require that the simulation model provide some form of lookahead guarantee to enforce deterministc ordering of simultaneous events. The article also shows that the lookahead requirements for many protocols can be weakened if the model allows simultaneous events to be processed in a nondeterministic order. Finally, the lookahead properties that must be satisfied by a model in order for its execution to make guaranteed progress are derived using various simulation protocols.",
    "cited_by_count": 38,
    "openalex_id": "https://openalex.org/W2082883720",
    "type": "article"
  },
  {
    "title": "Simulation optimization with countably infinite feasible regions",
    "doi": "https://doi.org/10.1145/1176249.1176252",
    "publication_date": "2006-10-01",
    "publication_year": 2006,
    "authors": "Sigrún Andradóttir",
    "corresponding_authors": "Sigrún Andradóttir",
    "abstract": "This article is concerned with proving the almost sure and global convergence of a broad class of algorithms for solving simulation optimization problems with countably infinite number of feasible points. We first describe the class of simulation optimization algorithms under consideration and discuss how the estimate of the optimal solution should be chosen when the feasible region of the underlying optimization problem is countably infinite. Then, we present a general result that guarantees the global convergence with probability one of the simulation optimization algorithms in this class. The assumptions of this result are sufficiently weak to allow the algorithms under consideration to be efficient, in that they are not required to either allocate the same amount of computer effort to all the feasible points these algorithms visit, or to spend an increasing amount of computer effort per iteration as the number of iterations grows. This article concludes with a discussion of how our assumptions can be satisfied and also generalized.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W1965057626",
    "type": "article"
  },
  {
    "title": "Algorithms for HLA-based distributed simulation cloning",
    "doi": "https://doi.org/10.1145/1113316.1113318",
    "publication_date": "2005-10-01",
    "publication_year": 2005,
    "authors": "Dan Chen; Stephen John Turner; Wentong Cai; Boon Ping Gan; Malcolm Yoke Hean Low",
    "corresponding_authors": "",
    "abstract": "Distributed simulation cloning technology is designed to analyze alternative scenarios of a distributed simulation concurrently within the same execution session. One important goal is to optimize execution by avoiding repeated computation among independent scenarios. Our research is concerned with the cloning of High Level Architecture (HLA)-based distributed simulations; a federate may spawn clones to explore different scenarios at a decision point. This article introduces the cloning mechanism and the supporting infrastructure. When enabling cloning, our approach ensures the state consistency and supports user transparency and reusability of federate codes. When a federate clones, it is desirable to replicate only those federates whose states will be affected while the rest are shared among the old and new scenarios. This article discusses the theory and issues involved in such an incremental cloning mechanism, which guarantees accurate sharing and initiates cloning only when absolutely necessary. Experiments have been carried out to compare the performance of entire cloning and incremental cloning mechanisms. Experimental results indicate that the proposed approach provides correct cloning and can significantly reduce the execution time for evaluating different scenarios of a distributed simulation. Moreover the incremental cloning mechanism significantly surpasses entire cloning in terms of execution efficiency.",
    "cited_by_count": 34,
    "openalex_id": "https://openalex.org/W2039029348",
    "type": "article"
  },
  {
    "title": "Automated container transport system between inland port and terminals",
    "doi": "https://doi.org/10.1145/1138464.1138465",
    "publication_date": "2006-04-01",
    "publication_year": 2006,
    "authors": "Jianlong Zhang; Pétros Ioannou; Anastasios Chassiakos",
    "corresponding_authors": "",
    "abstract": "In this article we propose a new concept called automated container transportation system between inland port and terminals (ACTIPOT) which involves the use of automated trucks to transfer containers between an inland port and container terminals. The inland port is located a few miles away from the terminals and is used for storing and processing import/export containers before distribution to customers or transfer to the terminals. We design and analyze the ACTIPOT system with particular attention paid to the overall supervisory controller that synchronizes all the operations inside the ACTIPOT system. We employ the technique of truck platooning in order to simplify the control of the overall system and to minimize the possibility of deadlocks, congestion, and failures. A microscopic simulation model is developed and used to demonstrate the overall performance of the ACTIPOT system. The contribution of this article is the design, analysis, and evaluation of the new concept ACTIPOT.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W2078510693",
    "type": "article"
  },
  {
    "title": "Development of a simulation model of colorectal cancer",
    "doi": "https://doi.org/10.1145/1315575.1315579",
    "publication_date": "2007-12-01",
    "publication_year": 2007,
    "authors": "Stephen D. Roberts; Lijun Wang; Robert W. Klein; Reid M. Ness; Robert S. Dittus",
    "corresponding_authors": "",
    "abstract": "Colorectal cancer (CRC) is deadly if not found early. Any protocols developed for screening and surveillance and any policy decisions regarding the availability of CRC resources should consider the nature of the disease and its impact over time on costs and quality-adjusted life years in a population. Simulation models can provide a flexible representation needed for such analysis; however, the development of a credible simulation model of the natural history of CRC is hindered by limited data and incomplete knowledge. To accommodate the extensive modeling and remodeling required to produce a credible model, we created an object-oriented simulation platform driven by a model-independent database within the .NET environment. The object-oriented structure not only encapsulated the needs of a simulation replication but created an extensible framework for specialization of the CRC components. This robust framework allowed development to focus modeling on the CRC events and their event relationships, conveniently facilitating extensive revision during model construction. As a second-generation CRC modeling activity, this model development benefited from prior experience with data sources and modeling difficulties. A graphical user interface makes the model accessible by displaying existing scenarios, showing input variables and their values, and permitting the creation of new scenarios and changes to its input. Output from the simulation is captured in familiar tabbed worksheets and stored in the database. The eventual CRC model was conceptualized through a series of assumptions that conformed to beliefs and data regarding the natural history of CRC. Throughout the development cycle, extensive verification and validation calibrated the model. The result is a simulation model that characterizes the natural history of CRC with sufficient accuracy to provide an effective means of evaluating numerous issues regarding the burden of this disease on individuals and society. Generalizations from this study are offered regarding the use of discrete-event simulation in disease modeling and medical decision making.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2030023090",
    "type": "article"
  },
  {
    "title": "Fidelity of network simulation and emulation",
    "doi": "https://doi.org/10.1145/1456645.1456649",
    "publication_date": "2008-12-01",
    "publication_year": 2008,
    "authors": "Roman Chertov; Sonia Fahmy; Ness B. Shroff",
    "corresponding_authors": "",
    "abstract": "In this article, we investigate the differences between simulation and emulation when conducting denial of service (DoS) attack experiments. As a case study, we consider low-rate TCP-targeted DoS attacks. We design constructs and tools for emulation testbeds to achieve a level of control comparable to simulation tools. Through a careful sensitivity analysis, we expose difficulties in obtaining meaningful measurements from the DETER, Emulab, and WAIL testbeds with default system settings. We find dramatic differences between simulation and emulation results for DoS experiments. Our results also reveal that software routers such as Click provide a flexible experimental platform, but require understanding and manipulation of the underlying network device drivers. Our experiments with commercial Cisco routers demonstrate that they are highly susceptible to the TCP-targeted attacks when ingress/egress IP filters are used.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W1987775546",
    "type": "article"
  },
  {
    "title": "Asymptotically optimal allocation of stratified sampling with adaptive variance reduction by strata",
    "doi": "https://doi.org/10.1145/1734222.1734225",
    "publication_date": "2010-04-01",
    "publication_year": 2010,
    "authors": "Reiichiro Kawai",
    "corresponding_authors": "Reiichiro Kawai",
    "abstract": "To enhance efficiency in Monte Carlo simulations, we develop an adaptive stratified sampling algorithm for allocation of sampling effort within each stratum, in which an adaptive variance reduction technique is applied. Given the number of replications in each batch, our algorithm updates allocation fractions to minimize the work-normalized variance of the stratified estimator of the mean. We establish the asymptotic normality of the stratified estimator of the mean as the number of batches tends to infinity. Although implementation of the proposed algorithm requires a small amount of initial work, the algorithm has the potential to yield substantial improvements in estimator efficiency. Equally important is that the adaptive framework avoids the need for frequent recalibration of the parameters of the variance reduction methods applied within each stratum when changes occur in the experimental conditions governing system performance. To illustrate the applicability and effectiveness of our algorithm, we provide numerical results for a Black--Scholes option pricing, where we stratify the underlying Brownian motion with respect to its terminal value and apply an importance sampling method to normal random variables filling in the Brownian path. Relative to the estimator variance with proportional allocation, the proposed algorithm achieved a fourfold reduction in estimator variance with a negligible increase in computing time.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2093442187",
    "type": "article"
  },
  {
    "title": "On Lyapunov Inequalities and Subsolutions for Efficient Importance Sampling",
    "doi": "https://doi.org/10.1145/2331140.2331141",
    "publication_date": "2012-08-01",
    "publication_year": 2012,
    "authors": "José Blanchet; Peter W. Glynn; Kevin Leder",
    "corresponding_authors": "",
    "abstract": "In this article we explain some connections between Lyapunov methods and subsolutions of an associated Isaacs equation for the design of efficient importance sampling schemes. As we shall see, subsolutions can be derived by taking an appropriate limit of an associated Lyapunov inequality. They have been recently proposed in several works of Dupuis, Wang, and others and applied to address several important problems in rare-event simulation. Lyapunov inequalities have been used for testing the efficiency of state-dependent importance sampling schemes in heavy-tailed or discrete settings in a variety of works by Blanchet, Glynn, and others. While subsolutions provide an analytic criterion for the construction of efficient samplers, Lyapunov inequalities are useful for finding more precise information, in the form of bounds, for the behavior of the coefficient of variation of the associated importance sampling estimator in the prelimit. In addition, Lyapunov inequalities provide insight into the various mollification procedures that are often required in constructing associated subsolutions. Our aim is to demonstrate that applying Lyapunov inequalities for verification of efficiency can help both guide the selection of various mollification parameters and sharpen the information on the efficiency gain induced by the sampler.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2098251485",
    "type": "article"
  },
  {
    "title": "Bridging the gap",
    "doi": "https://doi.org/10.1145/2379810.2379811",
    "publication_date": "2012-11-01",
    "publication_year": 2012,
    "authors": "Simon J. E. Taylor; Stephen John Turner; Steffen Straßburger; Navonil Mustafee",
    "corresponding_authors": "",
    "abstract": "In Operations Research and Management Science (OR/MS), Discrete Event Simulation (DES) models are typically created using commercial off-the-shelf simulation packages (CSPs) such as AnyLogic™, Arena™, Flexsim™, Simul8™, SLX™, Witness™, and so on. A DES model represents the processes associated with a system of interest. Some models may be composed of submodels running in their own CSPs on different computers linked together over a communications network via distributed simulation software. The creation of a distributed simulation with CSPs is still complex and typically requires a partnership of problem owners, modelers, CSP vendors, and distributed simulation specialists. In an attempt to simplify this development and foster discussion between modelers and technologists, the SISO-STD-006-2010 Standard for COTS Simulation Package Interoperability Reference Models has been developed. The standard makes it possible to capture interoperability capabilities and requirements at a DES modeling level rather than a computing technical level. For example, it allows requirements for entity transfer between models to be clearly specified in DES terms (e.g. the relationship between departure and arrival simulation times and input element (queue, workstation, etc.), buffering rules, and entity priority, instead of using specialist technical terminology. This article explores the motivations for distributed simulation in this area, related work, and the rationale for the standard. The four Types of Interoperability Reference Model described in the standard are discussed and presented (A. Entity Transfer, B. Shared Resource, C. Shared Event, and D. Shared Data Structure). Case studies in healthcare and manufacturing are given to demonstrate how the standard is used in practice.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W1984693076",
    "type": "article"
  },
  {
    "title": "Efficient rare event simulation for heavy-tailed compound sums",
    "doi": "https://doi.org/10.1145/1899396.1899397",
    "publication_date": "2011-02-01",
    "publication_year": 2011,
    "authors": "José Blanchet; Chenxin Li",
    "corresponding_authors": "",
    "abstract": "We develop an efficient importance sampling algorithm for estimating the tail distribution of heavy-tailed compound sums, that is, random variables of the form S M = Z 1 +…+ Z M where the Z i 's are independently and identically distributed (i.i.d.) random variables in R and M is a nonnegative, integer-valued random variable independent of the Z i 's. We construct the first estimator that can be rigorously shown to be strongly efficient only under the assumption that the Z i 's are subexponential and M is light-tailed. Our estimator is based on state-dependent importance sampling and we use Lyapunov-type inequalities to control its second moment. The performance of our estimator is empirically illustrated in various instances involving popular heavy-tailed models.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2050969304",
    "type": "article"
  },
  {
    "title": "Stochastic approximation over multidimensional discrete sets with applications to inventory systems and admission control of queueing networks",
    "doi": "https://doi.org/10.1145/2379810.2379812",
    "publication_date": "2012-11-01",
    "publication_year": 2012,
    "authors": "Eunji Lim",
    "corresponding_authors": "Eunji Lim",
    "abstract": "We propose new methods to solve simulation optimization problems over multidimensional discrete sets. The proposed methods are based on extending the objective function from a discrete domain to a continuous domain and applying stochastic approximation to the extended function. The extension of the objective function is constructed as a piecewise linear interpolation of the original objective function over a particular partition of ℝ d . The advantage of the proposed approach lies in that stochastic approximation is applied to the extension, not the original function, over ℝ d , so the estimated optimal solution at each iteration of the proposed methods is not restricted to be an integer point. Rather, we are free to approach the optimal solution aggressively by moving toward the direction of the steepest descent, thereby skipping over intervening points, thereby resulting in fast convergence in the early stage of the procedures. We provide a set of sufficient conditions under which the proposed methods guarantee the almost sure (a.s.) convergence to the optimal solution. One of such conditions is the multimodularity or L ♮ -convexity of the objective function, which arises in various inventory systems and queueing networks with controlled admission. Numerical examples illustrate the effectiveness of the proposed methods in such settings.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2037606745",
    "type": "article"
  },
  {
    "title": "The Power of Alternative Kolmogorov-Smirnov Tests Based on Transformations of the Data",
    "doi": "https://doi.org/10.1145/2699716",
    "publication_date": "2015-05-08",
    "publication_year": 2015,
    "authors": "Song‐Hee Kim; Ward Whitt",
    "corresponding_authors": "",
    "abstract": "The Kolmogorov-Smirnov (KS) statistical test is commonly used to determine if data can be regarded as a sample from a sequence of independent and identically distributed (i.i.d.) random variables with specified continuous cumulative distribution function (cdf) &lt;i&gt;F&lt;/i&gt;, but with small samples it can have insufficient power, that is, its probability of rejecting natural alternatives can be too low. However, in 1961, Durbin showed that the power of the KS test often can be increased, for a given significance level, by a well-chosen transformation of the data. Simulation experiments reported here show that the power can often be more consistently and substantially increased by a different transformation. We first transform the given sequence to a sequence of mean-1 exponential random variables, which is equivalent to a rate-1 Poisson process. We then apply the classical conditional-uniform transformation to convert the arrival times into i.i.d. random variables uniformly distributed on [0, 1]. And then, after those two preliminary steps, we apply the original Durbin transformation. Since these KS tests assume a fully specified cdf, we also investigate the consequence of having to estimate parameters of the cdf.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2087446395",
    "type": "article"
  },
  {
    "title": "SCORE Allocations for Bi-objective Ranking and Selection",
    "doi": "https://doi.org/10.1145/3158666",
    "publication_date": "2018-01-03",
    "publication_year": 2018,
    "authors": "Guy Feldman; Susan R. Hunter",
    "corresponding_authors": "",
    "abstract": "The bi-objective ranking and selection (R8S) problem is a special case of the multi-objective simulation optimization problem in which two conflicting objectives are known only through dependent Monte Carlo estimators, the decision space or number of systems is finite, and each system can be sampled to some extent. The solution to the bi-objective R8S problem is a set of systems with non-dominated objective vectors, called the set of Pareto systems. We exploit the special structure of the bi-objective problem to characterize the asymptotically optimal simulation budget allocation, which accounts for dependence between the objectives and balances the probabilities associated with two types of misclassification error. Like much of the R8S literature, our focus is on the case in which the simulation observations are bivariate normal. Assuming normality, we then use a certain asymptotic limit to derive an easily-implementable Sampling Criteria for Optimization using Rate Estimators (SCORE) sampling framework that approximates the optimal allocation and accounts for correlation between the objectives. Perhaps surprisingly, the limiting SCORE allocation exclusively controls for misclassification-by-inclusion events, in which non-Pareto systems are falsely estimated as Pareto. We also provide an iterative algorithm for implementation. Our numerical experience with the resulting SCORE framework indicates that it is fast and accurate for problems having up to ten thousand systems.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2782364332",
    "type": "article"
  },
  {
    "title": "A Product-Form Model for the Performance Evaluation of a Bandwidth Allocation Strategy in WSNs",
    "doi": "https://doi.org/10.1145/3155335",
    "publication_date": "2018-02-22",
    "publication_year": 2018,
    "authors": "Andrea Marin; Sabina Rossi; Dario Burato; Andrea Sina; Matteo Sottana",
    "corresponding_authors": "",
    "abstract": "Wireless Sensor Networks (WSNs) are important examples of Collective Adaptive System, which consist of a set of motes that are spatially distributed in an indoor or outdoor space. Each mote monitors its surrounding conditions, such as humidity, intensity of light, temperature, and vibrations, but also collects complex information, such as images or small videos, and cooperates with the whole set of motes forming the WSN to allow the routing process. The traffic in the WSN consists of packets that contain the data harvested by the motes and can be classified according to the type of information that they carry. One pivotal problem in WSNs is the bandwidth allocation among the motes. The problem is known to be challenging due to the reduced computational capacity of the motes, their energy consumption constraints, and the fully decentralised network architecture. In this article, we study a novel algorithm to allocate the WSN bandwidth among the motes by taking into account the type of traffic they aim to send. Under the assumption of a mesh network and Poisson distributed harvested packets, we propose an analytical model for its performance evaluation that allows a designer to study the optimal configuration parameters. Although the Markov chain underlying the model is not reversible, we show it to be ρ-reversible under a certain renaming of states. By an extensive set of simulations, we show that the analytical model accurately approximates the performance of networks that do not satisfy the assumptions. The algorithm is studied with respect to the achieved throughput and fairness. We show that it provides a good approximation of the max-min fairness requirements.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2792882998",
    "type": "article"
  },
  {
    "title": "Particle Algorithms for Optimization on Binary Spaces",
    "doi": "https://doi.org/10.1145/2414416.2414424",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Christian Schäfer",
    "corresponding_authors": "Christian Schäfer",
    "abstract": "We discuss a unified approach to stochastic optimization of pseudo-Boolean objective functions based on particle methods, including the cross-entropy method and simulated annealing as special cases. We point out the need for auxiliary sampling distributions, that is parametric families on binary spaces, which are able to reproduce complex dependency structures, and illustrate their usefulness in our numerical experiments. We provide numerical evidence that particle-driven optimization algorithms based on parametric families yield superior results on strongly multi-modal optimization problems while local search heuristics outperform them on easier problems.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2137091798",
    "type": "article"
  },
  {
    "title": "Intensional Couplings in Variable-Structure Models",
    "doi": "https://doi.org/10.1145/2818641",
    "publication_date": "2016-01-28",
    "publication_year": 2016,
    "authors": "Alexander Steiniger; Adelinde M. Uhrmacher",
    "corresponding_authors": "",
    "abstract": "In modular, hierarchical modeling, couplings (connections) describe and constrain the communication, and thus interaction, between model components. Defining couplings between a large set of components in an extensional manner—listing all existing couplings individually—often proves to be rather tedious. Moreover, if models change their structure, that is, composition and interaction patterns and, in some cases, even their interfaces during simulation, questions about the consistency of the couplings arise. For instance, an extensionally defined coupling may refer to a model that no longer exists. Instead, an intensional coupling definition, based on attributes of the components to couple and dynamically translated into concrete couplings during simulation, promises to alleviate these problems. We propose a concept that combines a flexible, yet expressive, definition of couplings that rests on component interfaces announcing attributes of interest. However, intensional couplings come at a price, as they need to be translated during simulation; in variable-structure models, this translation has to happen frequently. We illuminate our concept based on a revision of the modeling formalism Multilevel Discrete Event System Specification ( ML - DEVS ). Developed for multilevel modeling and simulation, ML - DEVS exhibits another alternative to intensional couplings, that is, sharing parts of model states for up- and downward causation. The intricate interplay between these different types of couplings is revealed in the abstract simulator of ML - DEVS .",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2266305855",
    "type": "article"
  },
  {
    "title": "Guest Editors' Introduction to Special Issue on the 2012 NSF workshop",
    "doi": "https://doi.org/10.1145/2676546",
    "publication_date": "2014-08-13",
    "publication_year": 2014,
    "authors": "L. Jeff Hong; Jian-Qiang Hu; Seong‐Hee Kim",
    "corresponding_authors": "",
    "abstract": "introduction Guest Editors' Introduction to Special Issue on the 2012 NSF workshop Editors: L. Jeff Hong City University of Hong Kong, China City University of Hong Kong, ChinaView Profile , J. Q. Hu Fudan University, China Fudan University, ChinaView Profile , Seong-Hee Kim Georgia Institute of Technology Georgia Institute of TechnologyView Profile Authors Info & Claims ACM Transactions on Modeling and Computer SimulationVolume 24Issue 4August 2014 Article No.: 19epp 1–3https://doi.org/10.1145/2676546Published:18 November 2014Publication History 5citation65DownloadsMetricsTotal Citations5Total Downloads65Last 12 Months1Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2068945250",
    "type": "article"
  },
  {
    "title": "Static Network Reliability Estimation under the Marshall-Olkin Copula",
    "doi": "https://doi.org/10.1145/2775106",
    "publication_date": "2016-01-13",
    "publication_year": 2016,
    "authors": "Zdravko I. Botev; Pierre L’Ecuyer; Richard Simard; Bruno Tuffin",
    "corresponding_authors": "",
    "abstract": "In a static network reliability model, one typically assumes that the failures of the components of the network are independent. This simplifying assumption makes it possible to estimate the network reliability efficiently via specialized Monte Carlo algorithms. Hence, a natural question to consider is whether this independence assumption can be relaxed while still attaining an elegant and tractable model that permits an efficient Monte Carlo algorithm for unreliability estimation. In this article, we provide one possible answer by considering a static network reliability model with dependent link failures, based on a Marshall-Olkin copula, which models the dependence via shocks that take down subsets of components at exponential times, and propose a collection of adapted versions of permutation Monte Carlo (PMC, a conditional Monte Carlo method), its refinement called the turnip method , and generalized splitting (GS) methods to estimate very small unreliabilities accurately under this model. The PMC and turnip estimators have bounded relative error when the network topology is fixed while the link failure probabilities converge to 0, whereas GS does not have this property. But when the size of the network (or the number of shocks) increases, PMC and turnip eventually fail, whereas GS works nicely (empirically) for very large networks, with over 5,000 shocks in our examples.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2237934651",
    "type": "article"
  },
  {
    "title": "Fast Random Integer Generation in an Interval",
    "doi": "https://doi.org/10.1145/3230636",
    "publication_date": "2019-01-24",
    "publication_year": 2019,
    "authors": "Daniel Lemire",
    "corresponding_authors": "Daniel Lemire",
    "abstract": "In simulations, probabilistic algorithms and statistical tests, we often generate random integers in an interval (e.g., [0,s)). For example, random integers in an interval are essential to the Fisher-Yates random shuffle. Consequently, popular languages like Java, Python, C++, Swift and Go include ranged random integer generation functions as part of their runtime libraries. Pseudo-random values are usually generated in words of a fixed number of bits (e.g., 32 bits, 64 bits) using algorithms such as a linear congruential generator. We need functions to convert such random words to random integers in an interval ([0,s)) without introducing statistical biases. The standard functions in programming languages such as Java involve integer divisions. Unfortunately, division instructions are relatively expensive. We review an unbiased function to generate ranged integers from a source of random words that avoids integer divisions with high probability. To establish the practical usefulness of the approach, we show that this algorithm can multiply the speed of unbiased random shuffling on x64 processors. Our proposed approach has been adopted by the Go language for its implementation of the shuffle function.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2803763401",
    "type": "article"
  },
  {
    "title": "Exact Simulation of a Truncated Lévy Subordinator",
    "doi": "https://doi.org/10.1145/3368088",
    "publication_date": "2020-05-31",
    "publication_year": 2020,
    "authors": "Angelos Dassios; Jia Wei Lim; Yan Qu",
    "corresponding_authors": "",
    "abstract": "A truncated Lévy subordinator is a Lévy subordinator in R + with Lévy measure restricted from above by a certain level b . In this article, we study the path and distribution properties of this type of process in detail and set up an exact simulation framework based on a marked renewal process. In particular, we focus on a typical specification of truncated Lévy subordinator, namely the truncated stable process. We establish an exact simulation algorithm for the truncated stable process, which is very accurate and efficient. Compared to the existing algorithm suggested in Chi, our algorithm outperforms over all parameter settings. Using the distributional decomposition technique, we also develop an exact simulation algorithm for the truncated tempered stable process and other related processes. We illustrate an application of our algorithm as a valuation tool for stochastic hyperbolic discounting, and numerical analysis is provided to demonstrate the accuracy and effectiveness of our methods. We also show that variations of the result can also be used to sample two-sided truncated Lévy processes, two-sided Lévy processes via subordinating Brownian motions, and truncated Lévy-driven Ornstein-Uhlenbeck processes.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2982303555",
    "type": "article"
  },
  {
    "title": "Uncertainty-aware Simulation of Adaptive Systems",
    "doi": "https://doi.org/10.1145/3589517",
    "publication_date": "2023-03-29",
    "publication_year": 2023,
    "authors": "Jean‐Marc Jezéquél; Antonio Vallecillo",
    "corresponding_authors": "",
    "abstract": "Adaptive systems manage and regulate the behavior of devices or other systems using control loops to automatically adjust the value of some measured variables to equal the value of a desired set-point. These systems normally interact with physical parts or operate in physical environments, where uncertainty is unavoidable. Traditional approaches to manage that uncertainty use either robust control algorithms that consider bounded variations of the uncertain variables and worst-case scenarios or adaptive control methods that estimate the parameters and change the control laws accordingly. In this article, we propose to include the sources of uncertainty in the system models as first-class entities using random variables to simulate adaptive and control systems more faithfully, including not only the use of random variables to represent and operate with uncertain values but also to represent decisions based on their comparisons. Two exemplar systems are used to illustrate and validate our proposal.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4361285401",
    "type": "article"
  },
  {
    "title": "Real-time simulation of dust behavior generated by a fast traveling vehicle",
    "doi": "https://doi.org/10.1145/333296.333366",
    "publication_date": "1999-04-01",
    "publication_year": 1999,
    "authors": "Jim X. Chen; Xiadong Fu; J. Wegman",
    "corresponding_authors": "",
    "abstract": "Simulation of physically realistic complex dust behavior is very useful in training, education, art, advertising, and entertainment. There are no published models for real-time simulation of dust behavior generated by a traveling vehicle. In this paper, we use particle systems, computational fluid dynamics, and behavioral simulation techniques to simulate dust behavior in real time. First, we analyze the forces and factors that affect dust generation and the behavior after dust particles are generated. Then, we construct physically-based empirical models to generate dust particles and control the behavior accordingly. We further simplify the numerical calculations by dividing dust behavior into three stages, and establishing simplified particle system models for each stage. We employ motion blur, particle blending texture mapping, and other computer graphics techniques to achieve the final results. Our contributions include constructing physically-based empirical models to generate dust behavior and achieving simulation of the behavior in real time.",
    "cited_by_count": 39,
    "openalex_id": "https://openalex.org/W2066647715",
    "type": "article"
  },
  {
    "title": "Graphical interactive simulation input modeling with bivariate Bézier distributions",
    "doi": "https://doi.org/10.1145/217853.217854",
    "publication_date": "1995-07-01",
    "publication_year": 1995,
    "authors": "Mary Ann Wagner; James R. Wilson",
    "corresponding_authors": "",
    "abstract": "A graphical interactive technique for modeling bivariate simulation inputs is based on a family of continuous univariate and bivariate probability distributions with bounded support that are described by Be´zier curves and surfaces, respectively. This family of distributions has a natural, extensible parameterization so that all parameters have a meaningful interpretation; and the complete family is capable of accurately representing an unlimited variety of shapes for marginal distributions together with many common types of bivariate stochastic dependence. This approach to simulation input modeling is implemented in a Windows-based software system called PRIME-PRobabilistic Input Modeling Environment. Several examples illustrate the application of PRIME to subjective and data-driven estimation of bivariate distributions representing simulation inputs.",
    "cited_by_count": 36,
    "openalex_id": "https://openalex.org/W1984220656",
    "type": "article"
  },
  {
    "title": "Fast simulation of packet loss rates in a shared buffer communications switch",
    "doi": "https://doi.org/10.1145/226275.226277",
    "publication_date": "1995-10-01",
    "publication_year": 1995,
    "authors": "Cheng‐Shang Chang; Philip Heidelberger; Perwez Shahabuddin",
    "corresponding_authors": "",
    "abstract": "This article describes an efficient technique for estimating, via simulation, the probability of buffer overflows in a queueing model that arises in the analysis of ATM (Asynchronous Transfer Mode) communication switches. There are multiple streams of (autocorrelated) traffic feeding the switch that has a buffer of finite capacity. Each stream is designated as being of either high or low priority. When the queue length reaches a certain threshold, only high priority packets are admitted to the switch's buffer. The problem is to estimate the loss rate of high priority packets. An asymptotically optimal importance sampling approach is developed for this rare event simulation problem. In this approach, the importance sampling is done in two distinct phases. In the first phase, an importance sampling change of measure is used to bring the queue length up to the threshold at which low priority packets get rejected. In the second phase a different importance sampling change of measure is used to move the queue length from the threshold to the buffer capacity.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W1965580719",
    "type": "article"
  },
  {
    "title": "A characterization of the simple failure-biasing method for simulations of highly reliable Markovian Systems",
    "doi": "https://doi.org/10.1145/174619.174621",
    "publication_date": "1994-01-01",
    "publication_year": 1994,
    "authors": "Marvin K. Nakayama",
    "corresponding_authors": "Marvin K. Nakayama",
    "abstract": "Simple failure biasing is an importance-sampling technique used to reduce the variance of estimates of performance measures and their gradients in simulations of highly reliable Markovian systems. Although simple failure biasing yields bounded relative error for the performance measure estimate when the system is balanced, it may not provide bounded relative error when the system is unbalanced. In this article, we provide a characterization of when the simple failure-biasing method produces estimators of a performance measure and its derivatives with bounded relative error. We derive a necessary and sufficient condition on the structure of the system for when the performance measure can be estimated with bounded relative error when using simple failure biasing. Furthermore, a similar condition for the derivative estimators is established. One interesting aspect of the conditions is that it shows that to obtain bounded relative error, not only the most likely paths to system failure must be examined but also some secondary paths leading to failure as well. We also show by example that the necessary and sufficient conditions for a derivative estimator do not imply those for the performance measure estimator; i.e., it is possible to estimate a derivative more efficiently than the performance measure when using simple failure biasing.",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W1972716520",
    "type": "article"
  },
  {
    "title": "Quick simulation of ATM buffers with on-off multiclass Markov fluid sources",
    "doi": "https://doi.org/10.1145/174153.174162",
    "publication_date": "1993-07-01",
    "publication_year": 1993,
    "authors": "George Kesidis; Jean Walrand",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Quick simulation of ATM buffers with on-off multiclass Markov fluid sources Authors: G. Kesidis University of Waterloo University of WaterlooView Profile , J. Walrand University of California University of CaliforniaView Profile Authors Info & Claims ACM Transactions on Modeling and Computer SimulationVolume 3Issue 3July 1993 pp 269–276https://doi.org/10.1145/174153.174162Published:01 July 1993Publication History 29citation311DownloadsMetricsTotal Citations29Total Downloads311Last 12 Months6Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 35,
    "openalex_id": "https://openalex.org/W2035191334",
    "type": "article"
  },
  {
    "title": "Flexible reference trace reduction for VM simulations",
    "doi": "https://doi.org/10.1145/778553.778554",
    "publication_date": "2003-01-01",
    "publication_year": 2003,
    "authors": "Scott F. Kaplan; Yannis Smaragdakis; Paul R. Wilson",
    "corresponding_authors": "",
    "abstract": "The unmanageably large size of reference traces has spurred the development of sophisticated trace reduction techniques. In this article we present two new algorithms for trace reduction: Safely Allowed Drop (SAD) and Optimal LRU Reduction (OLR). Both achieve high reduction factors and guarantee exact simulations for common replacement policies and for memories larger than a user-defined threshold. In particular, simulation on OLR-reduced traces is accurate for the LRU replacement algorithm, while simulation on SAD-reduced traces is accurate for the LRU and OPT algorithms. Both policies can easily be modified and extended to maintain timing information, thus allowing for exact simulation of the Working Set and VMIN policies. OLR also satisfies an optimality property: for a given original trace and chosen memory size, it produces the shortest possible reduced trace that has the same LRU behavior as the original for a memory of at least the chosen size. We present a proof of this optimality of OLR, and show that SAD, while not optimal, yields nearly optimal performance in practice.Our approach has multiple applications, especially in simulating virtual memory systems; many page replacement algorithms are similar to LRU in that more recently referenced pages are likely to be resident. For several replacement algorithms in the literature, SAD- and OLR-reduced traces yield exact simulations. For many other algorithms, our trace reduction eliminates information that matters little: we present extensive measurements to show that the error for simulations of the clock and segq (segmented queue) replacement policies (the most common LRU approximations) is under 3% for the vast majority of memory sizes. In nearly all cases, the error is much smaller than that incurred by the well-known stack deletion technique.SAD and OLR have many desirable properties. In practice, they achieve reduction factors up to several orders of magnitude. The reduction translates to both storage savings and simulation speedups. Both techniques require little memory and perform a single forward traversal of the original trace, making them suitable for online trace reduction. Neither requires that the simulator be modified to accept the reduced trace.",
    "cited_by_count": 33,
    "openalex_id": "https://openalex.org/W2014589371",
    "type": "article"
  },
  {
    "title": "Combining importance sampling and temporal difference control variates to simulate Markov Chains",
    "doi": "https://doi.org/10.1145/974734.974735",
    "publication_date": "2004-01-01",
    "publication_year": 2004,
    "authors": "Ramandeep S. Randhawa; S. Juneja",
    "corresponding_authors": "",
    "abstract": "It is well known that in estimating performance measures associated with a stochastic system a good importance sampling distribution (IS) can give orders of magnitude of variance reduction while a bad one may lead to large, even infinite, variance. In this paper we study how this sensitivity of the estimator variance to the importance sampling change of measure may be \"dampened\" by combining importance sampling with stochastic approximation based temporal difference (TD) method. We consider a finite state space discrete time Markov chain (DTMC) with one-step transition rewards and an absorbing set of states and focus on estimating the cumulative expected reward to absorption starting from any state. In this setting we develop sufficient conditions under which the estimate resulting from the combined approach has a mean square error that asymptotically equals zero even when the estimate formed by using only importance sampling change of measure has infinite variance. In particular, we consider the problem of estimating the small buffer overflow probability in a queuing network, where the change of measure suggested in literature is shown to have infinite variance under certain parameters and where the appropriate combination of IS and TD method can be empirically seen to have a much faster convergence rate compared to naive simulation.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W2073733021",
    "type": "article"
  },
  {
    "title": "Control variates for screening, selection, and estimation of the best",
    "doi": "https://doi.org/10.1145/1122012.1122015",
    "publication_date": "2006-01-01",
    "publication_year": 2006,
    "authors": "Barry L. Nelson; Jeremy Staum",
    "corresponding_authors": "",
    "abstract": "Ranking and selection procedures (R&amp;S) were developed by statisticians to search for the best among a small collection of populations or treatments, where the “best” treatment is typically the one with the largest or smallest expected(long-run average) response. R&amp;S procedures have been successfully extended to address situations that are encountered in stochastic simulation of alternative system designs, including unequal variances across alternatives, dependence both within the output of each system and across the outputs from alternative systems, and large numbers of alternatives to compare. In nearly all cases the estimator of the expected response is a (perhaps generalized) sample mean of the output of interest. In this article we derive R&amp;S procedures that employ control-variate estimators instead of sample means. Control variates can be much more statistically efficient than sample means, leading to R&amp;S procedures that are correspondingly more efficient. We also consider the related problem of estimating the expected value of the best (as opposed to the selected) system design.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2018821717",
    "type": "article"
  },
  {
    "title": "Efficient simulation of buffer overflow probabilities in jackson networks with feedback",
    "doi": "https://doi.org/10.1145/1113316.1113317",
    "publication_date": "2005-10-01",
    "publication_year": 2005,
    "authors": "Sandeep Juneja; Victor Nicola",
    "corresponding_authors": "",
    "abstract": "Consider a Jackson network that allows feedback and that has a single server at each queue. The queues in this network are classified as a single ‘target’ queue and the remaining ‘feeder’ queues. In this setting we develop the large deviations limit and an asymptotically efficient importance sampling estimator for the probability that the target queue overflows during its busy period, under some regularity conditions on the feeder queue-length distribution at the initiation of the target queue busy period. This importance sampling distribution is obtained as a solution to a non-linear program. We especially focus on the case where the feeder queues, at the initiation of the target queue busy period, have the steady state distribution corresponding to these instants. In this setting, we explicitly identify the importance sampling distribution when the feeder queue service rates exceed a specified threshold. We also relate our work to the existing large deviations literature to develop a perspective on successes and limitations of our results.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2094909089",
    "type": "article"
  },
  {
    "title": "Asymptotics and fast simulation for tail probabilities of maximum of sums of few random variables",
    "doi": "https://doi.org/10.1145/1225275.1225278",
    "publication_date": "2007-04-01",
    "publication_year": 2007,
    "authors": "S. Juneja; Rajeeva L. Karandikar; Perwez Shahabuddin",
    "corresponding_authors": "",
    "abstract": "We derive tail asymptotics for the probability that the maximum of sums of a few random variables exceeds an increasing threshold, when the random variables may be light as well as heavy tailed. These probabilities arise in many applications including in PERT networks where our interest may be in measuring the probability of large project delays. We also develop provably asymptotically optimal importance sampling techniques to efficiently estimate these probabilities. In the light-tailed settings we show that an appropriate mixture of exponentially twisted distributions efficiently estimates these probabilities. As is well known, exponential twisting based methods are not applicable in the heavy-tailed settings. To remedy this, we develop techniques that rely on “asymptotic hazard rate twisting” and prove their effectiveness in both light and heavy-tailed settings. We show that in many cases the latter may have implementation advantages over exponential twisting based methods in the light-tailed settings. However, our experiments suggest that when easily implementable, the exponential twisting based methods significantly outperform asymptotic hazard rate twisting based methods.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2065490365",
    "type": "article"
  },
  {
    "title": "Deterministic and stochastic models for the detection of random constant scanning worms",
    "doi": "https://doi.org/10.1145/1346325.1346329",
    "publication_date": "2008-04-01",
    "publication_year": 2008,
    "authors": "Kurt Rohloff; Tamer Bacşar",
    "corresponding_authors": "",
    "abstract": "This article discusses modeling and detection properties associated with the stochastic behavior of Random Constant Scanning (RCS) worms. Although these worms propagate by randomly scanning network addresses to find hosts that are susceptible to infection, traditional RCS worm models are fundamentally deterministic. A density-dependent Markov jump process model for RCS worms is presented and analyzed herein. Conditions are shown for when some stochastic properties of RCS worm propagation can be ignored and when deterministic RCS worm models can be used. A computationally simple hybrid deterministic/stochastic point-process model for locally observed scanning behavior due to the global propagation of an RCS scanning worm epidemic is presented. An optimal hypothesis-testing approach is presented to detect epidemics of these under idealized conditions based on the cumulative sums of log-likelihood ratios using the hybrid RCS worm model. This article presents in a mathematically rigorous fashion why detection techniques that are only based on passively monitoring local IP addresses cannot quickly detect the global propagation of an RCS worm epidemic with a low false alarm rate, even under idealized conditions.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W1988068747",
    "type": "article"
  },
  {
    "title": "A Decision Support System for Placement of Intrusion Detection and Prevention Devices in Large-Scale Networks",
    "doi": "https://doi.org/10.1145/2043635.2043640",
    "publication_date": "2011-12-01",
    "publication_year": 2011,
    "authors": "Rami Puzis; Meytal Tubi; Yuval Elovici; Chanan Glezer; Shlomi Dolev",
    "corresponding_authors": "",
    "abstract": "This article describes an innovative Decision Support System (DSS) for Placement of Intrusion Detection and Prevention Systems (PIDPS) in large-scale communication networks. PIDPS is intended to support network security personnel in optimizing the placement and configuration of malware filtering and monitoring devices within Network Service Providers’ (NSP) infrastructure, and enterprise communication networks. PIDPS meshes innovative and state-of-the-art mechanisms borrowed from the domains of graph theory, epidemic modeling, and network simulation. Scalable network exploitation models enable to define the communication patterns induced by network users (thereby establishing a virtual overlay network), and parallel attack models enable a PIDPS user to define various interdependent network attacks such as: Internet worms, Trojans horses, Denial of Service (DoS) attacks, and others. PIDPS incorporates a set of deployment strategies (employing graph-theoretic centrality measures) in order to facilitate intelligent placement of filtering and monitoring devices; as well as a dedicated network simulator in order to evaluate the various deployments. Experiments with PIDPS indicate that incorporating knowledge on the overlay network (network exploitation patterns) into the placement and configuration of malware filtering and monitoring devices substantially improves the effectiveness of intrusion detection and prevention systems in NSP and enterprise networks.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W1978850354",
    "type": "article"
  },
  {
    "title": "Optimizing Pairwise Box Intersection Checking on GPUs for Large-Scale Simulations",
    "doi": "https://doi.org/10.1145/2499913.2499918",
    "publication_date": "2013-07-01",
    "publication_year": 2013,
    "authors": "Shih‐Hsiang Lo; Che–Rung Lee; I‐Hsin Chung; Yeh‐Ching Chung",
    "corresponding_authors": "",
    "abstract": "Box intersection checking is a common task used in many large-scale simulations. Traditional methods cannot provide fast box intersection checking with large-scale datasets. This article presents a parallel algorithm to perform Pairwise Box Intersection checking on Graphics processing units (PBIG). The PBIG algorithm consists of three phases: planning, mapping and checking. The planning phase partitions the space into small cells, the sizes of which are determined to optimize performance. The mapping phase maps the boxes into the cells. The checking phase examines the box intersections in the same cell. Several performance optimizations, including load-balancing, output data compression/encoding, and pipelined execution, are presented for the PBIG algorithm. The experimental results show that the PBIG algorithm can process large-scale datasets and outperforms three well-performing algorithms.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2021332852",
    "type": "article"
  },
  {
    "title": "Simulating Multivariate Nonhomogeneous Poisson Processes Using Projections",
    "doi": "https://doi.org/10.1145/2331140.2331143",
    "publication_date": "2012-08-01",
    "publication_year": 2012,
    "authors": "Evan Saltzman; John H. Drew; Lawrence M. Leemis; Shane G. Henderson",
    "corresponding_authors": "",
    "abstract": "Established techniques for generating an instance of a multivariate NonHomogeneous Poisson Process (NHPP) such as thinning can become highly inefficient as the dimensionality of the process is increased, particularly if the defining intensity (or rate) function has a pronounced peak. To overcome this inefficiency, we propose an alternative approach where one first generates a projection of the NHPP onto a lower-dimensional space, and then extends the generated points to points in the original space by generating from appropriate conditional distributions. One version of this algorithm replaces a high-dimensional problem with a series of one-dimensional problems. Several examples are presented.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2036733400",
    "type": "article"
  },
  {
    "title": "Discrete Event Execution with One-Sided and Two-Sided GVT Algorithms on 216,000 Processor Cores",
    "doi": "https://doi.org/10.1145/2611561",
    "publication_date": "2014-05-02",
    "publication_year": 2014,
    "authors": "Kalyan S. Perumalla; Alfred J. Park; Vinod Tipparaju",
    "corresponding_authors": "",
    "abstract": "Global Virtual Time (GVT) computation is a key determinant of the efficiency and runtime dynamics of Parallel Discrete Event Simulations (PDES), especially on large-scale parallel platforms. Here, three execution modes of a generalized GVT computation algorithm are studied on high-performance parallel computing systems: (1) a synchronous GVT algorithm that affords ease of implementation, (2) an asynchronous GVT algorithm that is more complex to implement but can relieve blocking latencies, and (3) a variant of the asynchronous GVT algorithm to exploit one-sided communication in extant supercomputing platforms. Performance results are presented of implementations of these algorithms on up to 216,000 cores of a Cray XT5 system, exercised on a range of parameters: optimistic and conservative synchronization, fine- to medium-grained event computation, synthetic and nonsynthetic applications, and different lookahead values. Detailed PDES-specific runtime metrics are presented to further the understanding of tightly coupled discrete event dynamics on massively parallel platforms.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2051169247",
    "type": "article"
  },
  {
    "title": "Incorporating household structure into a discrete-event simulation model of tuberculosis and HIV",
    "doi": "https://doi.org/10.1145/2000494.2000499",
    "publication_date": "2011-08-01",
    "publication_year": 2011,
    "authors": "Georgina R. Mellor; Christine Currie; Elizabeth L. Corbett",
    "corresponding_authors": "",
    "abstract": "Human immunodeficiency virus (HIV) increases the risks of developing tuberculosis (TB) disease following infection, and speeds up disease progression. This has had a devastating effect on TB epidemics in sub-Saharan Africa, where incidence rates have more than trebled in the past twenty years. Current control methods for TB disease have failed to keep pace with this growth, and there is an urgent need to find TB control strategies that are effective in high-HIV prevalent settings. This article describes a discrete-event simulation model of endemic TB that includes the effects of HIV and of household structure on the transmission dynamics of TB. Incorporating a social structure allows us to compare the effectiveness of contact-tracing interventions with case-finding targeted at high risk groups. We describe the modeling of the household structure in some detail, as this has applications to the modeling of other infectious diseases.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2059032906",
    "type": "article"
  },
  {
    "title": "Screening for Dispersion Effects by Sequential Bifurcation",
    "doi": "https://doi.org/10.1145/2651364",
    "publication_date": "2014-12-08",
    "publication_year": 2014,
    "authors": "Bruce E. Ankenman; Russell Cheng; S. M. Lewis",
    "corresponding_authors": "",
    "abstract": "The mean of the output of interest obtained from a run of a computer simulation model of a system or process often depends on many factors; many times, however, only a few of these factors are important. Sequential bifurcation is a method that has been considered by several authors for identifying these important factors using as few runs of the simulation model as possible. In this article, we propose a new sequential bifurcation procedure whose steps use a key stopping rule that can be calculated explicitly, something not available in the best methods previously considered. Moreover, we show how this stopping rule can also be easily modified to efficiently identify those factors that are important in influencing the variability rather than the mean of the output. In empirical studies, the new method performs better than previously published fully sequential bifurcation methods in terms of achieving the prescribed Type I error. It also achieves higher power for detecting moderately large effects using fewer replications than earlier methods. To achieve this control for midrange effects, the new method sometimes requires more replications than other methods in the case where there are many very large effects.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2019843505",
    "type": "article"
  },
  {
    "title": "Multidimensional stochastic approximation",
    "doi": "https://doi.org/10.1145/2553085",
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "Mark Broadie; Deniz M. Cicek; Assaf Zeevi",
    "corresponding_authors": "",
    "abstract": "We consider prototypical sequential stochastic optimization methods of Robbins-Monro (RM), Kiefer-Wolfowitz (KW), and Simultaneous Perturbations Stochastic Approximation (SPSA) varieties and propose adaptive modifications for multidimensional applications. These adaptive versions dynamically scale and shift the tuning sequences to better match the characteristics of the unknown underlying function, as well as the noise level. We test our algorithms on a variety of representative applications in inventory management, health care, revenue management, supply chain management, financial engineering, and queueing theory.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2057226027",
    "type": "article"
  },
  {
    "title": "A Bayesian Approach to Parameter Inference in Queueing Networks",
    "doi": "https://doi.org/10.1145/2893480",
    "publication_date": "2016-08-16",
    "publication_year": 2016,
    "authors": "Weikun Wang; Giuliano Casale; Charles Sutton",
    "corresponding_authors": "",
    "abstract": "The application of queueing network models to real-world applications often involves the task of estimating the service demand placed by requests at queueing nodes. In this article, we propose a methodology to estimate service demands in closed multiclass queueing networks based on Gibbs sampling. Our methodology requires measurements of the number of jobs at resources and can accept prior probabilities on the demands. Gibbs sampling is challenging to apply to estimation problems for queueing networks since it requires one to efficiently evaluate a likelihood function on the measured data. This likelihood function depends on the equilibrium solution of the network, which is difficult to compute in closed models due to the presence of the normalizing constant of the equilibrium state probabilities. To tackle this obstacle, we define a novel iterative approximation of the normalizing constant and show the improved accuracy of this approach, compared to existing methods, for use in conjunction with Gibbs sampling. We also demonstrate that, as a demand estimation tool, Gibbs sampling outperforms other popular Markov Chain Monte Carlo approximations. Experimental validation based on traces from a cloud application demonstrates the effectiveness of Gibbs sampling for service demand estimation in real-world studies.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2342179536",
    "type": "article"
  },
  {
    "title": "Automatic Runtime Adaptation for Component-Based Simulation Algorithms",
    "doi": "https://doi.org/10.1145/2821509",
    "publication_date": "2015-10-19",
    "publication_year": 2015,
    "authors": "Tobias Helms; Roland Ewald; Stefan Rybacki; Adelinde M. Uhrmacher",
    "corresponding_authors": "",
    "abstract": "The state and structure of a model may vary during a simulation and, thus, also its computational demands. Adapting simulation algorithms to these demands at runtime can therefore improve their performance. While this is a general and cross-cutting concern, only few simulation systems offer reusable support for this kind of runtime adaptation. We present a flexible and generic mechanism for the runtime adaptation of component-based simulation algorithms. It encapsulates simulation algorithms applicable to a given problem and employs reinforcement learning to explore the algorithms’ performance during a simulation run. We evaluate our approach on a modeling formalism from computational biology and on a benchmark model defined in PDEVS, thereby investigating a broad range of options for improving its learning capabilities.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W1966407650",
    "type": "article"
  },
  {
    "title": "TADSim",
    "doi": "https://doi.org/10.1145/2699715",
    "publication_date": "2015-04-16",
    "publication_year": 2015,
    "authors": "Susan M. Mniszewski; Christoph Junghans; Arthur F. Voter; Danny Pérez; Stephan Eidenbenz",
    "corresponding_authors": "",
    "abstract": "Next-generation high-performance computing will require more scalable and flexible performance prediction tools to evaluate software--hardware co-design choices relevant to scientific applications and hardware architectures. We present a new class of tools called application simulators —parameterized fast-running proxies of large-scale scientific applications using parallel discrete event simulation. Parameterized choices for the algorithmic method and hardware options provide a rich space for design exploration and allow us to quickly find well-performing software--hardware combinations. We demonstrate our approach with a TADSim simulator that models the temperature-accelerated dynamics (TAD) method, an algorithmically complex and parameter-rich member of the accelerated molecular dynamics (AMD) family of molecular dynamics methods. The essence of the TAD application is captured without the computational expense and resource usage of the full code. We accomplish this by identifying the time-intensive elements, quantifying algorithm steps in terms of those elements, abstracting them out, and replacing them by the passage of time. We use TADSim to quickly characterize the runtime performance and algorithmic behavior for the otherwise long-running simulation code. We extend TADSim to model algorithm extensions, such as speculative spawning of the compute-bound stages, and predict performance improvements without having to implement such a method. Validation against the actual TAD code shows close agreement for the evolution of an example physical system, a silver surface. Focused parameter scans have allowed us to study algorithm parameter choices over far more scenarios than would be possible with the actual simulation. This has led to interesting performance-related insights and suggested extensions.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2022763261",
    "type": "article"
  },
  {
    "title": "PAM",
    "doi": "https://doi.org/10.1145/2827696",
    "publication_date": "2016-01-29",
    "publication_year": 2016,
    "authors": "Witold Dzwinel; Rafał Wcisło; David A. Yuen; Shea Miller",
    "corresponding_authors": "",
    "abstract": "Serious problems with bridging multiple scales in the scope of a single numerical model make computer simulations too demanding computationally and highly unreliable. We present a new concept of modeling framework that integrates the particle method with graph dynamical systems, called the particle automata model (PAM). We assume that the mechanical response of a macroscopic system on internal or external stimuli can be simulated by the spatiotemporal dynamics of a graph of interacting particles representing fine-grained components of biological tissue, such as cells, cell clusters, or microtissue fragments. Meanwhile, the dynamics of microscopic processes can be represented by evolution of internal particle states represented by vectors of finite-state automata. To demonstrate the broad scope of application of PAM, we present three models of very different biological phenomena: blood clotting, tumor proliferation, and fungal wheat infection. We conclude that the generic and flexible modeling framework provided by PAM may contribute to more intuitive and faster development of computational models of complex multiscale biological processes.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2252287572",
    "type": "article"
  },
  {
    "title": "A Large-Deviation-Based Splitting Estimation of Power Flow Reliability",
    "doi": "https://doi.org/10.1145/2875342",
    "publication_date": "2016-02-09",
    "publication_year": 2016,
    "authors": "Wander Wadman; Daan Crommelin; Bert Zwart",
    "corresponding_authors": "",
    "abstract": "Given the continued integration of intermittent renewable generators in electrical power grids, connection overloads are of increasing concern for grid operators. The risk of an overload due to injection variability can be described mathematically as a barrier-crossing probability of a function of a multidimensional stochastic process. Crude Monte Carlo is a well-known technique to estimate probabilities, but it may be computationally too intensive in this case as typical modern power grids rarely exhibit connection overloads. In this article, we derive an approximate rate function for the overload probability using results from large deviations theory. Based on this large deviations approximation, we apply a rare event simulation technique called splitting to estimate overload probabilities more efficiently than Crude Monte Carlo simulation. We show on example power grids with up to 11 stochastic power injections that for a fixed accuracy, Crude Monte Carlo would require tens to millions as many samples as the proposed splitting technique required. We investigate the balance between accuracy and workload of three splitting schemes, each based on a different approximation of the rate function. We justify the workload increase of large-deviation-based splitting compared to naive splitting—that is, splitting based on merely the Euclidean distance to the rare event set. For a fixed accuracy, naive splitting requires over 60 times as much CPU time as large-deviation-based splitting, illustrating its computational advantage. In these examples, naive splitting—unlike large-deviation-based splitting—requires even more CPU time than CMC simulation, illustrating its pitfall.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2254952565",
    "type": "article"
  },
  {
    "title": "Compositional Safe Approximation of Response Time Probability Density Function of Complex Workflows",
    "doi": "https://doi.org/10.1145/3591205",
    "publication_date": "2023-04-05",
    "publication_year": 2023,
    "authors": "Laura Carnevali; Marco Paolieri; Riccardo Reali; Enrico Vicario",
    "corresponding_authors": "",
    "abstract": "We evaluate a stochastic upper bound on the response time Probability Density Function (PDF) of complex workflows through an efficient and accurate compositional approach. Workflows consist of activities having generally distributed stochastic durations with bounded supports, composed through sequence, choice/merge, and balanced/unbalanced split/join operators, possibly breaking the structure of well-formed nesting. Workflows are specified using a formalism defined in terms of Stochastic Time Petri Nets that permits decomposition into a hierarchy of subworkflows with positively correlated response times, guaranteeing that a stochastically larger end-to-end response time PDF is obtained when intermediate results are approximated by stochastically larger PDFs and when dependencies are simplified by replicating activities appearing in multiple subworkflows. In particular, an accurate stochastically larger PDF is obtained by combining shifted truncated Exponential terms with positive or negative rates. Experiments are performed on sets of manually and randomly generated models with increasing complexity, illustrating under which conditions different decomposition heuristics work well in terms of accuracy and complexity and showing that the proposed approach outperforms simulation having the same execution time.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4362636163",
    "type": "article"
  },
  {
    "title": "OOPM/RT",
    "doi": "https://doi.org/10.1145/333296.333339",
    "publication_date": "1999-04-01",
    "publication_year": 1999,
    "authors": "Kangsun Lee; Paul A. Fishwick",
    "corresponding_authors": "",
    "abstract": "When we build a model of real-time systems, we need ways of representing the knowledge about the system and also time requirements for simulating the model. Considering these different needs, our question is “How can we determine the optimal model that simulates the system by a given deadline while still producing valid outputs at an acceptable level of detail?” We have designed OOPM/RT (Object-Oriented Physical Modeler for Real-Time Simulation) methodology. The OOPM/RT framework has three phases: (1) Generation of multimodels in OOPM using both structural and behavioral abstraction techniques, (2) Generation of AT (Abstraction Tree) which organizes the multimodels based on the abstraction relationship to facilitate the optimal model selection process, and (3) Selection of the optimal model that guarantees the deliver simulation results by the given amount of time. A more-detailed model (low abstraction model) is selected when we have enough time to simulate, while a less-detailed model (high abstraction model) is selected when the deadline is immediate. The basic idea of selection is to trade structural information for a faster runtime while minimizing the loss of behavioral information. We propose two possible approaches for the selection: an integer-programming-based approach and a search-based approach. By systematically handling simulation deadlines while minimizing the modeler's interventions, OOPM/RT provides an efficient modeling environment for real-time systems.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W1983558146",
    "type": "article"
  },
  {
    "title": "Improving the aircraft design process using Web-based modeling and simulation",
    "doi": "https://doi.org/10.1145/353735.353739",
    "publication_date": "2000-01-01",
    "publication_year": 2000,
    "authors": "John A. Reed; Gregory J. Follen; Abdollah A. Afjeh",
    "corresponding_authors": "",
    "abstract": "Designing and developing new aircraft systems is time-consuming and expensive. Computational simulation is a promising means for reducing design cycle times, but requires a flexible software environment capable of integrating advanced multidisciplinary and multifidelity analysis methods, dynamically managing data across heterogeneous computing platforms, and distributing computationally complex tasks. Web-based simulation, with its emphasis on collaborative composition of simulation models, distributed heterogeneous execution, and dynamic multimedia documentation, has the potential to meet these requirements. This paper outlines the current aircraft design process, highlighting its problems and complexities, and presents our vision of an aircraft design process using Web-based modeling and simulation.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2068331316",
    "type": "article"
  },
  {
    "title": "Supporting large-scale distributed simulation using HLA",
    "doi": "https://doi.org/10.1145/361026.361034",
    "publication_date": "2000-07-01",
    "publication_year": 2000,
    "authors": "Tainchi Lu; Chung‐Nan Lee; Wenyang Hsia; Ming‐tang Lin",
    "corresponding_authors": "",
    "abstract": "This article describes the design of a Web-based environment to support large-scale distributed simulation using Java and IEEE standard P1516 high level architecture (HLA) framework and rules. Based on the run-time infrastructure (RTI) services within the HLA and Java application programmer's interfaces (APIs) of the RTI, the proposed HLA-based environment provides an architectural foundation to enhance interactivity, portability, and interoperability for Web-based simulations. In addition, the proposed architectural design not only provides a client/server mechanism for simulation on the Web, but also supports a distributed federation execution over the network. A 3-level control mechanism (3LCM) was implemented to HLA-based middleware federateServer, in order to adaptively maintain information consistency and minimize message traffic for distributing information among client hosts and the federateServers. A dynamic filtering strategy (DFS), associated with the data distribution management (DDM) in the HLA RTI, is proposed to minimize false positive updates and enhance the filtering efficiency of subscription regions within an HLA federation. To verify the feasibility of this prototype, a distributed discrete event simulation application in Java was developed and performance of the proposed modeling design and Java RMI's distributed object model presented. From the experimental results, we show that the proposed environment based on HLA using 3LCM and DFS is workable and practical for supporting a large-scale distributed simulation.",
    "cited_by_count": 32,
    "openalex_id": "https://openalex.org/W2111509000",
    "type": "article"
  },
  {
    "title": "A new inversive congruential pseudorandom number generator with power of two modulus",
    "doi": "https://doi.org/10.1145/132277.132278",
    "publication_date": "1992-01-02",
    "publication_year": 1992,
    "authors": "Jürgen Eichenauer‐Herrmann; Holger Grothe",
    "corresponding_authors": "",
    "abstract": "Pseudorandom numbers are important ingredients of stochastic simulations. Their quality is fundamental for the strength of the simulation outcome. The inversive congruential method for generating uniform pseudorandom numbers is a particularly attractive alternative to linear congruential generators, which show many undesirable regularities. In the present paper a new inversive congruential generator with power of two modulus is introduced. Periodicity and statistical independence properties of the generated sequences are analyzed. The results show that these inversive congruential generators perform very satisfactorily.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W2011090873",
    "type": "article"
  },
  {
    "title": "Time-segmentation parallel simulation of networks of queues with loss or communication blocking",
    "doi": "https://doi.org/10.1145/226275.226276",
    "publication_date": "1995-10-01",
    "publication_year": 1995,
    "authors": "Sigrún Andradóttir; Teunis J. Ott",
    "corresponding_authors": "",
    "abstract": "This article is concerned with the efficient simulation of communication systems on parallel processors. In most of the article, we assume that the communication system under study can be modeled as a system of finite exponential queues with a general topology (in particular, the system need not be acyclic) and with either loss or communication blocking. We also assume that to estimate the performance measures of interest, it is necessary to generate long sample paths of the system. This is the case in both steady-state simulations and in long transient simulations. The approach involves applying each processor to simulate the entire system for a part of the time horizon of interest. We present theory guaranteeing the validity of the proposed approach, and numerical results proving the viability of the approach. The proof of the validity of the approach uses the fact that the queueing systems considered are continuous time Markov chains. An essential part of the argument is a proof that sample paths starting from different initial states eventually will couple (i.e., become identical), and a simple method for recognizing the time when a sample path has become independent of the original starting state (i.e., when all sample paths have coupled, regardless of the initial state). Finally, we discuss how our approach can be used for parallel trace-driven simulations of certain nonMarkovian systems, and state the implications of this research for the initialization bias problem in steady-state simulation.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W2037220898",
    "type": "article"
  },
  {
    "title": "Automatic modeling of file system workloads using two-level arrival processes",
    "doi": "https://doi.org/10.1145/290274.290317",
    "publication_date": "1998-07-01",
    "publication_year": 1998,
    "authors": "Peter P. Ware; Thomas W. Page; Barry L. Nelson",
    "corresponding_authors": "",
    "abstract": "This article describes a method for analyzing, modeling, and simulating a two-level arrival-counting process. This method is particlarly appropriate when the number of independent processes is large, as is the case in our motivating application which requires analyzing and representing computer file system trace data for activity on nearly 8,000 files. The method is also applicable to network trace data characterizing communication patterns between pairs of computers. We apply cluster analysis to separate the arrival process into groups or bursts of activity on a file. We then characterize the arrival procss in terms of the time between bursts of activity on file, the time between file events within bursts, and the number of events in a burst. Finally, we model these three components individually, then reassemble the results to produce a synthetic trace generator. In order to gauge the effectiveness of this method, we use synthetically generated (simulated) trace data produced in this way to drive a discrete-event simulation of a distributed replicated file system. We compare the results of the simulation driven by the synthetic trace with the same simulation driven by the original trace data, and conclude that the synthetic data capture the essential characteristics of the empirical trace.",
    "cited_by_count": 31,
    "openalex_id": "https://openalex.org/W2037358560",
    "type": "article"
  },
  {
    "title": "Rapid model parameterization from traffic measurements",
    "doi": "https://doi.org/10.1145/643114.643117",
    "publication_date": "2002-07-01",
    "publication_year": 2002,
    "authors": "Kun-Chan Lan; John Heidemann",
    "corresponding_authors": "",
    "abstract": "The utility of simulations and analysis heavily relies on good models of network traffic. While network traffic constantly is changing over time, existing approaches typically take years from collecting trace, analyzing the data to finally generating and implementing models. In this paper, we describe approaches and tools that support rapid parameterization of traffic models from live network measurements. Rather than treating measured traffic as a time-series of statistics, we utilize the traces to estimate end-user behavior and network conditions to generate application-level simulation models. We also show multi-scaling analytic techniques are helpful for debugging and validating the model. To demonstrate our approaches, we develop structural source-level models for web and FTP traffic and evaluate their accuracy by comparing the outputs of simulation against the original trace. We also compare our work with existing traffic generation tools and show our approach is more flexible in capturing the heterogeneity of traffic. Finally, we automate and integrate the process from trace analysis to model validation for easy model parameterization from new data.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W1994231736",
    "type": "article"
  },
  {
    "title": "A two-stage modeling and simulation process for web-based modeling and simulation",
    "doi": "https://doi.org/10.1145/643114.643118",
    "publication_date": "2002-07-01",
    "publication_year": 2002,
    "authors": "Taewoo Kim; Jinho Lee; Paul A. Fishwick",
    "corresponding_authors": "",
    "abstract": "The area of web-based simulation has thrived primarily on novel methods for executing models, with contributions to both client and server side applications based in Java and other web-accessible languages. However, there has not been a commensurate degree of research in the area of model design, and in the dissemination of models over the web to support discrete event simulation. For a web-based modeling framework to succeed, that framework must support at least one markup language for specifying model content, as well as methods for converting the markup to a programming language, which is used to simulate the model. We present such a framework, called rube , and focus on a two-stage translation process from one markup language (MXL) to another (DXL), with a final process to create either Java or Javascript target code. This framework has allowed us to leverage the eXtensible Markup Language (XML) for capturing the dynamics of a small number of key modeling types commonly found in computer simulation applications. Our approach allows us to achieve customization of a model's appearance, as well to obtain the transformative benefits of specifying model information within an XML format.",
    "cited_by_count": 30,
    "openalex_id": "https://openalex.org/W2034534866",
    "type": "article"
  },
  {
    "title": "Pseudorandom vector generation by the inversive method",
    "doi": "https://doi.org/10.1145/175007.175015",
    "publication_date": "1994-04-01",
    "publication_year": 1994,
    "authors": "Harald Niederreiter",
    "corresponding_authors": "Harald Niederreiter",
    "abstract": "Pseudorandom vectors are of importance for parallelized simulation methods. In this article we carry out a detailed analysis of the inversive method for the generation of uniform pseudorandom vectors. This method can be viewed as an analog of the inversive congruential method for pseudorandom number generation. We study, in particular, the periodicity properties and the behavior under the serial test for sequences of pseudorandom vectors generated by the inversive method.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2078078380",
    "type": "article"
  },
  {
    "title": "Model base management for multifacetted systems",
    "doi": "https://doi.org/10.1145/140765.140791",
    "publication_date": "1991-07-01",
    "publication_year": 1991,
    "authors": "Bernard P. Zeigler; Cheng‐Jye Luh; Tag-Gon Kim",
    "corresponding_authors": "",
    "abstract": "article Model base management for multifacetted systems Share on Authors: Bernard P. Zeigler View Profile , Cheng-Jye Luh View Profile , Tag-Gon Kim View Profile Authors Info & Claims ACM Transactions on Modeling and Computer SimulationVolume 1Issue 3July 1991 pp 195–218https://doi.org/10.1145/140765.140791Published:01 July 1991 14citation425DownloadsMetricsTotal Citations14Total Downloads425Last 12 Months7Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2078010599",
    "type": "article"
  },
  {
    "title": "Fast simulation of overflow probabilities in a queue with Gaussian input",
    "doi": "https://doi.org/10.1145/1138464.1138466",
    "publication_date": "2006-04-01",
    "publication_year": 2006,
    "authors": "A. B. Dieker; Michel Mandjes",
    "corresponding_authors": "",
    "abstract": "In this article, we study a queue fed by a large number n of independent discrete-time Gaussian processes with stationary increments. We consider the many-sources asymptotic regime, that is, the buffer-exceedance threshold B and the service capacity C are scaled by the number of sources ( B ≡ nb and C ≡ nc ).We discuss four methods for simulating the steady-state probability that the buffer threshold is exceeded: the single-twist method (suggested by large deviation theory), the cut-and-twist method (simulating timeslot by timeslot), the random-twist method (the twist is sampled from a discrete distribution), and the sequential-twist method (simulating source by source).The asymptotic efficiency of these four methods is analytically investigated for n → ∞. A necessary and sufficient condition is derived for the efficiency of the single-twist method, indicating that it is nearly always asymptotically inefficient. The other three methods, however, are asymptotically efficient. We numerically evaluate the four methods by performing a detailed simulation study where it is our main objective to compare the three efficient methods in practical situations.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2117093313",
    "type": "article"
  },
  {
    "title": "Simulation output analysis using integrated paths",
    "doi": "https://doi.org/10.1145/1243991.1243994",
    "publication_date": "2007-07-01",
    "publication_year": 2007,
    "authors": "James M. Calvin",
    "corresponding_authors": "James M. Calvin",
    "abstract": "This article considers the steady-state simulation output analysis problem for a process that satisfies a functional central limit theorem. We construct an estimator for the time-average variance constant that is based on iterated integrations of the sample path. When the observations are batched, the method generalizes the method of batch means. One advantage of the method is that it can be used without batching the observations; that is, it can allow for the process variance to be estimated at any time as the simulation runs without waiting for a fixed time horizon to complete. When used in conjunction with batching, the method can improve efficiency (the reciprocal of work times mean-squared error) compared with the standard method of batch means. In numerical experiments, efficiency improvement ranged from a factor of 1.5 (for the waiting time sequence in an M/M/1 queueing system with a single integrated path) up to a factor of 14 (for an autoregressive process and 19 integrated paths).",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2049003409",
    "type": "article"
  },
  {
    "title": "The optimizing-simulator",
    "doi": "https://doi.org/10.1145/1540530.1540535",
    "publication_date": "2009-06-01",
    "publication_year": 2009,
    "authors": "Tongqiang Tony Wu; Warren B. Powell; Alan W. Whisman",
    "corresponding_authors": "",
    "abstract": "There have been two primary modeling and algorithmic strategies for modeling operational problems in transportation and logistics: simulation, offering tremendous modeling flexibility, and optimization, which offers the intelligence of math programming. Each offers significant theoretical and practical advantages. In this article, we show that you can model complex problems using a range of decision functions, including both rule-based and cost-based logic, and spanning different classes of information. We show how different types of decision functions can be designed using up to four classes of information. The choice of which information classes to use is a modeling choice, and requires making specific choices in the representation of the problem. We illustrate these ideas in the context of modeling military airlift, where simulation and optimization have been viewed as competing methodologies. Our goal is to show that these are simply different flavors of a series of integrated modeling strategies.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2053123566",
    "type": "article"
  },
  {
    "title": "Modeling and simulation of SIP tandem server with finite buffer",
    "doi": "https://doi.org/10.1145/1899396.1899399",
    "publication_date": "2011-02-01",
    "publication_year": 2011,
    "authors": "Yang Hong; Changcheng Huang; James Yan",
    "corresponding_authors": "",
    "abstract": "Recent collapses of SIP servers (e.g., Skype outage) indicate that the built-in SIP overload control mechanism cannot mitigate overload effectively. We introduce our analytical approach by investigating an overloaded tandem server scenario. Our analytical model: (1) considers a general case that both arrival rate and service rate for signaling messages are generic random processes; (2) makes a detailed analysis of departure processes; (3) allows us to run fluid-based simulations to observe and analyze SIP system performance under some specific scenarios. This approach is much faster than event-driven simulation which needs to track thousands of retransmission timers for outstanding messages and may crash a simulator due to limited computing resources. Our numerical results help us reach a counterintuitive conclusion: A SIP system with a large buffer size may continuously exhibit overload and long queuing delay after experiencing a short period of demand burst or a temporary server slowdown. Small buffer size, on the other hand, can mitigate overload quickly by rejecting a large portion of the requests from a demand burst, and then resume normal operation after a short period of time. Furthermore, numerical results demonstrate that overload at a downstream server may propagate or migrate to its upstream servers and therefore cause widespread server crashes in a real SIP network.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2034644962",
    "type": "article"
  },
  {
    "title": "A dynamic sort-based DDM matching algorithm for HLA applications",
    "doi": "https://doi.org/10.1145/1921598.1921601",
    "publication_date": "2011-02-04",
    "publication_year": 2011,
    "authors": "Ke Pan; Stephen John Turner; Wentong Cai; Zengxiang Li",
    "corresponding_authors": "",
    "abstract": "Simulation is a low-cost and safe alternative to solve complex problems in various areas. To promote reuse and interoperability of simulation applications and link geographically dispersed simulation components, distributed simulation was introduced. The High-Level Architecture (HLA) is the IEEE standard for distributed simulation. To optimize communication efficiency between simulation components, HLA defines a Data Distribution Management (DDM) service group for filtering out unnecessary data exchange. It relies on the computation of overlap between update and subscription regions, which is called “matching”. There are many existing matching algorithms, among which a sort-based approach improves efficiency by sorting region bounds before the actual matching process, and is found to outperform other existing matching algorithms in many situations. However, the existing algorithm performs matching for all regions in one round and cannot dynamically deal with a selective region modification without processing all the regions once again. Realizing that in many spatial applications, only a small subset of all regions are actually modified in each time step, this article proposes a dynamic sort-based matching algorithm to deal with this efficiently. Theoretical analysis has been carried out for the proposed algorithm and experimental results show that the proposed algorithm has significantly better performance than major existing matching algorithms at dynamic matching.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2013573646",
    "type": "article"
  },
  {
    "title": "Cloning Agent-Based Simulation",
    "doi": "https://doi.org/10.1145/3013529",
    "publication_date": "2017-04-30",
    "publication_year": 2017,
    "authors": "Xiaosong Li; Wentong Cai; Stephen John Turner",
    "corresponding_authors": "",
    "abstract": "Simulation cloning is an efficient way to analyze multiple configurations in a parameter exploration task. A simulation model usually contains a set of tunable parameters for exploring different configurations of a system. To evaluate different design alternatives, multiple simulation instances need to be launched, each evaluating a different parameter configuration. It usually takes a considerable amount of time to execute these simulation instances. Simulation cloning is proposed to reuse computations among simulation instances and to shorten the overall execution time. It is a challenging task to design cloning strategies to explore the computation sharing among simulation instances while maintaining the correctness of execution. In this article, we propose two agent-based simulation (ABS) cloning strategies, the top-down cloning strategy and the bottom-up cloning strategy. The top-down cloning strategy is initially designed and can only be applied to limited scenarios. The bottom-up cloning strategy is an improved strategy to overcome the limitation of the top-down cloning strategy. In the experiments, the effectiveness of the two strategies is analyzed. To show the performance advantages and generality of the bottom-up cloning strategy, a large-scale ABS parameter exploration task is performed, and results are discussed in the article.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2618673685",
    "type": "article"
  },
  {
    "title": "A Fine-Grain Time-Sharing Time Warp System",
    "doi": "https://doi.org/10.1145/3013528",
    "publication_date": "2017-04-30",
    "publication_year": 2017,
    "authors": "Alessandro Pellegrini; Francesco Quaglia",
    "corresponding_authors": "",
    "abstract": "Several techniques have been proposed to improve the performance of Parallel Discrete Event Simulation platforms relying on the Time Warp (optimistic) synchronization protocol. Among them we can mention optimized approaches for state restore, as well as techniques for load balancing or (dynamically) controlling the speculation degree, the latter being specifically targeted at reducing the incidence of causality errors leading to waste of computation. However, in state-of-the-art Time Warp systems, events’ processing is not preemptable, which may prevent the possibility to promptly react to the injection of higher priority (say, lower timestamp) events. Delaying the processing of these events may, in turn, give rise to higher incidence of incorrect speculation. In this article, we present the design and realization of a fine-grain time-sharing Time Warp system, to be run on multi-core Linux machines, which makes systematic use of event preemption in order to dynamically reassign the CPU to higher priority events/tasks. Our proposal is based on a truly dual mode execution, application versus platform, which includes a timer-interrupt-based support for bringing control back to platform mode for possible CPU reassignment according to very fine grain periods. The latter facility is offered by an ad-hoc timer-interrupt management module for Linux, which we release, together with the overall time-sharing support, within the open source ROOT-Sim platform. An experimental assessment based on the classical PHOLD benchmark and two real-world models is presented, which shows how our proposal effectively leads to the reduction of the incidence of causality errors, especially when running with higher degrees of parallelism.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2619944023",
    "type": "article"
  },
  {
    "title": "NeMo",
    "doi": "https://doi.org/10.1145/3186317",
    "publication_date": "2018-09-07",
    "publication_year": 2018,
    "authors": "Mark Plagge; Christopher D. Carothers; Elsa Gonsiorowski; Neil McGlohon",
    "corresponding_authors": "",
    "abstract": "Neuromorphic computing is a broad category of non–von Neumann architectures that mimic biological nervous systems using hardware. Current research shows that this class of computing can execute data classification algorithms using only a tiny fraction of the power conventional CPUs require. This raises the larger research question: How might neuromorphic computing be used to improve application performance, power consumption, and overall system reliability of future supercomputers? To address this question, an open-source neuromorphic processor architecture simulator called NeMo is being developed. This effort will enable the design space exploration of potential heterogeneous compute systems that combine traditional CPUs, GPUs, and neuromorphic hardware. This article examines the design, implementation, and performance of NeMo . Demonstration of NeMo ’s efficient execution using 2,048 nodes of an IBM Blue Gene/Q system, modeling 8,388,608 neuromorphic processing cores is reported. The peak performance of NeMo is just over ten billion events-per-second when operating at this scale.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2892022932",
    "type": "article"
  },
  {
    "title": "A Data Assimilation Framework for Discrete Event Simulations",
    "doi": "https://doi.org/10.1145/3301502",
    "publication_date": "2019-06-18",
    "publication_year": 2019,
    "authors": "Xiaolin Hu; Peisheng Wu",
    "corresponding_authors": "",
    "abstract": "Discrete event simulation (DES) is traditionally used as an offline tool to help users to carry out analysis for complex systems. As real-time sensor data become more and more available, there is increasing interest of assimilating real-time data into DES to achieve on-line simulation to support real-time decision making. This article presents a data assimilation framework that works with DES models. Solutions are proposed to address unique challenges associated with data assimilation for DES. A tutorial example of discrete event road traffic simulation is developed to demonstrate the data assimilation framework as well as principles of data assimilation in general. This article makes contributions to the DES community by presenting a data assimilation framework for DES and a concrete tutorial example that helps readers to grasp the details of data assimilation for DES.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2951589481",
    "type": "article"
  },
  {
    "title": "Exact Sampling of Stationary and Time-Reversed Queues",
    "doi": "https://doi.org/10.1145/2822892",
    "publication_date": "2015-11-16",
    "publication_year": 2015,
    "authors": "José Blanchet; Aya Wallwater",
    "corresponding_authors": "",
    "abstract": "We provide the first algorithm that, under minimal assumptions, allows simulation of the stationary waiting-time sequence of a single-server queue backward in time, jointly with the input processes of the queue (interarrival and service times). The single-server queue is useful in applications of Dominated Coupling from the Past (DCFTP), which is a well-known protocol for simulation without bias from steady-state distributions. Our algorithm terminates in finite time, assuming only finite mean of the interarrival and service times. To simulate the single-server queue in stationarity until the first idle period in finite expected termination time, we require the existence of finite variance. This requirement is also necessary for such idle time (which is a natural coalescence time in DCFTP applications) to have finite mean. Thus, in this sense, our algorithm is applicable under minimal assumptions.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W158389970",
    "type": "article"
  },
  {
    "title": "Convergence of a Particle-Based Approximation of the Block Online Expectation Maximization Algorithm",
    "doi": "https://doi.org/10.1145/2414416.2414418",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Sylvain Le Corff; Gersende Fort",
    "corresponding_authors": "",
    "abstract": "Online variants of the Expectation Maximization (EM) algorithm have recently been proposed to perform parameter inference with large data sets or data streams, in independent latent models and in hidden Markov models. Nevertheless, the convergence properties of these algorithms remain an open problem at least in the hidden Markov case. This contribution deals with a new online EM algorithm that updates the parameter at some deterministic times. Some convergence results have been derived even in general latent models such as hidden Markov models. These properties rely on the assumption that some intermediate quantities are available in closed form or can be approximated by Monte Carlo methods when the Monte Carlo error vanishes rapidly enough. In this article, we propose an algorithm that approximates these quantities using Sequential Monte Carlo methods. The convergence of this algorithm and of an averaged version is established and their performance is illustrated through Monte Carlo experiments.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2033281746",
    "type": "article"
  },
  {
    "title": "Cluster-Based Spatiotemporal Background Traffic Generation for Network Simulation",
    "doi": "https://doi.org/10.1145/2667222",
    "publication_date": "2014-11-13",
    "publication_year": 2014,
    "authors": "Ting Li; Jason Liu",
    "corresponding_authors": "",
    "abstract": "To reduce the computational complexity of large-scale network simulation, one needs to distinguish foreground traffic generated by the target applications one intends to study from background traffic that represents the bulk of the network traffic generated by other applications. Background traffic competes with foreground traffic for network resources and consequently plays an important role in determining the behavior of network applications. Existing background traffic models either operate only at coarse time granularity or focus only on individual links. There is little insight on how to meaningfully apply realistic background traffic over the entire network. In this article, we propose a method for generating background traffic with spatial and temporal characteristics observed from real traffic traces. We apply data clustering techniques to describe the behavior of end hosts as a function of multidimensional attributes and group them into distinct classes, and then map the classes to simulated routers so that we can generate traffic in accordance with the cluster-level statistics. The proposed traffic generator makes no assumption on the target network topology. It is also capable of scaling the generated traffic so that the traffic intensity can be varied accordingly in order to test applications under different and yet realistic network conditions. Experiments show that our method is able to generate traffic that maintains the same spatial and temporal characteristics as in the observed traffic traces.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2085085827",
    "type": "article"
  },
  {
    "title": "Self-Avoiding Random Dynamics on Integer Complex Systems",
    "doi": "https://doi.org/10.1145/2414416.2414790",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Firas Hamze; Ziyu Wang; Nando de Freitas",
    "corresponding_authors": "",
    "abstract": "This article introduces a new specialized algorithm for equilibrium Monte Carlo sampling of binary-valued systems, which allows for large moves in the state space. This is achieved by constructing self-avoiding walks (SAWs) in the state space. As a consequence, many bits are flipped in a single MCMC step. We name the algorithm SARDONICS, an acronym for Self-Avoiding Random Dynamics on Integer Complex Systems. The algorithm has several free parameters, but we show that Bayesian optimization can be used to automatically tune them. SARDONICS performs remarkably well in a broad number of sampling tasks: toroidal ferromagnetic and frustrated Ising models, 3D Ising models, restricted Boltzmann machines and chimera graphs arising in the design of quantum computers.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2152980722",
    "type": "article"
  },
  {
    "title": "Parallel Simulation and Virtual-Machine-Based Emulation of Software-Defined Networks",
    "doi": "https://doi.org/10.1145/2834116",
    "publication_date": "2015-12-28",
    "publication_year": 2015,
    "authors": "Dong Jin; David M. Nicol",
    "corresponding_authors": "",
    "abstract": "The emerging software-defined networking (SDN) technology decouples the control plane from the data plane in a computer network with open and standardized interfaces, and hence opens up the network designers’ options and ability to innovate. The wide adoption of SDN in industry has motivated the development of large-scale, high-fidelity testbeds for evaluation of systems that incorporate SDN. In this article, we develop a framework to support OpenFlow-based SDN simulation and distributed emulation, by leveraging our prior work on a hybrid network testbed with a parallel network simulator and a virtual-machine-based emulation system. We show how to exploit typical SDN controller behaviors to handle performance issues caused by the centralized controller in parallel discrete-event simulation. In particular, we develop an asynchronous synchronization algorithm for passive SDN controllers and design a two-level architecture for active SDN controllers. We evaluate the system performance, showing good scalability. Finally, we present a case study, using the testbed, to evaluate network verification applications in an SDN-based data center network.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2255799302",
    "type": "article"
  },
  {
    "title": "FNM",
    "doi": "https://doi.org/10.1145/2735630",
    "publication_date": "2016-01-28",
    "publication_year": 2016,
    "authors": "Jun Wang; Zhenjiang Dong; Sudhakar Yalamanchili; George Riley",
    "corresponding_authors": "",
    "abstract": "As multicore computer systems become increasingly complex, parallel simulation is becoming an important tool for exploring design space and evaluating design tradeoffs. The key to the success of parallel simulation is the ability to maintain a high degree of parallelism under synchronization constraints. In this article, an enhanced Null-message algorithm called FNM is presented that uses domain-specific knowledge to improve the performance of the basic Null-message algorithm. Based on their runtime states, the components of the simulation model can make a conservative forecast of future interprocess events. The forecast information is carried in the enhanced Null-messages, and, by combining the forecast from both sides of an interprocess link, FNM can achieve a dynamic system lookahead that is much greater than what the static system structure provides. This improved lookahead allows better exploitation of the simulation model's inherent parallelism and leads to better performance. Compared with the basic Null-message algorithm, FNM greatly reduces the amount of Null-messages and improves parallel simulation performance as a result, while at the same time it guarantees simulation correctness as the basic Null-message algorithm does. In tests on cycle-level models with up to 128 cores, FNM shows good scalability and proves to be an effective method.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2294106864",
    "type": "article"
  },
  {
    "title": "Towards Data-Driven Simulation Modeling for Mobile Agent-Based Systems",
    "doi": "https://doi.org/10.1145/3289229",
    "publication_date": "2019-01-31",
    "publication_year": 2019,
    "authors": "Nicholas Keller; Xiaolin Hu",
    "corresponding_authors": "",
    "abstract": "Simulation models are widely used to study complex systems. Current simulation models are generally handcrafted using expert knowledge (knowledge-driven); however, this process is slow and introduces modeler bias. This article presents an approach towards data-driven simulation modeling by developing a framework that discovers simulation models in an automated way for mobile agent-based applications. The framework is comprised of three components: (1) a model space specification, (2) a search method (genetic algorithm), and (3) framework measurement metrics. The model space specification provides a formal specification for the general model structure from which various models can be generated. The search method is used to efficiently search the model space for candidate models that exhibit desired behavior patterns. The five framework measurement metrics: flexibility, comprehensibility, controllability, composability, and robustness, are developed to evaluate the overall framework. The results demonstrate that it is possible to discover a variety of interesting models using the framework.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2773773304",
    "type": "article"
  },
  {
    "title": "New Performance Modeling Methods for Parallel Data Processing Applications",
    "doi": "https://doi.org/10.1145/3309684",
    "publication_date": "2019-06-18",
    "publication_year": 2019,
    "authors": "Janki Bhimani; Ningfang Mi; Miriam Leeser; Zhengyu Yang",
    "corresponding_authors": "",
    "abstract": "Predicting the performance of an application running on parallel computing platforms is increasingly becoming important because of its influence on development time and resource management. However, predicting the performance with respect to parallel processes is complex for iterative and multi-stage applications. This research proposes a performance approximation approach FiM to predict the calculation time with FiM-Cal and communication time with FiM-Com of an application running on a distributed framework. FiM-Cal consists of two key components that are coupled with each other: (1) a Stochastic Markov Model to capture non-deterministic runtime that often depends on parallel resources, e.g., number of processes, and (2) a machine-learning model that extrapolates the parameters for calibrating our Markov model when we have changes in application parameters such as dataset. Along with the parallel calculation time, parallel computing platforms consume some data transfer time to communicate among different nodes. FiM-Com consists of a simulation queuing model to quickly estimate communication time. Our new modeling approach considers different design choices along multiple dimensions, namely (i) process-level parallelism, (ii) distribution of cores on multi-processor platform, (iii) application related parameters, and (iv) characteristics of datasets. The major contribution of our prediction approach is that FiM can provide an accurate prediction of parallel processing time for the datasets that have a much larger size than that of the training datasets. We evaluate our approach with NAS Parallel Benchmarks and real iterative data processing applications. We compare the predicted results (e.g., end-to-end execution time) with actual experimental measurements on a real distributed platform. We also compare our work with an existing prediction technique based on machine learning. We rank the number of processes according to the actual and predicted results from FiM and calculate the correlation between the actual and predicted rankings. Our results show that FiM obtains a high correlation in the range of 0.80 to 0.99, which indicates considerable accuracy of our technique. Such prediction provides data analysts a useful insight of optimal configuration of parallel resources (e.g., number of processes and number of cores) and also helps system designers to investigate the impact of changes in application parameters on system performance.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2953363138",
    "type": "article"
  },
  {
    "title": "Dimensionally Aware Multi-Objective Genetic Programming for Automatic Crowd Behavior Modeling",
    "doi": "https://doi.org/10.1145/3391407",
    "publication_date": "2020-07-05",
    "publication_year": 2020,
    "authors": "Daifeng Li; Jinghui Zhong",
    "corresponding_authors": "",
    "abstract": "One limitation of current data-driven automatic crowd modeling methods is that the models generated have low interpretability, which limits the practical applications of the models. In this article, we propose a new data-driven crowd modeling approach that can generate universal behavior rules with better interpretability. Higher interpretability helps people better understand and analyze the rules. Furthermore, the proposed approach considers both static and dynamic features during modeling to generate a realistic crowd, based on the assumption that humans tend to consider different features with respect to their states. In the proposed method, the automatic behavior rule generation problem is formulated as a symbolic regression problem. Then, the problem is solved by multi-objective genetic programming. On one hand, to improve the interpretability of the behavior rules found, a new mechanism is proposed to guide the algorithm to find concise and dimensionally consistent solutions. On the other hand, decisions made by considering static and dynamic features respectively are combined to improve the generated crowd realism. To validate the effectiveness of the proposed method, three real-world datasets are utilized for training and testing. The simulation results demonstrate that the proposed method is able to find universal behavior rules that are competitive to previous work in accuracy while having better interpretability.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W3039810648",
    "type": "article"
  },
  {
    "title": "Application of Simulation in Healthcare Service Operations",
    "doi": "https://doi.org/10.1145/3427753",
    "publication_date": "2020-12-31",
    "publication_year": 2020,
    "authors": "Sudipendra Nath Roy; Bhavin J. Shah; Hasmukh Gajjar",
    "corresponding_authors": "",
    "abstract": "The health system is intricate due to its dynamic nature and critical service requirements. The involvement of multiple layers of health service providers quadrupled this complexity and results in a complicated operating environment. Simulation is often considered an apt technique to model and study complex systems in the literature. The popularity of simulation in the healthcare domain had only accelerated with time and resulted in a large number of articles intended to solve myriad healthcare problems. This article analyzes healthcare simulation literature of the past decade (2007--2016) that addresses operations management issues in various healthcare service delivery levels and categorizes the literature accordingly. In the next step, we attempt to assimilate the entire literature to capture specific health issues addressed, operations management concepts applied, and simulation methods used, and identify major research gaps. Finally, we develop the research agenda from dividing these gaps into the contextual, conceptual, and methodological genre that is consistent with the previous state-of-the-art literature reviews in operations management. Furthermore, this article demonstrates other minute aspects such as “sources of funding” and “tools used for the research” to maintain coherence with the previous reviews in the healthcare simulation. The objective of this work is twofold: to connect the knowledge continuum to the present, and to provide potential research directions for future academicians.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W3117918501",
    "type": "article"
  },
  {
    "title": "Computing Cumulative Rewards Using Fast Adaptive Uniformization",
    "doi": "https://doi.org/10.1145/2688907",
    "publication_date": "2015-02-17",
    "publication_year": 2015,
    "authors": "Frits Dannenberg; Ernst Moritz Hahn; Marta Kwiatkowska",
    "corresponding_authors": "",
    "abstract": "The computation of transient probabilities for continuous-time Markov chains often employs uniformization, also known as the Jensen method. The fast adaptive uniformization method introduced by Mateescu et al. approximates the probability by neglecting insignificant states and has proven to be effective for quantitative analysis of stochastic models arising in chemical and biological applications. However, this method has only been formulated for the analysis of properties at a given point of time t . In this article, we extend fast adaptive uniformization to handle expected reward properties that reason about the model behavior until time t , for example, the expected number of chemical reactions that have occurred until t . To show the feasibility of the approach, we integrate the method into the probabilistic model checker PRISM and apply it to a range of biological models. The performance of the method is enhanced by the use of interval splitting. We compare our implementation to standard uniformization implemented in PRISM and to fast adaptive uniformization without support for cumulative rewards implemented in MARCIE, demonstrating superior performance.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W1985664547",
    "type": "article"
  },
  {
    "title": "Approximate Inference for Observation-Driven Time Series Models with Intractable Likelihoods",
    "doi": "https://doi.org/10.1145/2592254",
    "publication_date": "2014-05-01",
    "publication_year": 2014,
    "authors": "Ajay Jasra; Nikolas Kantas; Elena Ehrlich",
    "corresponding_authors": "",
    "abstract": "In this article, we consider approximate Bayesian parameter inference for observation-driven time series models. Such statistical models appear in a wide variety of applications, including econometrics and applied mathematics. This article considers the scenario where the likelihood function cannot be evaluated pointwise; in such cases, one cannot perform exact statistical inference, including parameter estimation, which often requires advanced computational algorithms, such as Markov Chain Monte Carlo (MCMC). We introduce a new approximation based upon Approximate Bayesian Computation (ABC). Under some conditions, we show that as n → ∞, with n the length of the time series, the ABC posterior has, almost surely, a Maximum A Posteriori (MAP) estimator of the parameters that is often different from the true parameter. However, a noisy ABC MAP, which perturbs the original data, asymptotically converges to the true parameter, almost surely. In order to draw statistical inference, for the ABC approximation adopted, standard MCMC algorithms can have acceptance probabilities that fall at an exponential rate in n and slightly more advanced algorithms can mix poorly. We develop a new and improved MCMC kernel, which is based upon an exact approximation of a marginal algorithm, whose cost per iteration is random, but the expected cost, for good performance, is shown to be O ( n 2 ) per iteration. We implement our new MCMC kernel for parameter inference from models in econometrics.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2005630497",
    "type": "article"
  },
  {
    "title": "Model-Based Annealing Random Search with Stochastic Averaging",
    "doi": "https://doi.org/10.1145/2641565",
    "publication_date": "2014-08-13",
    "publication_year": 2014,
    "authors": "Jiaqiao Hu; Enlu Zhou; Qi Fan",
    "corresponding_authors": "",
    "abstract": "The model-based methods have recently found widespread applications in solving hard nondifferentiable optimization problems. These algorithms are population-based and typically require hundreds of candidate solutions to be sampled at each iteration. In addition, recent convergence analysis of these algorithms also stipulates a sample size that increases polynomially with the number of iterations. In this article, we aim to improve the efficiency of model-based algorithms by reducing the number of candidate solutions generated per iteration. This is carried out through embedding a stochastic averaging procedure within these methods to make more efficient use of the past sampling information. This procedure not only can potentially reduce the number of function evaluations needed to obtain high-quality solutions, but also makes the underlying algorithms more amenable for parallel computation. The detailed implementation of our approach is demonstrated through an exemplary algorithm instantiation called Model-based Annealing Random Search with Stochastic Averaging (MARS-SA), which maintains the per iteration sample size at a small constant value. We establish the global convergence property of MARS-SA and provide numerical examples to illustrate its performance.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2131846131",
    "type": "article"
  },
  {
    "title": "Balanced and Approximate Zero-Variance Recursive Estimators for the Network Reliability Problem",
    "doi": "https://doi.org/10.1145/2674914",
    "publication_date": "2014-11-13",
    "publication_year": 2014,
    "authors": "Héctor Cancela; Mohamed El Khadiri; Gerardo Rubino; Bruno Tuffin",
    "corresponding_authors": "",
    "abstract": "Exact evaluation of static network reliability parameters belongs to the NP-hard family, and Monte Carlo simulation is therefore a relevant tool to provide their estimations. The first goal of this work is to review a Recursive Variance Reduction (RVR) estimator, which approaches the unreliability by recursively reducing the graph from the random choice of the first working link on selected cuts. We show that the method does not verify the bounded relative error (BRE) property as reliability of individual links goes to one—that is, that the estimator is not robust in general to high reliability of links. We then propose to use the decomposition ideas of the RVR estimator in conjunction with the importance sampling technique. Two new estimators are presented: the first one—the Balanced Recursive Decomposition estimator—chooses the first working link on cuts uniformly, whereas the second—the Zero-Variance Approximation Recursive Decomposition estimator—tries to mimic the estimator with variance zero for this technique. We show that in both cases the BRE property is verified and, moreover, that a vanishing relative error (VRE) property can be obtained for the Zero-Variance Approximation RVR under specific sufficient conditions. A numerical illustration of the power of the methods is provided on several benchmark networks.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2165556721",
    "type": "article"
  },
  {
    "title": "Computing Bayesian Means Using Simulation",
    "doi": "https://doi.org/10.1145/2735631",
    "publication_date": "2016-01-13",
    "publication_year": 2016,
    "authors": "Sigrún Andradóttir; Peter W. Glynn",
    "corresponding_authors": "",
    "abstract": "This article is concerned with the estimation of α = E { r ( Z )}, where Z is a random vector and the function values r ( z ) must be evaluated using simulation. Estimation problems of this form arise in the field of Bayesian simulation, where Z represents the uncertain (input) parameters of a system and r ( z ) is the expected performance of the system when Z = z . Our approach involves obtaining (possibly biased) simulation estimates of the function values r ( z ) for a number of different values of z , and then using a (possibly weighted) average of these estimates to estimate α. We start by considering the case where the chosen values of z are independent and identically distributed observations of the random vector Z (independent sampling). We analyze the resulting estimator as the total computational effort c grows and provide numerical results. Then we show that improved convergence rates can be obtained through the use of techniques other than independent sampling. Specifically, our results indicate that the use of quasi-random sequences yields a better convergence rate than independent sampling, and that in the presence of a suitable special structure, it may be possible to use other numerical integration techniques (such as Simpson’s rule) to achieve the best possible rate c − 1/2 as c → ∞. Finally, we present and analyze a general framework of estimators for α that encompasses independent sampling, quasi-random sequences, and Simpson’s rule as special cases.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2232844652",
    "type": "article"
  },
  {
    "title": "Morphological Coevolution for Fluid Dynamical-Related Risk Mitigation",
    "doi": "https://doi.org/10.1145/2856694",
    "publication_date": "2016-01-29",
    "publication_year": 2016,
    "authors": "Giuseppe Filippone; Donato D’Ambrosio; ‎Davide Marocco; William Spataro",
    "corresponding_authors": "",
    "abstract": "In the lava flow mitigation context, the determination of areas exposed to volcanic risk is crucial for diminishing consequences in terms of human causalities and damages of material properties. In order to mitigate the destructive effects of lava flows along volcanic slopes, the building and positioning of artificial barriers is fundamental for controlling and slowing down the lava flow advance. In this article, an evolutionary computation-based decision support system for defining and optimizing volcanic hazard mitigation interventions is proposed. In particular, the SCIARA-fv2 Cellular Automata numerical model has been applied for simulating lava flows at Mt. Etna (Italy) volcano and Parallel Genetic Algorithms (PGA) adopted for optimizing protective measures construction by morphological evolution. The PGA application regarded the optimization of the position, orientation, and extension of earth barriers built to protect Rifugio Sapienza, a touristic facility located near the summit of the volcano. A preliminary release of the algorithm, called single barrier (SBA) approach, was initially considered. Subsequently, a second GA strategy, called Evolutionary Greedy Strategy (EGS), was implemented by introducing multibarrier protection measures in order to improve the efficiency of the final solution. Finally, a Coevolutionary Cooperative Strategy (CCS), has been introduced where all barriers are encoded in the genotype and, because all the constituents parts of the solution interact with the GA environment, a mechanism of cooperation between individuals has been favored. The study has produced extremely positive results and represents, to our knowledge, the first application of morphological evolution for lava flow mitigation.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2257533236",
    "type": "article"
  },
  {
    "title": "Bayesian Optimisation vs. Input Uncertainty Reduction",
    "doi": "https://doi.org/10.1145/3510380",
    "publication_date": "2022-02-09",
    "publication_year": 2022,
    "authors": "Juan Ungredda; Michael Pearce; Juergen Branke",
    "corresponding_authors": "",
    "abstract": "Simulators often require calibration inputs estimated from real-world data, and the estimate can significantly affect simulation output. Particularly when performing simulation optimisation to find an optimal solution, the uncertainty in the inputs significantly affects the quality of the found solution. One remedy is to search for the solution that has the best performance on average over the uncertain range of inputs yielding an optimal compromise solution. We consider the more general setting where a user may choose between either running simulations or querying an external data source, improving the input estimate and enabling the search for a more targeted, less compromised solution. We explicitly examine the trade-off between simulation and real data collection to find the optimal solution of the simulator with the true inputs. Using a value of information procedure, we propose a novel unified simulation optimisation procedure called Bayesian Information Collection and Optimisation that, in each iteration, automatically determines which of the two actions (running simulations or data collection) is more beneficial. We theoretically prove convergence in the infinite budget limit and perform numerical experiments demonstrating that the proposed algorithm is able to automatically determine an appropriate balance between optimisation and data collection.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W4210899177",
    "type": "article"
  },
  {
    "title": "Contextual Ranking and Selection with Gaussian Processes and OCBA",
    "doi": "https://doi.org/10.1145/3633456",
    "publication_date": "2023-11-20",
    "publication_year": 2023,
    "authors": "Sait Cakmak; Yuhao Wang; Siyang Gao; Enlu Zhou",
    "corresponding_authors": "",
    "abstract": "In many real-world problems, we are faced with the problem of selecting the best among a finite number of alternatives, where the best alternative is determined based on context specific information. In this work, we study the contextual Ranking and Selection problem under a finite-alternative-finite-context setting, where we aim to find the best alternative for each context. We use a separate Gaussian process to model the reward for each alternative and derive the large deviations rate function for both the expected and worst-case contextual probability of correct selection. We propose the GP-C-OCBA sampling policy, which uses the Gaussian process posterior to iteratively allocate observations to maximize the rate function. We prove its consistency and show that it achieves the optimal convergence rate under the assumption of a non-informative prior. Numerical experiments show that our algorithm is highly competitive in terms of sampling efficiency, while having significantly smaller computational overhead.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4388827248",
    "type": "article"
  },
  {
    "title": "Performance Evaluation of Spintronic-Based Spiking Neural Networks Using Parallel Discrete-Event Simulation",
    "doi": "https://doi.org/10.1145/3649464",
    "publication_date": "2024-03-05",
    "publication_year": 2024,
    "authors": "Elkin Cruz-Camacho; Siyuan Qian; Ankit Shukla; Neil McGlohon; Shaloo Rakheja; Christopher D. Carothers",
    "corresponding_authors": "",
    "abstract": "Spintronics devices that use the spin of electrons as the information state variable have the potential to emulate neuro-synaptic dynamics and can be realized within a compact form-factor, while operating at ultra-low energy-delay point. In this paper, we benchmark the performance of a spintronics hardware platform designed for handling neuromorphic tasks. To explore the benefits of spintronics-based hardware on realistic neuromorphic workloads, we developed a Parallel Discrete-Event Simulation model called Doryta, which is further integrated with a materials-to-systems benchmarking framework. The benchmarking framework allows us to obtain quantitative metrics on the throughput and energy of spintronics-based neuromorphic computing and compare these against standard CMOS-based approaches. Although spintronics hardware offers significant energy and latency advantages, we find that for larger neuromorphic circuits, the performance is limited by the interconnection networks rather than the spintronics-based neurons and synapses. This limitation can be overcome by architectural changes to the network. Through Doryta we are also able to show the power of neuromorphic computing by simulating Conway’s Game of Life (GoL), thus showing that it is Turing complete. We show that Doryta obtains over 300 × speedup using 1,024 CPU cores when tested on a convolutional, sparse, neural architecture. When scaled-up 64 times, to a 200 million neuron model, the simulation ran in 3:42 minutes for a total of 2000 virtual clock steps. The conservative approach of execution was found to be faster in most cases than the optimistic approach, even when a tie-breaking mechanism to guarantee deterministic execution, was deactivated.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4392473011",
    "type": "article"
  },
  {
    "title": "Adaptive Synchronization and Pacing Control for Visual Interactive Simulation",
    "doi": "https://doi.org/10.1145/3673898",
    "publication_date": "2024-06-19",
    "publication_year": 2024,
    "authors": "Zhuoxiao Meng; Mingyue Gao; Margherita Grossi; Anibal Siguenza-Torres; Stefano Bortoli; Christoph Sommer; Alois Knoll",
    "corresponding_authors": "",
    "abstract": "Parallel and distributed computing enable the execution of large and complex simulations. Yet, the usual separation of (headless) simulation execution and (subsequent, offline) output analysis often renders the simulation endeavor long and inefficient. Recently, Visual Interactive Simulation (VIS) tools and methods that address this end-to-end efficiency are gaining relevance, offering in-situ visualization, real-time debugging, and computational steering. Here, the typically distributed computing nature of the simulation execution poses synchronization challenges between the headless simulation engine and the user-facing frontend required for Visual Interactive Simulation. To the best of our knowledge, state-of-the-art synchronization approaches fall short due to their rigidity and inability to adapt to real-time user-centric changes. This paper introduces a novel adaptive algorithm to dynamically adjust the simulation’s pacing through a buffer-based framework, informed by predictive workload analysis. Our extensive experimental evaluation across diverse synthetic scenarios illustrates our method’s effectiveness in enhancing runtime efficiency and synchronicity, significantly reducing end-to-end time while minimizing user interaction delays, thereby addressing key limitations of existing synchronization strategies.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4399809059",
    "type": "article"
  },
  {
    "title": "Simple cellular automata as pseudorandom <i>m</i> -sequence generators for built-in self-test",
    "doi": "https://doi.org/10.1145/272991.273007",
    "publication_date": "1998-01-01",
    "publication_year": 1998,
    "authors": "Makoto Matsumoto",
    "corresponding_authors": "Makoto Matsumoto",
    "abstract": "We propose an extremely simple and explicit construction of cellular automata (CA) generating pseudorandom m -sequences, which consist of only one type of cells. This construction has advantages over the previous researches in the following points. (1) There is no need to search for primitive polynomials; a simple sufficient number-theoretic condition realizes maximal periodic CA with periods 2 m − 1, m =2, 3, 5, 89, 9689, 21701, 859433. (2) The configuration does not require hybrid constructions. This makes the implementation much easier. This is a modification of the Rule-90 by Wolfram. We list our CAs with maximal period, up to the size 300. We also discuss the controllability of the CA, randomness of the generated sequence, and a two-dimensional version.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W1988348445",
    "type": "article"
  },
  {
    "title": "A rejection technique for sampling from log-concave multivariate distributions",
    "doi": "https://doi.org/10.1145/290274.290287",
    "publication_date": "1998-07-01",
    "publication_year": 1998,
    "authors": "Josef Leydold",
    "corresponding_authors": "Josef Leydold",
    "abstract": "Different universal methods (also called automatic or black-box methods) have been suggested for sampling form univariate log-concave distributions. The descriptioon of a suitable universal generator for multivariate distributions in arbitrary dimensions has not been published up to now. The new algorithm is based on the method of transformed density rejection. To construct a hat function for the rejection algorithm the multivariate density is transformed by a proper transformation T into a concave function (in the case of log-concave density T(x) = log( x ).) Then it is possible to construct a dominating function by taking the minimum of serveral tangent hyperplanes that are transformed back by T -1 into the original scale. The domains of different pieces of the hat function are polyhedra in the multivariate case. Although this method can be shown to work, it is too slow and complicated in higher dimensions. In this article we split the ℝ n into simple cones. The hat function is constructed piecewise on each of the cones by tangent hyperplanes. The resulting function is no longer continuous and the rejection constant is bounded from below but the setup and the generation remains quite fast in higher dimensions; for example, n = 8. The article describes the details of how this main idea can be used to construct algorithm TDRMV that generates random tuples from a multivariate log-concave distribution with a computable density. Although the developed algorithm is not a real black box method it is adjustable for a large class of log-concave densities.",
    "cited_by_count": 29,
    "openalex_id": "https://openalex.org/W2089603726",
    "type": "article"
  },
  {
    "title": "The implementation of temporal intervals in qualitative simulation graphs",
    "doi": "https://doi.org/10.1145/361026.361030",
    "publication_date": "2000-07-01",
    "publication_year": 2000,
    "authors": "Ricki G. Ingalls; Douglas J. Morrice; Andrew B. Whinston",
    "corresponding_authors": "",
    "abstract": "In this paper we develop and implement a simulation modeling methodology that combines discrete event simulation with qualitive simulation. Our main reason for doing so is to extend the application of discrete event simulation to systems found in business for which precise quantitative information is lacking. The approach discussed in this paper is the implementation of temporal interval specifications in the discrete event model and the construction of a temporal interval clock for the qualitative simulation model.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2040321167",
    "type": "article"
  },
  {
    "title": "The theory of direct probability redistribution and its application to rare event simulation",
    "doi": "https://doi.org/10.1145/333296.333349",
    "publication_date": "1999-04-01",
    "publication_year": 1999,
    "authors": "Zsolt Haraszti; J.K. Townsend",
    "corresponding_authors": "",
    "abstract": "Rare event simulation is an important area of simulation theory, producing algorithms that can significantly reduce the simulation time when analyzing problems that involve rare events. However, existing rare event simulation techniques are rather restrictive, i.e., applicable only to systems with modest complexity. In this paper, we first develop a Markov chain transformation theory that can redistribute steady-state probabilities in a finite-size discrete-time Markov chain in an arbitrary and controlled manner. We descriptively name the theoretical procedure “direct probability redistribution” (DPR). In the second part of the paper, we develop DPR theory into a simulation algorithm that uses trajectory splitting to realize the DPR effect without knowledge of the transition probability matrix, thus allowing for easy application to systems with realistic complexity. DPR-based splitting can significantly reduce the simulation time by increasing the visitng frequency of rare states, and it avoids the problems associated with the decreasing likelihood ratio, which can be a limitation in conventional Importance Sampling techniques. The main advantage of the DPR-based simulation technique over existing splitting techniques is that DPR does not impose any restrictions on the state transitions and it provides asymptotically unbiased estimates even if the rare event set overlaps many splitting partitions. We conclude by providing examples where DPR-based simulation has been successfully applied to nontrivial queuing problems, including a system with flow control.",
    "cited_by_count": 28,
    "openalex_id": "https://openalex.org/W2046005796",
    "type": "article"
  },
  {
    "title": "Execution-driven simulation of multiprocessors",
    "doi": "https://doi.org/10.1145/200883.200890",
    "publication_date": "1994-10-01",
    "publication_year": 1994,
    "authors": "Sandhya Dwarkadas; J. Robert Jump; James B. Sinclair",
    "corresponding_authors": "",
    "abstract": "This article describes and evaluates an efficient execution-driven technique for the simulation of multiprocessors that includes the simulation of system memory and that is driven by real program work loads. The technique produces correctly interleaved address traces at run-time without disk access overhead or hardware support, allowing accurate simulation of the effects of a variety of architectural alternatives on programs. We have implemented a simulator based on this technique that offers substantial advantages in terms of reduced time and space overheads when compared to instruction-driven or trace-driven simulation techniques, without significant loss of accuracy. The article presents the results of several validation experiments used to quantify the accuracy and efficiency of the simulator for sequential, distributed, and shared-memory multiprocessors, and several parallel programs. These experiments show that prediction errors of less than 5 percent as compared to actual execution times, and overheads 6 to 30 times lower than those incurred by cycle-level simulation can be achieved. Predictions of relative performance metrics such as speedup tend to be even more accurate, making this technique especially attractive as an efficient method for comparative investigations of parallel system designs.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2011387446",
    "type": "article"
  },
  {
    "title": "Fast algorithms for generating discrete random variates with changing distributions",
    "doi": "https://doi.org/10.1145/151527.151529",
    "publication_date": "1993-01-02",
    "publication_year": 1993,
    "authors": "Sanguthevar Rajasekaran; Keith W. Ross",
    "corresponding_authors": "",
    "abstract": "One of the most fundamental operations when simulating a stochastic discrete-event dynamic system is the generation of a nonuniform discrete random variate. The simplest form of this operation can be stated as follows: Generate a random variable X that is distributed over the integers 1,2,…, n such that P(X= i ) = a i /( a 1 +…+ a n ), where a i 's are fixed nonnegative numbers. The well-known “alias algorithm” is available to accomplish this task in O(1) time. A more difficult problem is to generate variates for X when the a i 's are changing with time. We present three rejection-based algorithms for this task, and for each algorithm we characterize the performance in terms of acceptance probability and the expected effort to generate a variate. We show that, under fairly unrestrictive conditions, the long-run expected effort is O(1). Applications to Markovian queuing networks are discussed. We also compare the three algorithms with competing schemes appearing in the literature.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2013491876",
    "type": "article"
  },
  {
    "title": "Bounds and approximations for self-initiating distributed simulation without lookahead",
    "doi": "https://doi.org/10.1145/130611.130615",
    "publication_date": "1991-10-01",
    "publication_year": 1991,
    "authors": "Robert E. Felderman; Leonard Kleinrock",
    "corresponding_authors": "",
    "abstract": "We provide upper and lower bounds and an approximation for speedup of an optimistic self-initiated distributed simulation using a very simple model. We assume an arbitrary number of processors and a uniform connection topology. By showing that the lower bound increases essentially linearly with P , the number of processors, we find that the optimistic approach scales well as P increases. The model tracks the progress of Global Virtual Time (GVT) and eliminates the need to know the virtual time positions of all processors, thus making the analysis quite straightforward.",
    "cited_by_count": 27,
    "openalex_id": "https://openalex.org/W2067470455",
    "type": "article"
  },
  {
    "title": "Digital inversive pseudorandom numbers",
    "doi": "https://doi.org/10.1145/200883.200896",
    "publication_date": "1994-10-01",
    "publication_year": 1994,
    "authors": "Jürgen Eichenauer‐Herrmann; Harald Niederreiter",
    "corresponding_authors": "",
    "abstract": "A new algorithm, the digital inversive method , for generating uniform pseudorandom numbers is introduced. This algorithm starts from an inversive recursion in a large finite field and derives pseudorandom numbers from it by the digital method. If the underlying finite field has q elements, then the sequences of digital inversive pseudorandom numbers with maximum possible period length q can be characterized. Sequences of multiprecision pseudorandom numbers with very large period lengths are easily obtained by this new method. Digital inversive pseudorandom numbers satisfy statistical independence properties that are close to those of truly random numbers in the sense of asymptotic discrepancy. If q is a power of 2, then the digital inversive method can be implemented in a very fast manner.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W1976217148",
    "type": "article"
  },
  {
    "title": "A sequential experimental design procedure for the estimation of first- and second-order simulation metamodels",
    "doi": "https://doi.org/10.1145/174153.174156",
    "publication_date": "1993-07-01",
    "publication_year": 1993,
    "authors": "Joan M. Donohue; Ernest C. Houck; Raymond H. Myers",
    "corresponding_authors": "",
    "abstract": "article A sequential experimental design procedure for the estimation of first- and second-order simulation metamodels Share on Authors: Joan M. Donohue University of South Carolina University of South CarolinaView Profile , Ernest C. Houck Virginia Polytechnic Institute and State University Virginia Polytechnic Institute and State UniversityView Profile , Raymond H. Myers Virginia Polytechnic Institute and State University Virginia Polytechnic Institute and State UniversityView Profile Authors Info & Claims ACM Transactions on Modeling and Computer SimulationVolume 3Issue 3July 1993 pp 190–224https://doi.org/10.1145/174153.174156Online:01 July 1993Publication History 22citation545DownloadsMetricsTotal Citations22Total Downloads545Last 12 Months4Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2003521840",
    "type": "article"
  },
  {
    "title": "Estimation of internet file-access/modification rates from indirect data",
    "doi": "https://doi.org/10.1145/1103323.1103326",
    "publication_date": "2005-07-01",
    "publication_year": 2005,
    "authors": "Norman Matloff",
    "corresponding_authors": "Norman Matloff",
    "abstract": "Consider an Internet file for which data on last time of access/modification (A/M) of the file are collected at periodic intervals, but for which direct A/M data are not available. Methodology is developed here that enables estimation of the A/M rates, in spite of having only indirect data of this nature. Both parametric and nonparametric methods are developed. Theoretical and empirical analyses are presented that indicate that the problem is indeed statistically tractable, and that the methods developed are of practical value. Behavior of the parametric estimators is examined when these assumptions are violated, and these estimators are found to be robust against some such violations.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2017983970",
    "type": "article"
  },
  {
    "title": "Staged simulation",
    "doi": "https://doi.org/10.1145/985793.985797",
    "publication_date": "2004-04-01",
    "publication_year": 2004,
    "authors": "Kevin Walsh; Emin Gün Sirer",
    "corresponding_authors": "",
    "abstract": "This article describes staged simulation , a technique for improving the run time performance and scale of discrete event simulators. Typical network simulations are limited in speed and scale due to redundant computations encountered both within a single simulation run and between successive runs. Staged simulation proposes to restructure discrete event simulators to operate in stages that precompute, cache, and reuse partial results to drastically reduce redundant computation within and across simulations. We present a general and flexible framework for staging, and identify the advantages and trade-offs of its application to wireless network simulations, a particularly challenging simulation domain. Experience with applying staged simulation to the ns2 simulator shows that staging can improve execution time by an order of magnitude or more and enable the simulation of wireless networks with tens of thousands of nodes.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2057517531",
    "type": "article"
  },
  {
    "title": "An alternative time management mechanism for distributed simulations",
    "doi": "https://doi.org/10.1145/1060576.1060577",
    "publication_date": "2005-04-01",
    "publication_year": 2005,
    "authors": "Wentong Cai; Stephen John Turner; Bu‐Sung Lee; Junlan Zhou",
    "corresponding_authors": "",
    "abstract": "Over the past few years, there has been a keen interest in the management of time in distributed simulation environments. Previous emphasis in time management (TM) services has been based on time stamp ordering, which is both computation and bandwidth intensive. This article discusses an alternative approach to time management based on causal ordering. Traditional causal ordering protocols incur a large amount of communication overhead, which is generally of the order of N 2 for a distributed system of N processes. A new causal ordering protocol proposed by the authors, the Modified Schiper-Eggli-Sandoz (MSES) protocol, is presented in this article. This new protocol minimizes the control information overhead of causal ordering by using the direct dependency tracking technique. The MSES protocol works well in both unicast and multicast environments, without relying on information about the underlying network topology and communication pattern among the processes of the distributed system. The MSES protocol has been successfully implemented as a middleware on top of DMSO RTI. Experiments have been conducted to benchmark the performance of the new time management mechanism with respect to the existing TM mechanisms available in DMSO RTI. The simulation scenarios of the experiments vary with different degrees of inter-federate dependency and federate event granularities. The ordering limitations of the causality based TM mechanism are addressed in this article and the trade-off of the degree of event ordering and execution speed of simulations is discussed.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2083384182",
    "type": "article"
  },
  {
    "title": "A model of the spread of randomly scanning Internet worms that saturate access links",
    "doi": "https://doi.org/10.1145/1346325.1346327",
    "publication_date": "2008-04-01",
    "publication_year": 2008,
    "authors": "George Kesidis; Ihab Hamadeh; Youngmi Jin; Soranun Jiwasurat; Milan Vojnović",
    "corresponding_authors": "",
    "abstract": "We present a simple, deterministic mathematical model for the spread of randomly scanning and bandwidth-saturating Internet worms. Such worms include Slammer and Witty, both of which spread extremely rapidly. Our model, consisting of coupled Kermack-McKendrick (a.k.a. stratified susceptibles-infectives (SI)) equations, captures both the measured scanning activity of the worm and the network limitation of its spread, that is, the effective scan-rate per worm/infective. The Internet is modeled as an ideal core network to which each peripheral (e.g., enterprise) network is connected via a single access link. It is further assumed in this note that as soon as a single end-system in the peripheral network is infected by the worm, the subsequent scanning of the rest of the Internet saturates the access link, that is, there is “instant” saturation. We fit our model to available data for the Slammer worm and demonstrate the model's ability to accurately represent Slammer's total scan-rate to the core.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2010338040",
    "type": "article"
  },
  {
    "title": "The impact of service demand variability on resource allocation strategies in a grid system",
    "doi": "https://doi.org/10.1145/1842722.1842724",
    "publication_date": "2010-10-01",
    "publication_year": 2010,
    "authors": "Stylianos Zikos; Helen D. Karatza",
    "corresponding_authors": "",
    "abstract": "Scheduling and resource management play an important role in building complex distributed systems, such as grids. In this article we study the impact on performance of job service demand variability in a two-level grid architecture, given that the grid and local schedulers are unaware of each job's service demand. We examine two scheduling policies at grid level, which utilize site load information and three policies at local level. A simulation model is used to evaluate performance. Results show that service demand variability degrades performance, and thus a suitable local resource allocation policy is needed to reduce its impact.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2047759079",
    "type": "article"
  },
  {
    "title": "Optimal scheduling in high-speed downlink packet access networks",
    "doi": "https://doi.org/10.1145/1870085.1870088",
    "publication_date": "2010-12-01",
    "publication_year": 2010,
    "authors": "Hussein Al-Zubaidy; Ioannis Lambadaris; J. Talim",
    "corresponding_authors": "",
    "abstract": "We present an analytic model and a methodology to determine the optimal packet scheduling policy in a High-Speed Downlink Packet Access (HSDPA) system. The optimal policy is the one that maximizes cell throughput while maintaining a level of fairness between the users in the cell. A discrete stochastic dynamic programming model for the HSDPA downlink scheduler is presented. Value iteration is then used to solve for the optimal scheduling policy. We use a FSMC (Finite State Markov Channel) to model the HSDPA downlink channel. A near-optimal heuristic scheduling policy is developed. Simulation is used to study the performance of the resulting heuristic policy and compare it to the computed optimal policy. The results show that the performance of the heuristic policy is very close to that of the optimal policy. The heuristic policy has much less computational complexity, which makes it easy to deploy, with only slight reduction in performance compared to the optimal policy.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2081195274",
    "type": "article"
  },
  {
    "title": "A mixed reality approach for interactively blending dynamic models with corresponding physical phenomena",
    "doi": "https://doi.org/10.1145/1842722.1842727",
    "publication_date": "2010-10-01",
    "publication_year": 2010,
    "authors": "John Quarles; Paul A. Fishwick; Samsun Lampotang; Ira Fischler; Benjamin Lok",
    "corresponding_authors": "",
    "abstract": "The design, visualization, manipulation, and implementation of models for computer simulation are key parts of the discipline. Models are constructed as a means to understand physical phenomena as state changes occur over time. One issue that arises is the need to correlate models and their components with the phenomena being modeled. For example, a part of an automotive engine needs to be placed into cognitive context with the diagrammatic icon that represents that part's function. A typical solution to this problem is to display a dynamic model of the engine in one window and the engine's CAD model in another. Users are expected to, on their own, mentally blend the dynamic model and the physical phenomenon into the same context. However, this contextualization is not trivial in many applications. Our approach expands upon this form of user interaction by specifying two ways in which dynamic models and the corresponding physical phenomena may be viewed, and experimented with, within the same human interaction space. We present a methodology and implementation of contextualization for diagram-based dynamic models using an anesthesia machine, and then follow up with a human study of its effects on spatial cognition.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2017693553",
    "type": "article"
  },
  {
    "title": "Reversible Parallel Discrete Event Formulation of a TLM-Based Radio Signal Propagation Model",
    "doi": "https://doi.org/10.1145/2043635.2043639",
    "publication_date": "2011-12-01",
    "publication_year": 2011,
    "authors": "Sudip K. Seal; Kalyan S. Perumalla",
    "corresponding_authors": "",
    "abstract": "Radio signal strength estimation is essential in many applications, including the design of military radio communications and industrial wireless installations. For scenarios with large or richly featured geographical volumes, parallel processing is required to meet the memory and computation time demands. Here, we present a scalable and efficient parallel execution of the sequential model for radio signal propagation recently developed by Nutaro et al. [2008]. Starting with that model, we (a) provide a vector-based reformulation that has significantly lower computational overhead for event handling, (b) develop a parallel decomposition approach that is amenable to reversibility with minimal computational overheads, (c) present a framework for transparently mapping the conservative time-stepped model into an optimistic parallel discrete event execution, (d) present a new reversible method, along with its analysis and implementation, for inverting the vector-based event model to be executed in an optimistic parallel style of execution, and (e) present performance results from implementation on Cray XT platforms. We demonstrate scalability, with the largest runs tested on up to 127,500 cores of a Cray XT5, enabling simulation of larger scenarios and with faster execution than reported before on the radio propagation model. This also represents the first successful demonstration of the ability to efficiently map a conservative time-stepped model to an optimistic discrete-event execution.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W1974144944",
    "type": "article"
  },
  {
    "title": "On importance sampling with mixtures for random walks with heavy tails",
    "doi": "https://doi.org/10.1145/2133390.2133392",
    "publication_date": "2012-03-01",
    "publication_year": 2012,
    "authors": "Henrik Hult; Jens Svensson",
    "corresponding_authors": "",
    "abstract": "State-dependent importance sampling algorithms based on mixtures are considered. The algorithms are designed to compute tail probabilities of a heavy-tailed random walk. The increments of the random walk are assumed to have a regularly varying distribution. Sufficient conditions for obtaining bounded relative error are presented for rather general mixture algorithms. Two new examples, called the generalized Pareto mixture and the scaling mixture, are introduced. Both examples have good asymptotic properties and, in contrast to some of the existing algorithms, they are very easy to implement. Their performance is illustrated by numerical experiments. Finally, it is proved that mixture algorithms of this kind can be designed to have vanishing relative error.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2073951051",
    "type": "article"
  },
  {
    "title": "The double CFTP method",
    "doi": "https://doi.org/10.1145/1899396.1899398",
    "publication_date": "2011-02-01",
    "publication_year": 2011,
    "authors": "Luc Devroye; Lancelot F. James",
    "corresponding_authors": "",
    "abstract": "We consider the problem of the exact simulation of random variables Z that satisfy the distributional identity Z = L VY + (1- V ) Z , where V ∈ [0,1] and Y are independent, and = L denotes equality in distribution. Equivalently, Z is the limit of a Markov chain driven by that map. We give an algorithm that can be automated under the condition that we have a source capable of generating independent copies of Y , and that V has a density that can be evaluated in a black-box format. The method uses a doubling trick for inducing coalescence in coupling from the past. Applications include exact samplers for many Dirichlet means, some two-parameter Poisson--Dirichlet means, and a host of other distributions related to occupation times of Bessel bridges that can be described by stochastic fixed point equations.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2164907674",
    "type": "article"
  },
  {
    "title": "Relaxing Synchronization in Parallel Agent-Based Road Traffic Simulation",
    "doi": "https://doi.org/10.1145/2994143",
    "publication_date": "2017-04-30",
    "publication_year": 2017,
    "authors": "Yadong Xu; Wentong Cai; Heiko Aydt; Michael Lees; Daniel Zehe",
    "corresponding_authors": "",
    "abstract": "Large-scale agent-based traffic simulation is computationally intensive. Parallel computing can help to speed up agent-based traffic simulation. Parallelization of agent-based traffic simulations is generally achieved by decomposing the road network into subregions. The agents in each subregion are executed by a Logical Process (LP). There are data dependencies between LPs which require synchronization of LPs. An asynchronous protocol allows LPs to progress and communicate asynchronously. LPs use lookahead to indicate the time to synchronize with other LPs. Larger lookahead means less frequent synchronization operations. High synchronization overhead is still a major performance issue of large-scale parallel agent-based traffic simulations. In this article, two methods to increase the lookahead of LPs for an asynchronous protocol are developed. They take advantage of uncertainties in traffic simulation to relax synchronization without altering simulation results statistically. Efficiency of the proposed methods is investigated in the parallel agent-based traffic simulator SEMSim Traffic. Experiment results showed that the proposed methods are able to reduce overall running time of the parallel simulation compared to existing methods.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2620095271",
    "type": "article"
  },
  {
    "title": "Mesoscopic Modelling of Pedestrian Movement Using C <scp>arma</scp> and Its Tools",
    "doi": "https://doi.org/10.1145/3155338",
    "publication_date": "2018-03-09",
    "publication_year": 2018,
    "authors": "Vashti Galpin; Natalia Zoń; Pia Wilsdorf; Stephen Gilmore",
    "corresponding_authors": "",
    "abstract": "In this article, we assess the suitability of the Carma (Collective Adaptive Resource-sharing Markovian Agents) modelling language for mesoscopic modelling of spatially distributed systems where the desired model lies between an individual-based (microscopic) spatial model and a population-based (macroscopic) spatial model. Our modelling approach is mesoscopic in nature because it does not model the movement of individuals as an agent-based simulation in two-dimensional space, nor does it make a continuous-space approximation of the density of a population of individuals using partial differential equations. The application that we consider is pedestrian movement along paths that are expressed as a directed graph. In the models presented, pedestrians move along path segments at rates that are determined by the presence of other pedestrians, and make their choice of the path segment to cross next at the intersections of paths. Information about the topology of the path network and the topography of the landscape can be expressed as separate functional and spatial aspects of the model by making use of C arma language constructs for representing space. We use simulation to study the impact on the system dynamics of changes to the topology of paths and show how C arma provides suitable modelling language constructs that make it straightforward to change the topology of the paths and other spatial aspects of the model without completely restructuring the C arma model. Our results indicate that it is difficult to predict the effect of changes to the network structure and that even small changes can have significant effects.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2790128392",
    "type": "article"
  },
  {
    "title": "Combining Simulation and Emulation Systems for Smart Grid Planning and Evaluation",
    "doi": "https://doi.org/10.1145/3186318",
    "publication_date": "2018-08-30",
    "publication_year": 2018,
    "authors": "Christopher Hannon; Jiaqi Yan; Dong Jin; Chen Chen; Jianhui Wang",
    "corresponding_authors": "",
    "abstract": "Software-defined networking (SDN) enables efficient network management. As the technology matures, utilities are looking to integrate those benefits to their operations technology (OT) networks. To help the community to better understand and evaluate the effects of such integration, we develop DSSnet, a testing platform that combines a power distribution system simulator and an SDN-based network emulator for smart grid planning and evaluation. DSSnet relies on a container-based virtual time system to achieve efficient synchronization between the simulation and emulation systems. To enhance the system scalability and usability, we extend DSSnet to support a distributed controller environment. To enhance system fidelity, we extend the virtual time system to support kernel-based switches. We also evaluate the system performance of DSSnet and demonstrate the usability of DSSnet with a resilient demand response application case study.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2888841863",
    "type": "article"
  },
  {
    "title": "Adaptive Resource Provisioning Mechanism in VEEs for Improving Performance of HLA-Based Simulations",
    "doi": "https://doi.org/10.1145/2717309",
    "publication_date": "2015-06-29",
    "publication_year": 2015,
    "authors": "Zengxiang Li; Wentong Cai; Stephen John Turner; Xiaorong Li; Ta Nguyen Binh Duong; Rick Siow Mong Goh",
    "corresponding_authors": "",
    "abstract": "Parallel and distributed simulations (or High-Level Architecture (HLA)-based simulations) employing optimistic synchronization allow federates to advance simulation time freely at the risk of overoptimistic executions and execution rollbacks. As a result, the simulation performance may degrade significantly due to the simulation workload imbalance among federates. In this article, we investigate the execution of parallel and distributed simulations on Cloud and data centers with Virtual Execution Environments (VEEs). In order to speed up simulation execution, an Adaptive Resource Provisioning Mechanism in Virtual Execution Environments (ArmVee) is proposed. It is composed of a performance monitor and a resource manager. The former measures federate performance transparently to the simulation application. The latter distributes available resources among federates based on the measured federate performance. Federates with different simulation workloads are thus able to advance their simulation times with comparable speeds, thus are able to avoid wasting time and resources on overoptimistic executions and execution rollbacks. ArmVee is evaluated using a real-world simulation model with various simulation workload inputs and different parameter settings. The experimental results show that ArmVee is able to speed up the simulation execution significantly. In addition, it also greatly reduces memory usage and is scalable.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W1978666265",
    "type": "article"
  },
  {
    "title": "Adaptive Equi-Energy Sampler",
    "doi": "https://doi.org/10.1145/2414416.2414421",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Amandine Schreck; Gersende Fort; Éric Moulines",
    "corresponding_authors": "",
    "abstract": "Markov chain Monte Carlo (MCMC) methods allow to sample a distribution known up to a multiplicative constant. Classical MCMC samplers are known to have very poor mixing properties when sampling multimodal distributions. The Equi-Energy sampler is an interacting MCMC sampler proposed by Kou, Zhou and Wong in 2006 to sample difficult multimodal distributions. This algorithm runs several chains at different temperatures in parallel, and allow lower-tempered chains to jump to a state from a higher-tempered chain having an energy “close” to that of the current state. A major drawback of this algorithm is that it depends on many design parameters and thus, requires a significant effort to tune these parameters. In this article, we introduce an Adaptive Equi-Energy (AEE) sampler that automates the choice of the selection mecanism when jumping onto a state of the higher-temperature chain. We prove the ergodicity and a strong law of large numbers for AEE, and for the original Equi-Energy sampler as well. Finally, we apply our algorithm to motif sampling in DNA sequences.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W1984978414",
    "type": "article"
  },
  {
    "title": "ParTejas",
    "doi": "https://doi.org/10.1145/3077582",
    "publication_date": "2017-07-31",
    "publication_year": 2017,
    "authors": "Geetika Malhotra; Rajshekar Kalayappan; Seep Goel; Pooja Aggarwal; Abhishek Sagar; Smruti R. Sarangi",
    "corresponding_authors": "",
    "abstract": "In this article, we present the design of a novel parallel architecture simulator called ParTejas . ParTejas is a timing simulation engine that gets its execution traces from instrumented binaries using a fast shared-memory-based mechanism. Subsequently, the waiting threads simulate the execution of multiple pipelines and an elaborate memory system with support for multilevel coherent caches. ParTejas is written in Java and primarily derives its speedups from the use of novel data structures. Specifically, it uses lock-free slot schedulers to design an entity called a parallel port that effectively models the contention at shared resources in the CPU and memory system. Parallel ports remove the need for fine-grained synchronization and allow each thread to use its local clock. Unlike conventional simulators that use barriers for synchronization at epoch boundaries, we use a sophisticated type of barrier, known as a phaser. A phaser allows threads to perform additional work without waiting for other threads to arrive at the barrier. Additionally, we use a host of Java-specific optimizations and use profiling to effectively schedule the threads. With all our optimizations, we demonstrate a speedup of 11.8× for a multi-issue in-order pipeline and 10.9× for an out-of-order pipeline with 64 threads, for a suite of seven Splash2 and Parsec benchmarks. The simulation error is limited to 2% to 4% as compared to strictly sequential simulation",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2740645752",
    "type": "article"
  },
  {
    "title": "Moving Least Squares Regression for High-Dimensional Stochastic Simulation Metamodeling",
    "doi": "https://doi.org/10.1145/2724708",
    "publication_date": "2016-01-09",
    "publication_year": 2016,
    "authors": "Peter Salemi; Barry L. Nelson; Jeremy Staum",
    "corresponding_authors": "",
    "abstract": "Simulation metamodeling is building a statistical model based on simulation output as an approximation to the system performance measure being estimated by the simulation model. In high-dimensional metamodeling problems, larger numbers of design points are needed to build an accurate and precise metamodel. Metamodeling techniques that are functions of all of these design points experience difficulties because of numerical instabilities and high computation times. We introduce a procedure to implement a local smoothing method called Moving Least Squares (MLS) regression in high-dimensional stochastic simulation metamodeling problems. Although MLS regression is known to work well when there are a very large number of design points, current procedures are focused on two- and three-dimensional cases. Furthermore, our procedure accounts for the fact that we can make replications and control the placement of design points in stochastic simulation. We provide a bound on the expected approximation error, show that the MLS predictor is consistent under certain conditions, and test the procedure with two examples that demonstrate better results than other existing simulation metamodeling techniques.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2223195372",
    "type": "article"
  },
  {
    "title": "Dynamic Data-driven Microscopic Traffic Simulation using Jointly Trained Physics-guided Long Short-Term Memory",
    "doi": "https://doi.org/10.1145/3558555",
    "publication_date": "2022-09-06",
    "publication_year": 2022,
    "authors": "Htet Naing; Wentong Cai; Nan Hu; Tiantian Wu; Liang Yu",
    "corresponding_authors": "",
    "abstract": "Symbiotic simulation systems that incorporate data-driven methods (such as machine/deep learning) are effective and efficient tools for just-in-time (JIT) operational decision making. With the growing interest on Digital Twin City, such systems are ideal for real-time microscopic traffic simulation. However, learning-based models are heavily biased towards the training data and could produce physically inconsistent outputs. In terms of microscopic traffic simulation, this could lead to unsafe driving behaviours causing vehicle collisions in the simulation. As for symbiotic simulation, this could severely affect the performance of real-time base simulation models resulting in inaccurate or unrealistic forecasts, which could, in turn, mislead JIT what-if analyses. To overcome this issue, a physics-guided data-driven modelling paradigm should be adopted so that the resulting model could capture both accurate and safe driving behaviours. However, very few works exist in the development of such a car-following model that can balance between simulation accuracy and physical consistency. Therefore, in this paper, a new “jointly-trained physics-guided Long Short-Term Memory (JTPG-LSTM)” neural network, is proposed and integrated to a dynamic data-driven simulation system to capture dynamic car-following behaviours. An extensive set of experiments was conducted to demonstrate the advantages of the proposed model from both modelling and simulation perspectives.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4294733434",
    "type": "article"
  },
  {
    "title": "Virtual Time III, Part 1: Unified Virtual Time Synchronization for Parallel Discrete Event Simulation",
    "doi": "https://doi.org/10.1145/3505248",
    "publication_date": "2022-09-23",
    "publication_year": 2022,
    "authors": "David Jefferson; P. D. Barnes",
    "corresponding_authors": "",
    "abstract": "Algorithms for synchronization of parallel discrete event simulation have historically been divided between conservative methods that require lookahead but not rollback, and optimistic methods that require rollback but not lookahead. In this paper we present a new approach in the form of a framework called Unified Virtual Time (UVT) that unifies the two approaches, combining the advantages of both within a single synchronization theory. Whenever timely lookahead information is available, a logical process (LP) executes conservatively using an irreversible event handler. When lookahead information is not available the LP does not block, as it would in a classical conservative execution, but instead executes optimistically using a reversible event handler. The switch from conservative to optimistic synchronization and back is decided on an event-by-event basis by the simulator, transparently to the model code. UVT treats conservative synchronization algorithms as optional accelerators for an underlying optimistic synchronization algorithm, enabling the speed of conservative execution whenever it is applicable, but otherwise falling back on the generality of optimistic execution. We describe UVT in a novel way, based on fundamental invariants, monotonicity requirements, and synchronization rules. UVT permits zero-delay messages and pays careful attention to tie-handling using superposition. We prove that under fairly general conditions a UVT simulation always makes progress in virtual time. This is Part 1 of a trio of papers describing the UVT framework for PDES, mixing conservative and optimistic synchronization and integrating throttling control.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W4296996780",
    "type": "article"
  },
  {
    "title": "Ideas towards Model Families for Multi-Criteria Decision Support: A COVID-19 Case Study",
    "doi": "https://doi.org/10.1145/3722217",
    "publication_date": "2025-03-07",
    "publication_year": 2025,
    "authors": "Martin Bicher; Claire Rippinger; Christoph Urach; Dominik Brunmeir; Melanie Zechmeister; Niki Popper",
    "corresponding_authors": "",
    "abstract": "Continued model-based decision support is associated with particular challenges, especially in long-term projects. Due to the regularly changing questions and the often changing understanding of the underlying system, the models used must be regularly re-evaluated, -modelled and -implemented with respect to changing modelling purpose, system boundaries and mapped causalities. Usually, this leads to models with continuously growing complexity and volume. In this work we aim to reevaluate the idea of the model family, dating back to the 1990s, and use it to promote this as a mindset in the creation of decision support frameworks in large research projects. The idea is to generally not develop and enhance a single standalone model, but to divide the research tasks into interacting smaller models which specifically correspond to the research question. This strategy comes with many advantages, which we explain using the example of a family of models for decision support in the COVID-19 crisis and corresponding case studies. We describe the individual models, explain their role within the family, and how they are used – individually and with each other.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408169825",
    "type": "article"
  },
  {
    "title": "Verifier's Dilemma in Proof-of-Work Public Blockchains: A Quantitative Analysis",
    "doi": "https://doi.org/10.1145/3723874",
    "publication_date": "2025-03-15",
    "publication_year": 2025,
    "authors": "Daria Smuseva; Andrea Marin; Sabina Rossi; Aad van Moorsel",
    "corresponding_authors": "",
    "abstract": "A blockchain is an immutable ledger driven by a distributed consensus protocol. In public blockchains, such as Bitcoin and Ethereum Classic, consensus is established through a computational effort called Proof-of-Work (PoW). Special users called miners contribute to the PoW in exchange for a fee and also verify the data stored in blocks mined by the other miners. Here is where the Verifier’s Dilemma emerges. Verification of blocks does not receive a reward, and to maximise their profits, miners may be incentivised to forego verifying blocks and to only invest their resources in PoW. In this paper, we study the Verifier’s Dilemma and a possible countermeasure consisting in the injection of invalid blocks using a quantitative model based on a Markovian process algebra. To avoid the state space explosion problem, we study the underlying Markov chain by using a lumping that allows us to derive closed-form solutions for interesting performance indices. The analysis demonstrates the circumstances under which non-verifying miners gain fees higher than those of verifying miners. The model also allows us to derive the optimal rate at which invalid blocks must be injected, so that skipping the verifying phase becomes economically disadvantageous while the throughput of the blockchain is only minimally reduced. The impact on miners’ rewards and overall performance is also assessed.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4408489133",
    "type": "article"
  },
  {
    "title": "NoCDAS: A Cycle-Accurate NoC-Based Deep Neural Network Accelerator Simulator",
    "doi": "https://doi.org/10.1145/3729169",
    "publication_date": "2025-04-10",
    "publication_year": 2025,
    "authors": "Wenyao Zhu; Yizhi Chen; Zhonghai Lu",
    "corresponding_authors": "",
    "abstract": "Network-on-Chip (NoC) has been widely adopted for Deep Neural Network (DNN) accelerator designs to solve the data communication problem for the large-scale processing element array. As the complexity of these DNN accelerators grows significantly, effective design-space exploration before hardware prototyping becomes crucial. However, the existing simulation tools for NoC-based DNN accelerators are limited in providing accurate hardware microarchitecture representations and execution time analysis. In this paper, we present a cycle-accurate simulator for NoC-based DNN accelerators called NoCDAS. The proposed NoCDAS can accurately simulate DNN computation flow on NoC hardware and the correctness of inference output is validated. In addition, NoCDAS supports highly flexible NoC hardware definitions to quantify the end-to-end latency, which allows efficient evaluation of different NoC-based DNN accelerator design parameters. We showcase this ability by running three DNN models on the proposed NoCDAS with various configurations encompassing NoC size, mapping strategy, and core placement.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409336674",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on QEST 2022, Part 2",
    "doi": "https://doi.org/10.1145/3724078",
    "publication_date": "2025-04-11",
    "publication_year": 2025,
    "authors": "Erika Ábrahám; Marco Paolieri",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409361748",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on PADS 2023",
    "doi": "https://doi.org/10.1145/3725782",
    "publication_date": "2025-04-11",
    "publication_year": 2025,
    "authors": "Dong Jin; Christopher D. Carothers",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4409362075",
    "type": "article"
  },
  {
    "title": "Reproducibility Report for the Paper: \"A Toolset for Predicting Performance of Legacy Real-Time Software Based on the RAST Approach\"",
    "doi": "https://doi.org/10.1145/3715113",
    "publication_date": "2025-05-17",
    "publication_year": 2025,
    "authors": "Emilio Incerto; Romolo Marotta",
    "corresponding_authors": "",
    "abstract": "The examined paper introduces a tool for predicting performance of legacy real-time software based on the RAST Approach. The artifact evaluated in this report is well documented and allows to easily reproduce the computational results presented in the article. Additionally, the artifact is hosted on permanent repositories, ensuring a long-term retention. This paper can thus receive the Artifacts Available , Artifacts Evaluated–Functional , and Artifact Validated–Results Reproduced badges.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410463312",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on PADS 2024",
    "doi": "https://doi.org/10.1145/3716824",
    "publication_date": "2025-05-17",
    "publication_year": 2025,
    "authors": "Alessandro Pellegrini",
    "corresponding_authors": "Alessandro Pellegrini",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410463386",
    "type": "article"
  },
  {
    "title": "Zero-Delay Cycles in Distributed Discrete-Event Systems using Lingua Franca",
    "doi": "https://doi.org/10.1145/3767727",
    "publication_date": "2025-09-16",
    "publication_year": 2025,
    "authors": "Peter Donovan; Erling Jellum; Byeonggil Jun; Hokeun Kim; Edward A. Lee; Shaokai Lin; Marten Lohstroh; Anirudh Rengarajan",
    "corresponding_authors": "",
    "abstract": "Discrete-event (DE) systems are concurrent programs where components communicate via tagged events, where tags are drawn from a totally ordered set. Distributed DE (DDE) systems are DE systems where the components (reactors) communicate over networks. Most execution platforms require that for DDE systems with cycles, each cycle must contain at least one logical delay, where the tag of events is incremented. Some impose an even stronger constraint, that no component produce outputs with the same timestamp as a triggering input (the “lookahead” for the component must be greater than zero). Such restrictions, however, are not required by the elegant fixed-point semantics of DE. The only fundamental requirement is that the program be constructive, meaning it is free from causality cycles. In this paper, we propose a way to coordinate the execution of DDE systems that can execute any constructive program, even one with zero-delay cycles (ZDC), facilitating the elegant programming of strongly consistent distributed real-time systems. The proposed coordination provides a formal model that exposes exactly the information that must be shared across networks for such execution to be possible. Our solution avoids speculative execution and rollback, making it suitable for situations that do not tolerate rollback, such as deployment (vs. simulation ) of cyber-physical systems (CPS’s). We describe an extension to the coordination mechanisms in Lingua Franca, a recent DE-based coordination language, to support ZDC.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414243991",
    "type": "article"
  },
  {
    "title": "RCR Report for the Paper : “Using (Not-so) Large Language Models to Generate Simulation Models in a Formal DSL: A Study on Reaction Networks”",
    "doi": "https://doi.org/10.1145/3766897",
    "publication_date": "2025-09-16",
    "publication_year": 2025,
    "authors": "Federica Montesano",
    "corresponding_authors": "Federica Montesano",
    "abstract": "The artifact evaluated in this report is relevant to the paper “Using (Not-so) Large Language Models to Generate Simulation Models in a Formal DSL: A Study on Reaction Networks”. The authors provided the code residing on a permanent repository, the instructions for building and executing the artifact are well-documented, as well as the dependencies needed and possible issues encountered during the reproducibility process. The process of running the experiments and generating data, plots and table terminates correctly. The results could be reproduced. The paper receives the badges Artifacts Available , Artifacts Evaluated—Reusable and Results Validated - Results Reproduced .",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414267818",
    "type": "article"
  },
  {
    "title": "Towards Standardizing Validation Practices in Agent-Based Modeling: A Hierarchical ABM Validation Framework",
    "doi": "https://doi.org/10.1145/3769857",
    "publication_date": "2025-09-27",
    "publication_year": 2025,
    "authors": "Zhou He; Qi Song; Junchao Lian; Y. Liu",
    "corresponding_authors": "",
    "abstract": "ABM (Agent-Based Modeling) has gained significant traction due to its ability to represent diverse agents’ behaviors and interactions accurately. However, ensuring the reliability and widespread acceptance of ABM necessitates rigorous validation. Unfortunately, existing literature often lacks integration of representative validation methods into a cohesive framework, hindering standardized validation practices. This study aims to propose a comprehensive and pragmatic validation framework. Initially, we clarify three fundamental concepts: calibration, verification, and validation. Subsequently, we review 17 distinct validation approaches and categorize them based on their data requirements and suitability for various simulation methodologies. Aligned with the ABM procedures, we introduce a Hierarchical ABM Validation (HAV) framework structured across three tiers: agent level, model level, and output level. Each tier recommends appropriate validation methods contingent upon data availability, enhancing the HAV’s applicability across diverse modeling scenarios. Finally, we develop an accessible Python code package (hav) and provide two examples of validating a traffic model and a wealth model. These cases exemplify the HAV’s implementation and underscore its efficacy in promoting standardized validation practices within ABM research.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414575041",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on PADS 2025",
    "doi": "https://doi.org/10.1145/3766909",
    "publication_date": "2025-10-08",
    "publication_year": 2025,
    "authors": "Adelinde M. Uhrmacher; Ernest H. Page",
    "corresponding_authors": "",
    "abstract": "Semantic heterogeneity is one of the key challenges in integrating and sharing data across disparate sources, data exchange and migration, data warehousing, model management, the Semantic Web and peer-to-peer databases. Semantic heterogeneity can arise ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4414968118",
    "type": "article"
  },
  {
    "title": "Computational issues for accessibility in discrete event simulation",
    "doi": "https://doi.org/10.1145/229493.229509",
    "publication_date": "1996-01-01",
    "publication_year": 1996,
    "authors": "Enver Yücesan; Sheldon H. Jacobson",
    "corresponding_authors": "",
    "abstract": "Several simulation model building and analysis issues have been studied using a computational complexity approach. More specifically, four problems related to simulation model building and analysis (accessibility of states, ordering of events, noninterchangeability of model implementations, and execution stalling) have been shown to be NP-hard search problems. These results imply that it is unlikely that a polynomial-time algorithm can be devised to verify structural properties of discrete event simulation models, unless P = NP. This article considers the problem of accessibility , identifies special cases that are polynomially solvable or remain NP-hard, and discusses implications with respect to the other three problems. A local search procedure and variations of simulated annealing are presented to address accessibility . Computational results illustrate these heuristics and demonstrate their strengths limitations.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W1977791712",
    "type": "article"
  },
  {
    "title": "Strong deviations from randomness in <i>m</i> -sequences based on trinomials",
    "doi": "https://doi.org/10.1145/232807.232815",
    "publication_date": "1996-04-01",
    "publication_year": 1996,
    "authors": "Makoto Matsumoto; Yoshiharu Kurita",
    "corresponding_authors": "",
    "abstract": "The fixed vector of any m -sequence based on a trinomial is explicitly obtained. Local nonrandomness around the fixed vector is analyzed through model-construction and experiments. We conclude that the initial vector near the fixed vector should be avoided.",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W1985016901",
    "type": "article"
  },
  {
    "title": "Active memory",
    "doi": "https://doi.org/10.1145/244804.244806",
    "publication_date": "1997-01-01",
    "publication_year": 1997,
    "authors": "Alvin R. Lebeck; David A. Wood",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Active memory: a new abstraction for memory system simulation Authors: Alvin R. Lebeck Duke Univ. Duke Univ.View Profile , David A. Wood Univ. of Wisconsin, Madison Univ. of Wisconsin, MadisonView Profile Authors Info & Claims ACM Transactions on Modeling and Computer SimulationVolume 7Issue 1Jan. 1997 pp 42–77https://doi.org/10.1145/244804.244806Published:01 January 1997Publication History 21citation470DownloadsMetricsTotal Citations21Total Downloads470Last 12 Months8Last 6 weeks4 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 26,
    "openalex_id": "https://openalex.org/W2080329299",
    "type": "article"
  },
  {
    "title": "Analysis of an efficient algorithm for the hard-sphere problem",
    "doi": "https://doi.org/10.1145/235025.235030",
    "publication_date": "1996-07-01",
    "publication_year": 1996,
    "authors": "Alan T. Krantz",
    "corresponding_authors": "Alan T. Krantz",
    "abstract": "Many similar algorithms for performing simulations of hard-sphere have been presented. Among these algorithms are those designed by Rapaport (RAP), Lubachevsky (LUB), Krantz (HAD), and Marin (HYBRID). These algorithms exhibit a similar design in that they each use an O (log n ) event queue which becomes the overwhelming bottleneck when simulating large systems. In this paper the design of HAD is presented and contrasted to RAP, LUB and HYBRID. Next, both an empirical and analytic analysis of HAD's performance are presented which show that HAD scales well. Finally, using the design differences of these algorithms, the performance of HAD is compared to RAP, LUB and HYBRID.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W1980849854",
    "type": "article"
  },
  {
    "title": "The patchwork rejection technique for sampling from unimodal distributions",
    "doi": "https://doi.org/10.1145/301677.301685",
    "publication_date": "1999-01-01",
    "publication_year": 1999,
    "authors": "Ernst Stadlober; Heinz Zechner",
    "corresponding_authors": "",
    "abstract": "We report on both theoretical developments and comutational experience with the patchwork rejection technique in Zechner and Stadlober [1993] and Zechner [1997]. The basic approach is due to Minh [1988], who suggested a special sampling method for the gamma distribution. The method's general objective is to rearrange the area below the density of histogram f ( x ) in the body of the distribution by certain point reflections such that variates may be generated efficiently within a large center interval. This is carried out via uniform hat functions, combined with minorizing rectangles for immediate acceptance of one transformed uniform deviate. The remaining tails of f ( x ) are covered by exponential functions. Experiments show that patchwork rejection algorithms are in general faster than their competitors at the cost of higher set-up times.",
    "cited_by_count": 25,
    "openalex_id": "https://openalex.org/W2082372905",
    "type": "article"
  },
  {
    "title": "Robust multiple comparisons under common random numbers",
    "doi": "https://doi.org/10.1145/174153.174158",
    "publication_date": "1993-07-01",
    "publication_year": 1993,
    "authors": "Barry L. Nelson",
    "corresponding_authors": "Barry L. Nelson",
    "abstract": "article Free Access Share on Robust multiple comparisons under common random numbers Author: Barry L. Nelson Ohio State Univ., Columbus Ohio State Univ., ColumbusView Profile Authors Info & Claims ACM Transactions on Modeling and Computer SimulationVolume 3Issue 3July 1993 pp 225–243https://doi.org/10.1145/174153.174158Online:01 July 1993Publication History 18citation346DownloadsMetricsTotal Citations18Total Downloads346Last 12 Months7Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2110077636",
    "type": "article"
  },
  {
    "title": "Efficient multiply-with-carry random number generators with maximal period",
    "doi": "https://doi.org/10.1145/945511.945514",
    "publication_date": "2003-10-01",
    "publication_year": 2003,
    "authors": "Mark Goresky; Andrew Klapper",
    "corresponding_authors": "",
    "abstract": "In this (largely expository) article, we propose a simple modification of the multiply-with-carry random number generators of Marsaglia [1994] and Couture and L'Écuyer [1997]. The resulting generators are both efficient (since they may be configured with a base b which is a power of 2) and exhibit maximal period. These generators are analyzed using a simple but powerful algebraic technique involving b -adic numbers.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2152673505",
    "type": "article"
  },
  {
    "title": "Performance evaluation of multiple time scale TCP under self-similar traffic conditions",
    "doi": "https://doi.org/10.1145/364996.365005",
    "publication_date": "2000-04-01",
    "publication_year": 2000,
    "authors": "Ki‐Hong Park; Tsunyi Tuan",
    "corresponding_authors": "",
    "abstract": "Measurements of network traffic have shown that self-similarity is a ubiquitous phenomenon spanning across diverse network environments. In previous work, we have explored the feasibility of exploiting long-range correlation structure in self-similar traffic for congestion control. We have advanced the framework of multiple time scale congestion control and shown its effectiveness at enhancing performance for rate-based feedback control. In this article, we extend the multiple time scale control framework to window-based congestion control, in particular, TCP. This is performed by interfacing TCP with a large time scale module that adjusts the aggressiveness of bandwidth consumpton behavior exhibited by TCP as a function of large time scale network state, that is, information that exceeds the time horizon of the feedback loop as determined by RTT. How to effectively utilize such information—due to its probabilistic nature, dispersion over multiple time scales, and realization on top of existing window-based congestion controls—is a nontrivial problem. First, we define a modular extension of TCP (a function call with a simple interface that applies to various flavors of TCP, e.g., Tahoe, Reno, and Vegas) and show that it significantly improves performance. Second, we show that multiple time scale TCP endows the underlying feedback control with proacativity by bridging the uncertainty gap associated with reactive controls which is exacerbated by the high delay-bandwidth product in broadband wide area networks. Third, we investigate the influence of three traffic control dimensions—tracking ability, connection duration, and fairness—on performance. Performance evaluation of multiple time scale TCP is facilitated by a simulation benchmark environment based on physical modeling of self-similar traffic. We explicate our methodology for disc",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W1989588625",
    "type": "article"
  },
  {
    "title": "A discrete event method for wave simulation",
    "doi": "https://doi.org/10.1145/1138464.1138468",
    "publication_date": "2006-04-01",
    "publication_year": 2006,
    "authors": "James Nutaro",
    "corresponding_authors": "James Nutaro",
    "abstract": "This article describes a discrete event interpretation of the finite difference time domain (FDTD) and digital wave guide network (DWN) wave simulation schemes. The discrete event method is formalized using the discrete event system specification (DEVS). The scheme is shown to have errors that are proportional to the resolution of the spatial grid. A numerical example demonstrates the relative efficiency of the scheme with respect to FDTD and DWN schemes. The potential for the discrete event scheme to reduce numerical dispersion and attenuation errors is discussed.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2160355169",
    "type": "article"
  },
  {
    "title": "How heavy-tailed distributions affect simulation-generated time averages",
    "doi": "https://doi.org/10.1145/1138464.1138467",
    "publication_date": "2006-04-01",
    "publication_year": 2006,
    "authors": "George S. Fishman; Ivo Adan",
    "corresponding_authors": "",
    "abstract": "For statistical inference based on telecommunications network simulation, we examine the effect of a heavy-tailed file-size distribution whose corresponding density follows an inverse power law with exponent α + 1, where the shape parameter α is strictly between 1 and 2. Representing the session-initiation and file-transmission processes as an infinite-server queueing system with Poisson arrivals, we derive the transient conditional mean and covariance function that describes the number of active sessions as well as the steady-state counterparts of these moments. Assuming the file size (service time) for each session follows the Lomax distribution, we show that the variance of the sample mean for the time-averaged number of active sessions tends to zero as the power of 1 − α of the simulation run length. Therefore, impractically large sample-path lengths are required to achieve point estimators with acceptable levels of statistical accuracy. This study compares the accuracy of point estimators based on the Lomax distribution with those for lognormal and Weibull file-size distributions whose parameters are determined by matching their means and a selected extreme quantile with those of the Lomax. Both alternatives require shorter run lengths than the Lomax to achieve a given level of accuracy. Although the lognormal requires longer sample paths than the Weibull, it better approximates the Lomax and leads to practicable run lengths in almost all scenarios.",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2168897623",
    "type": "article"
  },
  {
    "title": "A framework for the simulation of structural software evolution",
    "doi": "https://doi.org/10.1145/1391978.1391983",
    "publication_date": "2008-09-01",
    "publication_year": 2008,
    "authors": "Benjamin Stopford; Steve Counsell",
    "corresponding_authors": "",
    "abstract": "As functionality is added to an aging piece of software, its original design and structure will tend to erode. This can lead to high coupling, low cohesion and other undesirable effects associated with spaghetti architectures. The underlying forces that cause such degradation have been the subject of much research. However, progress in this field is slow, as its complexity makes it difficult to isolate the causal flows leading to these effects. This is further complicated by the difficulty of generating enough empirical data, in sufficient quantity, and attributing such data to specific points in the causal chain. This article describes a framework for simulating the structural evolution of software. A complete simulation model is built by incrementally adding modules to the framework, each of which contributes an individual evolutionary effect. These effects are then combined to form a multifaceted simulation that evolves a fictitious code base in a manner approximating real-world behavior. We describe the underlying principles and structures of our framework from a theoretical and user perspective; a validation of a simple set of evolutionary parameters is then provided and three empirical software studies generated from open-source software (OSS) are used to support claims and generated results. The research illustrates how simulation can be used to investigate a complex and under-researched area of the development cycle. It also shows the value of incorporating certain human traits into a simulation—factors that, in real-world system development, can significantly influence evolutionary structures.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2117682365",
    "type": "article"
  },
  {
    "title": "Finding probably best systems quickly via simulations",
    "doi": "https://doi.org/10.1145/1540530.1540533",
    "publication_date": "2009-06-01",
    "publication_year": 2009,
    "authors": "Takayuki Osogami",
    "corresponding_authors": "Takayuki Osogami",
    "abstract": "We propose an indifference-zone approach for a ranking and selection problem with the goal of reducing both the number of simulated samples of the performance and the frequency of configuration changes. We prove that with a prespecified high probability, our algorithm finds the best system configuration. Our proof hinges on several ideas, including the use of Anderson's probability bound, that have not been fully investigated for the ranking and selection problem. Numerical experiments show that our algorithm can select the best system configuration using up to 50% fewer simulated samples than existing algorithms without increasing the frequency of configuration changes.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W3138977666",
    "type": "article"
  },
  {
    "title": "A Holistic Approach for Collaborative Workload Execution in Volunteer Clouds",
    "doi": "https://doi.org/10.1145/3155336",
    "publication_date": "2018-03-09",
    "publication_year": 2018,
    "authors": "Stefano Sebastio; Michele Amoretti; Alberto Lluch Lafuente; Antonio Scala",
    "corresponding_authors": "",
    "abstract": "The demand for provisioning, using, and maintaining distributed computational resources is growing hand in hand with the quest for ubiquitous services. Centralized infrastructures such as cloud computing systems provide suitable solutions for many applications, but their scalability could be limited in some scenarios, such as in the case of latency-dependent applications. The volunteer cloud paradigm aims at overcoming this limitation by encouraging clients to offer their own spare, perhaps unused, computational resources. Volunteer clouds are thus complex, large-scale, dynamic systems that demand for self-adaptive capabilities to offer effective services, as well as modeling and analysis techniques to predict their behavior. In this article, we propose a novel holistic approach for volunteer clouds supporting collaborative task execution services able to improve the quality of service of compute-intensive workloads. We instantiate our approach by extending a recently proposed ant colony optimization algorithm for distributed task execution with a workload-based partitioning of the overlay network of the volunteer cloud. Finally, we evaluate our approach using simulation-based statistical analysis techniques on a workload benchmark provided by Google. Our results show that the proposed approach outperforms some traditional distributed task scheduling algorithms in the presence of compute-intensive workloads.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2791147593",
    "type": "article"
  },
  {
    "title": "A Role-Dependent Data-Driven Approach for High-Density Crowd Behavior Modeling",
    "doi": "https://doi.org/10.1145/3177776",
    "publication_date": "2018-09-17",
    "publication_year": 2018,
    "authors": "Mingbi Zhao; Jinghui Zhong; Wentong Cai",
    "corresponding_authors": "",
    "abstract": "In this article, we propose a role-dependent (RD) data-driven modeling approach to simulate pedestrians’ motion in high-density scenes. It is commonly observed that pedestrians behave quite differently when walking in a dense crowd. Some people explore routes toward their destinations. Meanwhile, some people deliberately follow others, leading to lane formation. Based on these observations, two roles are included in the proposed model: leader and follower. The motion behaviors of leader and follower are modeled separately. Leaders’ behaviors are learned from real crowd motion data using state-action pairs, while followers’ behaviors are calculated based on specific targets that are obtained dynamically during the simulation. The proposed RD model is trained and applied to different real-world datasets to evaluate its generality and effectiveness. The simulation results demonstrate that the RD model is capable of simulating crowd behaviors in crowded scenes realistically and reproducing collective crowd behaviors such as lane formation.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2892025844",
    "type": "article"
  },
  {
    "title": "Forwarding devices",
    "doi": "https://doi.org/10.1145/1899396.1899400",
    "publication_date": "2011-02-01",
    "publication_year": 2011,
    "authors": "Roman Chertov; Sonia Fahmy",
    "corresponding_authors": "",
    "abstract": "Most popular simulation and emulation tools use high-level models of forwarding behavior in switches and routers, and give little guidance on setting model parameters such as buffer sizes. Thus, a myriad of papers report results that are highly sensitive to the forwarding model or buffer size used. Incorrect conclusions are often drawn from these results about transport or application protocol performance, service provisioning, or vulnerability to attacks. In this article, we argue that measurement-based models for routers and other forwarding devices are necessary. We devise such a model and validate it with measurements from three types of Cisco routers and one Juniper router, under varying traffic conditions. The structure of our model is device-independent, but the model uses device specific parameters. The compactness of the parameters and simplicity of the model make it versatile for high-fidelity simulations that preserve simulation scalability. We construct a profiler to infer the parameters within a few hours. Our results indicate that our model approximates different types of routers significantly better than the default ns-2 simulator models. The results also indicate that queue characteristics vary dramatically among the devices we measure, and that backplane contention can be a factor.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W1995506584",
    "type": "article"
  },
  {
    "title": "Transparent optimistic synchronization in the high-level architecture via time-management conversion",
    "doi": "https://doi.org/10.1145/2379810.2379814",
    "publication_date": "2012-11-01",
    "publication_year": 2012,
    "authors": "Andrea Santoro; Francesco Quaglia",
    "corresponding_authors": "",
    "abstract": "Distributed simulation allows the treatment of large/complex models by having several interacting simulators running concurrently, each one in charge of a portion of the model. In order to effectively manage integration and interoperability aspects, the standard known as High Level Architecture (HLA) has been developed, which is based on a middleware component known as Run-Time-Infrastructure (RTI). One of the main issues faced by such a standard is synchronization, so that HLA supports both conservative and optimistic approaches. However, technical issues, combined with some peculiarities of the optimistic approach, force most simulators to use the conservative approach. In order to tackle these issues, we present the design and implementation of a Time Management Converter (TiMaC) for HLA based simulation systems. TiMaC is a state machine designed to be transparently interposed between the application layer and the underlying RTI, which performs mapping of the conservative HLA synchronization interface onto the optimistic one. Such a mapping allows transparent optimistic execution (and the related benefits) for simulators originally designed to rely on conservative synchronization. This is achieved without the need to modify the RTI services or alter the HLA standard. An experimental evaluation demonstrating the viability and effectiveness of our proposal is also reported, by integrating our TiMaC implementation with the Georgia Tech B-RTI package and running on it both (A) benchmarks relying on traces from simulated demonstration exercises collected using the Joint Semi-Automated Forces (JSAF) simulation program and (B) a self-federated Personal Communication System simulation application.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2014314650",
    "type": "article"
  },
  {
    "title": "Synchronised range queries in distributed simulations of multiagent systems",
    "doi": "https://doi.org/10.1145/2517449",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Vinoth Suryanarayanan; Georgios Theodoropoulos",
    "corresponding_authors": "",
    "abstract": "Range queries are an increasingly important associative form of data access encountered in different computational environments including peer-to-peer systems, wireless communications, database systems, distributed virtual environments, and, more recently, distributed simulations. In this article, we present and evaluate a system for performing logical-time synchronised Range-Queries over data in the context of distributed simulations of multiagent systems. This article presents algorithms performing instantaneous queries within an optimistic synchronisation framework and in the presence of dynamic migration of the simulation state. A quantitative evaluation of the effectiveness of the proposed algorithms under different conditions and for different benchmarks, including Boids , is also presented.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2038543535",
    "type": "article"
  },
  {
    "title": "Reusing Search Data in Ranking and Selection",
    "doi": "https://doi.org/10.1145/3170503",
    "publication_date": "2018-07-06",
    "publication_year": 2018,
    "authors": "David J. Eckman; Shane G. Henderson",
    "corresponding_authors": "",
    "abstract": "It is tempting to reuse replications taken during a simulation optimization search as input to a ranking-and-selection procedure. However, even when the random inputs used to generate replications are identically distributed and independent within and across systems, we show that for searches that use the observed performance of explored systems to identify new systems, the replications are conditionally dependent given the sequence of returned systems. Through simulation experiments, we demonstrate that reusing the replications taken during search in selection and subset-selection procedures can result in probabilities of correct and good selection well below the guaranteed levels. Based on these negative findings, we call into question the guarantees of established ranking-and-selection procedures that reuse search data. We also rigorously define guarantees for ranking-and-selection procedures after search and discuss how procedures that only provide guarantees in the preference zone are ill-suited to this setting.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2867490389",
    "type": "article"
  },
  {
    "title": "Efficient Simulation for Expectations over the Union of Half-Spaces",
    "doi": "https://doi.org/10.1145/3167969",
    "publication_date": "2018-07-13",
    "publication_year": 2018,
    "authors": "Dohyun Ahn; Kyoung-Kuk Kim",
    "corresponding_authors": "",
    "abstract": "We consider the problem of estimating expectations over the union of half-spaces. Such a problem arises in many applications such as option pricing and stochastic activity networks. More recent applications include systemic risk measurements of financial networks. Assuming that random variables follow a multivariate elliptical distribution, we develop a conditional Monte Carlo method and prove its asymptotic efficiencies. We then demonstrate the numerical performance of the proposed method in three different application areas.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2883519997",
    "type": "article"
  },
  {
    "title": "Characterizing per-application network traffic using entropy",
    "doi": "https://doi.org/10.1145/2457459.2457463",
    "publication_date": "2013-05-01",
    "publication_year": 2013,
    "authors": "Vladislav Petkov; Ram Rajagopal; Katia Obraczka",
    "corresponding_authors": "",
    "abstract": "The Internet has been evolving into a more heterogeneous internetwork with diverse new applications imposing more stringent bandwidth and QoS requirements. Already new applications such as YouTube, Hulu, and Netflix are consuming a large fraction of the total bandwidth. We argue that, in order to engineer future internets such that they can adequately cater to their increasingly diverse and complex set of applications while using resources efficiently, it is critical to be able to characterize the load that emerging and future applications place on the underlying network. In this article, we investigate entropy as a metric for characterizing per-flow network traffic complexity. While previous work has analyzed aggregated network traffic, we focus on studying isolated traffic flows. Per-application flow characterization caters to the need of network control functions such as traffic scheduling and admission control at the edges of the network. Such control functions necessitate differentiating network traffic on a per-application basis. The “entropy fingerprints” that we get from our entropy estimator summarize many characteristics of each application's network traffic. Not only can we compare applications on the basis of peak entropy, but we can also categorize them based on a number of other properties of the fingerprints.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2153818479",
    "type": "article"
  },
  {
    "title": "State-space Construction of Hybrid Petri Nets with Multiple Stochastic Firings",
    "doi": "https://doi.org/10.1145/3449353",
    "publication_date": "2021-07-31",
    "publication_year": 2021,
    "authors": "Jannik Hüls; Carina Pilch; Patricia Schinke; Henner Niehaus; Joanna Delicaris; Anne Remke",
    "corresponding_authors": "",
    "abstract": "Hybrid Petri nets have been extended to include general transitions that fire after a randomly distributed amount of time. With a single general one-shot transition the state space and evolution over time can be represented either as a Parametric Location Tree or as a Stochastic Time Diagram . Recent work has shown that both representations can be combined and then allow multiple stochastic firings. This work presents an algorithm for building the Parametric Location Tree with multiple general transition firings and shows how its transient probability distribution can be computed using multi-dimensional integration. We discuss the (dis-)advantages of an interval arithmetic and a geometric approach to compute the areas of integration. Furthermore, we provide details on how to perform a Monte Carlo integration either directly on these intervals or convex polytopes, or after transformation to standard simplices. A case study on a battery-backup system shows the feasibility of the approach and discusses the performance of the different integration approaches.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2972067279",
    "type": "article"
  },
  {
    "title": "A Modest Approach to Markov Automata",
    "doi": "https://doi.org/10.1145/3449355",
    "publication_date": "2021-07-31",
    "publication_year": 2021,
    "authors": "Yuliya Butkova; Arnd Hartmanns; Holger Hermanns",
    "corresponding_authors": "",
    "abstract": "Markov automata are a compositional modelling formalism with continuous stochastic time, discrete probabilities, and nondeterministic choices. In this article, we present extensions to M ODEST , an expressive high-level language with roots in process algebra, that allow large Markov automata models to be specified in a succinct, modular way. We illustrate the advantages of M ODEST over alternative languages. Model checking Markov automata models requires dedicated algorithms for time-bounded and long-run average reward properties. We describe and evaluate the state-of-the-art algorithms implemented in the mcsta model checker of the M ODEST T OOLSET . We find that mcsta improves the performance and scalability of Markov automata model checking compared to earlier and alternative tools. We explain a partial-exploration approach based on the BRTDP method designed to mitigate the state space explosion problem of model checking, and experimentally evaluate its effectiveness. This problem can be avoided entirely by purely simulation-based techniques, but the nondeterminism in Markov automata hinders their straightforward application. We explain how lightweight scheduler sampling can make simulation possible, and provide a detailed evaluation of its usefulness on several benchmarks using the M ODEST T OOLSET ’s modes simulator.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W3194650806",
    "type": "article"
  },
  {
    "title": "Towards Differentiable Agent-Based Simulation",
    "doi": "https://doi.org/10.1145/3565810",
    "publication_date": "2022-10-06",
    "publication_year": 2022,
    "authors": "Philipp Andelfinger",
    "corresponding_authors": "Philipp Andelfinger",
    "abstract": "Simulation-based optimization using agent-based models is typically carried out under the assumption that the gradient describing the sensitivity of the simulation output to the input cannot be evaluated directly. To still apply gradient-based optimization methods, which efficiently steer the optimization towards a local optimum, gradient estimation methods can be employed. However, many simulation runs are needed to obtain accurate estimates if the input dimension is large. Automatic differentiation (AD) is a family of techniques to compute gradients of general programs directly. Here, we explore the use of AD in the context of time-driven agent-based simulations. By substituting common discrete model elements such as conditional branching with smooth approximations, we obtain gradient information across discontinuities in the model logic. On the examples of a synthetic grid-based model, an epidemics model, and a microscopic traffic model, we study the fidelity and overhead of the differentiable simulations as well as the convergence speed and solution quality achieved by gradient-based optimization compared with gradient-free methods. In traffic signal timing optimization problems with high input dimension, the gradient-based methods exhibit substantially superior performance. A further increase in optimization progress is achieved by combining gradient-free and gradient-based methods. We demonstrate that the approach enables gradient-based training of neural network-controlled simulation entities embedded in the model logic. Finally, we show that the performance overhead of differentiable agent-based simulations can be reduced substantially by exploiting sparsity in the model logic.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W4303185468",
    "type": "article"
  },
  {
    "title": "Knowledge Equivalence in Digital Twins of Intelligent Systems",
    "doi": "https://doi.org/10.1145/3635306",
    "publication_date": "2023-12-05",
    "publication_year": 2023,
    "authors": "Nan Zhang; Rami Bahsoon; Nikos Tziritas; Georgios Theodoropoulos",
    "corresponding_authors": "",
    "abstract": "A digital twin contains up-to-date data-driven models of the physical world being studied and can use simulation to optimise the physical world. However, the analysis made by the digital twin is valid and reliable only when the model is equivalent to the physical world. Maintaining such an equivalent model is challenging, especially when the physical systems being modelled are intelligent and autonomous. The article focuses in particular on digital twin models of intelligent systems where the systems are knowledge-aware but with limited capability. The digital twin improves the acting of the physical system at a meta-level by accumulating more knowledge in the simulated environment. The modelling of such an intelligent physical system requires replicating the knowledge-awareness capability in the virtual space. Novel equivalence maintaining techniques are needed, especially in synchronising the knowledge between the model and the physical system. This article proposes the notion of knowledge equivalence and an equivalence maintaining approach by knowledge comparison and updates. A quantitative analysis of the proposed approach confirms that compared to state equivalence, knowledge equivalence maintenance can tolerate deviation thus reducing unnecessary updates and achieve more Pareto efficient solutions for the tradeoff between update overhead and simulation reliability.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4389337801",
    "type": "article"
  },
  {
    "title": "Supporting Large-Scale Distributed Simulations Through the Topic-Based Data Distribution Management System",
    "doi": "https://doi.org/10.1145/3716823",
    "publication_date": "2025-02-11",
    "publication_year": 2025,
    "authors": "Alberto Falcone; Alfredo Garro",
    "corresponding_authors": "",
    "abstract": "Modeling and Simulation (M&amp;S) is a fundamental technology to design and study complex systems in various industrial and scientific domains when real-world testing is impractical due to costs, safety concerns, and time constraints. To promote the reusability and interoperability of simulation entities, allowing them to interoperate without geographic barriers, distributed simulation has been introduced. One of the most widely adopted standards for distributed simulation is the IEEE 1516-2010 - High Level Architecture (HLA). Among the services provided by HLA, a key one is the Data Distribution Management (DDM) which allows to reduce the transmission and reception of unnecessary data in order to improve communication effectiveness among simulation entities. This is done by computing the so-called “region matching”, i.e., the overlap between “update” and “subscription” regions. In this paper, a novel DDM system called Topic-based publish-subscribe Messaging System (TBMS) is proposed to improve the performance, reliability, and scalability of DDM services. Unlike traditional algorithms, this system revolves around the concept of “topic” that favors a contextualized approach to the match, avoiding the filtering of the information actually of interest on the receiver side. Experiments have been conducted to evaluate the proposed TBMS in terms of utilization and waiting time metrics.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4407390876",
    "type": "article"
  },
  {
    "title": "An Analysis of Three C/C++ Cluster-Computing Libraries for Agent-Based Models",
    "doi": "https://doi.org/10.1145/3732778",
    "publication_date": "2025-05-01",
    "publication_year": 2025,
    "authors": "Munehiro Fukuda; K. Wang; Sarah Panther; Norazizah Ibrahim Wong; Ian Dudder; Qingran Shao",
    "corresponding_authors": "",
    "abstract": "Agent-based modeling (ABM) is increasing its popularity and is applied to practical simulation where millions of agents need to interact with each other over a large-scale logical space. Cluster computing is an approach to accommodating ABM’s needs of both CPU and spatial scalability. This research compares three parallel ABM libraries such as FLAME, Repast HPC, and our MASS C++ libraries, all modeling simulation programs in C/C++ and running them in parallel over a cluster system. Our comparative work selects seven benchmark programs from social, behavioral, and economic sciences, biology, and urban planning; parallelizes them with each of these three libraries; analyzes their programmability through the parallelization; and measures their parallel performance. Our results reach two findings. The programmability of each ABM library has different pros and cons in metrics such as total lines of code (LoC), boilerplate percentages, agent/space modeling and management LoC, lack of cohesion of methods (LCOM), ease of agent synchronizations, and semantically smooth coding. Therefore, there is no all-in-one ABM library for best programming any application domains. However, ABM parallel executions are heavily affected by each library’s design principles. In particular, FLAME’s frequent file accesses and message broadcasts as well as Repast HPC’s central agent managements incur system overheads or bottlenecks. These performance drawbacks give MASS C++ an advantage in performing fastest and scaling up simulation most successfully.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4410004930",
    "type": "article"
  },
  {
    "title": "Modeling and Simulation for Behavioral Analysis in Healthcare: A Review",
    "doi": "https://doi.org/10.1145/3742428",
    "publication_date": "2025-06-04",
    "publication_year": 2025,
    "authors": "Athary Alwasel; Masoud Fakhimi; Navonil Mustafee; Lampros Stergioulas",
    "corresponding_authors": "",
    "abstract": "Healthcare is a service-based system where patients and providers (clinicians, managers) are integral to the system's overall functioning. As a human-centric system at both the demand and supply sides, it is important to consider the behavioral aspects of the entities that make up the healthcare system and their effect on system performance. Modeling and Simulation (M&amp;S) is widely used in healthcare to give stakeholders better insights into real-world problems; incorporating human behavior makes the models more realistic and leads to more informed decision-making. M&amp;S for behavioral analysis (BA) seeks to understand and explore individuals' behavior and reactions to different interventions within healthcare systems. We reviewed 59 papers published between 1992 and 2023 to synthesize existing literature and ascertain its current development. Our findings show that in most studies, the impact of human behavior is either ignored, studied unsystematically, or treated as a second-order effect. The study contributes to the understanding of M&amp;S methodologies for BA in healthcare.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411031263",
    "type": "review"
  },
  {
    "title": "Calibrating Non-Identifiable High-Dimensional Simulation Models: A Framework via Eligibility Set",
    "doi": "https://doi.org/10.1145/3742427",
    "publication_date": "2025-06-12",
    "publication_year": 2025,
    "authors": "Yuanlu Bai; Tucker Balch; Haoxian Chen; Danial Dervovic; Henry Lam; Svitlana Vyetrenko",
    "corresponding_authors": "",
    "abstract": "Stochastic simulation aims to compute output performance for complex models that lack analytical tractability. To ensure accurate prediction, the model needs to be calibrated and validated against real data. Conventional methods approach these tasks by assessing the model-data match via simple hypothesis tests or distance minimization in an ad hoc fashion, but they can encounter challenges arising from non-identifiability and high dimensionality. In this paper, we investigate a framework to develop calibration schemes that satisfies rigorous frequentist statistical guarantees, via a basic notion that we call eligibility set designed to bypass non-identifiability via a set-based estimation. We investigate a feature extraction-then-aggregation approach to construct these sets that target at multivariate outputs. We demonstrate our methodology on several numerical examples, including an application to calibration of a limit order book market simulator (ABIDES).",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4411292547",
    "type": "article"
  },
  {
    "title": "Redundancy in model specifications for discrete event simulation",
    "doi": "https://doi.org/10.1145/347823.347831",
    "publication_date": "1999-07-01",
    "publication_year": 1999,
    "authors": "Richard E. Nance; C. Michael Overstreet; Ernest H. Page",
    "corresponding_authors": "",
    "abstract": "Although redundancy in model specification generally has negative connotations, we offer arguments for revising those convictions. Defining “representational redundancy” as the inclusion of any symbols not required to fulfill the study objectives, we cite several sources of redundancy, classified as accidental or intentional, that contribute positively to the model development tasks. Comparative benefits and detriments are discussed briefly. Focusing on the most interesting source of redundancy‐that which is intentionally induced by a modeling methodology—we demonstrate that automated elimination of redundancy can actually improve model execution time. Using four models drawn from the literature that are easily understood, but which represent some differences in size and complexity, the direct graphical representations shows improvements over a base case ranging from 27.3 percent to 68.1 percent in execution time. Further, increasing improvement is realized with increasing model size and complexity. These results are encouraging because they suggest that modeling methodologies with automated model diagnosis can significantly reduce both execution and developments time and cost.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2004555795",
    "type": "article"
  },
  {
    "title": "Bootstrap confidence intervals for ratios of expectations",
    "doi": "https://doi.org/10.1145/352222.352224",
    "publication_date": "1999-10-01",
    "publication_year": 1999,
    "authors": "Denis Choquet; Pierre L’Ecuyer; Christian Léger",
    "corresponding_authors": "",
    "abstract": "We are concerned with computing a confidence interval for the ratio E [ Y ]/ E [ X , where ( X,Y ) is a pair of random variables. This ratio estimation problem arises in, for instance, regenerative simulation. As an alternative to confidence intervals based on asymptotic normality, we study and compare different variants of the bootstrap for one-sided and two-sided intervals. We point out situations where these techniques provide confidence intervals with coverage much closer to the nominal value than do the classical methods.",
    "cited_by_count": 24,
    "openalex_id": "https://openalex.org/W2067876974",
    "type": "article"
  },
  {
    "title": "A note on polynomial arithmetic analogue of Halton sequences",
    "doi": "https://doi.org/10.1145/189443.189447",
    "publication_date": "1994-07-01",
    "publication_year": 1994,
    "authors": "Shu Tezuka; Takeshi Tokuyama",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on A note on polynomial arithmetic analogue of Halton sequences Authors: Shu Tezuka IBM Research, Tokyo Research Lab., Kanagawa, Japan IBM Research, Tokyo Research Lab., Kanagawa, JapanView Profile , Takeshi Tokuyama IBM Research, Tokyo Research Lab., Kanagawa, Japan IBM Research, Tokyo Research Lab., Kanagawa, JapanView Profile Authors Info & Claims ACM Transactions on Modeling and Computer SimulationVolume 4Issue 3July 1994 pp 279–284https://doi.org/10.1145/189443.189447Published:01 July 1994Publication History 17citation290DownloadsMetricsTotal Citations17Total Downloads290Last 12 Months1Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2012513033",
    "type": "article"
  },
  {
    "title": "Production and playback of human figure motion for visual simulation",
    "doi": "https://doi.org/10.1145/217853.217856",
    "publication_date": "1995-07-01",
    "publication_year": 1995,
    "authors": "John P. Granieri; Jonathan Crabtree; Norman I. Badler",
    "corresponding_authors": "",
    "abstract": "We describe a system for off-line production and real-time playback of motion for articulated human figures in 3D virtual environments. The key notion are (1) the logical storage of full-body motion in posture graphs, which provides a simple motion access method for playback, and (2) mapping the motions of high DOF figures to lower DOF figures using slaving to provide human models at several levels of detail, both in geometry and articulation, for later playback. We present our system in the context of a simple problem: animating human figures in a distributed simulation, using DIS protocols for communicating the human state information. We also discuss several related techniques for real-time animation of articulated figures in visual simulation.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2035127198",
    "type": "article"
  },
  {
    "title": "Generating triangulations at random",
    "doi": "https://doi.org/10.1145/189443.189446",
    "publication_date": "1994-07-01",
    "publication_year": 1994,
    "authors": "Peter Epstein; Jörg-Rüdiger Sack",
    "corresponding_authors": "",
    "abstract": "An O(n 3 ) algorithm is described to count triangulations of a simple polygon with n vertices. This algorithm is used to construct an O(n 4 ) algorithm to generate triangulations of a simple polygon at random with a uniform probability distribution. The problem of counting triangulations of a simple polygon is then related to existing problems in graph theory.",
    "cited_by_count": 23,
    "openalex_id": "https://openalex.org/W2082230637",
    "type": "article"
  },
  {
    "title": "Comparing the QoS of Internet audio mechanisms via formal methods",
    "doi": "https://doi.org/10.1145/379525.379526",
    "publication_date": "2001-01-01",
    "publication_year": 2001,
    "authors": "Alessandro Aldini; Marco Bernardo; Roberto Gorrieri; Marco Roccetti",
    "corresponding_authors": "",
    "abstract": "We compute and compare the quality of service (QoS) of three soft real-time applications for audio transmissions over the Internet. The main metric we want to capture is the average packet audio playout delay vs. the packet loss rate as perceived by users. Other metrics we take into account are the packet loss rate vs. the receiving buffer capacity, the lateness of discarded packets vs. average packet audio playout delay, and the waiting time in the receiver buffer for the played packets vs. the average packet audio playout delay. The study is conducted in the algebraic language EMPA, by way of formal descriptions of the three audio mechanisms. The mechanisms are analyzed via simulation using the software tool TwoTowers under various (experimentally obtained or randomly generated) traffic conditions. The stochastic process algebra EMPA is used because it compositionally supports system modeling, it allows functional properties of systems to be formally verified (unlike conventional simulatiors), and it represents generally distributed durations (which come into play in the three audio mechanisms). The comparison reveals that in general no one of the three mechanisms outperforms the other two, as their performance depends on the traffic conditions.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2004627178",
    "type": "article"
  },
  {
    "title": "Guest editors' introduction: special issue on uniform random number generation",
    "doi": "https://doi.org/10.1145/272991.275457",
    "publication_date": "1998-01-01",
    "publication_year": 1998,
    "authors": "Raymond Couture; Pierre L’Ecuyer",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2016507746",
    "type": "article"
  },
  {
    "title": "Packet delay in models of data networks",
    "doi": "https://doi.org/10.1145/502109.502110",
    "publication_date": "2001-07-01",
    "publication_year": 2001,
    "authors": "Henryk Fukś; Anna T. Ławniczak; Stanislav Volkov",
    "corresponding_authors": "",
    "abstract": "We investigate individual packet delay in a model of data networks with table-free, partial table and full table routing. We present analytical estimation for the average packet delay in a network with small partial routing table. Dependence of the delay on the size of the network and on the size of the partial routing table is examined numerically. Consequences for network scalability are discussed.",
    "cited_by_count": 22,
    "openalex_id": "https://openalex.org/W2117513650",
    "type": "article"
  },
  {
    "title": "Model representation with aesthetic computing",
    "doi": "https://doi.org/10.1145/1103323.1103327",
    "publication_date": "2005-07-01",
    "publication_year": 2005,
    "authors": "Paul A. Fishwick; Timothy A. Davis; Jane Douglas",
    "corresponding_authors": "",
    "abstract": "We define a new methodology, aesthetic computing , for customizing discrete structures found in mathematics, programming, and computer simulation. The methodology is presented as a procedure, defined within the context of the semantic web, involving a successive chain of model transformations from a source model to a target model , with the source model being the discrete structure to represent and the target model being the geometric model. We also present an implementation based on this methodology, and a class where empirical studies were performed to assess student perceptions on how customized model structures affected their understanding and preferences regarding visual and interactive model representations. Students appear to prefer the methodology as a way to inject creativity into model building, and to allow them to explore alternative, analogical representations influenced by the fields of aesthetics and the arts.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W1970517385",
    "type": "article"
  },
  {
    "title": "Efficient importance sampling heuristics for the simulation of population overflow in Jackson networks",
    "doi": "https://doi.org/10.1145/1225275.1225281",
    "publication_date": "2007-04-01",
    "publication_year": 2007,
    "authors": "Victor F. Nicola; Tatiana S. Zaburnenko",
    "corresponding_authors": "",
    "abstract": "In this article, we propose state-dependent importance sampling heuristics to estimate the probability of population overflow in Jackson queueing networks. These heuristics capture state-dependence along the boundaries (when one or more queues are empty), which is crucial for the asymptotic efficiency of the change of measure. The approach does not require difficult (and often intractable) mathematical analysis and is not limited by storage and computational requirements involved in adaptive importance sampling methodologies, particularly for a large state space. Experimental results on tandem, parallel, feed-forward, and feedback networks with a moderate number of nodes suggest that the proposed heuristics may yield asymptotically efficient estimators, possibly with bounded relative error, when applied to queueing networks wherein no other state-independent importance sampling techniques are known to be efficient. The heuristics are robust and remain effective for larger networks. Moreover, insights drawn from the basic networks considered in this article help understand sample path behavior along the boundaries, conditional on reaching the rare event of interest. This is key to the application of the methodology to networks of more general topologies. It is hoped that empirical findings and insights in this paper will encourage more research on related practical and theoretical issues.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2626153797",
    "type": "article"
  },
  {
    "title": "On the theoretical comparison of low-bias steady-state estimators",
    "doi": "https://doi.org/10.1145/1189756.1189760",
    "publication_date": "2007-01-01",
    "publication_year": 2007,
    "authors": "Hernan P. Awad; Peter W. Glynn",
    "corresponding_authors": "",
    "abstract": "The time-average estimator is typically biased in the context of steady-state simulation, and its bias is of order 1/ t , where t represents simulated time. Several “low-bias” estimators have been developed that have a lower order bias, and, to first-order, the same variance of the time-average. We argue that this kind of first-order comparison is insufficient, and that a second-order asymptotic expansion of the mean square error (MSE) of the estimators is needed. We provide such an expansion for the time-average estimator in both the Markov and regenerative settings. Additionally, we provide a full bias expansion and a second-order MSE expansion for the Meketon--Heidelberger low-bias estimator, and show that its MSE can be asymptotically higher or lower than that of the time-average depending on the problem. The situation is different in the context of parallel steady-state simulation, where a reduction in bias that leaves the first-order variance unaffected is arguably an improvement in performance.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2059277645",
    "type": "article"
  },
  {
    "title": "Perwez Shahabuddin,1962‐2005:職業的な評価",
    "doi": null,
    "publication_date": "2007-01-01",
    "publication_year": 2007,
    "authors": "Andradottir Sigrun; Glasserman Paul; W Glynn Peter; Heidelberger Philip; Sandeep Juneja",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2992563064",
    "type": "article"
  },
  {
    "title": "Mathematical programming models of closed tandem queueing networks",
    "doi": "https://doi.org/10.1145/1456645.1456648",
    "publication_date": "2008-12-01",
    "publication_year": 2008,
    "authors": "Wai Kin Victor Chan; Lee W. Schruben",
    "corresponding_authors": "",
    "abstract": "Closed tandem queueing networks are an important class of queueing models. Common approaches for analyzing these systems include Markov processes, renewal theory, and random walks. This article presents optimization models for sample paths of closed tandem queues. These mathematical models provide a new tool for analyzing these queueing systems using the techniques and algorithms from mathematical programming, and from graph theory in particular. We then apply operators from computer graphics (electronic picture manipulation) to graph theoretic representations of discrete-event system dynamics to establish some fundamental mathematical properties for these queueing systems.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2017386495",
    "type": "article"
  },
  {
    "title": "Performance of folded variance estimators for simulation",
    "doi": "https://doi.org/10.1145/1842713.1842714",
    "publication_date": "2010-09-01",
    "publication_year": 2010,
    "authors": "Christos Alexopoulos; Claudia Antonini; David Goldsman; Melike Meterelliyoz",
    "corresponding_authors": "",
    "abstract": "We extend and analyze a new class of estimators for the variance parameter of a steady-state simulation output process. These estimators are based on “folded” versions of the standardized time series (STS) of the process, and are analogous to the area and Cramér--von Mises estimators calculated from the original STS. In fact, one can apply the folding mechanism more than once to produce an entire class of estimators, all of which reuse the same underlying data stream. We show that these folded estimators share many of the same properties as their nonfolded counterparts, with the added bonus that they are often nearly independent of the nonfolded versions. In particular, we derive the asymptotic distributional properties of the various estimators as the run length increases, as well as their bias, variance, and mean squared error. We also study linear combinations of these estimators, and we show that such combinations yield estimators with lower variance than their constituents. Finally, we consider the consequences of batching, and we see that the batched versions of the new estimators compare favorably to benchmark estimators such as the nonoverlapping batch means estimator.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W1998442109",
    "type": "article"
  },
  {
    "title": "Generalized Lindley-type recursive representations for multiserver tandem queues with blocking",
    "doi": "https://doi.org/10.1145/1842722.1842726",
    "publication_date": "2010-10-01",
    "publication_year": 2010,
    "authors": "Wai Kin; Victor W. L. Chan",
    "corresponding_authors": "",
    "abstract": "Lindley's recursion is an explicit recursive equation that describes the recursive relationship between consecutive waiting times in a single-stage single-server queue. In this paper, we develop explicit recursive representations for multiserver tandem queues with blocking. We demonstrate the application of these recursive representations with simulations of large-scale tandem queueing networks. We compare the computational efficiency of these representations with two other popular discrete-event simulation approaches, namely, event scheduling and process interaction. Experimental results show that these representations are seven (or more) times faster than their counterparts based on the event-scheduling and process-interaction approaches.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2004434556",
    "type": "article"
  },
  {
    "title": "A survey of customization support in agent-based business process simulation tools",
    "doi": "https://doi.org/10.1145/1842713.1842717",
    "publication_date": "2010-09-01",
    "publication_year": 2010,
    "authors": "William N. Robinson; Yi Ding",
    "corresponding_authors": "",
    "abstract": "Agent-based business process simulation has grown in popularity, in part because of its analysis capabilities. The analyses depend on the kinds of simulations that can be built, adapted, and extended, which in turn depend on the underlying simulation framework. We report the results of our analysis of 19 agent-based process simulation tools and their simulation frameworks. We conclude that a growing number of simulation tools are using component-based software techniques. Nevertheless, most simulation tools do not directly support requirements models, their transformation into executable simulations, or the management of model variants over time. Such practices are becoming more widely applied in software engineering under the term software product line engineering (SPLE). Based on our analysis, agent-based process simulation tools may improve their customization capacity by: (1) supporting object modeling more completely and (2) supporting software product line engineering issues.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2066180948",
    "type": "article"
  },
  {
    "title": "A theoretical framework for interaction measure and sensitivity analysis in cross-layer design",
    "doi": "https://doi.org/10.1145/1870085.1870091",
    "publication_date": "2010-12-01",
    "publication_year": 2010,
    "authors": "Dalei Wu; Song Ci; Haiyan Luo; Hai-Feng Guo",
    "corresponding_authors": "",
    "abstract": "Cross-layer design has become one of the most effective and efficient methods to provide Quality of Service (QoS) over various communication networks, especially over wireless multimedia networks. However, current research on cross-layer design has been carried out in various piecemeal approaches, and lacks a methodological foundation to gain in-depth understanding of complex cross-layer behaviors such as multiscale temporal-spatial behavior, leading to a design paradox and/or unmanageable design problems. In this article, we propose a theoretical framework for quantitative interaction measures, which is further extended to sensitivity analysis by quantifying the contribution made by each design variable and by the interactions among them on the design objective. Thus, the proposed framework can significantly enhance our capability for cross-layer behavior characterization and provide design insights for future design. Furthermore, a case study on cross-layer optimized wireless multimedia communications has been adopted to illustrate major cross-layer design trade-offs and validate the proposed framework. Both analytical and experimental results show the correctness and effectiveness of the proposed framework.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2120189109",
    "type": "article"
  },
  {
    "title": "MNO--PQRS",
    "doi": "https://doi.org/10.1145/3067663",
    "publication_date": "2017-07-31",
    "publication_year": 2017,
    "authors": "Huifen Chen; Bruce W. Schmeiser",
    "corresponding_authors": "",
    "abstract": "In both cyclic and finite-horizon contexts, piecewise-constant rate functions are commonly encountered in models with nonhomogeneous Poisson processes. We develop an algorithm, with no user-specified parameters, that returns a smoother rate function that maintains the expected number of arrivals. The algorithm proceeds in two steps: PQRS (Piecewise-Quadratic Rate Smoothing) returns a continuous and differentiable piecewise-quadratic function without regard to negativity. If negative rates occur, then MNO (Max Nonnegativity Ordering) returns the maximum of zero and another piecewise-quadratic function. MNO maintains continuity of rates and first derivatives, but with some exceptions. Our analysis allows fitting the MNO--PQRS function to require storage complexity of the order of the number of intervals and computational complexity of the order of the number of intervals squared. MNO--PQRS can be used as a stand-alone routine, or as an endgame for the authors’ earlier algorithm, I-SMOOTH.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2741112706",
    "type": "article"
  },
  {
    "title": "Automated Estimation of Extreme Steady-State Quantiles via the Maximum Transformation",
    "doi": "https://doi.org/10.1145/3122864",
    "publication_date": "2017-10-31",
    "publication_year": 2017,
    "authors": "Christos Alexopoulos; David Goldsman; Anup C. Mokashi; James R. Wilson",
    "corresponding_authors": "",
    "abstract": "We present Sequem, a sequential procedure that delivers point and confidence-interval (CI) estimators for extreme steady-state quantiles of a simulation-generated process. Because it is specified completely, Sequem can be implemented directly and applied automatically. The method is an extension of the Sequest procedure developed by Alexopoulos et al. in 2014 to estimate nonextreme steady-state quantiles. Sequem exploits a combination of batching, sectioning, and the maximum transformation technique to achieve the following: (i) reduction in point-estimator bias arising from the simulation’s initial condition or from inadequate simulation run length; and (ii) adjustment of the CI half-length to compensate for the effects of skewness or autocorrelation on intermediate quantile point estimators computed from nonoverlapping batches of observations. Sequem’s CIs are designed to satisfy user-specified requirements concerning coverage probability and absolute or relative precision. In an experimental evaluation based on seven processes selected to stress-test the procedure, Sequem exhibited uniformly good performance.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2768123302",
    "type": "article"
  },
  {
    "title": "Scalable Cloning on Large-Scale GPU Platforms with Application to Time-Stepped Simulations on Grids",
    "doi": "https://doi.org/10.1145/3158669",
    "publication_date": "2018-01-31",
    "publication_year": 2018,
    "authors": "Srikanth B. Yoginath; Kalyan S. Perumalla",
    "corresponding_authors": "",
    "abstract": "Cloning is a technique to efficiently simulate a tree of multiple what-if scenarios that are unraveled during the course of a base simulation. However, cloned execution is highly challenging to realize on large, distributed memory computing platforms, due to the dynamic nature of the computational load across clones, and due to the complex dependencies spanning the clone tree. We present the conceptual simulation framework, algorithmic foundations, and runtime interface of C lone X, a new system we designed for scalable simulation cloning. It efficiently and dynamically creates whole logical copies of a dynamic tree of simulations across a large parallel system without full physical duplication of computation and memory. The performance of a prototype implementation executed on up to 1,024 graphical processing units of a supercomputing system has been evaluated with three benchmarks—heat diffusion, forest fire, and disease propagation models—delivering a speed up of over two orders of magnitude compared to replicated runs. The results demonstrate a significantly faster and scalable way to execute many what-if scenario ensembles of large simulations via cloning using the C lone X interface.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2786120259",
    "type": "article"
  },
  {
    "title": "Path-ZVA",
    "doi": "https://doi.org/10.1145/3161569",
    "publication_date": "2018-07-13",
    "publication_year": 2018,
    "authors": "Daniël Reijsbergen; Pieter-Tjerk de Boer; Werner Scheinhardt; Sandeep Juneja",
    "corresponding_authors": "",
    "abstract": "We introduce Path-ZVA: an efficient simulation technique for estimating the probability of reaching a rare goal state before a regeneration state in a (discrete-time) Markov chain. Standard Monte Carlo simulation techniques do not work well for rare events, so we use importance sampling; i.e., we change the probability measure governing the Markov chain such that transitions “towards” the goal state become more likely. To do this, we need an idea of distance to the goal state, so some level of knowledge of the Markov chain is required. In this article, we use graph analysis to obtain this knowledge. In particular, we focus on knowledge of the shortest paths (in terms of “rare” transitions) to the goal state. We show that only a subset of the (possibly huge) state space needs to be considered. This is effective when the high dependability of the system is primarily due to high component reliability, but less so when it is due to high redundancies. For several models, we compare our results to well-known importance sampling methods from the literature and demonstrate the large potential gains of our method.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2962786268",
    "type": "article"
  },
  {
    "title": "Data-driven Model-based Detection of Malicious Insiders via Physical Access Logs",
    "doi": "https://doi.org/10.1145/3309540",
    "publication_date": "2019-10-31",
    "publication_year": 2019,
    "authors": "Carmen Cheh; Uttam Thakore; Ahmed Fawaz; Binbin Chen; William G. Temple; William H. Sanders",
    "corresponding_authors": "",
    "abstract": "The risk posed by insider threats has usually been approached by analyzing the behavior of users solely in the cyber domain. In this article, we show the viability of using physical movement logs, collected via a building access control system, together with an understanding of the layout of the building housing the system’s assets, to detect malicious insider behavior that manifests itself in the physical domain. In particular, we propose a systematic framework that uses contextual knowledge about the system and its users, learned from historical data gathered from a building access control system, to select suitable models for representing movement behavior. We suggest two different models of movement behavior in this article and evaluate their ability to represent normal user movement. We then explore the online usage of the learned models, together with knowledge about the layout of the building being monitored, to detect malicious insider behavior. Finally, we show the effectiveness of the developed framework using real-life data traces of user movement in railway transit stations.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W3005790741",
    "type": "article"
  },
  {
    "title": "Guest editors’ introduction to special issue on the third INFORMS simulation society research workshop",
    "doi": "https://doi.org/10.1145/2555690",
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "Peter J. Haas; Shane G. Henderson; Pierre L’Ecuyer",
    "corresponding_authors": "",
    "abstract": "introduction Free Access Share on Guest editors’ introduction to special issue on the third INFORMS simulation society research workshop Editors: Peter J. Haas IBM Research IBM ResearchView Profile , Shane G. Henderson Cornell University Cornell UniversityView Profile , Pierre L'Ecuyer University of Montreal November 26, 2013 University of Montreal November 26, 2013View Profile Authors Info & Claims ACM Transactions on Modeling and Computer SimulationVolume 24Issue 101 January 2014Article No.: 1pp 1–3https://doi.org/10.1145/2555690Published:01 January 2014Publication History 0citation251DownloadsMetricsTotal Citations0Total Downloads251Last 12 Months9Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W1974828057",
    "type": "article"
  },
  {
    "title": "Efficient MCMC for Binomial Logit Models",
    "doi": "https://doi.org/10.1145/2414416.2414419",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Agnes Fussl; Sylvia Frühwirth‐Schnatter; R. Frühwirth",
    "corresponding_authors": "",
    "abstract": "This article deals with binomial logit models where the parameters are estimated within a Bayesian framework. Such models arise, for instance, when repeated measurements are available for identical covariate patterns. To perform MCMC sampling, we rewrite the binomial logit model as an augmented model which involves some latent variables called random utilities. It is straightforward, but inefficient, to use the individual random utility model representation based on the binary observations reconstructed from each binomial observation. Alternatively, we present in this article a new method to aggregate the random utilities for each binomial observation. Based on this aggregated representation, we have implemented an independence Metropolis-Hastings sampler, an auxiliary mixture sampler, and a novel hybrid auxiliary mixture sampler. A comparative study on five binomial datasets shows that the new aggregation method leads to a superior sampler in terms of efficiency compared to previously published data augmentation samplers.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W1978129083",
    "type": "article"
  },
  {
    "title": "Calibration, Validation, and Prediction in Random Simulation Models",
    "doi": "https://doi.org/10.1145/2699713",
    "publication_date": "2015-04-16",
    "publication_year": 2015,
    "authors": "Jun Yuan; Szu Hui Ng",
    "corresponding_authors": "",
    "abstract": "Model calibration and validation are important processes in the development of stochastic computer models of real complex systems. This article introduces an integrated approach for model calibration, validation, and prediction based on Gaussian process metamodels and a Bayesian approach. Within this integrated approach, a sequential approach is further proposed for stochastic computer model calibration. Several design criteria for this sequential stage are proposed and studied, including an entropy-based criterion and one based on minimizing prediction error. To further use the data resources to improve the performance of both calibration and prediction, an adaptive procedure that combines these criteria is introduced to balance the resource allocation between the calibration and prediction. The accuracy and efficiency of the proposed sequential calibration approach and the integrated approach are illustrated with several numerical examples.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W1981417332",
    "type": "article"
  },
  {
    "title": "Symbiotic Network Simulation and Emulation",
    "doi": "https://doi.org/10.1145/2717308",
    "publication_date": "2015-06-29",
    "publication_year": 2015,
    "authors": "Miguel A. Erazo; Rong Rong; Jason Liu",
    "corresponding_authors": "",
    "abstract": "A testbed capable of representing detailed operations of complex applications under diverse network conditions is invaluable for understanding the design and performance of new protocols and applications before their real deployment. We introduce a novel method that combines high-performance large-scale network simulation and high-fidelity network emulation, and thus enables real instances of network applications and protocols to run in real operating environments and be tested under simulated network settings. Using our approach, network simulation and emulation can form a symbiotic relationship, through which they are synchronized for an accurate representation of the network-scale traffic behavior. We introduce a model downscaling method along with an efficient queuing model and a traffic reproduction technique, which can significantly reduce the synchronization overhead and improve accuracy. We validate our approach with extensive experiments via simulation and with a real-system implementation. We also present a case study using our approach to evaluate a multipath data transport protocol.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2031021935",
    "type": "article"
  },
  {
    "title": "ConceVE",
    "doi": "https://doi.org/10.1145/2567897",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "Ross Gore; Saikou Y. Diallo; José J. Padilla",
    "corresponding_authors": "",
    "abstract": "In this article, we present ConceVE, an approach for designing and validating models before they are implemented in a computer simulation. The approach relies on (1) domain-specific languages for model specification, (2) the Alloy Specification Language and its constraint solving analysis capabilities for exploring the state space of the model dynamically, and (3) supporting visualization tools to relay the results of the analysis to the user. We show that our approach is applicable with generic languages such as the Web Ontology Language as well as special XML-based languages such as the Coalition Battle Management Language.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2051794812",
    "type": "article"
  },
  {
    "title": "Simulating Organogenesis",
    "doi": "https://doi.org/10.1145/2688908",
    "publication_date": "2015-02-17",
    "publication_year": 2015,
    "authors": "Arthur C. Schwaninger; Denis Menshykau; Dagmar Iber",
    "corresponding_authors": "",
    "abstract": "Recent advances in imaging technology now provide us with 3D images of developing organs. These can be used to extract 3D geometries for simulations of organ development. To solve models on growing domains, the displacement fields between consecutive image frames need to be determined. Here we develop and evaluate different landmark-free algorithms for the determination of such displacement fields from image data. In particular, we examine minimal distance, normal distance, diffusion-based, and uniform mapping algorithms and test these algorithms with both synthetic and real data in 2D and 3D. We conclude that in most cases, the normal distance algorithm is the method of choice and wherever it fails, diffusion-based mapping provides a good alternative.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2044773914",
    "type": "article"
  },
  {
    "title": "Data-driven simulation of complex multidimensional time series",
    "doi": "https://doi.org/10.1145/2553082",
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "Lee W. Schruben; Dashi I. Singham",
    "corresponding_authors": "",
    "abstract": "This article introduces a new framework for resampling general time series data. The approach, inspired by computer agent flocking algorithms, can be used to generate inputs to complex simulation models or for generating pseudo-replications of expensive simulation outputs. The method has the flexibility to enable replicated sensitivity analysis for trace-driven simulation, which is critical for risk assessment. The article includes two simple implementations to illustrate the approach. These implementations are applied to nonstationary and state-dependent multivariate time series. Examples using emergency department data are presented.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2061281125",
    "type": "article"
  },
  {
    "title": "Efficient simulations for the exponential integrals of Hölder continuous gaussian random fields",
    "doi": "https://doi.org/10.1145/2567892",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "Jingchen Liu; Gongjun Xu",
    "corresponding_authors": "",
    "abstract": "In this article, we consider a Gaussian random field f ( t ) living on a compact set T ⊂ R d and the computation of the tail probabilities P (∫ T e f ( t ) dt &gt; e b ) as b → ∞. We design asymptotically efficient importance sampling estimators for a general class of Hölder continuous Gaussian random fields. In addition to the variance control, we also analyze the bias (relative to the interesting tail probabilities) caused by the discretization.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2119671870",
    "type": "article"
  },
  {
    "title": "Multithreaded Stochastic PDES for Reactions and Diffusions in Neurons",
    "doi": "https://doi.org/10.1145/2987373",
    "publication_date": "2016-11-07",
    "publication_year": 2016,
    "authors": "Zhongwei Lin; Carl Tropper; Robert A. McDougal; Mohammand Nazrul Ishlam Patoary; William W. Lytton; Yiping Yao; Michael L. Hines",
    "corresponding_authors": "",
    "abstract": "Cells exhibit stochastic behavior when the number of molecules is small. Hence a stochastic reaction-diffusion simulator capable of working at scale can provide a more accurate view of molecular dynamics within the cell. This article describes a parallel discrete event simulator, Neuron Time Warp-Multi Thread (NTW-MT), developed for the simulation of reaction diffusion models of neurons. To the best of our knowledge, this is the first parallel discrete event simulator oriented toward stochastic simulation of chemical reactions in a neuron. The simulator was developed as part of the NEURON project. NTW-MT is optimistic and thread based, which attempts to capitalize on multicore architectures used in high performance machines. It makes use of a multilevel queue for the pending event set and a single rollback message in place of individual antimessages to disperse contention and decrease the overhead of processing rollbacks. Global Virtual Time is computed asynchronously both within and among processes to get rid of the overhead for synchronizing threads. Memory usage is managed in order to avoid locking and unlocking when allocating and deallocating memory and to maximize cache locality. We verified our simulator on a calcium buffer model. We examined its performance on a calcium wave model, comparing it to the performance of a process based optimistic simulator and a threaded simulator which uses a single priority queue for each thread. Our multithreaded simulator is shown to achieve superior performance to these simulators. Finally, we demonstrated the scalability of our simulator on a larger Calcium-Induced Calcium Release (CICR) model and a more detailed CICR model.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2549616710",
    "type": "article"
  },
  {
    "title": "Sufficient Conditions for Central Limit Theorems and Confidence Intervals for Randomized Quasi-Monte Carlo Methods",
    "doi": "https://doi.org/10.1145/3643847",
    "publication_date": "2024-02-15",
    "publication_year": 2024,
    "authors": "Marvin K. Nakayama; Bruno Tuffin",
    "corresponding_authors": "",
    "abstract": "Randomized quasi-Monte Carlo methods have been introduced with the main purpose of yielding a computable measure of error for quasi-Monte Carlo approximations through the implicit application of a central limit theorem over independent randomizations. But to increase precision for a given computational budget, the number of independent randomizations is usually set to a small value so that a large number of points are used from each randomized low-discrepancy sequence to benefit from the fast convergence rate of quasi-Monte Carlo. While a central limit theorem has been previously established for a specific but computationally expensive type of randomization, it is also known in general that fixing the number of randomizations and increasing the length of the sequence used for quasi-Monte Carlo can lead to a non-Gaussian limiting distribution. This paper presents sufficient conditions on the relative growth rates of the number of randomizations and the quasi-Monte Carlo sequence length to ensure a central limit theorem and also an asymptotically valid confidence interval. We obtain several results based on the Lindeberg condition for triangular arrays and expressed in terms of the regularity of the integrand and the convergence speed of the quasi-Monte Carlo method. We also analyze the resulting estimator’s convergence rate.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3186413696",
    "type": "article"
  },
  {
    "title": "Hyperparameter Tuning with Gaussian Processes for Optimal Abstraction Control in Simulation-based Optimization of Smart Semiconductor Manufacturing Systems",
    "doi": "https://doi.org/10.1145/3646549",
    "publication_date": "2024-02-17",
    "publication_year": 2024,
    "authors": "Moon Gi Seok; Wen Jun Tan; Boyi Su; Wentong Cai; Jisu Kwon; Seon Han Choi",
    "corresponding_authors": "",
    "abstract": "Smart manufacturing utilizes digital twins that are virtual forms of their production plants for analyzing and optimizing decisions. Digital twins have been mainly developed as discrete-event models (DEMs) to represent the detailed and stochastic dynamics of productions in the plants. The optimum decision is achieved after simulating the DEM-based digital twins under various what-if decision candidates; thus, simulation acceleration is crucial for rapid optimum determination for given problems. For the acceleration of discrete-event simulations, adaptive abstraction-level conversion approaches have been previously proposed to switch active models of each machine group between a set of DEM components and a corresponding lookup table-based mean-delay model during runtime. The switching is decided by detecting the machine group’s convergence into (or divergence from) a steady state. However, there is a tradeoff between speedup and accuracy loss in the adaptive abstraction convertible simulation (AACS), and inaccurate simulation can degrade the quality of the optimum (i.e., the distance between the calculated optimum and the actual optimum). In this article, we propose a simulation-based optimization that optimizes the problem based on a genetic algorithm while tuning specific hyperparameters (related to the tradeoff control) to maximize the speedup of AACS under a specified accuracy constraint. For each individual, the proposed method distributes the overall computing budget for multiple simulation runs (considering the digital twin’s probabilistic property) into the hyperparameter optimization (HPO) and fitness evaluation. We propose an efficient HPO method that manages multiple Gaussian process models (as speedup-estimation models) to acquire promising optimal hyperparameter candidates (that maximize the simulation speedups) with few attempts. The method also reduces each individual’s exploration overhead (as the population evolves) by estimating each hyperparameter’s expected speedup using previous exploration results of neighboring individuals without actual simulation executions. The proposed method was applied to optimize raw-material releases of a large-scale manufacturing system to prove the concept and evaluate the performance under various situations.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4391897104",
    "type": "article"
  },
  {
    "title": "Projected Gaussian Markov Improvement Algorithm for High-Dimensional Discrete Optimization via Simulation",
    "doi": "https://doi.org/10.1145/3649463",
    "publication_date": "2024-03-01",
    "publication_year": 2024,
    "authors": "Xinru Li; Eunhye Song",
    "corresponding_authors": "",
    "abstract": "This article considers a discrete optimization via simulation (DOvS) problem defined on a graph embedded in the high-dimensional integer grid. Several DOvS algorithms that model the responses at the solutions as a realization of a Gaussian Markov random field (GMRF) have been proposed exploiting its inferential power and computational benefits. However, the computational cost of inference increases exponentially in dimension. We propose the projected Gaussian Markov improvement algorithm (pGMIA), which projects the solution space onto a lower-dimensional space creating the region-layer graph to reduce the cost of inference. Each node on the region-layer graph can be mapped to a set of solutions projected to the node; these solutions form a lower-dimensional solution-layer graph. We define the response at each region-layer node to be the average of the responses within the corresponding solution-layer graph. From this relation, we derive the region-layer GMRF to model the region-layer responses. The pGMIA alternates between the two layers to make a sampling decision at each iteration. It first selects a region-layer node based on the lower-resolution inference provided by the region-layer GMRF, then makes a sampling decision among the solutions within the solution-layer graph of the node based on the higher-resolution inference from the solution-layer GMRF. To solve even higher-dimensional problems (e.g., 100 dimensions), we also propose the pGMIA+: a multi-layer extension of the pGMIA. We show that both pGMIA and pGMIA+ converge to the optimum almost surely asymptotically and empirically demonstrate their competitiveness against state-of-the-art high-dimensional Bayesian optimization algorithms.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4392357317",
    "type": "article"
  },
  {
    "title": "RayNet: A Simulation Platform for Developing Reinforcement Learning-Driven Network Protocols",
    "doi": "https://doi.org/10.1145/3653975",
    "publication_date": "2024-03-30",
    "publication_year": 2024,
    "authors": "Luca Giacomoni; Basil Benny; George Parisis",
    "corresponding_authors": "",
    "abstract": "Reinforcement Learning (RL) has gained significant momentum in the development of network protocols. However, RL-based protocols are still in their infancy, and substantial research is required to build deployable solutions. Developing a protocol based on RL is a complex and challenging process that involves several model design decisions and requires significant training and evaluation in real and simulated network topologies. Network simulators offer an efficient training environment for RL-based protocols because they are deterministic and can run in parallel. In this article, we introduce RayNet , a scalable and adaptable simulation platform for the development of RL-based network protocols. RayNet integrates OMNeT++, a fully programmable network simulator, with Ray/RLlib, a scalable training platform for distributed RL. RayNet facilitates the methodical development of RL-based network protocols so that researchers can focus on the problem at hand and not on implementation details of the learning aspect of their research. We developed a simple RL-based congestion control approach as a proof of concept showcasing that RayNet can be a valuable platform for RL-based research in computer networks, enabling scalable training and evaluation. We compared RayNet with ns3-gym , a platform with similar objectives to RayNet, and showed that RayNet performs better in terms of how fast agents can collect experience in RL environments.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4393342214",
    "type": "article"
  },
  {
    "title": "Selection of the Best in the Presence of Subjective Stochastic Constraints",
    "doi": "https://doi.org/10.1145/3664814",
    "publication_date": "2024-05-11",
    "publication_year": 2024,
    "authors": "Yuwei Zhou; Sigrún Andradóttir; Seong‐Hee Kim",
    "corresponding_authors": "",
    "abstract": "We consider the problem of finding a system with the best primary performance measure among a finite number of simulated systems in the presence of subjective stochastic constraints on secondary performance measures. When no feasible system exists, the decision maker may be willing to relax some constraint thresholds. We take multiple threshold values for each constraint as a user’s input and propose indifference-zone procedures that perform the phases of feasibility check and selection-of-the-best sequentially or simultaneously. Given that there is no change in the underlying simulated systems, our procedures recycle simulation observations to conduct feasibility checks across all potential thresholds. We prove that the proposed procedures yield the best system in the most desirable feasible region possible with at least a pre-specified probability. Our experimental results show that our procedures perform well with respect to the number of observations required to make a decision, as compared with straight-forward procedures that repeatedly solve the problem for each set of constraint thresholds, and that our simultaneously-running procedure provides the best overall performance.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4396827002",
    "type": "article"
  },
  {
    "title": "Generating Hidden Markov Models from Process Models Through Nonnegative Tensor Factorization",
    "doi": "https://doi.org/10.1145/3664813",
    "publication_date": "2024-06-10",
    "publication_year": 2024,
    "authors": "Erik Skau; Andrew Hollis; Stephan Eidenbenz; K. Ø. Rasmussen; Boian S. Alexandrov",
    "corresponding_authors": "",
    "abstract": "Monitoring of industrial processes is a critical capability in industry and in government to ensure reliability of production cycles, quick emergency response, and national security. Process monitoring allows users to gauge the progress of an organization in an industrial process or predict the degradation or aging of machine parts in processes taking place at a remote location. Similar to many data science applications, we usually only have access to limited raw data, such as satellite imagery, short video clips, event logs, and signatures captured by a small set of sensors. To combat data scarcity, we leverage the knowledge of Subject Matter Experts (SMEs) who are familiar with the actions of interest. SMEs provide expert knowledge of the essential activities required for task completion and the resources necessary to carry out each of these activities. Various process mining techniques have been developed for this type of analysis; typically such approaches combine theoretical process models built based on domain expert insights with ad-hoc integration of available pieces of raw data. Here, we introduce a novel mathematically sound method that integrates theoretical process models (as proposed by SMEs) with interrelated minimal Hidden Markov Models (HMM), built via nonnegative tensor factorization. Our method consolidates: (a) theoretical process models, (b) HMMs, (c) coupled nonnegative matrix-tensor factorizations, and (d) custom model selection. To demonstrate our methodology and its abilities, we apply it on simple synthetic and real world process models.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4399498532",
    "type": "article"
  },
  {
    "title": "A Toolset for Predicting Performance of Legacy Real-Time Software Based on the RAST Approach",
    "doi": "https://doi.org/10.1145/3673897",
    "publication_date": "2024-06-20",
    "publication_year": 2024,
    "authors": "Juri Tomak; Sergei Gorlatch",
    "corresponding_authors": "",
    "abstract": "Simulating and predicting the performance of a distributed software system that works under stringent real-time constraints poses significant challenges, particularly when dealing with legacy systems being in production use, where any disruption is intolerable. This challenge is exacerbated in the context of a System Under Evaluation (SUE) that operates within a resource-sharing environment, running concurrently with numerous other software components. In this paper, we introduce an innovative toolset designed for predicting the performance of such complex and time-critical software systems. Our toolset builds upon the RAST ( R egression A nalysis, S imulation, and load T esting) approach, significantly enhanced in this paper compared to its initial version. While current state-of-the-art methods for performance prediction often rely on data collected by Application Performance Monitoring (APM), the unavailability of APM tools for existing systems and the complexities associated with integrating them into legacy software necessitate alternative approaches. Our toolset, therefore, utilizes readily accessible system request logs as a substitute for APM data. We describe the enhancements made to the original RAST approach, we outline the design and implementation of our RAST-based toolset, and we showcase its simulation accuracy and effectiveness using the publicly available TeaStore benchmarking system. To ensure the reproducibility of our experiments, we provide open access to our toolset’s implementation and the utilized TeaStore model.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4399860150",
    "type": "article"
  },
  {
    "title": "SymBChainSim: A Novel Simulation System for Info-Symbiotic Blockchain Management",
    "doi": "https://doi.org/10.1145/3704917",
    "publication_date": "2024-11-19",
    "publication_year": 2024,
    "authors": "Georgios Diamantopoulos; Rami Bahsoon; Nikos Tziritas; Georgios Theodoropoulos",
    "corresponding_authors": "",
    "abstract": "Despite the recent increase in the popularity of blockchain, the technology suffers from the trilemma trade-off between security, decentralisation and scalability, prohibiting adoption and limiting the efficiency and effectiveness of the induced system. Addressing the trilemma trade-off calls for dynamic management and configuration of the blockchain system. In particular, choosing an effective and efficient consensus protocol for balancing the trilemma trade-off when inducing the blockchain-based system is acknowledged to be a challenging problem, given the dynamic and complex nature of the blockchain environment. DDDAS approaches are particularly suitable for tackling this challenge. In previous work, the authors presented a novel DDDAS-based blockchain architecture and demonstrated that it offers a promising approach for dynamically adjusting the parameters of a blockchain system to optimise for the trade-off. This paper presents a novel simulation tool that can support and satisfy the DDDAS requirements for a dynamically re-configurable blockchain system. The tool supports the simulation and the dynamic switching of consensus protocols, analysing their trilemma trade-off. The simulator design is modular and allows the implementation and analysis of a wide range of consensus protocols and their implementation scenarios. The paper also presents a quantitative evaluation of the tool.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4404505001",
    "type": "article"
  },
  {
    "title": "Preventing Workload Interference with Intelligent Routing and Flexible Job Placement Strategy on Dragonfly System",
    "doi": "https://doi.org/10.1145/3706104",
    "publication_date": "2024-12-02",
    "publication_year": 2024,
    "authors": "Xin Wang; Yao Kang; Zhiling Lan",
    "corresponding_authors": "",
    "abstract": "Dragonfly is an indispensable interconnect topology for exascale HPC systems. To link tens of thousands of compute nodes at a reasonable cost, Dragonfly shares network resources with the entire system such that network bandwidth is not exclusive to any single application. Since HPC systems are usually shared among multiple co-running applications at the same time, network competition between co-existing workloads is inevitable. This network contention manifests as workload interference, where a job’s network communication can be severely delayed by other jobs. This study presents a comprehensive examination of leveraging intelligent routing and flexible job placement to mitigate workload interference on Dragonfly systems. Specifically, we leverage the parallel discrete event simulation toolkit, SST, to investigate workload interference on Dragonfly with three contributions. We first present Q-adaptive routing, a multi-agent reinforcement learning routing scheme, and a flexible job placement strategy that, together, can mitigate workload interference based on workload communication characteristics. Next, we enhance SST with Q-adaptive routing and develop an automatic module that serves as the bridge between SST and HPC job scheduler for automatic simulation configuration and automated simulation launching. Finally, we extensively examine the workload interference under various job placement and routing configurations.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4404903269",
    "type": "article"
  },
  {
    "title": "Using permutations in regenerative simulations to reduce variance",
    "doi": "https://doi.org/10.1145/280265.280273",
    "publication_date": "1998-04-01",
    "publication_year": 1998,
    "authors": "James M. Calvin; Marvin K. Nakayama",
    "corresponding_authors": "",
    "abstract": "We propose a new estimator for a large class of performance measures obtained from a regenerative simulation of a system having two distinct sequences of regeneration times. To construct our new estimator, we first generate a sample path of a fixed number of cycles based on one sequence of regeneration times, divide the path into segments based on the second sequence of regeneration times, permute the segments, and calculate the performance on the new path using the first sequence of regeneration times. We average over all possible permutations to construct the new estimator. This strictly reduces variance when the original estimator is not simply an additive functional of the sample path. To use the new estimator in practice, the extra computational effort is not large since all permutations do not actually have to be computed as we derive explicit formulas for our new estimators. We examine the small-sample behavior of our estimators. In particular, we prove that for any fixed number of cycles from the first regenerative sequence, our new estimator has smaller mean squared error than the standard estimator. We show explicitly that our method can be used to derive new estimators for the expected cumulative reward until a certain set of states is hit and the time-average variance parameter of a regenerative simulation.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2003624153",
    "type": "article"
  },
  {
    "title": "A comparative study of parallel algorithms for simulating continuous time Markov chains",
    "doi": "https://doi.org/10.1145/226275.226278",
    "publication_date": "1995-10-01",
    "publication_year": 1995,
    "authors": "David M. Nicol; Philip Heidelberger",
    "corresponding_authors": "",
    "abstract": "This article describes methods for simulating continuous time Markov chain models, using parallel architectures. The basis of our method is the technique of uniformization; within this framework there are a number of options concerning optimism and aggregation. We describe four different variations, paying particular attention to an adaptive method that optimistically assumes upper bounds on the rate at which one processor affects another in simulation time, and recovers from violations of this assumption using global checkpoints. We describe our experiences with these methods on a variety of Intel multiprocessor architectures, including the Touchstone Delta, where excellent speedups of up to 220 using 256 processors are observed.",
    "cited_by_count": 21,
    "openalex_id": "https://openalex.org/W2006231437",
    "type": "article"
  },
  {
    "title": "Multiple comparisons with the best for steady-state simulation",
    "doi": "https://doi.org/10.1145/151527.151551",
    "publication_date": "1993-01-02",
    "publication_year": 1993,
    "authors": "Mingjian Yuan; Barry L. Nelson",
    "corresponding_authors": "",
    "abstract": "Multiple-comparison procedures are useful for comparing the performance of competing systems via simulation. In this paper we extend a particular multiple-comparison procedure, multiple comparisons with the best, to steady-state simulation by using an autoregressive-output-analysis method.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2006985101",
    "type": "article"
  },
  {
    "title": "A logic-based foundation of discrete event modeling and simulation",
    "doi": "https://doi.org/10.1145/174619.174620",
    "publication_date": "1994-01-01",
    "publication_year": 1994,
    "authors": "Ashvin Radiya; Robert G. Sargent",
    "corresponding_authors": "",
    "abstract": "A logic-based foundation of discrete event modeling and simulation is presented by defining (1) its fundamental concepts and terms from a perspective commonly held by logicians, (2) a modal Discrete Event Logic L DE . The ways of expressing models using L DE are discussed and compared with the ways of expressing models in simulation languages that support the event scheduling world view. The logic-based foundation provides fundamentally new insights. It asserts that events are logical propositions and the use of temporal operators is implicit in discrete event modeling and simulation languages. However, existing languages utilize only a few temporal operators in a restricted manner. The logic-based foundation enhances the ways of expressing models by using the operators implicit in existing languages in more general ways, new operators, and a parallel connective ||. The logic L DE and notions implicit in it form a new framework for understanding, defining, and studying logical combinations of events, variables, and time, and expressions containing a wide range of temporal operators including next, if, when, whenever, until, while, unless, and at.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2054237002",
    "type": "article"
  },
  {
    "title": "Fast simulation of networks of queues with effective and decoupling bandwidths",
    "doi": "https://doi.org/10.1145/301677.301684",
    "publication_date": "1999-01-01",
    "publication_year": 1999,
    "authors": "Matthias Falkner; Michael Devetsikiotis; Ioannis Lambadaris",
    "corresponding_authors": "",
    "abstract": "A significant difficulty when using Monte Carlo simulation for the performance analysis of communication networks is the long runtime required to obtain accurate statistical estimates. Under the proper conditions, importance sampling (IS) is a technique that can speed up simulations involving rare events in network (queuing) systems. Large speed-up factors in simulation runtime can be obtained with IS if the modification or bias of the underlying probability measures of certain random processes is carefully chosen. Fast simulation methods based on large deviation theory (LTD) have been successfully applied in many cases. In this paper, we set up an IS-based simulation of various elementary network topologies. These configurations are frequenly encountered in broadband ATM-based network components such as switches and multiplexers. Our objective in this study is to obtain the optimal or near-optimal biasing parameter values of the arrival processes for the importance sampling simulation. For this purpose we appropriately apply a technique presented by Chang et al. for certain portions of the networks (intree) while we develop a new algorithm, inspired by the work of De Veciana et al. on decoupling bandwidths, for the non-intree portion of the network.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2055029865",
    "type": "article"
  },
  {
    "title": "Rejection-inversion to generate variates from monotone discrete distributions",
    "doi": "https://doi.org/10.1145/235025.235029",
    "publication_date": "1996-07-01",
    "publication_year": 1996,
    "authors": "Wolfgang Hörmann; Gerhard Derflinger",
    "corresponding_authors": "",
    "abstract": "For discrete distributions a variant of reject from a continuous hat function is presented. The main advantage of the new method, called rejection-inversion , is that no extra uniform random number to decide between acceptance and rejection is required, which means that the expected number of uniform variates required is halved. Using rejection-inversion and a squeeze, a simple universal method for a large class of monotone discrete distributions is developed. It can be used to generate variates from the tails of most standard discrete distributions. Rejection-inversion applied to the Zipf (or zeta) distribution results in algorithms that are short and simple and at least twice as fast as the fastest methods suggested in the literature.",
    "cited_by_count": 20,
    "openalex_id": "https://openalex.org/W2067075954",
    "type": "article"
  },
  {
    "title": "Time warp simulation using time scale decomposition",
    "doi": "https://doi.org/10.1145/137926.137959",
    "publication_date": "1992-04-01",
    "publication_year": 1992,
    "authors": "H.H. Ammar; Su Deng",
    "corresponding_authors": "",
    "abstract": "In this paper we consider time scale decomposition as well as spatial decomposition to induce massive parallelism and reduce overhead in distributed discrete-event simulations. We confine our study to the Time Warp strategy and to systems where the durations of activities differ by several orders of magnitude (i.e., systems with fast and slow activities). We show that, for such systems, a large overhead due to rollbacks is encountered when spatial decomposition is used. Moreover, performance degrades as the difference increases between the rates of fast and slow events. Several initial experiments using queueing-network models were designed to evaluate the effectiveness of time scale decomposition in increasing the parallelism and reducing the overhead. These experiments were conducted on a distributed simulation testbed that was implemented on an 18-processor Multimax 320. The application of the above simulation techniques to stochastic Petri net models is illustrated using an example of performability analysis of a fault-tolerant distributed system.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W1981854412",
    "type": "article"
  },
  {
    "title": "Parallel shared-memory simulator performance for large ATM networks",
    "doi": "https://doi.org/10.1145/369534.369537",
    "publication_date": "2000-10-01",
    "publication_year": 2000,
    "authors": "Brian Unger; Zhonge Xiao; John J. Cleary; Jya-Jang Tsai; Carey Williamson",
    "corresponding_authors": "",
    "abstract": "A performance comparison between an optimistic and a conservative parallel simulation kernel is presented. Performance of the parallel kernels is also compared to a central-event-list sequential kernel. A spectrum of ATM network and traffic scenarios representative of those used by ATM networking researchers are used for the comparison. Experiments are conducted with a cell-level ATM network simulator and an 18-processor SGI PowerChallenge shared-memory multiprocessor. The results show the performance advantages of parallel simulation ove r sequential simulation for ATM networks. Speedups of 4-5 relative to a fast sequential kernel are achieved on 16 processors for several large irregular ATM benchmark scenarios and the optimistic kernel achieves 2 to 5 times speedup on all 7 benchmarks. However, the relative performance of the two parallel simulation kernels is dependent on the size of the ATM network, the number of traffic sources, and the traffic source types used in the simulation. For some benchmarks the best single point performance is provided by the conservative kernel even on a single processor. Unfortunately, the conservative kernel performance is susceptible to small changes in the modeling code and is outperformed by the optimistic kernel on 5 of the 7 benchmarks. The optimistic parallel simulation kernel thus provides most robust performance, but its speedup is limited by the overheads of its implementation, which make it approximately half the speed of the sequential kernel on one processor.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W1984785310",
    "type": "article"
  },
  {
    "title": "A new approach combining simulation and randomization for the analysis of large continuous time Markov chains",
    "doi": "https://doi.org/10.1145/280265.280274",
    "publication_date": "1998-04-01",
    "publication_year": 1998,
    "authors": "Peter Buchholz",
    "corresponding_authors": "Peter Buchholz",
    "abstract": "A new analysis method for continuous time Markov chains is introduced. The method combines, in some sense, simulation and numerical techniques for the analysis of large Markov chains. The basis of the new method is the description of a continuous-time Markov chain as a set of communicating processes. The state of all or some of the processes is described by a state vector, including a probability distribution over the set of locally reachable states. Simulation is used to determine the event times and message types exchanged between processes. Local transitions are realized by vector matrix products describing the next state distribution form the current one. In this way, the state explosion problem of numerical analysis is avoided, but it is still possible to obtain more accurate results that with pure simulation. The approach is therefore especially useful for the analysis of quantities with small probabilities or for the analysis of rare events.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2025641499",
    "type": "article"
  },
  {
    "title": "Estimating small cell-loss ratios in ATM switches via importance sampling",
    "doi": "https://doi.org/10.1145/379525.379528",
    "publication_date": "2001-01-01",
    "publication_year": 2001,
    "authors": "Pierre L’Ecuyer; Y. Champoux",
    "corresponding_authors": "",
    "abstract": "The cell-loss ratio at a given node in an ATM switch, defined as the steady-state fraction of packets of information that are lost at that node due to buffer overflow, is typically a very small quantity that is hard to estimate by simulation. Cell losses are rare events, and importance sampling is sometimes the appropriate tool in this situation. However, finding the right change of measure is generally difficult. In this article, importance sampling is applied to estimate the cell-loss ratio in an ATM switch modeled as a queuing network that is fed by several sources emitting cells according to a Markov-modulated ON/OFF process, and in which all the cells from the same source have the same destination. The charge of measure is obtained via an adaptation of a heuristic proposed by Chang et al. {1994} for intree networks. The numerical experiments confirm important efficiency improvements even for large nonintree networks and a large number of sources. Experiments with different variants of the importance sampling methodology are also reported, and a number of practical issues are illustrated and discussed.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2081141449",
    "type": "article"
  },
  {
    "title": "The weighted spectral test",
    "doi": "https://doi.org/10.1145/272991.273008",
    "publication_date": "1998-01-01",
    "publication_year": 1998,
    "authors": "Peter Hellekalek; Harald Niederreiter",
    "corresponding_authors": "",
    "abstract": "In this article, we present a new approach to assessing uniform random number generators, the weighted spectral test , or diaphony . In contrast to the usual spectral test the weighted spectral test is not limited to random number generators with a lattice structure. Its computational complexity is O (s·N 2 ) for any point set of cardinality N in the s-dimensional unit cube. As the main results of this article, we prove an analog of the classical inequality of Erdös-Turán-Koksma, present the necessary tools to transcribe known discrepancy bounds into bounds for diaphony, and provide bounds for the diaphony of multiplicative congruetial pseudorandom numbers. The last section contains numerical results.",
    "cited_by_count": 19,
    "openalex_id": "https://openalex.org/W2088210818",
    "type": "article"
  },
  {
    "title": "Simulation, technology, and the decision process",
    "doi": "https://doi.org/10.1145/116890.116891",
    "publication_date": "1991-04-01",
    "publication_year": 1991,
    "authors": "Philip J. Kiviat",
    "corresponding_authors": "Philip J. Kiviat",
    "abstract": "article Simulation, technology, and the decision process Share on Author: Philip J. Kiviat ICF Information Technology, Inc., Fairfax, VA ICF Information Technology, Inc., Fairfax, VAView Profile Authors Info & Claims ACM Transactions on Modeling and Computer SimulationVolume 1Issue 2April 1991 pp 89–98https://doi.org/10.1145/116890.116891Published:01 April 1991 8citation539DownloadsMetricsTotal Citations8Total Downloads539Last 12 Months4Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",
    "cited_by_count": 18,
    "openalex_id": "https://openalex.org/W2091316051",
    "type": "article"
  },
  {
    "title": "A large deviations analysis of the transient of a queue with many Markov fluid inputs",
    "doi": "https://doi.org/10.1145/511442.511443",
    "publication_date": "2002-01-01",
    "publication_year": 2002,
    "authors": "Michel Mandjes; Ad Ridder",
    "corresponding_authors": "",
    "abstract": "This article analyzes the transient buffer content distribution of a queue fed by a large number of Markov fluid sources. We characterize the probability of overflow at time t , given the current buffer level and the number of sources in the on-state. After scaling buffer and bandwidth resources by the number of sources n , we can apply large deviations techniques. The transient overflow probability decays exponentially in n . In case of exponential on/off sources, we derive an expression for the decay rate of the rare event probability under consideration. For general, Markov fluid sources, we present a plausible conjecture. We also provide the \"most likely path\" from the initial state to overflow (at time t ). Knowledge of the decay rate and the most likely path to overflow leads to (i) approximations of the transient overflow probability, and (ii) efficient simulation methods of the rare event of buffer overflow. The simulation methods, based on importance sampling, give a huge speed-up compared to straightforward simulations. The approximations are of low computational complexity, and accurate, as verified by means of simulation experiments.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2154169228",
    "type": "article"
  },
  {
    "title": "An asynchronous integration and event detection algorithm for simulating multi-agent hybrid systems",
    "doi": "https://doi.org/10.1145/1029174.1029177",
    "publication_date": "2004-10-01",
    "publication_year": 2004,
    "authors": "Joel M. Esposito; Vijay Kumar",
    "corresponding_authors": "",
    "abstract": "A simulation algorithm is presented for multi-agent hybrid systems---systems consisting of many sets of nonsmooth differential equations---such as systems involving multiple rigid bodies, vehicles, or airplanes. The differential equations are partitioned into coupled subsystems, called \"agents\"; and the conditions which trigger the discontinuities in the derivatives, called \"events\", may depend on the global state vector. Such systems normally require significant computational resources to simulate because a global time step is used to ensure the discontinuity is properly handled. When the number of systems is large, forcing all system to be simulated at the same rate creates a computational bottleneck, dramatically decreasing efficiency. By using a control systems approach for selecting integration step sizes, we avoid using a global time step. Each subsystem can be simulated asynchronously when the state is away from the event. As the state approaches the event, the simulation is able to synchronize each of the local time clocks in such a way that the discontinuities are properly handled without the need for \"roll back\". The algorithm's operation and utility is demonstrated on an example problem inspired by autonomous highway vehicles. Using a combination of stochastic modelling and numerical experiments we show that the algorithm requires significantly less computation time when compared with traditional simulation techniques for such problems, and scales more favorably with problem size.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W1970271321",
    "type": "article"
  },
  {
    "title": "HNS",
    "doi": "https://doi.org/10.1145/1010621.1010623",
    "publication_date": "2004-07-01",
    "publication_year": 2004,
    "authors": "Benjamin Melamed; Shuo Pan; Y. Wardi",
    "corresponding_authors": "",
    "abstract": "This article was motivated by the need to speed up complex network simulation, especially in telecommunications settings, where high bandwidth translates into exorbitant numbers of packets that take inordinate CPU time to simulate. Since the simulation complexity of fluid workload is invariant under bandwidth scaling, flows of discrete units of workload may be replaced by (approximate) fluid streams for savings in CPU time and memory storage.To this end, the article outlines the design of a new hybrid Java simulator, called HNS ( Hybrid Network Simulator ). HNS simulates the movement of workload in a queueing network, where transactions may be of two types: traditional discrete transactions (e.g., packets) and continuous (fluid) transactions, all of which arrive discretely at the network in traffic flows, and each discrete arrival carries a workload. Arriving transactions only differ in the way their workloads are transported: the user specifies whether the workload should be packetized or fluidized , respectively, for transport across the network. The novel feature of HNS is that it admits models with both discrete and continuous traffic flows, and collects detailed statistics for both, including arrival, loss, buffer contents, departure, and delay statistics. HNS facilitates the fundamental tradeoff between the modeling accuracy of discrete flows and savings in simulation time and storage often afforded by continuous flows by providing a common testbed for both discrete and continuous flows, where the user can readily select transport modes and achieve a good deal of variance reduction in assessing the accuracy and speeds of model versions with different mixtures of flow types. We caution, however, that while HNS is a generic hybrid simulator of discrete and continuous flows, mixed models should be handled with care, since they tend to counteract the advantages of pure-packet or pure-fluid models.In order to achieve a high degree of speedup for fluid-flow models, HNS introduces and utilizes the so-called streamlining methodology to identify and modify algorithms that cause \"turbulent\" fluid flow (namely, scenarios where fluid-flow rates have a large number of minor fluctuations, which are computationally expensive but have a minor impact on simulation statistics). Streamlining removes these fluctuations so as to speed up the simulation at a moderate loss of statistical accuracy. HNS can be extended by writing additional Java classes; in particular, stream behavior can be modified by extending built-in protocol classes. For example, HNS already has dual implementations (packet and fluid) of various telecommunications-specific protocols, such as ATM (Asynchronous Transfer Mode), User Datagram Protocol (UDP), and TCP (Transport Control Protocol), the latter with a streamlined fluid protocol approximation.In this article, we describe the architecture of HNS and its operational features, including traffic injection at network sources, generic workload transport, some telecommunications-specific protocols, and statistics collection and display via a graphic user interface (GUI). We then use HNS to validate the streamlining methodology and to study appropriate tradeoffs of simulation speed and accuracy in telecommunications networks by comparing pure-packet, pure-fluid, and mixed versions of the same network model.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2011501969",
    "type": "article"
  },
  {
    "title": "Modeling of discrete event systems",
    "doi": "https://doi.org/10.1145/1029174.1029178",
    "publication_date": "2004-10-01",
    "publication_year": 2004,
    "authors": "Carmen-Veronica Bobeanu; E. J. H. Kerckhoffs; Hendrik Van Landeghem",
    "corresponding_authors": "",
    "abstract": "In this article, the authors provide an alternative view on Petri nets modeling of discrete event systems. The proposed modeling procedure follows the Systems Specification guidelines underlying the well-known DEVS modeling formalism. The authors' endeavour is towards perfecting the design of reusable Petri nets-based models by searching for a good primitive for a modular model construction and the introduction of coupling templates as standardised means to couple building block components. Assuming that the real-world system to be modeled has been analyzed in depth beforehand through a suitable system analysis method (which itself is beyond the scope of the article), we present a systematic step-by-step approach to construct a model in the Petri nets domain together with its experimental frame. The construction adheres to well-defined rules, which enable computer-based model construction. The input for this systematic bottom-up construction of Petri nets-models is information (about, e.g., primitive system components, entity flows, routing constructs) gathered from the top-down system analysis. In the article, attention is also paid to the algebraic backgrounds underlying the proposed model construction. These provide the basis for formal correctness proofs, mapping Petri nets onto DEVS-models, and complexity reduction of the found Petri nets-models. By offering to the model builder the possibility to handle multiple abstraction levels and by addressing important issues related to the interfacing question of coupled models and model components described in Petri nets and DEVS formalism, the authors' work addresses two of the main research directions of Computer Automated Multi-Paradigm Modeling ([Mosterman and Vangheluwe 2002]): &lt;i&gt;model abstraction&lt;/i&gt; and &lt;i&gt;multiformalism modeling&lt;/i&gt;. The article concludes with an illustrative application example.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2081505989",
    "type": "article"
  },
  {
    "title": "A Co-Plot analysis of logs and models of parallel workloads",
    "doi": "https://doi.org/10.1145/1243991.1243993",
    "publication_date": "2007-07-01",
    "publication_year": 2007,
    "authors": "David Talby; Dror G. Feitelson; Adi Raveh",
    "corresponding_authors": "",
    "abstract": "We present a multivariate analysis technique called Co-Plot that is especially suitable for few samples of many variables. Co-Plot embeds the multidimensional samples in two dimensions, in a way that allows key variables to be identified, and relations between both variables and observations to be analyzed together. When applied to the workloads on parallel supercomputers, we find two stable perpendicular axes of highly correlated variables, one representing individual job attributes and the other representing multijob attributes. The different workloads, on the other hand, are rather different from one another, and may also change over time. Synthetic models for workload generation are also analyzed, and found to be reasonable in the sense that they span the same range of variable combinations as the real workloads. However, the spread of real workloads implies that a single model cannot be similar to all of them. This leads us to construct a parameterized model, with parameters that correspond to the two axes identified above. We also find that existing models do not model the temporal structure of the workload well, and hence are wanting for tasks such as comparing schedulers, and that the common methodology for load manipulation of workloads is problematic.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2071161381",
    "type": "article"
  },
  {
    "title": "On Automated Memoization in the Field of Simulation Parameter Studies",
    "doi": "https://doi.org/10.1145/3186316",
    "publication_date": "2018-09-24",
    "publication_year": 2018,
    "authors": "Mirko Stoffers; Daniel Schemmel; Oscar Soria Dustmann; Klaus Wehrle",
    "corresponding_authors": "",
    "abstract": "Processes in computer simulations tend to be highly repetitive. In particular, parameter studies further exasperate the situation as the same model is repeatedly executed with only partially varying parameters. Consequently, computer simulations perform identical computations, with identical code, identical input, and hence identical output. These redundant computations waste significant amounts of time and energy. Memoization, dating back to 1968, enables the caching of such identical intermediate results, thereby significantly speeding up those computations. However, until now, automated approaches were limited to pure functions. At ACM SIGSIM-PADS 2016 we published, to the best of our knowledge, the first practical approach for automated memoization for impure code. In this work, we extend this approach and evaluate the performance characteristics of a number of extensions that deal with questions posed at PADS: (1) To reduce and bound the memory footprint, we investigate several cache eviction strategies. (2) We allow the original and the memoized code to coexist via a runtime-switch and analyze the crossover point, thereby mitigating memoization overhead. (3) By optionally persisting the Memoization Cache to disk, we expand the scope to exploratory parameter studies where cached results can now be reused across multiple simulation runs. Altogether, automated memoization for impure code is a valuable technique, the versatility of which we explore further in this article. It sped up a case study of an OFDM network simulation by a factor of more than 80 with an only marginal increase of memory consumption.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2892874221",
    "type": "article"
  },
  {
    "title": "Parameterized activity cycle diagram and its application",
    "doi": "https://doi.org/10.1145/2501593",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Byoung K. Choi; Dong‐Hun Kang; Taesik Lee; Arwa A. Jamjoom; Maysoon Abulkhair",
    "corresponding_authors": "",
    "abstract": "The classical activity cycle diagram (ACD), which is a bipartite directed graph, is easy to learn and use for describing the dynamic behavior of a discrete-event system . However, the complexity of the classical ACD model increases rapidly as the system size increases. This article presents an enriched ACD called the parameterized ACD (P-ACD). In P-ACD, each node is allowed to have parameter variables , and parameter values are passed to the parameter variables through a directed arc . This article demonstrates how a single P-ACD model can be used to represent an entire class of very large-scale systems instead of requiring different ACD models for every instance . We also illustrate that the well-known activity scanning algorithm can be used to execute a P-ACD model. A prototype P-ACD simulator implemented in C# programming language is provided, and an illustrative example of a conveyor-driven serial production line with the prototype simulator is presented to illustrate construction and execution of a P-ACD model. In addition, it is demonstrated that the proposed P-ACD allows an effective and concise modeling of a job shop, which was not possible with the classical ACD.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2057490371",
    "type": "article"
  },
  {
    "title": "An Efficient Budget Allocation Approach for Quantifying the Impact of Input Uncertainty in Stochastic Simulation",
    "doi": "https://doi.org/10.1145/3129148",
    "publication_date": "2017-10-27",
    "publication_year": 2017,
    "authors": "Yuan Yi; Wei Xie",
    "corresponding_authors": "",
    "abstract": "Simulations are often driven by input models estimated from finite real-world data. When we use simulations to assess the performance of a stochastic system, there exist two sources of uncertainty in the performance estimates: input and simulation estimation uncertainty. In this article, we develop a budget allocation approach that can efficiently employ the potentially tight simulation resource to construct a percentile confidence interval quantifying the impact of the input uncertainty on the system performance estimates, while controlling the simulation estimation error. Specifically, nonparametric bootstrap is used to generate samples of input models quantifying both the input distribution family and parameter value uncertainty. Then, the direct simulation is used to propagate the input uncertainty to the output response. Since each simulation run could be computationally expensive, given a tight simulation budget, we propose an efficient budget allocation approach that can balance the finite sampling error introduced by using finite bootstrapped samples to quantify the input uncertainty and the system response estimation error introduced by using finite replications to estimate the system response at each bootstrapped sample. Our approach is theoretically supported, and empirical studies also demonstrate that it has better and more robust performance than direct bootstrapping.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2766871605",
    "type": "article"
  },
  {
    "title": "Doping Tests for Cyber-physical Systems",
    "doi": "https://doi.org/10.1145/3449354",
    "publication_date": "2021-07-31",
    "publication_year": 2021,
    "authors": "Sebastian Biewer; Pedro R. D’Argenio; Holger Hermanns",
    "corresponding_authors": "",
    "abstract": "The software running in embedded or cyber-physical systems is typically of proprietary nature, so users do not know precisely what the systems they own are (in)capable of doing. Most malfunctionings of such systems are not intended by the manufacturer, but some are, which means these cannot be classified as bugs or security loopholes. The most prominent examples have become public in the diesel emissions scandal, where millions of cars were found to be equipped with software violating the law, altogether polluting the environment and putting human health at risk. The behaviour of the software embedded in these cars was intended by the manufacturer, but it was not in the interest of society, a phenomenon that has been called software doping . Due to the unavailability of a specification, the analysis of doped software is significantly different from that for buggy or insecure software and hence classical verification and testing techniques have to be adapted. The work presented in this article builds on existing definitions of software doping and lays the theoretical foundations for conducting software doping tests, so as to enable uncovering unethical manufacturers. The complex nature of software doping makes it very hard to effectuate doping tests in practice. We explain the main challenges and provide efficient solutions to realise doping tests despite this complexity.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2971896296",
    "type": "article"
  },
  {
    "title": "Moment-Matching-Based Conjugacy Approximation for Bayesian Ranking and Selection",
    "doi": "https://doi.org/10.1145/3149013",
    "publication_date": "2017-10-31",
    "publication_year": 2017,
    "authors": "Qiong Zhang; Yongjia Song",
    "corresponding_authors": "",
    "abstract": "We study the conjugacy approximation method in the context of Bayesian ranking and selection with unknown correlations. Under the assumption of normal-inverse-Wishart prior distribution, the posterior distribution remains a normal-inverse-Wishart distribution thanks to the conjugacy property when all alternatives are sampled at each step. However, this conjugacy property no longer holds if only one alternative is sampled at a time, an appropriate setting when there is a limited budget on the number of samples. We propose two new conjugacy approximation methods based on the idea of moment matching. Both of them yield closed-form Bayesian prior updating formulas. This updating formula can then be combined with the knowledge gradient algorithm under the \"value of information\" framework. We conduct computational experiments to show the superiority of the proposed conjugacy approximation methods, including applications in wind farm placement and computer model calibration.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W3099199463",
    "type": "article"
  },
  {
    "title": "Fixed-Confidence, Fixed-Tolerance Guarantees for Ranking-and-Selection Procedures",
    "doi": "https://doi.org/10.1145/3432754",
    "publication_date": "2021-02-10",
    "publication_year": 2021,
    "authors": "David J. Eckman; Shane G. Henderson",
    "corresponding_authors": "",
    "abstract": "Ever since the conception of the statistical ranking-and-selection (R8S) problem, a predominant approach has been the indifference-zone (IZ) formulation. Under the IZ formulation, R8S procedures are designed to provide a guarantee on the probability of correct selection (PCS) whenever the performance of the best system exceeds that of the second-best system by a specified amount. We discuss the shortcomings of this guarantee and argue that providing a guarantee on the probability of good selection (PGS)—selecting a system whose performance is within a specified tolerance of the best—is a more justifiable goal. Unfortunately, this form of fixed-confidence, fixed-tolerance guarantee has received far less attention within the simulation community. We present an overview of the PGS guarantee with the aim of reorienting the simulation community toward this goal. We examine numerous techniques for proving the PGS guarantee, including sufficient conditions under which selection and subset-selection procedures that deliver the IZ-inspired PCS guarantee also deliver the PGS guarantee.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W3129771471",
    "type": "article"
  },
  {
    "title": "A Cross-Entropy Scheme for Mixtures",
    "doi": "https://doi.org/10.1145/2685030",
    "publication_date": "2015-01-14",
    "publication_year": 2015,
    "authors": "Hui Wang; Xiang Zhou",
    "corresponding_authors": "",
    "abstract": "We discuss how to generalize the classic cross-entropy method in the case where a family of mixture distributions, such as the mixture of multiple Gaussian modes, is used as an importance sampling distribution. A new iterative cross-entropy scheme, based on the idea of the EM method, is proposed to overcome the challenge of deciding the optimal weights for each mode in the mixture. Detailed studies of this new algorithm and its applications to the estimation of rainbow option prices are presented to demonstrate the efficiency of the scheme.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W1992050488",
    "type": "article"
  },
  {
    "title": "A Methodology to Model the Execution of Communication Software for Accurate Network Simulation",
    "doi": "https://doi.org/10.1145/2746233",
    "publication_date": "2015-07-21",
    "publication_year": 2015,
    "authors": "Stein Kristiansen; Thomas Plagemann; Vera Goebel",
    "corresponding_authors": "",
    "abstract": "Network simulation is commonly used to evaluate the performance of distributed systems, but these approaches do not account for the performance impact that protocol execution on nodes has on performance, which can be significant. We provide a methodology to extract from real devices models of communication software execution that can be used to extend network simulators to improve their accuracy. The models are obtained by instrumenting the target devices to obtain the events necessary to describe software execution. We specify which events must be captured, how to capture them, and how to transform the event traces into models that can be used to extend network simulators. The obtained models are based on high-level abstractions that can be used to describe the execution of a wide range of communication software, and the design principles to extend network simulators are not restricted to any specific network simulator. The same model of communication software execution can be used without modification in all discrete event-based network simulators that are extended according to our principles. The models are represented in a human-readable format that is suitable for modification and can therefore be used to predict how software modifications impact performance. We evaluate our models with two proof-of-concept extensions of Ns-3 that execute the models of two modern smartphones: the Google Nexus One (GN1) and the Nokia N900. We measure the accuracy of our models by comparing results from real experiments with those from simulations with our models and analyze the simulation overhead of our approach.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2048147807",
    "type": "article"
  },
  {
    "title": "Space-Time Matching Algorithms for Interest Management in Distributed Virtual Environments",
    "doi": "https://doi.org/10.1145/2567922",
    "publication_date": "2014-05-01",
    "publication_year": 2014,
    "authors": "Elvis S. Liu; Georgios Theodoropoulos",
    "corresponding_authors": "",
    "abstract": "Interest management in Distributed Virtual Environments (DVEs) is a data-filtering technique designed to reduce bandwidth consumption and therefore enhances the scalability of the system. This technique usually involves a process called interest matching , which determines what data should be sent to the participants as well as what data should be filtered. Although most of the existing interest matching approaches have been shown to meet their runtime performance requirements, they have a fundamental disadvantage—they perform interest matching at discrete time intervals. As a result, they would fail to report events between discrete timesteps. If participants of the DVE ignore these missing events, they would most likely perform incorrect simulations. This article presents a new approach called space-time interest matching , which aims to capture the missing events between discrete timesteps. Although this approach requires additional matching effort, a number of novel algorithms are developed to significantly improve its runtime efficiency.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2052809454",
    "type": "article"
  },
  {
    "title": "On Transience and Recurrence in Irreducible Finite-State Stochastic Systems",
    "doi": "https://doi.org/10.1145/2699721",
    "publication_date": "2015-05-08",
    "publication_year": 2015,
    "authors": "Peter W. Glynn; Peter J. Haas",
    "corresponding_authors": "",
    "abstract": "Long-run stochastic stability is a precondition for applying steady-state simulation output analysis methods to a discrete-event stochastic system, and is of interest in its own right. We focus on systems whose underlying stochastic process can be represented as a Generalized Semi-Markov Process (GSMP); a wide variety of stochastic systems fall within this framework. A fundamental stability requirement for an irreducible GSMP is that the states be “recurrent” in that the GSMP visits each state infinitely often with probability 1. We study recurrence properties of irreducible GSMPs with finite state space. Our focus is on the “clocks” that govern the occurrence of events, and we consider GSMPs in which zero, one, or at least two simultaneously active events can have clock-setting distributions that are “heavy tailed” in the sense that they have infinite mean. We establish positive recurrence, null recurrence, and, perhaps surprisingly, possible transience of states for these respective regimes. The transience result stands in strong contrast to Markovian or semi-Markovian GSMPs, where irreducibility and finiteness of the state space guarantee positive recurrence.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2085613018",
    "type": "article"
  },
  {
    "title": "A Personality-based Model of Emotional Contagion and Control in Crowd Queuing Simulations",
    "doi": "https://doi.org/10.1145/3577589",
    "publication_date": "2022-12-20",
    "publication_year": 2022,
    "authors": "Junxiao Xue; Mingchuang Zhang; Hui Yin",
    "corresponding_authors": "",
    "abstract": "Queuing is a frequent daily activity. However, long waiting lines equate to frustration and potential safety hazards. We present a novel, personality-based model of emotional contagion and control for simulating crowd queuing. Our model integrates the influence of individual personalities and interpersonal relationships. Through the interaction between the agents and the external environment parameters, the emotional contagion model based on well-known theories in psychology is used to complete the agents’ behavior planning and path planning function. We combine the epidemiological SIR model with the cellular automaton model to capture various emotional modelling for multi-agent simulations. The overall formulation involves different emotional parameters, such as patience, urgency, and friendliness, closely related to crowd queuing. In addition, to manage the order of the queue, governing agents are added to prevent the emotional outbreak. We perform qualitative and quantitative comparisons between our simulation results and real-world observations on various scenarios. Numerous experiments show that reasonably increasing the queue channel and adding governing agents can effectively improve the quality of queues.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W4312037467",
    "type": "article"
  },
  {
    "title": "Learning to Simulate Sequentially Generated Data via Neural Networks and Wasserstein Training",
    "doi": "https://doi.org/10.1145/3583070",
    "publication_date": "2023-04-03",
    "publication_year": 2023,
    "authors": "Tingyu Zhu; Haoyu Liu; Zeyu Zheng",
    "corresponding_authors": "",
    "abstract": "We propose a new framework of a neural network-assisted sequential structured simulator to model, estimate, and simulate a wide class of sequentially generated data. Neural networks are integrated into the sequentially structured simulators in order to capture potential nonlinear and complicated sequential structures. Given representative real data, the neural network parameters in the simulator are estimated and calibrated through a Wasserstein training process, without restrictive distributional assumptions. The target of Wasserstein training is to enforce the joint distribution of the simulated data to match the joint distribution of the real data in terms of Wasserstein distance. Moreover, the neural network-assisted sequential structured simulator can flexibly incorporate various kinds of elementary randomness and generate distributions with certain properties such as heavy-tail, without the need to redesign the estimation and training procedures. Further, regarding statistical properties, we provide results on consistency and convergence rate for the estimation procedure of the proposed simulator, which are the first set of results that allow the training data samples to be correlated. We then present numerical experiments with synthetic and real data sets to illustrate the performance of the proposed simulator and estimation procedure.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4362576976",
    "type": "article"
  },
  {
    "title": "DSMC Evaluation Stages: Fostering Robust and Safe Behavior in Deep Reinforcement Learning – Extended Version",
    "doi": "https://doi.org/10.1145/3607198",
    "publication_date": "2023-07-12",
    "publication_year": 2023,
    "authors": "Timo P. Gros; Joschka Groß; Daniel Höller; Jörg Hoffmann; Michaela Klauck; Hendrik Meerkamp; N. Müller; Lukas Schaller; Verena Wolf",
    "corresponding_authors": "",
    "abstract": "Neural networks (NN) are gaining importance in sequential decision-making. Deep reinforcement learning (DRL), in particular, is extremely successful in learning action policies in complex and dynamic environments. Despite this success, however, DRL technology is not without its failures, especially in safety-critical applications: (i) the training objective maximizes average rewards, which may disregard rare but critical situations and hence lack local robustness; (ii) optimization objectives targeting safety typically yield degenerated reward structures, which, for DRL to work, must be replaced with proxy objectives. Here, we introduce a methodology that can help to address both deficiencies. We incorporate evaluation stages (ES) into DRL, leveraging recent work on deep statistical model checking (DSMC), which verifies NN policies in Markov decision processes. Our ES apply DSMC at regular intervals to determine state space regions with weak performance. We adapt the subsequent DRL training priorities based on the outcome, (i) focusing DRL on critical situations and (ii) allowing to foster arbitrary objectives. We run case studies on two benchmarks. One of them is the Racetrack, an abstraction of autonomous driving that requires navigating a map without crashing into a wall. The other is MiniGrid, a widely used benchmark in the AI community. Our results show that DSMC-based ES can significantly improve both (i) and (ii).",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4384069847",
    "type": "article"
  },
  {
    "title": "Using Cache or Credit for Parallel Ranking and Selection",
    "doi": "https://doi.org/10.1145/3618299",
    "publication_date": "2023-09-04",
    "publication_year": 2023,
    "authors": "Harun Avcı; Barry L. Nelson; Eunhye Song; Andreas Wächter",
    "corresponding_authors": "",
    "abstract": "In this article, we focus on ranking and selection procedures that sequentially allocate replications to systems by applying some acquisition function. We propose an acquisition function, called gCEI, which exploits the gradient of the complete expected improvement with respect to the number of replications. We prove that the gCEI procedure, which adopts gCEI as the acquisition function in a serial computing environment, achieves the asymptotically optimal static replication allocation of Glynn and Juneja in the limit under a normality assumption. We also propose two procedures, called caching and credit, that extend any acquisition-function-based procedure in a serial environment into both synchronous and asynchronous parallel environments. While allocating replications to systems, both procedures use persistence forecasts for the unavailable outputs of the currently running replications, but differ in usage of the available outputs. We prove that, under certain assumptions, the caching procedure achieves the same asymptotic allocation as in the serial environment. A similar result holds for the credit procedure using gCEI as the acquisition function. In terms of efficiency and effectiveness, the credit procedure empirically performs as well as the caching procedure, despite not carefully controlling the output history as the caching procedure does, and is faster than the serial version without any number-of-replications penalty due to using persistence forecasts. Both procedures are designed to solve small-to-medium-sized problems on computers with a modest number of processors, such as laptops and desktops as opposed to high-performance clusters, and are superior to state-of-the-art parallel procedures in this setting.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4386410682",
    "type": "article"
  },
  {
    "title": "Divergence Reduction in Monte Carlo Neutron Transport with On-GPU Asynchronous Scheduling",
    "doi": "https://doi.org/10.1145/3626957",
    "publication_date": "2023-10-19",
    "publication_year": 2023,
    "authors": "Braxton Cuneo; Mike Bailey",
    "corresponding_authors": "",
    "abstract": "While Monte Carlo Neutron Transport (MCNT) is near-embarrasingly parallel, the effectively unpredictable lifetime of neutrons can lead to divergence when MCNT is evaluated on GPUs. Divergence is the phenomenon of adjacent threads in a warp executing different control flow paths; on GPUS, it reduces performance because each work group may only execute one path at a time. The process of Thread Data Remapping (TDR) resolves these discrepancies by moving data across hardware such that data in the same warp will be processed through similar paths. A common issue among prior implementations of TDR is the synchronous nature of its remapping and processing cycles, which exhaustively sort data produced by prior processing passes and exhaustively evaluate the sorted data. In another work, we defined a method of remapping data through an asynchronous scheduler which allows for work to be stored in shared memory and deferred arbitrarily until that work is a viable option for low-divergence evaluation. This article surveys a wider set of cases, with the goal of characterizing performance trends across a more comprehensive set of parameters. These parameters include cross sections of scattering/capturing/fission, use of implicit capture, source neutron counts, simulation time spans, and tuned memory allocations. Across these cases, we have recorded minimum and average execution times, as well as a heuristically tuned near-optimal memory allocation size for both synchronous and asynchronous scheduling. Across the collected data, it is shown that the asynchronous method is faster and more memory efficient in the majority of cases, and that it requires less tuning to achieve competitive performance.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W4387769983",
    "type": "article"
  },
  {
    "title": "Nearly optimal importance sampling for Monte Carlo simulation of loss systems",
    "doi": "https://doi.org/10.1145/369534.369541",
    "publication_date": "2000-10-01",
    "publication_year": 2000,
    "authors": "Pasi Lassila; Jorma Virtamo",
    "corresponding_authors": "",
    "abstract": "In this paper we consider the problem of estimating blocking probabili ties in the multiservice loss system via simulation, applying the static Monte Carlo method with importance sampling. Earlier approaches to this problem include the use of either a single exponentially twisted version of the steady state distribution of the system or a composite of individual exponentially twisted distributions. Here, a different approach is introduced, where the original estimation problem is first decomposed into independent simpler subproblems, each roughly corresponding to estimating the blocking probability contribution from a single link. Then two importance sampling distributions are presented, which very closely approximate the ideal importance sampling distribution for each subproblem. In both methods, the idea is to try to generate samples directly into the blocking state region. The difference between the methods is that the first method, the inverse convolution method, achieves this exactly, while the second one, using a fitted Gaussian distibution, only approximately. The inverse convolution algorithm, however, has a higher memory requirement. Finally, a dynamic control algorithm is given for optimally allocating the samples between different subproblems. The numerical results demonstrate that the variance reduction obtained with the methods, especially with the inverse convolution method, is tryly remarkable, between 670 and 1,000,000 in the examples under consideration.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W1999063931",
    "type": "article"
  },
  {
    "title": "Simplifying the modeling of multiple activities, multiple queuing, and interruptions",
    "doi": "https://doi.org/10.1145/159737.159755",
    "publication_date": "1993-10-01",
    "publication_year": 1993,
    "authors": "Ruth Davies; Robert M. O’Keefe; Huw Davies",
    "corresponding_authors": "",
    "abstract": "Most conventional discrete-event simulation software assumes a simple progression of entities through queues and activities. Such software cannot cope easily with modeling systems where entities can be present in more than one queue, can be involved in more than one activity (i.e., scheduled for more than one event), or can be interrupted while queuing or taking part in an activity in order to join another queue or take part in a different activity. Low-level data structures to address these problems have been implemented in Pascal by extending an existing suite of Pascal procedures, call Pascal_SIM. The problems and their solutions are discussed in the context of machine breakdown in a production system. Comparisons between the use of the new structures and the existing ones showed some gain in computational efficiency and considerable improvement in ease of modeling. The generality of the data structure is considered.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2075350552",
    "type": "article"
  },
  {
    "title": "Superfast parallel discrete event simulations",
    "doi": "https://doi.org/10.1145/232807.232818",
    "publication_date": "1996-04-01",
    "publication_year": 1996,
    "authors": "Albert Greenberg; Boris D. Lubachevsky; Isi Mitrani",
    "corresponding_authors": "",
    "abstract": "Nonconventional parallel simulations methods are presented, wherein speed-ups are not limited by the number of simulated components. The methods capitalize on Chandy and Sherman's space-time relaxation paradigm, and incorporate fast algorithms for solving recurrences. Special attention is paid to implementing these algorithms on currently available massively parallel SIMD computers. As examples, “superfast” simulations for open and closed queuing networks and for the slotted ALOHA communication protocol are discussed. Several of the simulations are implemented and show impressive computational speeds. This paper summarizes previous results of the authors and presents some new experiments and simulations.",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2076157327",
    "type": "article"
  },
  {
    "title": "A denotational semantics for a process-based simulation language",
    "doi": "https://doi.org/10.1145/290274.290303",
    "publication_date": "1998-07-01",
    "publication_year": 1998,
    "authors": "Chris Tofts; Graham Birtwistle",
    "corresponding_authors": "",
    "abstract": "In this article, we present semantic translations for the actions of μDemos, a proocess-based, discrete event simulation language. Our formal translation schema permits the automatic construction of a process algebraic representatioon of the underlying simulation model which can then be checked for freedom from deadlock and livelock, as well as system-specific safety and liveness properties. As simulation methodologies are increasingly being used to design and implement complex systems of interaction objects, the ability to perform such verifications in of increasing methodological importance. We also present a normal form for the syntactic construction of μDemos programs that allows for the direct comparison of such programs (two programs with the same normal form must execute in identical fashion), reduces model proof obligations by minimizing the number of language constructs, and permits an implementer to concentrate on the basic features of the language (since any program implementation that efficiently evaluates normal forms will be an efficient evaluator for the complete language).",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2084557509",
    "type": "article"
  },
  {
    "title": "Derivatives of likelihood ratios and smoothed perturbation analysis for the routing problem",
    "doi": "https://doi.org/10.1145/169702.169686",
    "publication_date": "1993-04-01",
    "publication_year": 1993,
    "authors": "Pierre Brémaud; Wenbo Gong",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Derivatives of likelihood ratios and smoothed perturbation analysis for the routing problem Authors: P. Brémaud Laboratoire des Signaux et Syste`mes CNRS-ESE, Gif sur Yvette, France Laboratoire des Signaux et Syste`mes CNRS-ESE, Gif sur Yvette, FranceView Profile , W.-B. Gong Univ. of Massachusetts, Amherst Univ. of Massachusetts, AmherstView Profile Authors Info & Claims ACM Transactions on Modeling and Computer SimulationVolume 3Issue 2April 1993 pp 134–161https://doi.org/10.1145/169702.169686Online:01 April 1993Publication History 11citation278DownloadsMetricsTotal Citations11Total Downloads278Last 12 Months1Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 17,
    "openalex_id": "https://openalex.org/W2088386141",
    "type": "article"
  },
  {
    "title": "Performance and fluid simulations of a novel shared buffer management system",
    "doi": "https://doi.org/10.1145/379525.379527",
    "publication_date": "2001-01-01",
    "publication_year": 2001,
    "authors": "Krishnan Kumaran; Debasis Mitra",
    "corresponding_authors": "",
    "abstract": "We consider a switching system that has multiple ports that share a common buffer, in which there is a FIFO logical queue for each port. Each port may support a large number of flows or connections, which are approximately homogeneous in their statistical characteristics, with common QoS requirements in cell loss and maximum delay. Heterogeneity may exist across ports. Our first contribution is a buffer management scheme based on Buffer Admission Control, which is integrated with Connection Admission Control at the switch. At the same time, this scheme is fair, efficient, and robust in sharing the buffer resources across ports. Our scheme is based on the resource-sharing technique of Virtual Partitioning. Our second major contribution is to advance the practice of discrete-event fluid simulations. Such simulations are approximations to cell-level simulations and offer orders of magnitude speed-up. A third contribution of the paper is the formulation and solution of a problem of optimal allocation of bandwidth and buffers to each port having specific delay bounds, in a lossless multiplexing framework. Finally, we report on extensive simulation results. The scheme is found to be effective, efficient, and robust.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2014166910",
    "type": "article"
  },
  {
    "title": "On the processor scheduling problem in time warp synchronization",
    "doi": "https://doi.org/10.1145/643114.643115",
    "publication_date": "2002-07-01",
    "publication_year": 2002,
    "authors": "Francesco Quaglia; Vittorio Cortellessa",
    "corresponding_authors": "",
    "abstract": "Time Warp is a synchronization mechanism for parallel/distributed simulation. It allows logical processes (LPs) to execute events without the guarantee of a causally consistent execution. Upon the detection of a causality violation, rollback procedures recover the state of the simulation to a correct value. When a rollback occurs there are two primary sources of performance loss: (1) CPU time must be spent for the execution of the rollback procedures and (2) waste of CPU time arises from the invalidation of event executions. In this paper we present a general framework for the problem of scheduling the next LP to be run on a processor in Time Warp simulations. The framework establishes a class of scheduling algorithms having the twofold aim to keep low the CPU time for the execution of the rollback procedures and also to guarantee low waste of time due to event executions invalidated by rollback. The combination of these two aims should actually lead to short completion time of Time Warp simulations.We collocate existing scheduling algorithms within the framework, pointing out how they miss previous aims, at least partially. Then we instantiate a Window-based Grain Sensitive (WGS) scheduling algorithm relying on the framework, which pursues the above twofold aim. We also identify the proper conditions, associated with the simulation model execution, under which any algorithm exploiting the framework structure is expected to benefit the performance of the Time Warp mechanism. Empirical evidence from an experimental study of WGS on classical benchmarks and on a mobile communication system simulation fully confirms the theoretical outcomes.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2033593488",
    "type": "article"
  },
  {
    "title": "Computer automated multi-paradigm modeling",
    "doi": null,
    "publication_date": "2002-01-01",
    "publication_year": 2002,
    "authors": "P.J. Mosterman; Hans Vangheluwe",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2993984500",
    "type": "article"
  },
  {
    "title": "Perfect sampling for queues and network models",
    "doi": "https://doi.org/10.1145/1122012.1122016",
    "publication_date": "2006-01-01",
    "publication_year": 2006,
    "authors": "Duncan J. Murdoch; Glen Takahara",
    "corresponding_authors": "",
    "abstract": "We review Propp and Wilson's [1996] CFTP algorithm and Wilson's [2000] ROCFTP algorithm. We then use these to construct perfect samplers for several queueing and network models: Poisson arrivals and exponential service times, several types of customers, and a trunk reservation protocol for accepting new customers; a similar protocol on a network switching model; a queue with a general arrival process; and a queue with both general arrivals and service times. Our samplers give effective ways to generate random samples from the steady-state distributions of these queues.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2022664864",
    "type": "article"
  },
  {
    "title": "The semi-regenerative method of simulation output analysis",
    "doi": "https://doi.org/10.1145/1147224.1147228",
    "publication_date": "2006-07-01",
    "publication_year": 2006,
    "authors": "James M. Calvin; Peter W. Glynn; Marvin K. Nakayama",
    "corresponding_authors": "",
    "abstract": "We develop a class of techniques for analyzing the output of simulations of a semi-regenerative process. Called the semi-regenerative method, the approach is a generalization of the regenerative method, and it can increase efficiency. We consider the estimation of various performance measures, including steady-state means, expected cumulative reward until hitting a set of states, derivatives of steady-state means, and time-average variance constants. We also discuss importance sampling and a bias-reduction technique. In each case, we develop two estimators: one based on a simulation of a single sample path, and the other a type of stratified estimator in which trajectories are generated in an independent and identically distributed manner. We establish a central limit theorem for each estimator so confidence intervals can be constructed.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2058785148",
    "type": "article"
  },
  {
    "title": "Replicated batch means variance estimators in the presence of an initial transient",
    "doi": "https://doi.org/10.1145/1176249.1176250",
    "publication_date": "2006-10-01",
    "publication_year": 2006,
    "authors": "Christos Alexopoulos; Sigrún Andradóttir; Nilay Tanık Argon; David Goldsman",
    "corresponding_authors": "",
    "abstract": "Independent replications (IR) and batch means (BM) are two of the most widely used variance-estimation methods for simulation output analysis. Alexopoulos and Goldsman conducted a thorough examination of IR and BM; and Andradóttir and Argon proposed the method of replicated batch means (RBM), which combines good characteristics of IR and BM. This article gives analy-tical results for the mean and variance of the RBM estimator for a class of processes having initial transients with an additive form. Along the way, we provide succinct complementary extensions of some of the results in the aforementioned papers. Our expressions explicitly show how the transient function affects estimator performance and suggest that in some cases, the RBM estimator is a good compromise choice with respect to bias and variance. However, care must be taken to avoid an excessive number of replications when the transient function is pervasive. An example involving a simple moving average process illustrates our findings.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2075712519",
    "type": "article"
  },
  {
    "title": "Efficient simulation of Internet worms",
    "doi": "https://doi.org/10.1145/1346325.1346326",
    "publication_date": "2008-04-01",
    "publication_year": 2008,
    "authors": "David M. Nicol",
    "corresponding_authors": "David M. Nicol",
    "abstract": "Simulation of Internet worms (and other malware) requires tremendous computing resources when every packet generated by the phenomena is modeled individually; on the other hand, models of worm growth based on differential equations lack the significant variability inherent in worms that sample targets randomly. This article addresses the problem with a model that focuses on times of infection. We propose a hybrid discrete-continuous model that minimizes execution time subject to an accuracy constraint on variance. We also develop an efficiently executed model of preferential random scanning and use it to investigate the sensitivity of worm propagation speed to the distribution of susceptible hosts through the network, and to the local preference probability. Finally, we propose and study two optimizations to a fluid-based simulation of scan traffic through a backbone network, observing an order-of-magnitude improvement in execution speed.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2030319179",
    "type": "article"
  },
  {
    "title": "Conversion of high-period random numbers to floating point",
    "doi": "https://doi.org/10.1145/1189756.1189759",
    "publication_date": "2007-01-01",
    "publication_year": 2007,
    "authors": "Jurgen A. Doornik",
    "corresponding_authors": "Jurgen A. Doornik",
    "abstract": "Conversion of unsigned 32-bit random integers to double precision floating point is discussed. It is shown that the standard practice can be unnecessarily slow and inflexible. It is argued that simulation experiments could benefit from making better use of the available precision.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2133079479",
    "type": "article"
  },
  {
    "title": "Analysis of nonstationary stochastic simulations using classical time-series models",
    "doi": "https://doi.org/10.1145/1502787.1502792",
    "publication_date": "2009-03-01",
    "publication_year": 2009,
    "authors": "Rita Marques Brandão; Acácio M. O. Porta Nova",
    "corresponding_authors": "",
    "abstract": "This article extends the use of classical autoregressive and moving average time-series models to the analysis of a variety of nonstationary discrete-event simulations. A thorough experimental evaluation shows that integrated and seasonal time-series models constitute very promising metamodels, especially for analyzing queueing system simulations under congested or cyclical traffic conditions. In some situations, stationarity-inducing transformations may be required before this methodology can be used. Our approach for efficient estimation of meaningful performance measures of selected responses in the target system is illustrated using a set of case studies taken from the simulation literature.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W1965732395",
    "type": "article"
  },
  {
    "title": "Probabilistic cost-effectiveness comparison of screening strategies for colorectal cancer",
    "doi": "https://doi.org/10.1145/1502787.1502789",
    "publication_date": "2009-03-01",
    "publication_year": 2009,
    "authors": "Ali Tafazzoli; Stephen D. Roberts; Robert W. Klein; Reid M. Ness; Robert S. Dittus",
    "corresponding_authors": "",
    "abstract": "A stochastic discrete-event simulation model of the natural history of Colorectal Cancer (CRC) is augmented with screening technology representations to create a base for simulating various screening strategies for CRC. The CRC screening strategies recommended by the American Gastroenterological Association (AGA) and the newest screening strategies for which clinical efficacy has been established are simulated. In addition to verification steps, validation of screening is pursued by comparison with the Minnesota Colon Cancer Control Study. The model accumulates discounted costs and quality-adjusted life-years. The natural variability in the modeled random variables for natural history is conditioned using a probabilistic sensitivity analysis through a two-stage sampling process that adds other random variables representing parametric uncertainty. The analysis of the screening alternatives in a low-risk population explores both deterministic and stochastic dominance to eliminate some screening alternatives. Net benefit analysis, based on willingness to pay for quality-adjusted life-years, is used to compare the most cost-effective strategies through acceptability curves and to make a screening recommendation. Methodologically, this work demonstrates how variability from the natural variation in the development, screening, and treatment of a disease can be combined with the variation in parameter uncertainty. Furthermore, a net benefit analysis that characterizes cost-effectiveness alternatives can explicitly depend on variation from all sources producing a probabilistic cost-effectiveness analysis of decision alternatives.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2023458464",
    "type": "article"
  },
  {
    "title": "State-dependent importance sampling for a Jackson tandem network",
    "doi": "https://doi.org/10.1145/1842713.1842718",
    "publication_date": "2010-09-01",
    "publication_year": 2010,
    "authors": "D.I. Miretskiy; Werner Scheinhardt; Michel Mandjes",
    "corresponding_authors": "",
    "abstract": "This article considers importance sampling as a tool for rare-event simulation. The focus is on estimating the probability of overflow in the downstream queue of a Jacksonian two-node tandem queue; it is known that in this setting “traditional” state-independent importance-sampling distributions perform poorly. We therefore concentrate on developing a state-dependent change of measure, that we prove to be asymptotically efficient. More specific contributions are the following. (i) We concentrate on the probability of the second queue exceeding a certain predefined threshold before the system empties. Importantly, we identify an asymptotically efficient importance-sampling distribution for any initial state of the system. (ii) The choice of the importance-sampling distribution is backed up by appealing heuristics that are rooted in large-deviations theory. (iii) The method for proving asymptotic efficiency relies on probabilistic arguments only. The article is concluded by simulation experiments that show a considerable speedup.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2139927803",
    "type": "article"
  },
  {
    "title": "Limit Theorems for Simulation-Based Optimization via Random Search",
    "doi": "https://doi.org/10.1145/2499913.2499915",
    "publication_date": "2013-07-01",
    "publication_year": 2013,
    "authors": "Yen Lin Chia; Peter W. Glynn",
    "corresponding_authors": "",
    "abstract": "This article develops fundamental theory related to the use of simulation-based nonadaptive random search as a means of optimizing a function that can be expressed as an expectation. Our results establish rates of convergence that express the trade-off between exploration and estimation, and fully characterize the limit distributions that arise. Our rates of convergence results should be viewed as a baseline against which to compare more intelligent algorithms.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2020329752",
    "type": "article"
  },
  {
    "title": "Reversible simulations of elastic collisions",
    "doi": "https://doi.org/10.1145/2457459.2457461",
    "publication_date": "2013-05-01",
    "publication_year": 2013,
    "authors": "Kalyan S. Perumalla; V. Protopopescu",
    "corresponding_authors": "",
    "abstract": "Consider a system of N identical hard spherical particles moving in a d -dimensional box and undergoing elastic, possibly multiparticle, collisions. We develop a new algorithm that recovers the precollision state from the post-collision state of the system, across a series of consecutive collisions, with essentially no memory overhead. The challenge in achieving reversibility for an n -particle collision (where, in general, n ≪ N ) arises from the presence of nd - d - 1 degrees of freedom (arbitrary angles) during each collision, as well as from the complex geometrical constraints placed on the colliding particles. To reverse the collisions in a traditional simulation setting, all of the particular realizations of these degrees of freedom (angles) during the forward simulation must be tracked. This requires memory proportional to the number of collisions, which grows very fast with N and d , thereby severely limiting the de facto applicability of the scheme. This limitation is addressed here by first performing a pseudorandomization of angles, which ensures determinism in the reverse path for any values of n and d . To address the more difficult problem of geometrical and dynamic constraints, a new approach is developed which correctly samples the constrained phase space. Upon combining the pseudorandomization with correct phase space sampling, perfect reversibility of collisions is achieved, as illustrated for n ≤ 3, d = 2, and n = 2, d = 3. This result enables, for the first time, reversible simulations of elastic collisions with essentially zero memory accumulation. In principle, the approach presented here could be generalized to larger values of n . The reverse computation methodology presented here uncovers important issues of irreversibility in conventional models, and the difficulties encountered in arriving at a reversible model for one of the most basic and widely used physical system processes, namely, elastic collisions for hard spheres. Insights and solution methodologies, with regard to accurate phase space coverage with reversible random sampling proposed in this context, can help serve as models and/or starting points for other reversible simulations.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2097093666",
    "type": "article"
  },
  {
    "title": "Guest Editorial for the Special Issue on FORmal methods for the quantitative Evaluation of Collective Adaptive SysTems (FORECAST)",
    "doi": "https://doi.org/10.1145/3177772",
    "publication_date": "2018-02-22",
    "publication_year": 2018,
    "authors": "Maurice H. ter Beek; Michele Loreti",
    "corresponding_authors": "",
    "abstract": "research-article Free Access Share on Guest Editorial for the Special Issue on FORmal methods for the quantitative Evaluation of Collective Adaptive SysTems (FORECAST) Authors: Maurice H. Ter Beek Consiglio Nazionale delle Ricerche (CNR), Istituto di Scienza e Tecnologie dell’Informazione (ISTI), Pisa (PI), Italy Consiglio Nazionale delle Ricerche (CNR), Istituto di Scienza e Tecnologie dell’Informazione (ISTI), Pisa (PI), Italy 0000-0002-2930-6367View Profile , Michele Loreti Università di Camerino, Scuola di Scienze e Tecnologie, Camerino (MC), Italy Università di Camerino, Scuola di Scienze e Tecnologie, Camerino (MC), Italy 0000-0003-3061-863XView Profile Authors Info & Claims ACM Transactions on Modeling and Computer SimulationVolume 28Issue 2April 2018 Article No.: 8pp 1–4https://doi.org/10.1145/3177772Published:22 February 2018Publication History 3citation307DownloadsMetricsTotal Citations3Total Downloads307Last 12 Months33Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2790170367",
    "type": "editorial"
  },
  {
    "title": "Efficient Parallel Simulation over Large-scale Social Contact Networks",
    "doi": "https://doi.org/10.1145/3265749",
    "publication_date": "2019-04-18",
    "publication_year": 2019,
    "authors": "Yulin Wu; Wentong Cai; Zengxiang Li; Wen Jun Tan; Xiangting Hou",
    "corresponding_authors": "",
    "abstract": "Social contact network (SCN) models the daily contacts between people in real life. It consists of agents and locations. When agents visit a location at the same time, the social interactions can be established among them. Simulations over SCN have been employed to study social dynamics such as disease spread among population. Because of the scale of SCN and the execution time requirement, the simulations are usually run in parallel. However, a challenge to the parallel simulation is that the structure of SCN is naturally skewed with a few hub locations that have far more visitors than others. These hub locations can cause load imbalance and heavy communication between partitions, which therefore impact the simulation performance. This article proposes a comprehensive solution to address this challenge. First, the hub locations are decomposed into small locations, so that SCN can be divided into partitions with better balanced workloads. Second, the agents are decomposed to exploit data locality, so that the overall communication across partitions can be greatly reduced. Third, two enhanced execution mechanisms are designed for locations and agents, respectively, to improve simulation parallelism. To evaluate the efficiency of the proposed solution, an epidemic simulation was developed and extensive experiments were conducted on two computer clusters using three SCN datasets with different scales. The results demonstrate that our approach can significantly improve the execution performance of the simulation.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2938848790",
    "type": "article"
  },
  {
    "title": "Random Variate Generation for Exponential and Gamma Tilted Stable Distributions",
    "doi": "https://doi.org/10.1145/3449357",
    "publication_date": "2021-07-23",
    "publication_year": 2021,
    "authors": "Yan Qu; Angelos Dassios; Hongbiao Zhao",
    "corresponding_authors": "",
    "abstract": "We develop a new efficient simulation scheme for sampling two families of tilted stable distributions: exponential tilted stable (ETS) and gamma tilted stable (GTS) distributions. Our scheme is based on two-dimensional single rejection. For the ETS family, its complexity is uniformly bounded over all ranges of parameters. This new algorithm outperforms all existing schemes. In particular, it is more efficient than the well-known double rejection scheme, which is the only algorithm with uniformly bounded complexity that we can find in the current literature. Beside the ETS family, our scheme is also flexible to be further extended for generating the GTS family, which cannot easily be done by extending the double rejection scheme. Our algorithms are straightforward to implement, and numerical experiments and tests are conducted to demonstrate the accuracy and efficiency.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W3155596100",
    "type": "article"
  },
  {
    "title": "Drug Resistance or Re-Emergence? Simulating Equine Parasites",
    "doi": "https://doi.org/10.1145/2627736",
    "publication_date": "2014-08-13",
    "publication_year": 2014,
    "authors": "Jie Xu; Anand N. Vidyashankar; Martin K. Nielsen",
    "corresponding_authors": "",
    "abstract": "Emerging drug resistance in parasitology and its impact on human and animal health are of serious concern. Attempts by the parasitology community to address this issue led to the introduction of so-called selective therapy where a proportion of the population is left untreated. This has led to re-emergence of parasites that have heretofore been controlled. Using stochastic simulations, this article explores the tradeoff between drug resistance and re-emergence. More importantly, the article identifies the importance of the parasite fitness parameter vector and its role in drug resistance. Suggestions for further biological work and statistical analyses are also provided.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2083277907",
    "type": "article"
  },
  {
    "title": "A restricted multinomial hybrid selection procedure",
    "doi": "https://doi.org/10.1145/2567891",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "Hélcio Vieira; Susan M. Sanchez; Paul J. Sánchez; Karl Heinz Kienitz; Mischel Carmen Neyra Belderrain",
    "corresponding_authors": "",
    "abstract": "Analysts using simulation models often must assess a large number of alternatives in order to determine which are most effective. If effectiveness corresponds to the likelihood of yielding the best outcome, this becomes a multinomial selection problem. Unfortunately, existing procedures were developed primarily for evaluating small sets of alternatives, so parameters required to implement them may not be readily available or the sampling costs may be prohibitive when a large number of alternatives are present. We propose a truncated, sequential multinomial subset selection procedure that restricts the maximum subset size. Numerical comparisons show that our procedure can be much more efficient than the leading unrestricted procedure. Our procedure requires only one calculated parameter rather than four. We provide extensive tables for cases involving large numbers of alternatives.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2089867914",
    "type": "article"
  },
  {
    "title": "Actor-Critic Algorithms with Online Feature Adaptation",
    "doi": "https://doi.org/10.1145/2868723",
    "publication_date": "2016-02-09",
    "publication_year": 2016,
    "authors": "K. J. Prabuchandran; Shalabh Bhatnagar; Vivek S. Borkar",
    "corresponding_authors": "",
    "abstract": "We develop two new online actor-critic control algorithms with adaptive feature tuning for Markov Decision Processes (MDPs). One of our algorithms is proposed for the long-run average cost objective, while the other works for discounted cost MDPs. Our actor-critic architecture incorporates parameterization both in the policy and the value function. A gradient search in the policy parameters is performed to improve the performance of the actor. The computation of the aforementioned gradient, however, requires an estimate of the value function of the policy corresponding to the current actor parameter. The value function, on the other hand, is approximated using linear function approximation and obtained from the critic. The error in approximation of the value function, however, results in suboptimal policies. In our article, we also update the features by performing a gradient descent on the Grassmannian of features to minimize a mean square Bellman error objective in order to find the best features. The aim is to obtain a good approximation of the value function and thereby ensure convergence of the actor to locally optimal policies. In order to estimate the gradient of the objective in the case of the average cost criterion, we utilize the policy gradient theorem, while in the case of the discounted cost objective, we utilize the simultaneous perturbation stochastic approximation (SPSA) scheme. We prove that our actor-critic algorithms converge to locally optimal policies. Experiments on two different settings show performance improvements resulting from our feature adaptation scheme.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2256989395",
    "type": "article"
  },
  {
    "title": "Efficient Flattening Algorithm for Hierarchical and Dynamic Structure Discrete Event Models",
    "doi": "https://doi.org/10.1145/2875356",
    "publication_date": "2016-02-22",
    "publication_year": 2016,
    "authors": "Jang Won Bae; Sang Won Bae; Il‐Chul Moon; Tag Gon Kim",
    "corresponding_authors": "",
    "abstract": "Discrete event models are widely used to replicate, analyze, and understand complex systems. DEVS (Discrete Event System Specification) formalism enables hierarchical modeling, so it provides an efficiency in the model development of complex models. However, the hierarchical modeling incurs prolonged simulation executions due to indirect event exchanges through the model hierarchy. Although direct event paths are applied to mitigate this overhead, the situation becomes even worse when a model changes its structures during simulation execution, called a dynamic structure model. This article suggests Coupling Relation Graph (CRG) and Strongly Coupled Component (SCC) concepts to improve hierarchical and dynamic structure DEVS simulation execution. CRG is a directed graph representing DEVS model structure, and SCC is a group of connected components in a CRG. Using CRG and SCC, this article presents (1) how to develop CRG from a DEVS model and (2) how to construct and update direct event paths with respect to dynamic structural changes. In particular, compared to the previous works, the proposed method focuses on the reduction of the updating costs for the direct event paths. Through theoretical and empirical analyses, this article shows that the proposed method significantly reduces the simulation execution time, especially when a simulation model contains lots of components and changes its model structures frequently. We expect that the proposed method would support the faster simulation executions of complex hierarchical and dynamic structure models.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2280517751",
    "type": "article"
  },
  {
    "title": "MTSS -- A Marine Traffic Simulation System and Scenario Studies for a Major Hub Port",
    "doi": "https://doi.org/10.1145/2897512",
    "publication_date": "2016-08-02",
    "publication_year": 2016,
    "authors": "Shell Ying Huang; Wen Jing Hsu; Hui Fang; Tiancheng Song",
    "corresponding_authors": "",
    "abstract": "Hub ports need to ensure that their navigational networks can fulfill increased demand in marine traffic. They also need to assess the possible impacts of an accident resulting in partial or complete closure of navigation channels. For lack of adequate analytical tools, modeling and simulation are the only means for such studies. To date, however, no adequate simulation tool exists for modeling and simulating the complex traffic at a large-scale hub port. The challenge is to efficiently model the large number of interacting vessels while accurately reflecting the navigational behaviors of various types of vessels whose movements must comply with prevailing protocols in a location- and situation-aware fashion. We present a systematic approach that enables the construction of a marine traffic simulation system called MTSS. MTSS was calibrated based on detailed analysis of historical records obtained from a major hub port, and it was validated by the domain experts. MTSS was used in a capacity study of marine traffic at a hub port that is unique in the scale and complexity of its waterway networks, the intricacies of its traffic patterns, and the required accuracy of the navigational behaviors of different types of vessels. The usefulness of MTSS is further demonstrated by applying it to assess the impacts of partial closure of a waterway under an emergency scenario. For large-scale hub ports, MTSS now opens up new possibilities of realistic scenario studies and disruption management.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2507022166",
    "type": "article"
  },
  {
    "title": "A Method Using Generative Adversarial Networks for Robustness Optimization",
    "doi": "https://doi.org/10.1145/3503511",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Niclas Feldkamp; Sören Bergmann; Florian Conrad; Steffen Straßburger",
    "corresponding_authors": "",
    "abstract": "The evaluation of robustness is an important goal within simulation-based analysis, especially in production and logistics systems. Robustness refers to setting controllable factors of a system in such a way that variance in the uncontrollable factors (noise) has minimal effect on a given output. In this paper, we present an approach for optimizing robustness based on deep generative models, a special method of deep learning. We propose a method consisting of two Generative Adversarial Networks (GANs) to generate optimized experiment plans for the decision factors and the noise factors in a competitive, turn-based game. In a case study, the proposed method is tested and compared to traditional methods for robustness analysis including Taguchi method and Response Surface Method.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W4214856323",
    "type": "article"
  },
  {
    "title": "Theory and application of Marsaglia's monkey test for pseudorandom number generators",
    "doi": "https://doi.org/10.1145/210330.210331",
    "publication_date": "1995-04-01",
    "publication_year": 1995,
    "authors": "Ora E. Percus; Paula A. Whitlock",
    "corresponding_authors": "",
    "abstract": "A theoretical analysis is given for a new test, the “Monkey” test, for pseudorandom number sequences, which was proposed by Marsaglia. Selected results, using the test on several pseudorandom number generators in the literature, are also presented.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W1973335145",
    "type": "article"
  },
  {
    "title": "Intelligent Manufacturing-Simulation Agents Tool (IMSAT)",
    "doi": "https://doi.org/10.1145/151527.151548",
    "publication_date": "1993-01-02",
    "publication_year": 1993,
    "authors": "Gajanana Nadoli; John E. Biegel",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Intelligent Manufacturing-Simulation Agents Tool (IMSAT) Authors: Gajanana Nadoli Intel Corp., San Jose, CA Intel Corp., San Jose, CAView Profile , John E. Biegel Univ. of Central Florida, Orlando Univ. of Central Florida, OrlandoView Profile Authors Info & Claims ACM Transactions on Modeling and Computer SimulationVolume 3Issue 1Jan. 1993 pp 42–65https://doi.org/10.1145/151527.151548Published:02 January 1993Publication History 13citation862DownloadsMetricsTotal Citations13Total Downloads862Last 12 Months18Last 6 weeks5 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2069286673",
    "type": "article"
  },
  {
    "title": "Combining antithetic variates and control variates in simulation experiments",
    "doi": "https://doi.org/10.1145/240896.240899",
    "publication_date": "1996-10-01",
    "publication_year": 1996,
    "authors": "Wei‐Ning Yang; Wei-Win Liou",
    "corresponding_authors": "",
    "abstract": "Antithetic variates and control variates are two well-known variance reduction techniques. We consider combining antithetic variates and control variates to estimate the mean response in a stochastic simulation experiment. When applying antithetic variates to generate control variates across paired replications, we show that the integrated control-variate estimator is unbiased and yields, under the assumption of common correlations induced for all control variates, a smaller variance than the conventional control-variate estimator without using antithetic variates. We examine the proposed estimator and two alternative integrated control-variate estimators when applying antithetic variates on control variates and show that the proposed estimator is the optimal integrated control-variate estimator We implement these three integrated control-variate estimators and the conventional control-variate estimator in a simulation model of a stochastic network to evaluate the performance of each control-variate estimator Empirical results show that the proposed estimator outperforms the other control-variate estimators.",
    "cited_by_count": 16,
    "openalex_id": "https://openalex.org/W2088897998",
    "type": "article"
  },
  {
    "title": "The Purdue University network-computing hubs",
    "doi": "https://doi.org/10.1145/353735.353738",
    "publication_date": "2000-01-01",
    "publication_year": 2000,
    "authors": "Nirav H. Kapadia; J.A.B. Fortes; Mark Lundstrom",
    "corresponding_authors": "",
    "abstract": "This paper describes the Web interface management infrastructure of a functioning network-computing system (PUNCH) that allows users to run unmodified simulation packages at geographically dispersed sites. The system currently contains more than fifty university and commercial simulation tools, and has been used to carry out more than two hundred thousand simulations via the World Wide Web. Dynamically-constructed virtual URLs allow the Web interface management infrastructure to support the semantics associated with an interface to computing services without requiring any changes to Web browsers or WWW protocols. Virtual URLs also facilitate customizable control of access to networked resources. Simulation tools with text-based interfaces are supported via dynamically-generated, virtual interfaces, whereas tools with graphical interfaces are supported by leveraging available remote display-management technologies. Virtual interface generation and interactivity emulation are handled by a programmable state machine in conjunction with a mechanism to embed variables and ovjects within standard HTML.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2013993676",
    "type": "article"
  },
  {
    "title": "The hierarchical simulation language HSL",
    "doi": "https://doi.org/10.1145/116890.116911",
    "publication_date": "1991-04-01",
    "publication_year": 1991,
    "authors": "D. Peter Sanderson; Ravi Sharma; Roman Rozin; Siegfried Treu",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on The hierarchical simulation language HSL: a versatile tool for process-oriented simulation Authors: D. P. Sanderson Univ. of Pittsburgh, Pittsburgh, PA Univ. of Pittsburgh, Pittsburgh, PAView Profile , R. Sharma Univ. of Pittsburgh, Pittsburgh, PA Univ. of Pittsburgh, Pittsburgh, PAView Profile , R. Rozin Univ. of Pittsburgh, Pittsburgh, PA Univ. of Pittsburgh, Pittsburgh, PAView Profile , S. Treu Univ. of Pittsburgh, Pittsburgh, PA Univ. of Pittsburgh, Pittsburgh, PAView Profile Authors Info & Claims ACM Transactions on Modeling and Computer SimulationVolume 1Issue 2April 1991 pp 113–153https://doi.org/10.1145/116890.116911Published:01 April 1991Publication History 11citation396DownloadsMetricsTotal Citations11Total Downloads396Last 12 Months9Last 6 weeks4 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2022200118",
    "type": "article"
  },
  {
    "title": "Empirical performance of bias-reducing estimators for regenerative steady-state simulations",
    "doi": "https://doi.org/10.1145/1029174.1029175",
    "publication_date": "2004-10-01",
    "publication_year": 2004,
    "authors": "Ming‐Hua Hsieh; Donald L. Iglehart; Peter W. Glynn",
    "corresponding_authors": "",
    "abstract": "When simulating a stochastic system, simulationists often are interested in estimating various steady-state performance measures. The classical point estimator for such a measure involves simply taking the time average of an appropriate function of the process being simulated. Since the simulation can not be initiated with the (unknown) steady-state distribution, the classical point estimator is generally biased. In the context of regenerative steady-state simulation, a variety of other point estimators have been developed in an attempt to minimize the bias. In this paper, we provide an empirical comparison of these estimators in the context of four different continuous-time Markov chain models. The bias of the point estimators and the coverage probabilities of the associated confidence intervals are reported for the four models. Conclusions are drawn from this experimental work as to which methods are most effective in reducing bias.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2085308126",
    "type": "article"
  },
  {
    "title": "Improving scalability of wireless network simulation with bounded inaccuracies",
    "doi": "https://doi.org/10.1145/1176249.1176251",
    "publication_date": "2006-10-01",
    "publication_year": 2006,
    "authors": "Zhengrong Ji; Junlan Zhou; Mineo Takai; Rajive Bagrodia",
    "corresponding_authors": "",
    "abstract": "Discrete event network simulators have emerged as popular tools for verification and performance evaluation of wireless networks. Nevertheless, the desire to model such networks at high fidelity implies high computational costs, limiting most researchers the ability to simulate networks with thousands of nodes. Previous attempts to optimize simulation of large-scale wireless networks have not appropriately modeled accumulation of weak interference, thereby suffering inaccuracies that may be further magnified in the evaluation of upper-layer protocols. This article presents a comprehensive analysis on the effects of common optimization techniques for large-scale wireless network simulation on the overall network performance. Based on the analysis, it formulates distance limit derivation and mobility update reduction that introduce bounded inaccuracy to the radio propagation simulation. It further proposes a novel technique, Lazy Event Scheduling with Corrective Retrospection, that reduces simulation events twenty-five fold without introducing any inaccuracy at all. The experimental results show that these optimizations can substantially improve the runtime performance of an already efficient wireless network simulator, by a factor of up to 55 for wireless networks with 3200 nodes without compromising the simulation's accuracy.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2080131229",
    "type": "article"
  },
  {
    "title": "Large-scale testing of the Internet's Border Gateway Protocol (BGP) via topological scale-down",
    "doi": "https://doi.org/10.1145/1371574.1371577",
    "publication_date": "2008-07-01",
    "publication_year": 2008,
    "authors": "Glenn Carl; George Kesidis",
    "corresponding_authors": "",
    "abstract": "The Internet is a critical communication infrastructure servicing billions of end-users world-wide. Ongoing studies of the Internet's operations show that data loss and increased latency are occurring due to weaknesses in its interdomain routing protocol, BGP. Many solutions have been proposed, but few have experienced widespread adoption. Both the delayed discovery of the protocol's shortcomings, and apathy for its proposed solutions, are partially due to inadequate testing practices. Internet interdomain routing technologies are not evaluated at appropriate scale. Better testing is suggested, which incorporates the specification of large-scale experimental topologies. This is necessary, as BGP performs the distributed operation of interdomain routing across the thousands of networks composing the Internet. However, only small to moderately sized topologies can be currently accommodated by today's testing platforms. A modeling methodology based on path preserving scale-down is proposed to extend the topological scale of interdomain routing experimentation. A given Internet topology is reduced in terms of its autonomous systems (ASes) using a combination of Gaussian elimination and several graphical heuristics. The interdomain routing paths generated by BGP on this reduced topology are also preserved. Path preservation keeps the length, composition, and ordering of these routing paths unchanged. When the routing paths guiding Internet traffic among ASes are preserved across the size reduction, the large-scale traffic engineering induced by BGP can be estimated at much lower scales. Internet data losses due to certain inappropriate interdomain routing behaviors can be identified. As an example, a persistent multiple origin autonomous system (MOAS) conflict is characterized over a topology containing 8826 ASes. It is shown that this problem's large-scale characterization can be obtained using scale-down models that are 70% smaller, and thus more accommodating to common testing platforms (e.g., simulation and networking testbeds).",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2013211561",
    "type": "article"
  },
  {
    "title": "Modeling, scheduling, and simulation of switched processing systems",
    "doi": "https://doi.org/10.1145/1371574.1371578",
    "publication_date": "2008-07-01",
    "publication_year": 2008,
    "authors": "Ying‐Chao Hung; George Michailidis",
    "corresponding_authors": "",
    "abstract": "Switched Processing Systems (SPS) serve as canonical models in a wide area of applications such as high performance computing, wireless networking, call centers, and flexible manufacturing. In this article, we model the SPS by considering both slotted and continuous time and analyze it under fairly mild stochastic assumptions. Two classes of scheduling policies are introduced and shown to maximize the throughput and maintain strong stability of the system. In addition, their performance with respect to the average job sojourn time is examined by simulating small SPS subject to different types of input traffic. By utilizing the simulation result of the proposed policies, a hybrid control policy is constructed to reduce the average job sojourn time when the system has unknown and changing input loads.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W1971511656",
    "type": "article"
  },
  {
    "title": "Optimal parameter trajectory estimation in parameterized SDEs",
    "doi": "https://doi.org/10.1145/1502787.1502791",
    "publication_date": "2009-03-01",
    "publication_year": 2009,
    "authors": "Shalabh Bhatnagar; Karmeshu; Vivek Mishra",
    "corresponding_authors": "",
    "abstract": "We consider the problem of estimating the optimal parameter trajectory over a finite time interval in a parameterized stochastic differential equation (SDE), and propose a simulation-based algorithm for this purpose. Towards this end, we consider a discretization of the SDE over finite time instants and reformulate the problem as one of finding an optimal parameter at each of these instants. A stochastic approximation algorithm based on the smoothed functional technique is adapted to this setting for finding the optimal parameter trajectory. A proof of convergence of the algorithm is presented and results of numerical experiments over two different settings are shown. The algorithm is seen to exhibit good performance. We also present extensions of our framework to the case of finding optimal parameterized feedback policies for controlled SDE and present numerical results in this scenario as well.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W1982273773",
    "type": "article"
  },
  {
    "title": "Cross-layer design for efficient resource utilization in wimedia UWB-based WPANs",
    "doi": "https://doi.org/10.1145/1870085.1870093",
    "publication_date": "2010-12-01",
    "publication_year": 2010,
    "authors": "Raed T. Al‐Zubi; Marwan Krunz",
    "corresponding_authors": "",
    "abstract": "Ultra-WideBand (UWB) communications has emerged as a promising technology for high data rate Wireless Personal Area Networks (WPANs). In this article, we address two key issues that impact the performance of a multihop UWB-based WPAN: throughput and transmission range. Arbitrary selection of routes in such a network may result in reserving an unnecessarily long channel time, and hence low network throughput and high blocking rate for prospective reservations. To remedy this situation, we propose a novel cross-layer resource allocation design. At the core of this design is a routing technique (called RTERU) that uses the allocated channel time as a routing metric. RTERU exploits the dependence of this metric on the multiple-rate capability of an UWB system. We show that selecting the route that consumes the minimum channel time while satisfying a target packet delivery probability over the selected route is an NP-hard problem. Accordingly, RTERU resorts to approximate path selection algorithms (implemented proactively and reactively) to find near-optimal solutions at reasonable computational/communication overhead. We further enhance the performance of RTERU by integrating into its design a packet overhearing capability. Simulations are used to demonstrate the performance of our proposed solutions.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2012262346",
    "type": "article"
  },
  {
    "title": "Reduction of closed queueing networks for efficient simulation",
    "doi": "https://doi.org/10.1145/1540530.1540531",
    "publication_date": "2009-06-01",
    "publication_year": 2009,
    "authors": "John Shortle; Brian L. Mark; Donald Gross",
    "corresponding_authors": "",
    "abstract": "This article gives several methods for approximating a closed queueing network with a smaller one. The objective is to reduce the simulation time of the network. We consider Jackson-like networks with Markovian routing and with general service distributions. The basic idea is to first divide the network into two parts—the core nodes of interest and the remaining nodes. We suppose that only metrics at the core nodes are of interest. The remaining nodes are collapsed into a reduced set of nodes, in an effort to approximate the flows into and out of the set of core nodes. The core nodes and their interactions are preserved in the reduced network. We test the network reductions for accuracy and speed. By randomly generating sample networks, we test the reductions on a large variety of test networks, rather than on a few specific cases. The main conclusion is that the reductions work well when the squared coefficients of variation of the service distributions are not all small (that is, the network is not close to being deterministic) and for nodes where the utilization is not too high or too low.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W2044237613",
    "type": "article"
  },
  {
    "title": "Posterior Expectation of Regularly Paved Random Histograms",
    "doi": "https://doi.org/10.1145/2414416.2414422",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Raazesh Sainudiin; Gloria Teng; Jennifer Harlow; Dominic Lee",
    "corresponding_authors": "",
    "abstract": "We present a novel method for averaging a sequence of histogram states visited by a Metropolis-Hastings Markov chain whose stationary distribution is the posterior distribution over a dense space of tree-based histograms. The computational efficiency of our posterior mean histogram estimate relies on a statistical data-structure that is sufficient for nonparametric density estimation of massive, multidimensional metric data. This data-structure is formalized as statistical regular paving (SRP). A regular paving (RP) is a binary tree obtained by selectively bisecting boxes along their first widest side. SRP augments RP by mutably caching the recursively computable sufficient statistics of the data. The base Markov chain used to propose moves for the Metropolis-Hastings chain is a random walk that data-adaptively prunes and grows the SRP histogram tree. We use a prior distribution based on Catalan numbers and detect convergence heuristically. The performance of our posterior mean SRP histogram is empirically assessed for large sample sizes simulated from several multivariate distributions that belong to the space of SRP histograms.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W1982072204",
    "type": "article"
  },
  {
    "title": "Information models for queueing system simulation",
    "doi": "https://doi.org/10.1145/1734222.1734224",
    "publication_date": "2010-04-01",
    "publication_year": 2010,
    "authors": "Thomas Roeder; Lee W. Schruben",
    "corresponding_authors": "",
    "abstract": "When planning simulations of large-scale systems, it is important to anticipate what information is required to model the system and obtain desired output. This can be done without tying the study to a specific simulation package or language. It is valuable to do so to avoid unnecessarily long development and execution times. In this article, we offer a simulation information model (SIM) designed to help organize system information in the early stages of a project. (It can also be used to analyze existing models.) The SIM allows complexity analysis of the system to be performed, and may lead to a better selection of simulation language. The SIM is illustrated using two examples, and its relationship to current formalisms is discussed.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W1990411408",
    "type": "article"
  },
  {
    "title": "A Framework for Selecting a Selection Procedure",
    "doi": "https://doi.org/10.1145/2331140.2331144",
    "publication_date": "2012-08-01",
    "publication_year": 2012,
    "authors": "Rolf Waeber; Peter I. Frazier; Shane G. Henderson",
    "corresponding_authors": "",
    "abstract": "For many discrete simulation optimization applications, it is often difficult to decide which Ranking and Selection (R&amp;S) procedure to use. To efficiently compare R&amp;S procedures, we present a three-layer performance evaluation process. We show that the two most popular performance formulations, namely the Bayesian formulation and the indifference zone formulation, have a common representation analogous to convex risk measures used in mathematical finance. We then specify how a decision maker can impose a performance requirement on R&amp;S procedures that is more adequate for her risk attitude than the indifference zone or the Bayesian performance requirements. Such a performance requirement partitions the space of R&amp;S procedures into acceptable and nonacceptable procedures. The minimal computational budget required for a procedure to become acceptable introduces an easy-to-interpret preference order on the set of R&amp;S policies. We demonstrate with a numerical example how the introduced framework can be used to guide the choice of selection procedure in practice.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2007492057",
    "type": "article"
  },
  {
    "title": "Modeling BitTorrent-like systems with many classes of users",
    "doi": "https://doi.org/10.1145/2457459.2457462",
    "publication_date": "2013-05-01",
    "publication_year": 2013,
    "authors": "Wei-Cherng Liao; Fragkiskos Papadopoulos; Konstantinos Psounis; Constantinos Psomas",
    "corresponding_authors": "",
    "abstract": "BitTorrent is one of the most successful peer-to-peer systems. Researchers have studied a number of aspects of the system, including its scalability, performance, efficiency and fairness. However, the complexity of the system has forced most prior analytical work to make a number of simplifying assumptions, for example, user homogeneity, or even ignore some central aspects of the protocol altogether, for example, the rate-based Tit-for-Tat (TFT) unchoking scheme, in order to keep the analysis tractable. Motivated by this, in this article we propose two analytical models that accurately predict the performance of the system while considering the central details of the BitTorrent protocol. Our first model is a steady-state one, in the sense that it is valid during periods of time where the number of users remains fixed. Freed by the complications of user time-dynamics, we account for many of the central details of the BitTorrent protocol and accurately predict a number of performance metrics. Our second model combines prior work on fluid models with our first model to capture the transient behavior as new users join or old users leave, while modelling many major aspects of BitTorrent. To the best of our knowledge, this is the first model that attempts to capture the transient behavior of many classes of heterogeneous users. Finally, we use our analytical methodology to introduce and study the performance of a flexible token-based scheme for BitTorrent, show how this scheme can be used to block freeriders and tradeoff between higher-bandwidth and lower-bandwidth users performance, and evaluate the scheme's parameters that achieve a target operational point.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2024109681",
    "type": "article"
  },
  {
    "title": "Temporal Integration of Emulation and Network Simulators on Linux Multiprocessors",
    "doi": "https://doi.org/10.1145/3154386",
    "publication_date": "2018-01-31",
    "publication_year": 2018,
    "authors": "Jereme Lamps; Vignesh Babu; David M. Nicol; Vladimir Adam; Rakesh Kumar",
    "corresponding_authors": "",
    "abstract": "Integration of emulation and simulation in virtual time requires that emulated execution bursts be ascribed a duration in virtual time and that emulated execution and simulation executions be coordinated within this common virtual time basis. This article shows how the open-source tool TimeKeeper for coordinating emulations in virtual time can be integrated with three different existing software emulations/simulations (CORE, Mininet, and EMANE) and with two existing network simulators (ns-3 and S3F). The integration does not require modification to those tools. However, the information that TimeKeeper needs to administer these emulations has to be extracted from each. We discuss the issues and challenges we encounter there, and the solutions. The S3F integration is specialized and shows how we can treat bursts of emulated execution just like an event handler in a discrete-event simulation. Through these case studies, we show the impact that the time dilation factor has on available resources, execution time, and fidelity of causality and that deleterious behaviors suffered under best-effort management of emulation processes can be corrected by integration with TimeKeeper. The key contribution is that we have shown how, using TimeKeeper, it is possible to bring virtual time to many existing emulators without needing to change them.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2786795437",
    "type": "article"
  },
  {
    "title": "Exact Simulation for a Class of Tempered Stable and Related Distributions",
    "doi": "https://doi.org/10.1145/3184453",
    "publication_date": "2018-07-06",
    "publication_year": 2018,
    "authors": "Angelos Dassios; Yan Qu; Hongbiao Zhao",
    "corresponding_authors": "",
    "abstract": "In this article, we develop a new scheme of exact simulation for a class of tempered stable (TS) and other related distributions with similar Laplace transforms. We discover some interesting integral representations for the underlying density functions that imply a unique simulation framework based on a backward recursive procedure. Therefore, the foundation of this simulation design is very different from existing schemes in the literature. It works pretty efficiently for some subclasses of TS distributions, where even the conventional acceptance-rejection mechanism can be avoided. It can also generate some other distributions beyond the TS family. For applications, this scheme could be easily adopted to generate a variety of TS-constructed random variables and TS-driven stochastic processes for modelling observational series in practice. Numerical experiments and tests are performed to demonstrate the accuracy and effectiveness of our scheme.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2848376736",
    "type": "article"
  },
  {
    "title": "Sequential Schemes for Frequentist Estimation of Properties in Statistical Model Checking",
    "doi": "https://doi.org/10.1145/3310226",
    "publication_date": "2019-10-31",
    "publication_year": 2019,
    "authors": "Cyrille Jégourel; Jun Sun; Jin Song Dong",
    "corresponding_authors": "",
    "abstract": "Statistical Model Checking (SMC) is an approximate verification method that overcomes the state space explosion problem for probabilistic systems by Monte Carlo simulations. Simulations might, however, be costly if many samples are required. It is thus necessary to implement efficient algorithms to reduce the sample size while preserving precision and accuracy. In the literature, some sequential schemes have been provided for the estimation of property occurrence based on predefined confidence and absolute or relative error. Nevertheless, these algorithms remain conservative and may result in huge sample sizes if the required precision standards are demanding. In this article, we compare some useful bounds and some sequential methods. We propose outperforming and rigorous alternative schemes based on Massart bounds and robust confidence intervals. Our theoretical and empirical analyses show that our proposal reduces the sample size while providing the required guarantees on error bounds.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2983704236",
    "type": "article"
  },
  {
    "title": "Modeling Resources to Simulate Business Process Reliability",
    "doi": "https://doi.org/10.1145/3381453",
    "publication_date": "2020-05-31",
    "publication_year": 2020,
    "authors": "Paolo Bocciarelli; Andrea D’Ambrogio; Andrea Giglio; Emiliano Paglia",
    "corresponding_authors": "",
    "abstract": "The combination of process modeling and simulation-based analysis provides a quantitative approach to analyze business processes, and to evaluate design alternatives before committing the required resources, to properly align operations with business strategies, improve operational efficiency, and gain competitive advantage. However, the use of simulation-based analysis is still limited in practice, mainly because it does not exploit process modeling standards and typically addresses performance-related properties only, such as time and cost. This article proposes a methodology that first extends the standard language for process modeling (i.e., BPMN) to introduce a flexible and accurate specification of business process resources, and then exploits the extended process specification to analyze and predict the process behavior by use of a simulation approach that takes into account reliability-related properties, to consider unexpected failures of process resources. The simulation-based analysis is implemented by use of a domain-specific process simulation language that preserves the BPMN execution semantics. An example application is introduced to show the importance of addressing both performance and reliability properties for the simulation-based analysis of business processes.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3029706295",
    "type": "article"
  },
  {
    "title": "Global-local Metamodel-assisted Stochastic Programming via Simulation",
    "doi": "https://doi.org/10.1145/3411080",
    "publication_date": "2020-12-31",
    "publication_year": 2020,
    "authors": "Wei Xie; Yuan Yi; Hua Zheng",
    "corresponding_authors": "",
    "abstract": "To integrate strategic, tactical, and operational decisions, stochastic programming has been widely used to guide dynamic decision-making. In this article, we consider complex systems and introduce the global-local metamodel-assisted stochastic programming via simulation that can efficiently employ the simulation resource to iteratively solve for the optimal first- and second-stage decisions. Specifically, at each visited first-stage decision, we develop a local metamodel to simultaneously solve a set of scenario-based second-stage optimization problems, which also allows us to estimate the optimality gap. Then, we construct a global metamodel accounting for the errors induced by: (1) using a finite number of scenarios to approximate the expected future cost occurring in the planning horizon, (2) second-stage optimality gap, and (3) finite visited first-stage decisions. Assisted by the global-local metamodel, we propose a new simulation optimization approach that can efficiently and iteratively search for the optimal first- and second-stage decisions. Our framework can guarantee the convergence of optimal solution for the discrete two-stage optimization with unknown objective, and the empirical study indicates that it achieves substantial efficiency and accuracy.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3117834163",
    "type": "article"
  },
  {
    "title": "Parallel Data Distribution Management on Shared-memory Multiprocessors",
    "doi": "https://doi.org/10.1145/3369759",
    "publication_date": "2020-01-31",
    "publication_year": 2020,
    "authors": "Moreno Marzolla; Gabriele D’Angelo",
    "corresponding_authors": "",
    "abstract": "The problem of identifying intersections between two sets of d -dimensional axis-parallel rectangles appears frequently in the context of agent-based simulation studies. For this reason, the High Level Architecture (HLA) specification—a standard framework for interoperability among simulators—includes a Data Distribution Management (DDM) service whose responsibility is to report all intersections between a set of subscription and update regions. The algorithms at the core of the DDM service are CPU-intensive, and could greatly benefit from the large computing power of modern multi-core processors. In this article, we propose two parallel solutions to the DDM problem that can operate effectively on shared-memory multiprocessors. The first solution is based on a data structure (the interval tree) that allows concurrent computation of intersections between subscription and update regions. The second solution is based on a novel parallel extension of the Sort Based Matching algorithm, whose sequential version is considered among the most efficient solutions to the DDM problem. Extensive experimental evaluation of the proposed algorithms confirm their effectiveness on taking advantage of multiple execution units in a shared-memory architecture.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3121272863",
    "type": "article"
  },
  {
    "title": "Regenerative Simulation for Queueing Networks with Exponential or Heavier Tail Arrival Distributions",
    "doi": "https://doi.org/10.1145/2699717",
    "publication_date": "2015-05-08",
    "publication_year": 2015,
    "authors": "Sarat Moka; Sandeep Juneja",
    "corresponding_authors": "",
    "abstract": "Multiclass open queueing networks find wide applications in communication, computer, and fabrication networks. Steady-state performance measures associated with these networks is often a topic of interset. Conceptually, under mild conditions, a sequence of regeneration times exists in multiclass networks, making them amenable to regenerative simulation for estimating steady-state performance measures. However, typically, identification of such a sequence in these networks is difficult. A well-known exception is when all interarrival times are exponentially distributed, where the instants corresponding to customer arrivals to an empty network constitute a sequence of regeneration times. In this article, we consider networks in which the interarrival times are generally distributed but have exponential or heavier tails. We show that these distributions can be decomposed into a mixture of sums of independent random variables such that at least one of the components is exponentially distributed. This allows an easily implementable embedded sequence of regeneration times in the underlying Markov process. We show that among all such interarrival time decompositions, the one with an exponential component that has the largest mean minimizes the asymptotic variance of the standard deviation estimator. We also show that under mild conditions on the network primitives, the regenerative mean and standard deviation estimators are consistent and satisfy a joint central limit theorem useful for constructing asymptotically valid confidence intervals.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2093720100",
    "type": "article"
  },
  {
    "title": "How Hard are Steady-State Queueing Simulations?",
    "doi": "https://doi.org/10.1145/2749460",
    "publication_date": "2015-11-16",
    "publication_year": 2015,
    "authors": "Eric C. Ni; Shane G. Henderson",
    "corresponding_authors": "",
    "abstract": "Some queueing systems require tremendously long simulation runlengths to obtain accurate estimators of certain steady-state performance measures when the servers are heavily utilized. However, this is not uniformly the case. We analyze a number of single-station Markovian queueing models, demonstrating that several steady-state performance measures can be accurately estimated with modest runlengths. Our analysis reinforces the meta result that if the queue is “well dimensioned,” then simulation runlengths will be modest. Queueing systems can be well dimensioned because customers abandon if they are forced to wait in line too long, or because the queue is operated in the “quality- and efficiency-driven regime” in which servers are heavily utilized but wait times are short. The results are based on computing or bounding the asymptotic variance and bias for several standard single-station queueing models and performance measures.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2101176228",
    "type": "article"
  },
  {
    "title": "Parallel Expanded Event Simulation of Tightly Coupled Systems",
    "doi": "https://doi.org/10.1145/2832909",
    "publication_date": "2016-01-06",
    "publication_year": 2016,
    "authors": "Georg Kunz; Mirko Stoffers; Olaf Landsiedel; Klaus Wehrle; James Gross",
    "corresponding_authors": "",
    "abstract": "The technical evolution of wireless communication technology and the need for accurately modeling these increasingly complex systems causes a steady growth in the complexity of simulation models. At the same time, multi-core systems have become the de facto standard hardware platform. Unfortunately, wireless systems pose a particular challenge for parallel execution due to a tight coupling of network entities in space and time. Moreover, model developers are often domain experts with no in-depth understanding of parallel and distributed simulation. In combination, both aspects severely limit the performance and the efficiency of existing parallelization techniques. We address these challenges by presenting parallel expanded event simulation , a novel modeling paradigm that extends discrete events with durations that span a period in simulated time. The resulting expanded events form the basis for a conservative synchronization scheme that considers overlapping expanded events eligible for parallel processing. We then put these concepts into practice by implementing H orizon , a parallel expanded event simulation framework specifically tailored to the characteristics of multi-core systems. Our evaluation shows that H orizon achieves considerable speedups in synthetic as well as real-world simulation models and considerably outperforms the current state-of-the-art in distributed simulation.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2224984904",
    "type": "article"
  },
  {
    "title": "Exploiting Social Capabilities in the Minority Game",
    "doi": "https://doi.org/10.1145/2996456",
    "publication_date": "2016-11-18",
    "publication_year": 2016,
    "authors": "Franco Cicirelli; Libero Nigro",
    "corresponding_authors": "",
    "abstract": "The minority game (MG) is an inductive binary-decision model that is able to study emergent behaviors in a population of agents who compete, through adaptation, for scarce resources. The original formulation of the game was inspired by the W.B. Arthur’s El Farol Bar problem: a fixed number of people have to independently decide, each week, whether to go to a bar having a limited capacity. People’s choices are only affected by the information about the number of visitors who attended the bar in the past weeks. Basic MG assumes that the information about the past game outcomes is publicly available, and it does not contemplate any communication among players. This article proposes the Dynamic Sociality Minority Game (DSMG). DSMG is an original variant of the classic MG where (1) information about the outcome of the previously played game step is only known to agents that really attended the bar the previous week, and (2) a dynamically established acquaintance network is introduced to propagate such information to nonattendant players. Specific settings of the game are identified in which DSMG is able to show a better coordination level among players with respect to the standard MG. Emergent properties of the DSMG along with players’ wellness are thoroughly analyzed through agent-based simulation of a simple road-traffic model.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2555698742",
    "type": "article"
  },
  {
    "title": "Parallel streams of linear random numbers in the spectral test",
    "doi": "https://doi.org/10.1145/301677.301682",
    "publication_date": "1999-01-01",
    "publication_year": 1999,
    "authors": "Karl Entacher",
    "corresponding_authors": "Karl Entacher",
    "abstract": "This paper reports analyses of subsequences of linear congruential pseudorandom numbers by means of the spectral test. Such subsequences occur in particular simulation setups or as methods to obtain parallel streams of pseudorandom numbers for parallel and distributed simulation. Especially in the latter case, two kinds of substreams are of special interest: lagged random numbers with step sizes k , and consecutive streams of random numbers of length l . We show how to analyze correlations within and between lagged subsequences with arbitrary step sizes k . Analyzing consecutive streams with the spectral test is related to the well-known long-range correlation analysis of linear congruential generators. Whereas the latter was carried out to show correlations between pairs of processors only, the spectral test provides a convenient method to study correlations between larger numbers of parallel streams as well.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2090324781",
    "type": "article"
  },
  {
    "title": "Asymptotically optimal importance sampling for product-form queuing networks",
    "doi": "https://doi.org/10.1145/174153.174160",
    "publication_date": "1993-07-01",
    "publication_year": 1993,
    "authors": "Keith W. Ross; Jie Wang",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Asymptotically optimal importance sampling for product-form queuing networks Authors: Keith W. Ross Univ. of Pennsylvania, Philadelphia Univ. of Pennsylvania, PhiladelphiaView Profile , Jie Wang Univ. of Pennsylvania, Philadelphia Univ. of Pennsylvania, PhiladelphiaView Profile Authors Info & Claims ACM Transactions on Modeling and Computer SimulationVolume 3Issue 3July 1993 pp 244–268https://doi.org/10.1145/174153.174160Online:01 July 1993Publication History 11citation267DownloadsMetricsTotal Citations11Total Downloads267Last 12 Months4Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2091481220",
    "type": "article"
  },
  {
    "title": "Minimum cost adaptive synchronization",
    "doi": "https://doi.org/10.1145/295251.295257",
    "publication_date": "1998-10-01",
    "publication_year": 1998,
    "authors": "Edward Mascarenhas; Felipe Knop; Reuben Pasquini; Vernon Rego",
    "corresponding_authors": "",
    "abstract": "We present a novel adaptive synchronization algorithm, called the minimum average cost (MAC) algorithm, in the context of the parasol parallel simulation system. ParaSol is a multithreaded system for parallel simulation on shared- and distributed-memory environments, designed to support domain-specific Simulation Object Libraries. The proposed MAC algorithm is based on minimizing the cost of synchronization delay and rollback at a process, whenever its simulation driver must decide whether to either proceed optimistically or to delay processing. In the former case the risk is rollback cost, in the event of a straggler's arrival. In the latter case the risk is unnecessary delay, in the event a latecomer is not a straggler. In addition to the MAC algorithm and an optimal delay computation model, we report on some early experiments comparing the performance of MAC-based adaptive synchronization to optimistic synchronization.",
    "cited_by_count": 15,
    "openalex_id": "https://openalex.org/W2122055599",
    "type": "article"
  },
  {
    "title": "Variance reduction applied to product form multiclass queuing networks",
    "doi": "https://doi.org/10.1145/268403.268419",
    "publication_date": "1997-10-01",
    "publication_year": 1997,
    "authors": "Bruno Tuffin",
    "corresponding_authors": "Bruno Tuffin",
    "abstract": "Performance of product-form multiclass queuing networks can be determined from normalization constants. For large models, the evaluation of these performance metrics is not possible because of the required amount of computer resources (either by using normalization constants or by using MVA approaches). Such large models can be evaluated with Monte Carlo summation and integration methods. This article proposes two cluster sampling Monte Carlo techniques to deal with such models. First, for a particular type of network, we propose a variance reduction technique based on antithetic variates. It leads to an improvement of Ross, Tsang and Wang's algorithm which is designed to analyze the same family of models. Second, for a more general class of models, we use a mixture of Monte Carlo and quasi-Monte Carlo methods to improve the estimate with respect to Monte Carlo alone.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W1979154598",
    "type": "article"
  },
  {
    "title": "Radix- <i>b</i> extensions to some common empirical tests for pseudorandom number generators",
    "doi": "https://doi.org/10.1145/240896.240906",
    "publication_date": "1996-10-01",
    "publication_year": 1996,
    "authors": "Brad C. Johnson",
    "corresponding_authors": "Brad C. Johnson",
    "abstract": "Empirical testing of computer generated pseudo-random sequences is widely practiced. Extensions to the coupon collector's and gap tests are presented that examine the distribution and independence of radix- b digit patterns in sequences with modulo of the form b w . An algorithm is given and the test is applied to a number of popular generators. Theoretical expected values are derived for a number of defects that may be present in a pseudorandom sequence and additional empirical evidence is given to support these values. The test has a simple model and a known distribution function. It is easily and efficiently implemented and easily adaptable to testing only the bits of interest, griven a certain application.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W1993655345",
    "type": "article"
  },
  {
    "title": "Synchronization mechanisms for distributed event-driven computation",
    "doi": "https://doi.org/10.1145/132277.132279",
    "publication_date": "1992-01-02",
    "publication_year": 1992,
    "authors": "Vijay K. Madisetti; David A. Hardaker",
    "corresponding_authors": "",
    "abstract": "We analyze distributed event-driven computation on message-passing parallel computing systems. Synchronization is the mechanism that ensures that causality in the ordering of stochastically generated events for execution during the computation is maintained. We characterize distributed event-driven computation into weakly coupled (weak interactions) and strongly coupled (strong interactions) distributed systems and propose and analyze a number of new algorithms for efficient synchronization. The analytical results for steady state performance are complemented with detailed simulations of the transient performance. We show that synchronization enforced separately from computation results in an efficient implementation. Effects of the memory hierarchy and communication delays are also incorporated.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2060640923",
    "type": "article"
  },
  {
    "title": "Getting rid of correlations among pseudorandom numbers",
    "doi": "https://doi.org/10.1145/347823.347835",
    "publication_date": "1999-07-01",
    "publication_year": 1999,
    "authors": "Stefan Wegenkittl; Makoto Matsumoto",
    "corresponding_authors": "",
    "abstract": "We consider the impact of discarding and tempering on modern huge period high speed linear generators, and illustrate how a simple strategy yields unexpected — and unwanted — success in a fair coin gambling which is simulated by a recently proposed generator. It becomes clear that discarding is no general rule to get rid of unwanted correlations.",
    "cited_by_count": 14,
    "openalex_id": "https://openalex.org/W2094776201",
    "type": "article"
  },
  {
    "title": "A hybrid technique for accelerated simulation of ATM networks and network elements",
    "doi": "https://doi.org/10.1145/384169.384172",
    "publication_date": "2001-04-01",
    "publication_year": 2001,
    "authors": "John Schormans; Enjie Liu; Laurie Cuthbert; J.M. Pitts",
    "corresponding_authors": "",
    "abstract": "Conventional simulation of cell- or packet-switched networks involves the use of discrete event simulators that model each individual cell through the network, typically called cell-level simulation. Each cells arrival at, or departure from, a network element is represented by an event. However, statistical considerations are such that very large numbers of cells have to be simulated to guarantee the accuracy of the results. This has always caused very long simulation times, often amounting to many hours of real time just to simulate a few minutes of simulated time. In this article we describe a novel methodology for accelerating simulation studies in cell-based communication networks, e.g., ATM, by using a hybrid analytical/simulation combination. The methodology uses a mathematical technique to seperate foreground traffic from background traffic, and focuses on accelerating cell by cell simulation.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2005530167",
    "type": "article"
  },
  {
    "title": "Asynchronous algorithms for the parallel simulation of event-driven dynamical systems",
    "doi": "https://doi.org/10.1145/140765.140783",
    "publication_date": "1991-07-01",
    "publication_year": 1991,
    "authors": "Vijay K. Madisetti; Jean Walrand; David G. Messerschmitt",
    "corresponding_authors": "",
    "abstract": "article Asynchronous algorithms for the parallel simulation of event-driven dynamical systems Share on Authors: Vijay K. Madisetti View Profile , Jean C. Walrand View Profile , David G. Messerschmitt View Profile Authors Info & Claims ACM Transactions on Modeling and Computer SimulationVolume 1Issue 3July 1991 pp 244–274https://doi.org/10.1145/140765.140783Online:01 July 1991Publication History 9citation434DownloadsMetricsTotal Citations9Total Downloads434Last 12 Months5Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2013177565",
    "type": "article"
  },
  {
    "title": "Optimizing static calendar queues",
    "doi": "https://doi.org/10.1145/361026.361028",
    "publication_date": "2000-07-01",
    "publication_year": 2000,
    "authors": "K. Bruce Erickson; Richard E. Ladner; Anthony LaMarca",
    "corresponding_authors": "",
    "abstract": "The calendar queue is an important implementation of a priority queue that is particularly useful in discrete event simulators. We investigate the performance of the static calendar queue that maintains N active events. The main contribution of this article is to prove that, under reasonable assumptions and with the proper parameter settings, the calendar queue data structure will have constant (independent of N ) expected time per event processed. A simple formula is derived to approximate the expected time per event. The formula can be used to set the parameters of the calendar queue to achieve optimal or near optimal performance. In addition, a technique is given to calibrate a specific calendar queue implementation so that the formula can be applied in a practical setting.",
    "cited_by_count": 12,
    "openalex_id": "https://openalex.org/W2083065042",
    "type": "article"
  },
  {
    "title": "Efficient simulation of queues in heavy traffic",
    "doi": "https://doi.org/10.1145/778553.778556",
    "publication_date": "2003-01-01",
    "publication_year": 2003,
    "authors": "Chia‐Li Wang; Ronald W. Wolff",
    "corresponding_authors": "",
    "abstract": "When simulating queues in heavy traffic, estimators of quantities such as average delay in queue d converge slowly to their true values. This problem is exacerbated when interarrival and service distributions are irregular. For the GI/G/ 1 queue, delay moments can be expressed in terms of moments of idle period I . Instead of estimating d directly by a standard regenerative estimator that we call DD, a method we call DI estimates d from estimated moments of I . DI was investigated some time ago and shown to be much more efficient than DD in heavy traffic. We measure efficiency as the factor by which variance is reduced. For the GI/G/ 1 queue, we show how to generate a sequence of realized values of the equilibrium idle period, I e , that are not independent and identically distributed, but have the correct statistical properties in the long run. We show how to use this sequence to construct a new estimator of d , called DE, and of higher moments of delay as well. When arrivals are irregular, we show that DE is more efficient than DI, in some cases by a large factor, independent of the traffic intensity . Comparing DE with DD, these factors multiply . For GI/G/c , we construct a control-variates estimator of average delay in queue d c that is efficient in heavy traffic. It uses DE to estimate the average delay for the corresponding fast single server. We compare the efficiency of this method with another method in the literature. For M/G/c , we use insensitivity to construct another control-variates estimator of d c . We compare the efficiency of this estimator with the two c-server estimators above.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W1974849092",
    "type": "article"
  },
  {
    "title": "Universal nonuniform random vector generator based on acceptance-rejection",
    "doi": "https://doi.org/10.1145/1103323.1103325",
    "publication_date": "2005-07-01",
    "publication_year": 2005,
    "authors": "Gleb Beliakov",
    "corresponding_authors": "Gleb Beliakov",
    "abstract": "The acceptance/rejection approach is widely used in universal nonuniform random number generators. Its key part is an accurate approximation of a given probability density from above by a hat function. This article uses a piecewise constant hat function, whose values are overestimates of the density on the elements of the partition of the domain. It uses a sawtooth overestimate of Lipschitz continuous densities, and then examines all local maximizers of such an overestimate. The method is applicable to multivariate multimodal distributions. It exhibits relatively short preprocessing time and fast generation of random variates from a very large class of distributions.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2009603165",
    "type": "article"
  },
  {
    "title": "Transfer Reinforcement Learning for Autonomous Driving",
    "doi": "https://doi.org/10.1145/3449356",
    "publication_date": "2021-07-18",
    "publication_year": 2021,
    "authors": "Aravind Balakrishnan; Jaeyoung Lee; Ashish Gaurav; Krzysztof Czarnecki; Sean Sedwards",
    "corresponding_authors": "",
    "abstract": "Reinforcement learning (RL) is an attractive way to implement high-level decision-making policies for autonomous driving, but learning directly from a real vehicle or a high-fidelity simulator is variously infeasible. We therefore consider the problem of transfer reinforcement learning and study how a policy learned in a simple environment using WiseMove can be transferred to our high-fidelity simulator, W ise M ove . WiseMove is a framework to study safety and other aspects of RL for autonomous driving. W ise M ove accurately reproduces the dynamics and software stack of our real vehicle. We find that the accurately modelled perception errors in W ise M ove contribute the most to the transfer problem. These errors, when even naively modelled in WiseMove , provide an RL policy that performs better in W ise M ove than a hand-crafted rule-based policy. Applying domain randomization to the environment in WiseMove yields an even better policy. The final RL policy reduces the failures due to perception errors from 10% to 2.75%. We also observe that the RL policy has significantly less reliance on velocity compared to the rule-based policy, having learned that its measurement is unreliable.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W3186212639",
    "type": "article"
  },
  {
    "title": "On simulating a class of Bernstein polynomials",
    "doi": "https://doi.org/10.1145/2133390.2133396",
    "publication_date": "2012-03-01",
    "publication_year": 2012,
    "authors": "Vineet Goyal; Karl Sigman",
    "corresponding_authors": "",
    "abstract": "Given a black box that generates independent Bernoulli samples with an unknown bias p, we consider the problem of simulating a Bernoulli random variable with bias f ( p ) (where f is a given function) using a finite (computable in advance) number of independent Bernoulli samples from the black box. We show that this is possible if and only if f is a Bernstein polynomial with coefficients between 0 and 1, and we explicitly give the algorithm. Our results differ from Keane and O'Brien [1994] in that our goal is more modest/stringent, since we are considering algorithms that use a finite number of samples as opposed to allowing a random number (such as in acceptance rejection algorithms).",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W1983786403",
    "type": "article"
  },
  {
    "title": "Steepest-ascent constrained simultaneous perturbation for multiobjective optimization",
    "doi": "https://doi.org/10.1145/1870085.1870087",
    "publication_date": "2010-12-01",
    "publication_year": 2010,
    "authors": "Daniel W. McClary; Violet R. Syrotiuk; Murat Külahçı",
    "corresponding_authors": "",
    "abstract": "The simultaneous optimization of multiple responses in a dynamic system is challenging. When a response has a known gradient, it is often easily improved along the path of steepest ascent. On the contrary, a stochastic approximation technique may be used when the gradient is unknown or costly to obtain. We consider the problem of optimizing multiple responses in which the gradient is known for only one response. We propose a hybrid approach for this problem, called simultaneous perturbation stochastic approximation steepest ascent, SPSA-SA or SP(SA)(2) for short. SP(SA)(2) is an SPSA technique that leverages information about the known gradient to constrain the perturbations used to approximate the others. We apply SP(SA)(2) to the cross-layer optimization of throughput, packet loss, and end-to-end delay in a mobile ad hoc network (MANET), a self-organizing wireless network. The results show that SP(SA)(2) achieves higher throughput and lower packet loss and end-to-end delay than the steepest ascent, SPSA, and the Nelder-Mead stochastic approximation approaches. It also reduces the cost in the number of iterations to perform the optimization.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W1997632632",
    "type": "article"
  },
  {
    "title": "Rare-event simulation for stochastic recurrence equations with heavy-tailed innovations",
    "doi": "https://doi.org/10.1145/2517451",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "José Blanchet; Henrik Hult; Kevin Leder",
    "corresponding_authors": "",
    "abstract": "In this article, rare-event simulation for stochastic recurrence equations of the form X n +1 = A n +1 X n + B n +1 , X 0 =0 is studied, where { A n ; n ≥ 1} and { B n ; n ≥ 1} are independent sequences consisting of independent and identically distributed real-valued random variables. It is assumed that the tail of the distribution of B 1 is regularly varying, whereas the distribution of A 1 has a suitably light tail. The problem of efficient estimation, via simulation, of quantities such as P { X n &gt;b} and P {sup k ≤ n X k &gt; b} for large b and n is studied. Importance sampling strategies are investigated that provide unbiased estimators with bounded relative error as b and n tend to infinity.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2063002075",
    "type": "article"
  },
  {
    "title": "The Extrinsic Noise Effect on Lateral Inhibition Differentiation Waves",
    "doi": "https://doi.org/10.1145/2832908",
    "publication_date": "2016-01-09",
    "publication_year": 2016,
    "authors": "Andreas I. Reppas; Georgios Lolas; Andreas Deutsch; Haralampos Hatzikirou",
    "corresponding_authors": "",
    "abstract": "Multipotent differentiation, where cells adopt one of several cell fates, is a determinate and orchestrated procedure that often incorporates stochastic mechanisms in order to diversify cell types. How these stochastic phenomena interact to govern cell fate is poorly understood. Nonetheless, cell fate decision-making procedure is mainly regulated through the activation of differentiation waves and associated signaling pathways. In the current work, we focus on the Notch/Delta signaling pathway, which is not only known to trigger such waves but also is used to achieve the principle of lateral inhibition (i.e., a competition for exclusive fates through cross-signaling between neighboring cells). Such a process ensures unambiguous stochastic decisions influenced by intrinsic noise sources, such as those found in the regulation of signaling pathways, and extrinsic stochastic fluctuations attributed to microenvironmental factors. However, the effect of intrinsic and extrinsic noise on cell fate determination is an open problem. Our goal is to elucidate how the induction of extrinsic noise affects cell fate specification in a lateral inhibition mechanism. Using a stochastic Cellular Automaton with continuous state space, we show that extrinsic noise results in the emergence of steady-state furrow patterns of cells in a “frustrated/transient” phenotypic state.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W1591159618",
    "type": "article"
  },
  {
    "title": "AIR",
    "doi": "https://doi.org/10.1145/2701420",
    "publication_date": "2015-04-16",
    "publication_year": 2015,
    "authors": "Jingjing Wang; Nael Abu‐Ghazaleh; Dmitry Ponomarev",
    "corresponding_authors": "",
    "abstract": "Parallel discrete event simulation (PDES) harnesses parallel processing to improve the performance and capacity of simulation, supporting bigger and more detailed models simulated for more scenarios. The presence of interference from other users can lead to dramatic slowdown in the performance of the simulation. Interference is typically managed using operating system scheduling support (e.g., gang scheduling), a heavyweight approach with some drawbacks. We propose an application-level approach to interference resilience through alternative simulation scheduling and mapping algorithms. More precisely, the most resilient simulators allow dynamic mapping of simulation event execution to processing resources (a work pool model). However, this model has significant scheduling overhead and poor cache locality. Thus, we investigate using application-level interference mitigation where the application detects the presence of interference and reacts by changing the thread task allocation. Specifically, we propose a locality-aware adaptive dynamic mapping (LADM) algorithm that adjusts the number of active threads on the fly by detecting the presence of interference. LADM avoids having the application stall when threads are inactive due to context switching. We investigate different mechanisms for monitoring the level of interference and different approaches for remapping tasks. We show that LADM can substantially reduce the impact of interference while maintaining memory locality.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2028089209",
    "type": "article"
  },
  {
    "title": "Selecting Stopping Rules for Confidence Interval Procedures",
    "doi": "https://doi.org/10.1145/2627734",
    "publication_date": "2014-05-02",
    "publication_year": 2014,
    "authors": "Dashi I. Singham",
    "corresponding_authors": "Dashi I. Singham",
    "abstract": "The sample size decision is crucial to the success of any sampling experiment. More samples imply better confidence and precision in the results, but require higher costs in terms of time, computing power, and money. Analysts often choose sequential stopping rules on an ad hoc basis to obtain confidence intervals with desired properties without requiring large sample sizes. However, the choice of stopping rule can affect the quality of the interval produced in terms of the coverage, precision, and replication cost. This article introduces methods for choosing and evaluating stopping rules for confidence interval procedures. We develop a general framework for assessing the quality of a broad class of stopping rules applied to independent and identically distributed data. We introduce coverage profiles that plot the coverage according to the stopping time and reveal situations when the coverage could be unexpectedly low. Finally, we recommend simple techniques for obtaining acceptable or optimal rules.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2092807286",
    "type": "article"
  },
  {
    "title": "Hybrid Simulations of Heterogeneous Biochemical Models in SBML",
    "doi": "https://doi.org/10.1145/2742545",
    "publication_date": "2015-04-06",
    "publication_year": 2015,
    "authors": "Hui Ju K. Chiang; François Fages; Jie Hong Roland Jiang; Sylvain Soliman",
    "corresponding_authors": "",
    "abstract": "Models of biochemical systems presented as a set of formal reaction rules can be interpreted in different formalisms, most notably as either deterministic Ordinary Differential Equations, stochastic continuous-time Markov Chains, Petri nets, or Boolean transition systems. While the formal composition of reaction systems can be syntactically defined as the (multiset) union of the reactions, the composition and simulation of models in different formalisms remain a largely open issue. In this article, we show that the combination of reaction rules and events, as already present in SBML, can be used in a nonstandard way to define stochastic and Boolean simulators and give meaning to the hybrid composition and simulation of heterogeneous models of biochemical processes. In particular, we show how two SBML reaction models can be composed into one hybrid continuous--stochastic SBML model through a high-level interface for composing reaction models and specifying their interpretation. Furthermore, we describe dynamic strategies for automatically partitioning reactions with stochastic or continuous interpretations according to dynamic criteria. The performances are then compared to static partitioning. The proposed approach is illustrated and evaluated on several examples, including the reconstructions of the hybrid model of the mammalian cell cycle regulation of Singhania et al. as the composition of a Boolean model of cell cycle phase transitions with a continuous model of cyclin activation, the hybrid stochastic--continuous models of bacteriophage T7 infection of Alfonsi et al., and the bacteriophage λ model of Goutsias, showing the gain in both accuracy and simulation time of the dynamic partitioning strategy.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2251927672",
    "type": "article"
  },
  {
    "title": "Replicated Computational Results (RCR) Report for “Automatic Moment-Closure Approximation of Spatially Distributed Collective Adaptive Systems”",
    "doi": "https://doi.org/10.1145/2893479",
    "publication_date": "2016-03-29",
    "publication_year": 2016,
    "authors": "Alexander Lück",
    "corresponding_authors": "Alexander Lück",
    "abstract": "“Automatic Moment-Closure Approximation of Spatially Distributed Collective Adaptive Systems” by Feng, Hilston, and Galpin presents detailed simulation analysis results for three models of spatially distributed collective adaptive systems. In this replicated computational results report, the corresponding implementation together with a documentation that was provided to the reviewer by the authors are considered. The software was installed, and new simulation results were generated and compared to the original results. The installation of the software did not result in any problems, and the comparison of the results yielded that the published results are replicable.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2330155812",
    "type": "article"
  },
  {
    "title": "Generating Fast Specialized Simulators for Stochastic Reaction Networks via Partial Evaluation",
    "doi": "https://doi.org/10.1145/3485465",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Till Köster; Tom Warnke; Adelinde M. Uhrmacher",
    "corresponding_authors": "",
    "abstract": "Domain-specific modeling languages allow a clear separation between simulation model and simulator and, thus, facilitate the development of simulation models and add to the credibility of simulation results. Partial evaluation provides an effective means for efficiently executing models defined in such languages. However, it also implies some challenges of its own. We illustrate this and solutions based on a simple domain-specific language for biochemical reaction networks as well as on the network representation of the established BioNetGen language. We implement different approaches adopting the same simulation algorithms: one generic simulator that parses models at runtime and one generator that produces a simulator specialized to a given model based on partial evaluation and code generation. For the purpose of better understanding, we additionally generate intermediate variants, where only some parts are partially evaluated. Akin to profile-guided optimization, we use dynamic execution of the model to further optimize the simulators. The performance of the approaches is carefully benchmarked using representative models of small to large biochemical reaction networks. The generic simulator achieves a performance similar to state-of-the-art simulators in the domain, whereas the specialized simulator outperforms established simulation tools with a speedup of more than an order of magnitude. Technical limitations in regard to the size of the generated code are discussed and overcome using a combination of link-time optimization and code separation. A detailed performance study is undertaken, investigating how and where partial evaluation has the largest effect.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3206138075",
    "type": "article"
  },
  {
    "title": "A Hierarchical Decision-Making Framework in Social Networks for Efficient Disaster Management",
    "doi": "https://doi.org/10.1145/3490027",
    "publication_date": "2022-01-07",
    "publication_year": 2022,
    "authors": "Seunghan Lee; Saurabh Jain; Young‐Jun Son",
    "corresponding_authors": "",
    "abstract": "One of the major challenges faced by the current society is developing disaster management strategies to minimize the effects of catastrophic events. Disaster planning and strategy development phases of this urgency require larger amounts of cooperation among communities or individuals in society. Social networks have also been playing a crucial role in the establishment of efficient disaster management planning. This article proposes a hierarchical decision-making framework that would assist in analyzing two imperative information flow processes (innovation diffusion and opinion formation) in social networks under the consideration of community detection. The proposed framework was proven to capture the heterogeneity of individuals using cognitive behavior models and evaluate its impact on diffusion speed and opinion convergence. Moreover, the framework demonstrated the evolution of communities based on their inter-and intracommunication. The simulation results with real social network data suggest that the model can aid in establishing an efficient disaster management policy using social sensing and delivery.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4205721840",
    "type": "article"
  },
  {
    "title": "Drawing Random Floating-point Numbers from an Interval",
    "doi": "https://doi.org/10.1145/3503512",
    "publication_date": "2022-04-11",
    "publication_year": 2022,
    "authors": "Frédéric Goualard",
    "corresponding_authors": "Frédéric Goualard",
    "abstract": "Drawing a floating-point number uniformly at random from an interval [ a , b ) is usually performed by a location-scale transformation of some floating-point number drawn uniformly from [0, 1). Due to the weak properties of floating-point arithmetic, such a transformation cannot ensure respect of the bounds, uniformity or spatial equidistributivity. We investigate and quantify precisely these shortcomings while reviewing the actual implementations of the method in major programming languages and libraries, and we propose a simple algorithm to avoid these shortcomings without compromising performances.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4223612752",
    "type": "article"
  },
  {
    "title": "Estimating Multiclass Service Demand Distributions Using Markovian Arrival Processes",
    "doi": "https://doi.org/10.1145/3570924",
    "publication_date": "2022-11-08",
    "publication_year": 2022,
    "authors": "Runan Wang; Giuliano Casale; Antonio Filieri",
    "corresponding_authors": "",
    "abstract": "Building performance models for software services in DevOps is costly and error-prone. Accurate service demand distribution estimation is critical to precisely modeling queueing behaviors and performance prediction. However, current estimation methods focus on capturing the mean service demand, disregarding higher-order moments of the distribution that still can largely affect prediction accuracy. To address this limitation, we propose to estimate higher moments of the service demand distribution for a microservice from monitoring traces. We first generate a closed queueing model to abstract software performance and use it to model the departure process of requests completed by the software service as a Markovian arrival process (MAP). This allows formulating the estimation of service demand into an optimization problem, which aims to find the first multiple moments of the service demand distribution that maximize the likelihood of the MAP using generated the measured inter-departure times. We then estimate the service demand distribution for different classes of service with a maximum likelihood algorithm and novel heuristics to mitigate the computational cost of the optimization process for scalability. We apply our method to real traces from a microservice-based application and demonstrate that its estimations lead to greater prediction accuracy than exponential distributions assumed in traditional service demand estimation approaches for software services.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W4308587711",
    "type": "article"
  },
  {
    "title": "SEH: Size Estimate Hedging Scheduling of Queues",
    "doi": "https://doi.org/10.1145/3580491",
    "publication_date": "2023-01-17",
    "publication_year": 2023,
    "authors": "Maryam Akbari‐Moghaddam; Douglas G. Down",
    "corresponding_authors": "",
    "abstract": "For a single server system, Shortest Remaining Processing Time (SRPT) is an optimal size-based policy. In this article, we discuss scheduling a single-server system when exact information about the jobs’ processing times is not available. When the SRPT policy uses estimated processing times, the underestimation of large jobs can significantly degrade performance. We propose an index-based policy with a single parameter, Size Estimate Hedging (SEH), that only uses estimated processing times for scheduling decisions. A job’s priority is increased dynamically according to an SRPT rule until it is determined that it is underestimated, at which time the priority is frozen. Numerical results suggest that SEH has desirable performance for estimation error variance that is consistent with what is seen in practice.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4317036118",
    "type": "article"
  },
  {
    "title": "NIM: Generative Neural Networks for Automated Modeling and Generation of Simulation Inputs",
    "doi": "https://doi.org/10.1145/3592790",
    "publication_date": "2023-04-19",
    "publication_year": 2023,
    "authors": "Wang Cen; Peter J. Haas",
    "corresponding_authors": "",
    "abstract": "Fitting stochastic input-process models to data and then sampling from them are key steps in a simulation study but highly challenging to non-experts. We present Neural Input Modeling (NIM), a Generative Neural Network (GNN) framework that exploits modern data-rich environments to automatically capture simulation input processes and then generate samples from them. The basic GNN that we develop, called NIM-VL , comprises (i) a variational autoencoder architecture that learns the probability distribution of the input data while avoiding overfitting and (ii) long short-term memory components that concisely capture statistical dependencies across time. We show how the basic GNN architecture can be modified to exploit known distributional properties—such as independent and identically distributed structure, nonnegativity, and multimodality—to increase accuracy and speed, as well as to handle multivariate processes, categorical-valued processes, and extrapolation beyond the training data for certain nonstationary processes. We also introduce an extension to NIM called Conditional Neural Input Modeling (CNIM), which can learn from training data obtained under various realizations of a (possibly time series valued) stochastic “condition,” such as temperature or inflation rate, and then generate sample paths given a value of the condition not seen in the training data. This enables users to simulate a system under a specific working condition by customizing a pre-trained model; CNIM also facilitates what-if analysis. Extensive experiments show the efficacy of our approach. NIM can thus help overcome one of the key barriers to simulation for non-experts.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4366407881",
    "type": "article"
  },
  {
    "title": "Stochastic Approximation for Estimating the Price of Stability in Stochastic Nash Games",
    "doi": "https://doi.org/10.1145/3632525",
    "publication_date": "2023-11-11",
    "publication_year": 2023,
    "authors": "Afrooz Jalilzadeh; Farzad Yousefian; Mohammadjavad Ebrahimi",
    "corresponding_authors": "",
    "abstract": "The goal in this article is to approximate the Price of Stability (PoS) in stochastic Nash games using stochastic approximation (SA) schemes. PoS is among the most popular metrics in game theory and provides an avenue for estimating the efficiency of Nash games. In particular, evaluating the PoS can help with designing efficient networked systems, including communication networks and power market mechanisms. Motivated by the absence of efficient methods for computing the PoS, first we consider stochastic optimization problems with a nonsmooth and merely convex objective function and a merely monotone stochastic variational inequality (SVI) constraint. This problem appears in the numerator of the PoS ratio. We develop a randomized block-coordinate stochastic extra-(sub)gradient method where we employ a novel iterative penalization scheme to account for the mapping of the SVI in each of the two gradient updates of the algorithm. We obtain an iteration complexity of the order ϵ -4 that appears to be best known result for this class of constrained stochastic optimization problems, where ϵ denotes an arbitrary bound on suitably defined infeasibility and suboptimality metrics. Second, we develop an SA-based scheme for approximating the PoS and derive lower and upper bounds on the approximation error. To validate the theoretical findings, we provide preliminary simulation results on a networked stochastic Nash Cournot competition.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4388600314",
    "type": "article"
  },
  {
    "title": "Design-time simulation of a large-scale, distributed object system",
    "doi": "https://doi.org/10.1145/295251.295255",
    "publication_date": "1998-10-01",
    "publication_year": 1998,
    "authors": "Svend Frølund; Pankaj Garg",
    "corresponding_authors": "",
    "abstract": "We present a case study in using simulation at design time to predict the performance and scalability properties of a large-scale distributed object system. The system, called Consul, is a network management system designing to support hundreds of operators managing millions of network devices. It is essential that a system such as Consul be designed with performance and scalability in mind, but due to Consul's complexity and scale, it is hard to reason about performance and scalability using ad hoc techniques. We built a simulation of Consul's design to guide the design process by enabling performanace and scalability analysis of various design alternatives. A major challenge in doing design-time simulation is that many parameters for the simulation are based on estimates rather than measurements. We developed analysis methods that derive conclusions that are valid in the presence of estimation errors. In this article, we describe our scalability analysis method for design simulations of distributed object systems. The main idea is to use relative and comparative reasoning to analyze design alternatives and compare transaction behaviors. We demonstrate the analysis approach by describing its application to Consul.",
    "cited_by_count": 13,
    "openalex_id": "https://openalex.org/W2054869130",
    "type": "article"
  },
  {
    "title": "Simulating markov-reward processes with rare events",
    "doi": "https://doi.org/10.1145/1060576.1060578",
    "publication_date": "2005-04-01",
    "publication_year": 2005,
    "authors": "Winfried K. Grassmann; Jingxiang Luo",
    "corresponding_authors": "",
    "abstract": "Simulating continuous-time Markov reward processes containing rarely visited, but economically important states requires long simulation times unless special measures are taken. In this article, we consider Markov reward processes in equilibrium, and we use the equilibrium equations to reallocate rewards. The effect of this reallocation is determined analytically for a number of small examples. In these examples, significant savings in run lengths were possible, especially in the case where the expected rewards are strongly influenced by low-probability boundary states. The emphasis of the article is on exploration; no large simulation problems have been considered.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2061086504",
    "type": "article"
  },
  {
    "title": "Network simulation enhancing network management in real-time",
    "doi": "https://doi.org/10.1145/985793.985798",
    "publication_date": "2004-04-01",
    "publication_year": 2004,
    "authors": "G. Warren; Ronald Nolte; Ken Funk; Brian Merrell",
    "corresponding_authors": "",
    "abstract": "This research investigates the application of network simulation to enhance network management in real-time. Its focus is on ad hoc wireless networks as needed for future public safety emergency response, homeland security, and future combat system networks where the network requirements and topology are rapidly changing. The network simulation is used to analyze user satisfaction with the network and to enable real-time what-if studies that show what the user satisfaction will be if the network is reconfigured in various ways. User satisfaction as a metric is defined to correspond to the impact of the network performance on the ability of the network user to complete their public safety, security or military functions. The current network size of interest is up to ad hoc wireless 500 nodes, a typical target load for a single network manager. At the same time, the approach is made scalable to use manager of managers approach to scale to larger networks. The article reports on the approach and performance results.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2089174742",
    "type": "article"
  },
  {
    "title": "Modeling Large-Scale Slim Fly Networks Using Parallel Discrete-Event Simulation",
    "doi": "https://doi.org/10.1145/3203406",
    "publication_date": "2018-08-30",
    "publication_year": 2018,
    "authors": "Noah Wolfe; Misbah Mubarak; Christopher D. Carothers; Robert Ross; Philip Carns",
    "corresponding_authors": "",
    "abstract": "As supercomputers approach exascale performance, the increased number of processors translates to an increased demand on the underlying network interconnect. The slim fly network topology, a new low-diameter, low-latency, and low-cost interconnection network, is gaining interest as one possible solution for next-generation supercomputing interconnect systems. In this article, we present a high-fidelity slim fly packet-level model leveraging the Rensselaer Optimistic Simulation System (ROSS) and Co-Design of Exascale Storage (CODES) frameworks. We validate the model with published work before scaling the network size up to an unprecedented 1 million compute nodes and confirming that the slim fly observes peak network throughput at extreme scale. In addition to synthetic workloads, we evaluate large-scale slim fly models with real communication workloads from applications in the Design Forward program with over 110,000 MPI processes. We show strong scaling of the slim fly model on an Intel cluster achieving a peak network packet transfer rate of 2.3 million packets per second and processing over 7 billion discrete events using 128 MPI tasks. Enabled by the strong performance capabilities of the model, we perform a detailed application trace and routing protocol performance study. Through analysis of metrics such as packet latency, hop count, and congestion, we find that the slim fly network is able to leverage simple minimal routing and achieve the same performance as more complex adaptive routing for tested DOE benchmark applications.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2889218820",
    "type": "article"
  },
  {
    "title": "A Variational Inference-Based Heteroscedastic Gaussian Process Approach for Simulation Metamodeling",
    "doi": "https://doi.org/10.1145/3299871",
    "publication_date": "2019-01-24",
    "publication_year": 2019,
    "authors": "Wenjing Wang; Nan Chen; Xi Chen; Linchang Yang",
    "corresponding_authors": "",
    "abstract": "In this article, we propose a variational Bayesian inference-based Gaussian process metamodeling approach (VBGP) that is suitable for the design and analysis of stochastic simulation experiments. This approach enables statistically and computationally efficient approximations to the mean and variance response surfaces implied by a stochastic simulation, while taking into full account the uncertainty in the heteroscedastic variance; furthermore, it can accommodate the situation where either one or multiple simulation replications are available at every design point. We demonstrate the superior performance of VBGP compared with existing simulation metamodeling methods through two numerical examples.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2913827949",
    "type": "article"
  },
  {
    "title": "Managing Pending Events in Sequential and Parallel Simulations Using Three-tier Heap and Two-tier Ladder Queue",
    "doi": "https://doi.org/10.1145/3265750",
    "publication_date": "2019-03-15",
    "publication_year": 2019,
    "authors": "Dhananjai M. Rao; Julius Higiro",
    "corresponding_authors": "",
    "abstract": "Performance of sequential and parallel Discrete Event Simulations (DES) is strongly influenced by the data structure used for managing and processing pending events. Accordingly, we propose and evaluate the effectiveness of our multi-tiered (two- and three-tier) data structures and our Two-tier Ladder Queue, for both sequential and optimistic parallel simulations on distributed memory platforms. Our experiments compare the performance of our data structures against a performance-tuned version of the Ladder Queue, which has been shown to outperform many other data structures for DES. The core simulation-based empirical assessments are in C++ and are based on 2,500 configurations of well-established PHOLD and PCS benchmarks. In addition, we use an Avian Influenza Epidemic Model (AIM) for experimental analyses. We have conducted experiments on two computing clusters with different hardware to ensure our results are reproducible. Moreover, to fully establish the robustness of our analysis and data structures, we have also implemented pertinent queues in Java and verified consistent, reproducible performance characteristics. Collectively, our analyses show that our three-tier heap and two-tier ladder queue outperform the Ladder Queue by 60× in some simulations, particularly those with higher concurrency per Logical Process (LP), in both sequential and Time Warp synchronized parallel simulations.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2920939777",
    "type": "article"
  },
  {
    "title": "PDES-A",
    "doi": "https://doi.org/10.1145/3302259",
    "publication_date": "2019-04-18",
    "publication_year": 2019,
    "authors": "Shafiur Rahman; Nael Abu‐Ghazaleh; Walid Najjar",
    "corresponding_authors": "",
    "abstract": "In this article, we present experiences implementing a general Parallel Discrete Event Simulation (PDES) accelerator on a Field Programmable Gate Array (FPGA). The accelerator can be specialized to any particular simulation model by defining the object states and the event handling code, which are then synthesized into a custom accelerator for the given model. The accelerator consists of several event processors that can process events in parallel while maintaining the dependencies between them. Events are automatically sorted by a self-sorting event queue. The accelerator supports optimistic simulation by automatically keeping track of event history and supporting rollbacks. The architecture is limited in scalability locally by the communication and port bandwidth of the different structures. However, it is designed to allow multiple accelerators to be connected to scale up the simulation. We evaluate the design and explore several design trade-offs and optimizations. We show that the accelerator can scale to 64 concurrent event processors relative to the performance of a single event processor. At this point, the scalability becomes limited by contention on the shared structures within the datapath. To alleviate this bottleneck, we also develop a new version of the datapath that partitions the state and event space of the simulation but allows these partitions to share the use of the event processors. The new design substantially reduces contention and improves the performance with 64 processors from 49x to 62x relative to a single processor design. We went through two iterations of the design of PDES-A, first using Verilog and then using Chisel (for the partitioned version of the design). We report in this article on some observations in the differences in prototyping accelerators using these two different languages. PDES-A outperforms the ROSS simulator running on a 12-core Intel Xeon machine by a factor of 3.2x with less than 15% of the power consumption. Our future work includes building multiple interconnected PDES-A cores.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2940447780",
    "type": "article"
  },
  {
    "title": "Predicting the Simulation Budget in Ranking and Selection Procedures",
    "doi": "https://doi.org/10.1145/3323715",
    "publication_date": "2019-06-18",
    "publication_year": 2019,
    "authors": "Sijia Ma; Shane G. Henderson",
    "corresponding_authors": "",
    "abstract": "The goal of ranking and selection (R8S) procedures is to identify the best among a finite set of alternative systems evaluated by stochastic simulation, providing a probability guarantee on the quality of the solution. To solve large-scale R8S problems, especially in parallel computing platforms where variable numbers of cores might be used, it is helpful to be able to predict the simulation budget, which is almost always the dominant portion of the running time of a given procedure for a given problem. Non-trivial issues arise due to the need to estimate the system configuration. We propose a set of methods for predicting the simulation budget. Numerical results compare our predictions for several leading R8S procedures.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2949641694",
    "type": "article"
  },
  {
    "title": "A Distributed Shared Memory Middleware for Speculative Parallel Discrete Event Simulation",
    "doi": "https://doi.org/10.1145/3373335",
    "publication_date": "2020-03-20",
    "publication_year": 2020,
    "authors": "Matteo Principe; Tommaso Tocci; Pierangelo Di Sanzo; Francesco Quaglia; Alessandro Pellegrini",
    "corresponding_authors": "",
    "abstract": "The large diffusion of multi-core machines has pushed the research in the field of Parallel Discrete Event Simulation (PDES) toward new programming paradigms, based on the exploitation of shared memory. On the opposite side, the advent of Cloud computing—and the possibility to group together many (low-cost) virtual machines to form a distributed memory cluster capable of hosting simulation applications—has raised the need to bridge shared memory programming and seamless distributed execution. In this article, we present the design of a distributed middleware that transparently allows a PDES application coded for shared memory systems to run on clusters of (Cloud) resources. Our middleware is based on a synchronization protocol called Event and Cross State Synchronization. It allows cross-simulation-object access by event handlers, thus representing a powerful tool for the development of various types of PDES applications. We also provide data for an experimental assessment of our middleware architecture, which has been integrated into the open source ROOT-Sim speculative PDES platform.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3013339054",
    "type": "article"
  },
  {
    "title": "Toward a Theory of Superdense Time in Simulation Models",
    "doi": "https://doi.org/10.1145/3379489",
    "publication_date": "2020-05-31",
    "publication_year": 2020,
    "authors": "James Nutaro",
    "corresponding_authors": "James Nutaro",
    "abstract": "We develop a theory of superdense time that encompasses existing uses of superdense time in discrete event simulations and points to new forms that have not previously been explored. A central feature of our development is a set of axioms for superdense time. The sufficiency of these axioms is demonstrated by using them to prove that a general model of a discrete event simulation procedure, expressed in terms of a mathematical system, constitutes a state transition function. Several forms of superdense time, both known and novel, are shown to satisfy the axioms.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3033450814",
    "type": "article"
  },
  {
    "title": "Inhomogeneous CTMC Birth-and-Death Models Solved by Uniformization with Steady-State Detection",
    "doi": "https://doi.org/10.1145/3373758",
    "publication_date": "2020-05-31",
    "publication_year": 2020,
    "authors": "Maciej Rafał Burak; Przemysław Korytkowski",
    "corresponding_authors": "",
    "abstract": "Time-inhomogeneous queueing models play an important role in service systems modeling. Although the transient solutions of corresponding continuous-time Markov chains (CTMCs) are more precise than methods using stationary approximations, most authors consider their computational costs prohibitive for practical application. This article presents a new variant of the uniformization algorithm that utilizes a modified steady-state detection technique. The presented algorithm is applicable for CTMCs when their stationary solution can be efficiently calculated in advance, particularly for many practically applicable birth-and-death models with limited size. It significantly improves computational efficiency due to an early prediction of an occurrence of a steady state, using the properties of the convergence function of the embedded discrete-time Markov chain. Moreover, in the case of an inhomogeneous CTMC solved in consecutive timesteps, the modification guarantees that the error of the computed probability distribution vector is strictly bounded at each point of the considered time interval.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3033959871",
    "type": "article"
  },
  {
    "title": "Discrete-Event Modeling and Simulation of Diffusion Processes in Multiplex Networks",
    "doi": "https://doi.org/10.1145/3434490",
    "publication_date": "2020-12-31",
    "publication_year": 2020,
    "authors": "Cristina Ruiz-Martín; Gabriel Wainer; Adolfo López‐Paredes",
    "corresponding_authors": "",
    "abstract": "A variety of phenomena (such as the spread of diseases, pollution in rivers, etc.) can be studied as diffusion processes over networks (i.e., the diffusion of the phenomenon over a set of interconnected entities). This research introduces a method to study such diffusion processes in multiplex dynamic networks. We use a formal Modeling and Simulation methodology (in our case, DEVS, Discrete-Event System Specification). We use DEVS formal models to integrate models defined using Agent-Based Modeling and Network Theory. We present (1) an Architecture to study Diffusion Processes in Multiplex dynamic networks (ADPM) and (2) a systematic Process to define, implement, and simulate diffusion processes over such networks. We show a theoretical definition and a concrete implementation of ADPM. We show how to use ADPM and the process in a case study based on a real nuclear emergency plan; this illustrates the application of the process, the architecture, and the developed software. Different scenarios are studied as Diffusion Processes to demonstrate the usability of ADPM.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3113794249",
    "type": "article"
  },
  {
    "title": "Uncertainty on Discrete-Event System Simulation",
    "doi": "https://doi.org/10.1145/3466169",
    "publication_date": "2021-09-27",
    "publication_year": 2021,
    "authors": "Damián Vicino; Gabriel Wainer; Olivier Dalle",
    "corresponding_authors": "",
    "abstract": "Uncertainty Propagation methods are well-established when used in modeling and simulation formalisms like differential equations. Nevertheless, until now there are no methods for Discrete-Dynamic Systems. Uncertainty-Aware Discrete-Event System Specification (UA-DEVS) is a formalism for modeling Discrete-Event Dynamic Systems that include uncertainty quantification in messages, states, and event times. UA-DEVS models provide a theoretical framework to describe the models’ uncertainty and their properties. As UA-DEVS models can include continuous variables and non-computable functions, their simulation could be non-computable. For this reason, we also introduce Interval-Approximated Discrete-Event System Specification (IA-DEVS), a formalism that approximates UA-DEVS models using a set of order and bounding functions to obtain a computable model. The computable model approximation produces a tree of all trajectories that can be traversed from the original model and some erroneous ones introduced by the approximation process. We also introduce abstract simulation algorithms for IA-DEVS, present a case study of UA-DEVS, its IA-DEVS approximation and, its simulation results using the algorithms defined.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W3203605282",
    "type": "article"
  },
  {
    "title": "Fast synthesis of persistent fractional Brownian motion",
    "doi": "https://doi.org/10.1145/2133390.2133395",
    "publication_date": "2012-03-01",
    "publication_year": 2012,
    "authors": "Pedro R. M. Inácio; Mário M. Freire; Manuela Pereira; Paulo P. Monteiro",
    "corresponding_authors": "",
    "abstract": "Due to the relevance of self-similarity analysis in several research areas, there is an increased interest in methods to generate realizations of self-similar processes, namely in the ones capable of simulating long-range dependence. This article describes a new algorithm to approximate persistent fractional Brownian motions with a predefined Hurst parameter. The algorithm presents a computational complexity of O ( n ) and generates sequences with n ( n ∈ N) values with a small multiple of log 2 ( n ) variables. Because it operates in a sequential manner, the algorithm is suitable for simulations demanding real-time operation. A network traffic simulator is presented as one of its possible applications.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W1981073983",
    "type": "article"
  },
  {
    "title": "Deriving Feasible Deployment Alternatives for Parallel and Distributed Simulation Systems",
    "doi": "https://doi.org/10.1145/2499913.2499917",
    "publication_date": "2013-07-01",
    "publication_year": 2013,
    "authors": "Turgay Çelik; Bedir Teki̇nerdoğan; Kayhan M. İmre",
    "corresponding_authors": "",
    "abstract": "Parallel and distributed simulations (PADS) realize the distributed execution of a simulation system over multiple physical resources. To realize the execution of PADS, different simulation infrastructures such as HLA, DIS and TENA have been defined. Recently, the Distributed Simulation Engineering and Execution Process (DSEEP) that supports the mapping of the simulations on the infrastructures has been defined. An important recommended task in DSEEP is the evaluation of the performance of the simulation systems at the design phase. In general, the performance of a simulation is largely influenced by the allocation of member applications to the resources. Usually, the deployment of the applications to the resources can be done in many different ways. DSEEP does not provide a concrete approach for evaluating the deployment alternatives. Moreover, current approaches that can be used for realizing various DSEEP activities do not yet provide adequate support for this purpose. We provide a concrete approach for deriving feasible deployment alternatives based on the simulation system and the available resources. In the approach, first the simulation components and the resources are designed. The design is used to define alternative execution configurations, and based on the design and the execution configuration; a feasible deployment alternative can be algorithmically derived. Tool support is developed for the simulation design, the execution configuration definition and the automatic generation of feasible deployment alternatives. The approach has been applied within a large-scale industrial case study for simulating Electronic Warfare systems.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W1983937507",
    "type": "article"
  },
  {
    "title": "Bayesian Learning of Noisy Markov Decision Processes",
    "doi": "https://doi.org/10.1145/2414416.2414420",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Sumeetpal S. Singh; Nicolás Chopin; Nick Whiteley",
    "corresponding_authors": "",
    "abstract": "We consider the inverse reinforcement learning problem, that is, the problem of learning from, and then predicting or mimicking a controller based on state/action data. We propose a statistical model for such data, derived from the structure of a Markov decision process. Adopting a Bayesian approach to inference, we show how latent variables of the model can be estimated, and how predictions about actions can be made, in a unified framework. A new Markov chain Monte Carlo (MCMC) sampler is devised for simulation from the posterior distribution. This step includes a parameter expansion step, which is shown to be essential for good convergence properties of the MCMC sampler. As an illustration, the method is applied to learning a human controller.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2001339644",
    "type": "article"
  },
  {
    "title": "Fitting Statistical Models of Random Search in Simulation Studies",
    "doi": "https://doi.org/10.1145/2499913.2499914",
    "publication_date": "2013-07-01",
    "publication_year": 2013,
    "authors": "Russell Cheng",
    "corresponding_authors": "Russell Cheng",
    "abstract": "We consider optimization of expected system performance by random search. There are two sources of random variation in this process: (i) a search-induced variability because the expected performance of the system will vary randomly according to the alternatives randomly selected for examination, and (ii) a simulation induced variability, because there will be random error in estimating expected system performance from finite simulation runs. We show that, in altering the balance between these two sources of variability, three distinct forms of asymptotic behavior of the estimate of the optimal expected system performance are possible. The form of the asymptotic results shows that they may be not be easy to apply in practical work. As an alternative, a methodology for fitting a statistical model that accounts for both types of variability is suggested. This then allows the distributional properties of quantities of interest, like the optimum performance value and the best value obtained by the search, to be estimated by resampling and which also allows a test of goodness of fit of the model. Four numerical examples are given.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2100930402",
    "type": "article"
  },
  {
    "title": "Modeling and Simulation of Extreme-Scale Fat-Tree Networks for HPC Systems and Data Centers",
    "doi": "https://doi.org/10.1145/2988231",
    "publication_date": "2017-04-30",
    "publication_year": 2017,
    "authors": "Ning Liu; Adnan Haider; Dong Jin; Xian‐He Sun",
    "corresponding_authors": "",
    "abstract": "As parallel and distributed systems are evolving toward extreme scale, for example, high-performance computing systems involve millions of cores and billion-way parallelism, and high-capacity storage systems require efficient access to petabyte or exabyte of data, many new challenges are posed on designing and deploying next-generation interconnection communication networks in these systems. Fat-tree networks have been widely used in both data centers and high-performance computing (HPC) systems in the past decades and are promising candidates of the next-generation extreme-scale networks. In this article, we present FatTreeSim, a simulation framework that supports modeling and simulation of extreme-scale fat-tree networks with the goal of understanding the design constraints of next-generation HPC and distributed systems and aiding the design and performance optimization of the applications running on these systems. We have systematically experimented FatTreeSim on Emulab and Blue Gene/Q and analyzed the scalability and fidelity of FatTreeSim with various network configurations. On the Blue Gene/Q Mira, FatTreeSim can achieve a peak performance of 305 million events per second using 16,384 cores. Finally, we have applied FatTreeSim to simulate several large-scale Hadoop YARN applications to demonstrate its usability.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2729311276",
    "type": "article"
  },
  {
    "title": "A Virtual WLAN Device Model for High-Fidelity Wireless Network Emulation",
    "doi": "https://doi.org/10.1145/3067664",
    "publication_date": "2017-07-31",
    "publication_year": 2017,
    "authors": "Takaaki Kawai; Shigeru Kaneda; Mineo Takai; Hiroshi Mineno",
    "corresponding_authors": "",
    "abstract": "The recent popularization of mobile devices has increased the amount of communication traffic. Hence, it is necessary both in academia and industry to research load distribution methods for mobile networks. An evaluation environment for large-scale networks that behaves like a practical system is necessary to evaluate these methods, and either a physical environment or simulation environment can be used. However, physical and simulation environments each have their advantages and disadvantages. A physical environment is suitable for practical operation because it is possible to obtain data from a real environment. In contrast, the cost for a large number of nodes and the difficulty of field preparation are its disadvantages. Reproducing radio propagation is also a challenge. Network simulators solve the disadvantages of the physical environment by modeling the entire evaluation environment. However, they do not exactly reproduce the physical environment because the nodes are abstracted. This article presents an evaluation environment that combines a network simulator and virtual machines with virtual wireless Local Area Network (LAN) devices. The virtual machines reproduce the physical environment with high fidelity by running the programs of the physical machines, and the virtual wireless LAN devices make it possible to emulate wireless LAN communication using default operating system drivers. A network simulator and virtual machines also reduce the cost for nodes, ease the burden of field preparation, and reproduce radio propagation by modeling the evaluation environment. In the evaluation, the proposed method decreased the difference from the physical environment to 5% in terms of transmission control protocol throughput. In the case of user datagram protocol, the proposed method decreased the difference from the physical environment down to 1.7%. The number of virtual machines available on a host machine and the practical use of the proposed method are also discussed.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2741594236",
    "type": "article"
  },
  {
    "title": "A Factor-Based Bayesian Framework for Risk Analysis in Stochastic Simulations",
    "doi": "https://doi.org/10.1145/3154387",
    "publication_date": "2017-10-31",
    "publication_year": 2017,
    "authors": "Wei Xie; Cheng Li; Pu Zhang",
    "corresponding_authors": "",
    "abstract": "Simulation is commonly used to study the random behaviors of large-scale stochastic systems with correlated inputs. Since the input correlation is often induced by latent common factors in many situations, to facilitate system diagnostics and risk management, we introduce a factor-based Bayesian framework that can improve both computational and statistical efficiency and provide insights for system risk analysis. Specifically, we develop a flexible Gaussian copula-based multivariate input model that can capture important properties in the real-world data. A nonparametric Bayesian approach is used to model marginal distributions, and it can capture the properties, including multi-modality and skewness. We explore the factor structure of the underlying generative processes for the dependence. Both input and simulation estimation uncertainty are characterized by the posterior distributions. In addition, we interpret the latent factors and estimate their effects on the system performance, which could be used to support diagnostics and decision making for large-scale stochastic systems. Our approach is supported by both asymptotic theory and empirical study.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2780275061",
    "type": "article"
  },
  {
    "title": "Modeling Cache Memory Utilization on Multicore Using Common Pool Resource Game on Cellular Automata",
    "doi": "https://doi.org/10.1145/2812808",
    "publication_date": "2016-01-09",
    "publication_year": 2016,
    "authors": "Michail‐Antisthenis Tsompanas; Christoforos Kachris; Georgios Ch. Sirakoulis",
    "corresponding_authors": "",
    "abstract": "Recent computing architectures are implemented by shared memory technologies to alleviate the high latency experienced by off-chip memory transfers, but the high architectural complexity of modern multicore processors has presented many questions. To tackle the design of efficient algorithms scheduling workloads over available cores, this article presents a parallel bioinspired model that simulates the utilization of shared memory on multicore systems. The proposed model is based on cellular automata (CA) and coupled with game theory principles. CA are selected due to their inherent parallelism and especially their ability to incorporate inhomogeneities. Furthermore, the novelty of the model is realized on the fact that multilevel CA are used to simulate the different levels of cache memory usually found in multicore processors. These characteristics make the model able to cope with the increasing diversity of cache memory hierarchies on modern and future processors. Nonetheless, by acquiring data from hardware performance counters and processing them with the proposed model online, the performance of the system can be calculated and a better scheduling strategy can be adopted in real time. The CA-based model was verified on the behavior of a real multicore system running a multithreaded application, and it successfully simulated the acceleration achieved by an increased number of cores available for the execution of the workload. More specifically, the example of common pool resource from game theory was used with two variations: a static and a variable initial endowment. The static variation of the model approximates slightly better the acceleration of a workload when the number of available processor cores increases, whereas the dynamic variation simulates better the moderate differences due to operation system’s scheduler alternations on the same amount of cores.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2222356325",
    "type": "article"
  },
  {
    "title": "Hybrid PDES Simulation of HPC Networks Using Zombie Packets",
    "doi": "https://doi.org/10.1145/3682060",
    "publication_date": "2024-07-30",
    "publication_year": 2024,
    "authors": "Elkin Cruz-Camacho; Kevin A. Brown; Xin Wang; Xiongxiao Xu; Kai Shu; Zhiling Lan; Bob Ross; Christopher D. Carothers",
    "corresponding_authors": "",
    "abstract": "Although high-fidelity network simulations have proven to be reliable and cost-effective tools to peer into architectural questions for high-performance computing (HPC) networks, they incur a high resource cost. The time spent in simulating a single millisecond of network traffic in the highest detail can take hours, even for static, well-behaved traffic patterns such as uniform random. Surrogate models offer a significant reduction in runtime, yet they cannot serve as complete replacements and should only be used when appropriate. Thus, there is a need for hybrid modeling, where high-fidelity simulation and surrogates run side-by-side. We present a surrogate model for HPC networks in which: packets bypass the network, while the network state is left untouched, i.e , suspended. To bypass the network, we use historical data to estimate the arrival time at which every packet should be scheduled at; to suspend the network, all in-flight packets are scheduled to arrive at their destinations, and are kept in the system to awaken as zombies when switching back to high-fidelity. Speedup for a hybrid model is relative to the proportion of surrogate to high-fidelity. This light-weight surrogate obtained up to 76 × speedup. Keeping the zombies in the network showed an increase in the accuracy of the high-fidelity simulation on restart when compared to restarting the network from an empty state.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4401124089",
    "type": "article"
  },
  {
    "title": "Towards a polynomial-time randomized algorithm for closed product-form networks",
    "doi": "https://doi.org/10.1145/290274.290277",
    "publication_date": "1998-07-01",
    "publication_year": 1998,
    "authors": "Wu‐Lin Chen; Colm Art O’Cinneide",
    "corresponding_authors": "",
    "abstract": "We present a Markov chain Monte Carlo method for class throughputs in closed multiclass product-form networks. The method is as follows. For a given network, we construct a “regularized” network with a highly simplified structure that has the same steady-state distribution. We then simulate the regularized network. The method has performed reasonably well across a broad range of problems. We give a heuristic explanation of this and prove that the regularized network “mixes in polynomial time” in some special cases.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W1969380327",
    "type": "article"
  },
  {
    "title": "Instability and performance limits of distributed simulators of feedforward queueing networks",
    "doi": "https://doi.org/10.1145/249204.249206",
    "publication_date": "1997-04-01",
    "publication_year": 1997,
    "authors": "Rajeev Shorey; Anurag Kumar; Kiran M. Rege",
    "corresponding_authors": "",
    "abstract": "In this article we study the performance of distributed simulation of open feedforward queueing networks, by analyzing queueing models of message flows in distributed discrete event simulators. We view each logical process in a distributed simulation as comprising a message sequencer with associated message queues, followed aby an event processor. We introduce the idealized, but analytically useful, comcept of maximum lookahead. We show that, with quite general stochatstic assumptions for message arrival and time-stamp processes, the meassage queues are unstable for conservative sequencing, and for conservative sequencing with maximum lookahead and hence for optimistic resequencing, and for any resequenceing algorithm that does not employ interprocessor flow control . Finally, we provide formulas for the throughput of distributed simulators of feedforward queueing networks.",
    "cited_by_count": 11,
    "openalex_id": "https://openalex.org/W2115072414",
    "type": "article"
  },
  {
    "title": "A fuzzy set theoretic approach to validate simulation models",
    "doi": "https://doi.org/10.1145/1176249.1176253",
    "publication_date": "2006-10-01",
    "publication_year": 2006,
    "authors": "Jurgen Martens; Ferdi Put; Etienne E. Kerre",
    "corresponding_authors": "",
    "abstract": "We develop a new approach to the validation of simulation models by exploiting elements from fuzzy set theory and machine learning. A fuzzy resemblance relation concept is used to set up a mathematical framework for measuring the degree of similarity between the input-output behavior of a simulation model and the corresponding behavior of the real system. A neuro-fuzzy inference algorithm is employed to automatically learn the required resemblance relation from real and simulated data. Ultimately, defuzzification strategies are applied to obtain a coefficient on the unit interval that characterizes the degree of model validity. An example in the airline industry illustrates the practical application of this methodology.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2077483188",
    "type": "article"
  },
  {
    "title": "Simulation output analysis using integrated paths II",
    "doi": "https://doi.org/10.1145/1540530.1540532",
    "publication_date": "2009-06-01",
    "publication_year": 2009,
    "authors": "James M. Calvin",
    "corresponding_authors": "James M. Calvin",
    "abstract": "This article is a sequel to a previous article that introduced a class of variance estimators for steady-state simulation output analysis. The estimators were constructed by applying a quadratic function to a vector obtained from iterated integrations of the simulation output. The previous article concentrated on deriving the limiting distributions of the estimators and on their computational efficiency for a particular choice of quadratic function. The present article considers estimators constructed from different quadratic functions, chosen mainly to reduce bias compared to the estimators of the previous article. Overlapping and nonoverlapping batch means versions of the estimators are discussed.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W1996242977",
    "type": "article"
  },
  {
    "title": "A stochastic approximation method with max-norm projections and its applications to the Q-learning algorithm",
    "doi": "https://doi.org/10.1145/1842713.1842715",
    "publication_date": "2010-09-01",
    "publication_year": 2010,
    "authors": "Sumit Kunnumkal; Hüseyin Topaloğlu",
    "corresponding_authors": "",
    "abstract": "In this article, we develop a stochastic approximation method to solve a monotone estimation problem and use this method to enhance the empirical performance of the Q-learning algorithm when applied to Markov decision problems with monotone value functions. We begin by considering a monotone estimation problem where we want to estimate the expectation of a random vector, η. We assume that the components of E {η} are known to be in increasing order. The stochastic approximation method that we propose is designed to exploit this information by projecting its iterates onto the set of vectors with increasing components. The novel aspect of the method is that it uses projections with respect to the max norm. We show the almost sure convergence of the stochastic approximation method. After this result, we consider the Q-learning algorithm when applied to Markov decision problems with monotone value functions. We study a variant of the Q-learning algorithm that uses projections to ensure that the value function approximation obtained at each iteration is also monotone. Computational results indicate that the performance of the Q-learning algorithm can be improved significantly by exploiting the monotonicity property of the value functions.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2029593259",
    "type": "article"
  },
  {
    "title": "ProPPA",
    "doi": "https://doi.org/10.1145/3154392",
    "publication_date": "2018-01-23",
    "publication_year": 2018,
    "authors": "Anastasis Georgoulas; Jane Hillston; Guido Sanguinetti",
    "corresponding_authors": "",
    "abstract": "Formal languages like process algebras have been shown to be effective tools in modelling a wide range of dynamic systems, providing a high-level description that is readily transformed into an executable model. However, their application is sometimes hampered because the quantitative details of many real-world systems of interest are not fully known. In contrast, in machine learning, there has been work to develop probabilistic programming languages, which provide system descriptions that incorporate uncertainty and leverage advanced statistical techniques to infer unknown parameters from observed data. Unfortunately, current probabilistic programming languages are typically too low-level to be suitable for complex modelling. In this article, we present a Probabilistic Programming Process Algebra (ProPPA), the first instance of the probabilistic programming paradigm being applied to a high-level, formal language, and its supporting tool suite. We explain the semantics of the language in terms of a quantitative generalisation of Constraint Markov Chains and describe the implementation of the language, discussing in some detail the different inference algorithms available and their domain of applicability. We conclude by illustrating the use of the language on simple but non-trivial case studies: here, ProPPA is shown to combine the elegance and simplicity of high-level formal modelling languages with an effective way of incorporating data, making it a promising tool for modelling studies.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2771927446",
    "type": "article"
  },
  {
    "title": "Visual Analytics to Identify Temporal Patterns and Variability in Simulations from Cellular Automata",
    "doi": "https://doi.org/10.1145/3265748",
    "publication_date": "2019-01-24",
    "publication_year": 2019,
    "authors": "Philippe J. Giabbanelli; Magda Baniukiewicz",
    "corresponding_authors": "",
    "abstract": "Cellular Automata (CA) are discrete simulation models, thus producing spatio-temporal data through experiments, as well as stochastic models, thus generating multi-run data. Identifying temporal patterns, such as cycles, is important to understand the behavior of the model. Assessing variability is also essential to estimate which parameter values may require more runs and what consensus emerges across simulation runs. However, these two tasks are currently arduous as the commonly employed slider-based visualizations offer little support to identify temporal trends or excessive model variability. In this article, we addressed these two tasks by developing, implementing, and evaluating a new visual analytics environment that uses several linked visualizations. Our empirical evaluation of the proposed environment assessed (i) whether modelers could identify temporal patterns and variability, (ii) how features of simulations impacted performances, and (iii) whether modelers can use the familiar slider-based visualization together with our new environment. Results shows that participants were confident on results obtained using our new environment. They were also able to accomplish the two target tasks without taking longer than they would with current solutions. Our qualitative analysis found that some participants saw value in switching between our proposed visualization and the commonly used slider-based version. In addition, we noted that errors were affected not only by the type of visualizations but also by specific features of the simulations. Future work may combine and adapt these visualizations depending on salient simulation parameters.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2911528329",
    "type": "article"
  },
  {
    "title": "Exposing Inter-process Information for Efficient PDES of Spatial Stochastic Systems on Multicores",
    "doi": "https://doi.org/10.1145/3301500",
    "publication_date": "2019-04-02",
    "publication_year": 2019,
    "authors": "Jonatan Lindén; Pavol Bauer; Stefan Engblom; Bengt Jönsson",
    "corresponding_authors": "",
    "abstract": "We present a new approach for efficient process synchronization in parallel discrete event simulation on multicore computers. We aim specifically at simulation of spatially extended stochastic system models where time intervals between successive inter-process events are highly variable and without lower bounds: This includes models governed by the mesoscopic Reaction-Diffusion Master Equation (RDME). A central part of our approach is a mechanism for optimism control, in which each process disseminates accurate information about timestamps of its future outgoing interprocess events to its neighbours. This information gives each process a precise basis for deciding when to pause local processing to reduce the risk of expensive rollbacks caused by future “delayed” incoming events. We apply our approach to a natural parallelization of the Next Subvolume Method (NSM) for simulating systems obeying RDME. Since this natural parallelization does not expose accurate timestamps of future interprocess events, we restructure it to expose such information, resulting in a simulation algorithm called Refined Parallel NSM (Refined PNSM). We have implemented Refined PNSM in a parallel simulator for spatial extended Markovian processes. On 32 cores, it achieves an efficiency ranging between 43--95% for large models, and on average 37% for small models, compared to an efficient sequential simulation without any code for parallelization. It is shown that the gain of restructuring the naive parallelization into Refined PNSM more than outweighs its overhead. We also show that our resulting simulator is superior in performance to existing simulators on multicores for comparable models.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2930848346",
    "type": "article"
  },
  {
    "title": "Ranking and Selection",
    "doi": "https://doi.org/10.1145/3241042",
    "publication_date": "2019-01-24",
    "publication_year": 2019,
    "authors": "Bjürn Görder; Michael Kolonko",
    "corresponding_authors": "",
    "abstract": "We introduce a new sampling scheme for selecting the best alternative out of a given set of systems that are evaluated with respect to their expected performances. We assume that the systems are simulated on a computer and that a joint observation of all systems has a multivariate normal distribution with unknown mean and unknown covariance matrix. In particular, the observations of the systems may be stochastically dependent as is the case if common random numbers are used for simulation. In each iteration of the algorithm, we allocate a fixed budget of simulation runs to the alternatives. We use a Bayesian set-up with a noninformative prior distribution and derive a new closed-form approximation for the posterior distributions that allows provision of a lower bound for the posterior probability of a correct selection (PCS). Iterations are continued until this lower bound is greater than 1−α for a given α. We also introduce a new allocation strategy that allocates the available budget according to posterior error probabilities. Our procedure needs no additional prior parameters and can cope with different types of ranking and selection tasks. Our numerical experiments show that our strategy is superior to other procedures from the literature, namely, KN ++ and P luck . In all of our test scenarios, these procedures needed more observation and/or had an empirical PCS below the required 1−α. Our procedure always had its empirical PCS above 1−α, underlining the practicability of our approximation of the posterior distribution.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2962804664",
    "type": "article"
  },
  {
    "title": "Analysis of Spatio-temporal Properties of Stochastic Systems Using TSTL",
    "doi": "https://doi.org/10.1145/3326168",
    "publication_date": "2019-10-31",
    "publication_year": 2019,
    "authors": "Ludovica Luisa Vissat; Michele Loreti; Laura Nenzi; Jane Hillston; Glenn Marion",
    "corresponding_authors": "",
    "abstract": "In this article, we present Three-Valued spatio-temporal Logic (TSTL), which enriches the available spatiotemporal analysis of properties expressed in Signal spatio-temporal Logic (SSTL), to give further insight into the dynamic behavior of systems. Our novel analysis starts from the estimation of satisfaction probabilities of given SSTL properties and allows the analysis of their temporal and spatial evolution. Moreover, in our verification procedure, we use a three-valued approach to include the intrinsic and unavoidable uncertainty related to the simulation-based statistical evaluation of the estimates; this can be also used to assess the appropriate number of simulations to use depending on the analysis needs. We present the syntax and three-valued semantics of TSTL and specific extended monitoring algorithms to check the validity of TSTL formulas. We introduce a reliability requirement for TSTL monitoring and an automatic procedure to verify it. Two case studies demonstrate how TSTL broadens the application of spatio-temporal logics in realistic scenarios, enabling analysis of threat monitoring and privacy preservation based on spatial stochastic population models.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3003487033",
    "type": "article"
  },
  {
    "title": "An Integrated Method for Simultaneous Calibration and Parameter Selection in Computer Models",
    "doi": "https://doi.org/10.1145/3364217",
    "publication_date": "2020-01-31",
    "publication_year": 2020,
    "authors": "Jun Yuan; Szu Hui Ng",
    "corresponding_authors": "",
    "abstract": "For many large and complex computer models, there usually exist a large number of unknown parameters. To improve the computer model's predictive performance for more reliable and confident decision making, two important issues have to be addressed. The first is to calibrate the computer model and the second is to select the most influential set of parameters. However, these two issues are often addressed independently, which may waste computational effort. In this article, a Gaussian process-based Bayesian method is first proposed to simultaneously calibrate and select parameters in stochastic computer models. The whole procedure can be conducted more efficient by sharing the data information between these two steps. To further ease the computational burden, an approximation approach based on a weighted normal approximation is proposed to evaluate the posteriors in the proposed Bayesian method. A sequential approximation procedure is further proposed to improve the approximation accuracy by allocating the sequential design points more appropriately. The efficiency and accuracy of the proposed approaches are compared in a building energy model and a pandemic influenza simulation model.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3005227448",
    "type": "article"
  },
  {
    "title": "Fidelity and Performance of State Fast-forwarding in Microscopic Traffic Simulations",
    "doi": "https://doi.org/10.1145/3366019",
    "publication_date": "2020-04-10",
    "publication_year": 2020,
    "authors": "Philipp Andelfinger; Yadong Xu; David Eckhoff; Wentong Cai; Alois Knoll",
    "corresponding_authors": "",
    "abstract": "Common car-following models for microscopic traffic simulation assume a time advancement using fixed-sized time steps. However, a purely time-driven execution is inefficient when the states of some agents are independent of other agents and thus predictable far into the simulated future. We propose a method to accelerate microscopic traffic simulations based on identifying independence among agent state updates. Instead of iteratively updating an agent’s state throughout a sequence of time steps, a computationally inexpensive “fast-forward” function advances the agent’s state to the time of its earliest possible interaction with other agents. We present an algorithm to determine independence intervals in microscopic traffic simulations and derive fast-forward functions for several well-known traffic models. In contrast to existing approaches based on reducing the level of detail, our approach retains the microscopic nature of the simulation. An evaluation is performed for a synthetic scenario and on the road network of Singapore. At low traffic densities, maximum speedup factors of about 2.6 and 1.6 are achieved, while at the highest considered densities, only few opportunities for fast-forwarding exist. We show that the deviation from purely time-driven execution is reduced to a minimum when choosing an adequate numerical integration scheme to execute the time-driven updates. Verification results show that the overall deviation in vehicle travel times is marginal.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3015201570",
    "type": "article"
  },
  {
    "title": "Hierarchical Gaussian Process Models for Improved Metamodeling",
    "doi": "https://doi.org/10.1145/3384470",
    "publication_date": "2020-07-07",
    "publication_year": 2020,
    "authors": "Nicolas Knudde; Vincent Dutordoir; Joachim van der Herten; Ivo Couckuyt; Tom Dhaene",
    "corresponding_authors": "",
    "abstract": "Simulations are often used for the design of complex systems as they allow one to explore the design space without the need to build several prototypes. Over the years, the simulation accuracy, as well as the associated computational cost, has increased significantly, limiting the overall number of simulations during the design process. Therefore, metamodeling aims to approximate the simulation response with a cheap to evaluate mathematical approximation, learned from a limited set of simulator evaluations. Kernel-based methods using stationary kernels are nowadays widely used. In many problems, the smoothness of the function varies in space, which we call nonstationary behavior [20]. However, using stationary kernels for nonstationary responses can be inappropriate and result in poor models when combined with sequential design. We present the application of two recent techniques: Deep Gaussian Processes and Gaussian Processes with nonstationary kernel, which are better able to cope with these difficulties. We evaluate the method for nonstationary regression on a series of real-world problems, showing that these recent approaches outperform the standard Gaussian Processes with stationary kernels. Results show that these techniques are suitable for the simulation community, and we outline the variational inference method for the Gaussian Process with nonstationary kernel.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3041549812",
    "type": "article"
  },
  {
    "title": "Impacts of radio-identification on cryo-conservation centers",
    "doi": "https://doi.org/10.1145/2000494.2000500",
    "publication_date": "2011-08-01",
    "publication_year": 2011,
    "authors": "Sylvain Housseman; Nabil Absi; Dominique Feillet; Stéphane Dauzère‐Pérès",
    "corresponding_authors": "",
    "abstract": "This article deals with the use of discrete-event simulation as a decision support tool for estimating the impact of Radio Frequency IDentification (RFID) technologies on processes and activities of biological sample storage areas (called biobanks). We first give a detailed description of biobank flows and identify subprocesses improved using RFID technologies. Several indicators, such as inventory reliability and human resource utilization, are compared and discussed for different scenarios involving the use of different RFID technologies. A special emphasis is put on the so-called rewarehousing activity, which RFID makes possible and which consists in reassigning tubes to empty places when boxes are emptied. For this particular activity, optimization algorithms are developed and embedded in the simulator. This study shows the potential use of RFID in biobanks and the value of simulation for estimating and optimizing its introduction in such complex socio-technical systems.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2069412889",
    "type": "article"
  },
  {
    "title": "Smoothed Functional Algorithms for Stochastic Optimization Using <i>q</i> -Gaussian Distributions",
    "doi": "https://doi.org/10.1145/2628434",
    "publication_date": "2014-05-02",
    "publication_year": 2014,
    "authors": "Debarghya Ghoshdastidar; Ambedkar Dukkipati; Shalabh Bhatnagar",
    "corresponding_authors": "",
    "abstract": "Smoothed functional (SF) schemes for gradient estimation are known to be efficient in stochastic optimization algorithms, especially when the objective is to improve the performance of a stochastic system. However, the performance of these methods depends on several parameters, such as the choice of a suitable smoothing kernel. Different kernels have been studied in the literature, which include Gaussian, Cauchy, and uniform distributions, among others. This article studies a new class of kernels based on the q -Gaussian distribution, which has gained popularity in statistical physics over the last decade. Though the importance of this family of distributions is attributed to its ability to generalize the Gaussian distribution, we observe that this class encompasses almost all existing smoothing kernels. This motivates us to study SF schemes for gradient estimation using the q -Gaussian distribution. Using the derived gradient estimates, we propose two-timescale algorithms for optimization of a stochastic objective function in a constrained setting with a projected gradient search approach. We prove the convergence of our algorithms to the set of stationary points of an associated ODE. We also demonstrate their performance numerically through simulations on a queuing model.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2076037361",
    "type": "article"
  },
  {
    "title": "Small Variance Estimators for Rare Event Probabilities",
    "doi": "https://doi.org/10.1145/2414416.2414423",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "Michel Broniatowski; Virgile Caron",
    "corresponding_authors": "",
    "abstract": "Improving Importance Sampling estimators for rare event probabilities requires sharp approximations of conditional densities. This is achieved for events defined through large exceedances of the empirical mean of summands of a random walk, in the domain of large or moderate deviations. The approximation of conditional density of the trajectory of the random walk is handled on long runs. The length of those runs which is compatible with a given accuracy is discussed; simulated results are presented, which enlight the gain of the present approach over classical Importance Sampling schemes. Detailed algorithms are proposed.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2124318305",
    "type": "article"
  },
  {
    "title": "Efficient importance sampling schemes for a feed-forward network",
    "doi": "https://doi.org/10.1145/2517450",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "Leila Setayeshgar; Hui Wang",
    "corresponding_authors": "",
    "abstract": "The aim of this article is to construct efficient importance sampling schemes for a rare event, namely, the buffer overflow associated with a feed-forward network with discontinuous dynamics. This is done through a piecewise constant change of measure, which is based on a suitably constructed subsolution to an HJB equation. The main task is to change the measure such that the logarithmic asymptotic optimality is achieved. To that end, we find an upper bound on the second moment of the importance sampling estimator that yields optimality. Numerical simulations illustrate the validity of theoretical results.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2143527799",
    "type": "article"
  },
  {
    "title": "Editorial for Principles of Advanced Discrete Simulation",
    "doi": "https://doi.org/10.1145/2845147",
    "publication_date": "2015-12-28",
    "publication_year": 2015,
    "authors": "Gabriel Wainer",
    "corresponding_authors": "Gabriel Wainer",
    "abstract": "editorial Free Access Share on Editorial for Principles of Advanced Discrete Simulation Author: Gabriel A. Wainer Dept. of Systems and Computer Engineering Carleton University Centre for Visualization and Simulation (V-Sim), ON, Canada Dept. of Systems and Computer Engineering Carleton University Centre for Visualization and Simulation (V-Sim), ON, CanadaView Profile Authors Info & Claims ACM Transactions on Modeling and Computer SimulationVolume 26Issue 1December 2015 Article No.: 1epp 1–3https://doi.org/10.1145/2845147Published:28 December 2015Publication History 0citation182DownloadsMetricsTotal Citations0Total Downloads182Last 12 Months12Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2199137628",
    "type": "article"
  },
  {
    "title": "Overlapping Batches for the Assessment of Solution Quality in Stochastic Programs",
    "doi": "https://doi.org/10.1145/2701421",
    "publication_date": "2015-04-16",
    "publication_year": 2015,
    "authors": "David Love; Güzi̇n Bayraksan",
    "corresponding_authors": "",
    "abstract": "Overlapping Batch Means (OBM) has long been used in simulation as a method of reusing data to generate variance estimators with asymptotically lower variance. In this article, we apply the OBM method to stochastic programming by formulating a variant of the multiple replications procedure used for assessing solution quality. We give conditions under which the resulting optimality gap point estimators are strongly consistent, the optimality gap interval estimators are asymptotically valid, and the OBM variance estimators for optimality gap have asymptotically lower variances relative to their nonoverlapping counterparts [Meketon and Schmeiser 1984; Welch 1987]. We investigate computational efficiency, a combined measure of variance and computation time, providing guidelines on the degree of overlap. Numerical experiments on several test problems are presented, examining the small-sample behavior and the empirical computational efficiency of the overlapping batches method in this context.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2622687685",
    "type": "article"
  },
  {
    "title": "Reproducibility Report for the Paper: \"Spatial/Temporal Locality-based Load-sharing in Speculative Discrete Event Simulation on Multi-core Machines\"",
    "doi": "https://doi.org/10.1145/3674144",
    "publication_date": "2024-06-21",
    "publication_year": 2024,
    "authors": "Wen Jun Tan",
    "corresponding_authors": "Wen Jun Tan",
    "abstract": "All Badges available in this process are awarded to the paper “Spatial/Temporal Locality-based Load-sharing in Speculative Discrete Event Simulation on Multi-core Machines”. The authors have uploaded their artifacts to Zenodo, which ensures a long-term retention of the artifact. This paper can thus receive the Artifacts Available badge. The artifact allows for easy re-running of experiments for 14 figures and 4 tables. All of the dependencies are documented. The software in the artifact runs correctly with minimal intervention, and is relevant to the paper, earning the Artifacts Evaluated–Functional badge. The experimental results are reproduced in 9 experiments, which gains the Results Reproduced badge. Furthermore, since the artifact is also available on GitHub, the paper is assigned the Artifacts Evaluated–Reusable badge.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4399891641",
    "type": "article"
  },
  {
    "title": "Modeling of biogas production from hydrothermal carbonization products in a continuous anaerobic digester.",
    "doi": "https://doi.org/10.1145/3680281",
    "publication_date": "2024-07-23",
    "publication_year": 2024,
    "authors": "Benaissa Dekhici; Boumédiène Benyahia; Brahim Cherki; Luca Fiori; Gianni Andreottola",
    "corresponding_authors": "",
    "abstract": "The coupling between anaerobic digestion and hydrothermal carbonization (HTC) is a promising alternative for sustainable energy production. This study presents a dynamic model tailored for a lab-scale anaerobic digester operating on HTC products, specifically hydrochar and HTC liquor derived from sewage and agro-industrial digestate. Leveraging a modified version of the Anaerobic Model 2 (AM2), our simplified model of four states integrates pH and biomass decay rates into biomass kinetics. Simulation results of the mode were compared with experimental data collected over 164 days from the digester. The obtained results have proven the ability of the proposed model to predict the trend of the biogas production as well as important measured outputs of the bioreactor. The developed model could be used to control and optimize the performance of the digester, which provides potential for bioenergy production from waste streams such as digestate and digestate treated through the HTC process.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4400919639",
    "type": "article"
  },
  {
    "title": "Reproducibility Report for the Paper \"Performance Evaluation of Spintronic-Based Spiking Neural Networks Using Parallel Discrete-Event Simulation\"",
    "doi": "https://doi.org/10.1145/3680283",
    "publication_date": "2024-07-24",
    "publication_year": 2024,
    "authors": "Andreas Ruscheinski",
    "corresponding_authors": "Andreas Ruscheinski",
    "abstract": "The examined paper introduces Doryta , a simulator for Spiking Neural Networks implemented as a ROSS model. The software artifact is available as part of the paper’s supplemental material and can be accessed via the journal’s website. It is well documented and enhances the overall quality of the paper by providing access to the source code of the Doryta simulator and necessary scripts to reproduce the results shown in the figure. Using the script, we reproduced all major results presented in the paper. Thus, the paper qualifies for the Artifact Available , the Artifact Evaluated–Reusable , and the Artifact Validated–Results Reproduced badges.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4400955168",
    "type": "article"
  },
  {
    "title": "SpecSims: A Scalable Speculative Tree-based Simulation Cloning Framework For Finite Memory Machines",
    "doi": "https://doi.org/10.1145/3708885",
    "publication_date": "2024-12-26",
    "publication_year": 2024,
    "authors": "Srikanth B. Yoginath; Pratishtha Shukla; James Nutaro; Sudip K. Seal",
    "corresponding_authors": "",
    "abstract": "Simulation cloning is a technique in which cloned simulations whose state spaces differ partially from their parent simulation due to intervening events are spawned at runtime and concurrently advanced. It is a powerful method to carry-out what-if analysis by speculatively exploring and evaluating the impact of various permutations of intervening cascade of events. Due to the exponential growth in the number of possible clones even for a small number of distinct intervening events, the practical efficacy of the approach is often severely limited by the maximum available memory of the computing host. In this paper, we introduce a novel speculative simulation cloning framework that executes a simulation cloning campaign capable of efficiently exploring an exponentially large space of clone simulations created by permutation of intervening events under a finite memory constraint. We provide a theoretical analysis of the runtime characteristics of our proposed approach and highlight its novel advantages such as memory-aware and as-long-as-needed execution. In support of our analytical findings and to demonstrate its practical feasibility, we implement a prototype of the cloning framework on a shared memory system and report its performance characteristics in the context of a heat diffusion simulation, and a power grid simulation subject to cascading disruptions from geomagnetic disturbances.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4405794774",
    "type": "article"
  },
  {
    "title": "Performance and dependability evaluation of scalable massively parallel computer systems with conjoint simulation",
    "doi": "https://doi.org/10.1145/295251.295254",
    "publication_date": "1998-10-01",
    "publication_year": 1998,
    "authors": "A. Hein; Mario Dal Cin",
    "corresponding_authors": "",
    "abstract": "Computer systems are becoming more and more a part of our daily life; business and industry rely on their service, and the health of human beings depends on their correct functioning. Computer systems used for critical tasks have to be carefully designed and tested during the early design stage, the prototype phase, and their operational life. Methods and tools are required to support and facilitate this vital task. In this article, we tackle the issue of system-level performance and dependability analysis of fault-tolerant scalable computer systems. A modeling methodology called “Conjoint Simulation” is presented, which is based on the parti tioning of the system model and the combination of several modeling techniques. Object-oriented model construction and process-based simulation are applied for architecture and workload modeling, and timed Petri nets are the core modeling technique representing the failure scenarios and repair policies. Splitting the overall model and exploiting appropriate modeling techniques ease the development, maintenance, and extensibility of large-scale and complex simulation models. Furthermore, techniques are provided for hierarchical model construction, object-oriented workload modeling, and simulated error injection in order to perform combined performance and dependability analysis.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2003558993",
    "type": "article"
  },
  {
    "title": "Estimation of blocking probabilities in cellular networks with dynamic channel assignment",
    "doi": "https://doi.org/10.1145/511442.511445",
    "publication_date": "2002-01-01",
    "publication_year": 2002,
    "authors": "Felisa J. Vázquez-Abad; Lachlan L. H. Andrew; D. Everitt",
    "corresponding_authors": "",
    "abstract": "Blocking probabilities in cellular mobile communication networks using dynamic channel assignment are hard to compute for realistic sized systems. This computational difficulty is due to the structure of the state space, which imposes strong coupling constraints amongst components of the occupancy vector. Approximate tractable models have been proposed, which have product form stationary state distributions. However, for real channel assignment schemes, the product form is a poor approximation and it is necessary to simulate the actual occupancy process in order to estimate the blocking probabilities.Meaningful estimates of the blocking probability typically require an enormous amount of CPU time for simulation, since blocking events are usually rare. Advanced simulation approaches use importance sampling (IS) to overcome this problem. In this article, we study two regimes under which blocking is a rare event: low-load and high cell capacity. Our simulations use the standard clock (SC) method. For low load, we propose a change of measure that we call static ISSC , which has bounded relative error. For high capacity, we use a change of measure that depends on the current state of the network occupancy. This is the dynamic ISSC method. We prove that this method yields zero variance estimators for single clique models, and we empirically show the advantages of this method over naïve simulation for networks of moderate size and traffic loads.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2143988898",
    "type": "article"
  },
  {
    "title": "The distributed mission training integrated threat environment system architecture and design",
    "doi": "https://doi.org/10.1145/379525.379529",
    "publication_date": "2001-01-01",
    "publication_year": 2001,
    "authors": "Martin R. Stytz; Sheila B. Banks",
    "corresponding_authors": "",
    "abstract": "We describe the architecture, design, components, and functionality of the Distributed Mission Training Integrated Threat Environment (DMTITE) software. The DMTITE architecture and design support the development and run-time operation of computer-generated actors (CGAs) in distributed simulations. The architecture and design employ object-oriented techniques, component software, object frameworks, containerization, and rapid prototyping technologies. The DMTITE architecture and design consist of highly modular components where interdependencies are well defined and minimized. DMTITE is an open architecture and open design, and most component and framework code is open source. The DMTITE architecture and design have been implemented (including all system components and frameworks) and currently support a number of types of computer-generated actors. The DMTITE architecture, design, and implementation are capable of supporting multiple reasoning, vehicle dynamics, skill level, and migration requirements for any type of CGA.",
    "cited_by_count": 8,
    "openalex_id": "https://openalex.org/W1970324309",
    "type": "article"
  },
  {
    "title": "Inverse transformed density rejection for unbounded monotone densities",
    "doi": "https://doi.org/10.1145/1276927.1276931",
    "publication_date": "2007-09-01",
    "publication_year": 2007,
    "authors": "Wolfgang Hörmann; Josef Leydold; Gerhard Derflinger",
    "corresponding_authors": "",
    "abstract": "A new algorithm for sampling from largely arbitrary monotone, unbounded densities is presented. The user has to provide a program to evaluate the density and its derivative and the location of the pole. Then the setup of the new algorithm constructs different hat functions for the pole region and tail region, respectively. For the pole region a new method is developed that uses a transformed density rejection hat function of the inverse density. As the order of the pole is calculated in the setup, conditions that guarantee correctness of the constructed hat functions are provided. Numerical experiments indicate that the new algorithm works correctly and moderately fast for many different unbounded densities.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W1966794994",
    "type": "article"
  },
  {
    "title": "Approximate bivariate gamma generator with prespecified correlation and different marginal shapes",
    "doi": "https://doi.org/10.1145/1391978.1391982",
    "publication_date": "2008-09-01",
    "publication_year": 2008,
    "authors": "Simon Rosenfeld",
    "corresponding_authors": "Simon Rosenfeld",
    "abstract": "A new algorithm is proposed for generating approximate bivariate gamma random samples with a prespecified correlation coefficient and different marginal shapes. A distinctive feature of this approach is computational simplicity and ease of control. Extensive testing demonstrates high accuracy of the proposed algorithm. An S-PLUS code implementing the algorithm is provided. Regression lines produced by the technique are nearly linear, even when marginal shapes are drastically different. This feature makes the approach especially useful in simulation studies associated with linear regression problems. A real-life example of application to the analysis of heteroscedastic regression models is presented. This analysis is a part of a bigger study aimed at quantification of risk factors in cancer research. Two-dimensional probabilistic patterns produced by the algorithm are compared to those generated by the well-known technique by Schmeiser and Lal [1982].",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2006770236",
    "type": "article"
  },
  {
    "title": "Discrete-time heavy-tailed chains, and their properties in modeling network traffic",
    "doi": "https://doi.org/10.1145/1276927.1276930",
    "publication_date": "2007-09-01",
    "publication_year": 2007,
    "authors": "José Alberto Hernández; Iain Phillips; Javier Aracil",
    "corresponding_authors": "",
    "abstract": "The particular statistical properties found in network measurements, namely self-similarity and long-range dependence, cannot be ignored in modeling network and Internet traffic. Thus, despite their mathematical tractability, traditional Markov models are not appropriate for this purpose, since their memoryless nature contradicts the burstiness of transmitted packets. However, it is desirable to find a similarly tractable model which is, at the same time, rigorous at capturing the features of network traffic. This work presents discrete-time heavy-tailed chains , a tractable approach to characterize network traffic as a superposition of discrete-time “on/off” sources. This is a particular case of the generic “on/off” heavy-tailed model, thus shows the same statistical features as the former, particularly self-similarity and long-range dependence, when the number of aggregated sources approaches infinity. The model is then applicable to characterize a number of discrete-time communication systems, for instance, ATM and optical packet switching, to further derive meaningful performance metrics such as average burst duration and the number of active sources in a random instant.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2088409801",
    "type": "article"
  },
  {
    "title": "Green Simulation with Database Monte Carlo",
    "doi": "https://doi.org/10.1145/3429336",
    "publication_date": "2021-01-21",
    "publication_year": 2021,
    "authors": "Mingbin Feng; Jeremy Staum",
    "corresponding_authors": "",
    "abstract": "In a setting in which experiments are performed repeatedly with the same simulation model, green simulation means reusing outputs from previous experiments to answer the question currently being asked of the model. In this article, we address the setting in which experiments are run to answer questions quickly, with a time limit providing a fixed computational budget, and then idle time is available for further experimentation before the next question is asked. The general strategy is database Monte Carlo for green simulation: the output of experiments is stored in a database and used to improve the computational efficiency of future experiments. In this article, the database provides a quasi-control variate, which reduces the variance of the estimated mean response in a future experiment that has a fixed computational budget. We propose a particular green simulation procedure using quasi-control variates, addressing practical issues such as experiment design, and analyze its theoretical properties. We show that, under some conditions, the variance of the estimated mean response in an experiment with a fixed computational budget drops to zero over a sequence of repeated experiments, as more and more idle time is invested in creating databases. Our numerical experiments on the procedure show that using idle time to create databases of simulation output provides variance reduction immediately, and that the variance reduction grows over time in a way that is consistent with the convergence analysis.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3123679449",
    "type": "article"
  },
  {
    "title": "Ratatoskr: An Open-Source Framework for In-Depth Power, Performance, and Area Analysis and Optimization in 3D NoCs",
    "doi": "https://doi.org/10.1145/3472754",
    "publication_date": "2021-09-27",
    "publication_year": 2021,
    "authors": "Jan Moritz Joseph; Lennart Bamberg; Imad Hajjar; Behnam Razi Perjikolaei; Alberto García-Ortiz; Thilo Pionteck",
    "corresponding_authors": "",
    "abstract": "We introduce Ratatoskr , an open-source framework for in-depth power, performance, and area (PPA) analysis in Networks-on-Chips (NoCs) for 3D-integrated and heterogeneous System-on-Chips (SoCs). It covers all layers of abstraction by providing an NoC hardware implementation on Register Transfer Level (RTL), an NoC simulator on cycle-accurate level and an application model on transaction level. By this comprehensive approach, Ratatoskr can provide the following specific PPA analyses: Dynamic power of links can be measured within 2.4% accuracy of bit-level simulations while maintaining cycle-accurate simulation speed. Router power is determined from RTL-to-gate-level synthesis combined with cycle-accurate simulations. The performance of the whole NoC can be measured both via cycle-accurate and RTL simulations. The performance (i.e., timing) of individual routers and the NoC area are obtained from RTL synthesis results. Despite these manifold features, Ratatoskr offers easy two-step user interaction: (1) A single point-of-entry allows setting design parameters. (2) PPA reports are generated automatically. For both the input and the output, different levels of abstraction can be chosen for high-level rapid network analysis or low-level improvement of architectural details. The synthesizable NoC-RTL model shows improved total router power and area in comparison to a conventional standard router. As a forward-thinking and unique feature not found in other NoC PPA-measurement tools, Ratatoskr supports heterogeneous 3D integration that is one of the most promising integration paradigms for upcoming SoCs. Thereby, Ratatoskr lays the groundwork to design their communication architectures. The framework is publicly available at https://github.com/ratatoskr-project .",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W3201876660",
    "type": "article"
  },
  {
    "title": "Gradient estimation for a class of systems with bulk services",
    "doi": "https://doi.org/10.1145/1540530.1540534",
    "publication_date": "2009-06-01",
    "publication_year": 2009,
    "authors": "Bernd Heidergott; Felisa J. Vázquez-Abad",
    "corresponding_authors": "",
    "abstract": "This article presents a comparison of different gradient estimators for the sensitivity of waiting times in a bulk server system. Inspired by a transportation network, our model is that of a bursty arrival process that waits at a “platform” until the server is available (representing a train or bus ready for departure). At the departure epochs, all waiting passengers leave at once. The departure process is assumed to be a renewal process and, based on a limiting result, the interdeparture times are approximated by truncated normal random variables. The interarrival times are assumed to be identically and independently distributed (i.i.d.), with a general distribution of bounded density. We are interested in calculating the sensitivities of the total cumulative waiting time of all passengers with respect to the interdeparture times. For this general model where neither the interarrival times nor the interdeparture times are exponential, there is no analytical formula available. However, the estimation of such sensitivities is an important problem for flow control in such networks. We establish a Smoothed Perturbation Analysis (SPA), a Measure-Valued Differentiation (MVD), and a Score Function (SF) estimator, including numerical experiments.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W1982876709",
    "type": "article"
  },
  {
    "title": "Cross-layer interactions in multihop wireless sensor networks",
    "doi": "https://doi.org/10.1145/1870085.1870089",
    "publication_date": "2010-12-01",
    "publication_year": 2010,
    "authors": "Song Yang; Yuguang Fang",
    "corresponding_authors": "",
    "abstract": "In this article, we propose a constrained queueing model to investigate the performance of multihop wireless sensor networks. Specifically, the cross-layer interactions of rate admission control, traffic engineering, dynamic routing, and adaptive link scheduling are studied jointly with the proposed queueing model. In addition, the stochastic network utility maximization problem in wireless sensor networks is addressed within this framework. We propose an adaptive network resource allocation scheme, called the ANRA algorithm, which provides a joint solution to the multiple-layer components of the stochastic network utility maximization problem. We show that the proposed ANRA algorithm achieves a near-optimal solution, that is, (1-ϵ) of the global optimum network utility where ϵ can be arbitrarily small, with a trade-off with the average delay experienced in the network. The proposed ANRA algorithm enjoys the merit of self-adaptability through its online nature and thus is of particular interest for time-varying scenarios such as multihop wireless sensor networks.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2003388162",
    "type": "article"
  },
  {
    "title": "Resampled Regenerative Estimators",
    "doi": "https://doi.org/10.1145/2699718",
    "publication_date": "2015-05-08",
    "publication_year": 2015,
    "authors": "James M. Calvin; Marvin K. Nakayama",
    "corresponding_authors": "",
    "abstract": "We discuss some estimators for simulations of processes having multiple regenerative sequences. The estimators are obtained by resampling trajectories without and with replacement, which correspond to a type of U -statistic and a type of V -statistic, respectively. The U -statistic estimator turns out to be equivalent to the permuted regenerative estimator, which we previously proposed, but the V -statistic estimator is new. We compare analytically some properties of these estimators and the semiregenerative estimator. We show that when estimating the second moment of a cycle reward, the semiregenerative estimator has positive bias, which is strictly larger than the (positive) bias of the V -statistic estimator. The permuted estimator is unbiased. All of the estimators have the same asymptotic central limit behavior, with reduced asymptotic variance compared to the standard regenerative estimator. Some numerical results are included.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W1963928340",
    "type": "article"
  },
  {
    "title": "Constrained Community-Based Gene Regulatory Network Inference",
    "doi": "https://doi.org/10.1145/2688909",
    "publication_date": "2015-02-17",
    "publication_year": 2015,
    "authors": "Ferdinando Fioretto; Agostino Dovier; Enrico Pontelli",
    "corresponding_authors": "",
    "abstract": "The problem of gene regulatory network inference is a major concern of systems biology. In recent years, a novel methodology has gained momentum, called community network approach. Community networks integrate predictions from individual methods in a “metapredictor,” in order to compose the advantages of different methods and soften individual limitations. This article proposes a novel methodology to integrate prediction ensembles using constraint programming , a declarative modeling and problem solving paradigm. Constraint programming naturally allows the modeling of dependencies among components of the problem as constraints, facilitating the integration and use of different forms of knowledge. The new paradigm, referred to as constrained community network , uses constraints to capture properties of the regulatory networks (e.g., topological properties) and to guide the integration of knowledge derived from different families of network predictions. The article experimentally shows the potential of this approach: The addition of biological constraints can offer significant improvements in prediction accuracy.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2075823265",
    "type": "article"
  },
  {
    "title": "Toward Data Center Digital Twins via Knowledge-based Model Calibration and Reduction",
    "doi": "https://doi.org/10.1145/3604283",
    "publication_date": "2023-06-10",
    "publication_year": 2023,
    "authors": "Ruihang Wang; Deneng Xia; Zhiwei Cao; Yonggang Wen; Rui Tan; Xin Zhou",
    "corresponding_authors": "",
    "abstract": "Computational fluid dynamics (CFD) models have been widely used for prototyping data centers. Evolving them into high-fidelity and real-time digital twins is desirable for the online operations of data centers. However, CFD models often have unsatisfactory accuracy and high computation overhead. Manually calibrating the CFD model parameters is tedious and labor-intensive. Existing automatic calibration approaches apply heuristics to search the model configurations. However, each search step requires a long-lasting process of repeatedly solving the CFD model, rendering them impractical, especially for complex CFD models. This article presents Kalibre , a knowledge-based neural surrogate approach that calibrates a CFD model by iterating four steps of (i) training a neural surrogate model, (ii) finding the optimal parameters through neural surrogate retraining, (iii) configuring the found parameters back to the CFD model, and (iv) validating the CFD model using sensor-measured data. Thus, the parameter search is offloaded to the lightweight neural surrogate. To speed up Kalibre’s convergence, we incorporate prior knowledge in training data initialization and surrogate architecture design. With about ten hours of computation on a 64-core processor, Kalibre achieves mean absolute errors (MAEs) of 0.57°C and 0.88°C in calibrating the CFD models of two production data halls hosting thousands of servers. To accelerate CFD-based simulation, we further propose Kalibreduce that incorporates the energy balance principle to reduce the order of the calibrated CFD model. Evaluation shows the model reduction only introduces 0.1°C to 0.27°C extra errors while accelerating the CFD-based simulations by thousand times.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4380153102",
    "type": "article"
  },
  {
    "title": "VT-IO: A Virtual Time System Enabling High-fidelity Container-based Network Emulation for I/O Intensive Applications",
    "doi": "https://doi.org/10.1145/3635307",
    "publication_date": "2023-12-05",
    "publication_year": 2023,
    "authors": "Gong Chen; Zheng Hu; Yanfeng Qu; Dong Jin",
    "corresponding_authors": "",
    "abstract": "Network emulation allows unmodified code execution on lightweight containers to enable accurate and scalable networked application testing. However, such testbeds cannot guarantee fidelity under high workloads, especially when many processes concurrently request resources (e.g., CPU, disk I/O, GPU, and network bandwidth) that are more than the underlying physical machine can offer. A virtual time system enables the emulated hosts to maintain their own notion of virtual time. A container can stop advancing its time when not running (e.g., in an idle or suspended state). The existing virtual time systems focus on precise time management for CPU-intensive applications but are not designed to handle other operations, such as disk I/O, network I/O, and GPU computation. In this paper, we develop a lightweight virtual time system that integrates precise I/O time for container-based network emulation. We model and analyze the temporal error during I/O operations and develop a barrier-based time compensation mechanism in the Linux kernel. We also design and implement Dynamic Load Monitor (DLM) to mitigate the temporal error during I/O resource contention. VT-IO enables accurate virtual time advancement with precise I/O time measurement and compensation. The experimental results demonstrate a significant improvement in temporal error with the introduction of DLM. The temporal error is reduced from 7.889 seconds to 0.074 seconds when utilizing the DLM in the virtual time system. Remarkably, this improvement is achieved with an overall overhead of only 1.36% of the total execution time.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4389337758",
    "type": "article"
  },
  {
    "title": "A note on the quality of random variates generated by the ratio of uniforms method",
    "doi": "https://doi.org/10.1145/174619.174623",
    "publication_date": "1994-01-01",
    "publication_year": 1994,
    "authors": "Wolfgang Hörmann",
    "corresponding_authors": "Wolfgang Hörmann",
    "abstract": "The one-dimensional distribution of pseudorandom numbers generated by the ratio of uniforms method using linear congruential generators (LCGs) as the source of uniform random number is investigated in this note. Due to the two-dimensional lattice structure of LCGs there is always a comparable large gap without a point in the one-dimensional distribution of any ratio of uniforms method. Lower bounds for these probabilities only depending on the modulus and the Beyer quotient of the LCG are proved for the case that Cauchy normal or exponential random numbers are generated. These bounds justify the recommendation not to use the ratio of uniform method combined with LCGs.",
    "cited_by_count": 10,
    "openalex_id": "https://openalex.org/W2026233669",
    "type": "article"
  },
  {
    "title": "Mechanisms for user-invoked retraction of events in time warp",
    "doi": "https://doi.org/10.1145/140765.214308",
    "publication_date": "1991-07-01",
    "publication_year": 1991,
    "authors": "Greg Lomow; Samir R. Das; Richard M. Fujimoto",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Mechanisms for user-invoked retraction of events in time warp Authors: Greg Lomow Jade Simulations International Corp., #80, 1833 Crowchild Trail, N. W. Calgary, Alberta Canada T2M 457 Jade Simulations International Corp., #80, 1833 Crowchild Trail, N. W. Calgary, Alberta Canada T2M 457View Profile , Samir Ranjan Das College of Computing, Georgia Institute of Technology, Atlanta, GA College of Computing, Georgia Institute of Technology, Atlanta, GAView Profile , Richard M. Fujimoto College of Computing, Georgia Institute of Technology, Atlanta, GA College of Computing, Georgia Institute of Technology, Atlanta, GAView Profile Authors Info & Claims ACM Transactions on Modeling and Computer SimulationVolume 1Issue 3July 1991 pp 219–243https://doi.org/10.1145/140765.214308Published:01 July 1991Publication History 6citation237DownloadsMetricsTotal Citations6Total Downloads237Last 12 Months11Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W1999063474",
    "type": "article"
  },
  {
    "title": "AI",
    "doi": "https://doi.org/10.1145/149516.149519",
    "publication_date": "1992-10-01",
    "publication_year": 1992,
    "authors": "David P. Miller; R. James Firby; Paul A. Fishwick; Jeff Rothenberg",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on AI: what simulationists really need to know Authors: David P. Miller Massachusetts Institute of Technology, Cambridge Massachusetts Institute of Technology, CambridgeView Profile , R. James Firby Univ. of Chicago, Chicago, IL Univ. of Chicago, Chicago, ILView Profile , Paul A. Fishwick Univ. of Florida, Gainesville Univ. of Florida, GainesvilleView Profile , Jeff Rothenberg RAND Corp., Santa Monica, CA RAND Corp., Santa Monica, CAView Profile Authors Info & Claims ACM Transactions on Modeling and Computer SimulationVolume 2Issue 4Oct. 1992 pp 269–284https://doi.org/10.1145/149516.149519Online:01 October 1992Publication History 6citation669DownloadsMetricsTotal Citations6Total Downloads669Last 12 Months7Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2030992499",
    "type": "article"
  },
  {
    "title": "Automating parallel simulation using parallel time streams",
    "doi": "https://doi.org/10.1145/333296.333359",
    "publication_date": "1999-04-01",
    "publication_year": 1999,
    "authors": "Victor Yau",
    "corresponding_authors": "Victor Yau",
    "abstract": "This paper describes a package for parallel steady-state stochastic simulation that was designed to overcome problems caused by long simulation times experienced in our ongoing research in performance evaluation of high-speed and integrated-services communication networks, while maintaining basic statistical rigors of proper analysis of simulation output data. The package, named AKAROA, accepts ordinary (nonparallel) simulation programs, and alll further stages of stochastic simulation should be transparent for users. The package employs a new method of sequential estimation for the Multiple-Replications-in-Parallel scenario. All basic functions, including the transformation of originally nonparallel simulators into ones suitable for parallel execution, control of the precision of estimates, and stopping of parallel simulation processes when the required precision of the overall steady-state estimates is achieved, are automated. The package can be used on multiprocessor systems and/or heterogeneous computer networks, involving an arbitrary number of processors. The design issues, architecture, and implementation of AKAROA, as well as the results of its preliminary performance studies are presented",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2036165221",
    "type": "article"
  },
  {
    "title": "Terrain database interoperability issues in training with distributed interactive simulation",
    "doi": "https://doi.org/10.1145/259207.259221",
    "publication_date": "1997-07-01",
    "publication_year": 1997,
    "authors": "Guy A. Schiavone; S. Sureshchandran; Kenneth C. Hardis",
    "corresponding_authors": "",
    "abstract": "In Distributed Interactive Simulation (DIS), each participating node is responsible for maintaining its own model of the synthetic environment. Problems may arise if significant inconsistencies are allowed to exist between these separate world views, resulting in unrealistic simulation results or negative training, and a corresponding degradation of interoperability in a DIS simulation exercise. In the DIS community, this is known as the simulator terrain database (TDB) correlation problem. This is part of the larger synthetic environment correlation problem in DIS, which includes atmosphere, ocean, space, and a wide variety of dynamic effects, behaviors and models. In this article, we investigate the terrain database correlation problem and the resultant effects on interoperability in DIS systems. The fundamental elements of terrain databases designed for real-time distributed simulation are introduced. A generic data pipeline for terrain database generation systems is developed for the purpose of illustrating causes of the correlation problem and issues of terrain database fidelity. Implications of the problem are discussed, and testing methodologies are recommended for its mitigation. Several statistical methods have been developed to analyze consistency between various elements of the synthetic environment across DIS platforms. Correlation metrics have been formulated for terrain elevations and features. Comparisons and consistency of final rendered images have been addressed. Finally, a suite of software tools that has been developed for interoperability investigations and visual comparison of terrain databases is presented.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2045346983",
    "type": "article"
  },
  {
    "title": "Simulation in exponential families",
    "doi": "https://doi.org/10.1145/347823.347824",
    "publication_date": "1999-07-01",
    "publication_year": 1999,
    "authors": "Philippe Barbe; Michel Broniatowski",
    "corresponding_authors": "Philippe Barbe",
    "abstract": "An acceptance-rejection algorithm for the simulation of random variables in statistical exponential families is described. This algorithm does not require any prior knowledge of the family, except sufficient stati stics and the value of the parameter. It allows simulation from many members of the exponential family. We present some bounds on computing time, as well as the main properties of the empirical measures of samples simulated by our methods (functional Glivenko-Cantelli and central limit theorems). This algorithm is applied in order to evaluate the distribution of M-estimators under composite alternatives; we also propose its use in Bayesian statistics in order to simulate from posterior distributions.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2139574002",
    "type": "article"
  },
  {
    "title": "Controlling the Time Discretization Bias for the Supremum of Brownian Motion",
    "doi": "https://doi.org/10.1145/3177775",
    "publication_date": "2018-07-13",
    "publication_year": 2018,
    "authors": "Krzysztof Bisewski; Daan Crommelin; Michel Mandjes",
    "corresponding_authors": "",
    "abstract": "We consider the bias arising from time discretization when estimating the threshold crossing probability $w(b) := \\mathbb{P}(\\sup_{t\\in[0,1]} B_t > b)$, with $(B_t)_{t\\in[0,1]}$ a standard Brownian Motion. We prove that if the discretization is equidistant, then to reach a given target value of the relative bias, the number of grid points has to grow quadratically in $b$, as $b$ grows. When considering non-equidistant discretizations (with threshold-dependent grid points), we can substantially improve on this: we show that for such grids the required number of grid points is independent of $b$, and in addition we point out how they can be used to construct a strongly efficient algorithm for the estimation of $w(b)$. Finally, we show how to apply the resulting algorithm for a broad class of stochastic processes; it is empirically shown that the threshold-dependent grid significantly outperforms its equidistant counterpart.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2616336847",
    "type": "article"
  },
  {
    "title": "Mean-payoff Optimization in Continuous-time Markov Chains with Parametric Alarms",
    "doi": "https://doi.org/10.1145/3310225",
    "publication_date": "2019-10-31",
    "publication_year": 2019,
    "authors": "Christel Baier; Clemens Dubslaff; Ľuboš Korenčiak; Antonı́n Kučera; Vojtěch Řehák",
    "corresponding_authors": "",
    "abstract": "Continuous-time Markov chains with alarms (ACTMCs) allow for alarm events that can be non-exponentially distributed. Within parametric ACTMCs, the parameters of alarm-event distributions are not given explicitly and can be the subject of parameter synthesis. In this line, an algorithm is presented that solves the ε-optimal parameter synthesis problem for parametric ACTMCs with long-run average optimization objectives. The approach provided in this article is based on a reduction of the problem to finding long-run average optimal policies in semi-Markov decision processes (semi-MDPs) and sufficient discretization of the parameter (i.e., action) space. Since the set of actions in the discretized semi-MDP can be very large, a straightforward approach based on an explicit action-space construction fails to solve even simple instances of the problem. The presented algorithm uses an enhanced policy iteration on symbolic representations of the action space. Soundness of the algorithm is established for parametric ACTMCs with alarm-event distributions that satisfy four mild assumptions, fulfilled by many kinds of distributions. Exemplifying proofs for the satisfaction of these requirements are provided for Dirac, uniform, exponential, Erlang, and Weibull distributions in particular. An experimental implementation shows that the symbolic technique substantially improves the efficiency of the synthesis algorithm and allows us to solve instances of realistic size.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2964179600",
    "type": "article"
  },
  {
    "title": "Integrating Simulation and Numerical Analysis in the Evaluation of Generalized Stochastic Petri Nets",
    "doi": "https://doi.org/10.1145/3321518",
    "publication_date": "2019-10-31",
    "publication_year": 2019,
    "authors": "Armin Zimmermann; Thomas Hotz",
    "corresponding_authors": "",
    "abstract": "The standard existing performance evaluation methods for discrete-state stochastic models such as Petri nets either generate the reachability graph followed by a numerical solution of equations or use some variant of simulation. Both methods have characteristic advantages and disadvantages depending on the size of the reachability graph and type of performance measure. This article proposes a hybrid performance evaluation algorithm for the steady-state solution of Generalized Stochastic Petri Nets that integrates elements of both methods. It automatically adapts its behavior depending on the available size of main memory and number of model states. As such, the algorithm unifies simulation and numerical analysis in a joint framework. It is proved to result in an unbiased estimator whose variance tends to zero with increasing simulation time. The article extends earlier results with an algorithm variant that starts with a small maximum number of particles and increases them during the run to increase the efficiency in cases that are rapidly solved by regular simulation. The algorithm’s applicability is demonstrated through case studies, including an example where it outperforms the standard methods.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3004549220",
    "type": "article"
  },
  {
    "title": "SubsetTrio",
    "doi": "https://doi.org/10.1145/1921598.1921605",
    "publication_date": "2011-02-04",
    "publication_year": 2011,
    "authors": "Zhanpeng Jin; Allen C. Cheng",
    "corresponding_authors": "",
    "abstract": "Motivated by excessively high benchmarking efforts caused by a rapidly expanding design space, increasing system complexity, and prevailing practices based on ad-hoc and subjective schemes, this article seeks to enhance architecture exploration and evaluation efficiency by strategically integrating a genetic algorithm, 3-D geometrical rendering, and multivariate statistical analysis into one unified methodology framework— SubsetTrio —capable of subsetting any given benchmark suite based on its inherent workload characteristics, desired workload space coverage, and the total execution time intended by the user. By encoding both representativity (i.e., workload space coverage represented by the volume of the convex hull of benchmarks) and efficiency (i.e., total run time) as a co-optimization objective of a survival-of-the-fittest evolutionary algorithm, we can systematically determine a globally “fittest” (i.e., most representative and efficient) benchmark subset according to the workload space coverage threshold specified by the user. We demonstrate the usage, efficacy, and efficiency of the proposed technique by conducting a thorough case study on the SPEC benchmark suite, and evaluate its validity based on 50 commercial computer systems. Compared to the state-of-the-art statistical subsetting approach based on the Principal Component Analysis (PCA), SubsetTrio could select a significantly more time-efficient subset, while covering the same or higher workload space.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W1971653726",
    "type": "article"
  },
  {
    "title": "On deriving and incorporating multihop path duration estimates in VANET protocols",
    "doi": "https://doi.org/10.1145/1899396.1899402",
    "publication_date": "2011-02-01",
    "publication_year": 2011,
    "authors": "Josiane Nzouonta; Marvin K. Nakayama; Cristian Borcea",
    "corresponding_authors": "",
    "abstract": "The expected duration of multihop paths can be incorporated at different layers in the protocol stack to improve the performance of mobile ad hoc networks. This article presents two discrete-time and discrete-space Markov chain-based methods, DTMC-CA and DTMC-MFT, to estimate the duration of multihop road-based paths in vehicular ad hoc networks (VANET). The duration of such paths does not depend on individual nodes because packets can be forwarded by any vehicle located along the roads forming the path. DTMC-CA derives probabilistic measures based only on vehicle density for a traffic mobility model, which in this article is the microscopic Cellular Automaton (CA) freeway traffic model. DTMC-MFT generalizes the approach used by DTMC-CA to any vehicular mobility model by focusing on the macroscopic information of vehicles rather than their microscopic characteristics. The proposed analytical models produce performance-measure values comparable to simulation estimates from the validated CA traffic model. Furthermore, this article demonstrates the benefits of incorporating expected path durations into a VANET routing protocol. Simulation results show that the network overhead associated with route maintenance can be reduced to less than half by using the expected path durations.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2051900951",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1899396",
    "publication_date": "2011-02-01",
    "publication_year": 2011,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4237235031",
    "type": "paratext"
  },
  {
    "title": "On the statistical independence of nonlinear congruential pseudorandom numbers",
    "doi": "https://doi.org/10.1145/174619.174622",
    "publication_date": "1994-01-01",
    "publication_year": 1994,
    "authors": "Jürgen Eichenauer‐Herrmann; Harald Niederreiter",
    "corresponding_authors": "",
    "abstract": "Recently, several nonlinear congruential methods for generating uniform pseudorandom numbers have been proposed and analysed. In the present note, further statistical independence properties of a general class of nonlinear congruential pseudorandom number generators are established. The results that are obtained are essentially best possible in an asymptotic sense and show that the generated pseudorandom numbers model truly random numbers very closely in terms of asymptotic discrepancy.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W1968784238",
    "type": "article"
  },
  {
    "title": "Efficient Monte-Carlo simulation of a product-form model for a cellular system with dynamic resource sharing",
    "doi": "https://doi.org/10.1145/203091.203092",
    "publication_date": "1995-01-01",
    "publication_year": 1995,
    "authors": "Philip J. Fleming; D. Schaeffer; Burton Simon",
    "corresponding_authors": "",
    "abstract": "There are many ways for users to share the radio spectrum allocated to a cell in a cellular phone system. We analyze a commonly proposed scheme wh ere the cell is divided into s sectors. Each sector has exclusive access to a certain number of channels. The remaining channels reside in a “common pool” and are shared among the sectors. The smallest unit of bandwidth that can be borrowed from the common pool is a “carrier,” which consists of c channels. When viewed as a multidimensional birth-death process, the steady-state distribution of the number of active channels in each sector has a “product form,” but because the state space is large and has a nonlinear boundary, direct calculation of quantities of interest is usually impractical. Ross and Wang have developed a Monte-Carlo technique that applies to our problem. We significantly improve the efficiency of their technique when applied to our problem by including certain (nonlinear) control variates. The kinds of control variates we use can be applied to other loss systems as well. We also explore the effect of importance sampling for our system. In many cases the variance reduction achieved from the combination of importance sampling and control variates is far greater than from either method alone. For systems with blocking probabilities in the range 0.001 to 0.1, the variance of the system-blocking probability estimator can be reduced by several orders of magnitude.",
    "cited_by_count": 9,
    "openalex_id": "https://openalex.org/W2024369681",
    "type": "article"
  },
  {
    "title": "Web-based network analysis and design",
    "doi": "https://doi.org/10.1145/353735.353737",
    "publication_date": "2000-01-01",
    "publication_year": 2000,
    "authors": "Dhananjai M. Rao; R. Radhakrishnan; Philip A. Wilsey",
    "corresponding_authors": "",
    "abstract": "The gradual acceptance of high-performance networks as a fundamental component of today's computing environment has allowed applications to evolve from static entities located on specific hosts to dynamic, distributed entities that are resident on one or more hosts. In addition, vital components of software and data used by an application may be distributed across the local/wide area network. Given such a fluid and dynamic environment, the design and analysis of high-performance communication networks (using off-the-shelf components offered by third party manufacturers) has been further complicated by the diversity of the available components. To alleviate these problems and to address the verification and validation issues involved in engineering such complex networks, a web-based framework for the design and analysis of computer networks was developed. Using the framework, a designer can explore design alternatives by constructing and analyzing configurations of the design using components offered by different researchers and manufacturers. The framework provides a flexible and robust environment for selecting and verifying the optimal solution from a large and complex solution space. This paper presents issues involved in the design and development of the framework.",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2058813169",
    "type": "article"
  },
  {
    "title": "Random number generation with primitive pentanomials",
    "doi": "https://doi.org/10.1145/508366.508368",
    "publication_date": "2001-10-01",
    "publication_year": 2001,
    "authors": "Pei‐Chi Wu",
    "corresponding_authors": "Pei‐Chi Wu",
    "abstract": "This paper presents generalized feedback shift register (GFSR) generators with primitive polynomials x p + x p -1 + x q + x q -1 + 1. The recurrence of these generators can be efficiently computed. We adopt Fushimi's initialization scheme, which guarantees the k -distribution property. Statistical and timing results are presented.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2083315391",
    "type": "article"
  },
  {
    "title": "Error analysis of burst level modeling of active-idle sources",
    "doi": "https://doi.org/10.1145/1010621.1010624",
    "publication_date": "2004-07-01",
    "publication_year": 2004,
    "authors": "Yujing Wu; Weibo Gong",
    "corresponding_authors": "",
    "abstract": "It is often not feasible to simulate high-speed networks at the packet level. One common technique for simulation speedup is to model traffic at coarser timescales, which we refer to as the abstract simulation . In this article, we analyze the sources for the accuracy degradation in abstract simulation. Specifically, we consider burst level modeling of active-idle sources and study a queue fed by such sources. The arrival rates vary during active periods. The burst level model assumes constant rates during each active period. Therefore, it can not track queue-length variations within an active period, which leads to the queue-length evaluation error. We study two scenarios. In the first scenario, the queue is always busy whenever the source is on. During each active period, the evolution of the queue length can be viewed as an integration process of the arrival rate. In this case, the error in the mean queue length shows nice properties. It is only determined by the traffic characteristics and does not change with the utilization. In the multiple-flow case, the error is the sum of the errors caused by abstraction on individual flows. We also show that the error does not propagate for tree-like networks or networks with probabilistic routing. In the first scenario, under very general conditions, burst level modeling does not cause significant underevaluation of the queue length. In the second scenario, the queue may empty during on periods of the source. We call these empty periods E intervals. We quantify the error in the mean queue length, which depends on the numbers, lengths, and positions of the E intervals. The burst level model significantly underevaluates the queue length if such intervals occur often. High utilizations, strong traffic burstiness at the active-idle level, and small arrival granularity tend to reduce such occurrences and the error in the mean queue length. This explains why these conditions favor traffic modeling at coarser timescales. Our findings are not limited to the specific traffic sources and the time-abstraction techniques. Instead, they shed light on the conditions needed for abstract models to deliver evaluation fidelity in a general context.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2048219479",
    "type": "article"
  },
  {
    "title": "Machine Learning–enabled Scalable Performance Prediction of Scientific Codes",
    "doi": "https://doi.org/10.1145/3450264",
    "publication_date": "2021-04-23",
    "publication_year": 2021,
    "authors": "Gopinath Chennupati; Nandakishore Santhi; Phillip Romero; Stephan Eidenbenz",
    "corresponding_authors": "",
    "abstract": "Hardware architectures become increasingly complex as the compute capabilities grow to exascale. We present the Analytical Memory Model with Pipelines (AMMP) of the Performance Prediction Toolkit (PPT). PPT-AMMP takes high-level source code and hardware architecture parameters as input and predicts runtime of that code on the target hardware platform, which is defined in the input parameters. PPT-AMMP transforms the code to an (architecture-independent) intermediate representation, then (i) analyzes the basic block structure of the code, (ii) processes architecture-independent virtual memory access patterns that it uses to build memory reuse distance distribution models for each basic block, and (iii) runs detailed basic-block level simulations to determine hardware pipeline usage. PPT-AMMP uses machine learning and regression techniques to build the prediction models based on small instances of the input code, then integrates into a higher-order discrete-event simulation model of PPT running on Simian PDES engine. We validate PPT-AMMP on four standard computational physics benchmarks and present a use case of hardware parameter sensitivity analysis to identify bottleneck hardware resources on different code inputs. We further extend PPT-AMMP to predict the performance of a scientific application code, namely, the radiation transport mini-app SNAP. To this end, we analyze multi-variate regression models that accurately predict the reuse profiles and the basic block counts. We validate predicted SNAP runtimes against actual measured times.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3158362295",
    "type": "article"
  },
  {
    "title": "Reflective Nested Simulations Supporting Optimizations within Sequential Railway Traffic Simulators",
    "doi": "https://doi.org/10.1145/3467965",
    "publication_date": "2021-09-27",
    "publication_year": 2021,
    "authors": "Roman Diviš; Antonín Kavička",
    "corresponding_authors": "",
    "abstract": "This article describes and discusses railway-traffic simulators that use reflective nested simulations. Such simulations support optimizations (decision-making) with a focus on the selection of the most suitable solution where selected types of traffic problems are present. This approach allows suspension of the ongoing main simulation at a given moment and, by using supportive nested simulations (working with an appropriate lookahead), assessment of the different acceptable solution variants for the problem encountered—that is, a what-if analysis is carried out. The variant that provides the best predicted operational results (based on a specific criterion) is then selected for continuing the suspended main simulation. The proposed procedures are associated, in particular, with the use of sequential simulators specifically developed for railway traffic simulations. Special attention is paid to parallel computations of replications both of the main simulation and of supportive nested simulations. The concept proposed, applicable to railway traffic modelling, has the following advantages. First, the solution variants for the existing traffic situation are analyzed with respect to the feasibility of direct monitoring and evaluation of the natural traffic indicators or the appropriate (multi-criterial) function. The indicator values compare the results obtained from the variants being tested. Second, the supporting nested simulations, which potentially use additional hierarchic nesting, can also include future occurrences of random effects (such as train delay), thereby enabling us to realistically assess future traffic in stochastic conditions. The guidelines presented (for exploiting nested simulations within application projects with time constraints) are illustrated on a simulation case study focusing on traffic assessment related to the track infrastructure of a passenger railway station. Nested simulations support decisions linked with dynamic assignments of platform tracks to delayed trains. The use of reflective nested simulations is appropriate particularly in situations in which a reasonable number of admissible variants are to be analyzed within decision-making problem solution. This method is applicable especially to the support of medium-term (tactical) and long-term (strategic) planning. Because of rather high computational and time demands, nested simulations are not recommended for solving short-term (operative) planning/control problems.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W3203998916",
    "type": "article"
  },
  {
    "title": "Profile-driven regression for modeling and runtime optimization of mobile networks",
    "doi": "https://doi.org/10.1145/1842713.1842720",
    "publication_date": "2010-09-01",
    "publication_year": 2010,
    "authors": "Daniel W. Mc Clary; Violet R. Syrotiuk; Murat Külahçı",
    "corresponding_authors": "",
    "abstract": "Computer networks often display nonlinear behavior when examined over a wide range of operating conditions. There are few strategies available for modeling such behavior and optimizing such systems as they run. Profile-driven regression is developed and applied to modeling and runtime optimization of throughput in a mobile ad hoc network, a self-organizing collection of mobile wireless nodes without any fixed infrastructure. The intermediate models generated in profile-driven regression are used to fit an overall model of throughput, and are also used to optimize controllable factors at runtime. Unlike others, the throughput model accounts for node speed. The resulting optimization is very effective; locally optimizing the network factors at runtime results in throughput as much as six times higher than that achieved with the factors at their default levels.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2018205269",
    "type": "article"
  },
  {
    "title": "Automatic Model Generation for Gate-Level Circuit PDES with Reverse Computation",
    "doi": "https://doi.org/10.1145/3046685",
    "publication_date": "2017-04-30",
    "publication_year": 2017,
    "authors": "Elsa Gonsiorowski; Justin M. LaPre; Christopher D. Carothers",
    "corresponding_authors": "",
    "abstract": "Gate-level circuit simulation is an important step in the design and validation of complex circuits. This step of the process relies on existing libraries for gate specifications. We start with a generic gate model for Rensselaer’s Optimistic Simulation System, a parallel discrete-event simulation framework. This generic model encompasses all functionality needed by optimistic simulation using reverse computation. We then describe a parser system that uses a standardized gate library to create a specific model for simulation. The generated model is composed of several functions, including those needed for an accurate model of timing behavior. To quantify the improvements that an automatically generated model can have over a hand written model, we compare two gate library models: an automatically generated lsi -10 k library model and a previously investigated, handwritten, simplified gtech library model. We conclude that the automatically generated model is a more accurate model of actual hardware. In comparison to previous results, we find that the automatically generated model is able to achieve better optimistic simulation performance when measured against conservative simulation. To test the automatically generated model, we evaluate the performance of a simulation of a full-scale OpenSPARC T2 processor model. This model consists of nearly 6 million LPs. We achieve a peak performance of 1.63 million events per second during a conservative simulation. To understand the relatively weaker performance of optimistic simulation, we investigate hot spots of event activity and visually identify a workload imbalance.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2617651661",
    "type": "article"
  },
  {
    "title": "Efficient Protocol Testing Under Temporal Uncertain Event Using Discrete-event Network Simulations",
    "doi": "https://doi.org/10.1145/3490028",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Minh Hai Vu; Lisong Xu; Sebastian Elbaum; Wei Sun; Kevin Qiao",
    "corresponding_authors": "",
    "abstract": "Testing network protocol implementations is difficult mainly because of the temporal uncertain nature of network events. To evaluate the worst-case performance or detect the bugs of a network protocol implementation using network simulators, we need to systematically simulate the behavior of the network protocol under all possible cases of the temporal uncertain events, which is time consuming. The recently proposed Symbolic Execution based Interval Branching (SEIB) simulates a group of uncertain cases together in a single simulation branch and thus is more efficient than brute force testing. In this article, we argue that the efficiency of SEIB could be further significantly improved by eliminating unnecessary comparisons of the event timestamps. Specifically, we summarize and present three general types of unnecessary comparisons when SEIB is applied to a general network simulator, and then correspondingly propose three novel techniques to eliminate them. Our extensive simulations show that our techniques can improve the efficiency of SEIB by several orders of magnitude, such as from days to minutes.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4214862058",
    "type": "article"
  },
  {
    "title": "The DEVStone Metric: Performance Analysis of DEVS Simulation Engines",
    "doi": "https://doi.org/10.1145/3543849",
    "publication_date": "2022-06-11",
    "publication_year": 2022,
    "authors": "Román Cárdenas; Kevin Henares; Patricia Arroba; José L. Risco‐Martín; Gabriel Wainer",
    "corresponding_authors": "",
    "abstract": "The DEVStone benchmark allows us to evaluate the performance of discrete-event simulators based on the Discrete Event System (DEVS) formalism. It provides model sets with different characteristics, enabling the analysis of specific issues of simulation engines. However, this heterogeneity hinders the comparison of the results among studies, as the results obtained on each research work depend on the chosen subset of DEVStone models. We define the DEVStone metric based on the DEVStone synthetic benchmark and provide a mechanism for specifying objective ratings for DEVS-based simulators. This metric corresponds to the average number of times that a simulator can execute a selection of 12 DEVStone models in 1 minute. The variety of the chosen models ensures that we measure different particularities provided by DEVStone. The proposed metric allows us to compare various simulators and to assess the impact of new features on their performance. We use the DEVStone metric to compare some popular DEVS-based simulators.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4287831525",
    "type": "article"
  },
  {
    "title": "A General Framework to Simulate Diffusions with Discontinuous Coefficients and Local Times",
    "doi": "https://doi.org/10.1145/3559541",
    "publication_date": "2022-08-26",
    "publication_year": 2022,
    "authors": "Kailin Ding; Zhenyu Cui",
    "corresponding_authors": "",
    "abstract": "In this article, we propose an efficient general simulation method for diffusions that are solutions to stochastic differential equations with discontinuous coefficients and local time terms. The proposed method is based on sampling from the corresponding continuous-time Markov chain approximation. In contrast to existing time discretization schemes, the Markov chain approximation method corresponds to a spatial discretization scheme and is demonstrated to be particularly suited for simulating diffusion processes with discontinuities in their state space. We establish the theoretical convergence order and also demonstrate the accuracy and robustness of the method in numerical examples by comparing it to the known benchmarks in terms of root mean squared error, runtime, and the parameter sensitivity.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4293181978",
    "type": "article"
  },
  {
    "title": "Virtual Time III, Part 2: Combining Conservative and Optimistic Synchronization",
    "doi": "https://doi.org/10.1145/3505249",
    "publication_date": "2022-09-23",
    "publication_year": 2022,
    "authors": "David Jefferson; P. D. Barnes",
    "corresponding_authors": "",
    "abstract": "This is Part 2 of a trio of works intended to provide a unifying framework in which conservative and optimistic synchronization for parallel discrete event simulations can be freely and transparently combined in the same logical process on an event-by-event basis. In this article, we continue the outline of an approach called Unified Virtual Time (UVT) that was introduced in Part 1 , showing in detail via two extended examples how conservative synchronization can be refactored and combined with optimistic synchronization in the UVT framework. We describe UVT versions of both a basic time windowing algorithm called Unified Simple Time Windows and a refactored version of the Chandy-Misra-Bryant Null Message algorithm called Unified CMB .",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4296837996",
    "type": "article"
  },
  {
    "title": "Batching Adaptive Variance Reduction",
    "doi": "https://doi.org/10.1145/3573386",
    "publication_date": "2022-12-01",
    "publication_year": 2022,
    "authors": "Chenxiao Song; Reiichiro Kawai",
    "corresponding_authors": "",
    "abstract": "Adaptive Monte Carlo variance reduction is an effective framework for running a Monte Carlo simulation along with a parameter search algorithm for variance reduction, whereas an initialization step is required for preparing problem parameters in some instances. In spite of the effectiveness of adaptive variance reduction in various fields of application, the length of the preliminary phase has often been left unspecified for the user to determine on a case-by-case basis, much like in typical sequential frameworks. This uncertain element may possibly be even fatal in realistic finite-budget situations, since the pilot run may take most of the budget, or possibly use up all of it. To unnecessitate such an ad hoc initialization step, we develop a batching procedure in adaptive variance reduction, and provide an implementable formula of the learning rate in the parameter search which minimizes an upper bound of the theoretical variance of the empirical batch mean. We analyze decay rates of the minimized upper bound towards the minimal estimator variance with respect to the predetermined computing budget, and provide convergence results as the computing budget increases progressively when the batch size is fixed. Numerical examples are provided to support theoretical findings and illustrate the effectiveness of the proposed batching procedure.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W4311057999",
    "type": "article"
  },
  {
    "title": "Computing blocking probabilities in multiclass wavelength routing networks",
    "doi": "https://doi.org/10.1145/364996.365001",
    "publication_date": "2000-04-01",
    "publication_year": 2000,
    "authors": "Sridhar Ramesh; George N. Rouskas; Harry G. Perros",
    "corresponding_authors": "",
    "abstract": "We present an approximate analytical method to evaluate efficiently and accurately the call blocking probabilities in wavelength routing networks with multiple classes of calls. The model is fairly general and allows each source-destination pair to service calls of different classes, with each call occupying one wavelength per link. Our approximate analytical approach involves two steps. The arrival process of calls on some routes is first modified slightly to obtain an approximate multiclass network model. Next, all classes of calls on a particular route are aggregated to give an equivalent single-class model. Thus, path decomposition algorithms for single-class wavelength routing networks may be readily extended to the multiclass case. This article is a first step towards understanding the issues arising in wavelength routing networks that serve multiple classes of customers.",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2052163491",
    "type": "article"
  },
  {
    "title": "Corrigendum",
    "doi": "https://doi.org/10.1145/1122012.1122017",
    "publication_date": "2006-01-01",
    "publication_year": 2006,
    "authors": "Soumyadip Ghosh; Shane G. Henderson",
    "corresponding_authors": "",
    "abstract": "This note corrects an error in Ghosh and Henderson [2003].",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W1969805705",
    "type": "erratum"
  },
  {
    "title": "Exploiting regenerative structure to estimate finite time averages via simulation",
    "doi": "https://doi.org/10.1145/1225275.1225279",
    "publication_date": "2007-04-01",
    "publication_year": 2007,
    "authors": "Wanmo Kang; Perwez Shahabuddin; Ward Whitt",
    "corresponding_authors": "",
    "abstract": "We propose nonstandard simulation estimators of expected time averages over finite intervals [0, t ], seeking to enhance estimation efficiency. We make three key assumptions: (i) the underlying stochastic process has regenerative structure, (ii) the time average approaches a known limit as time t increases and (iii) time 0 is a regeneration time. To exploit those properties, we propose a residual-cycle estimator , based on data from the regenerative cycle in progress at time t , using only the data after time t . We prove that the residual-cycle estimator is unbiased and more efficient than the standard estimator for all sufficiently large t . Since the relative efficiency increases in t , the method is ideally suited to use when applying simulation to study the rate of convergence to the known limit. We also consider two other simulation techniques to be used with the residual-cycle estimator. The first involves overlapping cycles , paralleling the technique of overlapping batch means in steady-state estimation; multiple observations are taken from each replication, starting a new observation each time the initial regenerative state is revisited. The other technique is splitting , which involves independent replications of the terminal period after time t , for each simulation up to time t . We demonstrate that these alternative estimators provide efficiency improvement by conducting simulations of queueing models.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2015428690",
    "type": "article"
  },
  {
    "title": "Replicated Computations Results (RCR) Report for “Mesoscopic Modelling of Pedestrian Movement using C <scp>arma</scp> and its Tools”",
    "doi": "https://doi.org/10.1145/3177773",
    "publication_date": "2018-02-22",
    "publication_year": 2018,
    "authors": "Michele Loreti",
    "corresponding_authors": "Michele Loreti",
    "abstract": "“Mesoscopic modeling of pedestrian movement using Carma and its tools” uses C arma (Collective Adaptive Resource-sharing Markovian Agents), a specification language recently introduced for modeling CAS, to model spatially distributed systems in which the desired model lies between an individual-based (microscopic) and a population-based (macroscopic) spatial model. The impact on the system dynamics of changes to the topology of paths is studied via simulation. The provided experiments show that it is difficult to predict the effect of changes to the network structure and that even small variations can produce significant effects. This replicated computations results report focuses on the prototypical tool implementation used in the article to perform such analysis. The software was straightforward to install and use, and all the experimental results from the article could be reproduced.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2791169627",
    "type": "article"
  },
  {
    "title": "Statistical Analysis of Simulation Output from Parallel Computing",
    "doi": "https://doi.org/10.1145/3186327",
    "publication_date": "2018-07-31",
    "publication_year": 2018,
    "authors": "Chen Zhang; Nan Chen",
    "corresponding_authors": "",
    "abstract": "This article addresses statistical output analysis of transient simulations in the parallel computing environment with fixed computing time. Using parallel computing, most commonly used unbiased estimators based on the output sequence compromise. To rectify this issue, this article proposes an estimation procedure in the Bayesian framework. The proposed procedure is particularly useful when the computing time depends on the output value in each simulation replication. The effectiveness of our method is demonstrated through studies on queuing simulation and control chart simulation.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2886268438",
    "type": "article"
  },
  {
    "title": "Exact-Differential Simulation",
    "doi": "https://doi.org/10.1145/3301499",
    "publication_date": "2019-06-18",
    "publication_year": 2019,
    "authors": "M. Hanai; Toyotaro Suzumura; Elvis S. Liu; Georgios Theodoropoulos; Kalyan S. Perumalla",
    "corresponding_authors": "",
    "abstract": "Using computer simulation to analyze large-scale discrete event systems requires repeated executions with various scenarios or parameters. Such repeated executions can induce significant redundancy in event processing when the modification from a prior scenario to a new scenario is relatively minor, and when the altered scenario influences only a small part of the simulation. For example, in a city-scale traffic simulation, an altered scenario of blocking one junction may only affect a small part of the city for considerable length of time. However, traditional simulation approaches would still repeat the simulation for the whole city even when the changes are minor. In this article, we propose a new redundancy reduction technique for large-scale discrete event simulations, called exact-differential simulation , which simulates only the altered portions of scenarios and their influences in repeated executions while still achieving the same results as the re-execution of entire simulations. This article presents the main concepts of the exact-differential simulation, the design of its algorithm, and an approach to build an exact-differential simulation middleware that supports multiple applications of discrete event simulation. We also evaluate our approach by using two case studies, PHOLD benchmark and a traffic simulation of Tokyo.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2950021238",
    "type": "article"
  },
  {
    "title": "Statistical Abstraction for Multi-scale Spatio-temporal Systems",
    "doi": "https://doi.org/10.1145/3366023",
    "publication_date": "2019-10-31",
    "publication_year": 2019,
    "authors": "Michalis Michaelides; Jane Hillston; Guido Sanguinetti",
    "corresponding_authors": "",
    "abstract": "Modelling spatio-temporal systems exhibiting multi-scale behaviour is a powerful tool in many branches of science, yet it still presents significant challenges. Here, we consider a general two-layer (agent-environment) modelling framework, where spatially distributed agents behave according to external inputs and internal computation; this behaviour may include influencing their immediate environment, creating a medium over which agent-agent interaction signals can be transmitted. We propose a novel simulation strategy based on a statistical abstraction of the agent layer, which is typically the most detailed component of the model and can incur significant computational cost in simulation. The abstraction makes use of Gaussian Processes, a powerful class of non-parametric regression techniques from Bayesian Machine Learning, to estimate the agent’s behaviour given the environmental input. We show on two biological case studies how this technique can be used to speed up simulations and provide further insights into model behaviour.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2996571848",
    "type": "article"
  },
  {
    "title": "Extending Explicitly Modelled Simulation Debugging Environments with Dynamic Structure",
    "doi": "https://doi.org/10.1145/3338530",
    "publication_date": "2020-01-31",
    "publication_year": 2020,
    "authors": "Simon Van Mierlo; Hans Vangheluwe; Simon Breslav; Rhys Goldstein; Azam Khan",
    "corresponding_authors": "",
    "abstract": "The widespread adoption of Modelling and Simulation (M8S) techniques hinges on the availability of tools supporting each phase in the M8S-based workflow. This includes tasks such as specifying, implementing, experimenting with, as well as debugging, simulation models. We have previously developed a technique where advanced debugging environments are generated from an explicit behavioural model of the user interface and the simulator. These models are extracted from the code of existing modelling environments and simulators and instrumented with debugging operations. This technique can be reused for a large family of modelling formalisms but was not yet considered for dynamic-structure formalisms; debugging models in these formalisms is challenging, as entities can appear and disappear during simulation. In this article, we adapt and apply our approach to accommodate dynamic-structure formalisms. To this end, we present a modular, reusable approach, which includes an architecture and a workflow. We observe that to effectively debug dynamic-structure models, domain-specific visualizations developed by the modeller should be (re)used for debugging tasks. To demonstrate our technique, we use Dynamic-Structure DEVS (a formalism that includes the characteristics of discrete-event and agent-based modelling paradigms) and an implementation of its simulation semantics in the PythonPDEVS tool as a running example. We apply our technique on NetLogo, a popular multi-agent simulation tool, to demonstrate the generality of our approach.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3004559630",
    "type": "article"
  },
  {
    "title": "Static Analysis Techniques for Semiautomatic Synthesis of Message Passing Software Skeletons",
    "doi": "https://doi.org/10.1145/2778888",
    "publication_date": "2015-06-29",
    "publication_year": 2015,
    "authors": "Matthew Sottile; Jason Dagit; Deli Zhang; Gilbert Hendry; Damian Dechev",
    "corresponding_authors": "",
    "abstract": "The design of high-performance computing architectures requires performance analysis of large-scale parallel applications to derive various parameters concerning hardware design and software development. The process of performance analysis and benchmarking an application can be done in several ways with varying degrees of fidelity. One of the most cost-effective ways is to do a coarse-grained study of large-scale parallel applications through the use of program skeletons. The concept of a “program skeleton” that we discuss in this article is an abstracted program that is derived from a larger program where source code that is determined to be irrelevant is removed for the purposes of the skeleton. In this work, we develop a semiautomatic approach for extracting program skeletons based on compiler program analysis. We demonstrate correctness of our skeleton extraction process by comparing details from communication traces, as well as show the performance speedup of using skeletons by running simulations in the SST/macro simulator.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2018853662",
    "type": "article"
  },
  {
    "title": "Variance estimation and sequential stopping in steady-state simulations using linear regression",
    "doi": "https://doi.org/10.1145/2567907",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "Vivek Gupta; Sigrún Andradóttir; David Goldsman",
    "corresponding_authors": "",
    "abstract": "We propose a method for estimating the variance parameter of a discrete, stationary stochastic process that involves combining variance estimators at different run lengths using linear regression. We show that the estimator thus obtained is first-order unbiased and consistent under two distinct asymptotic regimes. In the first regime, the number of constituent estimators used in the regression is fixed and the numbers of observations corresponding to the component estimators grow in a proportional manner. In the second regime, the number of constituent estimators grows while the numbers of observations corresponding to each estimator remain fixed. We also show that for m -dependent stochastic processes, one can use regression to obtain asymptotically normally distributed variance estimators in the second regime. Analytical and numerical examples indicate that the new regression-based estimators give good mean-squared-error performance in steady-state simulations. The regression methodology presented in this article can also be applied to estimate the bias of variance estimators. As an example application, we present a new sequential-stopping rule that uses the estimate for bias to determine appropriate run lengths. Monte Carlo experiments indicate that this “bias-controlling” sequential-stopping method has the potential to work well in practice.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2039736595",
    "type": "article"
  },
  {
    "title": "QRF",
    "doi": "https://doi.org/10.1145/2724709",
    "publication_date": "2016-01-09",
    "publication_year": 2016,
    "authors": "Giuliano Casale; Vittoria de Nitto Personé; Evgenia Smirni",
    "corresponding_authors": "",
    "abstract": "The Quadratic Reduction Framework (QRF) is a numerical modeling framework to evaluate complex stochastic networks composed of resources featuring queueing, blocking, state-dependent behavior, service variability, temporal dependence, or a subset thereof. Systems of this kind are abstracted as network of queues for which QRF supports two common blocking mechanisms: blocking-after-service and repetitive-service random-destination. State-dependence is supported for both routing probabilities and service processes. To evaluate these models, we develop a novel mapping, called Blocking-Aware Quadratic Reduction (BQR) , which can describe an intractably large Markov process by a large set of linear inequalities. Each model is then analyzed for bounds or approximate values of performance metrics using optimization programs that provide different levels of accuracy and error guarantees. Numerical results demonstrate that QRF offers very good accuracy and much greater scalability than exact analysis methods.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2232050021",
    "type": "article"
  },
  {
    "title": "Guests Editors’ Editorial Note on Special Issue of <i>Advances in Cellular Automata Modeling</i>",
    "doi": "https://doi.org/10.1145/2856511",
    "publication_date": "2016-01-13",
    "publication_year": 2016,
    "authors": "Stefania Bandini; Georgios Ch. Sirakoulis; Giuseppe Vizzari",
    "corresponding_authors": "",
    "abstract": "editorial Free Access Share on Guests Editors’ Editorial Note on Special Issue of Advances in Cellular Automata Modeling Editors: Stefania Bandini University of Milano-Bicocca, Milano, Italy University of Milano-Bicocca, Milano, ItalyView Profile , Georgios Ch. Sirakoulis Democritus University of Thrace, Xanthi, Greece Democritus University of Thrace, Xanthi, GreeceView Profile , Giuseppe Vizzari University of Milano-Bicocca, Milano, Italy University of Milano-Bicocca, Milano, ItalyView Profile Authors Info & Claims ACM Transactions on Modeling and Computer SimulationVolume 26Issue 3February 2016 Article No.: 17pp 1–3https://doi.org/10.1145/2856511Published:13 January 2016Publication History 1citation180DownloadsMetricsTotal Citations1Total Downloads180Last 12 Months9Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2270147472",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2043635",
    "publication_date": "2011-12-01",
    "publication_year": 2011,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "There are a number of sources of randomness that arise in military airlift operations. However, the cost of uncertainty can be difficult to estimate, and is easy to overestimate if we use simplistic decision rules. Using data from Canadian military ...",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4254219273",
    "type": "paratext"
  },
  {
    "title": "In defense of discrete-event simulation",
    "doi": "https://doi.org/10.1145/159737.159743",
    "publication_date": "1993-10-01",
    "publication_year": 1993,
    "authors": "Ernest H. Page",
    "corresponding_authors": "Ernest H. Page",
    "abstract": "article In defense of discrete-event simulation Share on Author: Ernest H. Page Virginia Tech, Blacksburg Virginia Tech, BlacksburgView Profile Authors Info & Claims ACM Transactions on Modeling and Computer SimulationVolume 3Issue 4Oct. 1993 pp 281–286https://doi.org/10.1145/159737.159743Online:01 October 1993Publication History 5citation322DownloadsMetricsTotal Citations5Total Downloads322Last 12 Months3Last 6 weeks3 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",
    "cited_by_count": 7,
    "openalex_id": "https://openalex.org/W2089921537",
    "type": "article"
  },
  {
    "title": "A reconfigurable hardware approach to network simulation",
    "doi": "https://doi.org/10.1145/244804.244809",
    "publication_date": "1997-01-01",
    "publication_year": 1997,
    "authors": "D. Stiliadis; Anujan Varma",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on A reconfigurable hardware approach to network simulation Authors: Dimitrios Stiliadis Univ. of California, Santa Cruz Univ. of California, Santa CruzView Profile , Anujan Varma Univ. of California, Santa Cruz Univ. of California, Santa CruzView Profile Authors Info & Claims ACM Transactions on Modeling and Computer SimulationVolume 7Issue 1Jan. 1997 pp 131–156https://doi.org/10.1145/244804.244809Published:01 January 1997Publication History 5citation609DownloadsMetricsTotal Citations5Total Downloads609Last 12 Months8Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2039157440",
    "type": "article"
  },
  {
    "title": "Fast generation of order statistics",
    "doi": "https://doi.org/10.1145/566392.566393",
    "publication_date": "2002-04-01",
    "publication_year": 2002,
    "authors": "Wolfgang Hörmann; Gerhard Derflinger",
    "corresponding_authors": "",
    "abstract": "Generating a single order statistic without generating the full sample can be an important task for simulations. If the density and the CDF of the distribution are given, then it is no problem to compute the density of the order statistic. In the main theorem it is shown that the concavity properties of that density depend directly on the distribution itself. Especially for log-concave distributions, all order statistics have log-concave distributions themselves. So recently suggested automatic transformed density rejection algorithms can be used to generate single order statistics. This idea leads to very fast generators. For example for the normal and gamma distributions, the suggested new algorithms are between 10 and 60 times faster than the algorithms suggested in the literature.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2063904488",
    "type": "article"
  },
  {
    "title": "A sequential procedure for simultaneous estimation of several means",
    "doi": "https://doi.org/10.1145/169702.169690",
    "publication_date": "1993-04-01",
    "publication_year": 1993,
    "authors": "Kimmo Raatikainen",
    "corresponding_authors": "Kimmo Raatikainen",
    "abstract": "article Free Access Share on A sequential procedure for simultaneous estimation of several means Author: Kimmo E. E. Raatikainen University of Helsinki University of HelsinkiView Profile Authors Info & Claims ACM Transactions on Modeling and Computer SimulationVolume 3Issue 2April 1993 pp 108–133https://doi.org/10.1145/169702.169690Published:01 April 1993Publication History 4citation333DownloadsMetricsTotal Citations4Total Downloads333Last 12 Months5Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 6,
    "openalex_id": "https://openalex.org/W2015348768",
    "type": "article"
  },
  {
    "title": "Declaration of unknowns in DAE-based hybrid system specification",
    "doi": "https://doi.org/10.1145/778553.778555",
    "publication_date": "2003-01-01",
    "publication_year": 2003,
    "authors": "D.A. van Beek; V. Bos; J.E. Rooda",
    "corresponding_authors": "",
    "abstract": "The majority of hybrid languages are based on the assumption that discontinuities in differential variables at discrete events are modeled by explicit mappings. When there are algebraic equations restricting the allowed new values of the differential variables, explicit remapping of differential variables forces the modeler to solve the algebraic equations. To overcome this difficulty, hybrid languages use many different language elements. This article shows that only one language element is needed for this purpose: an unknown declaration, which allows the explicit declaration of a variable as unknown. The syntax and semantics of unknown declarations are discussed. Examples are given, using the Chi language, in which unknown declarations are used for modeling multi-body collision, steady-state initialization, and consistent initialization of higher index systems. It is also illustrated how the declaration of unknowns can help to clarify the structure of the system of equations, and how it can help the modeler detect structurally singular systems of equations.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2059305253",
    "type": "article"
  },
  {
    "title": "Novel Approaches to Feasibility Determination",
    "doi": "https://doi.org/10.1145/3426359",
    "publication_date": "2021-01-08",
    "publication_year": 2021,
    "authors": "Daniel Solow; Roberto Szechtman; Enver Yücesan",
    "corresponding_authors": "",
    "abstract": "This article proposes two-stage Bayesian and frequentist procedures for determining whether a number of systems—each characterized by the same number of performance measures—belongs to a set Γ defined by a finite collection of linear inequalities. A system is “in (not in) Γ” if the vector of the means is in (not in) Γ, where the means must be estimated using Monte Carlo simulation. We develop algorithms for classifying the systems with a user-specified level of confidence using the minimum number of simulation replications so the probability of correct classification over all r systems satisfies a user-specified minimum value. Once the analyst provides prior values for the means and standard deviations of the random variables in each system, an initial number of simulation replications is performed to obtain current estimates of the means and standard deviations to assess whether the systems can be classified with the desired level of confidence. For any system that cannot be classified, heuristics are proposed to determine the number of additional simulation replications that would enable correct classification. Our contributions include the introduction of intuitive algorithms that are not only easy to implement, but also effective with their performance. Compared to other feasibility determination approaches, they also appear to be competitive. While the algorithms were initially developed in settings where system variance is assumed to be known and the random variables are independent, their performance remains satisfactory when those assumptions are relaxed.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3118719295",
    "type": "article"
  },
  {
    "title": "Explicit Modeling of Personal Space for Improved Local Dynamics in Simulated Crowds",
    "doi": "https://doi.org/10.1145/3462202",
    "publication_date": "2021-07-23",
    "publication_year": 2021,
    "authors": "Omar Hesham; Gabriel Wainer",
    "corresponding_authors": "",
    "abstract": "Crowd simulation demands careful consideration in regard to the classic trade-off between accuracy and efficiency. Particle-based methods have seen success in various applications in architecture, military, urban planning, and entertainment. This method focuses on local dynamics of individuals in large crowds, with a focus on serious games and entertainment. The technique uses an area-based penalty force that captures the infringement of each entity's personal space. This method does not need a costly nearest-neighbor search and allows for an inherently data-parallel implementation capable of simulating thousands of entities at interactive frame rates. The algorithm reproduces personal space compression around motion barriers for moving crowds and around points of interest for static crowds.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3184040039",
    "type": "article"
  },
  {
    "title": "Improved Penalty Function with Memory for Stochastically Constrained Optimization via Simulation",
    "doi": "https://doi.org/10.1145/3465333",
    "publication_date": "2021-08-16",
    "publication_year": 2021,
    "authors": "Jungmin Han; Seong‐Hee Kim; Chuljin Park",
    "corresponding_authors": "",
    "abstract": "Penalty function with memory (PFM) in Park and Kim [2015] is proposed for discrete optimization via simulation problems with multiple stochastic constraints where performance measures of both an objective and constraints can be estimated only by stochastic simulation. The original PFM is shown to perform well, finding a true best feasible solution with a higher probability than other competitors even when constraints are tight or near-tight. However, PFM applies simple budget allocation rules (e.g., assigning an equal number of additional observations) to solutions sampled at each search iteration and uses a rather complicated penalty sequence with several user-specified parameters. In this article, we propose an improved version of PFM, namely IPFM, which can combine the PFM with any simulation budget allocation procedure that satisfies some conditions within a general DOvS framework. We present a version of a simulation budget allocation procedure useful for IPFM and introduce a new penalty sequence, namely PS 2 + , which is simpler than the original penalty sequence yet holds convergence properties within IPFM with better finite-sample performances. Asymptotic convergence properties of IPFM with PS 2 + are proved. Our numerical results show that the proposed method greatly improves both efficiency and accuracy compared to the original PFM.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W3193927328",
    "type": "article"
  },
  {
    "title": "Guest Editorial for the TOMACS Special Issue on the Principles of Advanced Discrete Simulation (PADS)",
    "doi": "https://doi.org/10.1145/3084543",
    "publication_date": "2017-04-30",
    "publication_year": 2017,
    "authors": "Navonil Mustafee; Young‐Jun Son; Simon J. E. Taylor",
    "corresponding_authors": "",
    "abstract": "editorial Free Access Share on Guest Editorial for the TOMACS Special Issue on the Principles of Advanced Discrete Simulation (PADS) Editors: Navonil Mustafee PhD Business School University of Exeter Streatham Court, Rennes Drive, Exeter EX4 4PU PhD Business School University of Exeter Streatham Court, Rennes Drive, Exeter EX4 4PUView Profile , Young-Jun Son PhD Department of Systems and Industrial Engineering University of Arizona, Tucson, AZ 85721-0020 PhD Department of Systems and Industrial Engineering University of Arizona, Tucson, AZ 85721-0020View Profile , Simon J. E. Taylor PhD Department of Computer Science Brunel University London Kingston Lane, Uxbridge, Middlesex UB8 3PH PhD Department of Computer Science Brunel University London Kingston Lane, Uxbridge, Middlesex UB8 3PHView Profile Authors Info & Claims ACM Transactions on Modeling and Computer SimulationVolume 27Issue 2April 2017 Article No.: 7epp 1–3https://doi.org/10.1145/3084543Published:06 July 2017Publication History 2citation210DownloadsMetricsTotal Citations2Total Downloads210Last 12 Months17Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2732315478",
    "type": "editorial"
  },
  {
    "title": "A Tool for xMAS-Based Modeling and Analysis of Communication Fabrics in Simulink",
    "doi": "https://doi.org/10.1145/3005446",
    "publication_date": "2017-07-31",
    "publication_year": 2017,
    "authors": "Xueqian Zhao; Zhonghai Lu",
    "corresponding_authors": "",
    "abstract": "The eXecutable Micro-Architectural Specification (xMAS) language developed in recent years finds an effective way to model on-chip communication fabrics and enables performance-bound analysis with network calculus at the micro-architectural level. For network-on-Chip (NoC) performance analysis, model validation is essential to ensure correctness and accuracy. In order to facilitate the xMAS modeling and corresponding analysis validation, this work presents a unified platform based on xMAS in Simulink. The platform provides a friendly graphical user interface for xMAS modeling and parameter setup by taking advantages of the Simulink modeling environment. The regulator and latency-rate sever are added to the xMAS primitive set to support typical flow and service behaviors. Hierarchical model build-up and Verilog-HDL code generation are essentially supported to manage complex models and to conduct cycle-accurate bit-accurate simulations. Based on the generated simulation models of xMAS, this tool is applied to evaluate the tightness of analytical delay bound results. We demonstrate the application as well as the work flow of the xMAS tool through a two-agent communication example and an all-to-one communication example with a tree topology.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2749296210",
    "type": "article"
  },
  {
    "title": "Estimating Large Delay Probabilities in Two Correlated Queues",
    "doi": "https://doi.org/10.1145/3158667",
    "publication_date": "2018-01-23",
    "publication_year": 2018,
    "authors": "Ewan Jacov Cahen; Michel Mandjes; Bert Zwart",
    "corresponding_authors": "",
    "abstract": "This article focuses on evaluating the probability that both components of a two-dimensional stochastic process will ever, but not necessarily at the same time, exceed some large level u . An important application is in determining the probability of large delays occurring in two correlated queues. Since exact analysis of this probability seems prohibitive, we focus on deriving asymptotics and on developing efficient simulations techniques. Large deviations theory is used to characterise logarithmic asymptotics. The second part of this article focuses on efficient simulation techniques. Using “nearest-neighbour random walk” as an example, we first show that a “naive” implementation of importance sampling, based on the decay rate, is not asymptotically efficient. A different approach, which we call partitioned importance sampling, is developed and shown to be asymptotically efficient. The results are illustrated through various simulation experiments.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2784563913",
    "type": "article"
  },
  {
    "title": "Replicated Computations Results (RCR) Report for “A Holistic Approach for Collaborative Workload Execution in Volunteer Clouds”",
    "doi": "https://doi.org/10.1145/3182167",
    "publication_date": "2018-02-22",
    "publication_year": 2018,
    "authors": "Andrea Vandin",
    "corresponding_authors": "Andrea Vandin",
    "abstract": "“A Holistic Approach for Collaborative Workload Execution in Volunteer Clouds” [3] proposes a novel approach to task scheduling in volunteer clouds. Volunteer clouds are decentralized cloud systems based on collaborative task execution, where clients voluntarily share their own unused computational resources. By using simulation-based statistical analysis techniques—in particular, statistical model checking—the authors show that their approach can outperform existing distributed task scheduling algorithms in the case of computation-intensive workloads. The analysis considered a realistic workload benchmark provided by Google. This replicated computations results report focuses on the prototypical tool implementation used in the article to perform such analysis. The software was straightforward to install and use, and a representative part of the experimental results from the article could be reproduced in reasonable time using a standard laptop.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2789746602",
    "type": "article"
  },
  {
    "title": "Guest editorial—Simulation for training",
    "doi": "https://doi.org/10.1145/259207.259208",
    "publication_date": "1997-07-01",
    "publication_year": 1997,
    "authors": "Osman Balcı",
    "corresponding_authors": "Osman Balcı",
    "abstract": "No abstract available.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2048866459",
    "type": "editorial"
  },
  {
    "title": "An Adaptive Persistence and Work-stealing Combined Algorithm for Load Balancing on Parallel Discrete Event Simulation",
    "doi": "https://doi.org/10.1145/3364218",
    "publication_date": "2020-03-20",
    "publication_year": 2020,
    "authors": "Wenjie Tang; Yiping Yao; Tianlin Li; Xiao Song; Feng Zhu",
    "corresponding_authors": "",
    "abstract": "Load imbalance has always been a crucial challenge in Parallel Discrete Event Simulation (PDES). In the past few years, we have witnessed an increased interest in using multithreading PDES on multi/many-core platforms. In multithreading PDES, migrating logical processes and coordinating threads are more convenient and cause lower overhead, which provides a better circumstance for load balancing. However, current algorithms, including the persistence-based scheme and work-stealing-based scheme, have their drawbacks. On one hand, persistence-based load balancers, which use the historical data to predict the future, will inevitably make some error. On the other hand, the work-stealing scheme ignores the application-related characteristic, which may limit the potential performance improvement. In this article, we propose an adaptive persistence and work-stealing combined dynamic load balancing algorithm (APWS). The algorithm detects load imbalance, adaptively rebalances the distribution of logical processes, and uses a greedy lock-free work-stealing scheme to eliminate bias at runtime. We assess the performance of the APWS algorithm by a series of experiments. Results demonstrate that our APWS algorithm achieves better performance in different scenarios.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3013727557",
    "type": "article"
  },
  {
    "title": "Mechanisms for Precise Virtual Time Advancement in Network Emulation",
    "doi": "https://doi.org/10.1145/3478867",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Vignesh Babu; David M. Nicol",
    "corresponding_authors": "",
    "abstract": "Network emulators enable rapid prototyping and testing of applications. In a typical emulation, the execution order and process execution burst lengths are managed by the host platform’s operating system, largely independent of the emulator. Timerbased mechanisms are typically used, but the imprecision of timer firings introduces imprecision in the advancement of time. This leads to statistical variation in behavior that is not due to the model. This article describes an open-source tool called Kronos, which provides a set of mechanisms for precise instruction-level tracking of process execution and control over execution order of containers, thus improving the mapping of executed behavior to advancement in time. This, and control of execution and placement of emulated processes in virtual time make the behavior of the emulation independent of the CPU resources of the platform that hosts the emulation. Under Kronos each process has its own virtual clock that is advanced based on a count of the number of \\( \\times \\) 86 assembly instructions executed by its children. Two types of instruction counting techniques are discussed: (1) hardware-assisted mechanisms that are transparent to the executing application and (2) binary instrumentation-assisted mechanisms that modify the executing binary. We analyze the overheads associated with each approach and experimentally demonstrate the impact of Kronos’ time advancement precision by comparing it against emulations that, like Kronos, are embedded in virtual time, but unlike Kronos rely on Linux timers to control virtual machines and measure their progress in virtual time. We present two useful applications where Kronos aids in generating high-fidelity emulation results at low hardware costs: (1) analyzing protocol performance and (2) enabling analysis of cyber physical control systems. We also discuss limitations associated with simple linear conversions between instruction counts and ascribed virtual time and develop and evaluate more accurate virtual time conversion models.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4214885443",
    "type": "article"
  },
  {
    "title": "A Workflow Architecture for Cloud-based Distributed Simulation",
    "doi": "https://doi.org/10.1145/3503510",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Nauman Riaz Chaudhry; Anastasia Anagnostou; Simon J. E. Taylor",
    "corresponding_authors": "",
    "abstract": "Distributed Simulation has still to be adopted significantly by the wider simulation community. Reasons for this might be that distributed simulation applications are difficult to develop and access to multiple computing resources are required. Cloud computing offers low-cost on-demand computing resources. Developing applications that can use cloud computing can be also complex, particularly those that can run on different clouds. Cloud-based Distributed Simulation (CBDS) is potentially attractive, as it may solve the computing resources issue as well as other cloud benefits, such as convenient network access. However, as possibly shown by the lack of sustainable approaches in the literature, the combination of cloud and distributed simulation may be far too complex to develop a general approach. E-Infrastructures have emerged as large-scale distributed systems that support high-performance computing in various scientific fields. Workflow Management Systems (WMS) have been created to simplify the use of these e-Infrastructures. There are many examples of where both technologies have been extended to use cloud computing. This article therefore presents our investigation into the potential of using these technologies for CBDS in the above context and the WORkflow architecture for cLoud-based Distributed Simulation (WORLDS), our contribution to CBDS. We present an implementation of WORLDS using the CloudSME Simulation Platform that combines the WS-PGRADE/gUSE WMS with the CloudBroker Platform as a Service. The approach is demonstrated with a case study using an agent-based distributed simulation of an Emergency Medical Service in REPAST and the Portico HLA RTI on the Amazon EC2 cloud.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W4214906695",
    "type": "article"
  },
  {
    "title": "Site-Specific Models for Realistic Wireless Network Simulation",
    "doi": "https://doi.org/10.1145/2661630",
    "publication_date": "2014-11-13",
    "publication_year": 2014,
    "authors": "Cigdem Sengul; Mustafa Al-Bado; Anja Feldmann",
    "corresponding_authors": "",
    "abstract": "The utility of simulation-based performance evaluation for wireless networking has been under scrutiny as the community relies increasingly on testbed-based performance evaluations. While testbeds are invaluable tools for realistic network and protocol evaluation, these results are generally obtained after cumbersome system implementation and debugging. On the other hand, realistic simulation models can reduce the time and effort for concept testing of ideas. To this end, we develop BOWLsim PHY layer models—propagation, frame detection, and frame error models—based on extensive measurements in the Berlin Open Wireless Lab indoor and outdoor testbeds. Our models are integrated into the ns-3 simulator. We run an extensive measurement and simulation study, which illustrates that BOWLsim models represent network conditions at the physical (PHY) layer and transport layer accurately.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W1999446209",
    "type": "article"
  },
  {
    "title": "MaD0",
    "doi": "https://doi.org/10.1145/2856693",
    "publication_date": "2016-01-06",
    "publication_year": 2016,
    "authors": "Jie Li; Jianliang Zheng; Paula A. Whitlock",
    "corresponding_authors": "",
    "abstract": "In this article, we present MaD0, an ultrafast nonlinear pseudorandom number generator (PRNG) for noncryptographic applications. MaD0 uses byte-oriented operations for state initialization and fast integer-oriented operations for state transition and pseudorandom number generation. Its state transition follows a pseudorandom mapping. MaD0 generates high-quality pseudorandom numbers and reaches a generation speed of half cycle per byte on an Intel Core i3 processor. It has a state space of 2,240 bits and an expected period length around 2 1120 . It also shows other good properties, such as fast recovery from biased states and ease of use.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2227848088",
    "type": "article"
  },
  {
    "title": "Performance Analysis of Work Stealing Strategies in Large-Scale Multithreaded Computing",
    "doi": "https://doi.org/10.1145/3584186",
    "publication_date": "2023-02-16",
    "publication_year": 2023,
    "authors": "Grzegorz Kielanski; Benny Van Houdt",
    "corresponding_authors": "",
    "abstract": "Distributed systems use randomized work stealing to improve performance and resource utilization. In most prior analytical studies of randomized work stealing, jobs are considered to be sequential and are executed as a whole on a single server. In this article, we consider a homogeneous system of servers where parent jobs spawn child jobs that can feasibly be executed in parallel. When an idle server probes a busy server in an attempt to steal work, it may either steal a parent job or multiple child jobs. To approximate the performance of this system, we introduce a Quasi-Birth-Death Markov chain and express the performance measures of interest via its unique steady state. We perform simulation experiments that suggest that the approximation error tends to zero as the number of servers in the system becomes large. To further support this observation, we introduce a mean field model and show that its unique fixed point corresponds to the steady state of the Quasi-Birth-Death Markov chain. Using numerical experiments, we compare the performance of various simple stealing strategies as well as optimized strategies.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4321088709",
    "type": "article"
  },
  {
    "title": "An Improved Model of Wavelet Leader Covariance for Estimating Multifractal Properties",
    "doi": "https://doi.org/10.1145/3631522",
    "publication_date": "2023-11-03",
    "publication_year": 2023,
    "authors": "Garry Jacyna; Damon Frezza; David Slater; James R. Thompson",
    "corresponding_authors": "",
    "abstract": "Complex systems often produce multifractal signals defined by stationary increments that exhibit power-law scaling properties. The Legendre transform of the domain-dependent scaling function that defines the power law is known as the multifractal spectrum. The multifractal spectrum can also be defined by a power-series expansion of the scaling function and in practice the first two leading coefficients of that series are estimated from the discrete wavelet transform of the signal. To quantify, validate, and compare simulations of complex systems with data collected empirically from the actual system, practitioners require methods for approximating the variance associated with estimates of these coefficients. In this work, we generalize a previously developed semi-parametric statistical model for the values extracted from a discrete multi-scale wavelet transform to include both within-scale and between-scale covariance dependencies. We employ multiplicative cascades to simulate multifractals with known parameters to illustrate the necessity for this generalization and to test the precision of our improved model. The combined within- and between-scale model of covariance results in a more accurate estimate of the expected variance of the coefficients extracted from an empirical data set.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4388281992",
    "type": "article"
  },
  {
    "title": "LN: A Flexible Algorithmic Framework for Layered Queueing Network Analysis",
    "doi": "https://doi.org/10.1145/3633457",
    "publication_date": "2023-11-21",
    "publication_year": 2023,
    "authors": "Giuliano Casale; Yicheng Gao; Zifeng Niu; Lulai Zhu",
    "corresponding_authors": "",
    "abstract": "Layered queueing networks (LQNs) are an extension of ordinary queueing networks useful to model simultaneous resource possession and stochastic call graphs in distributed systems. Existing computational algorithms for LQNs have primarily focused on mean-value analysis. However, other solution paradigms, such as normalizing constant analysis and mean-field approximation, can improve the computation of LQN mean and transient performance metrics, state probabilities, and response time distributions. Motivated by this observation, we propose the first LQN meta-solver, called LN, that allows for the dynamic selection of the performance analysis paradigm to be iteratively applied to the submodels arising from layer decomposition. We report experiments where this added flexibility helps us to reduce the LQN solution errors. We also demonstrate that the meta-solver approach eases the integration of LQNs with other formalisms, such as caching models, enabling the analysis of more general classes of layered stochastic networks. Additionally, to support the accurate evaluation of the LQN submodels, we develop novel algorithms for homogeneous queueing networks consisting of an infinite server node and a set of identical queueing stations. In particular, we propose an exact method of moment algorithms, integration techniques for normalizing constants, and a fast non-iterative mean-value analysis technique.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4388846924",
    "type": "article"
  },
  {
    "title": "Empirical measurements of overheads in conservative asynchronous simulations",
    "doi": "https://doi.org/10.1145/200883.200901",
    "publication_date": "1994-10-01",
    "publication_year": 1994,
    "authors": "Mary L. Bailey; Michael A. Pagels",
    "corresponding_authors": "",
    "abstract": "In order to make formal and analytic models more realistic, overheads which were previously ignored or vastly simplified must be included. We consider the feasibility of characterizing the overheads in conservative asynchronous simulations for such models, and we focus on a single communication structure (i.e., meshes) and use both multicomputer programs and a queueing network as example applications. We find that the two most important issues for modeling are to understand how to estimate the time spent in sending null messages and how to account for the resulting overhead due to the input waiting rule. For null messages, we estimate both the number of messages sent as well as the cost per null message. The number of messages sent can often be estimated by the application, although irregularities in communication structure and edge effects in communication can affect these estimates. A constant is valid as a first-order approximation for the time per null message, but there are secondary factors that one may wish to model, such as load balancing and communication irregularities. The overhead attributable to the input waiting rule depends on several factors: communication structure, communication frequency, and processor load balancing. Irregularity in any of these dimensions can adversely affect the performance of the conservative strategy. It appears feasible to use the factors contributing to the overheads (i.e. context switch costs; null-message costs; percentage of looping due to the conservative synchronization) in a formal model to estimate the cost of the conservative overheads.",
    "cited_by_count": 5,
    "openalex_id": "https://openalex.org/W2159487318",
    "type": "article"
  },
  {
    "title": "FastSlim",
    "doi": "https://doi.org/10.1145/384169.384170",
    "publication_date": "2001-04-01",
    "publication_year": 2001,
    "authors": "Wei Jin; Xiaobai Sun; Jeffrey S. Chase",
    "corresponding_authors": "",
    "abstract": "Trace-driven simulation is a valuable tool for evaluating I/O systems. This article presents a new algorithm, called FASTSLIM, that reduces the size of I/O traces and improves simulation performance without compromising simulation accuracy. FASTSLIM is more general than existing trace reduction algorithms in two ways. First, it is prefetch-safe: traces reduced by FASTSLIM yield provably exact simulations of I/O systems that use prefetching, a key technique for improving I/O performance. Second, FASTSLIM is compatible with a wide range of replacement policies, including common practical approximations to LRU. FASTSLIM-reduced traces are safe for simulations of storage hierarchies and systems with parallel disks. This article gives a formal treatment of prefetching and replacement issues for trace reduction, introduces the FASTSLIM algorithm, proves that FASTSLIM and variants are safe for a broad range of I/O caching and prefetching systems, and presents empirical results comparing FASTSLIM to competing trace reduction algorithms.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2029483373",
    "type": "article"
  },
  {
    "title": "Guest introduction",
    "doi": "https://doi.org/10.1145/945511.945512",
    "publication_date": "2003-10-01",
    "publication_year": 2003,
    "authors": "Pierre L’Ecuyer",
    "corresponding_authors": "Pierre L’Ecuyer",
    "abstract": "introduction Share on Guest introduction Author: Pierre L'Ecuyer Université de Montréal, June 11, 2003 Université de Montréal, June 11, 2003View Profile Authors Info & Claims ACM Transactions on Modeling and Computer SimulationVolume 13Issue 4October 2003 pp 295–298https://doi.org/10.1145/945511.945512Published:01 October 2003Publication History 0citation511DownloadsMetricsTotal Citations0Total Downloads511Last 12 Months1Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2295070894",
    "type": "article"
  },
  {
    "title": "Introduction to special issue on healthcare modeling and simulation",
    "doi": "https://doi.org/10.1145/2000494.2000495",
    "publication_date": "2011-08-01",
    "publication_year": 2011,
    "authors": "Tillal Eldabi; Terry Young",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W1965733128",
    "type": "article"
  },
  {
    "title": "Asymptotic Simulation Efficiency Based on Large Deviations",
    "doi": "https://doi.org/10.1145/2499913.2499919",
    "publication_date": "2013-07-01",
    "publication_year": 2013,
    "authors": "Peter W. Glynn; Sandeep Juneja",
    "corresponding_authors": "",
    "abstract": "Consider a simulation estimator α ( c ) based on expending c units of computer time to estimate a quantity α . In comparing competing estimators for α , a natural figure of merit is to choose the estimator that minimizes the computation time needed to reduce the error probability P (| α ( c ) − α | &gt; ε ) to below some prescribed value δ . In this paper, we develop large deviations results that provide approximations to the computational budget necessary to reduce the error probability to below δ when δ is small. This approximation depends critically on both the distribution of the estimator itself and that of the random amount of computer time required to generate the estimator, and leads to different conclusions regarding the choice of preferred estimator than those obtained when one requires the error tolerance ε to be small. The “small ε ” regime leads to variance-based selection criteria, and has a long history in the simulation literature going back to Hammersley and Handscomb.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2016168378",
    "type": "article"
  },
  {
    "title": "Sharpening comparisons via gaussian copulas and semidefinite programming",
    "doi": "https://doi.org/10.1145/2379810.2379815",
    "publication_date": "2012-11-01",
    "publication_year": 2012,
    "authors": "Shane G. Henderson; Samuel M. T. Ehrlichman",
    "corresponding_authors": "",
    "abstract": "A common problem in operations research involves comparing two system designs through simulation of both systems. The comparison can often be made more accurate through careful control (coupling) of the random numbers that are used in simulating each system, with common random numbers being the standard example. We describe a new approach for coupling the random-number inputs to two systems that involves generating realizations of a Gaussian random vector and then transforming the Gaussian random vector into the desired random-number inputs. We use nonlinear semidefinite programming to select the correlation matrix of the Gaussian random vector, with the goal of sharpening the comparison.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2048065023",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2414416",
    "publication_date": "2013-01-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Online variants of the Expectation Maximization (EM) algorithm have recently been proposed to perform parameter inference with large data sets or data streams, in independent latent models and in hidden Markov models. Nevertheless, the convergence ...",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4256028417",
    "type": "paratext"
  },
  {
    "title": "Extending temporal logic to support high-level simulations",
    "doi": "https://doi.org/10.1145/210330.210336",
    "publication_date": "1995-04-01",
    "publication_year": 1995,
    "authors": "Alexander Tuzhilin",
    "corresponding_authors": "Alexander Tuzhilin",
    "abstract": "A high-level simulation language based on temporal logic is described. The language combines a large set of temporal tenses and a rich class of high-level modeling primitives. Also an implementation of the language interpreter is presented. Finally, a real-world case study is described that shows how a programmer can develop structured, reliable, and well-maintainable simulation programs using the language.",
    "cited_by_count": 4,
    "openalex_id": "https://openalex.org/W2083673874",
    "type": "article"
  },
  {
    "title": "FISTE",
    "doi": "https://doi.org/10.1145/1596519.1596521",
    "publication_date": "2009-10-01",
    "publication_year": 2009,
    "authors": "Benjamin Zhong Ming Feng; Changcheng Huang; Michael Devetsikiotis",
    "corresponding_authors": "",
    "abstract": "The goal of traffic engineering is to achieve a target Quality of Service (QoS) while maximizing network utilization. While determining the QoS for end-to-end paths in a network under self-similar traffic models is difficult, end-to-end network performance analysis is still essential in providing QoS to networks such as Virtual Private Networks (VPN) and Peer-to-Peer (P2P) networks. The Fast Importance Sampling based Traffic Engineering (FISTE) approach proposed in this article is a prediction-based approach that maps the ingress traffic levels of a network to the QoS of end-to-end path(s) in the network. Because FISTE is a hybrid of simulation analysis and closed-form analysis, it can treat a complex network as a black box. When we combined Simulated Annealing (SA) with FISTE, the resulting approach can provide a traffic engineering solution so that multiple end-to-end QoS requirements are satisfied while the network resource utilization is maximized. FISTE originated from the concept of Importance Sampling (IS), and our approach differs from the previous Importance Sampling based approaches since this is the first time that IS is applied to multi-queue systems under Fractional Gaussian Noise (FGN) input and traffic engineering.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W1990749876",
    "type": "article"
  },
  {
    "title": "Modeling the interactions between MAC and higher layer",
    "doi": "https://doi.org/10.1145/1870085.1870092",
    "publication_date": "2010-12-01",
    "publication_year": 2010,
    "authors": "S. Shabana Begum; Ahmed Helmy; Sandeep K. Gupta",
    "corresponding_authors": "",
    "abstract": "We propose a new framework for worst-case performance evaluation of MAC protocols for wireless ad hoc networks. Given a protocol, its performance metrics and a network topology, our framework first generates MAC scenarios which achieve poor performance at MAC level. In order to evaluate the impact of these MAC scenarios on the end performance, we model the interactions between MAC interface and the MAC layer using a state transition graph and generate high-level scenarios using enumeration techniques. These high-level scenarios can be simulated and compared with heuristics developed by others to identify high-level scenarios that are expected to lead to the worst-case end performance. In order to demonstrate its usefulness, we use our framework to evaluate the worst-case performance of IEEE 802.11 DCF protocol by generating a library of MAC- and high-level scenarios. We simulate the high-level scenarios to demonstrate that the scenarios we generate exhibit the worst performance among all the scenarios, including those generated by using heuristics recently proposed by other researchers.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2034639045",
    "type": "article"
  },
  {
    "title": "Replicated Computations Results (RCR) Report for “Semantics and Efficient Simulation Algorithms for an Expressive Multi-Level Modeling Language”",
    "doi": "https://doi.org/10.1145/3055539",
    "publication_date": "2017-04-27",
    "publication_year": 2017,
    "authors": "Jane Hillston",
    "corresponding_authors": "Jane Hillston",
    "abstract": "‘Semantics and Efficient Simulation Algorithms on an Expressive Multi-Level Modeling Language,” by Helms et al. presents new work on the domain-specific modelling and simulation language ML-Rules [Maus et al. 2011]. For the first time, the language is given a formal semantics that establishes the relationship between the language and its underlying mathematical model, continuous time Markov chains. Furthermore, subclasses of the language are identified for which it is possible to specify and implement more efficient approaches to simulation. These new algorithms are demonstrated on substantial case studies. This replicated computation report focuses on the ML-Rules modelling tool, specifically, the new algorithms as demonstrated in the case studies in the paper [Helms et al. 2017]. The software was straightforward to install and use, and all experimental results from the paper could be reproduced.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2607808600",
    "type": "article"
  },
  {
    "title": "Replicated Computations Results (RCR) Report for “MNO--PQRS: Max Nonnegativity Ordering—Piecewise-Quadratic Rate Smoothing”",
    "doi": "https://doi.org/10.1145/3097350",
    "publication_date": "2017-07-31",
    "publication_year": 2017,
    "authors": "Christos Alexopoulos",
    "corresponding_authors": "Christos Alexopoulos",
    "abstract": "The article “MNOPQRS: Max Nonnegativity Ordering—Piecewise-Quadratic Rate Smoothing” by Chen and Schmeiser constructs a smooth piecewise-quadratic rate estimate for a nonhomogeneuous Poisson process based on event counts over k adjacent time intervals. The event times can be generated by generating a Poisson process with unit rate and inverting the cumulative rate function or by the thinning technique. The overall algorithm has O ( k 2 ) time complexity and O ( k ) space requirements in the number of intervals. This replicated computation report focuses on the reproducibility of the experimental results in the aforementioned article.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2750957720",
    "type": "article"
  },
  {
    "title": "Corrigendum",
    "doi": "https://doi.org/10.1145/1596519.1596525",
    "publication_date": "2009-10-01",
    "publication_year": 2009,
    "authors": "Soumyadip Ghosh; Shane G. Henderson",
    "corresponding_authors": "",
    "abstract": "This note corrects an error in Ghosh and Henderson [2003].",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4241405541",
    "type": "erratum"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1596519",
    "publication_date": "2009-10-01",
    "publication_year": 2009,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Halton sequences have always been quite popular with practitioners, in part because of their intuitive definition and ease of implementation. However, in their original form, these sequences have also been known for their inadequacy to integrate ...",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4249711255",
    "type": "paratext"
  },
  {
    "title": "Statistical independence properties of inversive pseudorandom vectors over parts of the period",
    "doi": "https://doi.org/10.1145/280265.280271",
    "publication_date": "1998-04-01",
    "publication_year": 1998,
    "authors": "Frank Emmerich",
    "corresponding_authors": "Frank Emmerich",
    "abstract": "This article deals with the inversive method for generating uniform pseudorandom vectors. Statistical independence properties of the generated pseudorandom vector sequences over parts of the period are considered based on the discrete discrepancy of corresponding point sets. An upper bound for the average value of these discrete discrepancies is established.",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2019476304",
    "type": "article"
  },
  {
    "title": "Transitioning Spiking Neural Network Simulators to Heterogeneous Hardware",
    "doi": "https://doi.org/10.1145/3422389",
    "publication_date": "2021-04-23",
    "publication_year": 2021,
    "authors": "Quang Anh Pham Nguyen; Philipp Andelfinger; Wen Jun Tan; Wentong Cai; Alois Knoll",
    "corresponding_authors": "",
    "abstract": "Spiking neural networks (SNN) are among the most computationally intensive types of simulation models, with node counts on the order of up to 10 11 . Currently, there is intensive research into hardware platforms suitable to support large-scale SNN simulations, whereas several of the most widely used simulators still rely purely on the execution on CPUs. Enabling the execution of these established simulators on heterogeneous hardware allows new studies to exploit the many-core hardware prevalent in modern supercomputing environments, while still being able to reproduce and compare with results from a vast body of existing literature. In this article, we propose a transition approach for CPU-based SNN simulators to enable the execution on heterogeneous hardware (e.g., CPUs, GPUs, and FPGAs), with only limited modifications to an existing simulator code base and without changes to model code. Our approach relies on manual porting of a small number of core simulator functionalities as found in common SNN simulators, whereas the unmodified model code is analyzed and transformed automatically. We apply our approach to the well-known simulator NEST and make a version executable on heterogeneous hardware available to the community. Our measurements show that at full utilization, a single GPU achieves the performance of about 9 CPU cores. A CPU-GPU co-execution with load balancing is also demonstrated, which shows better performance compared to CPU-only or GPU-only execution. Finally, an analytical performance model is proposed to heuristically determine the optimal parameters to execute the heterogeneous NEST.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3159007080",
    "type": "article"
  },
  {
    "title": "A Practical Approach to Subset Selection for Multi-objective Optimization via Simulation",
    "doi": "https://doi.org/10.1145/3462187",
    "publication_date": "2021-08-16",
    "publication_year": 2021,
    "authors": "Christine Currie; Thomas Monks",
    "corresponding_authors": "",
    "abstract": "We describe a practical two-stage algorithm, BootComp, for multi-objective optimization via simulation. Our algorithm finds a subset of good designs that a decision-maker can compare to identify the one that works best when considering all aspects of the system, including those that cannot be modeled. BootComp is designed to be straightforward to implement by a practitioner with basic statistical knowledge in a simulation package that does not support sequential ranking and selection. These requirements restrict us to a two-stage procedure that works with any distributions of the outputs and allows for the use of common random numbers. Comparisons with sequential ranking and selection methods suggest that it performs well, and we also demonstrate its use analyzing a real simulation aiming to determine the optimal ward configuration for a UK hospital.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W3194102059",
    "type": "article"
  },
  {
    "title": "Perwez Shahabuddin, 1962--2005",
    "doi": "https://doi.org/10.1145/1225275.1225277",
    "publication_date": "2007-04-01",
    "publication_year": 2007,
    "authors": "Sigrún Andradóttir; Paul Glasserman; Peter W. Glynn; Philip Heidelberger; Sandeep Juneja",
    "corresponding_authors": "",
    "abstract": "Perwez Shahabuddin was an accomplished researcher, teacher, and participant in the simulation community. This article provides an overview of his career and a summary of some of his many professional accomplishments.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W1998500090",
    "type": "article"
  },
  {
    "title": "3D reconstruction and visualization of astrophysical wind volumes using physical models",
    "doi": "https://doi.org/10.1145/1391978.1391980",
    "publication_date": "2008-09-01",
    "publication_year": 2008,
    "authors": "Ioannis Pachoulakis",
    "corresponding_authors": "Ioannis Pachoulakis",
    "abstract": "This article reports on the application of a framework used to model, simulate and visualize the 3D structure of astrophysical wind volumes. The modeling methodology is similar to multidirectional medical tomography in that the spatial structure of an extended target can be reconstructed from a sequence of images obtained by scanning that target from several directions. Even though the controlled environment realized in diagnostic radiology cannot be replicated in the study of astrophysical phenomena, strong candidates for astrophysical tomography do exist in hot, close double stars locked in orbits around a common center of mass. As the Keplerian orbit is traced out, the geometry presented to the observer varies so that each star constitutes an analyzer upon its companion's wind and probes its structure. Since these targets are too far to be resolved spatially, we study and model the UV spectral lines of prominent wind ions, which scatter photospheric UV light so that line shapes vary as the stars revolve and as inhomogeneities form, propagate, and evolve in the composite wind. The framework presented is applied to two hot close binaries near the applicability limits of the discussed methodology. Two novel custom-made tools that aid the analysis of the spectra and the visualization of the results are also presented. The first of these, the Spectrum Analyzer and Animator , automates the derivation of light curves from the observed spectra and the generation of synthetic binary wind-line profiles, which reproduce the morphologies and variabilities of the observed wind profiles. After the composite wind structure of a binary has been recovered, the second tool, the Binary 3D Renderer —also authored in IDL—aids the visualization of the results by simulating the motion of the system (stars, winds and wind-interaction effects) around the binary's center of mass. The Binary 3D Renderer thus repackages the end product of a lengthy physical modeling process to generate physically sound, realistic multimedia content and increase the effectiveness and communication impact of the research results.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2000239147",
    "type": "article"
  },
  {
    "title": "Editor's introduction",
    "doi": "https://doi.org/10.1145/1225275.1225276",
    "publication_date": "2007-04-01",
    "publication_year": 2007,
    "authors": "James R. Wilson",
    "corresponding_authors": "James R. Wilson",
    "abstract": "No abstract available.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2044145873",
    "type": "article"
  },
  {
    "title": "Fast simulation of broadband telecommunications networks carrying long-range dependent bursty traffic",
    "doi": "https://doi.org/10.1145/502109.502112",
    "publication_date": "2001-07-01",
    "publication_year": 2001,
    "authors": "José R. Gallardo; Dimitrios Makrakis; Luis Orozco–Barbosa",
    "corresponding_authors": "",
    "abstract": "A technique for the fast simulation of broadband communication systems is proposed, which is based on regenerative Importance Sampling techniques and on large-deviation results. Our algorithm is applicable to estimate the probability of rare events when modeling the offered traffic using Fractional Stable Noise (FSN) processes (including Fractional Gaussian Noise as a particular case), which have been recently proved to be able to capture both the long-range dependence and the burstiness of today's aggregate network traffic. An exact description of FSN processes is given, as well as an approximation that allows for the application of Importance Sampling techniques. The results obtained for a simple example are also included.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2027801241",
    "type": "article"
  },
  {
    "title": "On the statistical independence of compound pseudorandom numbers over part of the period",
    "doi": "https://doi.org/10.1145/502109.502113",
    "publication_date": "2001-07-01",
    "publication_year": 2001,
    "authors": "Mordechay B. Levin",
    "corresponding_authors": "Mordechay B. Levin",
    "abstract": "This article deals with the compound methods with modulus m for generating uniform pseudorandom numbers, which have been introduced recently. Equidistribution and statistical independence properties of the generated sequences over part of the period are studied based on the discrepancy of d -tuples of successive pseudorandom numbers. It is shown that there exist parameters in compound methods such that the discrepancy over part of the period of the corresponding point sets in the d -dimensional unit cube is of an order magnitude of O ( N -1/2 (log N ) d+3 ) for all N =1, …, m . This result is applied to the compound nonlinear, inversive and explicit inversive congruential methods.",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2110936352",
    "type": "article"
  },
  {
    "title": "Replicated Computations Results (RCR) Report for “Design and Verification of Trusted Collective Adaptive Systems”",
    "doi": "https://doi.org/10.1145/3170502",
    "publication_date": "2018-02-22",
    "publication_year": 2018,
    "authors": "Maurice H. ter Beek",
    "corresponding_authors": "Maurice H. ter Beek",
    "abstract": "The article “Design and Verification of Trusted Collective Adaptive Systems” by Aldini proposes a process-algebraic framework for modeling and verifying trusted collective adaptive systems. To favor reuse, the system and trust models can be specified separately, only to be integrated at the semantic level. Through a combination of behavioral equivalence checking and model checking against branching-time temporal logic with trust predicates, the framework allows comparative analyses of different trust models as well as analyses of the effects of attacks to the trust models. The applicability of the formal framework is illustrated by means of two representative use cases: the security analysis of a trust-incentive service management system and a comparison of two different reputation systems. This replicated computations results report focuses on the reproducibility of the experiments performed in the aforementioned article, i.e. on the automatic verification of properties against models of these use cases encoded in the well-known NuSMV model checker. It was straightforward to reproduce all results from the article in reasonable time using a standard laptop machine.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2790066037",
    "type": "article"
  },
  {
    "title": "Variance and Derivative Estimation of Virtual Performance",
    "doi": "https://doi.org/10.1145/3209959",
    "publication_date": "2018-07-13",
    "publication_year": 2018,
    "authors": "Yujing Lin; Barry L. Nelson",
    "corresponding_authors": "",
    "abstract": "Virtual performance is a class of time-dependent performance measures conditional on a particular event occurring at time τ 0 for a (possibly) nonstationary stochastic process; virtual waiting time of a customer arriving to a queue at time τ 0 is one example. Virtual statistics are estimators of the virtual performance. In this article, we go beyond the mean to propose estimators for the variance, and for the derivative of the mean with respect to time, of virtual performance, examining both their small-sample and asymptotic properties. We also provide a modified K -fold cross validation method for tuning the parameter k for the difference-based variance estimator, and we evaluate the performance of both variance and derivative estimators via controlled studies and a realistic illustration. The variance and derivative provide useful information that is not apparent in the mean of virtual performance.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2883205547",
    "type": "article"
  },
  {
    "title": "Keddah",
    "doi": "https://doi.org/10.1145/3301503",
    "publication_date": "2019-06-19",
    "publication_year": 2019,
    "authors": "Jie Deng; Gareth Tyson; Félix Cuadrado; Steve Uhlig",
    "corresponding_authors": "",
    "abstract": "As a distributed system, Hadoop heavily relies on the network to complete data-processing jobs. While the traffic generated by Hadoop jobs is critical for job execution performance, the actual behaviour of Hadoop network traffic is still poorly understood. This lack of understanding greatly complicates research relying on Hadoop workloads. In this article, we explore Hadoop traffic through empirical traces. We analyse the generated traffic of multiple types of MapReduce jobs, with varying input sizes, and cluster configuration parameters. We present Keddah, a toolchain for capturing, modelling, and reproducing Hadoop traffic, for use with network simulators to better capture the behaviour of Hadoop. By imitating the Hadoop traffic generation process and considering the YARN resource allocation, Keddah can be used to create Hadoop traffic workloads, enabling reproducible Hadoop research in more realistic scenarios.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2949861569",
    "type": "article"
  },
  {
    "title": "Infinite Swapping using IID Samples",
    "doi": "https://doi.org/10.1145/3317605",
    "publication_date": "2019-06-18",
    "publication_year": 2019,
    "authors": "Paul Dupuis; Guo-Jhen Wu; Michael Snarski",
    "corresponding_authors": "",
    "abstract": "We propose a new method for estimating rare event probabilities when independent samples are available. It is assumed that the underlying probability measures satisfy a large deviation principle with a scaling parameter ε that we call temperature. We show how by combining samples at different temperatures, one can construct an estimator with greatly reduced variance. Although as presented here the method is not as broadly applicable as other rare event simulation methods, such as splitting or importance sampling, it does not require any problem-dependent constructions.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2963848399",
    "type": "article"
  },
  {
    "title": "Reproducibility Report for the Article: <i>Parallel Simulation of Quantum Networks with Distributed Quantum State Management</i>",
    "doi": "https://doi.org/10.1145/3639704",
    "publication_date": "2024-01-08",
    "publication_year": 2024,
    "authors": "Andrea Piccione",
    "corresponding_authors": "Andrea Piccione",
    "abstract": "The examined article introduces a parallel version of SeQUeNCe, a Discrete Event Simulator for quantum networks. The authors have deposited their artifact on Zenodo, meeting the criteria for long-term preservation required by the Artifacts Available badge. The software within the artifact functions correctly with minor adjustments, aligning with the article’s relevance and earning the Artifacts Evaluated—Functional badge. Additionally, due to the reasonable quality and customizability of the artifact, the Artifacts Evaluated—Reusable badge has also been awarded. The authors didn’t request the Artifacts Evaluated—Reproduced badge and they did not include the scripts used to generate and display their experimental results. As a result, it has not been possible to replicate the results published in their article.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4390660514",
    "type": "article"
  },
  {
    "title": "End-to-End Statistical Model Checking for Parameterization and Stability Analysis of ODE Models",
    "doi": "https://doi.org/10.1145/3649438",
    "publication_date": "2024-02-24",
    "publication_year": 2024,
    "authors": "David Julien; Gilles Ardourel; Guillaume Cantin; Benoît Delahaye",
    "corresponding_authors": "",
    "abstract": "We propose a simulation-based technique for the parameterization and the stability analysis of parametric Ordinary Differential Equations. This technique is an adaptation of Statistical Model Checking, often used to verify the validity of biological models, to the setting of Ordinary Differential Equations systems. The aim of our technique is to estimate the probability of satisfying a given property under the variability of the parameter or initial condition of the ODE, with any metrics of choice. To do so, we discretize the values space and use statistical model checking to evaluate each individual value w.r.t. provided data. Contrary to other existing methods, we provide statistical guarantees regarding our results that take into account the unavoidable approximation errors introduced through the numerical integration of the ODE system performed while simulating. In order to show the potential of our technique, we present its application to two case studies taken from the literature, one relative to the growth of a jellyfish population, and the other concerning a well-known oscillator model.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4392131609",
    "type": "article"
  },
  {
    "title": "Rate Lifting for Stochastic Process Algebra by Transition Context Augmentation",
    "doi": "https://doi.org/10.1145/3656582",
    "publication_date": "2024-07-10",
    "publication_year": 2024,
    "authors": "Amin Soltanieh; Markus Siegle",
    "corresponding_authors": "",
    "abstract": "This article presents an algorithm for determining the unknown rates in the sequential processes of a Stochastic Process Algebra (SPA) model, provided that the rates in the combined flat model are given. Such a rate lifting is useful for model reverse engineering and model repair. Technically, the algorithm works by solving systems of nonlinear equations and—if necessary—adjusting the model’s synchronisation structure, without changing its transition system. The adjustments cause an augmentation of a transition’s context and thus enable additional control over the transition rate. The complete pseudo-code of the rate lifting algorithm is included and discussed in the article, and its practical usefulness is demonstrated by two case studies. The approach taken by the algorithm exploits some structural and behavioural properties of SPA systems, which are formulated here for the first time and could be very beneficial also in other contexts, such as compositional system verification.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4394569816",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue for INFORMS Simulation Society (I-Sim) Workshop, 2021",
    "doi": "https://doi.org/10.1145/3655711",
    "publication_date": "2024-04-30",
    "publication_year": 2024,
    "authors": "Russell R. Barton; Marvin K. Nakayama; Uday V. Shanbhag; Eunhye Song",
    "corresponding_authors": "",
    "abstract": "The journal is pleased to publish the abstracts of the six finalists of the 2016 Manufacturing and Service Operations Management Society's student paper competition.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4396895949",
    "type": "article"
  },
  {
    "title": "Computation Offloading and Band Selection for IoT Devices in Multi-Access Edge Computing",
    "doi": "https://doi.org/10.1145/3670400",
    "publication_date": "2024-06-03",
    "publication_year": 2024,
    "authors": "Kaustabha Ray; Ansuman Banerjee",
    "corresponding_authors": "",
    "abstract": "The advent of Multi-Access Edge Computing (MEC) has enabled service providers to mitigate high network latencies often encountered in accessing cloud services. The key idea of MEC involves service providers deploying containerized application services on MEC servers situated near Internet-of-Things (IoT) device users. The users access these services via wireless base stations with ultra low latency. Computation tasks of IoT devices can then either be executed locally on the devices or on the MEC servers. A key cornerstone of the MEC environment is an offloading policy utilized to determine whether to execute computation tasks on IoT devices or to offload the tasks to MEC servers for processing. In this work, we propose a two phase Probabilistic Model Checking based offloading policy catering to IoT device user preferences. The first stage evaluates the trade-offs between local vs server execution while the second stage evaluates the trade-offs between choice of wireless communication bands for offloaded tasks. We present experimental results in practical scenarios on data gathered from an IoT test-bed setup with benchmark applications to show the benefits of an adaptive preference-aware approach over conventional approaches in the MEC offloading context.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4399292405",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on QEST 2022, Part 1",
    "doi": "https://doi.org/10.1145/3671146",
    "publication_date": "2024-07-10",
    "publication_year": 2024,
    "authors": "Erika Ábrahám; Marco Paolieri",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4400497348",
    "type": "article"
  },
  {
    "title": "Virtual Time III, Part 3: Throttling and Message Cancellation",
    "doi": "https://doi.org/10.1145/3678173",
    "publication_date": "2024-07-17",
    "publication_year": 2024,
    "authors": "David Jefferson; P. D. Barnes",
    "corresponding_authors": "",
    "abstract": "This is Part 3 of a trio of papers that unify in a natural way the two historically distinct parallel discrete event synchronization paradigms, optimistic and conservative, combining the best properties of both into a single framework called Unified Virtual Time (UVT) . In this part, we survey the synchronization effects that can be achieved by restricting to corner cases the relationships permitted among the control variables, GVT , CVT , TVT , and LVT , which were defined in Part 1 . We also survey various throttling policies from the literature and describe how they can be implemented in UVT by controlling the value of TVT , including policies that can take advantage of rollback in addition to LP blocking. A significant result is a new category of efficient and higher precision throttling algorithms for optimistic execution that are based on optimistic lookahead , defined in a way that is symmetric to what we now call the conservative lookahead information that is traditionally used for conservative synchronization. Finally, we present a novel algorithm allowing the choice between lazy and aggressive cancellation to be made on a message-by-message basis using either external logic expressed in the model code, or policy code internal to the simulator, or a mixture of both.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4400736761",
    "type": "article"
  },
  {
    "title": "Data Farming the Parameters of Simulation-Optimization Solvers",
    "doi": "https://doi.org/10.1145/3680282",
    "publication_date": "2024-08-13",
    "publication_year": 2024,
    "authors": "Sara Shashaani; David J. Eckman; Susan M. Sanchez",
    "corresponding_authors": "",
    "abstract": "The performance of a simulation-optimization algorithm, a.k.a. a solver, depends on its parameter settings. Much of the research to date has focused on how a solver’s parameters affect its convergence and other asymptotic behavior. While these results are important for providing a theoretical understanding of a solver, they can be of limited utility to a user who must set up and run the solver on a particular problem. When running a solver in practice, good finite-time performance is paramount. In this article, we explore the relationship between a solver’s parameter settings and its finite-time performance by adopting a data farming approach. The approach involves conducting and analyzing the outputs of a designed experiment wherein the factors are the solver’s parameters and the responses are assorted performance metrics measuring the solver’s speed and solution quality over time. We demonstrate this approach with a study of the ASTRO-DF solver when solving a stochastic activity network problem and an inventory control problem. Through these examples, we show that how some of the solver’s parameters are set greatly affects its ability to achieve rapid, reliable progress and gain insights into the solver’s inner workings. We discuss the implications of using this framework for tuning solver parameters, as well as for addressing related questions of interest to solver specialists and generalists.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4400919558",
    "type": "article"
  },
  {
    "title": "A Uniform Error Bound for Stochastic Kriging: Properties and Implications on Simulation Experimental Design",
    "doi": "https://doi.org/10.1145/3682059",
    "publication_date": "2024-07-29",
    "publication_year": 2024,
    "authors": "Xi Chen; Yutong Zhang; Guangrui Xie; Zhang Jingtao",
    "corresponding_authors": "",
    "abstract": "In this work, we propose a method to construct a uniform error bound for the SK predictor. In investigating the asymptotic properties of the proposed uniform error bound, we examine the convergence rate of SK’s predictive variance under the supremum norm in both fixed and random design settings. Our analyses reveal that the large-sample properties of SK prediction depend on the design-point sampling scheme and the budget allocation scheme adopted. Appropriately controlling the order of noise variances through budget allocation is crucial for achieving a desirable convergence rate of SK’s approximation error, as quantified by the uniform error bound, and for maintaining SK’s numerical stability. Moreover, we investigate the impact of noise variance estimation on the uniform error bound’s performance theoretically and numerically. We demonstrate the superiority of the proposed uniform bound to the Bonferroni correction-based simultaneous confidence interval under various experimental settings through numerical evaluations.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4401090734",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on PADS 2022",
    "doi": "https://doi.org/10.1145/3698273",
    "publication_date": "2024-11-25",
    "publication_year": 2024,
    "authors": "Dong Jin; Christopher D. Carothers",
    "corresponding_authors": "",
    "abstract": "Special Issue Part 1 (Issue 3) and Part 2 (Issue 4) of AIEDAM are based on a workshop on Learning and Creativity held at the 2002 conference on Artificial Intelligence in Design, AID '02 (www.cad.strath.ac.uk/AID02_workshop/Workshop_webpage.html; Gero, ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4404693757",
    "type": "article"
  },
  {
    "title": "Multitasking simulation of a boiler system using qualitative model-based reasoning",
    "doi": "https://doi.org/10.1145/149516.149522",
    "publication_date": "1992-10-01",
    "publication_year": 1992,
    "authors": "Yuh-Jeng Lee; James F. Stascavage",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Multitasking simulation of a boiler system using qualitative model-based reasoning Authors: Yuh-Jeng Lee Naval Postgraduate School, Monterey, CA Naval Postgraduate School, Monterey, CAView Profile , James F. Stascavage Naval Postgraduate School, Monterey, CA Naval Postgraduate School, Monterey, CAView Profile Authors Info & Claims ACM Transactions on Modeling and Computer SimulationVolume 2Issue 4Oct. 1992 pp 285–306https://doi.org/10.1145/149516.149522Published:01 October 1992Publication History 0citation658DownloadsMetricsTotal Citations0Total Downloads658Last 12 Months18Last 6 weeks7 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 3,
    "openalex_id": "https://openalex.org/W2082756870",
    "type": "article"
  },
  {
    "title": "Enhancing Response Predictions with a Joint Gaussian Process Model for Stochastic Simulation Models",
    "doi": "https://doi.org/10.1145/3364219",
    "publication_date": "2020-01-31",
    "publication_year": 2020,
    "authors": "Songhao Wang; Szu Hui Ng",
    "corresponding_authors": "",
    "abstract": "The stochastic Gaussian process model has been widely used in stochastic simulation metamodeling. In practice, the performance of this model can be largely affected by the noise in the observations. In this article, we propose an approach to mitigate the impact of the noisy observations by jointly modeling the response of interest with a correlated but less-noisy auxiliary response. The main idea is to leverage on and learn from the correlated and more accurate response to improve the prediction. To achieve this, we extend the existing deterministic multi-response model for stochastic simulation to jointly model the two responses, use some simplified examples to show the benefit of the proposed model, and investigate the input estimation of this model. Quantile prediction is used to illustrate the efficiency of the proposed approach by jointly modeling it with the expectation, which typically has a less noisy estimator compared with that of the quantile. Several numerical examples are then conducted, and the results show that the joint model can provide better performance. These promising results illustrate the potential of this joint model especially in situations where the response of interest is much noisier or when observations are scarce. We further propose a two-stage design approach based on the multi-response model to more efficiently utilize limited computing budget to improve predictions. We also see from these designs the benefits of the joint model, where the more accurate auxiliary response observations can be used to improve the response of interest.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3004650461",
    "type": "article"
  },
  {
    "title": "Generalized Probabilistic Bisection for Stochastic Root Finding",
    "doi": "https://doi.org/10.1145/3355607",
    "publication_date": "2020-01-31",
    "publication_year": 2020,
    "authors": "Sergio Rodriguez; Michael Ludkovski",
    "corresponding_authors": "",
    "abstract": "We consider numerical schemes for root finding of noisy responses through generalizing the Probabilistic Bisection Algorithm (PBA) to the more practical context where the sampling distribution is unknown and location dependent. As in standard PBA, we rely on a knowledge state for the approximate posterior of the root location. To implement the corresponding Bayesian updating, we also carry out inference of oracle accuracy, namely learning the probability of the correct response. To this end we utilize batched querying in combination with a variety of frequentist and Bayesian estimators based on majority vote, as well as the underlying functional responses, if available. For guiding sampling selection we investigate both entropy-directed sampling and quantile sampling. Our numerical experiments show that these strategies perform quite differently; in particular, we demonstrate the efficiency of randomized quantile sampling, which is reminiscent of Thompson sampling. Our work is motivated by the root-finding subroutine in pricing of Bermudan financial derivatives, illustrated in the last section of the article.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3004680087",
    "type": "article"
  },
  {
    "title": "Omnithermal Perfect Simulation for Multi-server Queues",
    "doi": "https://doi.org/10.1145/3361743",
    "publication_date": "2020-01-31",
    "publication_year": 2020,
    "authors": "Stephen B. Connor",
    "corresponding_authors": "Stephen B. Connor",
    "abstract": "A number of perfect simulation algorithms for multi-server First Come First Served queues have recently been developed. Those of Connor and Kendall [6] and Blanchet et al. [4] use dominated Coupling from the Past (domCFTP) to sample from the equilibrium distribution of the Kiefer--Wolfowitz workload vector for stable M / G / c and GI / GI / c queues, respectively, using Random Assignment queues as dominating processes. In this article, we answer a question posed by Connor and Kendall [6] by demonstrating how these algorithms may be modified to carry out domCFTP simultaneously for a range of values of c (the number of servers).",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3004830187",
    "type": "article"
  },
  {
    "title": "The Square Root Rule for Adaptive Importance Sampling",
    "doi": "https://doi.org/10.1145/3350426",
    "publication_date": "2020-03-20",
    "publication_year": 2020,
    "authors": "Art B. Owen; Yi Zhou",
    "corresponding_authors": "",
    "abstract": "In adaptive importance sampling and other contexts, we have K &gt; 1 unbiased and uncorrelated estimates μ ^ k of a common quantity μ. The optimal unbiased linear combination weights them inversely to their variances, but those weights are unknown and hard to estimate. A simple deterministic square root rule based on a working model that Var(μ ^ k ) ∝ k −1/2 gives an unbiased estimate of μ that is nearly optimal under a wide range of alternative variance patterns. We show that if Var(μ ^ k )∝ k − y for an unknown rate parameter y ∈[0,1], then the square root rule yields the optimal variance rate with a constant that is too large by at most 9/8 for any 0 ⩽ y ⩽ 1 and any number K of estimates. Numerical work shows that rule is similarly robust to some other patterns with mildly decreasing variance as k increases.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3013051644",
    "type": "article"
  },
  {
    "title": "ChunkedTejas",
    "doi": "https://doi.org/10.1145/3375397",
    "publication_date": "2020-05-31",
    "publication_year": 2020,
    "authors": "Rajshekar Kalayappan; Avantika Chhabra; Smruti R. Sarangi",
    "corresponding_authors": "",
    "abstract": "Research in computer architecture is commonly done using software simulators. The simulation speed of such simulators is therefore critical to the rate of progress in research. One of the less commonly used ways to increase the simulation speed is to decompose the benchmark’s execution into contiguous chunks of instructions and simulate these chunks in parallel. Two issues arise from this approach. The first is of correctness, as each chunk (other than the first chunk) starts from an incorrect state. The second is of performance: The decomposition must be done in such a way that the simulation of all chunks finishes at nearly the same time, allowing for maximum speedup. In this article, we study these two aspects and compare three different chunking approaches (two of them are novel) and two warmup approaches (one of them is novel). We demonstrate that average speedups of up to 5.39X can be achieved (while employing eight parallel instances), while constraining the error to 0.2% on average.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3033892728",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue for Towards an Ecosystem of Simulation Models and Data",
    "doi": "https://doi.org/10.1145/3425907",
    "publication_date": "2020-10-31",
    "publication_year": 2020,
    "authors": "Peter J. Haas; Georgios Theodoropoulos",
    "corresponding_authors": "",
    "abstract": "introduction Introduction to the Special Issue for Towards an Ecosystem of Simulation Models and Data Share on Authors: Peter J. Haas View Profile , Georgios Theodoropoulos View Profile Authors Info & Claims ACM Transactions on Modeling and Computer SimulationVolume 30Issue 4December 2020 Article No.: 20pp 1–3https://doi.org/10.1145/3425907Published:23 November 2020 0citation38DownloadsMetricsTotal Citations0Total Downloads38Last 12 Months38Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3107649325",
    "type": "article"
  },
  {
    "title": "Importance Sampling for a Simple Markovian Intensity Model Using Subsolutions",
    "doi": "https://doi.org/10.1145/3502432",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Boualem Djehiche; Henrik Hult; Pierre Nyquist",
    "corresponding_authors": "",
    "abstract": "This article considers importance sampling for estimation of rare-event probabilities in a specific collection of Markovian jump processes used for, e.g., modeling of credit risk. Previous attempts at designing importance sampling algorithms have resulted in poor performance and the main contribution of the article is the design of efficient importance sampling algorithms using subsolutions. The dynamics of the jump processes cause the corresponding Hamilton-Jacobi equations to have an intricate state-dependence, which makes the design of efficient algorithms difficult. We provide theoretical results that quantify the performance of importance sampling algorithms in general and construct asymptotically optimal algorithms for some examples. The computational gain compared to standard Monte Carlo is illustrated by numerical examples.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3215346238",
    "type": "article"
  },
  {
    "title": "A Scalable Quantum Key Distribution Network Testbed Using Parallel Discrete-Event Simulation",
    "doi": "https://doi.org/10.1145/3490029",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Xiaoliang Wu; Bo Zhang; Chen Gong; Dong Jin",
    "corresponding_authors": "",
    "abstract": "Quantum key distribution (QKD) has been promoted as a means for secure communications. Although QKD has been widely implemented in many urban fiber networks, the large-scale deployment of QKD remains challenging. Today, researchers extensively conduct simulation-based evaluations for their designs and applications of large-scale QKD networks for cost efficiency. However, the existing discrete-event simulators offer models for QKD hardware and protocols based on sequential event execution, which limits the scale of the experiments. In this work, we explore parallel simulation of QKD networks to address this issue. Our contributions lay in the exploration of QKD network characteristics to be leveraged for parallel simulation as well as the development of a parallel simulation framework for QKD networks. We also investigate three techniques to improve the simulation performance including (1) a ladder queue based event list, (2) memoization for computationally intensive quantum state transformation information, and (3) optimization of the network partition scheme for workload balance. The experimental results show that our parallel simulator is 10 times faster than a sequential simulator when simulating a 128-node QKD network. Our linear-regression-based network partition scheme can further accelerate the simulation experiments up to two times over using a randomized network partition scheme.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4214934282",
    "type": "article"
  },
  {
    "title": "A New Test for Hamming-Weight Dependencies",
    "doi": "https://doi.org/10.1145/3527582",
    "publication_date": "2022-03-28",
    "publication_year": 2022,
    "authors": "David Blackman; Sebastiano Vigna",
    "corresponding_authors": "",
    "abstract": "We describe a new statistical test for pseudorandom number generators (PRNGs). Our test can find bias induced by dependencies among the Hamming weights of the outputs of a PRNG, even for PRNGs that pass state-of-the-art tests of the same kind from the literature, and particularly for generators based on F 2 -linear transformations such as the dSFMT [ 22 ], xoroshiro1024+ [ 1 ], and WELL512 [ 19 ].",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4220902698",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1122012",
    "publication_date": "2006-01-01",
    "publication_year": 2006,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Many agent-based modeling and simulation researchers and practitioners have called for varying levels of simulation interoperability ranging from shared software architectures to common agent communications languages. These calls have been at least ...",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4232026544",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1138464",
    "publication_date": "2006-04-01",
    "publication_year": 2006,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4249297357",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1147224",
    "publication_date": "2006-07-01",
    "publication_year": 2006,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Current software-based microarchitecture simulators are many orders of magnitude slower than the hardware they simulate. Hence, most microarchitecture design studies draw their conclusions from drastically truncated benchmark simulations that are often ...",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4251417635",
    "type": "paratext"
  },
  {
    "title": "Efficient Simulation of Sparse Graphs of Point Processes",
    "doi": "https://doi.org/10.1145/3565809",
    "publication_date": "2022-11-15",
    "publication_year": 2022,
    "authors": "Cyrille Mascart; David R.C. Hill; Alexandre Muzy; Patricia Reynaud-Bouret",
    "corresponding_authors": "",
    "abstract": "We derive new discrete event simulation algorithms for marked time point processes. The main idea is to couple a special structure, namely the associated local independence graph, as defined by Didelez, with the activity tracking algorithm of Muzy for achieving high-performance asynchronous simulations. With respect to classical algorithms, this allows us to drastically reduce the computational complexity, especially when the graph is sparse.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W4309224332",
    "type": "article"
  },
  {
    "title": "Optimized Real-Time Stochastic Model of Power Electronic Converters based on FPGA",
    "doi": "https://doi.org/10.1145/3678174",
    "publication_date": "2024-07-17",
    "publication_year": 2024,
    "authors": "Shinan Wang; Xizheng Guo; Zonghui Sun; Yule Wang; Xiaojie You",
    "corresponding_authors": "",
    "abstract": "Stochastic models can effectively describe the operating characteristics of power electronic converters with stochastic parameters. However, it is difficult to implement the models in field programmable gate array– (FPGA) based real-time simulation, because their high order leads to a large calculation. This article proposes an optimized real-time stochastic modeling method for power electronic converters based on generalized polynomial chaos. First, an orthogonal polynomials construction method is used based on Schmidt orthogonalization to describe stochastic variables with atypical probability distributions and provide conditions for simplifying the system model. Second, the method of probability space transformation is adopted to divide the system model into multiple sub-models to suppress the exponential growth of the model order while maintaining the statistical properties. This method has performed over the traditional stochastic modeling method. The proposed model is built on FPGA-based hardware-in-the-loop experiment platform with 1us simulation step. The optimized model uses approximately 37% fewer resources than the traditional stochastic model while maintaining the same level of accuracy.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4400736607",
    "type": "article"
  },
  {
    "title": "Trap-driven memory simulation with Tapeworm II",
    "doi": "https://doi.org/10.1145/244804.244805",
    "publication_date": "1997-01-01",
    "publication_year": 1997,
    "authors": "Richard Uhlig; David F. Nagle; Trevor Mudge; Stuart Sechrest",
    "corresponding_authors": "",
    "abstract": "article Free Access Share on Trap-driven memory simulation with Tapeworm II Authors: Richard Uhlig Univ. of Michigan, Ann Arbor Univ. of Michigan, Ann ArborView Profile , David Nagle Univ. of Michigan, Ann Arbor Univ. of Michigan, Ann ArborView Profile , Trevor Mudge Univ. of Michigan, Ann Arbor Univ. of Michigan, Ann ArborView Profile , Stuart Sechrest Univ. of Michigan, Ann Arbor Univ. of Michigan, Ann ArborView Profile Authors Info & Claims ACM Transactions on Modeling and Computer SimulationVolume 7Issue 1Jan. 1997 pp 7–41https://doi.org/10.1145/244804.244805Online:01 January 1997Publication History 0citation374DownloadsMetricsTotal Citations0Total Downloads374Last 12 Months1Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 2,
    "openalex_id": "https://openalex.org/W2159781378",
    "type": "article"
  },
  {
    "title": "Achieving per-flow fair rate allocation in Diffserv",
    "doi": "https://doi.org/10.1145/384169.384171",
    "publication_date": "2001-04-01",
    "publication_year": 2001,
    "authors": "Na Li; M. Borrego; San-Qi Li",
    "corresponding_authors": "",
    "abstract": "This article addresses the fundamental issue of providing per-flow fairness. In particular, it focuses on fairness within the Diffserv framework. We propose the Fair Allocation Derivative Estimation (FADE) algorithm for estimating flow fair share in the absence of per-flow information. FADE calculates fair share feedback using a modified quasi-Newton method. This efficient method for estimating fair share provides a more precise model than other existing fairness estimation approaches. As such, it is able to more accurately estimate fair share and quickly converge to the proper rate. The simulation compares FADE to other proposals. The results demonstrate the overall effectiveness of the algorithm.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2030353651",
    "type": "article"
  },
  {
    "title": "Guest Editors’ Introduction to Special Issue Honoring Donald L. Iglehart",
    "doi": "https://doi.org/10.1145/2822375",
    "publication_date": "2015-10-12",
    "publication_year": 2015,
    "authors": "Peter W. Glynn; Peter J. Haas",
    "corresponding_authors": "",
    "abstract": "editorial Free Access Share on Guest Editors’ Introduction to Special Issue Honoring Donald L. Iglehart Editors: Peter W. Glynn Stanford University and IBM Research Stanford University and IBM ResearchView Profile , Peter J. Haas Stanford University and IBM Research Stanford University and IBM ResearchView Profile Authors Info & Claims ACM Transactions on Modeling and Computer SimulationVolume 25Issue 4November 2015 Article No.: 21pp 1–3https://doi.org/10.1145/2822375Published:12 October 2015Publication History 0citation91DownloadsMetricsTotal Citations0Total Downloads91Last 12 Months5Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2076098974",
    "type": "article"
  },
  {
    "title": "Guest Editors’ Introduction to Special Issue on Computational Methods in Systems Biology",
    "doi": "https://doi.org/10.1145/2745799",
    "publication_date": "2015-04-06",
    "publication_year": 2015,
    "authors": "Ashutosh Gupta; Thomas A. Henzinger",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2246814650",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2594460",
    "publication_date": "2014-02-01",
    "publication_year": 2014,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We propose a method for estimating the variance parameter of a discrete, stationary stochastic process that involves combining variance estimators at different run lengths using linear regression. We show that the estimator thus obtained is first-order ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4230409517",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2764453",
    "publication_date": "2015-04-16",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Next-generation high-performance computing will require more scalable and flexible performance prediction tools to evaluate software--hardware co-design choices relevant to scientific applications and hardware architectures. We present a new class of ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4231089040",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2661171",
    "publication_date": "2015-01-14",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Consider the context of constrained Simulation Optimization (SO); that is, optimization problems where the objective and constraint functions are known through dependent Monte Carlo estimators. For solving such problems on large finite spaces, we ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4232233938",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2616590",
    "publication_date": "2014-05-02",
    "publication_year": 2014,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "In this article, we consider approximate Bayesian parameter inference for observation-driven time series models. Such statistical models appear in a wide variety of applications, including econometrics and applied mathematics. This article considers the ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4232474306",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2578853",
    "publication_date": "2014-01-01",
    "publication_year": 2014,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We propose a two-stage stochastic version of the classical economic dispatch problem with alternating-current power flow constraints, a nonconvex optimization formulation that is central to power transmission and distribution over an electricity grid. ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4233397023",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2774955",
    "publication_date": "2015-11-16",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Multiclass open queueing networks find wide applications in communication, computer, and fabrication networks. Steady-state performance measures associated with these networks is often a topic of interset. Conceptually, under mild conditions, a sequence ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4234691539",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2875131",
    "publication_date": "2016-01-28",
    "publication_year": 2016,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "In modular, hierarchical modeling, couplings (connections) describe and constrain the communication, and thus interaction, between model components. Defining couplings between a large set of components in an extensional manner—listing all existing ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4237964193",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2982568",
    "publication_date": "2016-11-18",
    "publication_year": 2016,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "In the planning of steady-state simulations, a central issue is the initial transient problem, in which an initial segment of the simulation output is adversely contaminated by initialization bias. Our article makes several contributions toward the ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4238884593",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2737798",
    "publication_date": "2015-05-06",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Continuous-time Markov chains are commonly used in practice for modeling biochemical reaction networks in which the inherent randomness of the molecular interactions cannot be ignored. This has motivated recent research effort into methods for parameter ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4241634900",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2798338",
    "publication_date": "2015-12-28",
    "publication_year": 2015,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Parallel and distributed simulations (or High-Level Architecture (HLA)-based simulations) employing optimistic synchronization allow federates to advance simulation time freely at the risk of overoptimistic executions and execution rollbacks. As a ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4243202459",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2617568",
    "publication_date": "2014-08-13",
    "publication_year": 2014,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We develop confidence intervals (CIs) for quantiles when applying variance-reduction techniques (VRTs) and sectioning. Similar to batching, sectioning partitions the independent and identically distributed (i.i.d.) outputs into nonoverlapping batches ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4247273734",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2876000",
    "publication_date": "2016-01-29",
    "publication_year": 2016,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The Quadratic Reduction Framework (QRF) is a numerical modeling framework to evaluate complex stochastic networks composed of resources featuring queueing, blocking, state-dependent behavior, service variability, temporal dependence, or a subset ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4249904013",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2892241",
    "publication_date": "2016-05-02",
    "publication_year": 2016,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The parallel and distributed simulation field has evolved and grown from its origins in the 1970s and 1980s and remains an active field of research to this day. A brief overview of research in the field is presented. Future research topics are explored ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4254242297",
    "type": "paratext"
  },
  {
    "title": "Fluid Approximation–based Analysis for Mode-switching Population Dynamics",
    "doi": "https://doi.org/10.1145/3441680",
    "publication_date": "2021-02-10",
    "publication_year": 2021,
    "authors": "Paul Piho; Jane Hillston",
    "corresponding_authors": "",
    "abstract": "Fluid approximation results provide powerful methods for scalable analysis of models of population dynamics with large numbers of discrete states and have seen wide-ranging applications in modelling biological and computer-based systems and model checking. However, the applicability of these methods relies on assumptions that are not easily met in a number of modelling scenarios. This article focuses on one particular class of scenarios in which rapid information propagation in the system is considered. In particular, we study the case where changes in population dynamics are induced by information about the environment being communicated between components of the population via broadcast communication. We see how existing hybrid fluid limit results, resulting in piecewise deterministic Markov processes, can be adapted to such models. Finally, we propose heuristic constructions for extracting the mean behaviour from the resulting approximations without the need to simulate individual trajectories.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3131161163",
    "type": "article"
  },
  {
    "title": "Distributed Virtual Time-Based Synchronization for Simulation of Cyber-Physical Systems",
    "doi": "https://doi.org/10.1145/3446237",
    "publication_date": "2021-04-18",
    "publication_year": 2021,
    "authors": "Christopher Hannon; Jiaqi Yan; Dong Jin",
    "corresponding_authors": "",
    "abstract": "Our world today increasingly relies on the orchestration of digital and physical systems to ensure the successful operations of many complex and critical infrastructures. Simulation-based testbeds are useful tools for engineering those cyber-physical systems and evaluating their efficiency, security, and resilience. In this article, we present a cyber-physical system testing platform combining distributed physical computing and networking hardware and simulation models. A core component is the distributed virtual time system that enables the efficient synchronization of virtual clocks among distributed embedded Linux devices. Virtual clocks also enable high-fidelity experimentation by interrupting real and emulated cyber-physical applications to inject offline simulation data. We design and implement two modes of the distributed virtual time: periodic mode for scheduling repetitive events like sensor device measurements, and dynamic mode for on-demand interrupt-based synchronization. We also analyze the performance of both approaches to synchronization including overhead, accuracy, and error introduced from each approach. By interconnecting the embedded devices’ general purpose IO pins, they can coordinate and synchronize with low overhead, under 50 microseconds for eight processes across four embedded Linux devices. Finally, we demonstrate the usability of our testbed and the differences between both approaches in a power grid control application.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3154437392",
    "type": "article"
  },
  {
    "title": "Bias-corrected Estimation of the Density of a Conditional Expectation in Nested Simulation Problems",
    "doi": "https://doi.org/10.1145/3462201",
    "publication_date": "2021-07-23",
    "publication_year": 2021,
    "authors": "Ran Yang; David Kent; Daniel W. Apley; Jeremy Staum; David Ruppert",
    "corresponding_authors": "",
    "abstract": "Many two-level nested simulation applications involve the conditional expectation of some response variable, where the expected response is the quantity of interest, and the expectation is with respect to the inner-level random variables, conditioned on the outer-level random variables. The latter typically represent random risk factors, and risk can be quantified by estimating the probability density function (pdf) or cumulative distribution function (cdf) of the conditional expectation. Much prior work has considered a naïve estimator that uses the empirical distribution of the sample averages across the inner-level replicates. This results in a biased estimator, because the distribution of the sample averages is over-dispersed relative to the distribution of the conditional expectation when the number of inner-level replicates is finite. Whereas most prior work has focused on allocating the numbers of outer- and inner-level replicates to balance the bias/variance tradeoff, we develop a bias-corrected pdf estimator. Our approach is based on the concept of density deconvolution, which is widely used to estimate densities with noisy observations but has not previously been considered for nested simulation problems. For a fixed computational budget, the bias-corrected deconvolution estimator allows more outer-level and fewer inner-level replicates to be used, which substantially improves the efficiency of the nested simulation.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W3183218284",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2457459",
    "publication_date": "2013-05-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "A new method for estimating Sobol' indices is proposed. The new method makes use of 3 independent input vectors rather than the usual 2. It attains much greater accuracy on problems where the target Sobol' index is small, even outperforming some oracles ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4231380489",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2499913",
    "publication_date": "2013-07-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We consider optimization of expected system performance by random search. There are two sources of random variation in this process: (i) a search-induced variability because the expected performance of the system will vary randomly according to the ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4236894103",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2556946",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4243596091",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2000494",
    "publication_date": "2011-08-01",
    "publication_year": 2011,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The British National Health Service (NHS) has a performance management framework that aims to guarantee short waiting times for patients by including mandatory targets for hospitals. DGHPSim is a suite of four components that simulates the activities of ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4243842112",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2543898",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4249611922",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1921598",
    "publication_date": "2011-03-01",
    "publication_year": 2011,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We develop four algorithms for simulation-based optimization under multiple inequality constraints. Both the cost and the constraint functions are considered to be long-run averages of certain state-dependent single-stage functions. We pose the problem ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4250495115",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2556945",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The aim of this article is to construct efficient importance sampling schemes for a rare event, namely, the buffer overflow associated with a feed-forward network with discontinuous dynamics. This is done through a piecewise constant change of measure, ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4252445839",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2556947",
    "publication_date": "2013-10-01",
    "publication_year": 2013,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4255047111",
    "type": "paratext"
  },
  {
    "title": "Setwise and filtered gibbs samplers for teletraffic analysis",
    "doi": "https://doi.org/10.1145/1734222.1734223",
    "publication_date": "2010-04-01",
    "publication_year": 2010,
    "authors": "Lachlan L. H. Andrew; Guoqi Qian; Felisa J. Vázquez-Abad",
    "corresponding_authors": "",
    "abstract": "A setwise Gibbs sampler (SGS) method is developed to simulate stationary distributions and performance measures of network occupancy of Baskett-Chandy-Muntz-Palacios (BCMP) telecommunication models. It overcomes the simulation difficulty encountered in applying the standard Gibbs sampler to closed BCMP networks with constant occupancy constraints. We show Markov chains induced by SGS converge to the target stationary distributions. This article also investigates the filtered Gibbs sampler (FGS) as an efficient method for estimating various network performance measures. It shows that FGS's efficiency is considerable, but may be improperly overestimated. A more conservative performance estimator is then presented.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2021233452",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1842722",
    "publication_date": "2010-10-01",
    "publication_year": 2010,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We present a numerical inversion method for generating random variates from continuous distributions when only the density function is given. The algorithm is based on polynomial interpolation of the inverse CDF and Gauss-Lobatto integration. The user ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4230329201",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1540530",
    "publication_date": "2009-06-01",
    "publication_year": 2009,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4236583038",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2133390",
    "publication_date": "2012-03-01",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Ankenman et al. introduced stochastic kriging as a metamodeling tool for representing stochastic simulation response surfaces, and employed a very simple example to suggest that the use of Common Random Numbers (CRN) degrades the capability of ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4239444294",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1667072",
    "publication_date": "2010-01-01",
    "publication_year": 2010,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "This article explores possibilities for designing and executing simulation models with specific analysis goals in mind, and shows that a tight coupling of the modeling and analysis phases in a simulation project can lead to dramatic improvements in the ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4243335792",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2331140",
    "publication_date": "2012-08-01",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "In this article we explain some connections between Lyapunov methods and subsolutions of an associated Isaacs equation for the design of efficient importance sampling schemes. As we shall see, subsolutions can be derived by taking an appropriate limit ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4244979178",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1870085",
    "publication_date": "2010-12-01",
    "publication_year": 2010,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4247094343",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1502787",
    "publication_date": "2009-03-01",
    "publication_year": 2009,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4247933045",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1842713",
    "publication_date": "2010-09-01",
    "publication_year": 2010,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We extend and analyze a new class of estimators for the variance parameter of a steady-state simulation output process. These estimators are based on “folded” versions of the standardized time series (STS) of the process, and are analogous to the area ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4247963455",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1734222",
    "publication_date": "2010-04-01",
    "publication_year": 2010,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "A setwise Gibbs sampler (SGS) method is developed to simulate stationary distributions and performance measures of network occupancy of Baskett-Chandy-Muntz-Palacios (BCMP) telecommunication models. It overcomes the simulation difficulty encountered in ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4255714661",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/2379810",
    "publication_date": "2012-11-01",
    "publication_year": 2012,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "In Operations Research and Management Science (OR/MS), Discrete Event Simulation (DES) models are typically created using commercial off-the-shelf simulation packages (CSPs) such as AnyLogic™, Arena™, Flexsim™, Simul8™, SLX™, Witness™, and so on. A DES ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4255904740",
    "type": "paratext"
  },
  {
    "title": "Guest editors' introduction to special issue on successes in modeling and simulation methodologies",
    "doi": "https://doi.org/10.1145/1391978.1391979",
    "publication_date": "2008-09-01",
    "publication_year": 2008,
    "authors": "Simon J. E. Taylor; George Riley",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2031223614",
    "type": "article"
  },
  {
    "title": "Replicated Computations Results (RCR) Report for “Green Simulation: Reusing the Output of Repeated Experiments”",
    "doi": "https://doi.org/10.1145/3129738",
    "publication_date": "2017-10-27",
    "publication_year": 2017,
    "authors": "Barry L. Nelson",
    "corresponding_authors": "Barry L. Nelson",
    "abstract": "“Green Simulation: Reusing the Output of Repeated Experiments” by Feng and Staum describes methods based on likelihood ratio or importance sampling theory for reusing the outputs of simulation experiments at previous parameter settings to augment and improve (by reducing the estimator variance) simulation experiments at new parameter settings. The article presents empirical results for two realistic examples in the area of finance; Matlab code for these examples was made available by the authors. The examples were straightforward to run without extensive knowledge of Matlab, and both experiment and scenario parameters can be altered easily. All experiment results in the article were reproduced.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2765267657",
    "type": "article"
  },
  {
    "title": "Replicated Computational Results (RCR) Report for “ProPPA: Probabilistic Programming for Stochastic Dynamical Systems”",
    "doi": "https://doi.org/10.1145/3161568",
    "publication_date": "2017-12-14",
    "publication_year": 2017,
    "authors": "David Parker",
    "corresponding_authors": "David Parker",
    "abstract": "“ProPPA: Probabilistic Programming for Stochastic Dynamical Systems,” by Georgoulas, Hillston, and Sanguinetti, introduces the ProPPA formalism, which brings together ideas from stochastic process algebras with those from the paradigm of probabilistic programming. The article formally defines the ProPPA language and its semantics and presents a tool-set, along with results from illustrative examples. This replicated computational results report installs and runs the tool-set and repeats the simulation-based results from the article, finding that the published results are repeatable.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2772641823",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1276927",
    "publication_date": "2007-09-01",
    "publication_year": 2007,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We demonstrate that a majority of modern random number generators, such as the newest version of rand.c, ranlux, and combined multiple recursive generators, have some manifest correlations in their outputs if the initial state is filled up using another ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4231031402",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1391978",
    "publication_date": "2008-09-01",
    "publication_year": 2008,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "This article reports on the application of a framework used to model, simulate and visualize the 3D structure of astrophysical wind volumes. The modeling methodology is similar to multidirectional medical tomography in that the spatial structure of an ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4233272697",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1456645",
    "publication_date": "2008-12-01",
    "publication_year": 2008,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "This article describes a Time Warp simulation algorithm for discrete event models that are described in terms of the Discrete Event System Specification (DEVS). The article shows how the total state transition and total output function of a DEVS atomic ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4236273702",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3015562",
    "publication_date": "2017-07-06",
    "publication_year": 2017,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Cells exhibit stochastic behavior when the number of molecules is small. Hence a stochastic reaction-diffusion simulator capable of working at scale can provide a more accurate view of molecular dynamics within the cell. This article describes a ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4241089883",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1371574",
    "publication_date": "2008-07-01",
    "publication_year": 2008,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "This article describes a framework for generation and simulation of surrounding vehicles in a driving simulator. The proposed framework generates a traffic stream, corresponding to a given target flow and simulates realistic interactions between ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4241295314",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1189756",
    "publication_date": "2007-01-01",
    "publication_year": 2007,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4241483813",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1243991",
    "publication_date": "2007-07-01",
    "publication_year": 2007,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "In this article we describe HLA_AGENT, a tool for the distributed simulation of agent-based systems, which integrates the SIM_AGENT agent toolkit and the High Level Architecture (HLA) simulator interoperability framework. HLA_AGENT offers enhanced ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4243173218",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3155315",
    "publication_date": "2017-12-20",
    "publication_year": 2017,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The classical Adomian decomposition method frequently used to solve linear and nonlinear algebraic or integro-differential equations of ordinary and partial type is revisited. Rewriting the technique in an elegant form, a parameter so-called as the ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4245569421",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1346325",
    "publication_date": "2008-04-01",
    "publication_year": 2008,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Simulation of Internet worms (and other malware) requires tremendous computing resources when every packet generated by the phenomena is modeled individually; on the other hand, models of worm growth based on differential equations lack the significant ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4247190342",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1315575",
    "publication_date": "2007-12-01",
    "publication_year": 2007,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We motivate and describe improved fast simulation techniques for the accelerated performance evaluation of highly available services. In systems that provide such services, service unavailability events are rare due to a low component failure rate or ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4252761537",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1225275",
    "publication_date": "2007-04-01",
    "publication_year": 2007,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Perwez Shahabuddin was an accomplished researcher, teacher, and participant in the simulation community. This article provides an overview of his career and a summary of some of his many professional accomplishments.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4253048042",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3130329",
    "publication_date": "2017-09-07",
    "publication_year": 2017,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "The eXecutable Micro-Architectural Specification (xMAS) language developed in recent years finds an effective way to model on-chip communication fabrics and enables performance-bound analysis with network calculus at the micro-architectural level. For ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4253881499",
    "type": "paratext"
  },
  {
    "title": "Stochastic Approximation for Multi-period Simulation Optimization with Streaming Input Data",
    "doi": "https://doi.org/10.1145/3617595",
    "publication_date": "2023-08-29",
    "publication_year": 2023,
    "authors": "Linyun He; Uday V. Shanbhag; Eunhye Song",
    "corresponding_authors": "",
    "abstract": "We consider a continuous-valued simulation optimization (SO) problem, where a simulator is built to optimize an expected performance measure of a real-world system while parameters of the simulator are estimated from streaming data collected periodically from the system. At each period, a new batch of data is combined with the cumulative data and the parameters are re-estimated with higher precision. The system requires the decision variable to be selected in all periods. Therefore, it is sensible for the decision-maker to update the decision variable at each period by solving a more precise SO problem with the updated parameter estimate to reduce the performance loss with respect to the target system. We define this decision-making process as the multi-period SO problem and introduce a multi-period stochastic approximation (SA) framework that generates a sequence of solutions. Two algorithms are proposed: Re-start SA ( ReSA ) reinitializes the stepsize sequence in each period, whereas Warm-start SA ( WaSA ) carefully tunes the stepsizes, taking both fewer and shorter gradient-descent steps in later periods as parameter estimates become increasingly more precise. We show that under suitable strong convexity and regularity conditions, ReSA and WaSA achieve the best possible convergence rate in expected sub-optimality either when an unbiased or a simultaneous perturbation gradient estimator is employed, while WaSA accrues significantly lower computational cost as the number of periods increases. In addition, we present the regularized ReSA , which obviates the need to know the strong convexity constant and achieves the same convergence rate at the expense of additional computation.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4386251415",
    "type": "article"
  },
  {
    "title": "A Prescriptive Simulation Framework with Realistic Behavioural Modelling for Emergency Evacuations",
    "doi": "https://doi.org/10.1145/3633330",
    "publication_date": "2023-11-18",
    "publication_year": 2023,
    "authors": "Md. Shalihin Othman; Gary Tan",
    "corresponding_authors": "",
    "abstract": "Emergency and crisis simulations play a pivotal role in equipping authorities worldwide with the necessary tools to minimize the impact of catastrophic events. Various studies have explored the integration of intelligence into Multi-Agent Systems (MAS) for crisis simulation. This involves incorporating psychological behaviours from the social sciences and utilizing data-driven machine learning models with predictive capabilities. A recent advancement in behavioural modelling is the Conscious Movement Model (CMM), designed to modulate an agent’s movement patterns dynamically as the situation unfolds. Complementing this, the model incorporates a Conscious Movement Memory-Attention (CMMA) mechanism, enabling learnability through training on pedestrian trajectories extracted from video data. The CMMA facilitates mapping a pedestrian’s attention to their surroundings and understanding how their past decisions influence their subsequent actions. This study proposes an efficient framework that integrates the trained CMM into a simulation model specifically tailored for emergency evacuations, ensuring realistic outcomes. The resulting simulation framework automates strategy management and planning for diverse emergency evacuation scenarios. A single-objective method is presented for generating prescriptive analytics, offering effective strategy options based on predefined operational rules. To validate the framework’s efficacy, a case study of a theatre evacuation is conducted. In essence, this research establishes a robust simulation framework for crisis management, with a particular emphasis on modelling pedestrians during emergency evacuations. The framework generates prescriptive analytics to aid authorities in executing rescue and evacuation operations effectively.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4388798948",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on QEST 2021",
    "doi": "https://doi.org/10.1145/3631707",
    "publication_date": "2023-10-31",
    "publication_year": 2023,
    "authors": "Alessandro Abate; Andrea Marin",
    "corresponding_authors": "",
    "abstract": "introduction Share on Introduction to the Special Issue on QEST 2021 Authors: Alessandro Abate University of Oxford, UK University of Oxford, UK 0000-0002-5627-9093Search about this author , Andrea Marin University Ca’ Foscari of Venice, Italy University Ca’ Foscari of Venice, Italy 0000-0002-5958-1204Search about this author Authors Info & Claims ACM Transactions on Modeling and Computer SimulationVolume 33Issue 4Article No.: 13pp 1–2https://doi.org/10.1145/3631707Published:30 November 2023Publication History 0citation0DownloadsMetricsTotal Citations0Total Downloads0Last 12 Months0Last 6 weeks0 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4389210852",
    "type": "article"
  },
  {
    "title": "Parallelism in sequential multiprocessor simulation models",
    "doi": "https://doi.org/10.1145/210330.210333",
    "publication_date": "1995-04-01",
    "publication_year": 1995,
    "authors": "Hatem Sellami; Sudhakar Yalamanchili",
    "corresponding_authors": "",
    "abstract": "The design and analysis of multiprocessor simulation models represents a complex and computationally demanding application that is a candidate for parallel simulation. This paper examines the application of conservative parallel discrete event simulation on a set of existing “real-world” models created over the years with no thought given to the parallel execution. These models are based on a subset of Petri Nets known as Marked graphs . The results of the study of the transparent and automatic parallel execution of the simulation of these models will be presented. Focus will be on the amount of inherent parallelism in such models, means of exploiting it, obstacles prohibiting parallelization, and sensitivity to different system and model parameters.",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W2087159905",
    "type": "article"
  },
  {
    "title": "Concurrency and Discrete-Event Simulation.",
    "doi": null,
    "publication_date": "1993-01-01",
    "publication_year": 1993,
    "authors": "K. Mani Chandy; Rajive Bagrodia; Wen-Toh Liao",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 1,
    "openalex_id": "https://openalex.org/W22467963",
    "type": "article"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/985793.985794",
    "publication_date": "2004-04-01",
    "publication_year": 2004,
    "authors": "David Nicol",
    "corresponding_authors": "David Nicol",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2742597228",
    "type": "editorial"
  },
  {
    "title": "Replicated Computations Results (RCR) Report for “Reusing Search Data in Ranking and Selection",
    "doi": "https://doi.org/10.1145/3185337",
    "publication_date": "2018-07-31",
    "publication_year": 2018,
    "authors": "Xianyu Kuang; L. Jeff Hong",
    "corresponding_authors": "",
    "abstract": "“Reusing Search Data in Ranking and Selection: What Could Possibly Go Wrong?” [2] by Eckman and Henderson rigorously defines the statistical guarantees for ranking-and-selection (R8S) procedures after random search, and points out that the simulation replications collected in the search phase are conditionally dependent given the sequence of returned systems. Therefore, reusing the search data for R8S may affect the statistical guarantees. The authors further design random search algorithms to demonstrate that the correct selection guarantees of some ranking-and-selection procedures will be compromised if reusing the simulation replications taken during the search. This replicated computation report focuses on the reproducibility of the experiment results in the aforementioned article.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2885230273",
    "type": "article"
  },
  {
    "title": "Replicated Computational Results (RCR) Report for “Fast Random Integer Generation in an Interval”",
    "doi": "https://doi.org/10.1145/3239569",
    "publication_date": "2019-01-24",
    "publication_year": 2019,
    "authors": "Francesco Quaglia",
    "corresponding_authors": "Francesco Quaglia",
    "abstract": "The article “Fast Random Integer Generation in an Interval” by Lemire (2018) addressed the problem of reducing the cost of machine instructions needed for the random generation of integer values in a generic interval [0, s ). The approach taken by the author is the one of exploiting the rejection method (Neumann 1951) to build an algorithm that almost eliminates the need for performing integer division operations—the algorithm still exploits divisions by powers of two, implemented in the form of cheap shift operations. In more details, the likelihood of not requiring an integer division in the proposed algorithm is 2 L − s / 2 L , where L denotes the number of bits used to represent integer values. The author also presents a comparative experimental study where the new algorithm, and its implementation for x86 processors, are compared with solutions offered by common software libraries for different programming languages.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2914535271",
    "type": "article"
  },
  {
    "title": "Guest Editorial for the TOMACS Special Issue on the Principles of Advanced Discrete Simulation (PADS)",
    "doi": "https://doi.org/10.1145/3312749",
    "publication_date": "2019-03-15",
    "publication_year": 2019,
    "authors": "Kevin Jin; Philip A. Wilsey",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2939899730",
    "type": "editorial"
  },
  {
    "title": "Guest Editorial for the TOMACS Special Issue on the Principles of Advanced Discrete Simulation (PADS)",
    "doi": "https://doi.org/10.1145/3267459",
    "publication_date": "2018-10-03",
    "publication_year": 2018,
    "authors": "R.M. Fujimoto; C.D. Carothers",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2966397488",
    "type": "editorial"
  },
  {
    "title": "RCR Report for Analysis of Spatiotemporal Properties of Stochastic Systems Using TSTL",
    "doi": "https://doi.org/10.1145/3341093",
    "publication_date": "2019-10-31",
    "publication_year": 2019,
    "authors": "Andrea Vandin",
    "corresponding_authors": "Andrea Vandin",
    "abstract": "\"Analysis of Spatiotemporal Properties of Stochastic Systems Using TSTL\" [1] proposes a three-valued spatiotemporal logic to enrich the analysis framework for Signal Spatiotemporal Logic previously developed by the authors. This allows one to reason on the evolution of the satisfaction of properties expressed in a spatiotemporal logic, providing additional insight on the behavior of the studied system. The approach has been validated on two case studies: the fire spread and evacuation models originally presented in [2], and a novel case study on privacy in a communication network. This replicated computation result report focuses on the artifact accompanying the article, consisting in a prototypical tool implementation of the techniques presented in the article, together with all files necessary to replicate the analysis performed thereof. The artifact is available at https://ludovicalv.github.io/TOMACS/. After a few iterations with the authors, I found that the artifact agrees with the guidelines on availability (Artifact Avaliable) and replicability (Results Replicated) dictated in https://www.acm.org/publications/policies/artifact-review-badging. The software was made available in an accessible archival repository, and thanks to the instructions provided in the accompanying webapge, it has been straightforward to replicate the experimental results from the article.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2984512555",
    "type": "article"
  },
  {
    "title": "Replicated Computations Results (RCR) Report for “Statistical Abstraction for Multi-scale Spatio-temporal Systems”",
    "doi": "https://doi.org/10.1145/3341094",
    "publication_date": "2019-10-31",
    "publication_year": 2019,
    "authors": "Michele Loreti",
    "corresponding_authors": "Michele Loreti",
    "abstract": "“Statistical abstraction for multi-scale spatio-temporal systems” proposes a methodology that supports analysis of large-scaled spatio-temporal systems. These are represented via a set of agents whose behaviour depends on a perceived field. The proposed approach is based on a novel simulation strategy based on a statistical abstraction of the agents. The abstraction makes use of Gaussian Processes, a powerful class of non-parametric regression techniques from Bayesian Machine Learning, to estimate the agent’s behaviour given the environmental input. The authors use two biological case studies to show how the proposed technique can be used to speed up simulations and provide further insights into model behaviour. This replicated computation results report focuses on the scripts used in the paper to perform such analysis. The required software was straightforward to install and use. All the experimental results from the paper have been reproduced.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W2995508099",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on Qest 2017",
    "doi": "https://doi.org/10.1145/3363784",
    "publication_date": "2019-10-31",
    "publication_year": 2019,
    "authors": "Luca Bortolussi; Nathalie Bertrand",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3029100580",
    "type": "article"
  },
  {
    "title": "分散アプリケーショントラフィックのシミュレーションによるKEDDAHネットワーク評価【JST・京大機械翻訳】",
    "doi": null,
    "publication_date": "2019-01-01",
    "publication_year": 2019,
    "authors": "Deng Jie; Tyson Gareth; Cuadrado Felix; Steve Uhlig",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3193156931",
    "type": "article"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3236631",
    "publication_date": "2018-08-09",
    "publication_year": 2018,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Virtual performance is a class of time-dependent performance measures conditional on a particular event occurring at time τ0 for a (possibly) nonstationary stochastic process; virtual waiting time of a customer arriving to a queue at time τ0 is one ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4229824389",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3320014",
    "publication_date": "2019-04-25",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Performance of sequential and parallel Discrete Event Simulations (DES) is strongly influenced by the data structure used for managing and processing pending events. Accordingly, we propose and evaluate the effectiveness of our multi-tiered (two- and ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4236754697",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3372492",
    "publication_date": "2019-12-17",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4242295784",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3174299",
    "publication_date": "2018-01-31",
    "publication_year": 2018,
    "authors": "Jereme Lamps; Vignesh Babu; David M. Nicol; Vladimir Adam; Rakesh Kumar",
    "corresponding_authors": "",
    "abstract": "Integration of emulation and simulation in virtual time requires that emulated execution bursts be ascribed a duration in virtual time and that emulated execution and simulation executions be coordinated within this common virtual time basis. This ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4246929528",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3274766",
    "publication_date": "2018-10-13",
    "publication_year": 2018,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Processes in computer simulations tend to be highly repetitive. In particular, parameter studies further exasperate the situation as the same model is repeatedly executed with only partially varying parameters. Consequently, computer simulations perform ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4249709770",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/1176249",
    "publication_date": "2006-10-01",
    "publication_year": 2006,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4250889977",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3309768",
    "publication_date": "2019-02-23",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Simulation models are widely used to study complex systems. Current simulation models are generally handcrafted using expert knowledge (knowledge-driven); however, this process is slow and introduces modeler bias. This article presents an approach ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4251409211",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3190505",
    "publication_date": "2018-04-03",
    "publication_year": 2018,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "Collective adaptive systems (CAS) often adopt cooperative operating strategies to run distributed decision-making mechanisms. Sometimes, their effectiveness massively relies on the collaborative nature of individuals’ behavior. Stimulating cooperation ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4252730964",
    "type": "paratext"
  },
  {
    "title": null,
    "doi": "https://doi.org/10.1145/3341298",
    "publication_date": "2019-07-27",
    "publication_year": 2019,
    "authors": "No authors",
    "corresponding_authors": "",
    "abstract": "We propose a new method for estimating rare event probabilities when independent samples are available. It is assumed that the underlying probability measures satisfy a large deviation principle with a scaling parameter ε that we call temperature. We ...",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4256320607",
    "type": "paratext"
  },
  {
    "title": "Editorial from the New Editor-in-Chief",
    "doi": "https://doi.org/10.1145/3377148",
    "publication_date": "2020-01-31",
    "publication_year": 2020,
    "authors": "Francesco Quaglia",
    "corresponding_authors": "Francesco Quaglia",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3013315857",
    "type": "article"
  },
  {
    "title": "Editorial to the Special Issue on the Principles of Advanced Discrete Simulation (PADS)",
    "doi": "https://doi.org/10.1145/3381903",
    "publication_date": "2020-03-20",
    "publication_year": 2020,
    "authors": "Francesco Quaglia; Georgios Theodoropoulos; Alessandro Pellegrini",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3013321606",
    "type": "article"
  },
  {
    "title": "Simulation Study to Identify the Characteristics of Markov Chain Properties",
    "doi": "https://doi.org/10.1145/3361744",
    "publication_date": "2020-03-20",
    "publication_year": 2020,
    "authors": "Atiqur Rahman; Peter Kemper",
    "corresponding_authors": "",
    "abstract": "Markov models have a long tradition in modeling and simulation of dynamic systems. In this article, we look at certain properties of a discrete-time Markov chain, including entropy, trace, and second-largest eigenvalue to better understand their role for time-series analysis. We simulate a number of possible input signals, fit a discrete-time Markov chain, and explore properties with the help of Sobol indices, partial correlation coefficients, and the Morris elementary effect screening method. Our analysis suggests that the presence of a trend, periodicity, and autocorrelation impact entropy, trace, and second-largest eigenvalue to varying degrees but not independently of each other and with Markov chain parameter settings as other influencing factors. The properties of interest are promising to distinguish time-series data, as evidenced for the entropy measure by recent results in the analysis of cell development for Xenopus laevis in cell biology.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3016614282",
    "type": "article"
  },
  {
    "title": "Parametric Scenario Optimization under Limited Data",
    "doi": "https://doi.org/10.1145/3410152",
    "publication_date": "2020-10-31",
    "publication_year": 2020,
    "authors": "Henry Lam; Fengpei Li",
    "corresponding_authors": "",
    "abstract": "We consider optimization problems with uncertain constraints that need to be satisfied probabilistically. When data are available, a common method to obtain feasible solutions for such problems is to impose sampled constraints following the so-called scenario optimization approach. However, when the data size is small, the sampled constraints may not statistically support a feasibility guarantee on the obtained solution. This article studies how to leverage parametric information and the power of Monte Carlo simulation to obtain feasible solutions for small-data situations. Our approach makes use of a distributionally robust optimization (DRO) formulation that translates the data size requirement into a Monte Carlo sample size requirement drawn from what we call a generating distribution. We show that, while the optimal choice of this generating distribution is the one eliciting the data or the baseline distribution in a nonparametric divergence-based DRO, it is not necessarily so in the parametric case. Correspondingly, we develop procedures to obtain generating distributions that improve upon these basic choices. We support our findings with several numerical examples.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3106859944",
    "type": "article"
  },
  {
    "title": "Replication of Computational Results Report for “ <i>Green Simulation with Database Monte Carlo</i> ”",
    "doi": "https://doi.org/10.1145/3426823",
    "publication_date": "2020-12-31",
    "publication_year": 2020,
    "authors": "Alessandro Pellegrini",
    "corresponding_authors": "Alessandro Pellegrini",
    "abstract": "This article presents the reproducibility results associated with the article “ Green Simulation with Database Monte Carlo ,” by Mingbin Feng and Jeremy Staum. The authors have uploaded their artifact to Zenodo, which ensures a long-term retention of the artifact. The artifact, which is based on a set of R scripts, allows to easily regenerate data for the figures and the tables, it completes successfully, and allows to reproduce all the experimental results in the article. The article can thus receive the Artifacts Available , the Artifacts Evaluated—Functional , and the Results Reproduced badges.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3113881384",
    "type": "article"
  },
  {
    "title": "RCR Report of <i>“A Language for Agent-Based Discrete-Event Modeling and Simulation of Linked Lives”</i>",
    "doi": "https://doi.org/10.1145/3490030",
    "publication_date": "2022-01-07",
    "publication_year": 2022,
    "authors": "Romolo Marotta",
    "corresponding_authors": "Romolo Marotta",
    "abstract": "The artifact evaluated in this report is relevant to the article. In fact, it allows us to run the experiments and reproduce figures, and the dependencies are documented. The process to regenerate data presented in the article completes correctly, and the results are reproducible. Additionally, the authors have uploaded their artifact on permanent repositories, which ensures a long-term retention. This article can thus receive the Artifacts Available , Artifacts Evaluated–Reusable , and Results Reproduced badges.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4205789137",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Section on PADS 2020",
    "doi": "https://doi.org/10.1145/3498363",
    "publication_date": "2022-03-04",
    "publication_year": 2022,
    "authors": "Philippe J. Giabbanelli; Christopher D. Carothers",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4214814884",
    "type": "article"
  },
  {
    "title": "Replicated Computational Results (RCR) Report for “A New Test for Hamming-Weight Dependencies”",
    "doi": "https://doi.org/10.1145/3527583",
    "publication_date": "2022-03-31",
    "publication_year": 2022,
    "authors": "Xiaoliang Wu; Dong Jin",
    "corresponding_authors": "",
    "abstract": "In the paper “A New Test for Hamming-Weight Dependencies”, Blackman and Vigna propose a new statistical test for pseudorandom number generators (PRNG) . Compared with the state-of-the-art tests, the proposed test could find statistical bias in the Hamming weights of the output of the generator. The proposed test is evaluated by using generators in TestU01 [ 2 ]. The authors provide the C code used in the tests and the code successfully reproduced the results shown in the article.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4220671515",
    "type": "article"
  },
  {
    "title": "Performance Analysis of Speculative Parallel Adaptive Local Timestepping for Conservation Laws",
    "doi": "https://doi.org/10.1145/3545996",
    "publication_date": "2022-09-01",
    "publication_year": 2022,
    "authors": "Maximilian Bremer; John Bachan; Cy Chan; Clint Dawson",
    "corresponding_authors": "",
    "abstract": "Stable simulation of conservation laws, such as those used to model fluid dynamics and plasma physics applications, requires the satisfaction of the so-called Courant-Friedrichs-Lewy condition. By allowing regions of the mesh to advance with different timesteps that locally satisfy this stability constraint, significant work reduction can be attained when compared to a time integration scheme using a single timestep size. However, parallelizing this algorithm presents considerable difficulty. Since the stability condition depends on the state of the system, dependencies become dynamic and potentially non-local. In this article, we present an adaptive local timestepping algorithm using an optimistic (Timewarp-based) parallel discrete event simulation. We introduce waiting heuristics to limit misspeculation and a semi-static load balancing scheme to eliminate load imbalance as parts of the mesh require finer or coarser timesteps. Last, we outline an interface for separating the physics of the specific conservation law from the temporal integration allowing for productive adoption of our proposed algorithm. We present a misspeculation study for three conservation laws, demonstrating both the productivity of the local timestepping API, for which 74% of the lines of code are reused across different conservation laws, and the robustness of the waiting heuristics—at most 1.5% of element updates are rolled back. Our performance studies demonstrate up to a 2.8× speedup versus a baseline unoptimized local timestepping approach, a 4x improvement in per-node throughput compared to an MPI parallelization of synchronous timestepping, and scalability up to 3,072 cores on NERSC’s Cori Haswell partition.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4294031738",
    "type": "article"
  },
  {
    "title": "Replication of Computational Results Report for “Automatic Reuse, Adaption, and Execution of Simulation Experiments via Provenance Patterns”",
    "doi": "https://doi.org/10.1145/3577007",
    "publication_date": "2022-12-19",
    "publication_year": 2022,
    "authors": "Pierangelo Di Sanzo",
    "corresponding_authors": "Pierangelo Di Sanzo",
    "abstract": "In this article, a reproducibility study is presented, with reference to the computational results reported in the article “Automatic Reuse, Adaption, and Execution of Simulation Experiments via Provenance Patterns,” by P. Wilsdorf, A. Wolpers, J. Hilton, F. Haack, and A. M. Uhrmacher. Based on the achieved results, the Artifacts Available badge is assigned.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4313408453",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Section on PADS 2021",
    "doi": "https://doi.org/10.1145/3579840",
    "publication_date": "2022-10-31",
    "publication_year": 2022,
    "authors": "Saikou Y. Diallo; Andreas Tolk",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4321085085",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on PADS 2019",
    "doi": "https://doi.org/10.1145/3451235",
    "publication_date": "2021-03-26",
    "publication_year": 2021,
    "authors": "Jason Liu; Laxmikant Sanjay Kale",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3149101499",
    "type": "article"
  },
  {
    "title": "Introduction to the Special Issue on QEST 2019",
    "doi": "https://doi.org/10.1145/3463764",
    "publication_date": "2021-07-18",
    "publication_year": 2021,
    "authors": "David Parker; Verena Wolf",
    "corresponding_authors": "",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3183806883",
    "type": "article"
  },
  {
    "title": "Replicated Computational Results (RCR) Report for“A Practical Approach to Subset Selection for Multi-Objective Optimization via Simulation”",
    "doi": "https://doi.org/10.1145/3453987",
    "publication_date": "2021-07-23",
    "publication_year": 2021,
    "authors": "Philipp Andelfinger",
    "corresponding_authors": "Philipp Andelfinger",
    "abstract": "In “A Practical Approach to Subset Selection for Multi-Objective Optimization via Simulation,” Currie and Monks propose an algorithm for multi-objective simulation-based optimization. In contrast to sequential ranking and selection schemes, their algorithm follows a two-stage scheme. The approach is evaluated by comparing the results to those obtained using the existing OCBA-m algorithm for synthetic problems and for a hospital ward configuration problem. The authors provide the Python code used in the experiments in the form of Jupyter notebooks. The code successfully reproduced the results shown in the article.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W3184393151",
    "type": "article"
  },
  {
    "title": "Replication of Computational Results Report for “Doping Tests for Cyber-Physical Systems”",
    "doi": "https://doi.org/10.1145/3459667",
    "publication_date": "2021-07-18",
    "publication_year": 2021,
    "authors": "Gerrit Großmann",
    "corresponding_authors": "Gerrit Großmann",
    "abstract": "The article Doping Tests for Cyber-Physical Systems is accompanied by a prototype implementation in Python 2.7. The artifact (i.e., code and observational data) is hosted on a publicly available repository. The article contains comprehensive documentation in the appendix, and running the code is straightforward. The article Doping Tests for Cyber-Physical Systems can thus receive the Artifacts Evaluated—Reusable badge.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4235928255",
    "type": "article"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/244804.257875",
    "publication_date": "1997-01-01",
    "publication_year": 1997,
    "authors": "Philip Heidelberger",
    "corresponding_authors": "Philip Heidelberger",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4238586773",
    "type": "editorial"
  },
  {
    "title": "Editorial",
    "doi": "https://doi.org/10.1145/268403.269944",
    "publication_date": "1997-10-01",
    "publication_year": 1997,
    "authors": "David Nicol",
    "corresponding_authors": "David Nicol",
    "abstract": "No abstract available.",
    "cited_by_count": 0,
    "openalex_id": "https://openalex.org/W4245677192",
    "type": "editorial"
  }
]